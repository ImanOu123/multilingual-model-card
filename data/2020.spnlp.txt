{'en': 'CopyNext : Explicit Span Copying and Alignment in Sequence to Sequence Models C opy N ext: Explicit Span Copying and Alignment in Sequence to Sequence Models', 'ar': 'CopyNext: صريح Span Copy and Alignment in Sequence to Sequence Models', 'pt': 'CopyNext: Cópia Explícita de Span e Alinhamento em Sequência para Modelos de Sequência', 'es': 'CopyNext: Copia explícita de intervalos y alineación en modelos de secuencia a secuencia', 'fr': 'CopyNext\xa0: Copie de plage explicite et alignement en séquence par rapport aux modèles de séquence', 'ja': 'CopyNext ：シーケンスモデルへのシーケンスにおける明示的なスパンコピーとアライメント', 'zh': 'CopyNext: 显式跨度 以次复制齐', 'hi': 'CopyNext: स्पष्ट स्पैन प्रतिलिपि बनाना और अनुक्रम मॉडल के लिए अनुक्रम में संरेखण', 'ru': 'CopyNext: явное копирование диапазона и выравнивание в последовательности к моделям последовательности', 'ga': 'CopyNext: Cóipeáil Réise Soiléir agus Ailíniú i Seicheamh le Múnlaí Seichimh', 'el': 'Αντιγραφή Επόμενο: Εκφράκτη έκταση αντιγραφή και ευθυγράμμιση σε ακολουθία σε μοντέλα ακολουθίας', 'hu': 'CopyNext: Explicit Span Másolás és igazítás sorozat modellekhez', 'ka': 'შემდეგი კოპორიცია: შემდეგი მოდელების შემდეგი დაწყება და დაწყება', 'lt': 'KopijaNext: Explicit Span Copying and Alignment in Sequence Models', 'kk': 'Көшірмелеу', 'it': 'CopyNext: Espansione esplicita Copia e allineamento in sequenza a modelli di sequenza', 'mk': 'КопираNext: Explicit Span Copying and Alignment in Sequence to Sequence Models', 'ml': 'പകര്\u200dത്തുNext: Span Copying and Alignment in Sequence to Sequence Models', 'mn': 'Дараагийн хууль: Дараагийн загварын хууль болон тодорхойлолт', 'no': 'KopiereNext: Explicit Span Copying and Alignment in Sequence to Sequence Models', 'pl': 'CopyNext: Wyraźne kopiowanie i wyrównanie w sekwencji do modeli sekwencji', 'mt': 'KopjaNext: Copy Explicit Span Copying and Alignment in Sequence to Sequence Models', 'ms': 'SalinNext: Explicit Span Copying and Alignment in Sequence to Sequence Models', 'ro': 'CopyNext: Copierea și alinierea explicită a spațiului în secvență la modele secvențiale', 'sr': 'Kopirajuæi sledeæi: Eksplicitno kopiranje i ispravljanje spana u sekvenciji modela sekvencije', 'si': 'පස්සේ ප්\u200dරතිලිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළි', 'so': 'CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models', 'sv': 'CopyNext: Explicit Span Kopiering och justering i sekvens till sekvensmodeller', 'ta': 'நகல்Next: Span Copying and Alignment in Sequence to Sequence Models', 'ur': 'Next: Explicit Span Copying and Alignment in Sequence to Sequence Models', 'uz': 'Next: Explicit Span Copying and Alignment in Sequence to Sequence Models', 'vi': 'Thêm: Bắt chước và ký hiệu ứng theo chuỗi', 'bg': 'Копиране и подравняване в последователност към последователни модели', 'hr': 'Kopirajući sljedeći: Eksplicitno kopiranje i ispravljanje spana u sekvenciji modela sekvencije', 'da': 'CopyNext: Eksplicit Span Kopiering og justering i sekvens til sekvensmodeller', 'nl': 'CopyNext: Expliciet span kopiëren en uitlijnen in sequentie naar sequentiemodellen', 'ko': 'CopyNext: 시퀀스에서 시퀀스 모델로의 명시적 복사 및 정렬 경계', 'de': 'CopyNext: Explizites Kopieren und Ausrichten in Sequenz zu Sequenzmodellen', 'fa': 'کپی بعدی: کپی کردن و تنظیمات جستجو و تنظیمات جستجو در بعدی به مدل\u200cهای بعدی', 'id': 'SalinanNext: Explicit Span Copying and Alignment in Sequence to Sequence Models', 'sw': 'Next: Click Span Copying and Alignment in Sequence to Sequence Models', 'tr': 'Nusgala', 'sq': 'KopjeNext: Explicit Span Copying and Alignment in Sequence to Sequence Models', 'af': 'KopiëNext: Explicit Span Copying and Alignment in Sequence to Sequence Models', 'hy': 'Հաջորդ:', 'am': 'Next: Explicit Span Copying and Alignment in Sequence to Sequence Models', 'az': '쿮lav톛 Et', 'bs': 'Kopirajući sljedeći: Eksplicitno kopiranje i ispravljanje spana u sekvenciji modela sekvencije', 'bn': 'পরবর্তীNext: সেকেন্স মোডেলে স্প্যান কপিপি এবং একত্রিত করুন', 'cs': 'Kopírování a zarovnání v sekvenci na sekvenční modely', 'et': 'Kopeeri järgmine: Selgesõnaline ulatus Kopeerimine ja joondamine Sequence to Sequence Models', 'fi': 'KopioiSeuraava: Eksplicit Span Kopiointi ja tasaus sekvenssimalleissa', 'ca': 'CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence Models', 'he': 'עותקNext: Explicit Span Copying and Alignment in Queue to Sequence Models', 'sk': 'KopijNaslednji: Ekplicit Span Kopiranje in poravnavanje v Sequence to Sequence Models', 'ha': 'next: Explicit Space Copying and Alive in Sequence to Sequence Models', 'jv': 'Next', 'bo': 'འདྲ་བཤུ་བྱེད་པ： དབྱེ་སྟངས་ལ་སྔོན་སྒྲིག་དབྱེ་བ་དང་གོ་རིམ'}
{'en': 'Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where each token was copied from. Further, they require contiguous token sequences from the input (spans) to be copied individually. We present a  model  with an explicit token-level copy operation and extend it to copying entire spans. Our  model  provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like  information extraction . We demonstrate the approach on Nested Named Entity Recognition, achieving near state-of-the-art  accuracy  with an order of magnitude increase in decoding speed.', 'ar': 'يتم استخدام آليات النسخ بالتسلسل إلى نماذج التسلسل (seq2seq) لتوليد نسخ من الكلمات من المدخلات إلى المخرجات. هذه الأطر ، التي تعمل على مستوى النوع المعجمي ، تفشل في توفير محاذاة صريحة تسجل مكان نسخ كل رمز مميز. علاوة على ذلك ، فإنها تتطلب تسلسلات رمزية متجاورة من المدخلات (الامتدادات) ليتم نسخها بشكل فردي. نقدم نموذجًا مع عملية نسخ صريحة على مستوى الرمز المميز ونوسعها لنسخ امتدادات كاملة. يوفر نموذجنا محاذاة صلبة بين الامتدادات في المدخلات والمخرجات ، مما يسمح بالتطبيقات غير التقليدية لـ seq2seq ، مثل استخراج المعلومات. نوضح النهج المتبع في التعرف على الكيانات المسماة Nested Nested Entity ، مما يحقق دقة قريبة من أحدث ما توصلت إليه التقنية مع زيادة حجم سرعة فك التشفير.', 'fr': "Des mécanismes de copie sont utilisés dans des modèles séquence à séquence (seq2seq) pour générer des reproductions de mots de l'entrée vers la sortie. Ces structures, qui fonctionnent au niveau du type lexical, ne fournissent pas d'alignement explicite qui enregistre l'endroit à partir duquel chaque jeton a été copié. En outre, ils nécessitent que des séquences de jetons contigües provenant de l'entrée (spans) soient copiées individuellement. Nous présentons un modèle avec une opération de copie explicite au niveau du jeton et l'étendons à la copie de plages entières. Notre modèle fournit des alignements précis entre les plages en entrée et en sortie, ce qui permet des applications non traditionnelles de seq2seq, comme l'extraction d'informations. Nous démontrons l'approche de la reconnaissance d'entités nommées imbriquées, en obtenant une précision proche de la pointe avec une augmentation d'un ordre de grandeur de la vitesse de décodage.", 'pt': 'Mecanismos de cópia são empregados em modelos de sequência a sequência (seq2seq) para gerar reproduções de palavras da entrada para a saída. Essas estruturas, operando no nível do tipo léxico, não fornecem um alinhamento explícito que registra de onde cada token foi copiado. Além disso, eles exigem que sequências de token contíguas da entrada (spans) sejam copiadas individualmente. Apresentamos um modelo com uma operação de cópia explícita em nível de token e o estendemos para copiar períodos inteiros. Nosso modelo fornece alinhamentos rígidos entre spans na entrada e na saída, permitindo aplicações não tradicionais de seq2seq, como extração de informações. Demonstramos a abordagem no Reconhecimento de Entidades Nomeadas Aninhadas, alcançando uma precisão próxima do estado da arte com um aumento de ordem de magnitude na velocidade de decodificação.', 'es': 'Los mecanismos de copia se emplean en los modelos secuencia a secuencia (seq2seq) para generar reproducciones de palabras desde la entrada hasta la salida. Estos marcos, que operan a nivel de tipo léxico, no proporcionan una alineación explícita que registre de dónde se copió cada token. Además, requieren que las secuencias de tokens contiguas de la entrada (tramos) se copien individualmente. Presentamos un modelo con una operación de copia explícita a nivel de token y lo ampliamos para copiar tramos completos. Nuestro modelo proporciona alineaciones rígidas entre los intervalos en la entrada y la salida, lo que permite aplicaciones no tradicionales de seq2seq, como la extracción de información. Demostramos el enfoque del reconocimiento de entidades nombradas anidadas, logrando una precisión casi de vanguardia con un aumento de orden de magnitud en la velocidad de decodificación.', 'ja': 'コピーメカニズムは、入力から出力への単語の再現を生成するために、シーケンス（ seq 2 seq ）モデルへのシーケンスで使用される。これらのフレームワークは、辞書型レベルで動作しますが、各トークンがどこからコピーされたかを記録する明示的な整列を提供することができません。さらに、入力（スパン）からの連続したトークンシーケンスを個別にコピーする必要があります。明示的なトークンレベルのコピー操作を持つモデルを提示し、それをスパン全体のコピーに拡張します。当社のモデルは、入力と出力の間のスパンのハードアライメントを提供し、情報抽出のようなseq 2 seqの非伝統的なアプリケーションを可能にします。私たちは、ネストされた名前付きエンティティ認識のアプローチを実演し、デコード速度を数桁増加させて、ほぼ最先端の精度を達成します。', 'ru': 'Механизмы копирования используются последовательно к моделям последовательности (seq2seq) для генерации воспроизведения слов от входа к выходу. Эти фреймворки, работающие на уровне лексических типов, не обеспечивают четкого выравнивания, которое записывает, откуда был скопирован каждый маркер. Кроме того, они требуют, чтобы смежные последовательности токенов из входных данных (spans) копировались по отдельности. Мы представляем модель с явной операцией копирования на уровне токена и расширяем ее до копирования целых интервалов. Наша модель обеспечивает жесткое выравнивание между диапазонами ввода и вывода, позволяя нетрадиционные применения seq2seq, такие как извлечение информации. Мы демонстрируем подход к распознаванию вложенных именованных сущностей, достигая почти самой современной точности с увеличением скорости декодирования на порядок.', 'hi': 'प्रतिलिपि तंत्र अनुक्रम में अनुक्रम (seq2seq) मॉडल के लिए इनपुट से आउटपुट के लिए शब्दों के reproductions उत्पन्न करने के लिए नियोजित कर रहे हैं. ये चौखटे, लेक्सिकल प्रकार के स्तर पर काम करते हुए, एक स्पष्ट संरेखण प्रदान करने में विफल रहते हैं जो रिकॉर्ड करता है कि प्रत्येक टोकन की प्रतिलिपि कहां से बनाई गई थी। इसके अलावा, उन्हें व्यक्तिगत रूप से कॉपी करने के लिए इनपुट (स्पैन) से सन्निहित टोकन अनुक्रमों की आवश्यकता होती है। हम एक स्पष्ट टोकन-स्तर की प्रतिलिपि कार्रवाई के साथ एक मॉडल प्रस्तुत करते हैं और इसे पूरे स्पैन की प्रतिलिपि बनाने के लिए विस्तारित करते हैं। हमारा मॉडल इनपुट और आउटपुट में स्पैन के बीच कठिन संरेखण प्रदान करता है, जिससे जानकारी निष्कर्षण की तरह seq2seq के गैर-पारंपरिक अनुप्रयोगों के लिए अनुमति मिलती है। हम नेस्टेड नामित इकाई मान्यता पर दृष्टिकोण का प्रदर्शन करते हैं, डिकोडिंग गति में परिमाण वृद्धि के क्रम के साथ अत्याधुनिक सटीकता प्राप्त करते हैं।', 'zh': '复制制于序(seq2seq)模形中用之,以成输输之单词。 此框架词法之等级,不可书令牌复制位之显式齐也。 此外须独复制自输(spans)连令牌序。 建显式令牌级复制模样,扩至复制全跨度。 吾形给硬对齐于输输之跨度之间,许seq2seq非传统之用,如信息取之。 吾演嵌套名实体之法,近乎先进之准确性,解码速增一数量级。', 'ga': "Úsáidtear meicníochtaí cóipeála in ord chun múnlaí a sheicheamhú (seq2seq) chun atáirgeadh focal a ghiniúint ón ionchur go dtí an t-aschur. Teipeann ar na creataí seo, a fheidhmíonn ar leibhéal an chineáil fhoclóra, ailíniú follasach a thaifeadann an áit óna ndearnadh cóipeáil ar gach comhartha. Ina theannta sin, teastaíonn uathu seichimh chomharthaí comhtheagmhálacha ón ionchur (réisí) a chóipeáil ina n-aonar. Cuirimid samhail i láthair a bhfuil oibríocht chóipeála sainráite ag leibhéal dearbháin ann agus déanaimid é a leathnú go dtí raonta iomlána a chóipeáil. Soláthraíonn ár múnla ailínithe crua idir raonta san ionchur agus aschur, rud a ligeann d'fheidhmchláir neamhthraidisiúnta seq2seq, amhail eastóscadh faisnéise. Léirímid an cur chuige maidir le hAithint Aonán Ainmnithe Neadaithe, ag baint amach cruinnis den scoth atá gar do ardú céime ar luas díchódaithe.", 'ka': 'კოპიფიკაციის მექანსიებები შემდეგ შემდეგ (seq2seq) მოდელებისთვის გამოყენება სიტყვების გამოყენება შემდეგ შემდეგ გამოყენება. ეს ფრამები, რომელიც ლექსიკალური ტიპის დონეში მომუშავებულია, არ შეუძლებელია განსხვავებული სწორება, რომელიც ჩაწერა, სადაც ყოველ ტიპი კოპირებულია. დამატებით, ისინი განსხვავებული სიმბოლოების შეცდომის შეცდომის შეცდომის შეცდომის შეცდომა individually. ჩვენ მოდელის გამოყენება განსხვავებული token-დონის კოპერაციას და გავაკეთებთ ყველა განსხვავებას. ჩვენი მოდელეში ძალიან დამატებულება, როგორც ინფორმაციის გამოყენება და გამოყენება შორის განსხვავება, რომლებიც შეუძლებელია არ-траდიციონალური პროგრამების seq2 ჩვენ მოდენსტრებით პროგრამის მიღება სახელსაწინო ინტერტის განაცნობაზე, რომელიც მივიღეთ მსგავსი სახელსაწინო სიმართლეში სიმართლეში სიმართლეში.', 'hu': 'Másolási mechanizmusokat alkalmaznak szekvenciától szekvenciáig (seq2seq) modellek, hogy a bemenetből a kimenetből reprodukálják a szavakat. Ezek a keretrendszerek, amelyek a lexikális típusú szinten működnek, nem biztosítanak egy explicit igazítást, amely rögzíti, hogy honnan másolták az egyes token-eket. Továbbá szükségük van arra, hogy a bemenetből (tartományokból) összefüggő tokenszekvenciákat egyenként másoljanak. Bemutatunk egy modellt egy explicit token szintű másolási művelettel, és kiterjesztjük a teljes tartományok másolására. Modellünk kemény összehangolást biztosít a bemeneti és kimeneti tartományok között, lehetővé téve a szek2seq nem hagyományos alkalmazásait, mint például az információkitermelést. Bemutatjuk a beágyazott nevű entitások felismerésének megközelítését, amely a dekódolás sebességének nagyságrendjének növelésével a legkorszerűbb pontosságot érte el.', 'el': 'Οι μηχανισμοί αντιγραφής χρησιμοποιούνται κατά σειρά σε ακολουθία (seq2seq) μοντέλα για την παραγωγή αντιγράφων λέξεων από την είσοδο στην έξοδο. Αυτά τα πλαίσια, που λειτουργούν σε επίπεδο λεξικού τύπου, αποτυγχάνουν να παρέχουν μια σαφή ευθυγράμμιση που καταγράφει από πού αντιγράφηκε κάθε μάρκα. Επιπλέον, απαιτούν συνεχείς ακολουθίες συμβολαίων από την εισαγωγή (περιοχές) να αντιγραφούν μεμονωμένα. Παρουσιάζουμε ένα μοντέλο με μια ρητή λειτουργία αντιγραφής σε επίπεδο συμβολαίου και το επεκτείνουμε στην αντιγραφή ολόκληρων περιόδων. Το μοντέλο μας παρέχει σκληρές ευθυγραμμίσεις μεταξύ των διαστάσεων στην είσοδο και την έξοδο, επιτρέποντας τις μη παραδοσιακές εφαρμογές του όπως η εξαγωγή πληροφοριών. Επιδεικνύουμε την προσέγγιση της αναγνώρισης Ενσωματωμένης Οντότητας, επιτυγχάνοντας ακρίβεια σχεδόν τελευταίας τεχνολογίας με αύξηση της ταχύτητας αποκωδικοποίησης.', 'it': "I meccanismi di copia sono impiegati in sequenza a sequenza (seq2Seq) modelli per generare riproduzioni di parole dall'input all'output. Questi framework, che operano a livello di tipo lessicale, non riescono a fornire un allineamento esplicito che registra da dove ogni token è stato copiato. Inoltre, richiedono sequenze di token contigue dall'input (spans) per essere copiate individualmente. Presentiamo un modello con un'operazione esplicita di copia a livello token e lo estendiamo alla copia di interi intervalli. Il nostro modello fornisce allineamenti rigidi tra le campate in ingresso e in uscita, consentendo applicazioni non tradizionali di seq2seq, come l'estrazione di informazioni. Dimostriamo l'approccio con Nested Named Entity Recognition, raggiungendo una precisione quasi all'avanguardia con un aumento dell'ordine di grandezza nella velocità di decodifica.", 'kk': 'Көшірмелеу механизмтері реттеу (seq2seq) үлгілеріне келтірілген сөздерді келтіру үшін келтірілген (seq2seq) үлгілеріне қолданылады. Бұл фреймдер, лексикалық түрлі деңгейінде операцияланған, әрбір белгісінің көшірмесінің көшірмесінен жазылған жазылған түрлі түрлендіру қатесі. Келесіден келтірілген (spans) белгілерінен көшірмелеу керек. Біз үлгісін таңдау деңгейіндегі көшірмелеу операциясына келтіріп, толық бос орындарды көшірмелеу үшін кеңейту. Біздің үлгіміз келтірілген және шығыс арасындағы кеңістіктер арасындағы қиын түрлендіру керек. Мәліметті тарқату сияқты, seq2seq бағдарламаларының әдетті Біз келесі аталған нысандарды таңдау арқылы көрсетіп тұрмыз. Жұмыс күйінің жақындағы дұрыстығын декодтау жылдамдығын өзгерту ретінде жеткіземіз.', 'lt': 'Kopijavimo mechanizmai naudojami sekos po sekos (seq2seq) modelių, kad būtų galima atkurti žodžius nuo įvedimo iki išvedimo. Šiose sistemose, veikiančiose leksinio tipo lygmeniu, nėra aiškiai suderinta, iš kurios įrašomi įrašai, iš kurių kiekvienas ženklas buvo kopijuotas. Be to, jiems reikia atskirai kopijuoti iš įvedimo (intervalų) susijusias žymenų sekas. Pateikiame model į su aiškia ženklų lygio kopijavimo operacija ir išplėsime jį iki viso intervalo kopijavimo. Mūsų modelis užtikrina griežtą įvedimo ir išvedimo intervalų suderinimą, leidžiantį netradicinius seq2seq naudojimus, pavyzdžiui, informacijos gavimą. Mes demonstruojame metodą dėl Nested Named Entity Recognition, siekdami beveik naujausio lygio tikslumo ir didinant dekodinimo greitį.', 'mk': 'Механизмите за копирање се употребени во секвенца на секвенциски (seq2seq) модели за генерирање репродукции на зборови од влогот до излезот. Овие рамки, кои работат на ниво на лексикален тип, не успеваат да обезбедат експлицитно израмнување од кое се запишува секој знак. Покрај тоа, тие бараат континуирани секвенции на симболи од внесувањето (растојание) да бидат копирани индивидуално. We present a model with an explicit token-level copy operation and extend it to copying entire spans.  Нашиот модел обезбедува тешки подрачби помеѓу влезот и излезот, овозможувајќи нетрадиционални апликации на seq2seq, како екстракција на информации. Ние го демонстрираме пристапот на признавањето на именуваните суштества, постигнувајќи скоро најсовремена точност со редот на зголемување на големината на брзината на декодирање.', 'ml': 'ഇൻപുട്ടില്\u200d നിന്നും ഫലത്തിലേക്കുള്ള വാക്കുകളുടെ പുതുക്കുകള്\u200d നിര്\u200dമ്മിക്കാനുള്ള (seq2seq) മോഡലുകളിലേക്ക് പകര്\u200dത്തുന ലെക്സിക്കല്\u200d തരത്തില്\u200d പ്രവര്\u200dത്തിക്കുന്ന ഈ ഫ്രെയിമ്പുകള്\u200d, ഓരോ അടയാളം പകര്\u200dത്തിയിരിക്കുന്ന സ്ഥാനത്തുനിന്നും ഒരു പ്രത് Further, they require contiguous token sequences from the input (spans) to be copied individually.  ഒരു പ്രത്യക്ഷമായ ടോണ്\u200d ലെയില്\u200d പകര്\u200dത്തുന്ന പ്രവര്\u200dത്തനം കൊണ്ട് ഒരു മോഡല്\u200d കൊണ്ടുവന്നിട്ട് അത് മുഴുവന്\u200d സ്പേന്\u200d പകര നമ്മുടെ മോഡല്\u200d ഇന്\u200dപുട്ടിലും ഫലത്തിനും ഇടയിലുള്ള സ്പെന്\u200dസുകള്\u200dക്കും കഠിനമായ ഒരുമിച്ചുകാണിക്കുന്നു. വിവരങ്ങള്\u200d പുറത്തുവര നമ്മള്\u200d അടുത്ത പേരുള്ള എന്റിറ്റി തിരിച്ചറിയുന്നതിന്റെ പ്രോഗത്തില്\u200d നിന്ന് പ്രകടനം കാണിച്ചുകൊടുക്കുന്നു. കലാകാര്യത്തിന', 'ms': 'Mekanisme salin digunakan dalam jujukan ke model jujukan (seq2seq) untuk menghasilkan reproduksi perkataan dari input ke output. Bingkai-bingkai ini, berfungsi pada aras jenis leksikal, gagal menyediakan jajaran eksplisit yang merakam di mana setiap token disalin. Lagipun, mereka memerlukan urutan token berikut dari input (jangkauan) untuk disalin secara individu. Kami memperkenalkan model dengan operasi salinan aras-token secara eksplicit dan memperluasnya ke salinan seluruh jangkauan. Model kami menyediakan penyesuaian keras antara jangkauan dalam input dan output, membolehkan aplikasi tidak tradisional seq2seq, seperti ekstraksi maklumat. We demonstrate the approach on Nested Named Entity Recognition, achieving near state-of-the-art accuracy with an order of magnitude increase in decoding speed.', 'mn': 'Копирал механизмууд дарааллаар (seq2seq) загвар руу ашиглаж өгсөн үгнүүдийн үржүүлэлтийг гаргах зорилготой. Эдгээр хэлбэрүүд, лексикийн төрлийн түвшинд ажиллаж байгаа, тэмдэгт бүр хаанаас хуулбарлаж байгааг бичиж чадахгүй. Дараа нь, тэд нэг хэсэгт хуулбарлах боломжтой тэмдэгтийн дарааллыг хэрэгтэй. Бид загварыг тодорхой тодорхой хэмжээний хуулийн үйл ажиллагаатай тайлбарлаж, бүх загваруудыг хуулбарлах болно. Бидний загвар нь өгөгдлийн болон гаралтын орон зайн хоорондоо хэцүү тэгшитгэлийг хангадаг. Мэдээллийн гаралт гаргах мэт уламжлалтгүй seq2seq-ийн хэрэглээ боломж олгодог. Бид Нэгдсэн нэрлэгдсэн Entity Recognition-ын ойлголтыг харуулж, урлагийн байр суурь дахь зөв байдлын тохиромжтой байдлын тохиромжтой хурдаар нэмэгдүүлсэн.', 'no': 'Kopieringsmetodar vert arbeida i sekvens til sekvensmodeller (seq2seq) for å laga reproduksjon av ord frå inndata til utdata. Desse rammeverka, som fungerer på leksisk type-nivået, kan ikkje oppgje ei eksplisitt justering der kvar token vart kopiert frå. Dei krev at dei skal kopierast individuelt nødvendige teiknkombinasjonar frå inndata (mellomrom). Vi presenterer eit modell med ei eksplisitt kopi- operasjon for tokennivå og utvidar den til kopiering av heile mellomrom. Modellen vårt tilbyr vanskelige justeringar mellom mellomrom i inndata og utdata, som tillater ikkje tradisjonelle program av seq2seq, slik som utpakking av informasjon. Vi demonstrerer tilnærminga på gjenkjenning av neste namnet entitet, og nær nærleiken av kunsten er nøyaktig med rekkefølgje for å øke storleiken i dekoderfart.', 'pl': 'Mechanizmy kopiowania są stosowane w modelach sekwencji do sekwencji (seq2seq), aby generować reprodukcje słów od wejścia do wyjścia. Ramy te, działające na poziomie typu leksykalnego, nie zapewniają wyraźnego wyrównania, które rejestruje skąd każdy token został skopiowany. Ponadto wymagają one przyległych sekwencji tokenów z wejścia (zakresów) skopiowania indywidualnie. Przedstawiamy model z wyraźną operacją kopiowania na poziomie tokenów i rozszerzamy go na kopiowanie całych zakresów. Nasz model zapewnia twarde ustawienia między rozpięciami wejścia i wyjścia, pozwalając na nietradycyjne zastosowania seq2seq, takie jak ekstrakcja informacji. Przedstawiamy podejście do rozpoznawania zagnieżdżonych nazwań podmiotów, osiągające niemal najnowocześniejszą dokładność przy rzędzie wielkości wzrostu szybkości dekodowania.', 'mt': 'Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output.  Dawn l-oqfsa, li joperaw fil-livell tat-tip lexical, jonqsu milli jipprovdu allinjament espliċitu li jirreġistra minn fejn kull token ġie kkoppjat. Barra minn hekk, jeħtieġu li s-sekwenzi ta’ tokens kontigwi mill-input (spans) jiġu kopjati individwalment. Aħna nippreżentaw mudell b’operazzjoni ta’ kopja espliċit a fil-livell tat-token u nistennewha għall-kopja ta’ firxiet sħa ħ. Il-mudell tagħna jipprovdi allinjamenti iebsin bejn il-firxiet fl-input u l-output, li jippermettu applikazzjonijiet mhux tradizzjonali ta’ seq2seq, bħall-estrazzjoni tal-informazzjoni. Aħna nippruvaw l-approċċ dwar ir-Rikonoxximent ta’ Entitajiet Imniżżlin f’Nest, li jiksbu preċiżjoni qrib l-aktar avvanzata b’ordni ta’ żieda fid-daqs fil-veloċità tad-dekodifikazzjoni.', 'ro': 'Mecanismele de copiere sunt folosite în secvență la secvență (seq2seq) modele pentru a genera reproduceri de cuvinte de la intrare la ieșire. Aceste cadre, care operează la nivel de tip lexical, nu reușesc să ofere o aliniere explicită care să înregistreze de unde a fost copiat fiecare token. Mai mult, acestea necesită secvențe de token contigue din intrare (intervale) pentru a fi copiate individual. Prezentăm un model cu o operațiune explicită de copiere la nivel de token și îl extindem la copierea întregii intervale. Modelul nostru oferă alinieri dure între intervalele de intrare și ieșire, permițând aplicații netradiționale de sec2seq, cum ar fi extragerea informațiilor. Demonstrăm abordarea privind recunoașterea entităților numite cuibărite, obținând o precizie aproape de ultimă oră cu o creștere a vitezei de decodare a ordinului de mărime.', 'sr': 'Kopijski mehanizmi su zaposleni u sekvenci modela sekvence (seq2seq) kako bi stvorili reprodukciju reèi iz ulaza do izlaza. Ove okvire, koje rade na nivou leksičkog tipa, ne mogu pružiti objašnjenje koje snima odakle je svaki znak kopiran. Nadalje, oni zahtevaju da se kopiraju individualno povezane sekvence znakova iz unosa (spans). Predstavljamo model sa kopiranjem na nivou znakova i proširimo ga na kopiranje celih prostora. Naš model pruža teške poređenje između prostora u ulazu i izlazu, omogućavajući neotradicionalne aplikacije seq2seq, poput izvlačenja informacija. Pokazujemo pristup priznanju Nested imenovanih entiteta, postižeći blizu tačnosti države umjetnosti sa naredbom povećanja veličine brzine dekodiranja.', 'si': 'ප්\u200dරතිලිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිපිළිප මේ චෝරම් වර්ගය, ලෙක්සිකල් වර්ගයේ ස්ථානයෙන් වැඩ කරන්න, ප්\u200dරතිශීල සංවිධානයක් දෙන්න බැරි වුනා හැම තොකෙ ඊට පස්සේ, ඔවුන්ට ප්\u200dරතිකාරයෙන් ප්\u200dරතිකාරයෙන් ප්\u200dරතිකාරයෙන් ප්\u200dරතිකාරය කරන්න අවශ්\u200dයයි. අපි පැහැදිලි ටොකන් ලේවල් ප්\u200dරතිචාර ප්\u200dරකාරයක් තියෙනවා ඒක සම්පූර්ණ ප්\u200dරතිචාර කරන්න. අපේ මොඩේල් ප්\u200dරවෘත්තියේ ඇතුළුම් සහ ප්\u200dරවෘත්තියේ ස්පැන්ස් අතර අතර අතර අතර ප්\u200dරවෘත්තිය සහ ප්\u200dරවෘත්තිය අපි ප්\u200dරකාශ කරනවා නම් නම් අන්තිත්වය පිළිගන්න, ක්\u200dරියාත්මක විශේෂ වේගයක් විශාල විශාල විශාල විශාල විශාල විශ', 'so': 'Isticmaalka koopiga waxaa lagu isticmaalaa qaabab lagu soo bandhigi karo (seq2seq) si uu uga soo saaro soo celinta hadalka input-ga-soo-baxa. Jardiinooyinkaas oo ku shaqeeya darajada leksikada, ma suurtowdo in la siiyo safar cad oo lagu qoro meesha calaamad walba laga qoray. Sidoo kale waxay u baahan yihiin in loo koobo mid gaar ah calaamada la xiriira. Tusaale ahaan ayaan u soo bandhignaa qaab muuqasho ah oo ku qoran qoraalka koobka sawirka, waxaana ku fidinaynaa qorsheynta oo dhan. Tusaalkayagu wuxuu ka bixiyaa isbedelyo adag oo u dhexeeya barashada gudaha iyo midhaha, kaas oo u ogolaanaya codsiga caadi ah oo la xiriira xil2seq, sida soo bixinta macluumaadka. Waxaynu muujinnaa qaababka ku saabsan aqoonsashada magaca la joogo, taasoo gaadhaya saxda rasmiga ah ee farshaxanka, si aad u kordhiso kordhiska degdeg ah.', 'sv': 'Kopieringsmekanismer används i sekvens till sekvens (seq2seq) modeller för att generera reproduktioner av ord från ingången till utgången. Dessa ramverk, som fungerar på lexikal typnivå, ger inte en explicit justering som registrerar var varje token kopierades från. Dessutom kräver de sammanhängande token sekvenser från indata (spans) för att kopieras individuellt. Vi presenterar en modell med en explicit token-nivå kopiering operation och utökar den till att kopiera hela spännvidder. Vår modell ger hårda justeringar mellan spännvidden i input och output, vilket möjliggör icke-traditionella tillämpningar av sek2seq, som informationsutvinning. Vi visar tillvägagångssättet för Nested Named Entity Recognition och uppnår nästan toppmodern noggrannhet med en storleksordningsökning i avkodningshastighet.', 'ta': 'பின்வரிசையில் (seq2seq) மாதிரிகளில் நகல் முறைமைகள் பயன்படுத்தப்படுகிறது உள்ளீட்டிலிருந்து வெளியீட்டிற்கு வரை வார்த்தைகளி இந்த சட்டங்கள், லெக்சிக்சியல் வகை மட்டத்தில் செயல்படுத்தப்படுகிறது, ஒவ்வொரு குறியீடு நகலெடுக்கப்பட்டுள்ளது என்பதை பதிவு செய மேலும், அவர்கள் உள்ளீடு (ஸ்பென்கள்) இருந்து தனியாக நகலெடுக்க தேவைப்படுகிறது. நாம் ஒரு வெளிப்படையான குறியீடு- மட்டத்தில் நகல் செயல்பாட்டை கொண்டு ஒரு மாதிரி காண்பிக்கிறோம் மற்றும் ம எங்கள் மாதிரி உள்ளீடு மற்றும் வெளியீட்டிற்கும் இடையே ஸ்பேன்களுக்கு கடினமான ஒழுங்குகளை வழங்குகிறது, தகவல் பெறுதல் போன் நாம் நெடுக்கப்பட்ட பெயர் உள்ளீடு அறிவிப்பு மீது அணுகலை காட்டுகிறோம், நெருங்கி-கலை சரியான நிலையில் நிறைவேற்றும் வேகத்தில்', 'ur': 'کاپی مکانیزوں کو سطح (seq2seq) نمڈلوں میں استعمال کیا جاتا ہے کہ کلمات کو اینپیٹ سے اینپیٹ سے اینپیٹ تک پیدا کریں۔ یہ فرمورک، لکسیکل ٹائپ سطح پر عمل کرتی ہیں، ایک کھلی سیدھی سیدھی سیدھی سیدھی سیدھی سیدھی سیدھی سیدھی سیدھی سیدھی سیدھی سیدھی سیدھی سیدھی سیدھی سیدھ اور اس کے بعد ان کے لئے اپنا اپنا اپنا انپیٹ (spans) سے ایک دوسرے کے ساتھ کپیل کی ضرورت ہے۔ ہم ایک موڈل کو صریح ٹوکین-سطح کی کپی عملیات کے ساتھ پیش کرتے ہیں اور اسے تمام فضا کی کپی کرنے کے لئے پھیلاتے ہیں۔ ہمارا موڈل اینپیٹ اور آئٹ پیٹ کے درمیان سخت تعمیرات دیتا ہے، جسے سq2seq کے غیر منطقی کاروباروں کی اجازت دیتا ہے، جیسے اطلاعات اخراج کی طرح۔ ہم نے نہریں نامہ واحد کی پہچان کی طریقہ دکھائی ہے، اس طرح طریقہ کی تصدیق کرتی ہے کہ آرام کے نزدیک مضبوطی کو دکھانے کی سرعت میں زیادہ بڑھتی ہے۔', 'uz': '@ info: whatsthis Ushbu freymlar leksikal turi darajada ishlash muvaffaqiyatsiz tugadi. Koʻrsatilgan, ularning tugmalar birikmasidan nusxa olish kerak. Biz tashqi teg- darajadagi nusxa amallar bilan bir modelni hosil qilamiz va uni butun spanni nusxa olish uchun uzamiz. Our model provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like information extraction.  Биз яқинлашган маълумот номини аниқлаб кўриб турибмиз, бу ҳукумат маълумотлари аниқ кўриниб чиқади. Қўшиш тезроқ зиёда бўлиши мумкин.', 'vi': 'Chép các cơ chế được dùng theo thứ tự từng dãy (seq2seq) các mô hình để tạo ra bản sao của từ đầu nhập tới kết xuất. Các cấu trúc này, có tác dụng ở mức độ ngôn ngữ, không thể xác định rõ ràng các cấu trúc mà ghi lại nơi từng biểu đồ được sao chép. Thêm vào đó, họ cần những dãy tượng trưng liên tục trong nội dung được sao chép từng người một. Chúng tôi giới thiệu một mô hình có một thao tác bắt chước hiển thị và mở rộng nó cho việc sao chép to àn bộ nút cờ. Mẫu của chúng tôi cung cấp độ chính xác giữa chi tiết nhập và xuất, cho phép các ứng dụng không truyền thống của K2seq, như đào tạo thông tin. Chúng tôi chứng minh cách tiếp cận với Nhận dạng ủy quyền được đặt tên Tổ, đạt độ chính xác gần với mức độ cao của việc giải mã tăng tốc độ.', 'bg': 'Копиращите механизми се използват в последователност към последователност модели (seq2seq), за да се генерират репродукции на думи от входа към изхода. Тези рамки, работещи на ниво лексикален тип, не осигуряват изрично подравняване, което записва откъде е копиран всеки символ. Освен това те изискват съпътстващи последователности от токени от входа (интервали) да бъдат копирани поотделно. Представяме модел с изрична операция за копиране на ниво токен и го разширяваме, за да копираме цели интервали. Нашият модел осигурява твърди подравнявания между интервалите на входа и изхода, което позволява нетрадиционни приложения на като извличане на информация. Ние демонстрираме подхода за разпознаване на гнездени имена на субекти, постигайки почти най-съвременна точност с увеличение на скоростта на декодиране от порядък на величината.', 'nl': 'Kopieermechanismen worden gebruikt in seq2seq (seq2seq) modellen om reproducties van woorden te genereren van de invoer naar de uitvoer. Deze frameworks, die werken op lexicaal type niveau, bieden geen expliciete uitlijning die registreert waar elk token vandaan is gekopieerd. Verder vereisen ze dat aaneengesloten tokensequenties van de invoer (spans) afzonderlijk worden gekopieerd. We presenteren een model met een expliciete kopieeroperatie op token-niveau en breiden deze uit tot het kopiëren van volledige overspanningen. Ons model biedt harde uitlijningen tussen overspanningen in de input en output, waardoor niet-traditionele toepassingen van seq2seq mogelijk zijn, zoals informatie extractie. We demonstreren de aanpak van Nested Named Entity Recognition, waarbij bijna state-of-the-art nauwkeurigheid wordt bereikt met een grotere decoderingssnelheid.', 'da': 'Kopimekanismer anvendes i sekvens til sekvens (sekvens) modeller til at generere reproduktioner af ord fra input til output. Disse rammer, der fungerer på leksikalstypeniveau, giver ikke en eksplicit justering, der registrerer, hvor hvert token blev kopieret fra. Desuden kræver de sammenhængende token sekvenser fra input (spænder) at kopieres individuelt. Vi præsenterer en model med en eksplicit kopioperation på token-niveau og udvider den til at kopiere hele spænder. Vores model giver hårde justeringer mellem spænder i input og output, hvilket giver mulighed for ikke-traditionelle anvendelser af sek2seq, såsom informationsudvinding. Vi demonstrerer tilgangen til Nested Named Entity Recognition og opnår næsten topmoderne nøjagtighed med en størrelsesorden stigning i afkodningshastigheden.', 'de': 'Kopiermechanismen werden in Sequenz-to-Sequenz (seq2seq) Modellen verwendet, um Reproduktionen von Wörtern von der Eingabe bis zur Ausgabe zu erzeugen. Diese Frameworks, die auf lexikalischer Typebene arbeiten, bieten keine explizite Ausrichtung, die aufzeichnet, woher jedes Token kopiert wurde. Weiterhin müssen zusammenhängende Tokensequenzen aus der Eingabe (Spanns) einzeln kopiert werden. Wir präsentieren ein Modell mit einem expliziten Kopiervorgang auf Tokenebene und erweitern es auf das Kopieren ganzer Spanns. Unser Modell bietet harte Ausrichtungen zwischen den Spannen im Ein- und Ausgang, was nicht traditionelle Anwendungen von seq2seq, wie Informationsextraktion, ermöglicht. Wir demonstrieren den Ansatz der Nested Named Entity Recognition, der nahezu den neuesten Stand der Technik erreicht und die Decodiergeschwindigkeit um eine Größenordnung erhöht.', 'id': 'Mekanisme salinan digunakan dalam urutan ke model urutan (seq2seq) untuk menghasilkan reproduksi kata dari input ke output. Bingkai-bingkai ini, beroperasi di tingkat tipe leksik, gagal menyediakan penyesuaian eksplisit yang merekam dari mana setiap token disalin. Further, they require contiguous token sequences from the input (spans) to be copied individually.  Kami mempersembahkan model dengan operasi salinan tingkat token eksplicit dan memperluasnya ke salinan seluruh jangkauan. Model kami menyediakan penyesuaian keras antara jangkauan dalam input dan output, memungkinkan aplikasi tidak tradisional dari seq2seq, seperti ekstraksi informasi. Kami menunjukkan pendekatan pada Pengenalan Entitas bernama Nested, mencapai akurasi dekat-state-of-the-art dengan perintah besar meningkat kecepatan dekodasi.', 'ko': '순서부터 순서 (seq2seq) 모델에서 입력에서 출력까지의 단어를 복제하는 메커니즘을 사용합니다.문법 형식 단계에서 실행되는 이 프레임워크들은 명시적으로 정렬할 수 없으며, 각 태그가 어디에서 복사되었는지 기록할 수 없다.그 밖에 입력(경계)에서 연속된 영패 서열을 각각 복제해야 한다.우리는 현식 영패급 복제 조작을 가진 모델을 제시했고 이를 복제 전체 범위까지 확대했다.우리의 모델은 입력과 출력의 경계 사이의 하드 정렬을 제공하여 seq2seq의 비전통적인 응용, 예를 들어 정보 추출을 허용한다.우리는 플러그인 명명 실체 식별 방법을 보여 주었고 디코딩 속도가 수량급을 높이는 상황에서 가장 선진적인 수준에 가까운 정확도를 실현했다.', 'sw': 'Mifumo ya nakala inatumiwa kwa mfululizo wa mfululizo (seq2seq) ili kutengeneza uzalishaji wa maneno kutoka kwenye input hadi matokeo. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where each token was copied from.  Zaidi ya hayo, wanahitaji mfululizo wa alama za kuguswa kutoka kwenye viwanja vya habari (spania) ili kuunganishwa pekee. Tunaweza kutoa mfano wenye operesheni ya kina yenye kiwango cha nusura na kuongeza kwa ajili ya kuandika siku zote. Mfano wetu unatoa nafasi ngumu kati ya spani katika input na matokeo, na kusaidia matumizi yasiyo ya kawaida ya sekq2seq, kama vile utoaji wa habari. Tunaonyesha mbinu za kutambuliwa kwa jina la Ujadala, kupata ukweli wa hali ya sanaa kwa kiwango cha kuongezeka kwa kiwango cha kuongezeka kwa kasi.', 'fa': 'مکانیسم\u200cهای کپی در دستور به مدل\u200cهای کپی (seq2seq) برای تولید تولید تولید کلمات از ورودی به خروج استفاده می\u200cشوند. این فرم\u200cهایی که در سطح نوع زبان\u200cشناسی عملیات می\u200cکنند، شکست نمی\u200cدهند که یک تنظیم خاصی را پیشنهاد کنند که ثبت می\u200cکند که هر نشان از آن کپی شده است. بعلاوه، آنها نیاز دارند که ترکیب\u200cهای نشانه\u200cهای مختلف از ورودی (فضا) به فردی کپی شود. ما یک مدل را با یک عملیات کپی سطح علامت مشخص پیشنهاد می\u200cکنیم و آن را به کپی کل فضا گسترش می\u200cدهیم. مدل ما تطبیق سخت بین فضایی در ورودی و خروجی را پیشنهاد می\u200cدهد، که برای کاربردهای غیرسنتی از seq2seq، مانند خروج اطلاعات اجازه می\u200cدهد. ما روش شناسايي ناميده شده رو نشون ميديم که به نزديک وضعيت هنري با دستور افزايش بزرگي در سرعت decoding رسيديم.', 'tr': "Faýllar girişden çizgi üçin nusgalar (seq2seq) nusgalarda işledilýär. Bu faýllar, lexik hili derejesinde işleýän faýllar, her token nireden kop edilen ýerde gaýşartmakda hata boldy. Diňe, gaýd edilen tymsal hatlary birbirlerine kopyalanmasyna gerek. Biz bir nusgany möhüm bir token derejesi bilen görkez we ony tüm deplerniň kopya görkez. Biziň modelimiz girdi we çizdikde seleňler arasynda kyn çyzyglyklar täsir edýär. Maglumat gaýşarmak ýaly seq2seq'iň däplikeýşenleri üçin mümkin edýär. Biz bu ýagdaýyň ady Aýraty tanyşynyň golaýyny görkezip otyrýarys, ýagtylygyň golaýynyň düzgünligi bilen azaltýarys.", 'sq': 'Mekanizmat e kopjimit janë përdorur në sekuencë në modelet e sekuencës (seq2seq) për të gjeneruar riprodhime të fjalëve nga hyrja në dalje. Këto korniza, që funksionojnë në nivelin e tipit lexik, dështojnë të sigurojnë një rregullim të qartë që regjistron nga u kopjua çdo token. Përveç kësaj, ata kërkojnë sekuenca token të vazhdueshme nga input (spans) për të kopjuar individualisht. Ne paraqesim një model me një operacion të qartë kopjimi në nivel token dhe e zgjerojmë në kopjimin e të gjitha intervaleve. Modeli ynë ofron rregullime të vështira midis intervaleve në hyrje dhe dalje, duke lejuar aplikimet jo tradicionale të seq2seq, si nxjerrje e informacionit. Ne demonstrojmë metodën për njohjen e njësisë së quajtur Nested, duke arritur saktësinë më të lartë me një rend rritje të madhësisë në shpejtësinë e dekodimit.', 'af': "Kopie mekanisme word in volgorde na volgorde (seq2seq) modele gebruik om reproducasie van woorde te genereer van die invoer na die uitset. Hierdie raamwerke, wat op die leksies tipe vlak werk, het misluk om 'n eksplisiese oplyn te verskaf wat opneem waar elke token van gekopieer is. Verder, hulle benodig gemeenskaplike token sekwensies van die invoer (spans) om individueel gekopieer te word. Ons voorsien 'n model met 'n eksplisiese token-vlak kopie operasie en uitbrei dit na kopieer volledige spans. Ons model verskaf moeilike lyn tussen spans in die invoer en uitvoer, toelaat vir ongetradisionele toepassings van seq2seq, soos inligting uitvoer. Ons bevestig die toegang op Nested Named Entity Recognition, wat naby-staat-of-kuns-presies aankom met 'n volgorde van groot groot vergroot in dekodering spoed.", 'am': 'ምርጫዎች እነዚህ ፍሬማዎች በ ሊክሲካዊ ዓይነት ላይ የሚቻኮሉ፣ ምልክቱ ሁሉ ከየት የተከፈተበት የመረጃ ግልጾችን ማሳየት አይችሉም፡፡ ከዚህም በላይ የተጠቃሚ ምልክት ግንኙነት ለብቻው ለመቀበል ያስፈልጋል፡፡ እናሳየዋለን የግልጽ ምልክት-ደረጃ ቅጂ ማድረግ እናደርጋታለን በሙሉ ቁልፎችን ለመቅጂ እናዘረጋዋለን፡፡ ሞዴሌያችን በመስመር እና ውጤት ውስጥ በተጨማሪው ግንኙነት አሰራርቷል፡፡ እንደማህበረት መረጃ ለመውጣት የseq2seq ፕሮግራሞች እንዲፈቅድ ይችላል፡፡ We demonstrate the approach on Nested Named Entity Recognition, achieving near state-of-the-art accuracy with an order of magnitude increase in decoding speed.', 'hy': 'Կոպիայի մեխանիզմները օգտագործվում են հաջորդականության (SeQ2SeQ) մոդելների մեջ բառերի վերարտադրման համար ներմուծից մինչև արտադրությունը: Այս կառուցվածքները, որոնք գործում են լեքսիկական տիպի մակարդակում, չեն կարողանում բացատրական համապատասխան տալ, որտեղ գրում են յուրաքանչյուր նշան: Ավելին, նրանք պահանջում են նշանների հաջորդականություններ, որոնք տեղադրվում են ինֆորմացիայից (տարբերություններից) առանձին կոպիանելու համար: Մենք ներկայացնում ենք մի մոդել, որն ունի բացահայտ նշանների մակարդակի կոպիայի գործողություն և ընդլայնում է այն մինչև ամբողջ տարածքները: Our model provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like information extraction.  Մենք ցույց ենք տալիս Ձախտած Նվայնված Համակարգման մոտեցումը, հասնելով մոտավորապես ամենահետաքրքիր ճշգրտությունը, որի չափսերը բարձրանում են դեկոդավորման արագությամբ:', 'bn': 'ইনপুট থেকে আউটপুট পর্যন্ত শব্দ পুনরাবৃত্তি তৈরি করার জন্য সেকেন্ড (seq2seq) মডেলে অনুলিপি করা হয়েছে। লেক্সিক্সিকাল ধরনের স্তরে কাজ করা এই ফ্রেম কার্যকলাপ ব্যর্থ হয়েছে যেখান থেকে প্রত্যেক চিহ্ন কপি করা হয়েছে। এছাড়াও, তাদের ইনপুট (স্পেন) থেকে কপি করার জন্য কমপি প্রয়োজন। আমরা একটা মডেল উপস্থাপন করি একটি স্পষ্ট প্রমাণ-স্তর কপি অপারেশন এবং এটি পুরো স্পেন কপি করার জন্য প্রসারিত করি। Our model provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like information extraction.  আমরা নিজেদের নামের এন্টিটি স্বীকারের প্রতিক্রিয়া প্রদর্শন করছি, যার ফলে শিল্পের কাছাকাছি অবস্থানের সঠিকভাবে পৌঁছে যাচ্ছে, যার ফলে ত', 'hr': 'Kopirajući mehanizmi su zaposleni u sekvenciji modela sekvencije (seq2seq) kako bi se proizvela reprodukcija riječi iz ulaza do izlaza. Ove okvire, djelujući na razini leksičkog tipa, ne pružaju jasno poravnanje koje snima odakle je svaki znak kopiran. Dalje, oni zahtijevaju da se kopiraju individualno povezane sekvence znakova iz ulaska (spans). Predstavljamo model s eksplicitnom kopiranjem na nivou znakova i proširimo ga na kopiranje cijelih prostora. Naš model pruža teške poređenje između prostora u ulazu i izlazu, omogućavajući neprodične primjene seq2seq, poput izvlačenja informacija. Pokazujemo pristup priznanju Nested Named Entity, postići blizu tačnosti stanja umjetnosti s naredbom povećanja veličine brzine dekodiranja.', 'az': 'Kopyalama mehanizmil…ôri seq2seq modell…ôr…ô istifad…ô edilir ki, s√∂zl…ôrin yenil…ônm…ôsini daxilind…ôn √ßńĪxńĪb yoluna g…ôtirir. Bu fotogramlar, leksik n√∂v s…ôviyy…ôsind…ô iŇül…ôy…ôn s…ôviyy…ôd…ô, h…ôr bir token kimi kopyalandńĪńüńĪ yerd…ôn kaydedil…ôn a√ßńĪq-aydńĪn t…ôr…ôfl…ôr t…ôyin etm…ôdi. Daha sonra, girdi (spans) arasńĪndakńĪ m√ľxt…ôlif m√∂c√ľz…ôl…ôr t…ôkrarlanmasńĪnńĪ ist…ôyirl…ôr. Biz bir modeli a √ßńĪq bir token seviyesi kopyalama operasyonu il…ô g√∂st…ôririk v…ô b√ľt√ľn uzaqlarńĪn kopyalamasńĪna uzatńĪrńĪq. Modelimiz girdi v…ô √ßńĪxńĪŇü arasńĪndakńĪ uzaqlar arasńĪnda ańüńĪr t…ôr…ôfl…ôr t…ôklif edir, seq2seq olmayan t…ôr…ôfl…ôr kimi m…ôlumat √ßńĪxarmasńĪna imkan verir. Biz Nested AdlńĪ Entity Recognition t…ôrzini g√∂st…ôrdik, m√ľzik t…ôrzind…ô d…ôyiŇüiklik s√ľr…ôtini art ńĪrmaq √ľ√ß√ľn √ßox q√ľvv…ôtli t…ôrzi il…ô yaxńĪnlaŇüdńĪrdńĪq.', 'cs': 'Kopírovací mechanismy se používají v sekvenci na sekvenci (seq2seq) modelů k generování reprodukcí slov ze vstupu do výstupu. Tyto frameworky, které fungují na úrovni lexikálního typu, neposkytují explicitní zarovnání, které zaznamenává, odkud byl každý token zkopírován. Dále vyžadují, aby související sekvence tokenů ze vstupu (rozpětí) byly zkopírovány individuálně. Představujeme model s explicitní kopírovací operací na úrovni tokenů a rozšíříme ji na kopírování celých rozpětí. Náš model poskytuje pevné zarovnání mezi rozpětími ve vstupu a výstupu, což umožňuje netradiční aplikace seq2seq, jako je extrakce informací. Demonstrujeme přístup k rozpoznávání vnořených jmenovaných entit, který dosahuje téměř nejmodernější přesnosti s řádným zvýšením rychlosti dekódování.', 'et': 'Kopeerimismehhanisme kasutatakse järjestusest järjestuseni (seq2seq), et genereerida sõnade reproduktsioone sisendist väljundini. Need raamistikud, mis toimivad leksikaalse tüübi tasandil, ei suuda pakkuda selget joondust, mis kirjendab, kust iga märk kopeeriti. Lisaks nõuavad nad sisendist pärit külgnevaid märgisjärjestusi (spans) eraldi kopeerimist. Esitame mudeli, millel on selge tokeni tasemel kopeerimisoperatsioon ja laiendame seda kogu ulatuse kopeerimisele. Meie mudel pakub tugevaid joondusi sisendi ja väljundi vahel, võimaldades mittetraditsioonilisi rakendusi seq2seq, näiteks informatsiooni ekstraheerimist. Näitame pesastatud nimega isikute tuvastamise lähenemisviisi, saavutades peaaegu tipptasemel täpsuse dekodeerimise kiiruse suurusjärgus.', 'bs': 'Kopirajući mehanizmi su zaposleni u sekvenciji modela sekvencije (seq2seq) kako bi stvorili reprodukciju riječi iz ulaza do izlaza. Ove okvire, radeći na nivou leksičkog tipa, ne pružaju objašnjenje koje snima odakle je svaki znak kopiran. Dalje, oni zahtijevaju da se kopiraju individualno povezane sekvence znakova iz unosa (spans). Predstavljamo model sa eksplicitnom kopiranjem na nivou znakova i proširimo ga na kopiranje cijelih prostora. Naš model pruža teške poređenje između prostora u ulazu i izlazu, omogućavajući nekotradicionalne primjene seq2seq, poput izvlačenja informacija. Mi pokazujemo pristup priznanju neobično imenovanog entiteta, ostvarivši blizu tačnosti stanja umjetnosti s naredbom povećanja veličine brzine dekodiranja.', 'ca': "Els mecanismes de copia es fan servir en seqüència a models de seq2seq per generar reproduccions de paraules des de la entrada a la salida. Aquests marcs, operatius a nivell de tipus lèxic, no proporcionen un alliniament explícit que registra d'on es va copiar cada fitxa. També necessiten que les seqüències contigues de fitxes de les entrades (intervals) siguin copiades individualment. Presentam un model amb una operació de còpia explícita a nivell de fitxes i l'estendem a còpiar extensions senceres. Our model provides hard alignments between spans in the input and output, allowing for nontraditional applications of seq2seq, like information extraction.  Demonstrem l'enfocament en la Recognició d'Entitats Nomades Nivades, aconseguint una precisió gairebé última amb un ordre d'augment de magnitud en la velocitat de decodificació.", 'fi': 'Kopiomekanismeja käytetään sekvenssimalleissa (seq2seq) sanojen jäljentämiseen syötteestä lähtöön. Nämä viitekehykset, jotka toimivat sanastotyyppitasolla, eivät pysty tarjoamaan nimenomaista tasausta, joka kirjaa, mistä kukin tunnus on kopioitu. Lisäksi ne edellyttävät, että syötteen vierekkäiset merkkisekvenssit (spans) kopioidaan erikseen. Esittelemme mallin, jossa on eksplisiittinen token-tason kopiointitoiminto ja laajennamme sen kopioimaan kokonaisia alueita. Mallimme tarjoaa kovat linjaukset tulo- ja lähtövälien välillä, mikä mahdollistaa epäperinteiset seq2seq-sovellukset, kuten tiedon uuttamisen. Esittelemme Nested Named Entity Recognition -menetelmän, joka saavuttaa lähes huipputason tarkkuuden ja suurentaa dekoodausnopeutta suuruusluokkaan.', 'sk': 'Kopiralni mehanizmi se uporabljajo v zaporedju do zaporedja modelov (seq2seq) za ustvarjanje reprodukcij besed od vhoda do izhoda. Ti okviri, ki delujejo na ravni leksikalnega tipa, ne zagotavljajo izrecne poravnave, ki zabeleži, od kod je bil vsak žeton kopiran. Poleg tega zahtevajo posamezno kopiranje sosednjih zaporedij žetonov iz vhoda (razponov). Predstavljamo model z eksplicitnim kopiranjem na ravni žetonov in ga razširimo na kopiranje celotnih razponov. Naš model zagotavlja trde poravnave med razponi v vhodu in izhodu, kar omogoča netradicionalne aplikacije seq2seq, kot je pridobivanje informacij. Prikazujemo pristop k prepoznavanju gnezdenih imen subjektov, ki dosega skoraj najsodobnejšo natančnost z velikostjo povečanja hitrosti dekodiranja.', 'ha': "Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output.  Wannan firam, mai amfani da shi kan nau'in nau'in leksisi, ba ya kasa bãyar da wani juyi mai bayyanãwa na rubutu da aka kodi duk ãyar daga. Furan, suna tambaya sauri masu haɗi daga cikin ayuka (spanni) dõmin a kofi kodi guda. Kana gabatar da wani motel wanda ke da wani shirin kodi-daraja na bayyane kuma Muke shimfiɗa shi zuwa kodi duk kwanan wata. @ info: whatsthis Yana nũna kafin da aka gane sunan Entity", 'he': 'מנגנוני העתק משתמשים ברצף לרצף דוגמנים (seq2seq) כדי ליצור שיגורים של מילים מהכניסה אל ההוצאה. These frameworks, operating at the lexical type level, fail to provide an explicit alignment that records where each token was copied from.  חוץ מזה, הם דורשים שיוצאות סימנים משותפות מהכניסה (מרווחים) שיעותקו באופן אישי. אנחנו מציגים מודל עם מבצע עותק ברמה של סימנים ברור ומרחב אותו לעתק חלקים שלמים. הדוגמא שלנו מספקת התאמות קשות בין המערכות של הכניסה והיציאה, מאפשרת ליישומים לא מסורתיים של seq2seq, כמו חיפוש מידע. אנחנו מראים את הגישה על זיהוי איכות בשם הקרן, להשיג מדויקה קרובה למצב המיוחד עם סדר הגודל הגבוה במהירות הפענוח.', 'jv': 'string" in "context_BAR_stringNew structural navigation Tulung string" in "context_BAR_stringLink Monday Awak dhéwé éntuk akses Perangkat sing nêmên pangan Entité sing koyok, ditambah jewêr negoro-jewêr tentang karo aturan sing wis nguasakno tambah apik dhéwé.', 'bo': 'Copy mechanisms are employed in sequence to sequence (seq2seq) models to generate reproductions of words from the input to the output. གཟུགས་རིས་འདི་དག་གི་ཁྱད་ཆོས་དབྱེ་རིགས་ལ་བཀོལ་སྤྱོད་བྱེད་པའི་གྲལ་སྒྲིག འོན་ཀྱང་། ཁོང་ཚོའི་ནང་ཐོག་ལས་མཐུན་རྟགས་ཀྱི་དབྱེ་རིམ་དགོས་པ ང་ཚོས་གསལ་བཤད་པའི་ཊོ་ཀིར་གྱི་གནས་རིམ་འདྲ་བཤུ་བྱེད་པའི་མིག་སྔར་སྟོན་པ་དང་བར་སྟོང་ཆ་ཡོད་པ་ལ་འདྲ་བཤུ་བྱེད ང་ཚོའི་མ་དབྱིབས་ཀྱིས་མཐུན་གྲལ་སྒྲིག ང་ཚོས་རང་ཉིད་ཀྱི་རྒྱུ་དངོས་ཐོག་སྙད་པའི་གྲངས་སུ་འབྱུང་བའི་ཐབས་ལམ་ལ་ཕལ་མེད་སྟོན་པ།'}
{'en': 'Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking', 'fr': "Modélisation neuronale basée sur l'énergie pour le suivi de l'état de dialogue", 'ar': 'النمذجة العصبية المستندة إلى الطاقة لتتبع حالة حوار النطاقات المتعددة على نطاق واسع', 'es': 'Modelado neuronal basado en la energía para el seguimiento del estado del diálogo en múltiples dominios', 'pt': 'Modelagem Neural Baseada em Energia para Rastreamento de Estado de Diálogo de Múltiplos Domínios em Grande Escala', 'ja': '大規模な多領域ダイアログ状態追跡のためのエネルギーベースのニューラルモデリング', 'zh': '基于大量多域言与言同神经建模', 'hi': 'बड़े पैमाने पर एकाधिक डोमेन संवाद राज्य ट्रैकिंग के लिए ऊर्जा आधारित तंत्रिका मॉडलिंग', 'ru': 'Энергоориентированное нейронное моделирование для отслеживания состояний многодоменного диалога в крупном масштабе', 'ga': 'Samhaltú Néarach atá Bunaithe ar Fhuinneamh le haghaidh Rianú Stáit Idirphlé Iolra Fearainn ar Mhórscála', 'ka': 'დიდი- სკეალური დიალოგის დიალოგის სახელისთვის ძალიან დიალოგის მონაცემებისთვის ენერგია დაბათებული ნეიროლური მოდელირება', 'el': 'Νευρική μοντελοποίηση βασισμένη στην ενέργεια για την παρακολούθηση κατάστασης διαλόγου πολλαπλών τομέων μεγάλης κλίμακας', 'hu': 'Energiaalapú neurális modellezés nagyméretű több tartomány párbeszéd állapotkövetéséhez', 'it': "Modellazione neurale basata sull'energia per il monitoraggio dello stato di dialogo su più domini su larga scala", 'kk': 'Үлкен- масштабтау күйін қадағалау үшін энергия негіздеген невралды моделлеу', 'ml': 'Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking', 'mk': 'Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking', 'mn': 'Ихэнх хэмжээний хэмжээний диалог хэлбэрийн төвшин эрчим хүчний мэдрэлийн загвар загвар', 'mt': 'Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking', 'no': 'Energibasert neuralmodellering for større skalering av fleire domenedialogvindauge', 'pl': 'Energetyczne modelowanie neuronowe dla śledzenia stanu dialogu wielu domen na dużą skalę', 'lt': 'Energetika grindžiamas neurologinis modeliavimas didelio masto kelių domenų dialogo būsenos sekimui', 'ro': 'Modelare neurală bazată pe energie pentru urmărirea stării de dialog pe mai multe domenii la scară largă', 'ms': 'Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking', 'so': 'Muuqashada Neural-based', 'sr': 'Energetski neuromodeling za praćenje državnog dijaloga o velikoj skali višestrukog domena', 'si': 'වැඩ- ස්කේල් විශාල සංවාදය සංවාදය ස්ථානය පරීක්ෂණය සඳහා ශක්තිය අධාරිත න්\u200dයූරාල මොඩ', 'ur': 'انرژی بنیادی نیورال موڈلینگ بزرگ- اسکیل چند ڈومین ڈیلوگ سٹیٹ ٹراکینگ کے لئے', 'sv': 'Energibaserad neural modellering för spårning av tillstånd i storskalig dialog med flera domäner', 'ta': 'பெரிய- அளவு பல் டொமைன் உரையாடல் மாநிலை பின்பற்றுதலுக்கான சக்தி அடிப்படையிலான நெயுரல் மாதிரி உரையாடல்', 'uz': 'Name', 'vi': 'Truyền hình thần kinh trên luồng năng lượng cho dò tìm đa vảy đa mẫu miền', 'da': 'Energibaseret neural modellering til tilstandsporing af flere domæner i stor skala', 'bg': 'Енергийно базирано неврално моделиране за широкомащабно проследяване на състоянието на диалоговия диалог с множество домейни', 'nl': 'Energiegebaseerde neurale modellering voor grootschalige monitoring van de toestand van de dialoog met meerdere domeinen', 'hr': 'Neuralna modela na energiji za praćenje državnog dijaloga o višestrukoj skali', 'ko': '에너지 기반의 대규모 다역 대화 상태 추적 신경 네트워크 모델링', 'de': 'Energiebasierte neuronale Modellierung für die Zustandsverfolgung von Dialogen mit mehreren Domänen in großem Maßstab', 'fa': 'نمونه\u200cهای عصبی بر بنیاد انرژی برای ردیابی وضعیت محاورۀ محاورۀ محاورۀ چندین دامنی بزرگ', 'id': 'Modeling Neural berdasarkan energi untuk Penjejakan Negara Dialog Domain Berbanyak Skala Besar', 'sq': 'Modelimi neuronal i bazuar në energji për gjurmimin e shtetit të dialogut të madh të shumëdomeneve', 'sw': 'Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking', 'tr': 'Energiýa tabanly nural nusgasy Ullakan-Maglumat Dialog Durumy Gözlemek üçin', 'af': 'Energies- gebaseerde neurale modellering vir Groot- Skaal Veelvuldige Domein Dialoog Staat Volg', 'am': 'ምርጫዎች', 'hy': 'Էներգիայի հիմնված նյարդային մոդելավորումը մեծ մասշտաբի բազմաթիվ դաշտերի դասախոսության վիճակի հետևման համար', 'az': 'Enerji-tabanl캼 N칬ral Modeli B칲y칲k-Skala 칂ox Domen Dialoog Eyaleti 캻zl톛m톛si 칲칞칲n', 'bn': 'ব্যাপার- মাত্র ডোমেইন ডায়ালগের জন্য শক্তি ভিত্তিক নিউরেল মডেলিংল', 'bs': 'Neuralna modela na energiji za praćenje državnog dijaloga o velikoj skali višestrukog domena', 'ca': "Modell neuronal basat en l'energia per a seguir l'estat del diàleg multidominiu a gran escala", 'cs': 'Energeticky založené neuronové modelování pro sledování stavu dialogů s více doménami ve velkém měřítku', 'et': 'Energiapõhine neuromodelleerimine mitme domeeni dialoogi oleku jälgimiseks', 'fi': 'Energiapohjainen hermomallinnus laajamittaiseen usean verkkotunnuksen dialogin tilan seurantaan', 'jv': 'Language', 'sk': 'Energetsko osnovana živčna modeliranja za sledenje stanja velikega obsega pogovornega okna z več domenami', 'ha': '@ action', 'he': 'מודל נוירולי מבוסס באנרגיה עבור מעקב דיאלוגי שדות רבים במידה גדולה', 'bo': 'ནུས་མཐུན་དང་གཞི་བརྟེན་ནས་སྐྱེལ་བའི་མ་དབྱིབས་དཔེ་གཏོང་ཚད་ཆེ་བའི་གླེང་སྒྲོམ་གླེང་སྒྲོམ་གནས་སྟངས་རྗ'}
{'en': 'Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, such as slots across domains. We propose using energy-based structure prediction methods for large-scale dialogue state tracking task in two multiple domain dialogue datasets. Our results indicate that : (i) modelling variable dependencies yields better results ; and (ii) the structured prediction output aligns with the dialogue slot-value constraint principles. This leads to promising directions to improve state-of-the-art models by incorporating variable dependencies into their prediction process.', 'ar': 'يعد توسيع نطاق تتبع حالة الحوار إلى مجالات متعددة أمرًا صعبًا نظرًا للنمو في عدد المتغيرات التي يتم تتبعها. علاوة على ذلك ، لا تستخدم نماذج تتبع حالة الحوار بشكل صريح حتى الآن العلاقات بين متغيرات الحوار ، مثل الفتحات عبر المجالات. نقترح استخدام طرق التنبؤ بالهيكل القائم على الطاقة لمهمة تتبع حالة الحوار على نطاق واسع في مجموعتي بيانات حوار متعدد المجالات. تشير نتائجنا إلى أن: (1) نمذجة التبعيات المتغيرة تؤدي إلى نتائج أفضل ؛ و (2) ناتج التنبؤ المنظم يتوافق مع مبادئ قيود قيمة فتحة الحوار. يؤدي هذا إلى اتجاهات واعدة لتحسين النماذج الحديثة من خلال دمج التبعيات المتغيرة في عملية التنبؤ الخاصة بهم.', 'es': 'Ampliar el seguimiento del estado del diálogo a múltiples dominios es un desafío debido al aumento en el número de variables que se rastrean. Además, los modelos de seguimiento del estado de los diálogos aún no utilizan explícitamente las relaciones entre las variables de diálogo, como las ranuras en los dominios. Proponemos el uso de métodos de predicción de estructura basados en la energía para tareas de seguimiento del estado de diálogo a gran escala en dos conjuntos de datos de diálogo Nuestros resultados indican que: (i) modelar dependencias de variables produce mejores resultados; y (ii) la salida de predicción estructurada se alinea con los principios de restricción de valor de ranura de diálogo. Esto lleva a direcciones prometedoras para mejorar los modelos más avanzados mediante la incorporación de dependencias variables en su proceso de predicción.', 'pt': 'Aumentar o rastreamento do estado do diálogo para vários domínios é um desafio devido ao crescimento do número de variáveis que estão sendo rastreadas. Além disso, os modelos de rastreamento de estado de diálogo ainda não fazem uso explícito de relacionamentos entre variáveis de diálogo, como slots entre domínios. Propomos o uso de métodos de previsão de estrutura baseados em energia para tarefas de rastreamento de estado de diálogo em larga escala em dois conjuntos de dados de diálogo de múltiplos domínios. Nossos resultados indicam que: (i) a modelagem de dependências de variáveis produz melhores resultados; e (ii) a saída de previsão estruturada se alinha com os princípios de restrição de valor de slot de diálogo. Isso leva a direções promissoras para melhorar os modelos de última geração, incorporando dependências variáveis em seu processo de previsão.', 'fr': "L'extension du suivi de l'état du dialogue à plusieurs domaines est un défi en raison de la croissance du nombre de variables suivies. En outre, les modèles de suivi de l'état des dialogues n'utilisent pas encore explicitement les relations entre les variables de dialogue, telles que les créneaux entre les domaines. Nous proposons d'utiliser des méthodes de prédiction de structure basées sur l'énergie pour les tâches de suivi d'état de dialogue à grande échelle dans deux ensembles de données de dialogue Nos résultats indiquent que\xa0: (i) la modélisation des dépendances de variables donne de meilleurs résultats\xa0; et (ii) la sortie de la prédiction structurée s'aligne sur les principes de contrainte de valeur de créneau de dialogue. Cela conduit à des orientations prometteuses pour améliorer les modèles de pointe en intégrant des dépendances variables dans leur processus de prédiction.", 'ja': 'トラッキングする変数の数が増えているため、複数のドメインへのダイアログ状態トラッキングのスケールアップは困難です。さらに、ダイアログ状態トラッキングモデルは、ドメイン間のスロットなどのダイアログ変数間のリレーションシップをまだ明示的に使用していません。大規模なダイアログ状態追跡タスクのためのエネルギーベースの構造予測法を2つのマルチドメインのダイアログデータセットに使用することを提案します。我々の結果は、(i)変数依存性のモデリングはより良い結果をもたらすこと、および(ii)構造化予測出力が対話スロット値制約原理と一致することを示している。これにより、可変依存性を予測プロセスに組み込むことで、最先端モデルを改善するための有望な方向性につながります。', 'hi': 'एकाधिक डोमेन के लिए संवाद राज्य ट्रैकिंग स्केलिंग चर की संख्या में वृद्धि के कारण चुनौतीपूर्ण है ट्रैक किया जा रहा है. इसके अलावा, संवाद स्थिति ट्रैकिंग मॉडल अभी तक स्पष्ट रूप से संवाद चर के बीच संबंधों का उपयोग नहीं करते हैं, जैसे कि डोमेन में स्लॉट। हम दो एकाधिक डोमेन संवाद डेटासेट में बड़े पैमाने पर संवाद राज्य ट्रैकिंग कार्य के लिए ऊर्जा-आधारित संरचना पूर्वानुमान विधियों का उपयोग करने का प्रस्ताव करते हैं। हमारे परिणाम इंगित करते हैं कि: (i) मॉडलिंग चर निर्भरता बेहतर परिणाम देती है; और (ii) संरचित पूर्वानुमान आउटपुट संवाद स्लॉट-मूल्य बाधा सिद्धांतों के साथ संरेखित करता है। यह उनकी भविष्यवाणी प्रक्रिया में चर निर्भरताओं को शामिल करके अत्याधुनिक मॉडल में सुधार करने के लिए आशाजनक दिशाओं की ओर जाता है।', 'zh': '随变量增益,广言数挑战性。 与形未显式变量,如跨域槽。 发用量术,两多域大对。 吾结果表明:(i)于变量建模则善矣。 (ii)结构化占输与对话槽直约束同。 这为因变量以纳占,以进先进。', 'ru': 'Масштабирование отслеживания состояния диалога до нескольких доменов является сложной задачей из-за роста количества отслеживаемых переменных. Кроме того, в моделях отслеживания состояния диалогов пока явно не используются отношения между переменными диалога, такими как слоты между доменами. Мы предлагаем использовать методы прогнозирования структуры на основе энергии для задачи отслеживания состояния крупномасштабного диалога в двух множественных диалоговых наборах данных. Наши результаты показывают, что: (i) моделирование зависимостей переменных дает лучшие результаты; и (ii) структурированный результат предсказания согласуется с принципами ограничения значения слота диалога. Это приводит к многообещающим направлениям для улучшения современных моделей путем включения переменных зависимостей в процесс их прогнозирования.', 'ga': 'Tá sé dúshlánach rianú stáit idirphlé a mhéadú go réimsí iolracha mar gheall ar an bhfás ar líon na n-athróg atá á rianú. Ina theannta sin, ní bhaineann samhlacha rianaithe staide dialóige úsáid shainráite fós as na caidrimh idir athróga idirphlé, amhail sliotáin trasna na bhfearann. Tá sé beartaithe againn modhanna tuartha struchtúir atá bunaithe ar fhuinneamh a úsáid le haghaidh tasc rianaithe stáit idirphlé ar scála mór in dhá thacar sonraí comhphlé fearainn iolracha. Léiríonn ár dtorthaí: (i) go mbíonn torthaí níos fearr ag samhaltú spleáchais inathraithe; agus (ii) tá an t-aschur tuartha struchtúrtha ar aon dul leis na prionsabail um shrianadh ar luach sliotán idirphlé. Is é an toradh a bhíonn air seo ná treoracha geallta chun samhlacha úrscothacha a fheabhsú trí spleáchais athraitheacha a ionchorprú ina bpróiseas réamh-mheastacháin.', 'el': 'Η κλιμάκωση της παρακολούθησης κατάστασης διαλόγου σε πολλούς τομείς είναι πρόκληση λόγω της αύξησης του αριθμού των μεταβλητών που παρακολουθούνται. Επιπλέον, τα μοντέλα παρακολούθησης κατάστασης διαλόγου δεν χρησιμοποιούν ακόμη ρητά σχέσεις μεταξύ μεταβλητών διαλόγου, όπως υποδοχές μεταξύ τομέων. Προτείνουμε τη χρήση ενεργειακών μεθόδων πρόβλεψης δομών για την παρακολούθηση κατάστασης διαλόγου μεγάλης κλίμακας σε δύο πολλαπλά σύνολα δεδομένων διαλόγου. Τα αποτελέσματα μας δείχνουν ότι: (η μοντελοποίηση μεταβλητών εξαρτήσεων αποδίδει καλύτερα αποτελέσματα. και ii) το δομημένο αποτέλεσμα πρόβλεψης ευθυγραμμίζεται με τις αρχές περιορισμού της τιμής του διαλόγου. Αυτό οδηγεί σε ελπιδοφόρες κατευθύνσεις βελτίωσης μοντέλων τελευταίας τεχνολογίας ενσωματώνοντας μεταβλητές εξαρτήσεις στη διαδικασία πρόβλεψης τους.', 'hu': 'A párbeszédállapotkövetés több tartományra történő méretezése kihívást jelent a nyomon követett változók számának növekedése miatt. Továbbá a párbeszédállapot-követési modellek még nem használják ki kifejezetten a párbeszédváltozók közötti kapcsolatokat, például a tartományok közötti réseket. Javasoljuk, hogy energiaalapú struktúra-előrejelzési módszereket használjunk nagyszabású párbeszédállapot-követési feladatokhoz két több tartományban. Eredményeink azt mutatják, hogy: (i) a változó függőségek modellezése jobb eredményeket eredményez; és (ii) a strukturált előrejelzési kimenet összhangban van a párbeszédpanel slot-érték korlátozási elveivel. Ez ígéretes irányokat eredményez a legkorszerűbb modellek javítására a változó függőségek beépítésével a predikciós folyamatba.', 'ka': 'დიალოგის სტატის შენახვა მრავალ დიომენზე შესაძლებლობა შესაძლებელია, რადგან განცვლელების რაოდენობის გაზრდილების შესაძლებლობად. დამატებით, დიალოგის სტატური მოდელების შენახვედვის მოდელები უკვე არ გამოყენებს დიალოგის ცვლილების შორის შესახებ, როგორც დიალოგის სტატურების სტატურები ჩვენ შეგიძლიათ გამოყენოთ ენერგიის სტრუქტურაციის პროგრამების გამოყენება დიალოგის სტრუქტურაციის მონაცემებისთვის ორი დიალოგიის დიალოგის მონა ჩვენი წარმოდგენები გვაქვს, რომ: და ii) სტრუქტურურული წინასწორებული წინასწორება დიალოგის სიტყვა-მნიშვნელობის პრინციპების შედგენება. ეს გვეუბნება საკუთარი მოწყობილობების შესაძლებლობად მოდელების შესაძლებლობა, რომლებიც განცვლელების დასაწყებელობების პროცესში შეიყვანეთ.', 'it': "Scalare il monitoraggio dello stato dei dialoghi a più domini è difficile a causa della crescita del numero di variabili monitorate. Inoltre, i modelli di monitoraggio degli stati di dialogo non utilizzano ancora esplicitamente le relazioni tra variabili di dialogo, come slot tra domini. Proponiamo l'utilizzo di metodi di previsione della struttura basati sull'energia per attività di monitoraggio dello stato di dialogo su larga scala in due set di dati di dialogo a più domini. I nostri risultati indicano che: (i) modellare le dipendenze variabili produce risultati migliori; e (ii) l'output predittivo strutturato si allinea con i principi di vincolo slot-value di dialogo. Questo porta a direzioni promettenti per migliorare i modelli all'avanguardia incorporando dipendenze variabili nel loro processo di previsione.", 'kk': 'Диалогтың күйін бірнеше доменге қадағалау үшін өзгертілген айнымалылықтар санының өсімін себебі көмектеседі. Қосымша, диалог күйін қадағалау үлгілері домендердің арасындағы диалог айнымалыларының қатынасын, мысалы слоттарының қатынасын қолдануға болмайды. Біз енергия негіздеген құрылғының тапсырмаларын үлкен өлшемі диалог күйін қадағалау әдістерін қолдануға ұсынамыз. Біздің нәтижелеріміз: i) үлгілеу айнымалылық тәуелдіктері жақсы нәтижелерді береді. және ii) құрылған таңдау шығысы диалогтың слот мәнінің шектеу принципіне тең болады. Бұл өзгертілген тәуелдіктерді өзінің бақылау процесіне қосу арқылы өзінің күй- жай моделдерін жақсартуға болады.', 'ms': 'Mengukur pengesan keadaan dialog ke domain berbilang adalah menantang kerana pertumbuhan dalam bilangan pembolehubah yang dikesan. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, such as slots across domains.  Kami cadangkan menggunakan kaedah ramalan struktur berdasarkan tenaga untuk tugas pengesan keadaan dialog skala besar dalam dua set data dialog domain berbilang. Keputusan kami menunjukkan bahawa: (i) pemodelan dependensi pembolehubah menghasilkan keputusan yang lebih baik; dan (ii) output ramalan struktur menyesuaikan dengan prinsip halangan slot-value dialog. Ini membawa kepada arah yang berjanji untuk meningkatkan model-state-of-the-art dengan memasukkan dependensi pembolehubah dalam proses ramalan mereka.', 'mt': 'L-iskala tat-traċċar tal-istat tad-djalogu għal diversi dominji hija sfida minħabba t-tkabbir fin-numru ta’ varjabbli li qed jiġu traċċati. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, such as slots across domains.  Aħna nipproponu l-użu ta’ metodi ta’ tbassir tal-istruttura bbażati fuq l-enerġija għal kompitu ta’ traċċar tal-istat fuq skala kbira ta’ djalogu f’żewġ settijiet ta’ dejta ta’ djalogu b’diversi dominji. Ir-riżultati tagħna jindikaw li: (i) l-immudellar tad-dipendenzi varjabbli jagħti riżultati aħjar; and (ii) the structured prediction output aligns with the dialogue slot-value constraint principles.  Dan iwassal għal direzzjonijiet promettenti biex jittejbu l-mudelli l-aktar avvanzati billi jiġu inkorporati dipendenzi varjabbli fil-proċess tat-tbassir tagħhom.', 'lt': 'Didinti dialogo būklės sekimą į kelis sritis yra sunku, nes stebimų kintamųjų skaičius didėja. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, such as slots across domains.  Siūlome naudoti energija grindžiamus struktūros prognozavimo metodus didelio masto dialogo būklės sekimo užduotims dviejuose daugelio sričių dialogo duomenų rinkiniuose. Mūsų rezultatai rodo, kad: i) kintamųjų priklausomybių modeliavimas duoda geresnių rezultatų; ir ii) struktūrizuotas prognozės rezultatas atitinka dialogo laiko tarpsnių vertės apribojimo principus. This leads to promising directions to improve state-of-the-art models by incorporating variable dependencies into their prediction process.', 'ml': 'ഡയലോഗിന്റെ വലിപ്പം വലുതാക്കുന്നത് പല ഡൊമെനുകളുടെ ട്രാക്കിങ്ങ് ചെയ്യുന്നതിനാല്\u200d പല മാറ്റങ്ങളുടെയും വര്\u200dദ്ധ അതിനുശേഷം, ഡയലോഗ് സ്റ്റേറ്റ് ട്രാക്കിങ്ങ് മോഡലുകള്\u200d ഇപ്പോഴും വ്യക്തമായി ഉപയോഗിക്കുന്നില്ല, ഡയലോഗ് മാറ്റങ്ങള്\u200dക്കി നമ്മള്\u200d ഊര്\u200dജ്ജം അടിസ്ഥാനമാക്കുന്ന ഘടനയിലുള്ള പ്രവചനങ്ങള്\u200d ഉപയോഗിക്കുന്നത് രണ്ടു പല ഡൊമൈന്\u200d ഡയലോഗ് ഡാറ്റാസറ്റുകളില്\u200d പ നമ്മുടെ ഫലങ്ങള്\u200d കാണിച്ചുകൊണ്ടിരിക്കുന്നു: (i) മാറ്റല്\u200d ആശ്രയിക്കുന്നത് നല്ല ഫലങ്ങള്\u200d കൊണ്ട് വരും. (ii) നിര്\u200dമ്മിക്കപ്പെട്ട പ്രവചനത്തിന്റെ പുറത്ത് നിര്\u200dമ്മിക്കപ്പെട്ട സംസാരത്തിന്റെ സ്ലോട്ട്-മൂല്യ ഇത് വേരിയല്\u200d ആശ്രയിക്കുന്നതിനെ അവരുടെ പ്രവചന പ്രക്രിയയിലേക്ക് ചേര്\u200dക്കുന്നതിലൂടെ മാറ്റം വരുത്തുന്നതിനായി വാഗ്ദാന', 'mn': 'Диалог хэлбэрийн байр суурь олон хэсэг хүртэл хадгалах нь хувьсагчийн тооны өсөлтөөс шаардлагатай. Дараа нь диалогын байр суурь дагуулах загварууд диалогын хувьсалын хоорондын харилцааны хэрэглээ тодорхой ашиглахгүй. Бид энерги дээр суурилсан бүтэц таамаглах арга зам ашиглаж, том хэмжээний диалог хэлбэрийн байр сууриллагын дагуулах ажил хоёр олон домены диалог өгөгдлийн санд. Бидний үр дүн нь: i) загварчлал өөрчлөлтийн хамааралтай байдал нь илүү сайн үр дүнг гаргадаг. (ii) бүтээгдэхүүн таамаглалтын үр дүн нь диалогын слот-утгын хязгаарлалттай холбогддог. Энэ нь өөрчлөлтийн хамааралтай байдлыг таамаглах үйл явцдаа сайжруулахын тулд амлалтай замаар ирдэг.', 'no': 'Skalering av dialogtilstanden til fleire domene er vanskeleg på grunn av veksten i talet på variablar som vert spora. I tillegg kan dialogtilstand-sporingsmodeller ikkje bruka likevel forandringar mellom dialogvariabler, slik som plasser over domene. Vi foreslår å bruka energiebaserte strukturforhåndsvisingsmetodar for storskala dialogoppgåve for å spora oppgåve i to fleire domenedialogar. Resultatet våra tyder på at: i) modellering av variabelnader gjev betre resultat. og ii) utgangen av strukturerte forhåndsvising er tilsvarande med avgrensingsprinsippene for dialogvindauget. Dette fører til å foreslå retningar til å forbetra kunstmodeller ved å inkludere variabel avhengighet i forhåndsvisingsprosessen sin.', 'pl': 'Skalowanie śledzenia stanu dialogu do wielu domen jest trudne ze względu na wzrost liczby śledzonych zmiennych. Ponadto modele śledzenia stanów dialogowych nie wykorzystują jeszcze wyraźnie relacji między zmiennymi dialogowymi, takimi jak sloty w różnych domenach. Proponujemy wykorzystanie energetycznych metod przewidywania struktury do dużej skali zadań śledzenia stanu dialogu w dwóch zestawach danych dialogowych. Nasze wyniki wskazują, że: (i) modelowanie zależności zmiennych daje lepsze wyniki; oraz ii) strukturyzowany wynik prognozowania jest zgodny z zasadami ograniczenia wartości slotu dialogowego. Prowadzi to do obiecujących kierunków poprawy najnowocześniejszych modeli poprzez włączenie zmiennych zależności do ich procesu przewidywania.', 'mk': 'Разголемувањето на дијалогот за следење на состојбата на повеќе домени е предизвикувачко поради зголемувањето на бројот на променливи кои се следат. Покрај тоа, дијалошките модели за следење на состојбата сé уште не ги користат односите меѓу дијалошките променливи, како што се слотовите меѓу домените. Ние предлагаме употреба на енергетски методи за предвидување на структурата за голем дијалог задача за следење на состојбата во две многутрински дијалози на податоци. Нашите резултати покажуваат дека: (i) моделирањето на променливите зависности дава подобри резултати; and (ii) the structured prediction output aligns with the dialogue slot-value constraint principles.  This leads to promising directions to improve state-of-the-art models by incorporating variable dependencies into their prediction process.', 'ro': 'Scalarea urmăririi stării dialogului la mai multe domenii este dificilă datorită creșterii numărului de variabile urmărite. În plus, modelele de urmărire a stărilor de dialog nu utilizează încă în mod explicit relațiile dintre variabilele de dialog, cum ar fi sloturile între domenii. Propunem utilizarea metodelor de predicție a structurii bazate pe energie pentru activitățile de urmărire a stării dialogului la scară largă în două seturi de date de dialog cu domenii multiple. Rezultatele noastre indică faptul că: (i) modelarea dependențelor variabile dă rezultate mai bune; și (ii) rezultatul predicției structurate se aliniază cu principiile limitării slotului de dialog. Acest lucru duce la direcții promițătoare de îmbunătățire a modelelor de ultimă generație prin încorporarea dependențelor variabile în procesul lor de predicție.', 'sr': 'Razmjerenje države dijaloga za praćenje višestrukih domena je izazovno zbog rasta broja promjena koje se prate. Osim toga, modeli praćenja država dijaloga još ne koriste veze između promjena dijaloga, kao što su sloti preko domena. Predlažemo da koristimo metode predviđanja strukture na energiji za veliku skalu državnog praćenja dijaloga u dva seta podataka o dijalogu. Naši rezultati ukazuju na to da: i) modeliranje varijantnih zavisnosti donosi bolji rezultat; i ii) strukturirani izlaz predviđanja se uklapa sa principima ograničenja vrijednosti dijaloga. To vodi do obećavajućih uputa da poboljšaju modele stanja umjetnosti uključujući promjene zavisnosti u njihov proces predviđanja.', 'so': 'Kalajarida sawirada dowladda ee ku socoshada wadamada kala duduwan waa dhibaatooyin ku jirta koritaanka tirada bedelyada lagu soo dabarayo. Sidoo kale tusaalayaasha soo socoshada ee dowladu weli si cad uma isticmaalo xiriirka kala bedelada, tusaale ahaan sawirada dalalka ku yaala. Waxaynu horumarinaynaa isticmaalka qaababka sii-sheegidda dhismaha energiga ee u qoran diyaarinta dowladda oo ballaadhan oo ah shaqo ku socoshada labada sawir oo dowlad ah. Abaalkayaga waxaa loola jeedaa: (i) ku sameynta suuradaha kala duwan ee ku saabsan, waxay soo bixisaa resulto ka wanaagsan; ㈡ soo baxa wax la sii sheego oo la dhisay waxay ku siman yihiin qaynuunno xadhig ah oo ku qoran qoraalka. Taas waxay ku hoggaamisaa hagitaanka si uu u beddelo qaababka farshaxanta ah, si uu ugu dhigo xirfadaha kala duwan oo uu u beddelo baaritaankooda la sii sheegayo.', 'si': 'සංවාදය ස්ථානය පරීක්ෂණය ගොඩක් ඩෝමේන් වලට ප්\u200dරශ්නයක් වෙන්න ප්\u200dරශ්නයක් තියෙන්නේ වෙනස් සංඛ්යාව ඉතින්, සංවාද ස්ථානය පරීක්ෂණ මොඩල් තවමත් ප්\u200dරශ්නයෙන් සංවාද වෙනස් අතර සම්බන්ධය භාවිත කරන්නේ නෑ, ජාති අපි ප්\u200dරශ්නය කරන්නේ ශක්තිමත් අධාරිත සංවාදය ප්\u200dරශ්නය කරන්න ප්\u200dරශ්නයක් ලොකු සංවාදයේ සංවාදයේ වැඩි  අපේ ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dර සහ (i) සංස්ථාපනය විශ්වාස ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරත මේක පොරොන්දු විදිහට ප්\u200dරශ්නයක් තියෙනවා වෙනස් විශේෂ විශේෂ විශේෂ විශේෂ විශේෂ විශේෂ විධාරය', 'sv': 'Att skala upp spårning av dialogtillstånd till flera domäner är utmanande på grund av ökningen av antalet variabler som spåras. Dessutom använder modeller för spårning av dialogtillstånd ännu inte uttryckligen relationer mellan dialogvariabler, till exempel platser mellan domäner. Vi föreslår att man använder energibaserade strukturförutsägelsemetoder för storskalig dialog tillståndsspårning i två dialogdataset med flera domäner. Våra resultat indikerar att: (i) modellering av variabla beroenden ger bättre resultat; och ii) den strukturerade prognosutmatningen överensstämmer med principerna för begränsning av intervallvärden i dialogen. Detta leder till lovande riktningar för att förbättra state-of-the-art modeller genom att införliva variabla beroenden i deras förutsägelseprocess.', 'ta': 'மாறிகளின் எண்ணிக்கையில் பின்பற்றப்படும் மாறிகளின் எண்ணிக்கையினால் பலவிதமான தடங்களுக்கு உரையாடல் நிலை பாதுகா அதற்கும் மேலும், உரையாடல் நிலை பின்பற்றும் மாதிரிகளுக்கு இடையே உரையாடல் மாறிகளுக்கு இடையே உள்ள தொடர்புகளை தெளிவாக பயன்படுத பெரிய அளவு உரையாடல் நிலை பின்பற்றும் செயல்பாடு முடிவு (ii) உருவாக்கப்பட்ட முன்வெளியீடு உரையாடல் செருகு- மதிப்பு கட்டுப்பாட்டு விதிமுறைகளுடன் ஒதுங்கும். இது வாக்களிக்கப்பட்ட மாதிரிகளை மேம்படுத்துவதற்கு மாறிகளின் சார்புகளை முன்னோட்டத்திற்கு சேர்க்க வேண்டும்.', 'ur': 'بہت سی ڈومین میں ڈالیٹ ٹراک کرنے کے لئے ڈالیٹ ایٹ ایٹ اکل کرنے کی وجہ سے چلنے کا مشکل ہے. اور اس کے بعد، ڈیلوگ سٹیٹ تراکینگ موڈل ابھی صریح طریقے سے ڈیلوگ بدیروں کے درمیان رابطہ استعمال نہیں کرتا، جیسے ڈومین کے درمیان اسلوٹ. ہم نے انرژی بنیادی ساختاری پیش بینی طریقے کے استعمال کرنا پیش بینی کرتا ہے دو دفعہ ڈومین ڈاٹ سٹ میں دو دفعہ ڈاٹ سٹ میں ڈاٹ سیٹ ٹراک کرنے کے لئے. ہمارے نتیجے نشان دیتے ہیں کہ: (i) نمادلینگ متغیر dependencies اچھے نتیجے پاتے ہیں۔ اور (ii) ساختہ پیش بینی آئٹ پیٹ پیٹ پیٹ پیٹ پیٹ پیٹ ڈالٹ-ارزش کے ساتھ برابر ہوتی ہے۔ یہ وعدہ دینے کے لئے ہے کہ ان کی پیش بینی پرسس میں تبدیلی اعتباری کو جمع کریں۔', 'uz': "Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked.  Koʻrsatilgan, dialog holati taʼminlovchisi modellari hozirga muloqat oynalar oʻzgarishlarning orasidan bogʻlamalaridan foydalanmaydi, balki domen hamma narsalar kabi. Biz bir necha nechta domen muloqat oynasidagi muloqat muloqat muloqat muloqat muloqat muloqat muloqat oynasidagi energiya asosi tuzilishi metodlaridan foydalanishimizni talab qilamiz. Bizning natijalarimiz: (i) ulisidagi ishlatuvchilar yaxshi natijaga ega bo'ladi; ㈡ tashkilotli taqdimot natijasi dialog slot-qiymati cheksiz qoidalari bilan birga qarshi. Bu hodisa yordamchilarga o'zgarishlarni prediction jarayonini o'zgartirish imkoniyatini o'zgartirish imkoniyatini bajarishi mumkin.", 'vi': 'Phóng độ theo dõi trạng thái của đối thoại thành nhiều miền thách thức nhờ vào sự tăng trưởng số các biến số đang được lần theo. Hơn nữa, các mô hình định vị hộp thoại chưa được dùng trực tiếp các mối quan hệ giữa các biến số thoại, như các rãnh trên mỗi miền. Chúng tôi đề nghị sử dụng các phương pháp dự đoán cấu trúc năng lượng cho nhiệm vụ theo dõi tình trạng giao tiếp trên diện rộng trong hai tập tin dữ liệu đa miền. Kết quả của chúng ta cho thấy: i) sự phụ thuộc biến mẫu sẽ có kết quả tốt hơn. và thứ hai: kết quả dự đoán cấu trúc phù hợp với các nguyên tắc điều khiển giá trị thời gian. Điều này dẫn tới hướng dẫn hứa hẹn để cải thiện các mô hình hiện đại bằng cách nhập các mối quan hệ tùy biến vào quá trình dự đoán.', 'hr': 'Povećanje države dijaloga praćenja višestrukim domenama je izazovno zbog rasta broja promjena koje se prate. Osim toga, modeli praćenja stanja dijaloga još uvijek ne koriste jasno odnose između promjena dijaloga, poput mjesta preko domena. Predlažemo koristiti metode predviđanja strukture na energiji za veliku skalu zadatak praćenja države dijaloga u dvije podatke o dijalogu podataka na višestrukoj domenu. Naši rezultati ukazuju na to da: i) modeliranje varijačnih zavisnosti donosi bolji rezultat; i ii) strukturirani izlaz predviđanja odgovara principima ograničenja vrijednosti dijaloga. To vodi do obećavajućih smjera za poboljšanje modela stanja umjetnosti uključujući promjene zavisnosti u njihov proces predviđanja.', 'da': 'Det er en udfordring at skalere op dialogtilstandssporing til flere domæner på grund af væksten i antallet af variabler, der spores. Desuden bruger modellerne til sporing af dialogtilstande endnu ikke eksplicit relationer mellem dialogvariabler, såsom slots på tværs af domæner. Vi foreslår at bruge energibaserede strukturforudsigelsesmetoder til storstilet dialog state tracking opgave i to flere domæner dialogdatasæt. Vores resultater indikerer, at: (i) modellering af variable afhængigheder giver bedre resultater; og ii) den strukturerede forudsigelses output stemmer overens med principperne for begrænsning af dialogslots-værdi. Dette fører til lovende retninger til at forbedre state-of-the-art modeller ved at indarbejde variable afhængigheder i deres forudsigelsesproces.', 'nl': 'Het opschalen van dialoogstatus tracking naar meerdere domeinen is een uitdaging vanwege de groei van het aantal variabelen dat wordt gevolgd. Bovendien maken dialogstatetracking modellen nog niet expliciet gebruik van relaties tussen dialoogvariabelen, zoals slots tussen domeinen. We stellen voor om energiegebaseerde structuurvoorspellingsmethoden te gebruiken voor grootschalige dialoogstatus tracking taak in twee meerdere domeindialoog datasets. Onze resultaten geven aan dat: (i) het modelleren van variabele afhankelijkheden betere resultaten oplevert; en ii) de gestructureerde voorspelling-output is afgestemd op de beginselen van beperking van de dialoogsleuf-waarde. Dit leidt tot veelbelovende richtingen om state-of-the-art modellen te verbeteren door variabele afhankelijkheden in hun voorspellingsproces op te nemen.', 'id': 'Mengukur pelacakan keadaan dialog ke beberapa domain adalah tantangan karena pertumbuhan dalam jumlah variabel yang sedang dilacak. Selain itu, model pelacakan keadaan dialog belum secara eksplicit menggunakan hubungan antara variabel dialog, seperti slot di seluruh domain. We propose using energy-based structure prediction methods for large-scale dialogue state tracking task in two multiple domain dialogue datasets.  Hasil kami menunjukkan bahwa: (i) model dependensi variabel memberikan hasil yang lebih baik; and (ii) the structured prediction output aligns with the dialogue slot-value constraint principles.  Ini mengarah ke arah yang berjanji untuk meningkatkan model terbaik dengan memasukkan dependensi variabel dalam proses prediksi mereka.', 'de': 'Die Skalierung der Dialogzustandsverfolgung auf mehrere Domänen ist aufgrund der steigenden Anzahl der zu verfolgenden Variablen schwierig. Darüber hinaus nutzen Dialogstatus-Tracking-Modelle noch nicht explizit Beziehungen zwischen Dialogvariablen, wie z.B. Domänenübergreifenden Slots. Wir schlagen die Verwendung energiebasierter Strukturvorhersagemethoden für großangelegte Dialogzustandsverfolgungsaufgaben in zwei Domänendialogdatensätzen vor. Unsere Ergebnisse zeigen, dass: (i) die Modellierung variabler Abhängigkeiten bessere Ergebnisse liefert; und ii) die strukturierte Vorhersageausgabe stimmt mit den Dialogslotwert-Beschränkungsprinzipien überein. Dies führt zu vielversprechenden Richtungen zur Verbesserung modernster Modelle, indem variable Abhängigkeiten in ihren Vorhersageprozess einfließen.', 'fa': 'اندازه\u200cگیری وضعیت محاورۀ محاورۀ محاورۀ بیشتری به دامنۀ چندین مشکل است به دلیل رشد تعداد متغیرات که دنبال می\u200cشوند. علاوه بر این، مدل ردیابی وضعیت محاورۀ محاورۀ محاورۀ هنوز به طور واضح از رابطه بین متغیرات محاورۀ محاورۀ محاورۀ استفاده نمی\u200cکنند، مانند نقطه\u200cها در طول دامنه. ما پیشنهاد می\u200cکنیم با استفاده از روش پیش\u200cبینی ساختار بر بنیاد انرژی برای دنبال کردن وضعیت محاورژی بزرگ در دو مجموعه داده\u200cهای محاورژی متعدد دامنی. نتایج ما نشان می دهند که: i) مدل کردن بستگی متغیر به نتایج بهتر می دهد. و (ii) نتیجه پیش\u200cبینی ساخته شده با اصول\u200cهای محدودیت ارزش\u200cهای محدودیت محدودیت محدودیت محدودیت محدودیت می\u200cکند. این باعث می\u200cشود که راه\u200cهای قول\u200cدهنده برای بهبود مدل\u200cهای موجود هنر با توجه به بستگی\u200cهای متغیر در فرایند پیش\u200cبینی\u200cشان باشد.', 'sw': 'Kupambanua upya wa mazungumzo ya hali ya kufuatilia maeneo mbalimbali yanachanganya kutokana na ongezeko la mabadiliko yanayofuatiliwa. Zaidi ya hayo, mifano ya ufuatiliaji wa hali ya mazungumzo bado haijaweza kutumia mahusiano kati ya mabadiliko ya mazungumzo, kama vile sayari katika maeneo mbalimbali. Tunazipendekeza kutumia mbinu za kutabiri kwa ajili ya mazungumzo makubwa ya mazungumzo ya nchi katika seti mbili za mazungumzo ya ndani. Matokeo yetu yanaonyesha kuwa: (i) kutegemea mabadiliko yanaleta matokeo mazuri; na matokeo yaliyotengenezwa yanalinganisha na kanuni za msingi wa mazungumzo. Hii inapelekea maelekezo ya kuahidini kuboresha mitindo ya sanaa kwa kuingiza kutegemea tofauti katika mchakato wa kutabiri.', 'tr': 'Birden köp sahypa gözlemek üçin dijalog durumyny yzarlamak çykarylýar Munuň üstine, dialogyň durum taýýarlama modelleri heniz aňsatly dijalog üýtgeşmeleri ýaly nokatlar arasynda seretleri ullanyp bilmeýär. Biz enerji tabanly struktur öngörüm yöntemlerini uly ölçekli dijalog durumlary üçin iki sany domin düzümlerinde ulanmagy teklip edip görýäris Bizim netijelerimiz: (i) modelleme değerli bağlılıklar daha iyi netijeleri getirir. ii) düzümlenmiş önüm çykgyş dýalogyň süren kadalary bilen çykýar. Bu, farklı bağlılıkları tahmin sürecine koyarak sunucu modellerini geliştirmek için söz verilen yönlere yol gösterir.', 'af': 'Skaal ops dialoog staat agtervolg na veelvuldige domeine is uitgelaat vanweë die groei in die aantal veranderlikes wat agtervolg word. Verder, dialoog staat agtervolg modele maak nog nie uitsondelik gebruik van verhoudings tussen dialoog veranderlikes, soos slots oor domeine. Ons voorstel om energiegebaseerde strukturevoorskoumetodes te gebruik vir groot-skaal dialoog staat agtervolg taak in twee veelvuldige domein dialoog datastel. Ons resultate wys dat: (i) modellering veranderlike afhanklikhede beter resultate gee; en (ii) die struktureerde voorskou uitset gelyk met die dialoog slot-waarde beperking prinsipels. Hierdie lei na belofte rigtings om state-of-the-art modele te verbeter deur veranderlike afhanklikhede in hul voorskou proses te inkorporeer.', 'bg': 'Разширяването на проследяването на състоянието на диалоговия прозорец до няколко домейна е предизвикателство поради нарастването на броя на променливите, които се проследяват. Освен това моделите за проследяване на състоянието на диалоговия прозорец все още не използват изрично релации между променливи диалоговия прозорец, като слотове между домейни. Предлагаме използването на енергийни методи за прогнозиране на структурата за задача за широкомащабно проследяване на състоянието на диалога в два диалогови набора данни с множество домейни. Нашите резултати показват, че: (и) моделирането на променливи зависимости дава по-добри резултати; и ii) структурираният изход на прогнозирането съответства на принципите за ограничение на стойността на слота в диалоговия прозорец. Това води до обещаващи насоки за подобряване на съвременните модели чрез включване на променливи зависимости в процеса на прогнозиране.', 'sq': 'Shkallimi i gjendjes së dialogut për gjurmë në domena të shumta është i vështirë për shkak të rritjes së numrit të ndryshuesve që janë gjurmuar. Përveç kësaj, modelet e ndjekjes së gjendjes së dialogut ende nuk përdorin shprehësisht marrëdhëniet midis ndryshuesve të dialogut, të tilla si slots anembanë domeneve. Ne propozojmë përdorimin e metodave të parashikimit të strukturës bazuar në energji për detyrën e gjurmimit të gjendjes në shkallë të madhe të dialogut në dy grupe të dhënash të dialogut të shumëfishtë. Rezultatet tona tregojnë se: (i) modelimi i varësive të ndryshme jep rezultate më të mira; dhe (ii) rezultati i parashikimit të strukturuar përputhet me parimet e kufizimit të vlerës së slot-vlerës së dialogut. Kjo shpie në drejtime premtuese për të përmirësuar modelet më të larta duke përfshirë varësitë e ndryshme në procesin e parashikimit të tyre.', 'am': 'የጥያቄ መምረጫን በማሳመር ላይ የጥያቄ ስህተት በጥያቄ ላይ የመስመር ስህተት በጥያቄ ላይ የመስመር ማሳየት ነው፡፡ ከዚህም በላይ የመስኮት ግንኙነት የመስመር አካባቢ ምርጫዎች በአካባቢ ውስጥ እንደደረጃ መክፈቻ የሚደረግ ግንኙነትን በአካባቢ ላይ አይጠቀምም፡፡ የenergy-based ቅድመት ምርጫዎች ለትልቅ ስልጣን የመስኮት አካባቢ ስርዓት በሁለት ብዛት ዶሜን ዳታዎችን በመስጠት እናስጀጋለን፡፡ ፍሬዎቻችን እንዲህ የሚያሳየው:(i) የተለየ ተሟጋቾች ትክክል ፍሬዎችን ያፈራሉ፡ (ii) የተመሠረተውም የውጤት ውጤት በመስኮት ላይ ግንኙነት የግንኙነት አካል ነው፡፡ ይሄ የልዩ ተደጋጋፊዎችን ለመፍጠር ፕሮጀክታቸውን በመጠቀም የመስጠት ጉዳይ አካባቢ-የዓርት ሥርዓት አካባቢ እንዲያበጅል የሚያስፈራራው መንገድ ይመራል፡፡', 'ko': '추적 변수의 증가로 인해 대화 상태 추적을 여러 영역으로 확장하는 것은 도전이다.그 밖에 대화 상태 추적 모델은 대화 변수 간의 관계, 예를 들어 크로스 필드의 시간 간격을 명확하게 이용하지 못했다.우리는 두 개의 다역 대화 데이터에서 에너지를 바탕으로 하는 구조 예측 방법을 집중적으로 사용하여 대규모 대화 상태 추적 임무를 진행할 것을 건의합니다.우리의 결과는 다음과 같다. (i) 변수 의존성 모델링은 더욱 좋은 결과를 얻을 수 있다.(ii) 구조화 예측 출력은 대화 시간 간격값 제약 원칙에 부합된다.이는 변수 의존성을 예측 과정에 포함시켜 최첨단 모델을 개선하는 유망한 방향을 초래했다.', 'az': 'Daylı domena təqib etmək müxtəlif dəyişikliklərin sayısının artığına görə dəyişiklikdir. Daha sonra, dayalı durum izləmə modelləri halda dayalı dəyişikliklər arasındakı bağlantıları, domenalar arasındakı slotlar kimi istifadə etməzlər. Biz enerji tabanlı struktur tədbirli metodlarını böyük ölçüdə diyalog durumu izləmə işini iki dəstə dəyişdiririk. Sonuçlarımız belə göstərir ki: i) modellik dəyişiklik bağlılıqları daha yaxşı sonuçlar verir. və ii) qurulmuş tədbir çıxımı dialogın slot-qiyməti müəyyən dəyişikliklərinə uyğunlaşdırır. Bu, dəyişiklik bağımlılıqlarını önləşdirmək sürətinə qovuşdurduqları halda, sanat modellərini daha yaxşılaşdırmaq üçün vəd verilən yollara yönəldir.', 'bn': 'বেশ কয়েকটি ডোমেনের সংখ্যার জন্য ডায়ালগের রাষ্ট্র ট্র্যাকিং আকার করা হচ্ছে চ্যালেঞ্জালেঞ্জ। এছাড়াও, ডায়ালগ রাষ্ট্রীয় ট্র্যাকিং মডেল এখনো স্পষ্টভাবে ডায়ালগ ভেরিয়েলের মধ্যে সম্পর্ক ব্যবহার করে না, যেমন ডোমেনের সার আমরা বিশাল স্ক্যাল ডায়ালগের রাষ্ট্র ট্র্যাকিং কাজের জন্য শক্তি ভিত্তিক কাঠামো ভবিষ্যদ্বাণী পদ্ধতি ব্যবহার করার প্রস্ আমাদের ফলাফল নির্দেশ করছে যে: (i) মডেলিং ভেরিয়েল নির্ভর করা ভাল ফলাফল পাবে; (ii) যে ভবিষ্যৎবাণী আউটপুট তৈরি করা হয়েছে তার সাথে ডায়ালগের স্লোট-মান নিয়ন্ত্রণ নীতির মাধ্যমে। এটি প্রতিশ্রুতির দিকে প্রতিশ্রুতি প্রদান করে তাদের ভবিষ্যদ্বাণী প্রক্রিয়ায় ভিয়ারিভাবে নির্ভর করার মাধ্যমে শিল্প-', 'hy': "Շատ բնագավառներ հետևելու վիճակի մեծացնելը դժվար է, քանի որ հետևում են փոփոխականների թիվը աճում է: Ավելին, դասախոսության վիճակի հետևման մոդելները դեռ բացատրականորեն չեն օգտագործում դասախոսության փոփոխականների միջև եղած հարաբերությունները, ինչպիսիք են օրինակ բնագավառների տարբեր վայրերի ընթացքում գտնվո Մենք առաջարկում ենք օգտագործել էներգիայի վրա հիմնված կառուցվածքի կանխատեսման մեթոդներ մեծ շրջանակի հաղորդակցման վիճակի հետևման խնդիրների համար երկու բազմաթիվ բնագավառների հաղորդակցման տվ Մեր արդյունքները ցույց են տալիս, որ i) փոփոխական կախվածությունների մոդելավորումը ավելի լավ արդյունքներ է տալիս, և (ու) կառուցվածքավորված կանխատեսման արտադրությունը համապատասխանում է արժեքի-արժեքի սահմանափակումների սկզբունքներին: Սա հանգեցնում է խոստացող ուղղություններին բարելավելու նորագույն մոդելները' ներառելով փոփոխական կախվածությունները իրենց կանխատեսման գործընթացի մեջ:", 'bs': 'Razmjerenje države dijaloga za praćenje višestrukih domena je izazovno zbog rasta broja promjena koje se prate. Nadalje, modeli praćenja država dijaloga još uvijek ne koriste jasno odnose između promjena dijaloga, poput slova preko domena. Predlažemo koristiti metode predviđanja strukture na energiji za veliku skalu zadatak praćenja države dijaloga u dva kompeta podataka o dijalogu u više domena. Naši rezultati ukazuju na to da: i) modeliranje varijantnih zavisnosti donosi bolji rezultat; i ii) strukturirani izlaz predviđanja odgovara principima ograničenja vrijednosti dijaloga. To vodi do obećavajućih uputa da poboljšaju modele stanja umjetnosti uključujući varijačne zavisnosti u njihov proces predviđanja.', 'cs': 'Škálování sledování stavu dialogu na více domén je náročné vzhledem k růstu počtu sledovaných proměnných. Kromě toho modely sledování stavu dialogů dosud explicitně nevyužívají vztahy mezi proměnnými dialogu, jako jsou sloty napříč doménami. Navrhujeme využití energeticky založených metod predikce struktury pro úlohu sledování stavu dialogu ve dvou databázích dialogů s více doménami. Naše výsledky ukazují, že: (i) modelování variabilních závislostí přináší lepší výsledky; a ii) strukturovaný výstup predikce je v souladu se zásadami omezení hodnoty slotu dialogu. To vede k slibným směrům pro zlepšení nejmodernějších modelů začleněním proměnných závislostí do jejich predikčního procesu.', 'et': 'Dialoogi oleku jälgimise laiendamine mitmele domeenile on keeruline jälgitavate muutujate arvu kasvu tõttu. Peale selle ei kasuta dialoogi oleku jälgimise mudelid veel selgesõnaliselt dialoogimuutujate vahelisi seoseid, näiteks domeenide vahelisi pesasid. Teeme ettepaneku kasutada energiapõhiseid struktuuri ennustamismeetodeid dialoogi oleku jälgimiseks suuremahuliseks ülesandeks kahes mitme domeeni dialoogi andmekogumis. Meie tulemused näitavad, et: (i) muutuvate sõltuvuste modelleerimine annab paremaid tulemusi; ja ii) struktureeritud prognooside väljund on kooskõlas dialoogi teenindusaja väärtuse piiramise põhimõtetega. See toob kaasa paljutõotavad suunad kaasaegsete mudelite parandamiseks, lisades nende prognoosimisprotsessi muutuvad sõltuvused.', 'ca': "L'escala del seguiment de l'estat del diàleg a múltiples dominys és difícil degut al creixement del nombre de variables seguides. A més, els models de seguiment de l'estat del diàleg encara no utilitzen explícitament les relacions entre les variables del diàleg, com els slots a través de dominis. Proposem utilitzar mètodes de predicció d'estructura basat en l'energia per a la tasca de seguiment de l'estat en diàleg a gran escala en dos conjunts de dades de diàleg de dominis múltiples. Els nostres resultats indican que: (i) modelar les dependencies variables dóna millors resultats; i ii) la producció de predicció estructurada s'alineix amb els principis de restricció de valor de l'interval de diàleg. This leads to promising directions to improve state-of-the-art models by incorporating variable dependencies into their prediction process.", 'fi': 'Dialogin tilaseurannan skaalaaminen useaan verkkotunnukseen on haastavaa seurattavien muuttujien määrän kasvun vuoksi. Lisäksi valintaikkunan tilan seurantamallit eivät vielä nimenomaisesti käytä vuoropuhelumuuttujien välisiä suhteita, kuten eri toimialueiden välisiä aukkoja. Ehdotamme energiapohjaisten rakenteiden ennustusmenetelmien käyttöä laajamittaiseen dialogin tilan seurantaan kahdessa eri toimialueen dialogitiedostossa. Tuloksemme osoittavat, että: (i) muuttuvien riippuvuuksien mallintaminen tuottaa parempia tuloksia; ja ii) strukturoitu ennustetuotos on linjassa dialogin slot-value restriction periaatteiden kanssa. Tämä johtaa lupaaviin suuntiin nykyaikaisten mallien parantamiseen sisällyttämällä muuttuvia riippuvuuksia niiden ennustusprosessiin.', 'ha': "@ action: button Furan wannan, misãlai masu shiryi na halin zauren akwatin bayani ba za su iya amfani da tsari masu sãɓa tsakanin zauren akwatin zauren akwatin bayani, kamar misali slotts a kowace guda. Tuna goyyade yin amfani da hanyoyin bayani na baka nishatin nishati wa shirin bayanin zauren akwatin bayani mai girma cikin jerin shiryarwa cikin tsarin zauren akwatin bayani biyu masu yawa. MatamayinMu ke nũna cẽwa: (i) Shirin masu motsi da dabam-dabam za'a fitar da mafi alhẽri; Da matsayin da aka samar da ƙayyade kimar bango na zauren akwatin bayanin. Wannan yana ƙara wa shiryarwa mai yi musu wa'adi da ya kyautata misãlai-sanar da su, kuma ya shigar da masu inganci masu variant zuwa aikin basĩri.", 'he': 'מעקב המצב של הדיולוגים למספר תרומות הוא מאתגר בגלל הגידול במספר המשתנים שנעקבים. בנוסף, דוגמני מעקב מצב דיאלוג עדיין לא משתמשים באופן ברור במערכת יחסים בין משתנים דיאלוג, כמו שטחים ברחבי שטחים. אנו מציעים להשתמש בשיטות צפוי מבנה מבוסס באנרגיה עבור משימה מעקב מדיום במידה גדולה בשני קבוצות נתונים של דיאלוג מרובים. התוצאות שלנו מצביעות על: (i) הדוגמה תלויות משתנות מובילה תוצאות טובות יותר; (ii) יציאת התחזויות המבנית מתאימה לעקרונות ההגבלה של ערך השיחה. זה מוביל לכיוונים מבטיחים לשפר מודלים חדשים על ידי שיתוף תלויות משתנות לתהליך החזוי שלהם.', 'sk': 'Razširitev sledenja stanju pogovornega okna na več domen je zahtevna zaradi rasti števila spremenljivk, ki jim sledijo. Poleg tega modeli sledenja stanju pogovornega okna še ne izrecno uporabljajo odnosov med spremenljivkami pogovornega okna, kot so reže med domenami. Predlagamo uporabo energetsko temelječih metod napovedovanja strukture za opravilo velikega obsega sledenja stanja dialoga v dveh podatkovnih naborih več domenskih dialogov. Naši rezultati kažejo, da: (i) modeliranje variabilnih odvisnosti prinaša boljše rezultate; in (ii) strukturirani rezultat napovedi je usklajen z načeli omejitve vrednosti reže v dialogu. To vodi do obetavnih smeri za izboljšanje najsodobnejših modelov z vključitvijo variabilnih odvisnosti v njihov proces napovedovanja.', 'jv': 'text dialog We proposal use power-basic structural preview method for big-scale dialog state tracking task in Two Multiple domain dialog dataset. Reject string" in "context_BAR_stringLink text-tool-action', 'bo': 'ཚད་རྟགས་ཀྱིས་མཐུད་འགྱུར་བའི་ཚད་གླེང་སྒྲོམ་གྱི་གནས་ཚད་རྗེས་སུ་འབྲས་ཡོད་པ་ལས་དཀའ་ངལ་ཡོད། མ་ཟད། ཌའི་ལོག་གནས་སྟངས་རྗེས་སྟངས་ཀྱི་མི་དབྱིབས་གསལ་བཤད་ཀྱིས་སྤྱོད་རུང་བའི་ཌའི་ལོག་འགྱུར་སྣང་བྱེད་མི་འདུག We propose using energy-based structure prediction methods for large-scale dialog state tracking task in two domain dialog datasets. ང་ཚོའི་འབྲས་བུ and (ii) the structured prediction output aligns with the dialog slot-value constraint principles. འདིས་གནད་དོན་དག་གི་གནས་སྟངས་ཀྱི་སྒྱུ་རྩལ་བ་དག་ཡར་རྒྱས་གཏོང་བའི་གནས་སྟངས་ལ་ཡར་རྒྱས་གཏོང་བྱེད་ཀྱི་ཡོད།'}
{'en': 'Layer-wise Guided Training for BERT : Learning Incrementally Refined Document Representations BERT : Learning Incrementally Refined Document Representations', 'es': 'Capacitación guiada por capas para BERT: aprendizaje gradual de representaciones de documentos refinadas', 'ar': 'التدريب الموجه على مستوى الطبقات لـ BERT: تعلم تمثيلات المستندات المصقولة بشكل متزايد', 'pt': 'Treinamento Guiado em Camadas para BERT: Aprendendo Representações de Documentos Incrementalmente Refinadas', 'fr': 'Formation guidée par couche pour BERT\xa0: apprentissage de représentations de documents affinées de manière incrémentielle', 'ja': 'BERTのための層別ガイド付きトレーニング：漸進的に洗練された文書表現を学習する', 'hi': 'BERT के लिए परत-वार निर्देशित प्रशिक्षण: वृद्धिशील परिष्कृत दस्तावेज़ प्रतिनिधित्व सीखना', 'zh': '针 BERT 之逐层,学增量优化之文档', 'ru': 'Послойное обучение для BERT: Изучение поэтапно уточненных представлений документов', 'ga': 'Oiliúint Treoraithe Ciseal do BERT: Ag Foghlaim Léiriúcháin Doiciméad arna scagadh go hincriminteach', 'hu': 'Rétegszempontból vezetett képzés a BERT számára: tanulás fokozottan finomított dokumentumábrázolások', 'el': 'Καθοδηγημένη εκπαίδευση για το Μάθηση Εντατικά Εξελιγμένων Αντιπροσωπειών Έγγραφων', 'ka': 'BERT- ის მხარდაჭერად მხარდაჭერად მხარდაჭერად მხარდაჭერად მხარდაჭერად მოსწავლება', 'lt': 'BERT orientuotas mokymas pagal sluoksnius: vis labiau tobulinti dokumentų atstovavimai', 'mk': 'Водено обука за БЕРТ од страна на пласт: Учење на екстрементално рефинирани документи', 'kk': 'BERT үшін қабат және бағытталған оқыту: Құжатты көбірек сақталған құжаттарды оқыту', 'ms': 'Latihan Berpanduan Lapisan untuk BERT: Belajar Perwakilan Dokumen Terbahaya Semakin Tingkat', 'ml': 'BERT-നുള്ള കൈയ്യൂട്ട് കാണിച്ചുകൊണ്ടിരിക്കുന്നതിനുള്ള പരിശീലി: അധികമായി വിശേഷിച്ച രേഖകളുടെ പ്രതിന', 'mt': 'Taħriġ Gwida skont is-Sajd għall-BERT: Tagħlim Rappreżentanzi ta’ Dokumenti Rifinati b’mod Akkumentali', 'mn': 'БЕРТ-ын давхар давхар удирдлагатай багш суралцах: Ихэнхдээ сайжруулагдсан документийн төлөөлөлт суралцах', 'no': 'Håndsvising for BERT: Læring av større refined dokumentrepresentasjonar', 'pl': 'Szkolenie z przewodnikiem warstwowym dla BERT: Uczenie się coraz bardziej dopracowanych reprezentacji dokumentów', 'ro': 'Instruire ghidată pe niveluri pentru BERT: Reprezentarea documentelor rafinate din ce în ce mai mult', 'sr': 'Upravljena obuka na slojevima za BERT: Učenje povećano refinansiranog dokumenta', 'it': 'Formazione guidata a livello per BERT: Rappresentazioni dei documenti sempre più raffinate', 'si': 'BERT වෙනුවෙන් ස්ථාන- විස්තර ප්\u200dරධාන ප්\u200dරධානය: විස්තර ප්\u200dරධානය ප්\u200dරධානයක් ඉගෙනගන්න', 'so': 'Waxbarashada hagista Layer wise-wise BERT: Learning Incrementally Refused Documentation', 'sv': 'Lagervis guidad utbildning för BERT: Lärande Ökat Raffinerad Dokumentrepresentation', 'ta': 'BERT க்கான அடுக்கு வழிகாட்டி பயிற்சி', 'ur': 'BERT کے لئے لائر واسطے ہدایت کی تطارین: اضافہ تطاریق دکھانے کی تصویر سیکھنا', 'uz': 'Name', 'vi': 'Kiểu giáo dục hướng dẫn lớp cho BERT: Phụ trách giáo dục tài liệu phức tạp.', 'nl': 'Layer-wise begeleide training voor BERT: het leren van incrementele verfijning van documentrepresentaties', 'de': 'Ebenenweise geführtes Training für BERT: Lernen von inkrementell verfeinerten Dokumentdarstellungen', 'da': 'Lagvis guidet træning for BERT: Læring forbedret dokumentrepræsentation', 'id': 'Pelatihan Berpanduan Lapisan untuk BERT: Mempelajari Perwakilan Dokumen Terbahas Semakin Besar', 'bg': 'Обучение по пластово ръководство за Научаване на постепенно усъвършенствани представяния на документи', 'sw': 'Mafunzo yanayoongozwa kwa Layer-wise kwa ajili ya BERT: Kujifunza Mazungumzo ya Hukumu', 'fa': 'آموزش راهنمایی به طریق لایه برای BERT: یاد گرفتن نمایش سند بیشتری', 'af': 'Laag- wys Geleide Oefening vir BERT: Leer Oorvloedige Verwyder Dokument Voorstellings', 'hr': 'Upravljeno vježbanje na slojevima za BERT: Učenje povećano ispravljenih dokumenta', 'sq': 'Treinimi i drejtuar nga shtresa për BERT: Mësimi i përfaqësimeve të dokumenteve të rafinuara në mënyrë më të madhe', 'am': 'undo-type', 'ko': 'BERT를 위한 계층형 가이드 교육: 단계적으로 최적화된 문서 표현 학습', 'hy': 'ԲԵՌԹ-ի ուղղությամբ ուղղությամբ վարժեցվող ուսումնասիրությունը. Ավելի բարձրացված փաստաթղթերի ներկայացումներ սովորելը', 'az': 'BERT 칲칞칲n Y칲ks톛k Y칲ks톛k Y칲ks톛k Y칲ks톛k D톛st톛 G칬r칲nt칲l톛ri 칬yr톛nm톛k', 'ca': 'Formació orientada per capa per BERT: Aprendre representacions de documentos cada cop més refines', 'bn': 'বেরেট- এর জন্য স্তরের প্রশিক্ষণ: বেশীরভাগ নথি প্রতিনিধি শিক্ষা শিক্ষা', 'cs': 'Školení s vedením na vrstvě pro BERT: Učení se inkrementálně rafinovaných reprezentací dokumentů', 'et': 'BERT-i kihiline juhendatud koolitus: järkjärguliselt täpsustatud dokumendiesinduste õppimine', 'fi': 'BERT:n tasotason ohjattu koulutus: Oppiminen asteittain tarkentuneiden asiakirjojen esittämistä', 'tr': 'BERT üçin Kalaga gijerli Görniş Ewezam: Ululykdan Beýik Sened Namaýyşlaryny öwrenmek', 'bs': 'Upravljeno vježbanje na slojevima za BERT: Učenje povećano ispravljenih dokumenta', 'jv': 'Layer-Wise guided', 'he': 'אימונים מודרכים לכיוון שיער עבור BERT: ללמוד מייצגים מסמכים מוגדרים באופן נוסף', 'sk': 'Strojno vodeno usposabljanje za BERT: učenje postopoma izpopolnjenih predstavitev dokumentov', 'ha': 'KCharselect unicode block name', 'bo': 'BERT ་ལ་བང་རིམ་པ(Layer-wise Guided Training for Training: Learning Incrementally Refined Document Representations)'}
{'en': 'Although  BERT  is widely used by the  NLP community , little is known about its inner workings. Several attempts have been made to shed light on certain aspects of BERT, often with contradicting conclusions. A much raised concern focuses on BERT’s over-parameterization and under-utilization issues. To this end, we propose o novel approach to fine-tune BERT in a structured manner. Specifically, we focus on Large Scale Multilabel Text Classification (LMTC) where documents are assigned with one or more labels from a large predefined set of hierarchically organized labels. Our approach guides specific BERT layers to predict labels from specific hierarchy levels. Experimenting with two LMTC datasets we show that this structured fine-tuning approach not only yields better  classification  results but also leads to better parameter utilization.', 'ar': 'على الرغم من استخدام BERT على نطاق واسع من قبل مجتمع البرمجة اللغوية العصبية ، إلا أنه لا يُعرف سوى القليل عن أعماله الداخلية. تم إجراء عدة محاولات لتسليط الضوء على جوانب معينة من BERT ، مع استنتاجات متناقضة في كثير من الأحيان. يركز القلق الذي أثير كثيرًا على قضايا الإفراط في البارامترات ونقص الاستخدام في BERT. تحقيقًا لهذه الغاية ، نقترح نهجًا جديدًا لضبط BERT بطريقة منظمة. على وجه التحديد ، نحن نركز على تصنيف النص متعدد التسميات واسع النطاق (LMTC) حيث يتم تعيين المستندات مع تسمية واحدة أو أكثر من مجموعة كبيرة محددة مسبقًا من الملصقات المنظمة بشكل هرمي. يوجه نهجنا طبقات BERT محددة للتنبؤ بالتسميات من مستويات التسلسل الهرمي المحددة. من خلال تجربة مجموعتي بيانات LMTC ، نظهر أن نهج الضبط الدقيق المنظم هذا لا ينتج عنه نتائج تصنيف أفضل فحسب ، بل يؤدي أيضًا إلى استخدام أفضل للمعلمات.', 'fr': "Bien que le BERT soit largement utilisé par la communauté de la PNL, on en sait peu sur son fonctionnement interne. Plusieurs tentatives ont été faites pour faire la lumière sur certains aspects du BERT, dont les conclusions sont souvent contradictoires. Une préoccupation souvent soulevée concerne les problèmes de surparamétrage et de sous-utilisation du BERT. À cette fin, nous proposons une nouvelle approche pour affiner le BERT de manière structurée. Plus précisément, nous nous concentrons sur la classification de texte multiétiquette à grande échelle (LMTC) dans laquelle les documents se voient attribuer une ou plusieurs étiquettes à partir d'un grand ensemble prédéfini d'étiquettes organisées hiérarchiquement. Notre approche guide des couches BERT spécifiques pour prédire les étiquettes à partir de niveaux hiérarchiques spécifiques. En expérimentant deux ensembles de données LMTC, nous montrons que cette approche structurée de réglage fin produit non seulement de meilleurs résultats de classification, mais également une meilleure utilisation des paramètres.", 'pt': 'Embora o BERT seja amplamente utilizado pela comunidade de PNL, pouco se sabe sobre seu funcionamento interno. Várias tentativas foram feitas para esclarecer certos aspectos do BERT, muitas vezes com conclusões contraditórias. Uma preocupação muito levantada se concentra nos problemas de superparametrização e subutilização do BERT. Para este fim, propomos uma nova abordagem para ajustar o BERT de forma estruturada. Especificamente, focamos na Classificação de Texto Multilabel em Grande Escala (LMTC), onde os documentos são atribuídos com um ou mais rótulos de um grande conjunto predefinido de rótulos organizados hierarquicamente. Nossa abordagem orienta camadas BERT específicas para prever rótulos de níveis hierárquicos específicos. Experimentando com dois conjuntos de dados LMTC, mostramos que essa abordagem estruturada de ajuste fino não apenas produz melhores resultados de classificação, mas também leva a uma melhor utilização dos parâmetros.', 'es': 'Aunque BERT es ampliamente utilizado por la comunidad de PNL, se sabe poco sobre su funcionamiento interno. Se han hecho varios intentos para arrojar luz sobre ciertos aspectos del BERT, a menudo con conclusiones contradictorias. Una preocupación muy planteada se centra en los problemas de sobreparametrización y subutilización de BERT. Con este fin, proponemos un enfoque novedoso para ajustar BERT de manera estructurada. Específicamente, nos centramos en la clasificación de texto multietiqueta a gran escala (LMTC), en la que a los documentos se les asignan una o más etiquetas de un gran conjunto predefinido de etiquetas organizadas jerárquicamente. Nuestro enfoque guía las capas BERT específicas para predecir etiquetas a partir de niveles jerárquicos específicos. Experimentando con dos conjuntos de datos de LMTC, demostramos que este enfoque estructurado de ajuste fino no solo produce mejores resultados de clasificación, sino que también conduce a una mejor utilización de los parámetros.', 'ja': 'BERTはNLPコミュニティによって広く使用されていますが、その内面的な機能についてはほとんど知られていません。BERTの特定の側面を明らかにする試みがいくつか行われており、しばしば矛盾する結論が示されている。提起された懸念は、BERTの過剰パラメータ化と利用不足の問題に焦点を当てています。この目的のために、私たちは構造化された方法でBERTを微調整するための新しいアプローチを提案します。具体的には、階層的に整理されたラベルの大規模な事前定義されたセットから1つ以上のラベルをドキュメントに割り当てる大規模マルチラベルテキスト分類（ LMTC ）に焦点を当てます。当社のアプローチは、特定の階層レベルからラベルを予測するために特定のBERTレイヤーをガイドします。2つのLMTCデータセットを用いて実験すると、この構造化された微調整アプローチは、より良い分類結果をもたらすだけでなく、パラメータ利用率の向上にもつながることが示されます。', 'zh': '虽BERT为NLP社区博用,而运于内知之甚少。 既数试以明BERT,往往有隙。 一者,BERT之过参数化利用率不足也。 为此结构化调BERT新法。 注大多标签文本分 (LMTC),其文档从大预定义分中分一或数标。 吾道特定者 BERT 从特定层次结构级占之。 试之两LMTC数集,见其结构化之微调,不惟可以生善类,亦可以崇参数利用率。', 'ru': 'Хотя BERT широко используется сообществом NLP, мало что известно о его внутренней работе. Был предпринят ряд попыток пролить свет на некоторые аспекты БЕРТА, часто с противоречивыми выводами. Большое беспокойство вызывают вопросы чрезмерной параметризации и недоиспользования БЕРТ. С этой целью мы предлагаем новый подход к точной настройке БЕРТА структурированным образом. В частности, мы сосредоточены на крупномасштабной многоуровневой текстовой классификации (LMTC), где документам присваивается одна или несколько меток из большого предопределенного набора иерархически организованных меток. Наш подход направляет определенные слои BERT для предсказания меток с определенных уровней иерархии. Экспериментируя с двумя наборами данных РКРТ, мы показываем, что этот структурированный подход к доработке не только дает лучшие результаты классификации, но и приводит к лучшему использованию параметров.', 'hi': 'हालांकि BERT व्यापक रूप से NLP समुदाय द्वारा उपयोग किया जाता है, इसके आंतरिक कामकाज के बारे में बहुत कम जाना जाता है। BERT के कुछ पहलुओं पर प्रकाश डालने के लिए कई प्रयास किए गए हैं, अक्सर विरोधाभासी निष्कर्षों के साथ। एक बहुत अधिक उठाई गई चिंता BERT के अति-पैरामीटराइजेशन और कम-उपयोग के मुद्दों पर केंद्रित है। इस अंत के लिए, हम एक संरचित तरीके से BERT को ठीक करने के लिए ओ उपन्यास दृष्टिकोण का प्रस्ताव करते हैं। विशेष रूप से, हम बड़े पैमाने पर मल्टीलेबल टेक्स्ट क्लासिफिकेशन (LMTC) पर ध्यान केंद्रित करते हैं जहां दस्तावेज़ों को पदानुक्रमित रूप से व्यवस्थित लेबल के एक बड़े पूर्वनिर्धारित सेट से एक या अधिक लेबल के साथ असाइन किया जाता है। हमारा दृष्टिकोण विशिष्ट पदानुक्रम स्तरों से लेबल की भविष्यवाणी करने के लिए विशिष्ट BERT परतों का मार्गदर्शन करता है। दो LMTC डेटासेट के साथ प्रयोग करते हुए हम दिखाते हैं कि यह संरचित ठीक-ट्यूनिंग दृष्टिकोण न केवल बेहतर वर्गीकरण परिणाम देता है, बल्कि बेहतर पैरामीटर उपयोग की ओर भी जाता है।', 'ga': 'Cé go n-úsáideann an pobal NLP go forleathan as BERT, is beag atá ar eolas faoina oibreacha istigh. Rinneadh go leor iarrachtaí chun solas a chur ar ghnéithe áirithe de CRET, go minic le conclúidí contrártha. Díríonn imní go leor ar shaincheisteanna ró-pharaiméadair agus tearcúsáide BERT. Chuige sin, molaimid cur chuige nua maidir le mionchoigeartú a dhéanamh ar CRET ar bhealach struchtúrtha. Go sonrach, dírímid ar Aicmiú Téacs Illipéad Mórscála (LMTC) áit a sanntar doiciméid le lipéad amháin nó níos mó ó thacar mór lipéid réamhshainithe atá eagraithe go hordlathach. Treoraíonn ár gcur chuige sraitheanna sonracha CRET chun lipéid a thuar ó leibhéil ordlathais ar leith. Agus sinn ag tástáil le dhá thacar sonraí LMTC léirímid go n-eascraíonn an cur chuige mionchoigeartaithe struchtúrtha seo ní hamháin torthaí aicmithe níos fearr ach go n-eascraíonn úsáid níos fearr paraiméadar freisin.', 'ka': 'მაგრამ ბერტი NLP-ის საზოგადოებაში გამოიყენება, მარტივია იცია საშუალო მუშაობაზე. ბერტის რამდენიმე მოცდილობები იქნებიან, რომ BERT-ის განსაკუთრებული აპექტიკების გადაწყვეტილების გადაწყვეტილებით. ბევრი უფრო მეტი დარწმუნებული დარწმუნება ბერტის გარემო პარამეტრიზაციაზე და გამოიყენებული პრობლემებზე. ამ საკუთარი მიზეზით, ჩვენ პრომენტის პრომენტის მიზეზით BERT-ს სტრუქტურაციული ნაწილად. განსაკუთრებულია, ჩვენ დიდი სკალური მრავალური ტექსტის კლასიფიკაციაში (LMTC) ფონსკურებთ, სადაც დოკუმენტები ერთი ან მეტი ლაბეტებით უფრო განსაკუთრებული იერაქტიურად ორ ჩვენი პროგორმაცია განსაკუთრებული ბერტი სინაცემების გადაწყვეტილება განსაკუთრებული ჰიერაქტიის დონეზე. ორი LMTC მონაცემების ექსპერიმენტებით ჩვენ გამოჩვენებთ, რომ ეს სტრუქტურული კონფიგურაციის პროგრამი არა მხოლოდ უკეთესი კლასიფიკაციის შედეგი, მაგრამ უკეთესი', 'el': 'Αν και το χρησιμοποιείται ευρέως από την κοινότητα, ελάχιστα είναι γνωστά για την εσωτερική λειτουργία του. Έχουν γίνει αρκετές προσπάθειες να ρίξουν φως σε ορισμένες πτυχές του BERT, συχνά με αντιφατικά συμπεράσματα. Μια πολύ έντονη ανησυχία επικεντρώνεται στα ζητήματα υπερπαραμετροποίησης και υποχρησιμοποίησης του BERT. Για το σκοπό αυτό, προτείνουμε μια νέα προσέγγιση για τον συντονισμό του BERT με δομημένο τρόπο. Συγκεκριμένα, εστιάζουμε στην ταξινόμηση κειμένου μεγάλης κλίμακας (όπου τα έγγραφα αντιστοιχούν με μία ή περισσότερες ετικέτες από ένα μεγάλο προκαθορισμένο σύνολο ιεραρχικά οργανωμένων ετικετών. Η προσέγγισή μας καθοδηγεί συγκεκριμένα στρώματα για να προβλέψει ετικέτες από συγκεκριμένα επίπεδα ιεραρχίας. Πειραματιζόμενοι με δύο σύνολα δεδομένων δείχνουν ότι αυτή η δομημένη προσέγγιση συντονισμού όχι μόνο δίνει καλύτερα αποτελέσματα ταξινόμησης αλλά και οδηγεί σε καλύτερη αξιοποίηση παραμέτρων.', 'hu': 'Bár a BERT-t széles körben használják az NLP közösség, keveset tudunk belső működéséről. Több kísérletet tettek a BERT bizonyos vonatkozásaira, gyakran ellentmondó következtetésekkel. A BERT túlzott paraméterezési és alulfelhasználási kérdéseire összpontosít. Ennek érdekében új megközelítést javasolunk a BERT strukturált módon történő finomhangolására. Konkrétan a nagyméretű többcímkés szövegosztályozásra (LMTC) összpontosítunk, ahol a dokumentumok egy vagy több címkével vannak rendelve hierarchikusan szervezett címkék nagyméretű, előre meghatározott sorozatából. Megközelítésünk irányítja a BERT-rétegeket a címkék meghatározott hierarchiai szintekről történő előrejelzéséhez. Két LMTC adatkészlettel való kísérletezés során megmutatjuk, hogy ez a strukturált finomhangoló megközelítés nemcsak jobb osztályozási eredményeket eredményez, hanem jobb paraméterek használatát is eredményez.', 'it': 'Sebbene il BERT sia ampiamente usato dalla comunità NLP, poco si sa circa il suo funzionamento interno. Diversi tentativi sono stati compiuti per far luce su alcuni aspetti del BERT, spesso con conclusioni contraddittorie. Una preoccupazione molto sollevata si concentra sui problemi di sovraparametrizzazione e sottoutilizzo di BERT. A tal fine, proponiamo un nuovo approccio per ottimizzare BERT in modo strutturato. In particolare, ci concentriamo sulla classificazione del testo multietichetta su larga scala (LMTC), dove i documenti vengono assegnati con una o più etichette da un grande set predefinito di etichette organizzate gerarchicamente. Il nostro approccio guida specifici livelli BERT per prevedere le etichette da specifici livelli gerarchici. Sperimentando due set di dati LMTC dimostriamo che questo approccio strutturato di fine-tuning non solo produce risultati di classificazione migliori, ma porta anche a un migliore utilizzo dei parametri.', 'lt': 'Nors NLP bendruomenė plačiai naudoja BERT, mažai žinoma apie jos vidaus veikimą. Buvo keli bandymai išaiškinti tam tikrus BERT aspektus, dažnai pateikus prieštaringas išvadas. Didelį susirūpinimą kelia BERT pernelyg didelis parametrų nustatymas ir nepakankamas naudojimas. Šiuo tikslu siūlome struktūriškai pritaikyti naują BERT metodą. Konkrečiai, daugiausia dėmesio skiriame Didelio masto daugiametei teksto klasifikacijai (LMTC), kurioje dokumentai priskiriami vienai ar kelioms iš didelio iš anksto apibrėžto hierarchiškai organizuotų etikečių rinkinio etiketėms. Mūsų požiūris orientuoja į konkrečius BERT sluoksnius, kad būtų galima numatyti etiketes pagal konkrečius hierarchijos lygius. Eksperimentuojant su dviem LMTC duomenų rinkiniais mes rodome, kad šis struktūrizuotas patobulinimas ne tik duoda geresnių klasifikavimo rezultatų, bet ir lemia geresnį parametrų panaudojimą.', 'kk': 'БЕРТ NLP коммуникасы көп қолданылатын болса да, ішкі жұмыс туралы білмейді. БЕРТ кейбір аспектерінде бірнеше әрекеттер жарықтығын өзгерту үшін көптеген, көптеген соңында қарсы жарықтығы болады. BERT параметрлерінің артық параметрлеріне және қолданудың артық мәселелеріне көп көп қатынас бар. Бұл үшін БЕРТ бағдарламасын құрастыру үшін романдық тәртібін ұсынамыз. Ескерту үлкен Масштабын көп жарлық мәтін классификациясына (LMTC) ауыстырмыз. Бұл жерде бір не бірнеше жарлықтар бір немесе бірнеше жарлықтарды иерархиялық түрленген жарлықтардың үлке Біздің қасиетіміз белгілі иерархиялық деңгейінен белгілерді көрсету үшін BERT қабаттарын бағыттайды. Екі LMTC деректер қорларының тәжірибесімен біз бұл құрылған дұрыс түзету тәсілі тек жақсы классификациялау нәтижесін емес, сондай-ақ жақсы параметрлерді қолдануға болады.', 'mk': 'Иако БЕРТ е широко употребен од страна на заедницата НЛП, малку е познато за нејзиното внатрешно работење. Неколку обиди се направени за објаснување на одредени аспекти на БЕРТ, честопати со контрадиктни заклучоци. Многу зголемена загриженост се фокусира на прашањата на БЕРТ за премногу параметризација и недоволна употреба. За оваа цел, предложуваме нов пристап кон финетизирање на БЕРТ на структурен начин. Specifically, we focus on Large Scale Multilabel Text Classification (LMTC) where documents are assigned with one or more labels from a large predefined set of hierarchically organized labels.  Нашиот пристап води конкретни BERT слоеви да предвидат етикети од специфични хиерархиски нивоа. Експериментирајќи со два податоци на ЛМТЦ покажуваме дека овој структуриран пристап на финетирање не само што дава подобри резултати од класификацијата, туку и води до подобра употреба на параметрите.', 'ms': 'Walaupun BERT digunakan secara luas oleh komuniti NLP, sedikit diketahui tentang kerja dalamnya. Several attempts have been made to shed light on certain aspects of BERT, often with contradicting conclusions.  Kebimbangan yang banyak dibesarkan fokus pada masalah berparameterisasi-berlebihan BERT dan penggunaan-bawah. Untuk tujuan ini, kami cadangkan pendekatan baru untuk memperbaiki BERT secara struktur. Secara khusus, kita fokus pada Klasifikasi Teks Label Berlebihan Skala Besar (LMTC) di mana dokumen ditugaskan dengan satu atau lebih label dari set terdefinisi besar label yang diatur secara hierarkis. pendekatan kami memandu lapisan BERT khusus untuk meramalkan label dari tahap hierarki khusus. Experimenting with two LMTC datasets we show that this structured fine-tuning approach not only yields better classification results but also leads to better parameter utilization.', 'ml': 'എന്\u200dഎല്\u200dപി സമൂഹത്താല്\u200d ബെര്\u200dട്ടി വിശാലമായി ഉപയോഗിക്കുന്നുവെങ്കിലും, അതിന്\u200dറെ ഉള്ളിലെ പണിയെക്കുറിച്ച്  ബെര്\u200dട്ടിയുടെ ചില വിഭാഗങ്ങളില്\u200d വെളിച്ചം പ്രകാശിപ്പിക്കാന്\u200d പല ശ്രമിച്ചിരിക്കുന്നു. പലപ്പോഴും പരിധി വി ബെര്\u200dട്ടിന്\u200dറെ പാര്\u200dമീറ്ററേഷന്\u200dറെയും ഉപയോഗിക്കുന്നതിന്\u200dറെയും പേരില്\u200d ഒരുപാട് ഉയര്\u200dത്തപ്പെട്ട വിഷയത് ഈ അവസാനത്തിനു വേണ്ടി നമ്മള്\u200d നോവലിന്റെ അടുത്തേക്ക് നിര്\u200dദ്ദേശിക്കുന്നത് ഒരു സ്ഥാനമായ രീതിയിലാണ്. പ്രത്യേകിച്ച്, നമ്മള്\u200d വലിയ വലിയ വ്യാപാരത്തിന്റെ പേരില്\u200d ശ്രദ്ധിക്കുന്നു. പലിലേറ്റ് ടെക്സ്റ്റ് ക്ലാസിഷന്\u200d (LMTC) അവിടെ രേഖകള്\u200d ഒരോ കൂടുതല്\u200d ചി നമ്മുടെ അടുത്തേക്ക് പ്രത്യേകിച്ച ബെര്\u200dട്ടി തട്ടുകള്\u200d നേര്\u200dവഴിയിലാക്കുന്നു പ്രത്യേകിച്ച ഹിയറാര്\u200dച് രണ്ടു LMTC ഡാറ്റാസേറ്റുകള്\u200d കൊണ്ട് പരീക്ഷിക്കുന്നത് നമ്മള്\u200d കാണിച്ചുകൊണ്ടിരിക്കുന്നു ഈ സ്ഥിരപ്പെട്ടിരിക്കുന്ന നല്ല മ', 'mt': "Għalkemm il-BERT jintuża b’mod wiesa’ mill-komunità NLP, ftit huwa magħruf dwar il-ħidmiet interni tiegħu. Saru bosta tentattivi biex jitfa' dawl fuq ċerti aspetti tal-BERT, spiss b'konklużjonijiet kontradittivi. Tħassib li tqajjem ħafna jiffoka fuq il-kwistjonijiet ta’ parametrizzazzjoni żejda u ta’ utilizzazzjoni żejda tal-BERT. Għal dan il-għan, nipproponu approċċ ġdid għall-irfinar tal-BERT b’mod strutturat. Speċifikament, niffokaw fuq Klassifikazzjoni tat-Test Multitikketta fuq Skala kbira (LMTC) fejn id-dokumenti jiġu assenjati b’tikketta waħda jew aktar minn sett kbir predefinit ta’ tikketti ġerarkikament organizzati. L-approċċ tagħna jiggwida saffi speċifiċi tal-BERT biex jipprevedu t-tikketti minn livelli ta’ ġerarkija speċifiċi. L-esperimentazzjoni b’żewġ settijiet ta’ dejta tal-LMTC turi li dan l-approċċ strutturat ta’ rfinar mhux biss jagħti riżultati aħjar ta’ klassifikazzjoni iżda jwassal ukoll għal użu aħjar tal-parametri.", 'mn': 'BERT-г NLP-ын нийгэмд ашиглаж байгаа ч дотор ажиллах талаар бага мэддэг. БЕРТ-ын зарим талаар гэрлийг сайжруулахын тулд олон хичээл хийгдсэн бөгөөд ихэвчлэн эсрэг шийдвэрлэлтэй. БЕРТТ-ийн илүү параметрийн болон ашиглах дорой асуудлуудын тухай маш их сэтгэл санаа төвлөрүүлдэг. Энэ төгсгөлд бид БЕРТ-г зохион байгуулах шинэ арга зам зааж байна. Ялангуяа бид том хэмжээний хэмжээний хэмжээний хэмжээний хэмжээний хэмжээний хэмжээний хэмжээний хэмжээний хэмжээний хэмжээний хэмжээний хэмжээний хэмжээний хэлбэрээс (LMTC) баримтуудтай нэг эсвэл оло Бидний арга барилга нь тодорхой хэмжээсүүдийг тодорхойлох БЕРТ давхарыг удирдаж байна. Хоёр LMTC өгөгдлийн хэлбэрээр туршилтын туршилт бид энэ бүтээгдэхүүний сайн тодорхойлолтын арга нь зөвхөн сайн тодорхойлолтын үр дүнг биш, бас сайн параметр ашиглах боломжтой.', 'no': 'Selv om BERT vert breidd brukt av NLP-samfunnet, er lite kjent om innbygge arbeid. Fleire forsøk har blitt gjort til å skygge lys på nokre aspektar av BERT, ofte med motsetjande konklusjonar. Eit mykje høgd bekymring fokuserer på BERT sin overparameterisering og underbruk. I denne slutten foreslår vi om novel tilnærming til å finne BERT på ein strukturert måte. Spesifikke fokuserer vi på stor skalering av fleire merkelapper for tekstklassifikasjon (LMTC) der dokument er tildelt med ein eller fleire merkelapper frå eit stor føredefinert set av hierarkisk organiserte merkelapper. Tilnærminga vårt gjev spesifikke BERT-lag for å foregå merkelappar frå spesifikke hierarkivå. Eksperimentert med to LMTC-datasett viser vi at denne strukturerte finnstillingstilnærminga ikkje berre gjer bedre klassifikasjonslinjeresultat, men også fører til bedre bruk av parametrar.', 'ro': 'Deși BERT este utilizat pe scară largă de comunitatea PNL, se știe puține despre funcționarea sa interioară. Au fost făcute mai multe încercări de a face lumină asupra anumitor aspecte ale BERT, adesea cu concluzii contradictorii. O preocupare mult mai ridicată se concentrează asupra problemelor de supra-parametrizare și subutilizare ale BERT. În acest scop, propunem o abordare nouă pentru reglarea fin a BERT într-o manieră structurată. Mai exact, ne concentrăm pe Clasificarea Text Multilabel la scară largă (LMTC), unde documentele sunt atribuite cu una sau mai multe etichete dintr-un set mare predefinit de etichete organizate ierarhic. Abordarea noastră ghidează straturile BERT specifice pentru a prezice etichetele de la niveluri ierarhice specifice. Experimentând cu două seturi de date LMTC, demonstrăm că această abordare structurată de reglare fină nu numai că oferă rezultate mai bune de clasificare, ci duce și la o mai bună utilizare a parametrilor.', 'pl': 'Chociaż BERT jest szeroko stosowany przez społeczność NLP, niewiele wiadomo o jego wewnętrznym działaniu. Podjęto kilka prób rzucenia światła na pewne aspekty BERT, często z sprzecznymi wnioskami. Dużo podniesionych obaw skupia się na kwestiach nadmiernej parametryzacji i niewykorzystania BERT. W tym celu proponujemy nowatorskie podejście do precyzyjnego dostrojenia BERT w ustrukturyzowany sposób. W szczególności skupiamy się na dużej skali wieloznakowej klasyfikacji tekstu (LMTC), gdzie dokumenty są przypisywane jedną lub więcej etykiet z dużego, predefiniowanego zestawu hierarchicznie uporządkowanych etykiet. Nasze podejście kieruje określonymi warstwami BERT do przewidywania etykiet z określonych poziomów hierarchii. Eksperymentując z dwoma zestawami danych LMTC pokazujemy, że to ustrukturyzowane podejście do dostrajania nie tylko daje lepsze wyniki klasyfikacji, ale także prowadzi do lepszego wykorzystania parametrów.', 'sr': 'Iako BERT široko koristi NLP zajednica, malo je poznato o svojim unutrašnjim radovima. Nekoliko pokušaja je provedeno da se svetlosti na određenim aspektima BERT-a, često sa suprotnim zaključima. Mnogo povećane zabrinutosti fokusiraju se na probleme sa preparameterizacijom BERT-a i nepotrebnim korištenjem. Za ovaj cilj, predlažemo novi pristup dobrom korištenju BERT na strukturiran način. Posebno, fokusiramo se na veliku skalu klasifikaciju multietiketa teksta (LMTC) gde su dokumenti dodeljeni jednim ili više etiketa iz velikog predodređenog set a hijerarhički organizovanih etiketa. Naš pristup vodi određene slojeve BERT-a da predvidimo etikete iz određenih nivoa hijerarhije. Eksperimentovanje sa dve komplete podataka LMTC pokazujemo da ovaj strukturirani pristup fino prilagođenja ne samo pruža bolji rezultat klasifikacije, nego i vodi do boljih korištenja parametara.', 'si': 'BERT ප්\u200dරමාණය NLP සමාජයෙන් පාවිච්චි කරලා තියෙනවා නමුත්, ඒකේ ඇතුළු වැඩේ ගැන පොඩ්ඩක් දන්නවා. BERT වලින් කිසිම ප්\u200dරශ්නයක් ගොඩක් උත්සාහ කරලා තියෙනවා, සමහර විරුද්ධ විරුද්ධ අවස්ථාවක් තියෙනවා. BERT ගේ ප්\u200dරමාණය සහ ප්\u200dරයෝජනය අඩු ප්\u200dරශ්න ප්\u200dරශ්නයක් ගැන බලාපොරොත්තු කරනවා. මේ අවසානයෙන්, අපි නිර්මාණය විදිහට නිර්මාණය කරන්න පුළුවන් නිර්මාණය කරනවා. විශේෂයෙන්, අපි ලොකු විශාල ප්\u200dරමාණය විශේෂණය (LMTC) ගොඩක් ලේබල් වලට බලන්න කියලා, ලේබල් එකක් නැත්තම් වඩා ලේබල් වලින් ලොකු ව අපේ ප්\u200dරවේශනය විශේෂ BERT ස්ථානයක් ප්\u200dරවේශනය කරනවා විශේෂ ස්ථානයෙන් ලේබිල් අනතුරු කරන්න LMTC දත්ත සෙට් දෙකක් සමග පරීක්ෂණය කරන්න අපි පෙන්වන්නේ මේ සංවිධානය සඳහා හොඳ විශේෂණ පත්තිය නෙවෙයි නමුත් හොඳ විශ', 'sv': 'Även om BERT används i stor utsträckning av NLP-gemenskapen, är det inte mycket känt om dess inre funktion. Flera försök har gjorts för att belysa vissa aspekter av BERT, ofta med motstridiga slutsatser. En mycket väckt oro fokuserar på BERT:s överparametrisering och underutnyttjande. Därför föreslår vi ett nytt tillvägagångssätt för att finjustera BERT på ett strukturerat sätt. Specifikt fokuserar vi på Large Scale Multilabel Text Classification (LMTC) där dokument tilldelas med en eller flera etiketter från en stor fördefinierad uppsättning hierarkiskt organiserade etiketter. Vårt tillvägagångssätt vägleder specifika BERT-lager för att förutsäga etiketter från specifika hierarkiska nivåer. Genom att experimentera med två LMTC-datauppsättningar visar vi att denna strukturerade finjusteringsmetod inte bara ger bättre klassificeringsresultat utan också leder till bättre parameterutnyttjande.', 'so': 'In kastoo BERT uu si ballan ah u isticmaalo bulshada NLP, wax yar waxaa la yaqaan shaqada gudaha ah. Jaribo badan waxaa la sameeyay in la fududeeyo dhinacyada qaarkood ee BERT, marar badan waxaa lagu sameeyaa dhamaadyo ka duwan. Carruurta aad u sareeya waxay ku focus yihiin arimaha BERT oo ka badan parameterization iyo arimaha aan isticmaaleyn karin. Taas darteed, waxaynu soo jeedaynaa qaab dhisan o o ku habboon BERT. Si gaar ah, waxaynu ku kalsoonaynaa qiyaastii aad u weyn (LMTC) meesha dukumentiyada looga qeybeeyo hal ama ka badan calaamado laga soo qoray sawir aad u weyn oo hierarchically organized. Dhaqdhaqaalkayagu wuxuu ku hagaa qasnadaha gaar ah ee BERT si aan looga sii sheego calaamadaha laga soo hor mariyo heerarka gaarka ah. Imtixaanka labada LMTC databases waxaan tusaynaa in qaababkan la dhisay ay oo qurxoon ma bixiyo midhihiisa fasaxa oo kaliya, laakiin wuxuu kaloo keenaa isticmaalka parameter oo ka wanaagsan.', 'ta': 'BERT என்றாலும் NLP சமுதாயத்தாலும் விரிவாக பயன்படுத்தப்படுகிறது, அதன் உள்ளே வேலைகளை பற்றி சிறிது தெரியும். பிரெட்டின் சில பகுதிகளில் வெளிச்சமாக்க பல முயற்சிகள் செய்யப்பட்டுள்ளது, பெரும்பாலும் எதிர்பார்த்த முடிவ மிகவும் உயர்ந்த கவலைப்படுத்தல் பெர்ட்டின் அதிக அளபுருவை மற்றும் பயன்படுத்தும் பிரச்சனைகளை கவனம் செலுத் To this end, we propose o novel approach to fine-tune BERT in a structured manner.  குறிப்பிட்டு, நாம் பெரிய அளவு பல்வேறு உரை வகைப்படுத்தல் (LMTC) மீது கவனம் செலுத்துகிறோம். அதில் ஆவணங்கள் ஒரு அல்லது அதற்கு மேற்பட்ட சிட்டைகளுடன் ஒரு பெரி நம்முடைய செயல்பாடு குறிப்பிட்ட பிரெட் அடுக்குகளை நேர்வழி காட்டுகிறது குறிப்பிட்ட அட்டவணையில் இருந இரண்டு LMTC தரவுத்தளங்களுடன் முயற்சி செய்து நாம் காண்பிக்கிறோம் இந்த அமைப்பு நன்றாக குறிப்பிடும் வழிகளை மட்டும் சிறந்த வகுப்பு', 'ur': 'Although BERT is widely used by the NLP community, little is known about its inner workings. بہت سی کوشش کی گئی ہیں BERT کے بعض منطق پر روشنی کرنا، بہت سی موقعیت کے باعث مخالف نتیجے کے ساتھ. بہت زیادہ اچھی نگرانی BERT کی زیادہ پارامتریزی اور کم استعمال کے مسائل پر تمرکز کرتی ہے. اس کے لئے ہم ایک ساختہ طریقہ سے BERT کے لئے نیوی طریقہ کا پیشنهاد کرتے ہیں. خاص طور پر، ہم بڑے اسکیل Multilabel Text Classification (LMTC) پر fokus کریں جہاں ایک یا زیادہ لابل کے ساتھ ایک یا زیادہ لابل مقرر کیے جاتے ہیں ایک بڑے پہلے مقرر کیے ہوئے مجموعہ لیبل کے ساتھ. ہماری تقریبا مخصوص BERT لہروں کی ہدایت کرتا ہے کہ مخصوص سطح سے لہروں کی پیش بینی کریں۔ ہم دو LMTC ڈیٹ سٹ کے ساتھ تجربہ کررہے ہیں کہ یہ ساختہ پاکیزہ تنظیم طریقہ نہیں صرف بہتر کلاسیفوں کا نتیجہ پہنچاتا ہے بلکہ بہتر پارامتر استعمال کی طرف بھی پہنچاتا ہے.', 'vi': 'Dù cho trường Vớ vẩn được sử dụng rộng rãi bởi cộng đồng NLP, nhưng ít người biết về hoạt động trong nó. Đã có nhiều nỗ lực làm sáng tỏ vài khía cạnh của cây trồng chuối, thường bằng những kết luận mâu thuẫn. Một mối quan tâm được đưa ra nhiều về vấn đề vượt nhiệt hạch của BERT và bị thiểu năng s ử dụng. Để đạt được mục đích, chúng tôi đề nghị một phương pháp mới để thúc đẩy giao kèo BERT theo một cách cấu trúc. Cụ thể, chúng tôi tập trung vào mô tả đa nhãn mực lớn (LMTC) nơi tài liệu được chỉ định với một hay nhiều nhãn từ một bộ nhãn được xác định sẵn lớn của các nhãn có tổ chức theo thứ tự. Cách tiếp cận của chúng tôi hướng dẫn các lớp thực phẩm BERT để dự đoán các nhãn từ các cấp độ phân loại cụ thể. Thí nghiệm với hai tập tin LMTC, chúng ta cho thấy phương pháp cấu trúc tinh chỉnh sửa không chỉ đem lại hiệu quả phân loại tốt hơn, mà còn dẫn đến một cách sử dụng tham số tốt hơn.', 'uz': "Agar BERT NLP jamiyati juda katta ishlatilgan bo'lsa, uning ichki ishlari haqida qisqa bilmagan. Ko'pchilik jarayonlar BERT'ning bir necha darajada ko'pchilik qo'shildi, ko'pchilik murakkab natijalari bilan Ko'pchilik foydalanish muammolari BERT'ning boshqa parametrlariga va foydalanish muammolarini foydalanadi. Hozir uchun biz quyidagi bir tashkilotga BERT'ning yaxshi darajaga qarang. @ label Bizning muvaffaqiyatlarimiz foydalanishimiz foydalanilgan BERT qatlamlarini koʻrsatish uchun foydalanadi. Ikki LMTC maʼlumot bazasini tajriba qilish mumkin, bu tuzilgan suhbat usuli faqat yaxshi darajalashtirish natijalarini bajarishi mumkin, balki yaxshi parametrlar foydalanishiga ega bo'ladi.", 'bg': 'Въпреки че BERT е широко използван от общността на НЛП, малко се знае за вътрешното му функциониране. Направени са няколко опита да се хвърли светлина върху някои аспекти на BERT, често с противоречиви заключения. Много повдигната загриженост се фокусира върху проблемите с свръхпараметризацията и недостатъчното използване на BERT. За тази цел предлагаме нов подход за фина настройка на BERT по структуриран начин. По-конкретно, ние се фокусираме върху широкомащабната многоетикетна текстова класификация (където документите са присвоени с един или повече етикети от голям предварително определен набор от йерархично организирани етикети. Нашият подход насочва конкретни слоеве за прогнозиране на етикети от конкретни йерархични нива. Експериментирайки с два набора от данни показва, че този структуриран подход за фино настройване не само дава по-добри резултати за класификация, но и води до по-добро използване на параметрите.', 'da': "Selvom BERT anvendes i vid udstrækning af NLP-samfundet, kendes der ikke meget om dets indre funktion. Der er gjort adskillige forsøg på at kaste lys over visse aspekter af BERT, ofte med modstridende konklusioner. En meget udtrykt bekymring fokuserer på BERT's spørgsmål om overparametrisering og underudnyttelse. Med henblik herpå foreslår vi en ny tilgang til at finjustere BERT på en struktureret måde. Specielt fokuserer vi på Large Scale Multilabel Text Classification (LMTC), hvor dokumenter tildeles med en eller flere etiketter fra et stort foruddefineret sæt hierarkisk organiserede etiketter. Vores tilgang guider specifikke BERT-lag til at forudsige etiketter fra specifikke hierarki niveauer. Ved at eksperimentere med to LMTC datasæt viser vi, at denne strukturerede finjusteringsmetode ikke kun giver bedre klassificeringsresultater, men også fører til bedre parameterudnyttelse.", 'de': 'Obwohl BERT von der NLP-Community weit verbreitet ist, ist wenig über sein Innenleben bekannt. Es wurden mehrere Versuche unternommen, bestimmte Aspekte des BERT zu beleuchten, oft mit widersprüchlichen Schlussfolgerungen. Eine viel geäußerte Besorgnis konzentriert sich auf die Themen Überparametrierung und Unterauslastung von BERT. Zu diesem Zweck schlagen wir einen neuartigen Ansatz zur strukturierten Feinabstimmung von BERT vor. Insbesondere konzentrieren wir uns auf die Large Scale Multilabel Text Classification (LMTC), bei der Dokumente mit einem oder mehreren Labels aus einem großen vordefinierten Satz hierarchisch organisierter Labels zugewiesen werden. Unser Ansatz führt spezifische BERT-Ebenen an, um Etiketten von bestimmten Hierarchieebenen vorherzusagen. Durch Experimente mit zwei LMTC-Datensätzen zeigen wir, dass diese strukturierte Feinabstimmung nicht nur bessere Klassifizierungsergebnisse liefert, sondern auch zu einer besseren Parameterauslastung führt.', 'ko': 'BERT는 NLP 커뮤니티에서 널리 사용되고 있지만 내부 작업 메커니즘에 대해서는 잘 알지 못한다.사람들은 일찍이 여러 차례 버트의 어떤 방면을 천명하려고 시도했지만, 왕왕 서로 모순되는 결론을 얻는다.주목받는 문제는 버트의 과도한 매개 변수화와 이용 부족에 집중되었다.이를 위해 BERT를 구조적으로 미세 조정하는 새로운 방법을 제시했습니다.구체적으로 말하자면, 우리가 주목하는 것은 대규모 다중 라벨 텍스트 분류 (LMTC) 이다. LMTC에서 문서는 미리 정의된 층별 조직 라벨의 라벨을 하나 이상 분배받는다.우리의 방법은 특정 층이 특정 차원에서 라벨을 예측하도록 지도한다.두 개의 LMTC 데이터 집합에 대한 실험을 통해 우리는 이런 구조화된 마이크로스피커 방법은 더욱 좋은 분류 결과를 얻을 수 있을 뿐만 아니라 매개 변수를 더욱 잘 이용할 수 있다는 것을 발견하였다.', 'fa': 'اگرچه BERT توسط جامعه NLP وسیع استفاده می\u200cشود، کمی در مورد کارهای داخلی آن آگاه می\u200cشود. چندتا تلاش برای تغییر روشنی بر روی بعضی منطقه های BERT انجام شده است، اغلب با نتیجه\u200cهای مخالف. نگرانی بسیار بزرگ روی مسائل زیادی پارامتریزی و کمی استفاده از BERT تمرکز می\u200cکند. برای این قسمت، ما پیشنهاد می\u200cکنیم که به طریق ساخته\u200cشده روش نویسی برای برت تنظیم کنیم. به طور خاصی، ما روی کلاس\u200cسازی متن Multilabel Multilabel (LMTC) مقیاس بزرگ تمرکز می\u200cکنیم جایی که مدارک با یک یا بیشتر برچسب\u200cهای بزرگ از مجموعهٔ پیش\u200cتعریف شده از برچسب\u200cهای سازمان\u200cشده\u200cاند. روش ما لایه\u200cهای BERT خاص را هدایت می\u200cکند تا لایه\u200cهای مخصوص را از سطح قانونی خاص پیش بینی کنند. تجربه با دو مجموعه داده\u200cهای LMTC نشان می\u200cدهیم که این روش ساخته\u200cشده اصلاح\u200cسازی نه تنها نتیجه\u200cهای مختصات بهتر می\u200cدهد بلکه به استفاده از پارامتر بهتر می\u200cرسد.', 'hr': 'Iako se BERT široko koristi zajednica NLP-a, malo se zna o svojim unutrašnjim radovima. Nekoliko pokušaja je provedeno da se svjetlo na određenim aspektima BERT-a, često s suprotnim zaključima. Mnogo povećana zabrinutost usredotočila se na probleme s preparameterizacijom BERT-a i nepotrebnim korištenjem. Za taj cilj, predlažemo novi pristup dobroj mjeri BERT na strukturovan način. Posebno, fokusiramo se na veliku skalu klasifikaciju multietiketa teksta (LMTC) gdje su dokumenti dodijeljeni jednim ili više etiketa iz velikog predodređenog set a hierarhički organiziranih etiketa. Naš pristup vodi određene slojeve BERT-a kako bi predvidjeli etikete iz određene nivoe hierarkije. Eksperimentiranje s dvije podatke LMTC pokazujemo da ovaj strukturirani pristup fino prilagođavanja ne samo daje bolji rezultati klasifikacije, već i vodi do boljih korištenja parametara.', 'nl': "Hoewel BERT veel wordt gebruikt door de NLP gemeenschap, is er weinig bekend over zijn innerlijke werking. Er zijn verscheidene pogingen ondernomen om licht te werpen op bepaalde aspecten van BERT, vaak met tegenstrijdige conclusies. Een veel geuite bezorgdheid richt zich op BERT's over-parametrisering en onderbenutting kwesties. Hiertoe stellen we een nieuwe aanpak voor om BERT gestructureerd af te stemmen. Specifiek richten we ons op Large Scale Multilabel Text Classification (LMTC), waarbij documenten worden toegewezen aan één of meer labels uit een grote vooraf gedefinieerde set hiërarchisch georganiseerde labels. Onze aanpak begeleidt specifieke BERT-lagen bij het voorspellen van labels vanuit specifieke hiërarchieniveaus. Door te experimenteren met twee LMTC datasets laten we zien dat deze gestructureerde fine-tuning aanpak niet alleen betere classificatieresultaten oplevert, maar ook leidt tot een beter gebruik van parameters.", 'sw': 'Although BERT is widely used by the NLP community, little is known about its inner workings.  Jaribio kadhaa zimetokea mwangaza juu ya baadhi ya vipengele vya BERT, mara nyingi kwa makubaliano yanayopingana. Masuala mengi yanayoongezeka yanajikita kwenye masuala ya upasuaji na matumizi ya BERT. Kwa mwisho huu, tunapendekeza njia ya riwaya ya kufikia vizuri vya BERT kwa njia ya ujenzi. Kwa ujumla, tunajikita kwenye Ukumbusho mkubwa wa Mitandao ya Kimataifa (LMTC) ambapo nyaraka zinawekwa kwa alama moja au zaidi kutoka kwenye seti kubwa zilizochapishwa vizuri. Vyombo vyetu vinaongoza vipande maalum vya BERT kutabiri maabara kutoka kwenye ngazi maalum za ubunifu. Kujaribu kwa takwimu mbili za LMTC tunaonyesha kuwa mbinu hii ya mafunzo mazuri si tu kwamba inatoa matokeo bora ya usambazaji bali pia yanapelekea matumizi bora ya kipimo cha kipimo.', 'id': "Meskipun BERT sangat digunakan oleh komunitas NLP, sedikit dikenal tentang kerja dalamnya. Several attempts have been made to shed light on certain aspects of BERT, often with contradicting conclusions.  Sebuah kebimbangan yang besar fokus pada BERT's over-parameterization dan masalah under utilization. Untuk tujuan ini, kami mengusulkan pendekatan baru untuk memperbaiki BERT dengan cara yang struktur. Secara spesifik, kita fokus pada Klasifikasi Teks Multilabel Skala Besar (LMTC) dimana dokumen ditugaskan dengan satu atau lebih label dari set besar predefinisi label yang terorganisir secara hierarkis. Our approach guides specific BERT layers to predict labels from specific hierarchy levels.  Eksperimen dengan dua set data LMTC kami menunjukkan bahwa pendekatan strukturasi fine-tuning ini tidak hanya memberikan hasil klasifikasi yang lebih baik tetapi juga mengarah ke penggunaan parameter yang lebih baik.", 'am': "ምንም እንኳን BERT ከNLP ማኅበረሰብ የተሰፋበ ቢሆንም እንኳን የውስጠኛው ሥራ ትንሽ ታወቀ፡፡ ብኤርቴን በተቃወመ ውይይት በተቃወመ ውይይቶች ላይ ብዙዎች ሞከረች፡፡ A much raised concern focuses on BERT's over-parameterization and under-utilization issues.  ለዚህ ምክንያት፣ የመረጃ መቀናቀል በጥሩ BERT ለመቀበል እናስባለን፡፡ በተለየ ጊዜ በብዙ መለኪያ የጽሑፍ መክፈቻ (LMTC) where documents with one or more labels from a large set of hierarchically organized labels are assigned. የደረጃችን መግለጫ የBERT ደረጃዎች በተለያዩ ደረጃዎች ላይ ምልክቶችን ለመቀጠል ይመራል፡፡ በሁለት LMTC ዳታዎችን በመፈተና እንደምናሳየው ይህ የተመሠረተ ጥያቄ ጥያቄ የተሻለ የግንኙነት ፍጥረቶች ብቻ አይደለም ነገር ግን የተሻለ መጠቀሚያ መጠቀሚያ እንዲያሳየው ነው፡፡", 'sq': 'Megjithëse BERT përdoret gjerësisht nga komuniteti NLP, pak është e njohur për punët e tij të brendshme. Several attempts have been made to shed light on certain aspects of BERT, often with contradicting conclusions.  Një shqetësim shumë i ngritur përqëndrohet në çështjet e mbiparametrizimit dhe nënpërdorimit të BERT. Për këtë qëllim, ne propozojmë një metodë të re për të rregulluar BERT në një mënyrë të strukturuar. Veçanërisht, ne përqëndrohemi në Klasifikimin e Tekstit me Shkallë të Madhe Multilabel (LMTC) ku dokumentet janë caktuar me një apo më shumë etiketa nga një grup të madh të paracaktuar të etiketave të organizuara hierarkikisht. Përqafimi ynë udhëzon nivele specifike BERT për të parashikuar etiketat nga nivelet specifike të hierarkisë. Experimenting with two LMTC datasets we show that this structured fine-tuning approach not only yields better classification results but also leads to better parameter utilization.', 'af': "Alhoewel BERT heeltemal gebruik word deur die NLP gemeenskap, is klein bekend oor sy binneste werke. Verskeie probeers is gemaak om lig op sekere aspekte van BERT te gooi, dikwels met teenstelende conclusies. 'n baie vergroot bekommerding fokus op BERT se oor-parameterisasie en onder-gebruikersake. Op hierdie einde, voorstel ons om 'n roman toegang tot fin-tune BERT op 'n struktureerde manier. Spesifieke, ons fokus op Groot Skaleer Veelheid etiket Teks Klassifikasie (LMTC) waar dokumente met een of meer etikette toegelaat word van 'n groot vooraf gedefinieerde stel van hierarkies organiseerde etikette. Ons toegang gids spesifieke BERT laag om etikette te voorskou van spesifieke hierarkie vlakke. Eksperimenteering met twee LMTC datastelle wys ons dat hierdie struktureerde fyn-tuning toegang nie net beter klasifikasie resultate gee nie, maar ook lei na beter parameter gebruik.", 'az': "BERT NLP toplumunun vasitəsilə istifadə edildiyi halda, iç işləri barəsində az bilər. BERT'nin bəzi bölümlərində işıq çəkmək üçün bir neçə nəticə edilmişdir, çox zaman müqayisədə olan sonuçlar ilə. BERT'nin aşırı parameterizliklərinə və istifadə edilməsi məqsədilə çox yüksək endiriləndir. Bu səbəbdə, BERT'u düzgün düzəltməyə yeni tərzim təklif edirik. Biz böyük ölçüdə çoxlu etiketli Metin Sınıflamasına (LMTC) odaqlanırıq ki, bir ya da çoxlu etiketlərlə hiyerarşik müəyyən edilmiş böyük etiketlərdən birini və çoxlu etiketlərlə təyin edilir. Yaxınlığımız müəyyən hiyerarhiyat səviyyələrindən etiketləri təmin etmək üçün BERT katlarını doğru yola yönəldir. İki LMTC veri quruluğu ilə təcrübə edirik ki, bu strukturlandırılmış yaxşı təcrübə metodları yalnız daha yaxşı klasifikasiya sonuçlarını verir, lakin daha yaxşı parametru istifadəsinə yol verir.", 'hy': 'Չնայած ԲԵՌՏը լայնորեն օգտագործվում է ՆԼՊ համայնքի կողմից, շատ քիչ է հայտնի իր ներքին աշխատանքի մասին: Բազմաթիվ փորձեր են արվել լուսավորելու BER-ի որոշ ասպեկտները, հաճախ հակառակ եզրակացություններով: Շատ բարձրացված մտահոգվածությունը կենտրոնանում է BER-ի չափազանց պարամետրիզացիայի և չափազանց քիչ օգտագործման խնդիրների վրա: Այս նպատակով, մենք առաջարկում ենք նոր մոտեցում կառուցվածքավորված կերպով բարելավելու BER-ին: Մասնավորապես, մենք կենտրոնանում ենք մեծ մակարդակի բազմապիտակ տեքստի դասակարգում (LMTC), որտեղ փաստաթղթերը տրված են մեկ կամ ավելի պիտակներով հիերարխիկ կազմակերպված պիտակների մեծ շարքից: Մեր մոտեցումը ուղղորդում է որոշակի BER շերտերը, որպեսզի կանխատեսենք կոնկրետ հիերարխիայի մակարդակները: Երկու LMTC տվյալների համակարգերով փորձարկումները ցույց են տալիս, որ այս կառուցվածքավորված բարելավման մոտեցումը ոչ միայն ավելի լավ դասակարգման արդյունքներ է բերում, այլ նաև ավելի լավ օգտագործում է պարամետրերը:', 'tr': "BERT NLP jemgyýeti tarapyndan köplenç ulanýan bolsa-da, daşarydaky işlemler barada birnäçe tanalýar. BERT'iň birnäçe sahypalarynda ýagtylygy çykarmak üçin birnäçe synanyş edildi, köplenç näme netijä garşy bolan çözümler bilen. BERT'yň aşa parameteriýasynda we ulanylaryň aşa meselelerine köp endişelenýär. Şu üçin, BERT'i düzgün şeklinde düzenlemek üçin roman yaklaşmagyny teklip edýäris. Adatça, biz Ullakan Mykdary Metin Sözlemine (LMTC) fokus edýäris we ýene bir ýada köp etitler bilen iýerarhiýan düzenlenen etitlerden berilýar. Biziň ýaryşymyz häzirki iýerarhiýanyň derejesinden etitleri önlemek üçin BERT katlary gurultaýar. LMTC veri setirleri bilen örenmek üçin bu düzgün ajaýyp taýýarlama metodumyň diňe gowy klasifikasyon netijesini däldir ýöne gowy parameterler ullanyşyna ýok edip biler.", 'bn': 'যদিও এনএলপি সম্প্রদায় বিবের্ট ব্যাপক ব্যবহার করে, তবুও তাদের ভেতরের কাজ সম্পর্কে অল্প পরিচিত। বেরেটির কিছু বিষয়ে বেশ কয়েকটি প্রচেষ্টা প্রকাশ করা হয়েছে, প্রায়শই বিতর্কিত সমাপ্তি নিয়ে। বার্টির প্যারামিটারেশন এবং ব্যবহারের নীচে ব্যবহারের ব্যাপারে একটি বেশী উদ্বেগ নিয়ে মনোযোগ দিয়েছে। এই শেষ পর্যন্ত আমরা ভালোভাবে ভালো ভাবে বেরেটের প্রস্তাব করি। বিশেষ করে, আমরা বিশাল পরিমাণের উপর মনোযোগ দিচ্ছি বহুল লেখা টেক্সট ক্লাসিফেশন (এলএমটিসি) যেখানে ডকুমেন্টগুলো একটি বা বেশী লেবেল দিয়ে বিনিময় করা হয় আমাদের পদক্ষেপ নির্দিষ্ট হিয়ারার্চি স্তর থেকে লেবেলের ভবিষ্যদ্বাণী করার জন্য বিবেরেট টের বিশেষ স্ দুই এলএমটিসি ডাটাসেট দ্বারা পরীক্ষা করা হচ্ছে আমরা দেখাচ্ছি যে এই ভালো সংস্কারের প্রযুক্তি শুধুমাত্র ভালো শ্রেণীবিভাগের ফল', 'ca': "Although BERT is widely used by the NLP community, little is known about its inner workings.  S'han fet molts intents de llumir sobre certs aspectes del BERT, sovint amb conclusions contradits. Una preocupació molt plantejada es centra en els problemes de l'sobreparametrització i l'insuficient ús del BERT. A aquest efecte, proposem un enfocament nou per a millorar el BERT d'una manera estructurada. Concretament, ens centrem en la Classificació Multi-Etiqueta a gran escala (LMTC), on els documents s'asignen amb una o més Etiquetes d'un conjunt de etiquetes jeràrquicament organitzades predefinits. El nostre enfocament guia capes BERT específices per predir etiquetes de nivells jerarquics específics. Experimentant amb dos conjunts de dades LMTC demostram que aquest enfocament estructurat d'ajustament no només produeix millors resultats de classificació, sinó també porta a una millor utilizació de paràmetres.", 'cs': 'Přestože je BERT široce používán komunitou NLP, o jeho vnitřním fungování je známo málo. Bylo učiněno několik pokusů o osvětlení některých aspektů BERT, často s protichůdnými závěry. Velmi vznesené obavy se zaměřují na problémy s nadměrnou parametrizací a nedostatečným využitím BERT. Za tímto účelem navrhujeme nový přístup k jemnému ladění BERT strukturovaným způsobem. Konkrétně se zaměřujeme na Large Scale Multilabel Text Classification (LMTC), kde jsou dokumenty přiřazeny s jedním nebo více štítky z velké předdefinované sady hierarchicky uspořádaných štítků. Náš přístup vede specifické vrstvy BERT k předpovědi štítků z určitých úrovní hierarchie. Experimentováním se dvěma LMTC datovými sadami ukazujeme, že tento strukturovaný přístup jemného ladění přináší nejen lepší výsledky klasifikace, ale také vede k lepšímu využití parametrů.', 'fi': 'Vaikka NLP-yhteisö käyttää BERT:tä laajalti, sen sisäisestä toiminnasta tiedetään vähän. BERT-sopimuksen eräitä näkökohtia on yritetty valaista useaan otteeseen, usein ristiriitaisin päätelmin. Paljon esiin tuotu huoli keskittyy BERTin liialliseen parametrisointiin ja alikäyttöön liittyviin kysymyksiin. Tätä varten ehdotamme uutta lähestymistapaa BERT-järjestelmän hienosäätöön jäsennellyllä tavalla. Tarkemmin sanottuna keskitymme LMTC:hen (Large Scale Multilabel Text Classification), jossa dokumenteille annetaan yksi tai useampi tunniste suuresta ennalta määritellystä hierarkisesti järjestetyistä tunnisteista. Lähestymistapamme ohjaa BERT-tasoja ennustamaan tarroja tietyiltä hierarkian tasoilta. Kokeilemalla kahta LMTC-aineistoa osoitamme, että tämä strukturoitu hienosäätömenetelmä tuottaa parempia luokitustuloksia ja johtaa myös parempaan parametrien hyödyntämiseen.', 'et': 'Kuigi uue tööprogrammi kogukond kasutab BERTi laialdaselt, on selle sisemisest toimimisest vähe teada. BERTi teatavatele aspektidele on tehtud mitmeid katseid valgustada, sageli koos vastuoluliste järeldustega. Palju tõstatatud mure keskendub BERTi üleparameerimise ja alakasutamise küsimustele. Selleks pakume välja uudse lähenemisviisi BERTi struktureeritud häälestamiseks. Täpsemalt keskendume suuremahulisele mitmemärgiselisele tekstiklassifikatsioonile (LMTC), kus dokumentidele määratakse üks või mitu silti suurest eelmääratud hierarhiliselt organiseeritud siltide komplektist. Meie lähenemisviis juhendab konkreetseid BERT kihte, et ennustada märgiseid konkreetsetelt hierarhiatasemetelt. Kahe LMTC andmekogumiga katsetades näitame, et see struktureeritud peenhäälestuse lähenemine mitte ainult ei anna paremaid klassifitseerimistulemusi, vaid toob kaasa ka parema parameetrite kasutamise.', 'bs': 'Iako se BERT široko koristi zajednica NLP-a, malo se zna o svojim unutrašnjim radovima. Napravljeno je nekoliko pokušaja da prosvjetli određene aspekte BERT-a, često sa suprotnim zaključkima. Mnogo povećane zabrinutosti usredsređuju se na probleme preparameterizacije BERT-a i nepotrebne primjene. Za ovaj cilj, predlažemo novi pristup dobroj mjeri BERT na strukturovan način. Posebno, fokusiramo se na veliku klasifikaciju multietiketa teksta (LMTC) gdje su dokumenti dodijeljeni jednim ili više etiketa iz velikog predodređenog set a hijerarhički organiziranih etiketa. Naš pristup vodi određene slojeve BERT-a da predvidimo etikete iz određenih nivoa hijerarhije. Eksperimentiranje sa dvije podatke LMTC pokazujemo da ovaj strukturirani pristup fino prilagođavanja ne samo donosi bolji rezultat klasifikacije, već i vodi do boljih korištenja parametara.', 'jv': 'Nanging, BERT mesthi diuntingi podho komunitas NLP, akeh liyane dadi kapan pangan ning acara sampeyan ingkang ambabaran. text-tool-action Mbok Asekan sing paling nggawe barang nggawe barang BERT iki dadi kapan-perusahaan lan kelangan usual Ngomongke iki, kita supoyo nganggo barêng-barêng kanggo ngilangno BERT nganggo klebu maneh. string" in "context_BAR_stringLink EMAIL OF TRANSLATORS Wang', 'ha': "Ingawa BERT ke yi amfani da ɗabi'a na NLP, sai kaɗan an sani game da aikininsa guda. An sami wasu jarrabo da aka yi haske a kan wasu aspecti na BERT, ko da yawa, da fassarar da taratibu. Bayan muhimmin sha'awa na fokus a kan BERT'ar tsohon-parameterization da masu yin amfani da shi. Ga wannan, Munã goyyar da inuwa zuwa BERT mai kyãwo, a cikin an daidaita. @ label: listbox MataimakinMu na shiryar da zane ƙayyade BERT-zane don ya yi gafakar alama daga daraja masu ƙayyade hiera. Aka jarraba da data biyu na LMTC, ko da za mu nuna cewa, wannan hanyor mai gyarawa na samar-tun, bã zai iya fitar da matsalar tsofarawa kawai, kuma yana ƙara amfani da mafiya amfani da parameter.", 'sk': 'Čeprav skupnost NLP pogosto uporablja BERT, je o njegovem notranjem delovanju malo znano. Več poskusov je bilo, da bi osvetlili nekatere vidike BERT, pogosto z nasprotujočimi sklepi. Zelo izražena zaskrbljenost se osredotoča na vprašanja BERT-ove prekomerne parametrizacije in premalo uporabe. V ta namen predlagamo nov pristop k natančnemu nastavitvi BERT na strukturiran način. Natančneje se osredotočamo na veliko razvrščanje več oznak besedila (LMTC), kjer so dokumenti dodeljeni z eno ali več oznakami iz velikega vnaprej določenega nabora hierarhično organiziranih oznak. Naš pristop vodi določene plasti BERT za predvidevanje oznak iz določenih hierarhijskih ravni. Z eksperimentiranjem z dvema naboroma podatkov LMTC smo pokazali, da ta strukturirani pristop finega uravnavanja ne prinaša le boljših rezultatov klasifikacije, temveč vodi tudi k boljši uporabi parametrov.', 'he': 'למרות שBERT משתמש באופן רחב על ידי קהילת NLP, מעט ידוע על הפעולות הפנימיות שלה. עשו כמה ניסיונות להדליק אור על היבטים מסוימים של BERT, לעתים קרובות עם מסקנות מתנגדות. דאגה גדולה מאוד מתמקדת בנושאים של BERT פרמטריזציה יתר והשימוש נמוך. למטרה זו, אנו מציעים גישה חדשה להתאים את BERT בצורה מבוססת. במיוחד, אנו מתמקדים במסגרת טקסט מסוג גדול עם תוויות רבות (LMTC) שבו מסמכים מוסגרים עם תוויות אחת או יותר ממסגרת גדולה מוגדרת מראש של תוויות מאורגנות הייררכית. הגישה שלנו מונחת שכבות BERT ספציפיות לחזות תוויות מרמות הייררכיה ספציפיות. ניסויים עם שני קבוצות מידע LMTC אנחנו מראים שהגישה המבנית הזאת מתאימה לא רק מובילה תוצאות מסווג טובות יותר, אלא גם מובילה להשתמש בפרמטרים טובים יותר.', 'bo': "BERT སྤྱི་ཚོགས་ཚོགས་གྱིས་NLP སྤྱི་ཚོགས་ཀྱིས་ལག་ལེན་འཐབ་པ་ཡིན་ནའང་དེ་ནང་གི་ལས་ཀ་སྒྲུང་ནང་ལས་ཕལ་ཆེན BERT ཡི་ཟུར་བ་ཤས་གཅིག་གི་ནང་དུ་ཉིད་ཁ་ཤས་གཅིག་ལས་དཔའ་བཅས་མང་པོ་ཞིག་ཡོད། A much raised concern focuses on BERT's over-parameterization and under-utilization issues. མཇུག་མ་དེ་ལྟར། འུ་ཅག་གིས་གསར་གཏོད་ཀྱི་གཟུགས་བརྙན་ཡན་རྒྱས་གཏོང་ཐབས་ལམ་ལ་བཟོ་བཅོས་བྱེད་ཀྱི་ཡོད། Specifically, we focus on Large Scale Multilabel Text Classification (LMTC) where documents are assigned with one or more labels from a large predefined set of hierarchically organized labels. Our approach guides specific BERT layers to predict labels from specific hierarchy levels. Experimenting with two LMTC datasets we show that this structured fine-tuning approach not only yields better classification results but also leads to better parameter utilization."}
{'en': 'On the Discrepancy between  Density Estimation  and Sequence Generation', 'ar': 'حول التناقض بين تقدير الكثافة وتوليد التسلسل', 'es': 'Sobre la discrepancia entre la estimación de la densidad y la generación de secuencias', 'fr': "Sur l'écart entre l'estimation de la densité et la génération de séquences", 'pt': 'Sobre a discrepância entre estimativa de densidade e geração de sequência', 'ja': '密度推定とシーケンス生成の不一致について', 'zh': '其密度估计序生之间异也', 'ru': 'О несоответствии между оценкой плотности и формированием последовательности', 'hi': 'घनत्व अनुमान और अनुक्रम जनरेशन के बीच विसंगति पर', 'ga': 'Ar an Neamhréireacht idir Meastachán Dlúis agus Giniúint Seicheamh', 'el': 'Σχετικά με τη διαφορά μεταξύ εκτίμησης πυκνότητας και δημιουργίας ακολουθίας', 'hu': 'A sűrűségbecslés és a szekvencia generáció közötti különbség', 'ka': 'დისპრეპონციის განსაზღვრება და განსაზღვრების განსაზღვრება', 'it': 'Sulla discrepanza tra stima della densità e generazione di sequenze', 'kk': 'Жылтықтығы мен реттеу арасындағы дискрепциясы', 'lt': 'Dėl tankio vertinimo ir sekos generacijos skirtumo', 'mk': 'За дискрепантноста помеѓу проценката на густината и генерацијата на секвенција', 'ms': 'Pada Kegagalan diantara Perkiraan Densiti dan Jenerasi Sekuensi', 'ml': 'ഡെന്\u200dസിറ്റി എസ്റ്റിമേഷനും സെക്കന്റ് ജനിപ്പിനും തമ്മിലുള്ള ഡിസിക്കെന്\u200dസിയില്\u200d', 'mt': 'Dwar id-Diskrepanza bejn l-Istima tad-Densità u l-Ġenerazzjoni tas-Sekwenza', 'mn': 'Хүмүүсийн дүгнэлт болон дарааллын төлөвлөгөөний хоорондын хувьд', 'no': 'På diskrepansen mellom estimaen av tettleik og sekvensgenerering', 'pl': 'Na temat różnicy między oszacowaniem gęstości a generowaniem sekwencji', 'ro': 'Despre discrepanța dintre estimarea densității și generarea secvențelor', 'sr': 'O diskreciji između procjene gustoće i generacije sekvence', 'si': 'සීමාවත් අනුමාණය සහ අනුමාණය අතර විශාලය සඳහා විශාලය', 'so': 'Ka baaraandegista xisaabinta iyo xisaabinta', 'sv': 'Om skillnaden mellan densitetsuppskattning och sekvensgenerering', 'ta': 'அடர்த்தி கணக்கீடு மற்றும் வரிசை உருவாக்கத்திற்கும் இடையே தீர்ப்பு', 'ur': 'تنگی کا اندازہ اور سکنس کی نسل کے درمیان تفرقہ پر', 'uz': 'On the Discrepancy between Density Estimation and Sequence Generation', 'vi': 'Dựa vào độ phân tán giữa độ Density Esmation and Sequence Production', 'bg': 'Относно разликата между оценката на плътността и генерирането на последователност', 'hr': 'O diskreciji između procjene gustoće i generacije sekvence', 'da': 'Om forskellen mellem tæthedsestimering og sekvensgenerering', 'nl': 'Over het verschil tussen dichtheidsschatting en opeenvolgende generatie', 'de': 'Zur Diskrepanz zwischen Dichteschätzung und Sequenzgeneration', 'id': 'Pada diskrepansi antara Perkiraan Densitas dan Generasi Sekuensi', 'ko': '밀도 추정과 서열 생성의 차이', 'fa': 'در اختلاف بین ارزیابی گستردگی و نسل سطح', 'af': 'Op die diskrepansie tussen Densiteit Estimatie en SewensieGenerasie', 'tr': 'S철첵g체t Ta첵첵arlama we Ta첵첵arlama Suratynda', 'sq': 'Në diskrepancën midis vlerësimit të densitetit dhe gjenerimit të sekuencës', 'sw': 'Kuhusu Ulipuko kati ya Uzazi na Uzalishaji', 'hy': 'On the Discrepancy between Density Estimation and Sequence Generation', 'am': 'ምርጫዎች', 'az': 'S캼x캼nl캼q Hesab캼 v톛 S캼radan M톛xluqat캼 aras캼nda', 'bn': 'ডেন্সিটি গণনা এবং সেকেন্স জেনারেশনের মধ্যে স্বীকার', 'et': 'Tiheduse hindamise ja järjestuse genereerimise vahelise lahknevuse kohta', 'ca': "En la discrepancia entre l'estimació de la densitat i la generació de seqüències", 'bs': 'O diskreciji između procjene gustoće i generacije sekvence', 'cs': 'Rozdíly mezi odhadem hustoty a generováním sekvencí', 'fi': 'Tiheys-arvion ja sekvenssien muodostumisen välinen ero', 'sk': 'O razliki med oceno gostote in ustvarjanjem zaporedja', 'jv': 'Ngawe Perintah Panjenengan langgar Density', 'ha': '@ action', 'he': 'On the Discrepancy between Density Estimation and Sequence Generation', 'bo': 'Density Estimation and Sequence Generation དབར་གྱི་འཕེལ་རྩིས་འཁོར་ལ་དག'}
{'en': 'Many sequence-to-sequence generation tasks, including  machine translation  and  text-to-speech , can be posed as estimating the density of the output y given the input x : p(y|x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y given an input x, and each task has its own downstream metric R that scores a model output by comparing against a set of references y * : R(y, y * | x). While we hope that a  model  that excels in  density estimation  also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. In this paper, by comparing several density estimators on five machine translation tasks, we find that the correlation between rankings of models based on  log-likelihood  and BLEU varies significantly depending on the range of the model families being compared. First,  log-likelihood  is highly correlated with  BLEU  when we consider  models  within the same family (e.g. autoregressive models, or  latent variable models  with the same parameterization of the prior).', 'fr': "De nombreuses tâches de génération de séquence à séquence, y compris la traduction automatique et la synthèse vocale, peuvent être considérées comme une estimation de la densité de la sortie y compte tenu de l'entrée x\xa0: p (y|x). Compte tenu de cette interprétation, il est naturel d'évaluer des modèles séquence à séquence en utilisant le log-vraisemblance conditionnel sur un ensemble de tests. Cependant, l'objectif de la génération séquence à séquence (ou prédiction structurée) est de trouver la meilleure sortie y compte tenu d'une entrée x, et chaque tâche possède sa propre métrique en aval R qui note une sortie de modèle en la comparant à un ensemble de références y*\xa0: R (y, y* | x). Bien que nous espérions qu'un modèle qui excelle dans l'estimation de la densité fonctionne également bien sur la métrique en aval, la corrélation exacte n'a pas été étudiée pour les tâches de génération de séquences. Dans cet article, en comparant plusieurs estimateurs de densité sur cinq tâches de traduction automatique, nous constatons que la corrélation entre les classements des modèles basés sur le log-vraisemblance et l'UEBL varie considérablement en fonction de la gamme des familles de modèles comparées. Tout d'abord, le log-vraisemblance est fortement corrélé avec l'UEBL lorsque l'on considère des modèles appartenant à la même famille (par exemple des modèles autorégressifs ou des modèles à variables latentes avec le même paramétrage que le modèle précédent). Cependant, nous n'observons aucune corrélation entre les classements des modèles de différentes familles\xa0: (1) parmi les modèles à variables latentes non autorégressifs, une distribution préalable flexible est meilleure pour l'estimation de la densité mais donne une qualité de génération inférieure à celle d'un modèle antérieur simple, et (2) des modèles autorégressifs\noffrent les meilleures performances de traduction dans l'ensemble, tandis que les modèles à variables latentes avec un flux de normalisation préalable donnent la probabilité de log la plus élevée parmi tous les ensembles de données. Par conséquent, nous recommandons d'utiliser un simple prior pour le modèle non autorégressif à variable latente lorsqu'une vitesse de génération rapide est souhaitée.", 'ar': 'يمكن طرح العديد من مهام إنشاء التسلسل إلى التسلسل ، بما في ذلك الترجمة الآلية وتحويل النص إلى كلام ، كتقدير لكثافة الإخراج y بالنظر إلى المدخلات x: p (y | x). بالنظر إلى هذا التفسير ، من الطبيعي تقييم نماذج التسلسل إلى التسلسل باستخدام احتمالية السجل الشرطي في مجموعة اختبار. ومع ذلك ، فإن الهدف من إنشاء التسلسل إلى التسلسل (أو التنبؤ المنظم) هو العثور على أفضل ناتج y مع إعطاء إدخال x ، ولكل مهمة مقياسها المتلقي الخاص بها R الذي يسجل مخرجات النموذج من خلال المقارنة مع مجموعة من المراجع y *: ص (ص ، ص * | س). بينما نأمل أن يعمل النموذج الذي يتفوق في تقدير الكثافة أيضًا بشكل جيد على مقياس المصب ، لم تتم دراسة الارتباط الدقيق لمهام إنشاء التسلسل. في هذا البحث ، من خلال مقارنة العديد من مقدرات الكثافة في خمس مهام ترجمة آلية ، وجدنا أن الارتباط بين تصنيفات النماذج بناءً على احتمالية السجل و BLEU يختلف اختلافًا كبيرًا اعتمادًا على نطاق عائلات النماذج التي تتم مقارنتها. أولاً ، ترتبط احتمالية السجل ارتباطًا وثيقًا بـ BLEU عندما نفكر في النماذج داخل نفس العائلة (على سبيل المثال ، نماذج الانحدار التلقائي ، أو النماذج المتغيرة الكامنة مع نفس المعلمات السابقة). ومع ذلك ، لم نلاحظ أي ارتباط بين تصنيفات النماذج عبر عائلات مختلفة: (1) بين النماذج المتغيرة الكامنة غير الانحدارية ، يكون التوزيع المسبق المرن أفضل في تقدير الكثافة ولكنه يعطي جودة توليد أسوأ من النماذج السابقة البسيطة و (2) الانحدار التلقائي\nتقدم أفضل أداء للترجمة بشكل عام ، في حين أن النماذج المتغيرة الكامنة ذات التدفق الطبيعي السابق تعطي أعلى احتمالية لتسجيل الدخول في جميع مجموعات البيانات. لذلك ، نوصي باستخدام طريقة مسبقة بسيطة للنموذج الكامن المتغير غير الانحدار التلقائي عندما تكون سرعة التوليد السريع مطلوبة.', 'ja': '機械翻訳及びテキスト読み上げを含む多くのシーケンス間生成タスクは、入力ｘ ： ｐ （ ｙ ｜ ｘ ）が与えられたときの出力ｙの密度を推定するものとして提示され得る。 この解釈を考えると、試験セット上の条件付き対数尤度を使用してシーケンス間モデルを評価するのは当然である。 しかしながら、シーケンス間生成（または構造化予測）の目標は、入力xを与えられた最良の出力yを見つけることであり、各タスクは、参照y *: R (y, y *| x)のセットと比較することによってモデル出力をスコア付けする独自の下流メトリックRを有する。 密度推定に優れたモデルが下流のメトリックでも優れたパフォーマンスを発揮することを願っていますが、シーケンス生成タスクについては正確な相関関係は研究されていません。 本論文では， 5つの機械翻訳タスクのいくつかの密度推定子を比較することで，比較対象のモデルファミリーの範囲によって，対数尤度に基づくモデルのランキングとBLEUの相関関係が大きく異なることがわかる． まず、同じファミリー内のモデル（例えば、自己回帰モデル、または先行モデルと同じパラメータを持つ潜在的な変数モデル）を考えると、対数尤度はBLEUと高度に相関します。 しかし、異なるファミリーにわたるモデルのランキング間の相関は観察されません。（ 1 ）非自己回帰的潜在変数モデルのうち、柔軟な事前分布は密度推定に優れていますが、単純な事前分布よりも生成品質が悪く、（ 2 ）自己回帰モデル\nは全体的に最高の翻訳パフォーマンスを提供し、正規化フローを持つ潜在的な変数モデルは、すべてのデータセットにわたって最大のホールドアウトログ可能性を提供します。したがって、高速生成が望ましい場合には、潜在変数非自動回帰モデルに単純な事前処理を使用することをお勧めします。', 'pt': 'Muitas tarefas de geração de sequência a sequência, incluindo tradução automática e conversão de texto em fala, podem ser colocadas como estimativa da densidade da saída y dada a entrada x: p(y|x). Dada esta interpretação, é natural avaliar modelos de sequência a sequência usando log-verossimilhança condicional em um conjunto de teste. No entanto, o objetivo da geração de sequência a sequência (ou previsão estruturada) é encontrar a melhor saída y dada uma entrada x, e cada tarefa tem sua própria métrica de downstream R que pontua uma saída do modelo comparando com um conjunto de referências y *: R(y, y* | x). Embora esperemos que um modelo que se destaque na estimativa de densidade também tenha um bom desempenho na métrica a jusante, a correlação exata não foi estudada para tarefas de geração de sequência. Neste artigo, comparando vários estimadores de densidade em cinco tarefas de tradução automática, descobrimos que a correlação entre os rankings de modelos baseados em log-likelihood e BLEU varia significativamente dependendo do intervalo das famílias de modelos que estão sendo comparadas. Primeiro, a probabilidade de log é altamente correlacionada com o BLEU quando consideramos modelos dentro da mesma família (por exemplo, modelos autorregressivos ou modelos de variáveis latentes com a mesma parametrização do anterior). No entanto, não observamos correlação entre classificações de modelos em diferentes famílias: (1) entre modelos de variáveis latentes não autorregressivas, uma distribuição a priori flexível é melhor na estimativa de densidade, mas dá pior qualidade de geração do que uma a priori simples e (2) modelos autorregressivos\noferecem o melhor desempenho geral de tradução, enquanto os modelos de variáveis latentes com um fluxo de normalização anterior fornecem a maior probabilidade de log em todos os conjuntos de dados. Portanto, recomendamos o uso de uma prévia simples para o modelo não autorregressivo de variável latente quando a velocidade de geração rápida for desejada.', 'es': 'Muchas tareas de generación de secuencia a secuencia, incluida la traducción automática y la conversión de texto a voz, se pueden plantear como una estimación de la densidad de la salida y dada la entrada x: p (y|x). Dada esta interpretación, es natural evaluar los modelos de secuencia a secuencia utilizando log-verosimilitud condicional en un conjunto de pruebas. Sin embargo, el objetivo de la generación de secuencia a secuencia (o predicción estructurada) es encontrar la mejor salida y dada una entrada x, y cada tarea tiene su propia métrica R descendente que puntúa una salida del modelo comparándola con un conjunto de referencias y*: R (y, y* | x). Si bien esperamos que un modelo que sobresalga en la estimación de la densidad también funcione bien en la métrica descendente, no se ha estudiado la correlación exacta para las tareas de generación de secuencias. En este artículo, al comparar varios estimadores de densidad en cinco tareas de traducción automática, encontramos que la correlación entre las clasificaciones de los modelos basadas en la probabilidad logarítmica y BLEU varía significativamente según el rango de las familias de modelos que se comparan. En primer lugar, la probabilidad logarítmica está altamente correlacionada con BLEU cuando consideramos modelos dentro de la misma familia (por ejemplo, modelos autorregresivos o modelos de variables latentes con la misma parametrización del anterior). Sin embargo, no observamos correlación entre las clasificaciones de los modelos de diferentes familias: (1) entre los modelos de variables latentes no autorregresivas, una distribución previa flexible es mejor en la estimación de la densidad, pero da peor calidad de generación que un modelo previo simple, y (2) los modelos autorregresivos\nofrecen el mejor rendimiento de traducción en general, mientras que los modelos de variables latentes con un flujo de normalización anterior proporcionan la probabilidad de registro más alta en todos los conjuntos de datos. Por lo tanto, recomendamos utilizar un prior simple para el modelo no autorregresivo de variables latentes cuando se desea una velocidad de generación rápida.', 'zh': '诸序至序成务,机器翻译文本至语音转换,可为给定输x:p(y|x)度输y密度。 鉴于此说,试集上用之对数似然以质序自然。 然序至(结构化测)者,于给定输 x 得最佳输出 y,而每务有下流指标 R,当指标 R 与一朋引 y*较之对模型输评分:R(y, y* | x)。 虽欲形于密度度表现出色形于下流指标亦见善于上,而于序成之任,未究其相关性。 夫文者,校五机器翻译之密度估计器,见对数似然、BLEU之间相关性显变化,决于系列也。 先虑一家之(,自归于同参数化者在变量),对数似然与BLEU高相关。 然则观其系列列名之间无相关性:(1)在非自归变量中,灵先验布在密度估计,而愈于约先验有更差者,与(2)自反\n体上供至善,而有归一化流先验者变量形于所有之数,以至高者存对数似然。 故当速成速度,请于潜变量非自归模用简先验。', 'hi': 'मशीन अनुवाद और टेक्स्ट-टू-स्पीच सहित कई अनुक्रम-से-अनुक्रम पीढ़ी के कार्यों को इनपुट x: p(y|x) को देखते हुए आउटपुट y के घनत्व का अनुमान लगाने के रूप में प्रस्तुत किया जा सकता है। इस व्याख्या को देखते हुए, परीक्षण सेट पर सशर्त लॉग-संभावना का उपयोग करके अनुक्रम-से-अनुक्रम मॉडल का मूल्यांकन करना स्वाभाविक है। हालांकि, अनुक्रम-से-अनुक्रम पीढ़ी (या संरचित भविष्यवाणी) का लक्ष्य एक इनपुट एक्स को देखते हुए सबसे अच्छा आउटपुट वाई ढूंढना है, और प्रत्येक कार्य का अपना डाउनस्ट्रीम मीट्रिक आर होता है जो संदर्भों के एक सेट के खिलाफ तुलना करके एक मॉडल आउटपुट स्कोर करता है y *: R(y, y* | x)। जबकि हम आशा करते हैं कि घनत्व अनुमान में उत्कृष्टता प्राप्त करने वाला एक मॉडल डाउनस्ट्रीम मीट्रिक पर भी अच्छा प्रदर्शन करता है, अनुक्रम पीढ़ी के कार्यों के लिए सटीक सहसंबंध का अध्ययन नहीं किया गया है। इस पेपर में, पांच मशीन अनुवाद कार्यों पर कई घनत्व अनुमानकों की तुलना करके, हम पाते हैं कि लॉग-संभावना और BLEU के आधार पर मॉडल की रैंकिंग के बीच सहसंबंध तुलना किए जा रहे मॉडल परिवारों की सीमा के आधार पर काफी भिन्न होता है। सबसे पहले, लॉग-संभावना BLEU के साथ अत्यधिक सहसंबद्ध है जब हम एक ही परिवार के भीतर मॉडल पर विचार करते हैं (उदाहरण के लिए autoregressive मॉडल, या अव्यक्त चर मॉडल पूर्व के एक ही पैरामीटराइजेशन के साथ)। हालांकि, हम विभिन्न परिवारों में मॉडल की रैंकिंग के बीच कोई संबंध नहीं देखते हैं: (1) गैर-ऑटोरिग्रेसिव अव्यक्त चर मॉडल के बीच, एक लचीला पूर्व वितरण घनत्व अनुमान पर बेहतर है, लेकिन एक साधारण पूर्व की तुलना में बदतर पीढ़ी की गुणवत्ता देता है, और (2) autoregressive मॉडल\nसमग्र रूप से सबसे अच्छा अनुवाद प्रदर्शन प्रदान करते हैं, जबकि एक सामान्यीकृत प्रवाह के साथ अव्यक्त चर मॉडल सभी डेटासेट में उच्चतम आयोजित लॉग-आउट लॉग-संभावना देते हैं। इसलिए, हम अव्यक्त चर गैर-autoregressive मॉडल के लिए एक सरल पूर्व का उपयोग करने की सलाह देते हैं जब तेजी से पीढ़ी की गति वांछित है।', 'ru': 'Многие задачи построения последовательности в последовательности, включая машинный перевод и преобразование текста в речь, могут быть поставлены как оценка плотности выходного сигнала y при вводе x: p(y|x). Учитывая эту интерпретацию, естественно оценивать модели последовательности к последовательности, используя условную логарифмическую правдоподобие на тестовом наборе. Однако цель генерации последовательности в последовательность (или структурированного предсказания) заключается в том, чтобы найти лучший выход y при заданном входе x, и каждая задача имеет свою собственную последующую метрику R, которая оценивает выходную модель путем сравнения с набором ссылок y*: R(y, y* | x). Хотя мы надеемся, что модель, которая превосходит в оценке плотности, также хорошо работает по нисходящей метрике, точная корреляция не была изучена для задач генерации последовательности. В этой работе, сравнивая несколько оценщиков плотности по пяти задачам машинного перевода, мы обнаруживаем, что корреляция между ранжированием моделей на основе логарифмической правдоподобия и BLEU значительно варьируется в зависимости от диапазона сравниваемых семейств моделей. Во-первых, логарифмическая правдоподобие в значительной степени коррелируется с BLEU, когда мы рассматриваем модели в пределах одного семейства (например, авторегрессивные модели или модели скрытых переменных с той же параметризацией предшествующего). Тем не менее, мы не наблюдаем корреляции между ранжированиями моделей для различных семейств: (1) среди неавторегрессивных моделей скрытых переменных гибкое предварительное распределение лучше при оценке плотности, но дает худшее качество генерации, чем простые предыдущие, и (2) авторегрессивные модели\nобеспечивают наилучшую эффективность трансляции в целом, в то время как латентные переменные модели с нормализующим потоком до дают наибольшую выдержку логарифмической правдоподобия во всех наборах данных. Поэтому мы рекомендуем использовать простой приоритет для неавторегрессивной модели скрытых переменных, когда требуется быстрая скорость генерации.', 'ga': 'Is féidir go leor tascanna giniúna seicheamh-go-seicheamh, lena n-áirítear aistriúchán meaisín agus téacs-go-hurlabhra, a chur mar mheastachán ar dhlús an aschuir y nuair a thugtar an t-ionchur x: p(y|x). I bhfianaise an léirmhínithe seo, tá sé nádúrtha samhlacha seicheamh-go-seicheamh a mheas ag baint úsáide as log-dóchúlacht choinníollach ar thacar tástála. Mar sin féin, is é an sprioc a bhaineann le giniúint seicheamh-go-seicheamh (nó tuartha struchtúrtha) ná an t-aschur is fearr y a fháil nuair a thugtar ionchur x, agus tá a mhéadracht iartheachtach R féin ag gach tasc a scórálann aschur samhla trí chomparáid a dhéanamh i gcoinne sraith tagairtí y *: R(y, y* | x). Cé go bhfuil súil againn go n-éireoidh go maith le samhail a sháraíonn i meastachán dlúis ar an méadrach iartheachtacha, níl staidéar déanta ar an gcomhghaolmhaireacht chruinn le haghaidh tascanna giniúna seichimh. Sa pháipéar seo, trí roinnt meastóirí dlúis a chur i gcomparáid le cúig thasc meaisín-aistriúcháin, feicimid go n-athraíonn an comhghaol idir rátálacha samhlacha bunaithe ar chosúlachtaí loga agus BLEU go mór ag brath ar raon na dteaghlach samhlacha a bhfuiltear ag déanamh comparáide orthu. Ar an gcéad dul síos, tá comhghaolmhaireacht mhór idir dóchúlacht logála agus BLEU nuair a bhreithnímid ar shamhlacha laistigh den teaghlach céanna (m.sh. samhlacha uath-aischéimnitheacha, nó samhlacha athróg folaigh a bhfuil an paraiméadarú céanna acu ar an gceann roimhe). Mar sin féin, ní thugaimid faoi deara aon chomhghaol idir rátálacha samhlacha thar theaghlaigh éagsúla: (1) i measc samhlacha athróg folaigh neamh-uathchéimnitheacha, is fearr dáileadh solúbtha roimh ré ag meastachán dlúis ach tugann sé cáilíocht giniúna níos measa ná réamhchéimnitheach simplí, agus (2) samhlacha uath-aischéimnitheacha.\ncuireann siad an fheidhmíocht aistriúcháin is fearr ar fáil ar an iomlán, agus tugann samhlacha athróg folaigh le sreabhadh normalaithe roimhe seo an dóchúlacht logála amach is airde thar gach tacar sonraí. Dá bhrí sin, molaimid úsáid a bhaint as réamhshimplí don mhúnla athróg neamh-uath-chéimnitheach folaigh nuair atá luas giniúna gasta ag teastáil.', 'hu': 'Számos szekvencia-szekvencia generálási feladat, beleértve a gépi fordítást és a szöveg-beszédet, úgy állítható fel, mint a kimenet y sűrűségének becslése x: p(y|x bemenet). Ezt az értelmezést figyelembe véve természetes, hogy egy teszthalmazon feltételes naplóvalószínűséggel értékeljük a szekvencia-szekvencia modelleket. A szekvencia-szekvencia generálás (vagy strukturált előrejelzés) célja azonban az, hogy megtalálja a legjobb y kimenetet egy x bemenettel, és minden feladatnak megvan a saját downstream metrika R, amely a modell kimenetét egy y*: R(y, y* | x referenciasorozattal összehasonlítva értékeli. Bár reméljük, hogy a sűrűségbecslésben kiváló modell a downstream metrikán is jól teljesít, a pontos korrelációt nem tanulmányozták a sorozatgyártási feladatok esetében. Ebben a tanulmányban több sűrűségbecslőt összehasonlítva öt gépi fordítási feladatra vonatkozóan azt találjuk, hogy a log-valószínűség és a BLEU kategóriák közötti korreláció jelentősen eltérő az összehasonlított modellcsaládok tartományától függően. Először is, a log-valószínűség nagymértékben korrelálódik a BLEU-val, ha ugyanazon családon belüli modelleket vesszük figyelembe (pl. autoregresszív modelleket, vagy látens változó modelleket, amelyek ugyanazon paraméterezéssel rendelkeznek, mint a korábbi). Nem figyelünk azonban összefüggést a modellek különböző családok közötti rangsorolások között: (1) a nem-autoregresszív látens változó modellek között a rugalmas előzetes eloszlás jobb a sűrűség becslésénél, de rosszabb generációs minőséget biztosít, mint egy egyszerű korábbi modellek, és (2) autoregresszív modellek\nÖsszességében a legjobb fordítási teljesítményt nyújtja, míg a normalizáló áramlást megelőző látens változó modellek a legnagyobb tartós naplózási valószínűséget biztosítják az összes adatkészletben. Ezért javasoljuk, hogy a látens változó nem autoregresszív modellhez egy egyszerű előzetet használjon, ha gyors generációs sebességre van szükség.', 'it': "Molte attività di generazione sequenza-sequenza, tra cui traduzione automatica e testo-parola, possono essere poste come stimare la densità dell'output y dato l'input x: p(y|x). Data questa interpretazione, è naturale valutare modelli sequenza-sequenza utilizzando la probabilità di log condizionale su un set di test. Tuttavia, l'obiettivo della generazione sequenza-sequenza (o predizione strutturata) è quello di trovare l'output migliore y dato un input x, e ogni attività ha la sua metrica a valle R che segna un output modello confrontando un insieme di riferimenti y*: R(y, y* | x). Mentre ci auguriamo che un modello che eccelle nella stima della densità funzioni bene anche sulla metrica a valle, l'esatta correlazione non è stata studiata per le attività di generazione di sequenze. In questo articolo, confrontando diversi stimatori di densità su cinque compiti di traduzione automatica, troviamo che la correlazione tra le classifiche dei modelli basate su log-probability e BLEU varia significativamente a seconda della gamma delle famiglie di modelli confrontati. In primo luogo, la probabilità di log è altamente correlata con BLEU quando consideriamo modelli all'interno della stessa famiglia (ad esempio modelli autoregressivi, o modelli variabili latenti con la stessa parametrizzazione del precedente). Tuttavia, non osserviamo alcuna correlazione tra le classifiche dei modelli in diverse famiglie: (1) tra i modelli variabili latenti non autoregressivi, una distribuzione precedente flessibile è migliore alla stima della densità ma dà una qualità di generazione peggiore rispetto a un semplice precedente, e (2) modelli autoregressivi\noffrono le migliori prestazioni di traduzione in generale, mentre i modelli variabili latenti con un flusso normalizzante precedente offrono la più alta probabilità di log-out in tutti i set di dati. Pertanto, si consiglia di utilizzare un semplice precedente per il modello non autoregressivo variabile latente quando si desidera una velocità di generazione veloce.", 'ka': 'მნიშვნელოვანი შემდეგ შემდეგ შემდეგ შემდეგ მოქმედება, რომელიც მაქსინური შემდეგ და ტექსტის შემდეგ სიტყვა, შეიძლება იყოს, როგორც შემდეგ შემდეგ y გადატანა შემდეგ გადატანა x: p(y). ამ ინტერპუქციის შესახებ, შესახებ ტესტის სეტის შესახებ შესახებ შესახებ მოდელების შესახებ შესახებ მოდელების შესახებ. მაგრამ, შემდეგ შემდეგ შემდეგ შემდეგ (ან სტრუქტურული პროგრამების) მიზეზი იქნება, რომ საკეთესო შემდეგ y შემდეგ გადატანა x, და ყოველ დავალება აქვს საკეთესო მეტრიკური R, რომელიც მოდელური გადატანა, რომელიც შემდეგ შე თუმცა ჩვენ გვემედით, რომ მოდელი, რომელიც მცირეობის განსაზღვრებულობაში უფრო კარგი გავაკეთებს მეტრიკში, მარტივი კორელაცია არ მოსწავლია შემდეგების მოქმედებისთვის. ამ დოკუნეში, რამდენიმე მნიშვნელობის განსაზღვრებით ხუთი მანქანის განსაგულისხმებით, ჩვენ აღმოჩნეთ, რომ მოდელის რენექციების რენექციები და BLEU-ის შესაძლებლობაზე მნიშვნელოვანია, რომელიც მოდელის ოჯა პირველი, ჩვენ იგივე ოჯახში მოდელები (მაგალითად, ავტორეგრესიური მოდელები, ან წინასწორი ცვლილებული მოდელები იგივე პარამეტრიზაციით) დავფიქრობთ. მაგრამ, ჩვენ არაფერი მოდელების რენექციების შორის კოლექცია განსხვავებული ოჯახში: (1) არაფერადრეგრესიური ლატენტიური ცვლილების მოდელების შორის, ძალიან წინაფერად წინაფერად განსხვავებული განსხვავება უფრო მეტია, მაგრამ უფ\nყველაზე უკეთესი გაგრძელების გამოყენება, მაგრამ ლატენტი ცვლილების მოდელები, რომლებიც ნორმალიზური გამოყენება წინ უფრო უფრო მეტი გაგრძელებული მონაცემების შესაძ ამიტომ, ჩვენ შევძლებთ გამოყენოთ საუკეთესო წინასწარმოდგენილი არ ავტორეგრესიური მოდელის გამოყენება, როდესაც სწრაფად წინასწარმოდგენილი სიჩქარე', 'el': 'Πολλές εργασίες δημιουργίας αλληλουχίας σε αλληλουχία, συμπεριλαμβανομένης της μηχανικής μετάφρασης και της ομιλίας κειμένου, μπορούν να τεθούν ως εκτίμηση της πυκνότητας της εξόδου δεδομένου της εισόδου x: p(y|x). Δεδομένου αυτής της ερμηνείας, είναι φυσικό να αξιολογούνται μοντέλα ακολουθίας-ακολουθίας χρησιμοποιώντας πιθανή καταγραφή υπό όρους σε ένα σύνολο δοκιμών. Ωστόσο, ο στόχος της δημιουργίας αλληλουχίας σε αλληλουχία (ή δομημένης πρόβλεψης) είναι να βρεθεί η καλύτερη παραγωγή με δεδομένη εισαγωγή x, και κάθε εργασία έχει τη δική της μεταγενέστερη μετρική R που βαθμολογεί μια παραγωγή μοντέλου συγκρίνοντας με ένα σύνολο αναφορών y*: R(y, y*,x). Ενώ ελπίζουμε ότι ένα μοντέλο που υπερέχει στην εκτίμηση πυκνότητας αποδίδει επίσης καλά στην μεταγενέστερη μέτρηση, η ακριβής συσχέτιση δεν έχει μελετηθεί για εργασίες δημιουργίας αλληλουχιών. Σε αυτή την εργασία, συγκρίνοντας διάφορους εκτιμητές πυκνότητας σε πέντε εργασίες μηχανικής μετάφρασης, διαπιστώνουμε ότι η συσχέτιση μεταξύ των ταξινομήσεων των μοντέλων με βάση την πιθανότητα καταγραφής και την πιθανότητα καταγραφής ποικίλλει σημαντικά ανάλογα με το εύρος των οικογενειών μοντέλων που συγκρίνονται. Πρώτον, η πιθανότητα καταγραφής συσχετίζεται ιδιαίτερα με την BLEU όταν εξετάζουμε μοντέλα εντός της ίδιας οικογένειας (π.χ. αυτοανακριτικά μοντέλα, ή λανθάνοντα μεταβλητά μοντέλα με την ίδια παραμετροποίηση του προηγούμενου). Ωστόσο, δεν παρατηρούμε συσχέτιση μεταξύ των ταξινομήσεων των μοντέλων σε διαφορετικές οικογένειες: (1) μεταξύ των μη αυτοανακριτικών λανθάνοντων μεταβλητών μοντέλων, μια ευέλικτη προγενέστερη κατανομή είναι καλύτερη στην εκτίμηση της πυκνότητας αλλά δίνει χειρότερη ποιότητα παραγωγής από ένα απλό προηγούμενο, και (2) αυτοανακριτικά μοντέλα\nπροσφέρουν την καλύτερη απόδοση μετάφρασης συνολικά, ενώ τα λανθάνοντα μεταβλητά μοντέλα με ομαλοποίηση της ροής πριν παρέχουν την υψηλότερη πιθανότητα καταγραφής σε όλα τα σύνολα δεδομένων. Ως εκ τούτου, συστήνουμε τη χρήση ενός απλού προηγούμενου για το λανθάνουσα μεταβλητό μη-αυτοανακριτικό μοντέλο όταν η γρήγορη ταχύτητα παραγωγής είναι επιθυμητή.', 'lt': 'Daugelis iš eilės į eilę sukuriamų užduočių, įskaitant mašininį vertimą ir tekstą į žodį, gali būti laikoma išėjimo y tankio apskaičiavimu, atsižvelgiant į įvestą x: p(y[UNK]x). Atsižvelgiant į šį aiškinimą, natūralu įvertinti sekos modelius iš eilės į seką naudojant sąlyginę log tikimybę bandymų rinkinyje. Tačiau sekos į seką sukūrimo (arba struktūrizuotos prognozės) tikslas – rasti geriausią išėjimą y, nurodant įvestą x, ir kiekviena užduotis turi savo tolesnę metrinę R, kuri vertina modelio išėjimą palygindama su nuorodų rinkiniu y*: R(y, y* \uf0a5 x). Nors tikimės, kad modelis, kuris geriau vertina tankį, taip pat gerai veikia tolesnės grandinės metrinėmis sąlygomis, tiksli koreliacija nebuvo tirta sekos generavimo užduočių atžvilgiu. Šiame dokumente, palygindami kelis tankio vertinimo rodiklius su penkiomis mašinų vertimo užduotimis, nustatome, kad modelių klasifikacijų, pagrįstų log tikimybe, ir BLEU, koreliacija labai skiriasi priklausomai nuo lyginamų modelių šeimų įvairovės. Pirma, log tikimybė yra labai koreliuojama su BLEU, kai atsižvelgiame į modelius toje pačioje šeimoje (pvz., autoregresinius modelius arba latentinius kintamuosius modelius su ta pačia ankstesne parametrizacija). However, we observe no correlation between rankings of models across different families: (1) among non-autoregressive latent variable models, a flexible prior distribution is better at density estimation but gives worse generation quality than a simple prior, and (2) autoregressive models\nsiūlo geriausius vertimo rezultatus apskritai, o latentiniai kintamieji modeliai su normalizuojančiu srautu prieš tai suteikia didžiausią laikomo išėjimo iš visų duomenų rinkinių tikimybę. Todėl rekomenduojame naudoti paprastą prielaidą latentiniam kintamajam ne autoregresiniam modeliui, kai norima greitos kartos greičio.', 'kk': 'Бірнеше рет мен рет жасау тапсырмалары, компьютердің аударуы мен мәтіннен сөйлесу дегенде, y келтірілген y- нің жиілігін бағалау үшін болады. Бұл толыққан сияқты, сынақ жиынында тәуелді журнал мүмкіндігін қолданатын sequence- to- sequence үлгілерін бағалауға тәуелді. Бірақ, реттеу мен реттеу құрылғысының (немесе құрылған алдын- алау) мақсаты - y келтірілген x- нің ең жақсы шығысын табу, және әрбір тапсырманың төменгі метрикалық R- сілтемелеріне сәйкес келтірілген үлгі шығысын есептеп, y*: R( y, y Біз тұтықтығының бағалау үлгісін басқа төменгі метрикалық үлгісінде жақсы жұмыс істеу үшін үміттенеміз, дұрыс корелация реттеу тапсырмалары үшін оқылмайды. Бұл қағазда, бес машинаны аудару тапсырмалардың бірнеше жиілік оқиғаларын салыстырып, журнал мүмкіндігіне негізделген модельдердің жолдарының және BLEU әдістеріне негізделген модельдердің аумағына қатысты. Біріншіден, журнал мүмкіндігі бір отбасындағы үлгілер үлгілерін қарастырып тұрғанда, BLEU- мен көп қатынасыз (мысалы, авторегрессиялық үлгілер, немесе алдыңғы параметрлерінің бірдей параметрлері болса Бірақ біз әртүрлі отбасындағы үлгілердің жолдары арасындағы қатынасыз жоқ: (1) авторегрессиялық қатынасыз үлгілер арасында, алдындағы гибсиялық үлестіріміз тыныштық бағалауында жақсы, бірақ қарапайым алдындағы үлгілерден жақсы сапатт\nБүкіл деректер жиындарында ең жоғары жұмыс істеу мүмкіндігін көрсетеді. Сондықтан, жылдам жасау жылдамдығын қалаған кезде, келесі айнымалылығы авторегрессивні емес үлгі үшін қарапайым алдында қолдануға рекомендіреміз.', 'mk': 'Многу задачи за генерација од секвенца до секвенца, вклучително и машински превод и текст до говор, можат да бидат поставени како проценка на густината на излезот y со влогот x: p( y[UNK]x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set.  Сепак, целта на генерацијата од секвенца до секвенца (или структурирано предвидување) е да се најде најдобриот излез y даден со влог x, и секоја задача има своја метрична Р на потег која постигнува излез од модел споредувајќи се со множина референции y*: R( y, y* \uf0a5 x). И покрај тоа што се надеваме дека моделот кој е одличен во проценката на густината, исто така, ќе функционира добро и на понатамошната метрика, точната корелација не е проучена за задачите на генерација на секвенца. Во овој весник, споредувајќи неколку проценувачи на густина на пет машински преведувачки задачи, откриваме дека корелацијата помеѓу рангирањата на моделите базирани на лог-веројатноста и БЛЕ се разликува значително во зависност од дометот на моделните семејства што се споредуваат. Прво, веројатноста на логот е високо поврзана со БЛЕУ кога разгледуваме модели во истото семејство (на пример авторегресивни модели или лантни модели со исти параметризации од претходното). Сепак, не забележуваме корелација помеѓу рангирањата на моделите во различните семејства: (1) помеѓу неавторегресивните лантни модели, флексибилната претходна дистрибуција е подобра во проценката на густината, но дава полош квалитет на генерацијата отколку едноставен претходен, и\nнудат најдобра резултат на превод вкупно, додека latent variable models with a normalizing flow prior give the highest held-out log-out probability across all datasets. Затоа, препорачуваме да се користи едноставен претход за latent variable non-autoregressive model кога е желба брза генерација брзина.', 'ms': 'Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y|x).  Mengingat interpretasi ini, ia adalah semulajadi untuk menilai model urutan-ke-urutan menggunakan log-kemungkinan syarat pada set ujian. Namun, tujuan generasi jujukan-ke-jujukan (atau ramalan struktur) adalah untuk mencari output terbaik y yang diberikan input x, dan setiap tugas mempunyai R metrik turun sendiri yang mencetak output model dengan membandingkan dengan set rujukan y*: R( y, y *\uf0a5 x). Sementara kita berharap bahawa model yang melebihi dalam penilaian ketepatan juga berjalan dengan baik pada metrik turun, korelasi tepat belum dipelajari untuk tugas generasi urutan. Dalam kertas ini, dengan membandingkan beberapa pengiraan densiti pada lima tugas terjemahan mesin, kami mendapati bahawa korelasi antara rangkaian model berdasarkan kemungkinan log dan BLEU berbeza secara signifikan bergantung pada julat keluarga model yang dibandingkan. Pertama, kemungkinan log sangat berkorelasi dengan BLEU apabila kita mempertimbangkan model dalam keluarga yang sama (cth. model autoregresif, atau model pembolehubah latent dengan parameterisasi yang sama dari sebelumnya). Namun, kita tidak memperhatikan korelasi antara rangkaian model melalui keluarga yang berbeza: (1) diantara model pembolehubah yang tersembunyi bukan-autoregressif, distribusi sebelumnya fleksibel lebih baik pada penilaian densiti tetapi memberikan kualiti generasi yang lebih buruk daripada model sebelumnya sederhana, dan (2) model autoregressif\nmenawarkan prestasi terjemahan terbaik secara keseluruhan, sementara model pembolehubah tersembunyi dengan aliran normalisasi sebelum memberikan kemungkinan log-out tertinggi di seluruh set data. Oleh itu, kami cadangkan menggunakan awal sederhana untuk model pembolehubah tidak-autoregresif yang tersembunyi apabila kelajuan generasi pantas diinginkan.', 'ml': 'മെഷിന്\u200d പരിഭാഷവും വാക്കുകളും ചേര്\u200dന്നിരിക്കുന്ന പ്രവൃത്തികളില്\u200d ഒരുപാട് പ്രവൃത്തികള്\u200dക്ക് എണ്ണിപ്പുട്ട് കൊടുത്തിരിക്കുന്ന ഉള്\u200dപ്പ ഈ വ്യാഖ്യാനം നല്\u200dകിയാല്\u200d, പരീക്ഷണസജ്ജീകരണത്തില്\u200d സെക്കന്\u200dസ് മോഡലുകള്\u200d പരിഗണിക്കുന്നത് ഉപയോഗിച്ച് സ്വാഭാവികമാണ്. എന്നാലും, സെക്കന്\u200dസ് തലമുറയുടെ ലക്ഷ്യം (അല്ലെങ്കില്\u200d നിര്\u200dമ്മിക്കപ്പെട്ട പ്രവചനം) ഒരു ഇന്\u200dപുട്ട് നല്\u200dകപ്പെട്ട ഏറ്റവും നല്ല ഫലം കണ്ടെത്തുന്നതാണ്, ഓരോ ജോലിയും അതിന്\u200dറെ സ്വന്തം താഴ്ന് തൂക്കത്തിന്റെ കണക്കിന് മുകളില്\u200d ഉയര്\u200dന്ന ഒരു മോഡല്\u200d പ്രവര്\u200dത്തിക്കുന്നത് പോലും താഴ്വരയുടെ മെട്രിക്കിലും നല്ല പ്രവര്\u200dത്തിക്കുന്നതാണെ ഈ പത്രത്തില്\u200d, അഞ്ചു മെഷീന്\u200d പരിഭാഷണത്തിന്റെ ജോലികളില്\u200d കുറച്ച് കഠിനമായ ഗണിറ്ററികള്\u200d തുല്യമാക്കുന്നതിനാല്\u200d, ലോഗ്-സാധ്യതകള്\u200d അടിസ്ഥാനമായി മോഡലുകളുടെ മാതൃ ആദ്യം, ലോഗ്- സാധ്യതകള്\u200d ഒരേ കുടുംബത്തിന്റെ ഉള്ളില്\u200d മോഡലുകള്\u200d കാണുമ്പോള്\u200d ബിലൂയുമായി വളരെ ബന്ധപ്പെട്ടിരിക്കുന്നു (ഉദാഹരണത്തിനായി സ്വയമാക എങ്കിലും വ്യത്യസ്ത കുടുംബങ്ങളില്\u200d മാതൃകങ്ങളുടെ മാതൃകങ്ങളുടെ മാതൃകങ്ങള്\u200dക്കിടയില്\u200d നമുക്ക് ബന്ധമൊന്നും കാണാനാവില്ല: (1) സ്വാതന്ത്രികമാക്കുന്ന സാധാരണ മാതൃകങ്\nഎല്ലാ ഡാറ്റാസറ്റുകളിലും ഏറ്റവും മികച്ച വിവരങ്ങളുടെ പ്രവര്\u200dത്തനങ്ങള്\u200d മൊത്തം നല്\u200dകുക, പുതിയ മാറ്റലുകള്\u200d സാധാരണമായി നീങ്ങുന്ന അതുകൊണ്ട്, നമ്മള്\u200d പുതിയ മുന്\u200dപ് ഉപയോഗിക്കുന്നത് സ്വയം നിരീക്ഷിക്കാത്ത മോഡലിന് വേഗത്തില്\u200d തലമുറതലമുറയുടെ', 'mt': 'Ħafna kompiti ta’ ġenerazzjoni minn sekwenza għal sekwenza, inklużi t-traduzzjoni tal-magna u t-test-għad-diskors, jistgħu jiġu ppreżentati bħala stima tad-densità tal-output y mogħtija l-input x: p(y[UNK]x). Minħabba din l-interpretazzjoni, huwa naturali li jiġu evalwati mudelli minn sekwenza għal sekwenza bl-użu tal-probabbiltà kondizzjonali ta’ log fuq sett tat-test. Madankollu, l-għan tal-ġenerazzjoni minn sekwenza għal sekwenza (jew tbassir strutturat) huwa li jinstab l-a ħjar output y mogħti input x, u kull kompitu għandu r-R metriku downstream tiegħu stess li jikkalkula output mudell billi jitqabbel ma’ sett ta’ referenzi y*: R(y, y* \uf0a5 x). Filwaqt li nittamaw li mudell li jkun eċċellenti fl-istima tad-densità jwettaq prestazzjoni tajba wkoll fuq il-metrika downstream, il-korrelazzjoni eżatta ma ġietx studjata għal kompiti ta’ ġenerazzjoni ta’ sekwenzi. F’dan id-dokument, billi tqabblu diversi stimaturi tad-densità fuq ħames kompiti ta’ traduzzjoni bil-magni, isibu li l-korrelazzjoni bejn il-klassifikazzjonijiet tal-mudelli bbażati fuq il-probabbiltà tal-ġurnal u l-BLEU tvarja b’mod sinifikanti skont il-firxa tal-familji tal-mudelli li qed jitqabblu. L-ewwel nett, il-probabbiltà ta’ log hija korrelata ħafna mal-BLEU meta nikkunsidraw mudelli fl-istess familja (e ż. mudelli awtoregressivi, jew mudelli varjabbli moħbija bl-istess parametrizzazzjoni ta’ qabel). Madankollu, ma osservajna l-ebda korrelazzjoni bejn il-klassifikazzjonijiet tal-mudelli bejn familji differenti: (1) fost mudelli varjabbli moħbija mhux awtoregressivi, distribuzzjoni flessibbli minn qabel hija a ħjar fl-istima tad-densità iżda tagħti kwalità agħar tal-ġenerazzjoni minn mudelli sempliċi minn qabel, u (2) awtoregressivi\njoffru l-a ħjar prestazzjoni tat-traduzzjoni b’mod ġenerali, filwaqt li mudelli varjabbli latenti b’fluss normalizzanti minn qabel jagħtu l-ogħla probabbiltà ta’ log-out miżmuma fis-settijiet tad-dejta kollha. Għalhekk, nirrakkomandaw l-użu ta’ qabel sempliċi għall-mudell varjabbli mhux awtoregressiv moħbi meta tkun mixtieqa veloċità ta’ ġenerazzjoni mgħa ġġla.', 'mn': 'Ихэнх дарааллаас дарааллаар үржүүлэх үйл ажиллагаа, машины хөрөнгө оруулалт, текст-т-хэлбэл, y-н орлуулалтын жинтэй байдлыг тооцоолж чадна. Энэ тодорхойлолтоор, дарааллаас дарааллаар дүрслэх загваруудыг шалгалтын хэлбэрээр ашиглаж байх нь байгалийн юм. Гэхдээ дарааллаас дарааллаар үржүүлэх (эсвэл бүтээгдэхүүний таамаглалт) зорилго нь y-г нэвтрүүлэх хамгийн сайн үржүүлэлтийг олох ба ажил бүр өөрийн доорх метрик R байдаг. Энэ нь загварын үржүүлэлтийг y*: R(y, y*.x-тэй харьцуулахад модель үржүүлэлтийг гаргадаг Хэдийгээр бид жинтэй тооцооллын загвар мөн доорх метрик дээр сайн ажилладаг гэдгийг найдаж байна. Яг тохиромжтой холбоотой нь дарааллын үеийн даалгаврыг судалж чадахгүй. Энэ цаасан дээр олон жинтэй тооцоологчийг таван машины орчуулалтын ажил дээр харьцуулахад бид Log-likelihood болон BLEU-ын загварын харьцуулалтын хоорондын хоорондын хоорондын хоорондын хоорондын хоорондын хоорондын хоорондоо маш чухал өөрчлөгдөж байна. Эхлээд, бид ижил гэр бүлийн дотор загваруудыг бодох үед BLEU-тэй маш холбоотой (жишээ нь автоregressive загваруудын загварууд, эсвэл өмнөх нь ижил параметрийг тодорхойлж байгаа соронзон өөрчлөлтийн загварууд). Гэхдээ бид өөр гэр бүлийн загварын хоорондох холбоотой зүйлийг анзаарч байхгүй: (1) авторегрессийн хувьсалын загваруудын хоорондох холбоотой зүйлийг анзаарч байхгүй. Өмнөх хувьсалын хэвлэл нь жинтэй тооцооллоос илүү сайн байдаг гэхдээ энгийн өмнөх үеэс\nХамгийн сайн орчуулагчийн үйл ажиллагааг дэлгэрүүлэхэд, хамгийн өндөр хувьсагчийн загварууд нь бүх өгөгдлийн санд хамгийн өндөр байрлалт гаргах боломжтой байдаг. Тиймээс бид хурдан үеийн хурдыг хүсч буй хурдтай хурдан автоматжуулах бус өөрчлөлтийн хувьд энгийн өмнө хэрэглэхэд энгийн загварыг зөвлөе.', 'no': 'Mange oppgåver for generering av sekvens-til-sekvens, inkludert maskinsomsetjing og tekst-til-tale, kan plasserast som estimerer tettleiken p å utdata y gitt inn x: p(y.x). Gjennomsikt denne tolkinga, er det naturleg å evaluera sequence-to-sequence-modeller ved hjelp av vilkårleg loggsannsynlighet på eit testsett. Målet for å generera sequence-to-sequence (eller strukturerte forhåndsvising) er imidlertid å finna den beste utdata y gitt ein inndata x, og kvar oppgåve har sitt eigne nedstrømte metrisk R som scorer ein modell utdata ved å sammenligna med eit sett referanser y*: R( y, y *. x). Mens vi håper at eit modell som overstyrer tettheitsestisering også utfører godt på nedstrømmemetrikken, er det nøyaktige korrelasjonen ikkje studiert for sekvensgenerasjonsoppgåver. I denne papiret, ved samanlikning av fleire tettheitsestimatorar på fem maskineoversettelsesoppgåver, finn vi at korrelasjonen mellom rekningane av modeller basert på logsannsynligheten og BLEU varierer betydelig avhengig av området av modellefamiliane som vert samanlikna. Først er loggsannsynligheten svært korrelatert med BLEU når vi forstår modeller i samme familie (f.eks. autoregressiv modeller, eller latent variabel modeller med same parameterisering av førre). Men vi observerer ingen korrelasjon mellom rekningar av modeller i ulike familier: (1) blant ikkje-autoregressiv latent variabel modeller, fleksibel fordeling er bedre ved estimerering av tetthet, men gir dørre genereringskvalitet enn ein enkelt førre, og (2) autoregressiv modeller\ntilbyr den beste utviklinga av omsetjinga overalt, mens latente variabel- modeller med ein normalisering av flyt før gjer den høgste likelykken for loggen på alle datasett. Derfor anbefaler vi å bruka ein enkel før for den latente variabelen som ikkje er autoregressiv modellen når rask genereringsfart er ønskt.', 'pl': 'Wiele zadań generowania sekwencji na sekwencję, w tym tłumaczenie maszynowe i tekst na mowę, może być postawionych jako oszacowanie gęstości wyjścia y, biorąc pod uwagę wejście x: p(y|x). Biorąc pod uwagę tę interpretację naturalną jest ocena modeli sekwencji do sekwencji przy użyciu warunkowej prawdopodobieństwa logowania na zestawie testowym. Jednakże celem generowania sekwencji do sekwencji (lub strukturalnej predykcji) jest znalezienie najlepszego wyjścia y przy danym wejściu x, a każde zadanie ma własną dolną metrykę R, która ocenia wyjście modelu poprzez porównanie z zestawem odniesień y*: R(y, y*, x). Chociaż mamy nadzieję, że model, który wyróżnia się w szacowaniu gęstości, również sprawdza się dobrze w metryce dolnej, dokładna korelacja nie została zbadana dla zadań generowania sekwencji. W niniejszym artykule, porównując kilka oszacowaczy gęstości na pięciu zadaniach tłumaczenia maszynowego, stwierdzono, że korelacja pomiędzy rankingami modeli opartymi na log-prawdopodobieństwie a BLEU różni się znacząco w zależności od zakresu porównywanych rodzin modeli. Po pierwsze, log-prawdopodobieństwo jest wysoce skorelowane z BLEU, gdy rozważamy modele należące do tej samej rodziny (np. modele autoregresywne lub modele zmienne utajone z taką samą parametryzacją jak poprzednie). Nie obserwujemy jednak korelacji pomiędzy rankingami modeli w różnych rodzinach: (1) wśród modeli zmiennych nieautoregresyjnych latentnych elastyczna dystrybucja poprzednia jest lepsza w szacowaniu gęstości, ale daje gorszą jakość generacji niż zwykły prior oraz (2) modele autoregresyjne\noferują najlepszą ogólną wydajność tłumaczenia, podczas gdy ukryte modele zmiennych z normalizującym przepływem wcześniej dają największe prawdopodobieństwo utrzymywania logu we wszystkich zbiorach danych. Dlatego zalecamy użycie prostego uprzedniego modelu dla zmiennej latentnej nieautoregresywnej, gdy pożądana jest szybka prędkość generowania.', 'ro': 'Multe sarcini de generare secvență-la-secvență, inclusiv traducerea automată și text-la-vorbire, pot fi considerate estimarea densității ieșirii y dată fiind intrarea x: p(y|x). Având în vedere această interpretare, este firesc să se evalueze modelele secvență-la-secvență utilizând probabilitatea logului condiționat pe un set de teste. Cu toate acestea, scopul generarii secvență-la-secvență (sau predicția structurată) este de a găsi cea mai bună ieșire y dată de intrare x, iar fiecare activitate are propria metrică din aval R care scorează o ieșire de model comparând cu un set de referințe y*: R(y, y* | x). Deși sperăm că un model care excelează în estimarea densității funcționează bine și pe metrica din aval, corelația exactă nu a fost studiată pentru sarcinile de generare a secvențelor. În această lucrare, prin compararea mai multor estimatoare de densitate pe cinci sarcini de traducere automată, constatăm că corelația dintre clasamentele modelelor bazate pe log-probabilitate și BLEU variază semnificativ în funcție de gama familiilor de modele comparate. În primul rând, probabilitatea log-ului este foarte corelată cu BLEU atunci când luăm în considerare modele din aceeași familie (de exemplu modele autoregresive, sau modele variabile latente cu același parametrizare a celui anterior). Cu toate acestea, nu observăm nicio corelație între clasamentele modelelor din diferite familii: (1) în rândul modelelor variabile latente non-autoregressive, o distribuție anterioară flexibilă este mai bună la estimarea densității, dar oferă o calitate a generației mai proastă decât un simplu anterior, și (2) modele autoregressive\nOferă cea mai bună performanță de traducere generală, în timp ce modelele variabile latente cu un flux de normalizare anterior oferă cea mai mare probabilitate de logare reținută în toate seturile de date. Prin urmare, vă recomandăm să utilizați un simplu anterior pentru modelul non-autoregresiv variabil latent atunci când se dorește viteza de generare rapidă.', 'sr': 'Mnogi zadatak generacije sekvence do sekvence, uključujući prevod mašine i tekst do govora, mogu se postaviti kao procjenjivanje gustine proizvoda y s obzirom na ulaz x: p(y.x). S obzirom na ovu interpretaciju, prirodno je proceniti modele sekvence do sekvence koristeći uslovnu verovatnoću log a na testu. Međutim, cilj generacije sekvence do sekvence (ili strukturirane predviđanja) je pronaći najbolji izlaz y koji je dao ulaz x, a svaki zadatak ima svoj vlastiti donji metrički R koji rezultira izlaz modela uspoređujući sa setom referencija y*: R(y, y*.x). Iako se nadamo da model koji nadmašuje procjenu gustine takoðe dobro izvodi na metriji dole, taèna korelacija nije proučena za zadatak generacije sekvence. U ovom papiru, uspoređujući nekoliko procjena gustine na pet zadataka za prevod mašine, smatramo da se povezanost između redova modela baziranog na log-verovatnosti i BLEU značajno razlikuje u ovisnosti o rasponu modelnih porodica u usporedbi. Prvo, vjerojatnost dnevnika je vrlo povezana sa BLEU kada razmišljamo o modelima unutar iste porodice (npr. autoregresivne modele, ili latentne varijantne modele sa istom parameterizacijom prethodnog). Međutim, mi ne posmatramo nikakvu korelaciju između redova modela u različitim porodicama: 1) između ne autoregresivnih latentnih modela, fleksibilna prethodna distribucija je bolja za procjenu gustosti, ali daje gore kvalitet generacije nego jednostavno ranije, i (2) autoregresivne modele\nponudite najbolju provedbu prevođenja ukupno, dok latentne varijantne modele sa normalizacijskim tokom prije pružaju najveću verovatnoću izvršenog izveštaja na svim podacima. Stoga preporučujemo korištenje jednostavnog ranije za latentni promjenni ne-autoregresivni model kada se žele brzina generacije.', 'si': 'ගොඩක් පරීක්ෂණ-ට-පරීක්ෂණ පරීක්ෂණ වැඩක්, පරික්ෂණ පරීක්ෂණය සහ පාළු පරික්ෂණය-ට-පරීක්ෂණය සම්බන්ධ වෙනුවෙන්, ප්\u200dරවේ මේ අභිවාදයක් නිසා, පරීක්ෂණ සෙට් එකේ සාමාන්\u200dය ලොක් වර්ගයක් භාවිත කරන්න ප්\u200dරතිභාවිතයි. හැබැයි, sequence-to-sequence පරීක්ෂණය (නැත්තම් සංස්ථාපිත ප්\u200dරශ්නයක්) ලක්ෂණය තමයි හොඳම ප්\u200dරශ්නයක් හොයාගන්න x, හැබැයි වැඩේ වැඩේ ප්\u200dරශ්නයක් තියෙනවා මෙට්\u200dරික් R අපි බලාපොරොත්තු වෙන්නේ මොඩේල් එකක් විශාල විශාල විශාල විශාල කරනවා කියලා, පහත් මෙට්\u200dරික් වලින් හොඳ විශාල කරනවා, සිද මේ පැත්තේ, මැෂින් වාර්ථාව පහක් විතරයි, අපි හොයාගන්නේ ලොක් වර්ගයක් සහ BLUE වර්ගයක් සඳහා මොඩල් වර්ගයක් සම්බන්ධ වෙනුවෙන් සම්බන්ධ වෙන මුලින්ම, ලොග් ප්\u200dරමාණය බොහොම සම්බන්ධ වෙන්නේ BLUE එක්ක, අපි එකම පවුලේ මොඩේල් හිතන්නේ කියලා (උදාහරණය ස්වයංග්\u200dරේෂිත මොඩේල්, නැත්ත නමුත්, අපි වෙනස් පවුලෙන් වෙනස් පවුලෙන් ප්\u200dරමාණයක් අතර කිසිම සම්බන්ධයක් නැහැ: (1) ස්වයංක්\u200dරියාත්මක වෙනස් මොඩේලන් අතර නැහැ ස්වයංක්\u200dරියාත්මක විශේෂ\nසාමාන්\u200dය වෙනස් මොඩල් එක්ක සාමාන්\u200dය වෙනස් වෙනුවෙන් හොඳම වාර්ථාව ප්\u200dරවෘත්තියක් සාමාන්\u200dය වෙනුවෙන් ප්\u200dරවෘත්තිය ස ඉතින්, අපි ඉක්මනින් වේගයේ වේගයක් අවශ්\u200dය වෙලාවේ ස්වයංක්\u200dරියාත්මක වෙන්න පුළුවන් ප්\u200dරයෝජනයක් ප්\u200dරයෝ', 'so': 'Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y|x).  Turjumaankan la siiyo waa dabiicadda in lagu qiimeeyo tusaalaha imtixaanka lagu soo bandhigi karo qoraalka xaaladda ah. Si kastaba ha ahaatee qarniga soo dabaasha (ama la hor dhigi karo) waa in la helo midhaha ugu wanaagsan oo laga soo saaray input, shaqo walbana wuxuu leeyahay metric R oo kooban midhaha hoose ka soo socda, si a y u barbaroorto midhaha y*: R(y* * ;) x). Intaan rajaynayno in model ka sarreeya qiimeynta qiimeynta qiimeynta qarsoon xittaa uu si wanaagsan uga sameeyo metricka hoose, xiriirka saxda ah lama baran shaqooyinka qarniga dabadeed. Qoraalkan waxaa ku qoran warqaddan, iyadoo isbarbardhiga qiyaastii qiimeeya oo ku qoran shan shaqooyin turjuman oo machine ah, waxaynu aragnaa in xiriirka kala duduwanaya qoraalka sameynta iyo habaarka qoraalka iyo BLEU aad bay u kala duwan tahay, waxayna ku xiran tahay tirada qoysaska isbarbardhiga. Marka ugu horeysa, suurtagalka log-gelintu aad buu ugu xiran yahay BLEU markaynu ka fiirsanayno tusaalooyin isku mid ah qoyska dhexdiisa (tusaale ahaan qaababka iskaa-regressive ama modelalka bedelan e e ugu dambeeya oo ku saabsan isku mid is-bedelka hore). Si kastaba ha ahaatee, ma fiirinno xiriir kala duwan oo kala duduwan qoysaska kala duduwan:(1) Mid ka mid ah noocyada aan autoregative-latent bedelan, qaybinta hore waa ka wanaagsan tahay qiimeynta qiimeynta hoose, laakiin waxay siisaa qiimaha qarniga ka sii xumaa qiimo yar tan horaad, iyo (2) qaabab madax u ah\nwaxay bixisaa muuqashada turjumaadda ugu fiican, isla markaasna tusaalaha ugu dambeeya ee kala duwan oo ay leeyihiin durdurka caadiga ah kahor waxay siisaa suurtagalka ugu sarreeya qoraal-ka-baxa oo dhan. Sidaa darteed waxaan ku talinaynaa inaad isticmaasho tusaale fudud ee ugu dambeeya oo aan iskumarin, marka loo baahdo dhaqso dhaqdhaqaaq.', 'ta': 'பின்வரும் தலைமுறை பணிகள், இயந்திரம் மொழிபெயர்ப்புகள் மற்றும் உரையில் இருந்து பேச்சுகள் உள்ளீட்டு x: p( y\\.x) கொடுக்கப்பட்ட வெளியீட்டின் density இந்த விளக்கம் கொடுத்தால், சோதனையின் அமைப்பில் நிலையான பதிவு சாத்தியமான மாதிரிகளை மதிப்பிட இயல்பாகும். தொடர்ந்து வரும் தலைமுறை (அல்லது கட்டுப்படுத்தப்பட்ட முன்னோட்டம்) உள்ளீட்டு கொடுக்கப்பட்ட சிறந்த வெளியீட்டு வெளியீட்டை கண்டுபிடிக்க, ஒவ்வொரு செயலுக்கும் அதன் சொந்த கீழ்நீர நாம் நம்புகிறோம் அதிக மதிப்பின் மாதிரி அதிகமாக இருக்கும் என்று ஒரு மாதிரி கீழ்நீர் மெட்ரிக்கில் நன்றாக செயல்படுத்துகிறது, தொடர்பு  இந்த காகிதத்தில், ஐந்து இயந்திரம் மொழிபெயர்ப்பு பணிகளை ஒப்பிடும் பல தூக்கத்திற்குரிய கணக்கீட்டாளர்களை ஒப்பிடும் மாதிரி குடும்பங்களை ஒப்பிடும் வித்தி முதலில், பதிவு வாய்ப்பு பிலூயுடன் மிகவும் இணைந்து இருக்கிறது நாம் ஒரே குடும்பத்தில் உள்ள மாதிரிகளை கருதும்போது (உ. ம். தானாகவே கட்டுப்பாடு மாத ஆனால், நாம் வேறு குடும்பத்திற்கு முழுவதும் மாதிரிகளின் மாதிரிகளுக்கிடையே உறவு பார்க்கவில்லை: (1) தன்னியக்கமாக கட்டுப்பாட்டு சாதாரண மாதிரிகளில், ஒரு flexible முன் வ\nசிறந்த மொழிபெயர்ப்பு செயல்பாட்டை மொத்தமாக கொடுக்கவும், ஆனால் சமாதாயமான மாறிகள் மாதிரிகள் வழங்கும் முன்னால் அனைத்து தரவ அதனால், நாம் ஒரு சுலபமான முன்னால் பயன்படுத்தி தற்போதைய மாறி தானே கட்டுப்பாட்டு மாதிரிக்கு பரிந்துரைக்கிறோம்', 'sv': 'Många sekvens-till-sekvensgenereringsuppgifter, inklusive maskinöversättning och text-till-tal, kan framställas som att uppskatta densiteten av utmatningen y givet inmatningen x: p(y|x). Med tanke på denna tolkning är det naturligt att utvärdera sekvens-till-sekvensmodeller med villkorad loggsannolikhet på en testuppsättning. Målet med sekvens-till-sekvensgenerering (eller strukturerad förutsägelse) är dock att hitta den bästa utmatningen y givet en inmatning x, och varje uppgift har sin egen nedströms metrisk R som får en modellutmatning genom att jämföra med en uppsättning referenser y*: R(y, y* | x). Även om vi hoppas att en modell som utmärker sig i densitetsberäkning också presterar bra på nedströmsmålet, har den exakta korrelationen inte studerats för sekvensgenereringsuppgifter. I denna uppsats, genom att jämföra flera densitetsestimatorer på fem maskinöversättningsuppgifter, finner vi att korrelationen mellan placeringar av modeller baserat på log-sannolikhet och BLEU varierar avsevärt beroende på intervallet av de modellfamiljer som jämförs. För det första är loggsannolikheten starkt korrelerad med BLEU när vi betraktar modeller inom samma familj (t.ex. autoregressiva modeller, eller latenta variabelmodeller med samma parametrisering som tidigare). Vi observerar dock ingen korrelation mellan placeringar av modeller över olika familjer: (1) bland icke-autoregressiva latenta variabelmodeller är en flexibel tidigare fördelning bättre vid densitetsuppskattning men ger sämre generationskvalitet än en enkel tidigare, och (2) autoregressiva modeller\nger den bästa översättningsprestandan totalt, medan latenta variabelmodeller med normaliserande flödesförinnan ger den högsta hållna loggsannolikheten för alla datauppsättningar. Därför rekommenderar vi att du använder en enkel förkortning för den latenta variabeln icke-autoregressiv modell när snabb genereringshastighet önskas.', 'ur': 'اور بہت سی سی کائنات-سے-کائنات کی نسل کے کام، ماشین ترجمہ اور متن-سے-کلام میں شامل ہوتے ہیں، اس کے گھمنڈ کی گھمنڈی کا مطالبہ کرسکتے ہیں، اِن وروت کی x: p(y) x۔ اس تفسیر کی وجہ سے، ایک تست سٹ پر کانڈیسینل لاگ-likelihood کے استعمال سے سٹ-سے-کٹ-کٹ-کٹ موڈل کا ارزش کرنا طبیعی ہے. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y given an input x, and each task has its own downstream metric R that scores a model output by comparing a set of references y*: R(y, y* .x). حالانکہ ہم امید رکھتے ہیں کہ ایک مدل جو گہرے اندازے میں زیادہ اضافہ کرتا ہے، نیچے سیارے میٹریک پر بھی اچھا کام کرتا ہے، دقیق تعلق کی تعلق کی تعلیم کے لئے نہیں کی گئی ہے۔ اس کاغذ میں، پانچ ماشین ترجمہ کے تابع پر چند گھونٹی کا مصاحبہ مقایسہ کرنے کے ذریعہ، ہم دیکھتے ہیں کہ لوگ-احتمال اور بلیوس کے متعلق مدل خاندان کے مقایسہ پر متفاوت ہے. پہلے، لوگ-احتمال بلیوس کے ساتھ بہت سی تعلق ہے جب ہم ایک خاندان کے اندر موڈل سمجھتے ہیں (جیسے autoregressive models، یا latent variable models with the same parameterization) پہلے سے۔ لیکن ہم مختلف خاندان کے درمیان مدل کے درمیان کوئی تعلق نہیں دیکھتے: (1) غیر autoregressive latent variable models میں، ایک flexible prior distribution density estimation میں بہتر ہے لیکن ایک ساده پہلے سے بدترین نسل کیفیت دیتا ہے، اور (2) autoregressive models\nسب سے بہترین ترجمہ کے عملکرد کو پیش کریں، حالانکہ لاٹینٹ ویرائیٹ موڈلوں کو اس سے پہلے ایک سیدھی فلوپ کے ساتھ بہترین ترجمہ کرنا چاہیے کہ تمام ڈیٹ سٹ میں سب سے بالاترین لگ-آئٹ کا احتمال دے۔ لہٰذا، ہم ایک ساده پہلے کے استعمال کرنے کی توصیف دیتے ہیں لٹینٹ ویرئیٹ غیر-autoregressive موڈل کے لئے جب تیز نسل کی سرعت خواہش کی جاتی ہے۔', 'uz': "Ketma- ketlik tarjima va matn tahrirlash vazifalari bilan bir nechta tarjima qilinadi. Foydalanilgan matn tarjima va matn tahrirchi natijasining qiymatini qidirish mumkin. Bu tarjima berilganda, sinov tugmasining imkoniyatini yordamida sezgiruvchi modellarni qiymatish oddiy. Lekin keyingi tarqatish avval (yoki tuzuvchi 预测lik) eng eng yaxshi natijani topish, va har bir vazifaning qismi o'zida metrik R o'zida model natijasini qo'yish mumkin. Ko'rib chiqish usulida o'xshash modeli va quyidagi metrikda yaxshi bajaradi deb umid qilamiz, biz bir xil avval vazifalari uchun haqiqiqiy bogʻlanmagan. Ushbu qogʻozda, besh mashina tarjima vazifalarining bir necha qiyin qiymatlarini kamaytirish bilan o'rganamiz, biz o'ylaymiz, log- imkoniyatlarining orasidagi modellar orasidagi bog'liq va BLEU model oilalarining qismiga bog'liqdir. Birinchi so'zda, log-o'zgarishlar bir xil qoidadagi modellarni tasavvur qilayotganda BLEU bilan juda bog'liq boʻladi (masalan, avto-regressive modellari yoki latent variable modellari bir xil parametrlari bilan birga ega boʻlganda). However, we observe no correlation between rankings of models across different families: (1) among non-autoregressive latent variable models, a flexible prior distribution is better at density estimation but gives worse generation quality than a simple prior, and (2) autoregressive models\n@ info: whatsthis Shunday qilib, biz keyingi oʻzgaruvchining avto-regressiv modeli uchun oddiy oldin foydalanishimizni talab qilamiz.", 'vi': 'Rất nhiều công việc sản xuất chuỗi-tới-số, bao gồm cả dịch-máy và văn-tới-phát-biểu, có thể được đặt là ước tính mật độ của sản xuất y dựa vào nhập x: p(y). Dựa vào giải pháp này, đánh giá các mô- trình-tới-trình mô-suất dựa trên khả năng đăng ký với một bộ thử là bình thường. Tuy nhiên, mục tiêu của chế tạo chuỗi-tới-trình (hay dự đoán cấu trúc) là tìm nguồn xuất tốt nhất từng được cho một nhập x, và mỗi nhiệm vụ có một khẩu R theo dòng chảy để đạt điểm xuất mô hình bằng cách so sánh với một bộ tham khảo y y y*: R(y, y* (« 124; x). Trong khi chúng tôi hy vọng rằng một mô hình xuất sắc trong khả năng đánh giá mật độ cũng tiến triển tốt theo hệ thống xuôi dòng, thì sự tương quan chính xác chưa được nghiên cứu cho các nhiệm vụ sản xuất chuỗi. Trong tờ giấy này, bằng cách so sánh vài bộ tộc mật độ với năm công việc dịch chuyển máy, chúng tôi thấy sự tương quan giữa phân hạng các mô-đun dựa trên khả năng đăng kí và tiếng bíp khác biệt đáng kể tùy thuộc vào giới hạn các gia đình mô hình được so sánh. Đầu tiên, khả năng đăng ký có mối quan hệ rất cao với tiếng bíp khi chúng ta xem xét các mô hình trong cùng một gia đình (v. d. mô hình tự tụt lại, hoặc các mô hình biến cố tiềm năng với đo giới hạn tương tự của người trước). Tuy nhiên, chúng tôi không quan sát mối tương quan giữa xếp hạng các mô- đun khác nhau: 1) giữa các mô- đun tiềm năng không tự động, phân phối trước có thể thuận theo mật độ tốt hơn, nhưng tạo thế hệ tệ hơn các mô- đun trước, và (2) các mô- đun tự vệ.\ncung cấp khả năng dịch chuyển tốt nhất, trong khi các mô- đun tiềm năng với dòng chảy bình thường trước đó cho khả năng vượt trội cao nhất trong tất cả các tập tin dữ liệu. Vì vậy, chúng tôi đề nghị sử dụng một phép trước đơn giản cho mô hình tiềm năng không có tự lực khi nào tốc độ sản xuất nhanh được muốn.', 'bg': 'Много задачи за генериране на последователност по последователност, включително машинен превод и текст в реч, могат да се представят като оценка на плътността на изходния y при въвеждането х: p(y|x). Като се има предвид това тълкуване, естествено е моделите последователност към последователност да се оценяват с помощта на условна логаритметична вероятност върху тестов набор. Въпреки това, целта на генерирането последователност към последователност (или структурирано прогнозиране) е да се намери най-добрият изход у при дадено входно х, и всяка задача има свой собствен метричен показател надолу по веригата, който оценява изхода на модела чрез сравняване с набор от препратки у*: R(y, y* | x). Макар и да се надяваме, че модел, който се отличава в оценката на плътността, също се справя добре с метриката надолу по веригата, точната корелация не е проучена за задачите за генериране на последователност. В тази статия, сравнявайки няколко оценителя на плътността на пет задачи за машинен превод, откриваме, че корелацията между класирането на моделите въз основа на log-вероятността и варира значително в зависимост от обхвата на сравняваните семейства модели. Първо, лог-вероятността е силно корелирана с BLU, когато разглеждаме модели от едно и също семейство (например авторегресивни модели или латентни променливи модели със същата параметризация като предишните). Въпреки това, не наблюдаваме корелация между класирането на моделите в различните семейства: (1) при неавторегресивните латентни променливи модели гъвкавото предварително разпределение е по-добро при оценката на плътността, но дава по-лошо качество на генериране от обикновените предварителни модели, и (2) авторегресивни модели\nпредлагат най-добрата производителност на превода като цяло, докато латентните променливи модели с нормализиращ поток дават най-висока вероятност за излизане на запис във всички набори от данни. Ето защо препоръчваме използването на прост преди за латентната променлива неавторегресивен модел, когато е желана бърза скорост на генериране.', 'hr': 'Mnogi zadatak generacije sekvence do sekvence, uključujući prevod strojeva i tekst do govora, mogu se postaviti kao procjena gustoće izlaza y s obzirom na ulaz x: p(y.x). S obzirom na ovu interpretaciju, prirodno je procijeniti modele sekvence do sekvence koristeći uvjetnu mogućnost log-vjerojatnosti na testu. Međutim, cilj generacije sekvence-do-sekvence (ili strukturirane predviđanje) je pronaći najbolji izlaz y dajući ulaz x, a svaki zadatak ima svoj vlastiti doljeni metrički R koji rezultira izlaz modela uspoređujući s setom referencija y*: R(y, y*.x). Iako se nadamo da model koji nadmašuje procjenu gustine također dobro izvršava na donjem metriku, tačna korelacija nije ispitivana za zadatke generacije sekvencije. U ovom papiru, uspoređujući nekoliko procjena gustoće na pet zadataka prevoda strojeva, smatramo da se povezanost između redova modela baziranog na log-vjerojatnosti i BLEU-u značajno razlikuje u ovisnosti o rasponu modelnih obitelji koje se uspoređuju. Prvo, vjerojatnost dnevnika je vrlo povezana s BLEU-om kada razmotrimo modele unutar iste obitelji (npr. autoregresivne modele ili latentne varijantne modele s istim parameterizacijom prije). Međutim, mi ne primjećujemo korelaciju između redova modela u različitim obiteljima: 1) između modela ne autoregresivnih latentnih promjena, fleksibilna prethodna distribucija je bolja za procjenu gustosti, ali daje gore kvalitet generacije nego jednostavno ranije, i (2) autoregresivne modele\nnude najbolje učinkovito prevoda ukupno, dok latentni varijačni modeli s normalizacijskim tokom prije daju najveću vjerojatnost izvršenog izvještaja u svim podacima. Stoga preporučujemo korištenje jednostavnog ranije za latentnu promjenu ne autoregresivnog model a kada se žele brzina generacije.', 'nl': 'Veel sequence-to-sequence generatietaken, waaronder machinevertaling en tekst-to-spraak, kunnen worden gesteld als het schatten van de dichtheid van de uitvoer y gegeven de invoer x: p(y|x). Gezien deze interpretatie is het natuurlijk om sequentiemodellen te evalueren met behulp van voorwaardelijke logwaarschijnlijkheid op een testset. Het doel van opeenvolging-tot-opeenvolging genereren (of gestructureerde voorspelling) is echter om de beste output y te vinden bij een invoer x, en elke taak heeft zijn eigen downstream metric R die een model output scoort door te vergelijken met een reeks referenties y*: R(y, y*, x). Hoewel we hopen dat een model dat uitblinkt in dichtheidsschatting ook goed presteert op de downstream metric, is de exacte correlatie niet bestudeerd voor sequentie generatie taken. In dit artikel, door verschillende dichtheidsschatters te vergelijken op vijf machinevertaaltaken, ontdekken we dat de correlatie tussen rangschikkingen van modellen op basis van log-waarschijnlijkheid en BLEU aanzienlijk varieert afhankelijk van het bereik van de te vergelijken modelfamilien. Ten eerste is log-waarschijnlijkheid sterk gecorreleerd met BLEU wanneer we modellen binnen dezelfde familie bekijken (bijvoorbeeld autoregressieve modellen, of latente variabele modellen met dezelfde parametrisering als de vorige). We zien echter geen correlatie tussen de ranglijsten van modellen over verschillende families: (1) onder niet-autoregressieve latente variabele modellen, een flexibele voorafgaande verdeling is beter in dichtheidsschatting, maar geeft een slechtere generatie kwaliteit dan een eenvoudige voorafgaande, en (2) autoregressieve modellen\nbieden de beste vertaalprestaties over het algemeen, terwijl latente variabele modellen met een normaliserende flow voorafgaand de hoogste vastgehouden logwaarschijnlijkheid bieden voor alle datasets. Daarom raden we aan een eenvoudige voorafgaand te gebruiken voor het latente variabele niet-autoregressieve model wanneer een snelle generatie snelheid gewenst is.', 'de': 'Viele Sequenz-zu-Sequenz-Generierungsaufgaben, einschließlich maschineller Übersetzung und Text-in-Sprache, können als Schätzung der Dichte der Ausgabe y bei Eingabe x: p(y|x) dargestellt werden. Angesichts dieser Interpretation ist es natürlich, Sequenz-zu-Sequenz-Modelle mit bedingter Log-Likelihood auf einem Testsatz zu bewerten. Das Ziel der Sequenz-zu-Sequenz-Generierung (oder strukturierter Vorhersage) besteht jedoch darin, die beste Ausgabe y bei Eingabe x zu finden, und jede Aufgabe hat ihre eigene nachgelagerte Metrik R, die eine Modellausgabe durch Vergleich mit einer Menge von Referenzen y* bewertet: R(y, y*,x). Während wir hoffen, dass ein Modell, das sich in der Dichteschätzung auszeichnet, auch auf der nachgeschalteten Metrik gut abschneidet, wurde die genaue Korrelation für Sequenzgenerationsaufgaben nicht untersucht. Durch den Vergleich mehrerer Dichteschätzer für fünf maschinelle Übersetzungsaufgaben stellen wir in dieser Arbeit fest, dass die Korrelation zwischen Rankings von Modellen basierend auf Log-Likelihood und BLEU je nach Umfang der zu vergleichenden Modellfamilien signifikant variiert. Erstens ist Log-Likelihood stark mit BLEU korreliert, wenn wir Modelle innerhalb derselben Familie betrachten (z.B. autoregressive Modelle oder latente Variablenmodelle mit der gleichen Parametrisierung wie das vorherige). Wir beobachten jedoch keine Korrelation zwischen den Ranglisten von Modellen über verschiedene Familien: (1) unter nicht autoregressiven latenten Variablenmodellen ist eine flexible vorherige Verteilung besser bei der Dichteabschätzung, liefert aber schlechtere Generationsqualität als ein einfaches Prior, und (2) autoregressive Modelle\nSie bieten die beste Übersetzungsleistung insgesamt, während latente Variablenmodelle mit einem normalisierenden Flow Prevor die höchste festgehaltene Log-Likelihood für alle Datensätze bieten. Daher empfehlen wir für das latente variable nicht-autoregressive Modell ein einfaches Vorab zu verwenden, wenn eine schnelle Erzeugungsgeschwindigkeit gewünscht wird.', 'da': 'Mange sekvens-til-sekvens generationsopgaver, herunder maskinoversættelse og tekst-til-tale, kan stilles som estimering af tætheden af output y givet input x: p(y|x). I betragtning af denne fortolkning er det naturligt at vurdere sekvens-til-sekvensmodeller ved hjælp af betinget logsandsynlighed på et testsæt. Målet med sekvens-til-sekvens-generering (eller struktureret forudsigelse) er dog at finde det bedste output y givet et input x, og hver opgave har sin egen downstream metric R, der scorer en model output ved at sammenligne med et sæt referencer y*: R(y, y* | x). Selvom vi håber, at en model, der udmærker sig i densitetsestimering, også klarer sig godt på downstream metric, er den nøjagtige korrelation ikke blevet undersøgt for sekvensgenereringsopgaver. Ved at sammenligne flere densitetsestimatorer på fem maskinoversættelsesopgaver finder vi i denne artikel, at sammenhængen mellem placeringer af modeller baseret på log-sandsynlighed og BLEU varierer betydeligt afhængigt af rækkevidden af de modelfamilier, der sammenlignes. For det første er log-sandsynlighed stærkt korreleret med BLEU, når vi overvejer modeller inden for samme familie (f.eks. autoregressive modeller eller latente variable modeller med samme parametrisering som tidligere). Vi observerer dog ingen sammenhæng mellem placeringer af modeller på tværs af forskellige familier: (1) blandt ikke-autoregressive latente variable modeller er en fleksibel forudgående fordeling bedre til densitetsestimering, men giver dårligere generationskvalitet end en simpel forudgående, og (2) autoregressive modeller\ntilbyder den bedste oversættelseseffekt generelt, mens latente variable modeller med en normaliserende flow forud giver den højeste holdt-out log-sandsynlighed på tværs af alle datasæt. Derfor anbefaler vi at bruge en simpel forud for den latente variabel ikke-autoregressive model, når der ønskes hurtig genereringshastighed.', 'id': 'Banyak tugas generasi urutan-ke-urutan, termasuk terjemahan mesin dan teks-ke-pidato, dapat diposisikan sebagai perhitungan densitas output y diberikan input x: p(y\uf0a5x). Mengingat interpretasi ini, alami untuk mengevaluasi model urutan ke urutan menggunakan kemungkinan log kondisional pada set tes. Namun, tujuan generasi urutan-ke-urutan (atau prediksi strukturasi) adalah untuk menemukan output terbaik y yang diberikan input x, dan setiap tugas memiliki R metrik turun sendiri yang mencetak output model dengan membandingkan dengan set referensi y*: R(y, y* \uf0a5 x). Sementara kita berharap bahwa model yang melebihi dalam penilaian densitas juga bekerja dengan baik pada metrik turun, korelasi tepat belum dipelajari untuk tugas generasi urutan. Dalam kertas ini, dengan membandingkan beberapa penilai ketepatan pada lima tugas terjemahan mesin, kami menemukan bahwa korelasi antara penilaian model berdasarkan kemungkinan log dan BLEU berbeda secara signifikan bergantung pada jangkauan dari keluarga model yang dibandingkan. Pertama, kemungkinan log sangat berkorelasi dengan BLEU ketika kita mempertimbangkan model dalam keluarga yang sama (misalnya model autoregresif, atau model variabel latent dengan parameterisasi yang sama dari sebelumnya). Namun, kita tidak memperhatikan korelasi antara rangkaian model di antara keluarga yang berbeda: (1) diantara model variabel laten bukan-autoregresif, distribusi sebelumnya fleksibel lebih baik pada penilaian densitas tetapi memberikan kualitas generasi yang lebih buruk daripada model sebelumnya sederhana, dan (2) model autoregresif\nmenawarkan prestasi terjemahan terbaik secara keseluruhan, sementara model variabel latent dengan aliran normalisasi sebelumnya memberikan kemungkinan log-out tertinggi di seluruh set data. Oleh karena itu, kami merekomendasikan menggunakan Sebelumnya sederhana untuk model variabel tidak-autoregresif latent ketika kecepatan generasi cepat diinginkan.', 'ko': '기계 번역과 텍스트 음성을 포함한 많은 시퀀스에서 시퀀스 생성 작업은 주어진 입력 x:p(y | x) 상황에서 출력 y의 밀도를 추정하는 것으로 가정할 수 있다.이런 해석을 감안하면 테스트 집합에서 조건 대수를 사용하여 서열 간 모델을 평가하는 것은 자연스러운 것이다.그러나 시퀀스에서 시퀀스 생성(또는 구조화 예측)까지의 목표는 입력 x를 지정한 상황에서 가장 좋은 출력 y를 찾는 것이다. 모든 작업은 자신의 하류 도량 R을 가지고 참고 값 y*:R(y, y*|x)과 비교하여 모델 출력을 평가한다.밀도 평가에서 뛰어난 모델도 하류 도량에서 양호하게 나타나기를 희망하지만 서열 생성 임무의 정확한 관련성은 아직 연구되지 않았다.본고에서 다섯 가지 기계 번역 임무의 몇 가지 밀도 평가를 비교한 결과 대수 유사성과 BLEU의 모델 순위 간의 관련성은 비교된 모델족의 범위에 따라 현저히 다르다는 것을 알 수 있다.먼저, 동일한 패밀리의 모델(예: 회귀 모델, 또는 동일한 선험적 매개 변수가 있는 잠재 변수 모델)을 고려할 때 대수는 BLEU 높이와 관련이 있는 것처럼 보입니다.그러나 우리는 서로 다른 가족의 모델 순위 사이에 관련성이 없다는 것을 관찰했다. (1) 비자귀환 잠재 변수 모델에서 유연한 선험 분포는 밀도 평가에 있어 더 좋지만 간단한 선험 분포보다 질이 떨어진다. (2) 자귀환 모델.\n전체적으로 가장 좋은 전환 성능을 제공했고 모든 데이터가 집중되어 규범화 흐름을 가진 잠재적 변수 모델은 가장 높은 보존 대수 가능성을 제시했다.따라서 빠른 생성 속도가 필요할 때 잠재 변수의 비자귀환 모델에 대해 간단한 선험을 사용하는 것을 권장합니다.', 'af': "Baie volgorde-na-volgorde geneeringstaak, insluitend masjien vertaling en teks-na-woord, kan wees posisioneer as estimatiseer van die densiteit van die uitset y gegee die invoer x: p(y.x). Gien hierdie uitlegging, is dit natuurlik om sekwensie-na-sekwensie modele te evalueer deur voorwaardes log-waarskynlik op 'n toets stel te gebruik. Maar die doel van volgorde-na-volgorde generasie (of struktureerde voorskou) is om die beste uitvoer y gegee 'n invoer x te vind, en elke taak het sy eie onderstreem metriese R wat 'n model uitvoer tel deur vergelyking teen' n stel van verwysing y*: R( y, y *. x). Alhoewel ons hoop dat 'n model wat oorvloei in densiteit estimatie ook goed uitvoer op die onderstreem metriese, die eksakte korrelasie is nie onderwerp vir sekwensiegenerasie taak nie. In hierdie papier, deur vergelyking van verskeie densiteit estimatiërs op vyf masjien vertaling opdragte, vind ons dat die korrelasie tussen rankings van modele gebaseer op log-waarskynlik en BLEU verander betekeurig afhanklik van die omvang van die model geslagte wat vergelyk word. Eerste, log-waarskynlik is baie korrelasieer met BLEU wanneer ons beskou modele binne dieselfde familie (bv. autoregressiewe modele, of latent veranderlike modele met dieselfde parameter van die vooraf). Ons hou tog geen korrelasie tussen rankings van modele oor verskillende geslagte nie: (1) onder nie-autoregressiewe latente veranderlike modele, 'n fleksibel voorheede verspreiding is beter by densiteit estimatie maar gee verdere generasie kwaliteit as 'n eenvoudige voorheede en (2) autoregressiewe modele\ngee die beste vertaling prestasie totaal, terwyl latente veranderlike modele met 'n normaliseerde vloei voorheen gee die hoogste gehou- uit log- likelikheid oor alle datastelle. Daarom, ons aanbeveel om 'n eenvoudige vooraf te gebruik vir die latente veranderlike nie- autoregressiewe model wanneer vinnige generasie spoed verwil word.", 'sw': 'Kazi nyingi za vizazi vya mfululizo kwa mfululizo, ikiwa ni pamoja na tafsiri ya mashine na hotuba ya maandishi, zinaweza kuchukuliwa kama kutathmini kiwango cha uchungufu wa matokeo yaliyotolewa kwa ajili ya input x: p(y\\.x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set.  Hata hivyo, lengo la kizazi cha mfululizo (au utabiri wa umetengenezwa) ni kutafuta matokeo bora zaidi yaliyotolewa kwa ajili y a input, na kila kazi ina metric yake ya chini ya mto ambayo inachezea matokeo ya mifano kwa kulinganisha mfululizo wa maoni ya y*: R(y* *; x). Wakati tunatumaini kuwa mifano yenye kiwango cha uchunguzi pia kinafanya vizuri katika mito ya chini, mahusiano sahihi hayajafunzwa kwa kazi za vizazi vya mfululizo. Katika karatasi hii, kwa kulinganisha idadi mbalimbali za uchunguzi kuhusu kazi za kutafsiri mashine tano, tunagundua kuwa uhusiano kati ya mabwawa ya mitindo yenye uwezekano wa log na BLEU hutofauti sana na kutegemea kwa kiwango kikubwa cha familia za mifano zinazolinganishwa. Kwanza, uwezekano wa kujilogu umeunganishwa sana na BLEU pale tunapofikiria mifano ndani ya familia hiyo (kwa mfano mifano ya kujikandamiza, au mifano ya mabadiliko ya hivi karibuni yenye kipimo sawa cha kipimo cha awali). Hata hivyo, hatukuona uhusiano kati ya mabadiliko ya mifano katika familia mbalimbali: (1) miongoni mwa mifano ya mabadiliko ya hivi karibuni isiyo na utawala wa kujitegemea, usambazaji wa zamani ni bora zaidi kwa kadiri ya uchungu lakini inatoa viwango vibaya zaidi ya vizazi vibaya kuliko moja kabla, na (2) mifano ya kujidhibiti\nkutoa ufanisi bora wa tafsiri kwa ujumla, wakati mifano ya mabadiliko ya hivi karibuni yenye mafuriko ya kawaida kabla ya kutoa uwezekano wa kuzungumzwa zaidi katika seti zote za data. Kwa hiyo, tunapendekeza kutumia kipindi rahisi kwa ajili ya mifano ya hivi karibuni isiyobadilika ya kudhibiti kujitegemea wakati haraka ya kizazi kinahitajika.', 'sq': 'Shumë detyra të gjenerimit sekuencë-në-sekuencë, duke p ërfshirë përkthimin e makinës dhe tekstin-në-fjalim, mund të përcaktohen si vlerësimin e densitetit të daljes y të dhënë input x: p(y[UNK]x). Duke pasur parasysh këtë interpretim, është natyrore të vlerësohen modelet sekuencë-në-sekuencë duke përdorur logaritmin e kushtueshëm-gjasa në një set testimi. Megjithatë, qëllimi i gjenerimit nga sekuenca në sekuencë (ose parashikimit strukturuar) është të gjejë daljen më të mirë y të dhënë një input x, dhe secila detyrë ka R metrike të vet poshtë që shënon një model dalje duke krahasuar me një sërë referencash y*: R(y, y* \uf0a5 x). While we hope that a model that excels in density estimation also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks.  Në këtë letër, duke krahasuar disa vlerësime të densitetit në pesë detyra përkthimi makinash, gjejmë se korrelacioni midis renditjeve të modeleve bazuar në rregjistrimin e mundësive të rregjistrimit dhe BLEU ndryshon në mënyrë të konsiderueshme në varësinë e gamës së modeleve që janë krahasuar familjet. Së pari, gjasa e regjistrimit është shumë e korreluar me BLEU kur konsiderojmë modele brenda të njëjtës familje (për shembull modele autoregresive apo modele të ndryshueshme latente me të njëjtin parametrizim të mëparshëm). Megjithatë, ne nuk vëzhgojmë korrelacion midis renditjeve të modeleve nëpër familje të ndryshme: (1) midis modeleve jo-autoregresive të ndryshme të fshehta, një shpërndarje fleksible e mëparshme është më e mirë në vlerësimin e densitetit por jep cilësi më të keqe gjeneratës se një model i thjeshtë të mëparshëm, dhe (2) modele autoregresive\nofrojnë performancën më të mirë të përkthimit përgjithësisht, ndërsa modelet e ndryshueshme të fshehta me një rrjedhje normalizuese përpara japin gjasa më të lartë të regjistrimit të mbajtur në të gjitha grupet e të dhënave. Prandaj, ne rekomandojmë përdorimin e një paraardhjeje të thjeshtë për modelin e ndryshueshëm të fshehtë jo-autoregresiv kur kërkohet shpejtësia e gjeneratës së shpejtë.', 'fa': 'کار های بسیاری از نسل\u200cهای مختلف به مختلف، شامل ترجمه\u200cهای ماشین و متن-به-سخن، می\u200cتوانند به عنوان ارزیابی density از نتیجه y به عنوان ورودی x: p(y.x) قرار دهند. با توجه به این تعبیر، این مدل\u200cها را با استفاده از احتمال log شرایط در یک مجموعه آزمایش تحقیق کردن طبیعی است. با این حال، هدف نسل ردیابی به ردیابی (یا پیش\u200cبینی ساخته شده) بهترین نتیجه y را پیدا کردند که به عنوان یک ورودی x داده شده، و هر کاری متریک ردیابی خود را دارد که نتیجه مدل را با مقایسه کردن با مجموعه ردیابی y*: R(y, y* .x) می\u200cدهد. در حالی که امیدواریم یک مدل که در ارزیابی density زیاده\u200cتر می\u200cشود در متریک پایین، ارتباط دقیقا برای کار های نسل\u200cها مطالعه نشده است. در این کاغذ، با مقایسه کردن تعدادی از ارزش density در پنج تابع ترجمه ماشین، ما یافتیم که ارتباط بین رشته\u200cهای مدل بر اساس احتمال log و BLEU بستگی بستگی بستگی بستگی به مجموعه خانواده\u200cهای مدل مقایسه می\u200cشود. اول، احتمال لوگو با BLEU بسیار ارتباط دارد وقتی ما مدل\u200cهای داخل یک خانواده را در نظر می\u200cگیریم (مثلا مدل\u200cهای autoregressive یا مدل\u200cهای متغیر latent با همان پارامتریزی پیشینیان). با این حال، ما هیچ ارتباطی بین رشته\u200cهای مدل در خانواده\u200cهای مختلف را نمی\u200cبینیم: (۱) بین مدل\u200cهای متغیر غیر خودگریزگریزگریزگریزگریزگریزگریزگریزگریزگریزگریزگریز، تقسیم پیشینیان flexible در ارتباط density بهتر است اما کیفیت نسل بدتر از یک مدل ساده\nبهترین فعالیت ترجمه را در کل پیشنهاد می\u200cدهد، در حالی که مدل\u200cهای متغیر latent با یک جریان عامل\u200cسازی پیش از آنکه بالاترین احتمال یافته\u200cای در تمام مجموعه\u200cهای داده\u200cها را می\u200cدهد. بنابراین، ما پیشنهاد می\u200cدهیم که با استفاده از یک پیشینه ساده برای مدل متغیر latent غیر autoregressive زمانی که سرعت نسل سریع خواسته می\u200cشود استفاده کنیم.', 'tr': "Otomatik terjime we tekst-we-çykyş bilen köp terjime täblikleri, girişinde berilen y girişiniň ýiginligini x: p(y.x). Bu terjime görä, süzme düzümlerinde şartlı log-mümkinçiligi ulanmak tebigdir. Ýöne, sequence-to-sequence döwletleriniň (ýa-da strukturly çaklama) iň gowy çykyş y girdi x'ini tapmak, we her zadyň öz a şak metriýa R'iniň düşürmek üçin bir nusga çykyş mümkin edýän bu nusga y*: R(y, y* .x). Ýagtyklyk hasaplamada ýokary bir nusga a şdyrylýan metriýada gowy gazanýar diýip umyt edýäris. Diňe bir correliýan düzgün dünýä netijesi üçin öwrenmediler. Bu kagyzda, beş maşynyň terjime täbliklerinde birnäçe ýigrenlik pikirimçiliklerini karşılaşdyryp, logg-mümkinçiligine we BLEU düzümlerniň düzümleriniň düzümleriniň derejesinde baglanyşygy örän möhüm döwletlere görä baglanýar. Ilkinji, g ünlük mümkinçiligi bir maşgalada modelleri düşünýän wagtymyz BLEU bilen örän ködleşdirilýär (mysal. otoregressiv modeller, ýa öňki parameteriýasy bilen soňky üýtgeşik modeller). Emma, biz dürli maşgalalaryň arasynda modelleriň a ýratynyň arasynda hiç hili bir görnüşimi ýok etmeýäris: (1) awtomatik-regressiv ýok nusgalaryň arasynda, fleksibil öňki daýratynyň çykyş hasaplamasynda has gowydyr ýöne ýöne jenaýat täsirinden has gowydyr we (2) awtomatik regressiv nusgalarynda\nedit-action Şol sebäpli, geçmişi çalt döredijilik tizligi isleýän wagtlar üçin basit bir öňki ulanmagy maslahat berýäris.", 'hy': 'Շատ հաջորդականություն-հաջորդականություն ստեղծող առաջադրանքներ, ներառյալ մեքենայի թարգմանությունը և տեքստ-խոսքը, կարելի է դիտարկել որպես արտադրության y խտությունը հաշվարկված x: Եթե հաշվի առնենք այս մեկնաբանությունը, բնական է գնահատել հաջորդականություն հաջորդականության մոդելները՝ օգտագործելով փորձարկման համակարգի պայմանավոր հավանականությունը: Այնուամենայնիվ, հաջորդականության հաջորդականության (կամ կառուցվածված կանխատեսումների) նպատակն է գտնել լավագույն արտադրությունը y-ի, որը տրվում է x-ի, և յուրաքանչյուր խնդիր ունի իր սեփական ներքևի մետրական R-ը, որը գնահատում է մոդելի արտադրությունը համեմատելով y*: R(y, y*՝ x) խորհրդ Մինչդեռ մենք հույս ունենք, որ խտության գնահատման մեդելը լավ է աշխատում նաև հետագա մետրիկայի վրա, ճշգրիտ համեմատությունը չի ուսումնասիրել հաջորդականության սերունդների առաջադրանքների համար: Այս թղթի մեջ, համեմատելով որոշ խտության գնահատողներ հինգ մեքենայի թարգմանման առաջադրանքների վրա, մենք հայտնաբերում ենք, որ մոդելների գնահատականների կապը, որը հիմնված է լոգ-հավանականության և ԲԼԵՎ-ի վրա, նշանակաբար տարբերվում է կախ Առաջինը, լոգ-հավանականությունը շատ կապված է ԲԼԵՎ-ի հետ, երբ մենք դիտարկում ենք նույն ընտանիքի մոդելներ (օրինակ, ինքնագրեսիվ մոդելներ, կամ թաքնված փոփոխական մոդելներ, որոնք ունեն նախկինում նույն պարամետրիզացիան): Այնուամենայնիվ, տարբեր ընտանիքների մոդելների դասակարգումների միջև հարաբերակցություն չենք նկատում: (1) ոչ ինքնաարգեսիվ թաքնված փոփոխականների մոդելների միջև, ճկուն նախկին բաշխվածությունը ավելի լավ է խտության գնահատման ժամանակ, բայց ավելի վատ է սերունդի որակը, քան պարզ նախկին, և (2)\nներկայացնում են ամենալավ թարգմանման արդյունքները ընդհանուր առմամբ, մինչդեռ թաքնված փոփոխականների մոդելները, որոնք նախկինում նորմալ հոսք ունեն, բոլոր տվյալների համակարգերի մեծ հավանականությունն են: Այդ պատճառով, մենք խորհուրդ ենք տալիս օգտագործել պարզ նախօրինակ թաքնված փոփոխականի ոչ ինքնաարգեսիվ մոդելի համար, երբ արագ սերունդ արագություն է ցանկանում:', 'az': 'Makina çevirilməsi və mətn-sözünə daxil olan çox sequence-to-sequence nəsil işləri, y girdi x: p(y.x). Bu yorumlayıcıya görə, sınama qutusunda müəyyən bir log mümkünlüyünü istifadə edən sequence-to-sequence modellerini təhsil etmək təbiidir. Lakin, sequence-to-sequence nəsillərin (y a da strukturlu təsirlərin) ən y a x şısını x verilən y-ni tapmaq məqsədilədir, və hər işin öz düşük-düşük metrik R vardır ki modellərin çıxışını y*: R(y, y*.x ilə qarşılaşdırılır. Biz ümid edirik ki, yoğunluq değerlendirməsindən daha yaxşı bir modellər də a şağı metrikdə yaxşı işlər edir, həqiqətən, bu bağlantı seçmə nəsillərinin işləri üçün təhsil edilməmişdir. Bu kağızda, beş maşına çevirilmiş işlərdə bir neçə yoxluq hesablayıcını salıb, çoxlu modellərin səviyyələrinin və BLEU ehtimalına dayanan modellərin səviyyələrinin müqayisədə çoxlu dəyişikliyini görürük. Əvvəlcə, bir ailədə modelləri düşündüyümüz zaman çox çox BLEU ilə bağlı olaraq (həmçinin autoregressiv modelləri, ya da keçmişdəkilərin eyni parameterizləri ilə latent dəyişiklik modelləri). Ancaq biz müxtəlif ailələr arasında modellərin səviyyələri arasında heç bir bağlılığı görmürük: (1) autoregressiv olmayan latent modellərin arasında, fleksibil əvvəlki dağıtım yoxluq değerində daha yaxşıdır, amma nəsillərin basit əvvəlki modellərdən daha kötüsünü verir, və (2) autoregressiv modellərin\nƏn yaxşı tercümə performansını təbliğ et, lakin latent dəyişiklik modelləri, əvvəl normalizasyon axışı ilə, bütün veri qurğularının ən yüksək mümkünlüyünü verir. Bu yüzden, hızlı nəsil sürəti istədiyi zaman, latent dəyişiklik olmayan autoregressiv modeli üçün basit bir əvvəl istifadə etməyi tavsiye edirik.', 'bs': 'Mnogi zadatak generacije sekvence do sekvence, uključujući prevod strojeva i tekst do govora, mogu se postaviti kao procjena gustoće proizvoda y s obzirom na ulaz x: p(y.x). S obzirom na ovu interpretaciju, prirodno je procijeniti modele sekvence do sekvence koristeći uvjetnu vjerojatnost log a na testu. Međutim, cilj generacije sekvence-do-sekvence (ili strukturirane predviđanja) je pronaći najbolji izlaz y dajući ulaz x, a svaki zadatak ima svoj vlastiti niz metrički R koji rezultira izlaz modela uspoređujući s setom referencija y*: R(y, y*.x). Iako se nadamo da model koji nadmaže procjenu gustine također dobro izvršava na donjem metriku, tačna korelacija nije proučena zbog zadataka generacije sekvence. U ovom papiru, uspoređujući nekoliko procjena gustine na pet zadataka za prevod mašine, smatramo da se povezanost između redova modela baziranog na log-verovatnosti i BLEU značajno razlikuje ovisno o rasponu modelnih porodica u usporedbi. Prvo, vjerojatnost dnevnika je vrlo povezana sa BLEU kada razmatramo modele unutar iste porodice (npr. autoregresivne modele, ili latentne varijantne modele sa istim parameterizacijom prethodnog). Međutim, mi ne posmatramo korelaciju između redova modela u različitim porodicama: 1) među modelima ne autoregresivnih latentnih promjena, fleksibilna prethodna distribucija je bolja za procjenu gustosti, ali daje gore kvalitet generacije nego jednostavno ranije, i (2) autoregresivne modele\nponudite najbolje učinkovito prevođenja ukupno, dok latentni varijantni modeli sa normalizacijskim tokom prije daju najveću vjerojatnost izvršenog izveštaja u svim podacima. Stoga preporučujemo korištenje jednostavnog ranije za latentnu promjenu ne autoregresivnog model a kada se žele brzina generacije.', 'cs': 'Mnoho úloh generování sekvence na sekvenci, včetně strojového překladu a text-to-Speech, lze představit jako odhad hustoty výstupu y při vstupu x: p(y|x). Vzhledem k této interpretaci je přirozené vyhodnocovat modely sekvence na sekvenci pomocí podmíněné log-pravděpodobnosti na testovací sadě. Cílem generování sekvence na sekvenci (nebo strukturované predikce) však je najít nejlepší výstup y při vstupu x a každý úkol má svou vlastní následnou metriku R, která skóruje výstup modelu porovnáním se sadou referencí y*: R(y, y*, x). Zatímco doufáme, že model, který vyniká v odhadu hustoty, také funguje dobře na následné metrice, přesná korelace nebyla studována pro úlohy generování sekvencí. V tomto článku, porovnáním několika odhadů hustoty na pěti úlohách strojového překladu, zjišťujeme, že korelace mezi hodnocením modelů založenými na log-pravděpodobnosti a BLEU se výrazně liší v závislosti na rozsahu porovnávaných modelových rodin. Za prvé, log-pravděpodobnost je vysoce korelována s BLEU, pokud bereme v úvahu modely ve stejné rodině (např. autoregresivní modely nebo latentní proměnné modely se stejnou parametrizací jako předchozí). Nicméně, nezaznamenáváme žádnou korelaci mezi žebříčky modelů napříč různými rodinami: (1) mezi non-autoregresivními latentními proměnnými modely je flexibilní předchozí distribuce lepší při odhadu hustoty, ale poskytuje horší kvalitu generace než jednoduchý prior, a (2) autoregresivní modely\nnabízejí celkově nejlepší překladový výkon, zatímco latentní proměnné modely s normalizujícím tokem předtím poskytují nejvyšší pravděpodobnost odhlášení přes všechny datové sady. Proto doporučujeme použít jednoduchý předpis pro latentní variabilní non-autoregresivní model, pokud je požadována rychlá generační rychlost.', 'am': 'የመረጃ ትርጉም እና የጽሑፍ-ለንግግር እና የውጤት ውጤት ውጤት ቁጥጥር በቁጥጥር ይቆጥራል፡፡ ይህንን ትርጓሜ በተሰጠ፣ የሥርዓት-ወደ-sequence models በተፈተና ላይ የሞክራዊ የlog-ምናልባት በመጠቀም ያስተካክል ነው፡፡ ምንም እንኳን የsequence-to-sequence ትውልድ (or structured prediction) የእርሱን የውጤት ውጤት መግኘት ነው፣ ለሁሉም ስራ የራሱ የውጤት ሜትሪክ R ነው፡፡ y* - y* - x በብርቱ ድምፅ ላይ የሚሻል ምሳሌ ከውኃው ማተሚያ ደግሞ መልካም እንዲሠራ ተስፋ እናደርጋለን፣ የእውነቱ ግንኙነት ለትውልድ ትውልድ ስራ አልተማረም፡፡ በዚህ ካላት፣ አምስት መሣሪያዎች ትርጉም ስራዎችን በማስተካከል ብዙዎች ድጋፍ፣ የሞዴላዎች ግንኙነት እና በቢሌዩም በሞዴል ቤተሰቦች ላይ በተያያያየው ግንኙነት በትክክል ይለያያል፡፡ መጀመሪያ፣ የlog-ምናልባት ከBLEU ጋር እጅግ ተገናኝቷል፡፡ ምንም እንኳን፣ በተለዩ ቤተሰቦች መካከል የሞላት ግንኙነት ምንም አናይም::\nየተመረጠውን ትርጉም ማድረግ በሙሉ ያቀረቡ፤ በተቃውሞ የተለየ ሞዴላዎች በተመሳሳይ ፈሳሽ አስቀድሞ ከዳታ ማህበረሰብ ሁሉ በላይ የተመሳሳይ የlog-ምናልባት ይሰጣል፡፡ ስለዚህም የፍጥነት ትውልድ ፈጥኖ በተፈለገ ጊዜ የተለየ ለራሱ-ሥልጣን ሞዴል ሳይሆን ቀላል ቀድሞ ለመጠቀም እንመክራለን፡፡', 'bn': 'অনেক প্রজন্মের কাজ, যার মধ্যে রয়েছে মেশিন অনুবাদ এবং টেক্সট-থেকে বক্তৃতার মধ্যে, ইনপুট এক্স: p( y) প্রদান করা আউটপুটের গভীর হিসেবে হিসেবে প এই ব্যাখ্যা দিয়ে এটা স্বাভাবিক যে পরীক্ষার সেটে সেকেন্স-থেকে সেকেন্স মোডেল ব্যবহার করে সংক্রান্ত ব্যবস্থা ব্যবহার কিন্তু প্রজন্মের উদ্দেশ্য হচ্ছে ইনপুট দেওয়া সবচেয়ে ভালো আউটপুটের প্রজন্ম (অথবা কাঠামো ভবিষ্যৎবাণী) খুঁজে বের করা যায়, আর প্রত্যেক ক কাজের নিজের নিজের নিজস্ব প্রান্তের মে যদিও আমরা আশা করি যে একটি মডেল যা গভীর হিসেবে বেশী প্রাপ্ত হয়েছে তা নিচের নদীর মেট্রিকে ভালোভাবে প্রকাশ করেছে, কিন্তু সেকেন্ড প্রজন্মের ক এই পত্রিকায়, পাঁচটি মেশিন অনুবাদের কাজের উপর কয়েকটি গুরুত্বপূর্ণ হিসেব গুরুত্বপূর্ণ হিসেবের মাধ্যমে আমরা দেখতে পাচ্ছি যে মডেলের রান্নাকারীদের মধ্যে যে সম প্রথমত, লোগ-সম্ভাবনা বিলুর সাথে অত্যন্ত সংশ্লিষ্ট যখন আমরা একই পরিবারের মধ্যে মডেল বিবেচনা করি (উদাহরণস্বয়ংক্রিয় মডেল, অথবা সাম্প্রতিক পরিবর্তনের মডে তবে আমরা বিভিন্ন পরিবারের মধ্যে মডেলের রাঙ্কের মাঝে কোন সম্পর্ক দেখতে পাই না: (১) স্বয়ংক্রিয়ভাবে সাম্প্রতিক পরিবর্তনের মডেলের মধ্যে একটি ফ্লিক্সিয়াল বিতরণের আগের গু\nসবচেয়ে ভালো অনুবাদ প্রদর্শনের প্রচারণা প্রদান করুন, যদিও সাম্প্রতিক ভেরিয়েবল মডেল স্বাভাবিক প্রবাহের পূর্বে স্বাভাবিক প্রব Therefore, we recommend using a simple prior for the latent variable non-autoregressive model when fast generation speed is desired.', 'fi': 'Monien sekvenssin ja sekvenssin generointitehtävien, kuten konekääntämisen ja tekstin puheeksi muuntamisen, voidaan esittää arvioivan tulosteen y tiheyttä syötteellä x: p(y|x). Tämän tulkinnan perusteella on luonnollista arvioida sekvenssimalleja käyttämällä ehdollista log-todennäköisyyttä testisarjassa. Sekvenssin ja sekvenssin välisen generoinnin (tai strukturoidun ennusteen) tavoitteena on kuitenkin löytää paras tuotos y syötteellä x, ja jokaisella tehtävällä on oma loppupään metriikkansa R, joka tuottaa mallin tuotoksen vertaamalla viitesarjaan y*: R(y, y* | x). Vaikka toivomme, että tiheyden estimoinnissa erinomainen malli toimii hyvin myös loppupään metriikassa, tarkkaa korrelaatiota ei ole tutkittu sekvenssien luontitehtävissä. Vertailemalla useita tiheyden estimaattoreita viidessä konekäännöstehtävässä havaitsimme, että log-todennäköisyyteen perustuvien mallien ja BLEU:n sijoitusten korrelaatio vaihtelee merkittävästi vertailtavien malliperheiden välillä. Ensinnäkin log-todennäköisyys korreloi voimakkaasti BLEU:n kanssa, kun tarkastelemme samaan perheeseen kuuluvia malleja (esim. autoregressiivisia malleja tai latentteja muuttujamalleja, joilla on sama parametrisointi kuin aiemmalla). Emme kuitenkaan havainneet korrelaatiota mallien sijoitusten välillä eri perheissä: (1) ei-autoregressiivisten latenttien muuttujamallien kohdalla joustava ennakkojakauma on parempi tiheyden estimoinnissa, mutta huonompi sukupolvilaatu kuin yksinkertainen aiempi, ja (2) autoregressiiviset mallit\ntarjoaa parhaan käännöstehokkuuden yleisesti ottaen, kun taas piilevät muuttujamallit, joissa on normalisoiva virtaus ennen, antavat suurimman lokin todennäköisyyden kaikissa tietosarjoissa. Siksi suosittelemme käyttämään yksinkertaista prioria piilevälle muuttujalle ei-autoregressiiviselle mallille, kun halutaan nopeaa generaationopeutta.', 'ca': "Moltes tasques de generació de seqüència a seqüència, incloent traducció de màquina i text a discurs, es poden posar com a estimació de la densitat de la producció y dada la entrada x: p(y[UNK]x). Tenint en compte aquesta interpretació, és natural avaluar models de seqüència a seqüència utilitzant probabilitats condicionals de registre en un conjunt de proves. Tanmateix, l'objectiu de la generació seqüència a seqüència (o predicció estructurada) és trobar la millor producció y dada una entrada x, i cada tasca té la seva pròpia R mètrica avall que puntueix una producció del model comparant-se amb un conjunt de referències y*: R(y, y* [UNK] x). Mentre esperem que un model que excel·li en l'estimació de densitat també funcioni bé en la mètrica avall, la correlació exact a no s'ha estudiat per a tasques de generació de seqüències. En aquest paper, comparant diverses estimadores de densitat en cinc tasques de traducció màquina, descobrim que la correlació entre les classificacions dels models basades en probabilitats de registre i BLEU varien significativament segons l'interval de les famílies models comparades. En primer lloc, la probabilitat de registre està molt correlacionada amb el BLEU quan considerem models dins la mateixa família (per exemple, models autoregressius o models variables latents amb la mateixa paràmetrització que la anterior). No obstant això, no observem correlació entre les classificacions dels models entre les diferents famílies: (1) entre els models latents i no autoregressius, una distribució anterior flexible és millor a la estimació de densitat però dóna una qualitat de generació pitjor que un model anterior simple, i (2) models autoregressius\nofereixen el millor rendiment de traducció en general, mentre que els models variables latents amb un flux de normalització anterior donen la més alta probabilitat de rexistruació de tots els conjunts de dades. Per tant, recomanem l'ús d'un simple anterior per al model variable latent no autoregressiu quan es desitge una velocitat de generació ràpida.", 'et': 'Paljud järjestuste genereerimise ülesanded, sealhulgas masintõlke ja teksti kõneks muutmine, võivad kujutada väljundi y tiheduse hindamiseks sisendi x järgi: p(y|x). Seda tõlgendust arvestades on loomulik hinnata jada-jada mudeleid, kasutades katsekogumil tingimuslikku logitõenäosust. Siiski on järjestuse genereerimise (või struktureeritud prognoosimise) eesmärk leida parim väljund y sisendi x korral ja igal ülesandel on oma allvoolu mõõdik R, mis hindab mudeli väljundit, võrreldes viitekogumiga y*: R(y, y* | x). Kuigi loodame, et tiheduse hindamisel silmapaistev mudel toimib hästi ka allvoolu mõõdikus, ei ole täpset korrelatsiooni jada genereerimise ülesannete puhul uuritud. Selles töös, võrreldes mitmeid tiheduse hindajaid viie masintõlketöö puhul, leiame, et korrelatsioon log-tõenäosusel põhinevate mudelite järjestuste ja BLEU vahel varieerub oluliselt sõltuvalt võrreldavate mudelite perekondade vahemikust. Esiteks on logaritmiline tõenäosus väga korrelatsioonis BLEU-ga, kui kaalume sama perekonna mudeleid (nt autoregressiivseid mudeleid või latentseid muutujamudeleid, mille parameetrid on sama kui eelmisel). Kuid me ei tähelda korrelatsiooni mudelite järjestuste vahel erinevate perekondade kaupa: (1) mitte-autoregressiivsete latentsete muutujate mudelite puhul on paindlik eelnev jaotus tiheduse hindamisel parem, kuid annab halvema generatsiooni kvaliteedi kui lihtsad eelnevad mudelid, ja (2) autoregressiivsed mudelid\npakuvad parimat tõlkimisjõudlust üldiselt, samas kui latentsed muutujate mudelid, mille voog on normaliseeritud, annavad kõigi andmekogumite puhul kõige suurema sisselogimise tõenäosuse. Seetõttu soovitame kasutada lihtsat eelvaadet latentse muutuja mittearregressiivse mudeli jaoks, kui soovitakse kiiret generatsioonikiirust.', 'ha': "Suna iya ƙayyade aikin masu ƙaramar-da-sauri, kamar fassarar maɓalli da kuma masu faɗi-zuwa-matsayi, za'a iya iya ƙayyade girma ga matsalar da aka bãyar da cikin cikin akwatin x: p(y).x). Idan an gaskata wannan fassarar, yana da amfani da shiryoyin-durowa zuwa-sequence masu amfani da shiryoyin log-inganci kan wata fitina. Babu kasa, jiyyan kizalin da aka saka (ko kuma an daidaita gaura) shine a gane shi mafi kyaun fitarwa wanda aka bai wa inki, kuma kõwane aikin yana da metric R wanda ke ƙara wata motel da yana sami'a da tsarin misalin y*: R(y, y* * - x). A lokacin da Muke tsammãni da wata misali wanda ya fi ƙaranci ga damƙara, yana aiki mai kyau a kan metric ƙarƙashin ruwa, kuma ba a karanta mazaunin da hakki ba ga aikin dangi na dabam. Ga wannan takardan, a sami da masu ƙidãya masu ƙaranci a kan aikin fassarar mashinaki shan, za'a gane da mazaunin tsakanin da suka yi danganta a kan masu tsari-zane-zane da BLEU yana daidaita mai girma, kuma yana daidaita ga tsarin iyãlanta misãlai da za'a sammenliki. First, log-likelihood is highly correlated with BLEU when we consider models within the same family (e.g. autoregressive models, or latent variable models with the same parameterization of the prior).  Babu, ko kuma ba mu ga wata danganci ba a tsakanin danganta masu motsi na daban-jama'a guda: (1) cikin misãlai masu motsi na daban-daban mai nuna ko-kandamati, wani fleksibo ne mafi alhẽri a kwanan rabin rabon nau'in, kuma yana bãyar da nau'in kizafi mafi girma daga birnin gaba ɗaya, da kuma (2) misãlai masu kandamatin fara-regressive\nKa motsa mafi kyaun fassarar aiki a jumla, da kuma motel masu variant na farko da shirin cire-mai normal gabanin ka bãyar da mafi kyauta-log-inganci duk tsarin database. Saboda haka, muna shawarar mu da amfani da wani abu mai sauƙi wa motel na daban-daban mai nuna-regressive, idan an tambaye haraka wa zaɓen tebur.", 'jv': 'Multiple Nanging titik-titik iki, lak wis rak dadi nggawe model sing sekènsi-to-sekènsi ning gambar condional log-likely nang ujian seten. politenessoffpolite"), and when there is a change ("assertivepoliteness Genjer dhéwé nambah kuwi model sing apik kesempalahan kanggo nggawe kesempalahan ngono nggawe barang kelas Metric, coralation sing diapakan ora bisa digaweh kanggo ngerasahan seneng operasi layarané Nang paper iki, nggawe gerarangke sampulan kanggo sampek dadi tanggal 5 nggo tarjamahan Sampeyan politenessoffpolite"), and when there is a change ("assertive\ntranslation Laptop" and "Desktop', 'sk': 'Številna opravila generiranja zaporedja v zaporedje, vključno s strojnim prevajanjem in besedilom v govor, se lahko predstavljajo kot ocena gostote izhoda y glede na vhod x: p(y|x). Glede na to razlago je naravno oceniti modele zaporedja do zaporedja z uporabo pogojne log verjetnosti na preskusnem nizu. Vendar pa je cilj generiranja zaporedja v zaporedje (ali strukturirane napovedi) poiskati najboljši izhod y ob upoštevanju vhodnega x, vsaka opravila pa ima svojo merilno mero R, ki ocenjuje rezultat modela s primerjavo z nizom referenc y*: R(y, y* | x). Čeprav upamo, da model, ki odlikuje pri ocenjevanju gostote, uspešno deluje tudi pri metričnem merilu, natančna korelacija ni bila raziskana za naloge generiranja zaporedja. V tem prispevku smo s primerjavo več ocenjevalcev gostote pri petih nalogah strojnega prevajanja ugotovili, da se korelacija med razvrstitvami modelov na podlagi log verjetnosti in BLEU bistveno razlikuje glede na obseg primerjanih družin modelov. Prvič, log verjetnost je zelo korelacijska z BLEU, kadar upoštevamo modele znotraj iste družine (npr. avtoregresivni modeli ali latentni variabilni modeli z isto parametrizacijo kot prejšnji). Vendar pa ne opažamo nobene korelacije med razvrstitvami modelov v različnih družinah: (1) pri neavtoregresivnih latentnih variabilnih modelih je fleksibilna predhodna porazdelitev boljša pri ocenjevanju gostote, vendar daje slabšo kakovost generacije kot preprosti predhodni modeli, in (2) avtoregresivni modeli\nzagotavljajo najboljšo učinkovitost prevajanja na splošno, medtem ko latentni modeli spremenljivk s pretokom normalizacije pred tem zagotavljajo največjo verjetnost odjave v vseh naborih podatkov. Zato priporočamo uporabo preprostega prejšnjega modela za latentno spremenljivko ne-avtoregresivno, kadar je želena hitra generacijska hitrost.', 'bo': 'Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y.x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y given an input x, and each task has its own downstream metric R that scores a model output by comparing against a set of references y*: R(y, y* .x). While we hope that a model that excels in density estimation also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. ཤོག་བྱང་འདིའི་ནང་དུ་རྩིས་འཁོར་གཞུང་གི་ཚད་རྩིས་ཅན་མི་འདྲ་ཞིག་དང་མཉམ་དུ་མཐུན་པ་ཡིན། First, log-likelihood is highly correlated with BLEU when we consider models within the same family (e.g. autoregressive models, or latent variable models with the same parameterization of the prior). Do not translate the keyword between brackets (e.g. ServerName, ServerAdmin, etc.) འོན་ཀྱང་། ང་ཚོར་བཅས་ཚང་མི་འདྲ་བར་གྱི་རིམ་པ་ལས་དབྱེ་རིམ་མཐུན་གྱི་མིག་ལམ་ལ་མཐུན་རྐྱེན་མེད།\nརྗེས་མའི་འགྱུར་ཅན་གྱི་དཔེ་དབྱིབས་ཡོད་ཚད་ལྟ་བུའི་སྐྱེས་སྐར་ཡིག དེར་བརྟེན།', 'he': 'משימות רבות של יוצר רצף-לרצף, כולל תרגום מכונת וטקסט-לנאום, יכולות להתייצב כהערכה של צפיפות ההוצאה y בהתחשב בהוצאה x: p( y[UNK]x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set.  בכל אופן, המטרה של יוצר רצף לרצף (או חזיון מבוסס) היא למצוא את ההוצאה הטובה ביותר y שנתן כניסה x, וכל משימה יש את R המטרי המטרי המאוחר שלה שמוציא תוצאה מודל על ידי השוואה נגד קבוצה של התייחסות y*: R( y, y* \uf0a5 x). למרות שאנחנו מקווים שמודל שמתפתח בערכת צפיפות גם מתבצע היטב במטרית התחתונה, התשובה המדויקת לא נבדקה עבור משימות יוצר רצף. בעיתון הזה, על ידי שיוות מספר מערכי צפיפות על חמישה משימות התרגום מכונות, אנו מוצאים שהקשר בין הדרגות של דוגמנים מבוססים על סבירות לוג-סבירות ולBLEU שונה באופן משמעותי תלוי בטווח של משפחות הדוגמנים שנשוואים. קודם כל, סבירות לוג-קושר מאוד עם BLEU כאשר אנו שוקלים דוגמנים בתוך אותה משפחה (למשל דוגמנים אוטומגרסיבים, או דוגמנים משתנים אוטומטיים עם אותו פרמטריזציה של קודם). בכל אופן, אנחנו שומרים על שום קשר בין הדרגות של דוגמנים ברחבי משפחות שונות: (1) בין דוגמנים לא אוטו-אוטו-אגרסיביים משתנים אוטו-אגרסיביים, פיצוי קודם גמיש הוא טוב יותר בערכת צפיפות, אבל נותן איכות דור גרועה יותר מאשר דוגמנים קודמים פשוטים, ו\nמציעים את ביצועי התרגום הטובים ביותר באופן כללי, בעוד מודלים משתנים מוסתרים עם זרם נורמלי קודם נותנים את הסיכוי הגבוה ביותר של היציאה מחזיקה בכל קבוצות נתונים. לכן, אנו ממליצים להשתמש בעבר פשוט למודל השתנה הלא-אוטומגרסיבי השתנה הלא-אוטומגרסיבי כשמהירות הדור המהירה מבוקשת.'}
{'en': 'Deeply Embedded Knowledge Representation & Reasoning For Natural Language Question Answering : A Practitioner’s Perspective', 'ar': 'تمثيل المعرفة واستنتاجها بشكل عميق للإجابة على أسئلة اللغة الطبيعية: وجهة نظر الممارس', 'fr': "Représentation des connaissances et raisonnement profondément ancrés pour répondre aux questions en langage naturel\xa0: le point de vue d'un praticien", 'pt': 'Representação e raciocínio de conhecimento profundamente incorporados para resposta a perguntas em linguagem natural: a perspectiva de um praticante', 'es': 'Representación y razonamiento del conocimiento profundamente arraigados para responder a preguntas en lenguaje natural: la perspectiva de un profesional', 'ja': '深く埋め込まれた知識表現と自然言語の質問に対する推論回答：実践者の視点', 'zh': '自然语言问答深层推理从业者论', 'hi': 'गहरी एम्बेडेड ज्ञान प्रतिनिधित्व और प्राकृतिक भाषा प्रश्न उत्तर के लिए तर्क: एक व्यवसायी का परिप्रेक्ष्य', 'ru': 'Глубоко укоренившееся представление знаний и обоснование ответа на вопрос о естественном языке: точка зрения практикующего врача', 'ga': 'Eolas Leabaithe go Doimhneach Léiriú & Réasúnaíocht do Theanga Nádúrtha Freagra Ceist: Dearcadh an Chleachtóra', 'el': 'Βαθιά ενσωματωμένη αναπαράσταση γνώσης για την απάντηση ερωτήσεων φυσικής γλώσσας: Η προοπτική ενός πρακτικού', 'ka': 'მნიშვნელოვანი მეცნიერების გამოსახულება და მიზეზი სახელის კითხვების პასუხისთვის: პრექტიფიკაციის პრექტიფიკაცია', 'kk': "Тіпті ендірілген білім кескіншісі және табиғи тіл сұрақтарының жауап беру үшін себептері: A practitioner's perspective", 'it': 'Rappresentazione profonda della conoscenza e ragionamento per il linguaggio naturale Risposta alle domande: la prospettiva di un praticante', 'lt': 'Išsamiai įdiegtos žinios atstovavimas ir gamtos kalbos pagrindimas atsakymas į klausimus: praktikanto perspektyva', 'hu': 'Mélyen beágyazott tudás reprezentáció és érvelés a természetes nyelv kérdéseinek megválaszolása: A gyakorló perspektívája', 'ms': 'Perwakilan Pengetahuan dan Pengesanan Keadaan Dalam-Dalam Untuk Pertanyaan Bahasa Biasa Jawapan: Perspektif Penglatih', 'mn': 'Магадгүй хэлний асуултын хариултын тулд гүн гүнзгий нэвтрэгдсэн мэдлэг төлөөлөл болон шалтгаан: Сургуульчийн Перспектив', 'mk': 'Длабоко вградена претстава на знаење и причина за природниот јазик Одговор на прашањата: Перспектива на практикот', 'pl': 'Głęboko osadzona reprezentacja wiedzy a uzasadnianie odpowiedzi na pytania języka naturalnego: perspektywa praktyka', 'mt': "Rappreżentazzjoni u Raġuni tal-Għarfien Integrat fil-fond għal Mistoqsijiet dwar il-Lingwa Naturali: Perspettiva ta' Prattikant", 'ro': 'Reprezentarea profundă a cunoștințelor încorporate și raționarea pentru limbajul natural Răspunsul la întrebări: Perspectiva unui practicant', 'ml': "ആഴത്തിലുള്ള അറിവ് പ്രതിനിധിയും സ്വാഭാവിക ഭാഷയുടെ ചോദ്യം ഉത്തരം: A practitioner's perspective", 'no': 'Dette innebygd kjennomrepresentasjon og grunn for å svara på naturspråk: Perspektiv for ein praktiserar', 'sr': 'Duboko uključeni predstavnik znanja i razlog za odgovor na pitanje prirodnog jezika: perspektiva praktičnika', 'si': 'ස්වභාවික භාෂාව ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් වෙනුවෙන් ගොඩක් සම්බන්ධ විද්\u200dයානය සහ හේතුව ප්\u200dරශ්නයක් ප්\u200dරශ', 'sv': 'Djupt inbäddad kunskapsrepresentation och resonemang för naturligt språk Fråga Svar: En utövares perspektiv', 'ta': 'ஆழமாக உட்பொதிந்த அறிவிப்பு பிரதிநிதியும் இயல்பான மொழி கேள்விக்கான காரணமும்: ஒரு பயிற்சியாளரின் முன்னோட்டம்', 'so': "Xuquuqda aqoonta hoose u soo geliyey iyo sababta su'aalaha afka asalka ah: A Practitioner's Perspective", 'ur': "سیدھے اندھے پیدا ہوئے علم Representation & Reasoning for Natural Language Question Answering: A Practitioner's Perspective", 'uz': 'Name', 'vi': 'Bố trí hiểu biết s âu sắc... Phụ trách câu hỏi về ngôn ngữ tự nhiên:', 'bg': 'Дълбоко вградено знание Представяне и мотивиране за естествения език Отговори на въпроси: Перспективата на практикуващия', 'da': 'Dybt indlejret vidensrepræsentation og begrundelse for naturligt sprog Spørgsmål besvarelse: En praktiserens perspektiv', 'nl': 'Diep ingebedde kennisrepresentatie voor het beantwoorden van natuurlijke taalvragen: het perspectief van een beoefenaar', 'id': 'Pertanyaan Pertanyaan Bahasa Alam Diperkembangkan Dalam Pertanyaan: Perspektif Praktikant', 'hr': 'Duboko uključeni predstavnik znanja i razlog odgovora na pitanje prirodnog jezika: perspektiva učitelja', 'de': 'Tief eingebettete Wissensrepräsentation zur Begründung natürlicher Sprachfragen: Die Perspektive eines Praktikers', 'sw': "Representation of Knowledge Embedded & Ready for Question of Asili: A Practitioner's perspective", 'ko': '자연 언어 문답의 심층 지식 표시와 추리: 실천자의 시각', 'tr': 'Taryh diller soragy jogaplamak üçin derin daşarylan Bilim Namaýyşy & Sebepleri: A Praktikçinin Perspektive', 'sq': 'Përfaqësues dhe arsyetimi i njohurive të thella për pyetjen e gjuhës natyrore: Perspektiva e një praktikanti', 'am': 'አቀራቢያ', 'az': "T…ôbi…ôtli dil suallarƒ±nƒ±n cavabƒ±: A Practitioner's Perspective", 'hy': "Գիտության խորը ներգրավված ներկայացումը և պատճառը բնական լեզվի հարցերի պատասխանը: A PRACTICER's", 'bn': 'গভীরভাবে আটকে পড়া জ্ঞান প্রতিনিধি এবং প্রাকৃতিক ভাষার প্রশ্নের উত্তর: একজন প্রশিক্ষকের দৃষ্টিভঙ্গি', 'ca': "Deeply Embedded Knowledge Representation & Reasoning For Natural Language Question Answering: A Practitioner's Perspective", 'cs': 'Hluboce vložené znalosti reprezentace a odůvodnění otázek přirozeného jazyka: perspektiva praktika', 'fa': 'نماینده\u200cی دانش و دلیل جواب سوال زبان طبیعی عمیقاً درگیر شده: یک نماینده\u200cی تمرین\u200cکننده', 'af': "Deep ingesluit kennis voorstel en rede vir Natuurlike taal vraag antwoord: ' n Praktiseerder se Perspektief", 'et': 'Sügavalt manustatud teadmiste esindamine ja loodusliku keele põhjendamine Küsimusele vastamine: Praktiku perspektiiv', 'fi': 'Syvälle sulautettu tietämys edustaminen ja järkeily luonnolliselle kielelle Kysymyksiin vastaaminen: Harjoittajan näkökulma', 'bs': 'Duboko uključeni predstavnik znanja i razlog za odgovor na pitanje prirodnog jezika: perspektiva učitelja', 'jv': 'deep', 'sk': 'Globoko vgrajeno znanje Reprezentacija in razlogi za naravni jezik Odgovor na vprašanja: Perspektiva praktika', 'ha': '@ action', 'bo': "Deeply Embedded Knowledge Representation & Reasoning For Natural Language Question Answering: A Practitioner's Perspective", 'he': 'מייצג ידע מעורב ומגיון לשפה טבעית תשובה: Perspective של המאמן'}
{'en': 'Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser. Even though several projects have been launched in the pursuit of developing a universal meaning representation language, the existence of an accurate universal parser is far from reality. This has severely limited the application of knowledge representation and reasoning (KR) in the field of  NLP  and also prevented a proper evaluation of KR based NLU systems. Our goal is to build KR based systems for  Natural Language Understanding  without relying on a  parser . Towards this we propose a method named Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) where we replace the parser by a  neural network , soften the symbolic representation so that a deterministic mapping exists between the parser neural network and the interpretable logical form, and finally replace the symbolic solver by an equivalent  neural network , so the model can be trained end-to-end. We evaluate our method with respect to the task of Qualitative Word Problem Solving on the two available datasets (QuaRTz and QuaRel). Our system achieves same  accuracy  as that of the state-of-the-art  accuracy  on QuaRTz, outperforms the state-of-the-art on QuaRel and severely outperforms a traditional KR based system. The results show that the bias introduced by a KR solution does not prevent it from doing a better job at the end task. Moreover, our  method  is interpretable due to the bias introduced by the KR approach.', 'ar': 'التطبيق الناجح لتمثيل المعرفة والاستدلال (KR) في فهم اللغة الطبيعية (NLU) مقيد إلى حد كبير بتوافر محلل لغة طبيعي قوي وعامة الأغراض. على الرغم من إطلاق العديد من المشاريع في السعي لتطوير لغة تمثيل المعنى العالمي ، إلا أن وجود محلل عالمي دقيق بعيد كل البعد عن الواقع. وقد حد هذا بشدة من تطبيق تمثيل المعرفة والاستدلال (KR) في مجال البرمجة اللغوية العصبية وكذلك منع التقييم المناسب لأنظمة NLU القائمة على KR. هدفنا هو بناء أنظمة قائمة على KR لفهم اللغة الطبيعية دون الاعتماد على المحلل اللغوي. لتحقيق هذا ، نقترح طريقة تسمى Deeply Embedded Knowledge Fairation & Reasoning (DeepEKR) حيث نستبدل المحلل اللغوي بشبكة عصبية ، ونخفف التمثيل الرمزي بحيث يوجد رسم حتمي بين الشبكة العصبية المحلل والشكل المنطقي القابل للتفسير ، وأخيراً نستبدل الحل الرمزي بواسطة شبكة عصبية مكافئة ، لذلك يمكن تدريب النموذج من طرف إلى طرف. نقوم بتقييم طريقتنا فيما يتعلق بمهمة حل مشكلة الكلمات النوعية على مجموعتي البيانات المتاحتين (QuaRTz و QuaRel). يحقق نظامنا نفس الدقة التي تتمتع بها الدقة المتطورة في QuaRTz ، ويتفوق على أحدث التقنيات في QuaRel ويتفوق بشدة على النظام التقليدي القائم على KR. تظهر النتائج أن التحيز الذي يقدمه حل KR لا يمنعه من القيام بعمل أفضل\nوظيفة في نهاية المهمة. علاوة على ذلك ، فإن طريقتنا قابلة للتفسير بسبب التحيز الذي قدمه نهج KR.', 'fr': "L'application réussie de la représentation et du raisonnement des connaissances (KR) dans la compréhension du langage naturel (NLU) est largement limitée par la disponibilité d'un analyseur de langage naturel robuste et polyvalent. Même si plusieurs projets ont été lancés dans le but de développer un langage de représentation de sens universel, l'existence d'un analyseur universel précis est loin d'être une réalité. Cela a fortement limité l'application de la représentation et du raisonnement des connaissances (KR) dans le domaine de la PNL et a également empêché une évaluation correcte des systèmes NLU basés sur la KR. Notre objectif est de créer des systèmes basés sur KR pour la compréhension du langage naturel sans dépendre d'un analyseur. Pour cela, nous proposons une méthode appelée DeepEkr (DeepEkr), dans laquelle nous remplaçons l'analyseur par un réseau de neurones, adoucissons la représentation symbolique afin qu'un mappage déterministe existe entre le réseau de neurones de l'analyseur et la forme logique interprétable, et enfin, nous remplaçons le solveur symbolique par un réseau de neurones équivalent, de sorte que le modèle puisse être entraîné de bout en bout. Nous évaluons notre méthode par rapport à la tâche de résolution qualitative des problèmes de mots sur les deux ensembles de données disponibles (Quartz et QuareL). Notre système atteint la même précision que la précision de pointe sur Quartz, surpasse les performances de pointe sur QuareL et surpasse largement un système traditionnel basé sur KR. Les résultats montrent que le biais introduit par une solution KR ne l'empêche pas de faire mieux\ntravail à la fin de la tâche. De plus, notre méthode est interprétable en raison du biais introduit par l'approche KR.", 'es': 'La aplicación exitosa de Representación y Razonamiento del Conocimiento (KR) en la Comprensión del Lenguaje Natural (NLU) está limitada en gran medida por la disponibilidad de un analizador de lenguaje natural robusto y de uso general. A pesar de que se han lanzado varios proyectos con el objetivo de desarrollar un lenguaje de representación de significados universal, la existencia de un analizador universal preciso está lejos de ser realidad. Esto ha limitado gravemente la aplicación de la representación del conocimiento y el razonamiento (KR) en el campo de la PNL y también ha impedido una evaluación adecuada de los sistemas de NLU basados en KR. Nuestro objetivo es crear sistemas basados en KR para la comprensión del lenguaje natural sin depender de un analizador. Para ello, proponemos un método llamado Representación y Razonamiento del Conocimiento Profundamente Embebido (DeepeKR) en el que reemplazamos el analizador sintáctico por una red neuronal, suavizamos la representación simbólica para que exista un mapeo determinista entre la red neuronal del analizador sintáctico y la forma lógica interpretable, y finalmente reemplazamos el solucionador simbólico mediante una red neuronal equivalente, por lo que el modelo se puede entrenar de extremo a extremo. Evaluamos nuestro método con respecto a la tarea de Resolución Cualitativa de Problemas de Palabras en los dos conjuntos de datos disponibles (QuArtZ y QuArel). Nuestro sistema logra la misma precisión que la precisión de última generación de QuArtZ, supera al de QuArel de última generación y supera con creces a un sistema tradicional basado en KR. Los resultados muestran que el sesgo introducido por una solución de KR no impide que funcione mejor\ntrabajo al final de la tarea. Además, nuestro método es interpretable debido al sesgo introducido por el enfoque KR.', 'pt': 'A aplicação bem-sucedida de Representação e Raciocínio do Conhecimento (KR) no Entendimento de Linguagem Natural (NLU) é amplamente limitada pela disponibilidade de um analisador de linguagem natural robusto e de propósito geral. Embora vários projetos tenham sido lançados na busca do desenvolvimento de uma linguagem de representação de significado universal, a existência de um analisador universal preciso está longe da realidade. Isso limitou severamente a aplicação da representação e raciocínio do conhecimento (KR) no campo da PNL e também impediu uma avaliação adequada dos sistemas NLU baseados em KR. Nosso objetivo é construir sistemas baseados em KR para compreensão de linguagem natural sem depender de um analisador. Para isso propomos um método denominado Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) onde substituímos o parser por uma rede neural, suavizamos a representação simbólica para que exista um mapeamento determinístico entre a rede neural do parser e a forma lógica interpretável, e finalmente substituímos o solucionador simbólico por uma rede neural equivalente, para que o modelo possa ser treinado de ponta a ponta. Avaliamos nosso método com relação à tarefa de resolução qualitativa de problemas de palavras nos dois conjuntos de dados disponíveis (QuaRTz e QuaRel). Nosso sistema atinge a mesma precisão que a precisão de última geração do QuaRTz, supera o desempenho de última geração do QuaRel e supera severamente um sistema tradicional baseado em KR. Os resultados mostram que o viés introduzido por uma solução de KR não a impede de fazer uma melhor\ntrabalho na tarefa final. Além disso, nosso método é interpretável devido ao viés introduzido pela abordagem KR.', 'ja': 'ナチュラルランゲージ理解（ NLU ）における知識表現と推論（ KR ）の成功の応用は、堅牢で汎用的な自然言語パーサーの利用可能性によってほとんど制限されている。 普遍的な意味表現言語の開発を追求していくつかのプロジェクトが立ち上げられているにもかかわらず、正確な普遍的構文解析器の存在は現実とはかけ離れている。 これは、NLPの分野における知識表現と推論（ KR ）の適用を著しく制限し、KRベースのNLUシステムの適切な評価も妨げている。 私たちの目標は、パーサーに依存せずに自然言語を理解するためのKRベースのシステムを構築することです。 これに向けて、Deeply Embedded Knowledge Representation &amp; Reasoning （ DeepEKR ）と呼ばれる方法を提案します。この方法では、構文解析器をニューラルネットワークに置き換え、構文解析器ニューラルネットワークと解釈可能な論理形式との間に決定論的マッピングが存在するように記号的表現を柔らかくし、最後に記号的ソルバーを等価なニューラルネットワークに置き換えて、モデルをエンドツーエンドでトレーニングすることができます。 私たちは、2つの利用可能なデータセット（ QuaRTzとQuaRel ）の定性的な単語問題解決のタスクに関して、私たちの方法を評価します。 当社のシステムは、QuaRTzの最先端の精度と同等の精度を実現し、QuaRelの最先端の精度を上回り、従来のKRベースのシステムを大きく上回ります。 その結果、KRソリューションが導入したバイアスは、KRソリューションの改善を妨げるものではないことが示された。\nタスクの最後にジョブを表示します。また、KRアプローチが導入したバイアスにより、当社の手法は解釈可能である。', 'zh': '知见推理(KR)于自然语言解(NLU)成功宜于强大通用之自然语言解析器可用性之限。 虽开一义以动数目,而一解析器之存远非。 此甚限知理(KR)NLP域之用,而止KR之NLU统也。 构建基于KR自然语言解统,而不赖于解析器。 为言Deep Embedded Knowledge Representation & Reasoning(DeepEKR)之法,以神经网络易解析器,软化符号示之,以映射于解析器神经网络、逻辑之间,最后以等效神经网络替换符号求解器,故其形可端到端也。 二者可以数集(QuaRTz与QuaRel)上之定性单词,以论吾法。 吾统得与QuaRTz上最先进之精同,优于QuaRel上之最先进,而优于KR之旧统。 结果表明,KR解决方案引入偏差不妨为愈。\n事毕作业。 此外KR引入偏差,亦可解也。', 'ru': 'Успешное применение представления и рассуждения о знаниях (KR) в понимании естественного языка (NLU) в значительной степени ограничено наличием надежного и универсального анализатора естественного языка. Несмотря на то, что было начато несколько проектов по разработке универсального языка представления смысла, существование точного универсального парсера далеко от реальности. Это серьезно ограничило применение представления и аргументации знаний (KR) в области NLP, а также помешало надлежащей оценке систем NLU на основе KR. Наша цель - построить системы на основе KR для понимания естественного языка, не полагаясь на парсер. Для этого мы предлагаем метод под названием Deeply Embedded Knowledge Representation &amp; Reasoning (DeepEKR), где мы заменяем парсер нейронной сетью, смягчаем символическое представление, так что существует детерминированное отображение между нейронной сетью парсера и интерпретируемой логической формой, и, наконец, заменяем символический решатель эквивалентной нейронной сетью, так что модель может быть обучена сквозному. Мы оцениваем наш метод с точки зрения задачи качественного решения задач Word на двух доступных наборах данных (QuaRTz и QuaRel). Наша система достигает такой же точности, как и современная точность на QuaRTz, превосходит современную точность на QuaRel и значительно превосходит традиционную систему на основе KR. Результаты показывают, что смещение, введенное решением КР, не мешает ему добиться лучшего\nзадание в конце задачи. Более того, наш метод интерпретируем из-за погрешности, введенной подходом КР.', 'hi': 'प्राकृतिक भाषा समझ (एनएलयू) में ज्ञान प्रतिनिधित्व और तर्क (केआर) का सफल अनुप्रयोग काफी हद तक एक मजबूत और सामान्य उद्देश्य प्राकृतिक भाषा पार्सर की उपलब्धता से सीमित है। भले ही एक सार्वभौमिक अर्थ प्रतिनिधित्व भाषा विकसित करने की खोज में कई परियोजनाएं शुरू की गई हैं, एक सटीक सार्वभौमिक पार्सर का अस्तित्व वास्तविकता से बहुत दूर है। इसने एनएलपी के क्षेत्र में ज्ञान प्रतिनिधित्व और तर्क (केआर) के आवेदन को गंभीर रूप से सीमित कर दिया है और केआर आधारित एनएलयू प्रणालियों के उचित मूल्यांकन को भी रोक दिया है। हमारा लक्ष्य पार्सर पर भरोसा किए बिना प्राकृतिक भाषा समझ के लिए केआर आधारित प्रणालियों का निर्माण करना है। इस दिशा में हम डीपली एम्बेडेड नॉलेज रिप्रेजेंटेशन एंड रीजनिंग (डीपईकेआर) नामक एक विधि का प्रस्ताव करते हैं जहां हम पार्सर को एक तंत्रिका नेटवर्क द्वारा प्रतिस्थापित करते हैं, प्रतीकात्मक प्रतिनिधित्व को नरम करते हैं ताकि पार्सर तंत्रिका नेटवर्क और व्याख्यात्मक तार्किक रूप के बीच एक नियतात्मक मानचित्रण मौजूद हो, और अंत में प्रतीकात्मक सॉल्वर को एक समकक्ष तंत्रिका नेटवर्क द्वारा प्रतिस्थापित किया जा सके, इसलिए मॉडल को एंड-टू-एंड प्रशिक्षित किया जा सके। हम दो उपलब्ध डेटासेट (QuaRTz और QuaRel) पर गुणात्मक शब्द समस्या को हल करने के कार्य के संबंध में हमारी विधि का मूल्यांकन करते हैं। हमारी प्रणाली QuaRTz पर अत्याधुनिक सटीकता के समान सटीकता प्राप्त करती है, QuaRel पर अत्याधुनिक को मात देती है और गंभीर रूप से एक पारंपरिक केआर आधारित प्रणाली को मात देती है। परिणाम बताते हैं कि एक KR समाधान द्वारा पेश किया गया पूर्वाग्रह इसे बेहतर करने से नहीं रोकता है\nअंतिम कार्य में नौकरी। इसके अलावा, हमारी विधि केआर दृष्टिकोण द्वारा पेश किए गए पूर्वाग्रह के कारण व्याख्या योग्य है।', 'ga': 'Tá sé teoranta den chuid is mó go bhfuil fáil ar pharsálaí teanga nádúrtha atá láidir agus ginearálta ag baint le cur i bhfeidhm rathúil na hUiríoll Eolais agus na Réasúnaíochta (KR) sa Tuiscint ar Theanga Nádúrtha (NLU). Cé gur seoladh go leor tionscadal chun teanga léiriúcháin brí uilechoiteann a fhorbairt, níl parsálaí cruinn uilíoch i bhfad ó réaltacht. Chuir sé seo srian mór ar chur i bhfeidhm ionadaíochta eolais agus réasúnaíochta (KR) i réimse an NLP agus chuir sé cosc freisin ar mheastóireacht cheart ar chórais NLU bunaithe ar KR. Is é an sprioc atá againn córais KR-bhunaithe a thógáil le haghaidh Tuiscint Teanga Nádúrtha gan a bheith ag brath ar pharsálaí. Chuige seo molaimid modh darb ainm Léiriú & Réasúnú Eolais Leabaithe go Doimhneach (DeepEKR) áit a ndéanaimid gréasán néarúil a ionadú an pharsálaí, an léiriú siombalach a mhaolú ionas go mbeidh léarscáiliú cinntitheach ann idir gréasán na parsálaí agus an fhoirm loighciúil inmhínithe, agus ar deireadh athchur. an réiteoir siombalach trí líonra néarúil coibhéiseach, ionas gur féidir an tsamhail a oiliúint ó cheann ceann go ceann. Déanaimid ár modh a mheas maidir leis an tasc a bhaineann le Réiteach Cáilíochtúil Fadhbanna Focal ar an dá thacar sonraí atá ar fáil (QuaRTz agus QuaRel). Baineann ár gcóras an chruinneas céanna amach agus atá ag an gcruinneas úrscothach ar QuaRTz, sáraíonn sé an ceann is déanaí ar QuaRel agus sáraíonn sé córas traidisiúnta KR-bhunaithe go mór. Léiríonn na torthaí nach gcuireann an claonadh a tugadh isteach le réiteach KR cosc air níos fearr a dhéanamh\npost ag an tasc deiridh. Ina theannta sin, is féidir ár modh a léirmhíniú mar gheall ar an gclaonadh a thug an cur chuige KR isteach.', 'it': "L'applicazione di successo della Rappresentazione e Ragionamento della Conoscenza (KR) nella comprensione del linguaggio naturale (NLU) è ampiamente limitata dalla disponibilità di un parser di linguaggio naturale robusto e di uso generale. Anche se diversi progetti sono stati avviati nella ricerca di un linguaggio universale di rappresentazione del significato, l'esistenza di un parser universale accurato è lontana dalla realtà. Ciò ha fortemente limitato l'applicazione della rappresentazione e del ragionamento della conoscenza (KR) nel campo della PNL e ha anche impedito una corretta valutazione dei sistemi NLU basati su KR. Il nostro obiettivo è quello di costruire sistemi basati su KR per la comprensione del linguaggio naturale senza fare affidamento su un parser. Per questo proponiamo un metodo chiamato Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) dove sostituiamo il parser con una rete neurale, ammorbidiamo la rappresentazione simbolica in modo che esista una mappatura deterministica tra la rete neurale parser e la forma logica interpretabile, e infine sostituiamo il risolutore simbolico con una rete neurale equivalente, in modo che il modello possa essere addestrato end-to-end. Valutiamo il nostro metodo rispetto al compito di Qualitative Word Problem Solving sui due dataset disponibili (Quartz e QuaRel). Il nostro sistema raggiunge la stessa precisione di quella dell'accuratezza all'avanguardia su Quartz, supera lo stato dell'arte su QuaRel e supera gravemente un sistema basato su KR tradizionale. I risultati mostrano che il bias introdotto da una soluzione KR non impedisce di fare meglio", 'el': 'Η επιτυχής εφαρμογή της Γνωσιακής Αντιπροσωπείας και Λογιστικής (KR) στην Κατανόηση Φυσικής Γλώσσας (NLU) περιορίζεται σε μεγάλο βαθμό από τη διαθεσιμότητα ενός εύρωστου και γενικού σκοπού αναλυτή φυσικής γλώσσας. Παρόλο που έχουν ξεκινήσει αρκετά έργα με στόχο την ανάπτυξη μιας γλώσσας αναπαράστασης καθολικών εννοιών, η ύπαρξη ενός ακριβούς καθολικού αναλυτή απέχει πολύ από την πραγματικότητα. Αυτό έχει περιορίσει σημαντικά την εφαρμογή της αναπαράστασης και συλλογισμού της γνώσης (KR) στον τομέα της NLP και επίσης εμπόδισε την ορθή αξιολόγηση των συστημάτων NLU βασισμένων στην KR. Στόχος μας είναι να οικοδομήσουμε συστήματα για την κατανόηση της φυσικής γλώσσας χωρίς να βασιζόμαστε σε έναν αναλυτή. Προς το σκοπό αυτό προτείνουμε μια μέθοδο που ονομάζεται Βαθιά Ενσωματωμένη Αντιπροσώπευση Γνώσης, Λογιστική (όπου αντικαθιστούμε τον αναλυτή από ένα νευρωνικό δίκτυο, μαλακώνουμε τη συμβολική αναπαράσταση έτσι ώστε να υπάρχει μια ντετερμινιστική χαρτογράφηση μεταξύ του νευρικού δικτύου του αναλυτή και της ερμηνευόμενης λογικής μορφής, και τελικά αντικαθιστούμε τον συμβολικό επιλυτή από ένα ισοδύναμο νευρωνικό δίκτυο, έτσι το μοντέλο μπορεί να εκπαιδευτεί από άκρη σε άκρη. Αξιολογούμε τη μέθοδο μας σε σχέση με το έργο της ποιοτικής επίλυσης προβλημάτων λέξεων στα δύο διαθέσιμα σύνολα δεδομένων (Quartz και QuaRel). Το σύστημά μας επιτυγχάνει την ίδια ακρίβεια με αυτή της τελευταίας τεχνολογίας ακρίβειας στο Quartz, ξεπερνά την υπερσύγχρονη στο QuaRel και ξεπερνά σημαντικά ένα παραδοσιακό σύστημα βασισμένο σε KR. Τα αποτελέσματα δείχνουν ότι η προκατάληψη που εισάγεται από μια λύση KR δεν την εμποδίζει να κάνει καλύτερη', 'ka': 'მეცნიერების გამოსახულება და გამოსახულება (KR) ნახვადასხვა ენერგიის განსხვავებაში (NLU) ძალიან მუშაობელია ძალიან და სხვადასხვა საზოგადოებო ენერგიის პანსერტორის ხელსახულება. თუმცა რამდენიმე პროექტი გახსნა უნივერსალური განსაზღვრება ენაზე, მარტივი უნივერსალური პროექტირების არსებობა რეალობიდან უფრო დიდ. ეს მნიშვნელოვანია განსაზღვრება მეცნიერების განსაზღვრება და პარამენტირება (KR) NLP- ის პანელში და ასევე შეუძლებელია სწორი განსაზღვრება KR- ის განსაზღვრებული NLU სისტემების. ჩვენი მიზეზი არის KR-ის ბაზეული სისტემების შექმნარება თავისუფალური ენათის განსხვავებისთვის, ანუ პასუტერისთვის გადარჩენა. ჩვენ ამის მიხედვით მინდომენტი Deeply-ის შეყვარებული მეცნიერების გამოსახულება და გამოსახულება (DeepEKR) სადაც ჩვენ პანსერას ნეიროლური ქსელით დავცვლეთ, სიმბოლოგიური გამოსახულებას დამატებით, რომ პერსერის ნეიროლური ქსელის შორის განსახულებელი ფორმაზე ამიტომ მოდელის შესაძლებელია დასრულებული დასრულება. ჩვენ გავამუშავებთ ჩვენი მეთოდის კვალიტატიური სიტყვების პრობლემების გარეშე ორი ხელმისათვის მონაცემების კონფიგურაცია (QuaRTz და QuaRel). ჩვენი სისტემა იგივე წარმოდგენა, როგორც QuaRTz-ის წარმოდგენების წარმოდგენების წარმოდგენების წარმოდგენების წარმოდგენება QuaRel-ზე და ძალიან უფრო გავაკეთება ტრადიციონალური KR-ის ბაზაზე. წარმოდგენები ჩვენებს, რომ KR- ის გადაწყვეტილი წარმოდგენება უფრო უკეთესი გავაკეთება', 'mk': 'Успешната апликација на претставувањето на знаењето и размислувањето (KR) во Природното Разбирање на јазикот (NLU) е во голема мера ограничена од достапноста на силен и општ анализатор на природен јазик. Иако неколку проекти се започнати во потрага за развој на универзален јазик на претставување на значењето, постоењето на точен универзален анализатор е далеку од реалноста. Ова сериозно ја ограничи апликацијата на претставувањето на знаењето и размислувањето (KR) во областа на НЛП и, исто така, спречи соодветна проценка на системите на НЛУ базирани на КР. Нашата цел е да изградиме системи базирани на KR за разбирање на природниот јазик без да се потпираме на анализатор. Надвор кон ова предложуваме метод наречен Длабоко вграден знање претставување и размислување (DeepEKR) каде што го заменуваме анализаторот со нервна мрежа, го мекнуваме симболичното претставување за да постои децентристичко мапирање помеѓу нервната мрежа на анализаторот и интерпретабилната логична форма, и конечно го заменуваме симболичниот расчистувач со еквивалент За моделот да може да биде трениран од крај до крај. Го проценуваме нашиот метод во однос на задачата за квалитетно решавање на проблемите со зборовите на двата достапни податоци (QuaRTz и QuaRel). Нашиот систем постигнува иста точност како и оној на најсовремената точност на QuaRTz, го надминува најсовремениот систем на QuaRel и сериозно надминува традиционалниот систем базиран на KR. Резултатите покажуваат дека предрасудата воведена од решение на KR не го спречува тоа да направи подобро', 'hu': 'A Tudás Reprezentáció és Észlelés (KR) sikeres alkalmazását a természetes nyelv megértésében (NLU) nagyrészt korlátozza egy robusztus és általános célú természetes nyelv elemző rendelkezésre állása. Annak ellenére, hogy számos projekt indult az univerzális jelentésképviseleti nyelv kifejlesztésére, a pontos univerzális elemző létezése messze van a valóságtól. Ez súlyosan korlátozta a tudásmegjelenítés és érvelés (KR) alkalmazását az NLP területén, és megakadályozta a KR alapú NLU rendszerek megfelelő értékelését is. Célunk, hogy KR alapú rendszereket építsünk a természetes nyelv megértéséhez elemző nélkül. Ennek érdekében javasoljuk a DeepEKR (Deeply Embedded Knowledge Representation & Reasoning) elnevezésű módszert, ahol az elemzőt neurális hálózatra cseréljük, lágyítjuk a szimbolikus reprezentációt úgy, hogy determinisztikus leképezés létezik az elemző neurális hálózat és az értelmezhető logikai forma között, és végül a szimbolikus megoldót egyenértékű neurális hálózatra cseréljük. Így a modell végtől végig kiképezhető. Módszerünket a két rendelkezésre álló adathalmaz (Quartz és QuaRel) alapján értékeljük a minőségi szó problémamegoldás feladatának tekintetében. Rendszerünk ugyanolyan pontosságot ér el, mint a Quartz legkorszerűbb pontossága, felülmúlja a QuaRel legkorszerűbb pontosságát és súlyosan felülmúlja a hagyományos KR alapú rendszereket. Az eredmények azt mutatják, hogy a KR megoldás által bevezetett előrejelzés nem akadályozza meg, hogy jobb eredményeket végezzen', 'lt': 'Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser.  Nors buvo pradėti keli projektai siekiant sukurti universalią reikšmę atstovaujančią kalbą, tikslaus universalaus analizatoriaus egzistavimas yra toli nuo tikrovės. Tai griežtai apribojo žinių atstovavimo ir pagrįstumo taikymą NLP srityje ir taip pat trukdė tinkamai įvertinti KR grindžiamas NLU sistemas. Mūsų tikslas – sukurti KR pagrįstas gamtos kalbos supratimo sistemas nepasitikius analizatoriumi. Šiuo tikslu siūlome metodą, vadinamą giliai įdiegtos žinios atstovavimu ir pagrįstumu (DeepEKR), kuriame pakeičiame analizatorių nerviniu tinklu, švelniname simbolinį atstovavimą taip, kad tarp analizatoriaus nervinio tinklo ir aiškinamos loginės formos egzistuotų deterministinis žemėlapis, o simbolinis tirpiklis galiausiai pakeičiamas lygiaverčiu nerviniu tinklu, - kad modelis galėtų būti apmokytas nuo pabaigos iki pabaigos. Vertiname savo metodą, kiek tai susiję su kokybinio žodžio problemų sprendimo užduotimi dviejuose turimuose duomenų rinkiniuose (QuaRTz ir QuaRel). Mūsų sistema pasiekia tą patį tikslumą, kaip ir pažangiausio tikslumo QuaRTz atveju, viršija pažangiausią QuaRel sistemą ir smarkiai viršija tradicinę KR sistemą. Iš rezultatų matyti, kad KR sprendimu nustatytas nukrypimas netrukdo jam geriau', 'kk': 'Табиғи тілдерді түсініп (NLU) түсініктерінде білім представляциясы және түсініктер (KR) бағдарламасының сәтті қолдануы көбінде құпты және жалпы мақсатты тілдерді талдау мүмкіндігіне шект Бірнеше жобалар әлемдік мәлімет тілін жасау үшін жегілді, бірақ дұрыс әлемдік талдаушы шындығынан тыс. Бұл NLP өрісінде білім мәліметтерді түсіндіру мен түсіндіру (KR) қолданбаны шектеп, және КР негіздеген NLU жүйелерді дұрыс бағалау мүмкін емес. Біздің мақсатымыз - табиғи тілді ойлау үшін KR негізделген жүйелерді талдау үшін, талдаушына сеніміз жоқ. Бұл үшін біз Deeply ендірілген білім резентациясы мен түсініктерді (DeepEKR) деп аталатын әдісін ұсынамыз. Пансорды невралды желі арқылы ауыстыру үшін символдық түсініктерді теңдеу үшін, пансорлық невралды желі мен түсінікті логикалық пішімі арасындағы деңгейін Бұл үлгі соңғы соңғы жағдай оқылған болады. Қол жеткізетін деректер жиындарының (QuaRTz және QuaRel) тапсырмасына сапатты сөз мәселесін шешу арқылы әдімізді бағалаймыз. Біздің жүйеміз QuaRTz күйінің дәрежесінің дәрежесіне сәйкес жеткізеді, QuaRel жүйесінің күйінің дәрежесіне жеткізеді және әдімгі KR негіздеген жүйесіне жеткізеді. Нәтижелер KR шешімінен келтірілген тәртіпсіздігін жақсы жасау арқылы', 'ms': 'Aplikasi sukses Perwakilan Pengetahuan dan Pengakapan (KR) dalam Pemahaman Bahasa Semulajadi (NLU) adalah kebanyakan terbatas oleh kebebasan penghurai bahasa yang kuat dan tujuan umum. Walaupun beberapa projek telah dilancarkan dalam mengembangkan bahasa mewakili makna universal, wujud penghurai universal yang tepat jauh dari realiti. Ini telah mencegah penggunaan perwakilan pengetahuan dan alasan (KR) dalam medan NLP dan juga mencegah penilaian yang tepat sistem NLU berasaskan KR. Tujuan kita ialah membina sistem berasaskan KR untuk Pemahaman Bahasa Alami tanpa bergantung pada penghurai. Ke arah ini, kami cadangkan kaedah bernama Deep Embedded Knowledge Representation & Reasoning (DeepEKR) di mana kami menggantikan penghurai dengan rangkaian saraf, lembut perwakilan simbolik sehingga peta deterministik wujud antara rangkaian saraf penghurai dan bentuk logik yang boleh diterangkan, dan akhirnya menggantikan penyelesair simbolik dengan rangkaian saraf yang sama, Jadi model boleh dilatih akhir-akhir. Kami menilai kaedah kami terhadap tugas penyelesaian masalah perkataan kualitif pada dua set data yang tersedia (QuaRTz dan QuaRel). Our system achieves same accuracy as that of the state-of-the-art accuracy on QuaRTz, outperforms the state-of-the-art on QuaRel and severely outperforms a traditional KR based system.  Hasil menunjukkan bahawa bias yang diperkenalkan oleh penyelesaian KR tidak mencegahnya daripada melakukan lebih baik', 'no': 'Vellukkast bruk av kunnskape- representasjon og grunnlag (KR) i naturspråk- forståking (NLU) er stort begrenset av tilgjengeligheten av ein robust og generell naturspråk- tolkar. Selv om fleire prosjekt er starta i følgje til å utvikla ein universell betydning representasjonspråk, er det eksisterende for ein nøyaktig universell tolkar langt frå realiteten. Dette har kraftig begrenset programmet for kunnskapsrepresentasjon og rasjon (KR) i NLP- feltet og hindra også ein rett evaluering av KR- baserte NLU- systemar. Målet vårt er å bygge KR-baserte systemer for naturspråk forståking utan å forståke på ei tolkar. I denne måten foreslår vi ein metode med namnet Deeply Innebygd kjennomrepresentasjon og grunnlag (DeepEKR) der vi erstattar tolkaren med ein neuralnettverk, måk symbolsk representasjon slik at ein deterministisk karting finst mellom tolkaren neuralnettverk og den tolkare logiske formen, og til slutt erstatt symbolsk løysar med eit ekvivalent neuralnettverk, Så modellen kan trenast til slutt til slutt. Vi evaluerer metoden vårt med respekt til oppgåva av kvalitative ord- problem å løysa på to tilgjengelege datasett (QuaRTz og QuaRel). Sistemet vårt når det samme nøyaktighet som det i kvaRTz-tilstanden til kunsten, utfører kunsten til QuaRel og utfører ein tradisjonell KR-basert system. Resultatet viser at forskyvinga som er introdusert av ein KR- løysing ikkje hindrar det å gjera eit bedre', 'ml': 'പരിജ്ഞാനത്തിന്റെ പ്രതിനിധിയും പ്രതികരിക്കുന്നതിന്റെ വിജയിച്ച പ്രയോഗത്തിന്റെ സ്വാഭാവ ഭാഷയുടെ (NLU) സ്വാഭാവികമായ ഭാഷയുടെ (KR) വിവരങ്ങള ഒരു പ്രധാനപ്പെടുത്തുന്ന ഭാഷയുടെ പിന്നാലെ പല പ്രോജക്റ്റുകളും തുടങ്ങിയിരുന്നുവെങ്കിലും സത്യത്തില്\u200d നിന്നും വളരെ ദൂരെ This has severely limited the application of knowledge representation and reasoning (KR) in the field of NLP and also prevented a proper evaluation of KR based NLU systems.  നമ്മുടെ ലക്ഷ്യം സ്വാഭാവ ഭാഷയ്ക്കുള്ള കെആര്\u200d അടിസ്ഥാനമായ സിസ്റ്റമുണ്ടാക്കുക എന്നതാണ്. ഒരു പാര്\u200dസറിനെ  ഇതിന്റെ മുന്നില്\u200d നമ്മള്\u200d ആഴത്തിലുള്ള അറിവ് പ്രതിനിധിയും പ്രതിനിധിയും എന്ന പേരിലുള്ള ഒരു രീതിയില്\u200d പ്രായശ്ചിത്തമാക്കുന്നു. അവിടെ ന്യൂറല്\u200d നെറ്റാല്\u200d നെറ്റാല്\u200d നെറ്റല്\u200d നെറ്റിലുള്ള നെറുല്\u200d നെറ്റാള അതുകൊണ്ട് മോഡല്\u200d അവസാനം പരിശീലിക്കാന്\u200d കഴിയും. നമ്മള്\u200d നമ്മുടെ രീതിയെ വിലയില്\u200d പരിഗണിക്കുന്നു. രണ്ട് ലഭ്യമായ വാക്ക് പ്രശ്നത്തിന്\u200dറെ പ്രശ്നത്തിന്\u200dറെ (ക്വാര്\u200dട്ടിസും ക് ഞങ്ങളുടെ സിസ്റ്റത്തില്\u200d ക്വാര്\u200dട്ടിസിന്റെ സ്റ്റേറ്റിന്റെ സ്റ്റേറ്റിന്റെ സ്ഥിതിയുടെ വ്യക്തമായ വിവരങ്ങള്\u200d പ്രവര്\u200dത്തിപ്പിക്കുന്നു. ക്വാ അതിന്റെ ഫലങ്ങള്\u200d', 'mt': 'L-applikazzjoni b’suċċess tar-Rappreżentanza u r-Raġunar tal-Għarfien (KR) fil-Ftehim tal-Lingwa Naturali (NLU) hija fil-biċċa l-kbira limitata mid-disponibbiltà ta’ analizzatur tal-lingwi naturali b’saħħtu u bi skop ġenerali. Minkejja li tnedew bosta proġetti bil-għan li tiġi żviluppata lingwa ta’ rappreżentanza ta’ tifsira universali, l-eżistenza ta’ analizzatur universali preċiż hija ’l bogħod mir-realtà. Dan illimita b’mod sever l-applikazzjoni tar-rappreżentanza tal-għarfien u r-raġunament (KR) fil-qasam tal-NLP u evita wkoll evalwazzjoni xierqa tas-sistemi NLU bbażati fuq KR. L-għan tagħna huwa li nibnu sistemi bbażati fuq KR għall-Ftehim tal-Lingwi Naturali mingħajr ma nibnu fuq parser. Lejn dan qed nipproponu metodu msejjaħ Rappreżentazzjoni u Raġunar tal-Għarfien Integrat fil-Ħafna (DeepEKR) fejn nistostitwixxu l-analizzatur b’netwerk newrali, nirrakkomandaw ir-rappreżentazzjoni simbolika sabiex teżisti mmappjar determinanti bejn in-netwerk newrali tal-analizzatur u l-form a loġika interpretabbli, u fl-a ħħar nett nistostitwixxu s-solver simboliku b’netwerk newrali ekwivalenti, • sabiex il-mudell ikun jista’ jitħarreġ minn tarf għal tarf. Aħna jevalwaw il-metodu tagħna fir-rigward tal-kompitu tas-Soluzzjoni Kwalitattiva tal-Problemi tal-Kliem fuq iż-żewġ settijiet tad-dejta disponibbli (QuaRTz u QuaRel). Our system achieves same accuracy as that of the state-of-the-art accuracy on QuaRTz, outperforms the state-of-the-art on QuaRel and severely outperforms a traditional KR based system.  Ir-riżultati juru li l-preġudizzju introdott minn soluzzjoni KR ma jimpedixxix li tagħmel a ħjar', 'ro': 'Aplicarea cu succes a reprezentării și raționării cunoștințelor (KR) în înțelegerea limbajului natural (NLU) este în mare măsură limitată de disponibilitatea unui parser de limbaj natural robust și de scop general. Deși au fost lansate mai multe proiecte în urmărirea dezvoltării unui limbaj universal de reprezentare a semnificațiilor, existența unui parser universal precis este departe de realitate. Acest lucru a limitat grav aplicarea reprezentării și raționamentului cunoștințelor (KR) în domeniul PNL și a împiedicat, de asemenea, o evaluare adecvată a sistemelor NLU bazate pe KR. Scopul nostru este de a construi sisteme bazate pe KR pentru înțelegerea limbajului natural fără a ne baza pe un parser. În acest sens, propunem o metodă numită Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) în care înlocuim parserul cu o rețea neurală, înmuiem reprezentarea simbolică astfel încât să existe o mapare deterministă între rețeaua neurală parser și forma logică interpretabilă și, în cele din urmă, înlocuim rezolvatorul simbolic cu o rețea neurală echivalentă, astfel încât modelul să poată fi antrenat end-to-end. Evaluăm metoda noastră în ceea ce privește sarcina de rezolvare a problemelor calitative Word pe cele două seturi de date disponibile (Quartz și QuaRel). Sistemul nostru atinge aceeași precizie ca cea a acurateții de ultimă oră pe Quartz, depășește performanțele de ultimă oră pe QuaRel și depășește grav un sistem tradițional bazat pe KR. Rezultatele arată că părtinirea introdusă de o soluție KR nu o împiedică să facă o mai bună', 'sr': 'Uspješna primjena predstavljanja znanja i razloga (KR) u razumijevanju prirodnog jezika (NLU) u velikoj mjeri ograničava dostupnost robnog i općeg cilja prirodnog razmatrača jezika. Iako su pokrenuti nekoliko projekta u potrazi za razvojom univerzalnog jezika predstavljanja značenja, postojanje tačnog univerzalnog analizatora je daleko od stvarnosti. To je ozbiljno ograničilo primjenu zastupanja znanja i razmatranja (KR) na polju NLP-a i sprečavalo je odgovarajuću procjenu sustava NLU-a na KR-u. Naš cilj je da izgradimo sisteme zasnovane na KR-u za razumevanje prirodnog jezika bez oslanjanja na analizatora. Predlažemo metodu pod nazivom Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) gde zamenimo analizatora neuralnom mrežom, omekšamo simboličku predstavu tako da postoji deterministička mapa između analizatorske neuralne mreže i interpretabilne logičke forme, i konačno zamijenimo simboličkog rješavača ekvivalentnom neuralnom mrežom, Tako da model može biti obučen do kraja. Procjenjujemo našu metodu u odnosu na zadatak problema sa kvalitativnim riječima rješavanja na dve dostupne sete podataka (QuaRTz i QuaRel). Naš sistem postiže isto taènost kao i taènost države umjetnosti na QuaRTzu, iznosi stanje umjetnosti na QuaRelu i teško iznosi tradicionalni sistem zasnovanog na KR-u. Rezultati pokazuju da predrasude koje je uvodila rješenje KR-a ne sprečava to da radi bolje.', 'si': 'සමහර විශ්වාසයෙන් දැනගැනීම ප්\u200dරතිස්ථාපනය සහ හැකියාව (KR) ස්වභාවික භාෂාව තේරුම්ගන්න (NLU) ගොඩක් සීමාවිත විශ්වාසය ව්\u200dයාපෘතියක් පටන් ගත්තා නමුත් සාමාන්\u200dය අදහස් භාෂාවක් විශ්වාස කරනවා නමුත්, සාමාන්\u200dය විශ්වාසික විශ් Name අපේ අරමුණ තමයි ස්වාභාවික භාෂාව තේරුම්ගන්න සඳහා KR අධාරිත පද්ධතිය නිර්මාණය කරන්න. මේකෙන් අපි ප්\u200dරශ්නයක් දිප්ලි ඇම්බෙඩ් දැනගැනීම් ප්\u200dරශ්නය & Reasoning (DeepEKR) කිරීමෙන් ප්\u200dරශ්නය කරන්න ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් තියෙන්නේ, සංකේතික ජාලයෙන් ප්\u200dරශ්නය කරන්න, ඉතින් සං ඉතින් මොඩේල් එක ඉවරයෙන් ඉවරයෙන් පුළුවන්. අපි අපේ විධානය විශ්වාස කරන්නේ ප්\u200dරශ්ණ වචන ප්\u200dරශ්නයක් විසඳන්න පුළුවන් දත්ත සෙට් දෙකට (QuaRTz සහ QuaREL) ගැන. අපේ පද්ධතිය ක්වාර්ට්ස් වලින් ක්\u200dරියාර්ට්ස් වලින් ස්ථිතියෙන් හරියට හරියට අවස්ථාවක් ලැබෙනවා, QuaREL වලින් ක්\u200dරියාර්ට්ස් වලින් ස්ථ @ info', 'ta': '@ info பல திட்டங்கள் ஒரு பொதுவான பிரதிநிர்வாக மொழியை உருவாக்கும் பின்தொடர்ச்சியில் துவங்கப்பட்டுள்ளது என்றாலும், சரியான பொதுவ இது NLP புலத்தில் அறிவு குறிப்பிடுதல் மற்றும் காரணங்கள் (KR) பயன்பாட்டை கடுமையாக வரையறுக்கப்பட்டுள்ளது மற்றும் KR அடிப்படையிலான N Our goal is to build KR based systems for Natural Language Understanding without relying on a parser.  இப்பொழுது நாம் ஆழமான உட்பொதிந்த அறிவு பிரதிநிர்வாக்குதல் மற்றும் (DeepEKR) பெயரிடும் ஒரு முறையை நாம் புதிய வலைப்பின்னலால் மாற்றுகிறோம், அதாவது ஒரு குறியீட்டு குறியீட்டு குறியீட்டு குறியீட்டு  மாதிரி முடிவுக்கு பயிற்சி செய்யலாம். நாம் எங்கள் முறையை மதிப்பிடுகிறோம் தரவு வார்த்தை பிரச்சனையின் செயல்பாட்டின் மதிப்பிடுகிறோம் இரு கிடைக்கும் தகவல் அம எங்கள் அமைப்பு அதே சரியான பெறுகிறது என்று குவார்டிஸ் நிலையில் உள்ள - கலை சரியான திட்டம், குவார்லில் உள்ள - கலை நிலைமையை வெளியேற்றுகிறது மற்றும் ஒரு  முடிவு', 'so': 'Codsiga succeeded u baaraandegista aqoonta ku qoran afka asalka ah ee aqoonta waxgarashada (KR) waxaa si weyn u xadgudbay isticmaalka luqada dabiicadda ah ee loo isticmaalo karo iyo guud luqada asalka ah. Inkastoo waxaa la soo saaray wax badan oo laga soo saaray raadkii horumarinta luqada macnaheedka caalamiga ah, marka la joogo baaritaanka saxda jaamacadda ayaa ka fog runta. This has severely limited the application of knowledge representation and reasoning (KR) in the field of NLP and also prevented a proper evaluation of KR based NLU systems.  Ujeedkeennu waa in la dhiso nidaamka KR ee lagu saleeyay afka asalka ah ee garashada afka dabiicadda ah iyadoon ku rajaynayn baaritaanka. Taas ayaannu u soo jeedaynaa qaab magaca Deep Embedded Knowledge Representation & Reasoning (DeepEKR) oo aynu ku bedelaynaa Parser shabakadda neural, si a an u qaboojino sawir u eg shabakadda neural, si ay u jiraan kartonka xisaabta si uu u dhexeeyo shabakadda balbalada neural iyo foomka macluumaadka la turjumayo, ugudambaystana waxan ku bedelaynaa sawir oo isku mid ah shabakadda neural, marka qaababka waxaa la baran karaa dhammaadka ugu dambaysta. Waxaynu qiimeynaynaa qaabkeenna ku saabsan shaqada Qualitative Word dhibaatada ku saabsan labada databases (QuaRTz iyo QuaRel). Our system achieves same accuracy as that of the state-of-the-art accuracy on QuaRTz, outperforms the state-of-the-art on QuaRel and severely outperforms a traditional KR based system.  Abaalku waxay muuqataa in khilaafka lagu soo saaray xafiiska KR uusan ka hor marin in uu sameeyo wax ka wanaagsan', 'ur': 'علم Representation and Reasoning (KR) کے کامیاب کاروبار (NLU) کی زیادہ محدود ہے ایک مضبوط اور عمومی مقصد طبیعی زبان پھیلانے والے کی موجودگی سے۔ اگرچہ بہت سی پروژے آغاز کیے گئے ہیں کہ ایک عمومی معنی کی زبان کی پیدائش کریں، ایک سیدھی عمومی پارچر کی موجودگی حقیقت سے دور ہے. اس نے NLP کے کھیلے میں علم کی نمایش اور منظورت (KR) کی کاربری کو سخت محدود کر دیا ہے اور اس نے KR کی بنیادی NLU سیستموں کی مطابق مطابق تحقیق کرنے سے روکا ہے۔ ہمارا مقصد یہ ہے کہ کری بنیادی سیسٹم بنانے کے لئے طبیعی زبان سمجھنے کے لئے بغیر پارچر پر بھروسہ کرنا ہے. ہم اس کی طرف ایک طریقہ پیشنهاد کرتے ہیں جو Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) کا نام ہے جہاں ہم پارچر کو نئورل نیٹ ورک سے بدل دیتے ہیں، سیمبولیک نمونٹ کو نرم کر دیتے ہیں تاکہ پارچر نئورل نیٹ ورک کے درمیان ایک مقررہ نمونٹ نقشه موجود ہوتی ہے اور آخر میں سیمبولیک حل کرنے والا ایک برابر نئورل نیٹ ورک اس لئے مدل آخر و آخر کی تعلیم کی جاتی ہے۔ ہم اپنے طریقے کا ارزش کررہے ہیں کہ دو دسترسی ڈیٹسٹ (QuaRTz اور QuaRel) پر حل کرنے کے لئے کیلوٹیٹیو ویر مشکل کے کام کے بارے میں۔ ہماری سیستم کوارٹز پر قابل صحیح سیستم کے مطابق اسی طرح موجود ہوتی ہے، کوارٹز پر قابل صحیح سیستم کو زیادہ اضافہ کرتا ہے اور اس سے زیادہ زیادہ اضافہ کرتا ہے ایک سنتی کری بنیاد سیستم. نتیجے دکھاتے ہیں کہ KR حل کے ذریعے معلوم ہونے کی بحث اس کو اچھی طرح کرنے سے روکتی نہیں ہے', 'pl': 'Skuteczne zastosowanie wiedzy Reprezentacja i Rozumowanie (KR) w rozumieniu języka naturalnego (NLU) jest w dużej mierze ograniczone przez dostępność solidnego i ogólnego przeznaczenia parser języka naturalnego. Mimo że kilka projektów zostało uruchomionych w dążeniu do opracowania uniwersalnego języka reprezentacji znaczenia, istnienie dokładnego uniwersalnego parsera jest dalekie od rzeczywistości. Znacznie ograniczyło to stosowanie reprezentacji i rozumowania wiedzy (KR) w dziedzinie NLP, a także uniemożliwiło właściwą ocenę systemów NLU opartych na KR. Naszym celem jest budowanie systemów opartych na KR dla rozumienia języka naturalnego bez polegania na parserze. W tym celu proponujemy metodę o nazwie Deeply Embedded Knowledge Representation, Rozumowanie (DeepEKR), w której zastępujemy parser siecią neuronową, zmiękczymy reprezentację symboliczną tak, aby istniało odwzorowanie deterministyczne między siecią neuronową parsera a interpretowalną formą logiczną, a na koniec zastąpimy symboliczny rozwiązacz równoważną siecią neuronową, więc model może być trenowany od końca do końca. Oceniamy naszą metodę pod kątem zadania jakościowego rozwiązywania problemów słowa na dwóch dostępnych zbiorach danych (Quartz i Quarel). Nasz system osiąga taką samą dokładność jak najnowocześniejsza dokładność w Quartz, przewyższa najnowocześniejszą w QuaRel i znacznie przewyższa tradycyjny system oparty na KR. Wyniki pokazują, że uprzedzenia wprowadzane przez rozwiązanie KR nie uniemożliwia mu lepszego osiągnięcia', 'sv': 'Lycklig tillämpning av Kunskapsrepresentation och förnuft (KR) i Natural Language Understanding (NLU) begränsas till stor del av tillgången på en robust och allmänt ändamålsenlig naturspråktolkning. Även om flera projekt har inletts i strävan att utveckla ett universellt betydelserepresentationsspråk är förekomsten av en korrekt universell tolkning långt ifrån verklighet. Detta har kraftigt begränsat tillämpningen av kunskapsrepresentation och resonemang (KR) på området för NLP och förhindrat en korrekt utvärdering av KR-baserade NLU-system. Vårt mål är att bygga KR-baserade system för naturlig språkförståelse utan att förlita sig på en parser. Mot detta föreslår vi en metod som heter Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) där vi ersätter parsern med ett neuralt nätverk, mjukar upp den symboliska representationen så att en deterministisk kartläggning existerar mellan parsern neurala nätverk och den tolkningsbara logiska formen, och slutligen ersätter den symboliska lösaren med ett motsvarande neuralt nätverk, så att modellen kan tränas från början till slut. Vi utvärderar vår metod med avseende på uppgiften Kvalitativ Word Problemlösning utifrån de två tillgängliga datauppsättningarna (Quartz och QuaRel). Vårt system uppnår samma noggrannhet som den senaste noggrannheten på Quartz, överträffar det senaste på QuaRel och överträffar kraftigt ett traditionellt KR-baserat system. Resultaten visar att den bias som en KR-lösning introducerar inte hindrar den från att göra en bättre', 'mn': 'Байгалийн хэл ойлголтын (NLU) мэргэжлийн төлөөлөгч болон шалтгааныг амжилттай ашиглах нь ихэнхдээ хүчтэй, ерөнхий зорилготой байгалийн хэл хуваалцагч байдлаар хязгаарлагддаг. Хэдийгээр олон төслүүд универсал утгыг үзүүлэх хэл хөгжүүлэхэд эхэлсэн ч гэсэн, ертөнцийн хуваалцагчийн байдал бодит байдлаас хол байдаг. Энэ нь NLP салбарт мэдлэгтэй илэрхийлэл, урам зориулалт (KR) болон KR-ын системийн зөв дүгнэлт хийх боломжтой болсон. Бидний зорилго бол Байгалийн хэл ойлголтын KR-ын үндсэн системүүдийг хуваагч дээр итгэлгүйгээр бий болгох юм. Энэ талаар бид Deeply Integrated Knowledge Representation & Reasoning (DeepEKR) гэдэг арга загварыг шийдвэрлэгчийг мэдрэлийн сүлжээгээр орлуулж, символын төлөөлөлийг багасгаж, хуваагч мэдрэлийн сүлжээний хооронд тодорхойлолтой газрын зураг бий болгож, эцэст нь символын шийдвэрлэгчийг тэнцүү мэдрэлийн сүлжээнд Тиймээс загвар нь төгсгөлд суралцах боломжтой. Бид хэрэглэгдэх хоёр өгөгдлийн санг (QuaRTz болон QuaRel) шийдэх асуудалтай хамааралтай ажлыг үнэлэх аргыг үнэлдэг. Бидний систем QuaRTz-ын урлагийн зөв байдалтай адилхан зөв байдлыг гаргаж, QuaRel-ын урлагийн байдлыг нэмэгдүүлж, уламжлалт КР-ын системээс илүү их нэмэгдүүлдэг. Үүний үр дүнд КР-ын шийдвэрлэлээр танилцуулсан өрөөсгөл нь', 'vi': 'Sự ứng dụng thành công của sự phân bổ tri thức và lẽ phải (KR) trong hiểu biết ngôn ngữ tự nhiên (Nlu) bị giới hạn phần lớn bởi khả năng có một cha xứ ngôn ngữ tự nhiên mục đích mạnh mẽ và chung. Mặc dù đã có rất nhiều dự án được tiến hành để phát triển một ngôn ngữ đại diện có ý nghĩa chung, nhưng sự tồn tại của một nhà phân biệt vũ trụ chính xác thì không hề có thực tế. Việc này đã giới hạn nghiêm trọng việc sử dụng phân tích và lý lẽ về tri thức (KR) trong lĩnh vực Nchọc (KR) và cũng đã ngăn chặn một cuộc đánh giá thích đáng về hệ thống Nluu dựa trên KR. Mục tiêu của chúng ta là xây dựng hệ thống KR dựa trên ngôn ngữ tự nhiên mà không dựa vào một cha xứ. Về hướng này, chúng tôi đề nghị một phương pháp được gọi là Bố trí Kiến thức Có vẻ hợp lý (DeepwEK) nơi chúng ta thay thế cỗ máy phân tích bằng mạng thần kinh, làm giảm biểu tượng tượng tượng tượng tượng của chúng, để có một bản đồ phân tích cố vấn tồn tại giữa mạng thần kinh phân phân tích và cấu hình lý học được giải, và cuối cùng thay thế chất lỏng bằng một mạng thần kinh tương đương, để người mẫu có thể được huấn luyện từ đầu tới cuối. Chúng tôi đánh giá phương pháp của chúng tôi về nhiệm vụ giải quyết Từ giáo dục về hai tập tin có sẵn (QuaRTz và QuaRell). Hệ thống của chúng ta đạt được độ chính xác cao nhất của QuaRTz, vượt qua trình độ chính xác truyền thống trên QuaRell và hoàn toàn vượt qua hệ thống CR truyền thống. Kết quả cho thấy khả năng thiên vị được đưa ra bởi một giải pháp KR sẽ không ngăn nó làm tốt hơn\ncông việc cuối cùng. Hơn nữa, phương pháp của chúng tôi được giải thích là do khuynh hướng gây ra bởi phương pháp KR.', 'uz': "Name Ko'pchilik loyihalar bir necha loyihani butun taʼminlovchi tillarni ishga tushirishda ishga tushirilgan bo'lsa, butunlay ajoyib parametrlarning mavjudligi haqiqatdan juda qulay emas. Name Our goal is to build KR based systems for Natural Language Understanding without relying on a parser.  Bu yerda biz ajratilgan aqlli taʼminni aniqlash va qayta olish (DeepEKR) nomli usulni talab qilamiz. Ularni neyrol tarmoqning orqali almashtirish mumkin, diagrammani aniqlash mumkin. Parametr tarmoq va tarmoq tarkibi tarmoqning orasidagi qiymatni aniqlash mumkin, va oxiriga oʻxshash neyrolik tarmoqda signalni aniqlash. shuning uchun model oxiriga o'rganishi mumkin. Biz quyidagi ikkita maʼlumot sahifalar (QuaRTz va QuaRel) haqida qiymatmiz. Bizning tizimmiz QuaRTz davlatning davlat haqida bir tashkilotni amalga oshadi, QuaRel davlatini bajaradi va qiziq KR asosiy tizimni bajaradi. @ info\njob at the end task.  Ko'pchilik, bizning usuli KR usuli bilan ishga tushirilgan bias sababda turadi.", 'nl': 'De succesvolle toepassing van Kennisrepresentatie en Redenen (KR) in Natural Language Understanding (NLU) wordt grotendeels beperkt door de beschikbaarheid van een robuuste en universele natuurlijke taalparser. Hoewel verschillende projecten zijn gestart in het streven naar het ontwikkelen van een universele betekenisrepresentatietaal, is het bestaan van een nauwkeurige universele parser verre van de werkelijkheid. Dit heeft de toepassing van kennisrepresentatie en redeneren (KR) op het gebied van NLP sterk beperkt en ook een goede evaluatie van op KR gebaseerde NLU systemen verhinderd. Ons doel is om KR gebaseerde systemen te bouwen voor Natural Language Understanding zonder te vertrouwen op een parser. Hiertoe stellen we een methode voor genaamd Deeply Embedded Knowledge Representation.Reasoning (DeepEKR) waarbij we de parser vervangen door een neuraal netwerk, de symbolische representatie verzachten zodat er een deterministische mapping bestaat tussen het parser neuraal netwerk en de interpreteerbare logische vorm, en ten slotte de symbolische oplosser vervangen door een equivalent neuraal netwerk, zodat het model end-to-end getraind kan worden. We evalueren onze methode met betrekking tot de taak van Kwalitatieve Woordproblemen Oplossen op de twee beschikbare datasets (Quartz en Quarel). Ons systeem bereikt dezelfde nauwkeurigheid als die van de state-of-the-art nauwkeurigheid op Quartz, overtreft de state-of-the-art op QuaRel en overtreft sterk een traditioneel KR-gebaseerd systeem. De resultaten tonen aan dat de bias die door een KR-oplossing wordt geïntroduceerd, er niet van weerhoudt om het beter te doen.\ntaak aan het einde van de taak. Bovendien is onze methode interpreteerbaar vanwege de bias geïntroduceerd door de KR-benadering.', 'da': 'Succesfuld anvendelse af Knowledge Representation and Reasoning (KR) i Natural Language Understanding (NLU) er stort set begrænset af tilgængeligheden af en robust og generel formål natursprogfortolker. Selv om flere projekter er blevet iværksat i stræben efter at udvikle et universelt meningsrepræsentationssprog, er eksistensen af en nøjagtig universel fortolker langt fra virkeligheden. Dette har i høj grad begrænset anvendelsen af vidensrepræsentation og ræsonnement (KR) inden for NLP og forhindret en korrekt evaluering af KR-baserede NLU-systemer. Vores mål er at opbygge KR baserede systemer til forståelse af natursprog uden at stole på en fortolker. Derimod foreslår vi en metode, der hedder Deeply Embedded Knowledge Representation & Reasoning (DeepEKR), hvor vi erstatter parseren med et neuralt netværk, blødgør den symbolske repræsentation, så der eksisterer en deterministisk kortlægning mellem parserens neurale netværk og den fortolkbare logiske form, og endelig erstatter den symbolske løser med et tilsvarende neuralt netværk, så modellen kan trænes fra start til slut. Vi evaluerer vores metode i forhold til opgaven med kvalitativ ordproblemløsning på de to tilgængelige datasæt (Quartz og QuaRel). Vores system opnår samme nøjagtighed som den avancerede nøjagtighed på Quartz, overgår det avancerede på QuaRel og overgår kraftigt et traditionelt KR baseret system. Resultaterne viser, at bias introduceret af en KR-løsning ikke forhindrer den i at gøre en bedre\njob ved slutopgaven. Desuden kan vores metode fortolkes på grund af den bias, som KR-metoden indfører.', 'bg': 'Успешното прилагане на Представителството на знанието и разума (КР) в разбирането на естествения език (НЛУ) до голяма степен се ограничава от наличието на здрав и общоприет анализатор на естествения език. Въпреки че са стартирани няколко проекта в стремежа към разработване на универсален език за представяне на смисъла, съществуването на точен универсален анализатор е далеч от реалността. Това сериозно ограничи прилагането на представянето на знанията и разсъжденията (КР) в областта на НЛП и също така възпрепятства правилната оценка на системите на НЛУ, базирани на КР. Нашата цел е да изградим базирани системи за разбиране на естествения език, без да разчитаме на анализатор. За тази цел предлагаме метод, наречен Дълбоко вградено представяне и разсъждаване на знанието (DeepEKR), при който заменяме анализатора с неврална мрежа, омекотяваме символичното представяне, така че да съществува детерминистично картографиране между невралната мрежа на анализатора и интерпретираната логическа форма, и накрая заменяме символичното решаване с еквивалентна неврална мрежа, Така моделът може да бъде обучен от край до край. Ние оценяваме метода си по отношение на задачата за качествено решаване на словесни проблеми на двата налични набора от данни (QuaRTz и QuaRel). Нашата система постига същата точност като тази на най-съвременната точност на КВАРц, превъзхожда най-съвременните на КВАРЕЛ и сериозно превъзхожда традиционната система, базирана на КР. Резултатите показват, че отклонението, въведено от разтвора на КР, не му пречи да направи по-добро\nРабота в края на задачата. Освен това методът ни е интерпретиран поради пристрастието, въведено от подхода КР.', 'id': 'Aplikasi sukses dari Perwakilan Pengetahuan dan Alasan (KR) dalam Pemahaman Bahasa Alami (NLU) adalah kebanyakan terbatas oleh keberhasilan pemeriksa bahasa alam yang kuat dan tujuan umum. Meskipun beberapa proyek telah dilancarkan dalam pencarian mengembangkan bahasa representation arti universal, keberadaan parser universal yang akurat jauh dari kenyataan. Hal ini telah sangat membatasi aplikasi perwakilan pengetahuan dan alasan (KR) dalam bidang NLP dan juga mencegah evaluasi yang tepat dari sistem NLU berdasarkan KR. Our goal is to build KR based systems for Natural Language Understanding without relying on a parser.  Ke arah ini kami mengusulkan metode bernama Deep Embedded Knowledge Representation & Reasoning (DeepEKR) di mana kami menggantikan parser dengan jaringan saraf, lembut representation simbolik sehingga peta deterministik ada antara jaringan saraf parser dan bentuk logis yang dapat diterjemahkan, dan akhirnya menggantikan penyelesair simbolik dengan jaringan saraf yang sama, Jadi model bisa dilatih akhir-akhir. Kami mengevaluasi metode kami terhadap tugas Solving Qualitative Word Problem pada dua set data yang tersedia (QuaRTz dan QuaRel). Sistem kita mencapai akurasi yang sama dengan akurasi state-of-the-art di QuaRTz, melampaui state-of-the-art di QuaRel dan melampaui batas sistem berdasarkan KR tradisional. Hasilnya menunjukkan bahwa bias yang diperkenalkan oleh solusi KR tidak mencegahnya dari melakukan lebih baik\npekerjaan pada tugas akhir. Selain itu, metode kita dapat diterjemahkan karena bias yang diperkenalkan oleh pendekatan KR.', 'de': 'Die erfolgreiche Anwendung von Knowledge Representation and Reasoning (KR) im Natural Language Understanding (NLU) wird weitgehend durch die Verfügbarkeit eines robusten und universellen natürlichen Sprachparsers eingeschränkt. Obwohl mehrere Projekte gestartet wurden, um eine universelle Bedeutungsdarstellungssprache zu entwickeln, ist die Existenz eines genauen universellen Parsers weit von der Realität entfernt. Dies hat die Anwendung von Knowledge Representation and Reasoning (KR) im NLP-Bereich stark eingeschränkt und auch eine ordnungsgemäße Bewertung von KR-basierten NLU-Systemen verhindert. Unser Ziel ist es, KR-basierte Systeme für das Verständnis natürlicher Sprache zu entwickeln, ohne sich auf einen Parser zu verlassen. Zu diesem Zweck schlagen wir eine Methode namens Deeply Embedded Knowledge Representation,Reasoning (DeepEKR) vor, bei der wir den Parser durch ein neuronales Netzwerk ersetzen, die symbolische Darstellung erweichen, so dass eine deterministische Zuordnung zwischen dem Parser neuronalen Netzwerk und der interpretierbaren logischen Form existiert, und schließlich den symbolischen Solver durch ein äquivalentes neuronales Netzwerk ersetzen. So kann das Modell Ende-zu-Ende trainiert werden. Wir evaluieren unsere Methode hinsichtlich der Aufgabe der qualitativen Wortproblemlösung auf den beiden verfügbaren Datensätzen (Quartz und Quarel). Unser System erreicht die gleiche Genauigkeit wie die State-of-the-Art Genauigkeit auf Quartz, übertrifft den State-of-the-Art auf QuaRel und übertrifft ein traditionelles KR-basiertes System erheblich. Die Ergebnisse zeigen, dass die von einer KR-Lösung eingeführte Verzerrung nicht daran hindert, eine bessere Leistung zu erzielen.\nAufgabe am Ende der Aufgabe. Darüber hinaus ist unsere Methode aufgrund der Verzerrung durch den KR-Ansatz interpretierbar.', 'ko': '지식 표시와 추리(KR)의 자연언어 이해(NLU)에서의 성공적인 응용은 어느 정도 건장한 통용 자연언어 해석기의 제한을 받는다.비록 이미 몇 가지 프로젝트를 시작하여 유니버설 의미 표시 언어를 개발하였지만, 정확한 유니버설 해석기의 존재는 아직 매우 비현실적이다.이는 지식 표시와 추리(KR)의 자연 언어 처리 영역에서의 응용을 심각하게 제한하고 KR 기반의 자연 언어 처리 시스템에 대한 정확한 평가를 방해한다.해석기에 의존하지 않고 KR 기반의 자연 언어 이해 시스템을 구축하는 것이 목표입니다.이를 위해 우리는 깊이 삽입식 지식 표시와 추리(DeepEKR)라고 하는 방법을 제시했다. 신경 네트워크로 해석기를 교체하고, 기호 표시를 연화시켜 해석기 신경 네트워크와 해석 가능한 논리 형식 사이에 확정적인 반사가 존재하게 하며, 마지막으로 등효 신경 네트워크로 기호 해산기를 교체한다.따라서 모델에 대해 끝에서 끝까지의 훈련을 할 수 있다.우리는 두 개의 사용 가능한 데이터 집합 (QuaRTz와QuaRel) 에서 우리의 정성 단어 문제 해결 방법을 평가했다.우리 시스템은 석영에서 가장 선진적인 정밀도와 같은 정밀도를 실현하여QuaRel에서 가장 선진적인 정밀도보다 우수하고 전통적인 KR 기반 시스템보다 심각하게 우수하다.그 결과 KR 용액 도입의 편차가 그것을 더 잘 만드는 데 방해가 되지 않는다는 것이 밝혀졌다\n임무가 끝날 때의 숙제.또한 KR 접근 방식의 편차 때문에 Google 접근 방식은 해석할 수 있습니다.', 'sw': 'Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser.  Hata kama miradi kadhaa imezinduliwa kufuatia kuendeleza lugha ya uwakilizaji wa kimataifa, kuwepo kwa mhadhiri sahihi wa ulimwengu ni mbali na ukweli. Hii imezuia kwa kiasi kikubwa matumizi ya uwakilishi wa maarifa na sababu (KR) katika uwanja wa NLP na pia imezuia uchunguzi sahihi wa mfumo wa NLU anayeishi KR. Lengo letu ni kutengeneza mifumo yenye msingi wa KR kwa ajili ya kuelewa lugha ya asili bila kutegemea mchambuzi. Mpaka huu tunapendekeza njia inayoitwa Representation & Reading (DeepEKR) ambapo tunabadilisha mchambuzi kwa mtandao wa neurali, tunaorodhesha uwakilizaji wa alama ili ramani ya kina kuwepo kati ya Mtandao wa Parser neural na aina ya mantiki inayotafsiriwa, na hatimaye kubadilisha suluhisho la alama kwa mtandao wa neural, ili muundo unaweza kufundishwa mwisho wa mwisho. Tunatathmini mbinu yetu kwa kuhusiana na jukumu la Tatizo la Tatizo la neno la Utawala linalohitajika kwenye seti mbili za data (QuaRTz na QuaRel). Our system achieves same accuracy as that of the state-of-the-art accuracy on QuaRTz, outperforms the state-of-the-art on QuaRel and severely outperforms a traditional KR based system.  Matokeo yanaonyesha kuwa upendeleo uliotolewa na ufumbuzi wa KR haukuzuia kufanya vizuri zaidi\nkazi katika kazi ya mwisho. Zaidi ya hayo, mbinu yetu inatafsiriwa kutokana na upendeleo uliotolewa na mbinu za KR.', 'hr': 'Uspješna primjena predstavljanja znanja i razloga (KR) u razumijevanju prirodnog jezika (NLU) u velikoj mjeri ograničava dostupnost robnog i općeg cilja prirodnog razmatrača jezika. Iako su pokrenuti nekoliko projekta u potrazi za razvojom univerzalnog jezika predstavljanja značaja, postojanje točnog univerzalnog analizatora je daleko od stvarnosti. To je ozbiljno ograničilo primjenu zastupanja znanja i razmišljanja (KR) u polju NLP-a te je također spriječilo odgovarajuću procjenu sustava NLU-a na KR-u. Naš cilj je izgraditi sustave temeljene na KR-u za razumijevanje prirodnog jezika bez oslanjanja na analizatora. Za to predlažemo metodu po imenu Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) gdje zamijenimo analizatora neuralnom mrežom, smanjiti simboličku predstavu tako da postoji deterministička mapa između analizatorske neuralne mreže i interpretabilnog logičkog oblika, i konačno zamijenimo simboličkog rješavača ekvivalentnom neuralnom mrežom, Dakle, model može biti obučen do kraja. Procjenjujemo našu metodu s obzirom na zadatak problema s kvalitativnim riječima rješavanja na dvije dostupne sete podataka (QuaRTz i QuaRel). Naš sustav postiže isto preciznost kao što je državna preciznost umjetnosti na QuaRTzu, iznosi državu umjetnosti na QuaRelu i teško iznosi tradicionalni sustav temeljenog na KR-u. Rezultati pokazuju da predrasude koje je uvodila rješenje KR-a ne sprječava da radi bolje\nposao na kraju zadatka. Osim toga, naš metod je interpretabilan zbog pristrasnosti uvedenog pristupom KR-a.', 'sq': 'Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser.  Edhe pse disa projekte janë nisur në kërkim të zhvillimit të një gjuhe përfaqësimi universal, ekzistenca e një analizuesi të saktë universal është larg realitetit. This has severely limited the application of knowledge representation and reasoning (KR) in the field of NLP and also prevented a proper evaluation of KR based NLU systems.  Qëllimi ynë është të ndërtojmë sisteme bazuar në KR për kuptimin e gjuhës natyrore pa u mbështetur në një analizues. Përballë kësaj propozojmë një metodë të quajtur Përfaqësues dhe arsyetimi i njohurive të thella të përfshira (DeepEKR) ku ne zëvendësojmë analizuesin me një rrjet nervor, lehtësojmë përfaqësimin simbolik në mënyrë që një hartim përcaktues ekziston midis rrjetit nervor të analizuesit dhe form ës logjike të interpretueshme, dhe më në fund zëvendësojmë zgjidhësin simbolik me një rrjet ekvivalent nervor, Kështu që modeli mund të trajnohet nga fundi në fund. Ne vlerësojmë metodën tonë lidhur me detyrën e zgjidhjes së problemeve të fjalëve kualitative në dy grupet e të dhënave të disponueshme (QuaRTz dhe QuaRel). Sistemi ynë arrin të njëjtën saktësi si ajo e saktësisë së state-of-the-art në QuaRTz, e tejkalon state-of-the-art në QuaRel dhe e tejkalon ashpër një sistem tradicional bazuar në KR. Rezultatet tregojnë se paragjykimi i futur nga një zgjidhje KR nuk e pengon atë që të bëjë një më të mirë\npunë në detyrën e fundit. Përveç kësaj, metoda jonë është e interpretueshme për shkak të paragjykimit të futur nga qasja KR.', 'hy': 'Գիտության ներկայացման և տրամաբանության (ՔՌ) հաջողակ կիրառումը բնական լեզվի հասկացության մեջ հիմնականում սահմանափակված է ուժեղ և ընդհանուր նպատակի բնական լեզվի վերլուծության միջոցով: Չնայած որ մի քանի նախագծեր են սկսվել համընդհանուր իմաստի ներկայացման լեզու զարգացման համար, ճշգրիտ համընդհանուր վերլուծողի գոյությունը հեռու է իրականությունից: Սա խիստ սահմանափակել է գիտելիքների ներկայացման և մտածողության (ՔՌ) կիրառումը ՆԼՊ-ի ոլորտում, ինչպես նաև խոչընդոտել է ՔՌ-ի հիմնված ՆԼՊ-ի համակարգերի ճիշտ գնահատման: Մեր նպատակն է կառուցել բնական լեզվի հասկանալու համար հիմնված ԿՌ համակարգեր առանց հույս դնել վերլուծում: Towards this we propose a method named Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) where we replace the parser by a neural network, soften the symbolic representation so that a deterministic mapping exists between the parser neural network and the interpretable logical form, and finally replace the symbolic solver by an equivalent neural network, Այսպիսով, մոդելը կարող է վերջ-վերջ սովորեցնել: Մենք գնահատում ենք մեր մեթոդը որակավոր բառերի խնդիրների լուծման խնդիրների վերաբերյալ երկու հասանելի տվյալների (QuaRz և QuaREL) համակարգերի վրա: Մեր համակարգը հասնում է նույն ճշգրտությունը, ինչ ամենաբարձր ճշգրտությունը QuaRz-ում, գերազանցում է QuaREL-ի ամենաբարձր արդյունքը և խիստ գերազանցում է ավանդական ԿՌ-ի հիմնված համակարգը: Արդյունքները ցույց են տալիս, որ ԿՌ լուծության կողմից ներկայացված կողմնականությունը չի կանխում նրան ավելի լավ\nաշխատանք վերջին խնդիրը: Ավելին, մեր մեթոդը մեկնաբանելի է ՔՌ մոտեցումների կողմից ներկայացված կողմից:', 'fa': 'کاربرد موفقیت نماینده\u200cی دانش و دلیل\u200cسازی (KR) در درک زبان طبیعی (NLU) بسیار محدود است از دسترسی دسترسی زبان\u200cهای قوی و عمومی طبیعی. اگرچه چند پروژه در دنبال توسعه یک زبان نمایش عمومی آغاز شده است، وجود یک بازیگر جهانی دقیق دور از واقعیت است. این کاربرد نمایش و منطقی علم (KR) را در زمینه NLP بسیار محدود کرده است و همچنین جلوگیری از ارزیابی مناسب سیستم NLU بر پایه KR است. هدف ما اینه که سیستم\u200cهای بنیادی KR برای فهمیدن زبان طبیعی بدون اعتماد به تقسیم کننده بسازیم. به سمت این، یک روش به نام Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) را پیشنهاد می\u200cکنیم که آن را با یک شبکه عصبی جایگزین می\u200cکنیم، نمایش نمایش نمایش نمایش نمایش\u200cشناسی را نرم می\u200cسازیم تا یک نقشه تصمیم\u200cگیری بین شبکه عصبی و فرم منطقی قابل تعبیر وجود داشته باشد، و بالاخره حل نمایش\u200cشناسی را با بنابراین مدل می تواند به پایان آموزش داده شود. ما روش خودمان را با ارزیابی به کار مشکل کلمات کیفیت حل در دو مجموعه داده موجود (QuaRTz و QuaRel) ارزیابی می\u200cکنیم. سیستم ما دقیق همانند دقیق موقعیت هنری در QuaRTz را می\u200cرساند، وضعیت هنری در QuaRel را بیشتر انجام می\u200cدهد و سیستم سنتی را بر اساس KR به شدت بیشتر می\u200cدهد. نتیجه\u200cها نشان می\u200cدهند که پیشرفت توسط یک راه حل KR معرفی شده از کار بهتری جلوگیری نمی\u200cکند\nکاری در کار پایان است. علاوه بر این، روش ما قابل تعبیر است به دلیل طبیعی که توسط طریق KR معرفی شده است.', 'tr': "Dobiýal dil düşünmesinde Bilim Görkezilişi we Sebepleri (KR) ýerleşdirişi (NLU) kuvvetli we umumy maksady täze dil täzelikleriniň bar bolmagynyň (güýçli we umumy) mümkinçiligi tarapyndan çykarylýar. Birnäçe proýjekler uniwersal bir meýdança dili geliştirmek üçin başlandyrylyp hem, uniwersal täzeleçi bolmagyň dogry bir çözümlerden uzakdır. Bu NLP sahypasynda bilim temsili we razylygynyň (KR) uygulamasyny çykardy we KR sistemleriniň dogry deňlemesini çykardy. Biziň maksadymyz, KR-iň tabanly sistemalary tebigy dil düşünmesi üçin paýlaşma üçin gurmak. Bu yönünde Deeply Girişmiş Bilgi Temsilcisi & Reasoning (DeepEKR) adlı bir yöntem teklif ediyoruz. Biz parçacıyı neural a ğ ile eşittirip, simbolik temsilleri yumuşatmak üçin bir deterministik haritalama parçalayıcı nöral ağ arasında bulunuyor ve sonunda simbolik çözücüsünü ekvivalent nöral ağ tarafından almak üçin, Bu üçin modi so ňra-soňra bilip biler. Biz öz yönümizi Qualitative Word Problemi çözmek üçin çözümlendiris (QuaRTz we QuaRel). Bizim sistemimiz QuaRTz'de bir sanat taýýarlygynyň dogrylygyny ýetip bilýär, QuaRel'de sanat taýýarlygynyň durumyny üstün edýär we däpli bir KR tabanly sisteminden üstün edýär. Netijeler KR çözümleri tarapyndan tanyşyrlan biasleriň oňa has gowy bir iş etmekden saklanmaýandygyny görkezýär.\nsoňky zadyň işi. Munuň üçin biziň täzimimiz KR approach tarapyndan tanyşdyrylýan biaslarynyň sebäbi ylalaşýar.", 'am': 'የአውቀት መልዕክት እና አቀማመጥ (KR) በመፍለጋዊ ቋንቋ ማስታወቂያ (NLU) በተለየ የረጢት እና የጠቅላላ አቀፍ ቋንቋ ማቀናጃ ማግኘት በተለየ ነው፡፡ ምንም እንኳን ብዙ ፕሮጀክቶች የዓለምአቀፍ አካባቢ ቋንቋን ለማሳደግ በተከፈቱት ቢሆንም እንኳ የእውነት ትክክለኛ የዓለባዊ ተሳታፊ መኖሪያ ከእውነት ራቅ ነው፡፡ ይህ በNLP እርሻ ውስጥ የእውቀት መልዕክት እና ማስተዋል አግኝቷል፣ የKR-based NLU ስርዓቶችን በመጠቀም አስተያየት አግኝቷል፡፡ ጉዳዩ የKR የተመሠረተውን የፍጥረት ቋንቋ ለማስተዋል ነው፡፡ ወደዚህም አቀራቢ የእውቀት መልዕክት እና መቀናቀል (DeepEKR) በተለወጠን ማተሚያውን በኔural መረብ እናተካክላለን፣ በተለያዩ የኔural መረብ መካከል የሚያስተካክል ማረብ እና በሚተርጉም የግንኙነት መረብ መካከል እንዲኖር እና በሚተረጉም የግንኙነት ማረፊያ እና በማስተካከል የባሕላዊ መረብ በመለወጥ እና በሚተረጉት የሚተረጉም የግንኙነትን ፈተና በመለወጥ እና በኋለኛው በአ ይሄ ሞዴላው መጨረሻ ለመጀመር ይችላል፡፡ We evaluate our method with respect to the task of Qualitative Word Problem Solving on the two available datasets (QuaRTz and QuaRel).  ስርዓታችን የክዓርቲን ግዛት የክዓርት ግዛት እርግጠኛ አግኝቷል፡፡ ፍሬዎቹ የKR መፍትሄ የተደረገው ውጤቶች ከመልካም ማድረግ አይከለክለውም፡፡\nመጨረሻ ስራ ከዚህም ጋር የKR ሥርዓት የተደረገውን ጥያቄ ምክንያት ልማድ ነው፡፡', 'bn': 'স্বাভাবিক ভাষায় জ্ঞান প্রতিনিধি এবং প্রতিক্রিয়া বুঝতে পারে (এনএলইউ) জ্ঞানের সফল অ্যাপ্লিকেশন বেশীরভাগ সীমাবদ্ধ এবং সাধারণ উদ্দেশ্যের Even though several projects have been launched in the pursuit of developing a universal meaning representation language, the existence of an accurate universal parser is far from reality.  এনএলপির ক্ষেত্রে জ্ঞানের প্রতিনিধিত্ব এবং কারণের (কেআর) অ্যাপলিকেশন গুরুতর সীমাবদ্ধ করেছে এবং এছাড়াও কেআর ভিত্তিক NLU সিস্ আমাদের লক্ষ্য হচ্ছে স্বাভাবিক ভাষার জন্য কেআর ভিত্তিক সিস্টেম বানানোর জন্য যা প্রকৃতি ভাষার বুঝতে পা এদিকে আমরা একটি প্রস্তাব করি যার নাম গভীর ভিতরে অভ্যন্তরীণ জ্ঞান প্রতিনিধি এবং প্রতিনিধি প্রতিনিধিত্ব (DeepEKR) নামের একটি নিউরেল নেটওয়ার্ক দ্বারা প্রতিনিধিত্ব প্রতিনিধিত্ব প্রস্তাব করি, যেখানে আমরা প্যারেস্ট তাই মডেল শেষ পর্যন্ত প্রশিক্ষণ প্রদান করতে পারে। We evaluate our method with respect to the task of Qualitative Word Problem Solving on the two available datasets (QuaRTz and QuaRel).  আমাদের সিস্টেম সেই সঠিকভাবে পৌঁছায় যেমন কুয়ারটিজের রাষ্ট্রের-শিল্পের পরিস্থিতির সঠিকভাবে সঠিকভাবে পৌঁছায়, কুয়ারেলের রাষ্ট্রের প ফলাফল দেখা যাচ্ছে যে একটি কেআর এর সমাধানের প্রযুক্তি তৈরি করা হয়েছে তা আরো ভালো করা থেকে বাধা দেয় না।\nশেষ কাজের কাজ। Moreover, our method is interpretable due to the bias introduced by the KR approach.', 'ca': "L'aplicació satisfactoria de la Representació i la Rasonació del Conèixement (KR) a l'Entensió del Linguatge Natural (NLU) està limitada en gran part per la disponibilitat d'un analitzador de llenguatges naturals robust i de propòsit general. Encara que s'han llançat molts projectes en busca de desenvolupar un llenguatge de representació de sentit universal, l'existència d'un analitzador universal precis és lluny de la realitat. Això ha limitat gravement l'aplicació de la representació del coneixement i el raonament (KR) en el camp del NLP i també ha evitat una evaluació adequada dels sistemes NLU basats en KR. El nostre objectiu és construir sistemes basats en KR per a entendre la llengua natural sense confiar en un analitzador. Towards this we propose a method named Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) where we replace the parser by a neural network, soften the symbolic representation so that a deterministic mapping exists between the parser neural network and the interpretable logical form, and finally replace the symbolic solver by an equivalent neural network, Així que el model pot ser entrenat de cap a cap. Evaluam el nostre mètode en relació a la tasca de solució qualitativa de problemes de paraules en els dos conjunts de dades disponibles (QuaRTz i QuaRel). El nostre sistema aconsegueix la mateixa precisió que la de l'última precisió de QuaRTz, supera l'última de QuaRel i supera gravement un sistema tradicional basat en KR. Els resultats mostren que el bias introduït per una solució KR no ho impedeix fer una millor\nfeina al final. A més, el nostre mètode és interpretable degut al bias introduït per l'enfocament KR.", 'cs': 'Úspěšná aplikace znalostní reprezentace a rozumování (KR) v porozumění přirozenému jazyce (NLU) je do značné míry omezena dostupností robustního a univerzálního parseru přirozeného jazyka. I když bylo zahájeno několik projektů za účelem vývoje univerzálního jazyka reprezentace významu, existence přesného univerzálního parseru je daleko od reality. To výrazně omezilo uplatňování znalostní reprezentace a uvažování (KR) v oblasti NLP a také zabránilo řádnému hodnocení NLU systémů založených na KR. Naším cílem je vytvořit systémy založené na KR pro porozumění přirozenému jazyku bez spoléhání na parser. Za tímto účelem navrhujeme metodu s názvem Deeply Embedded Knowledge Representation,Rozumování (DeepEKR), kde nahradíme parser neuronovou sítí, změkčíme symbolickou reprezentaci tak, aby existovalo deterministické mapování mezi parserovou neuronovou sítí a interpretovatelnou logickou formou a nakonec nahradíme symbolický řešitel ekvivalentní neuronovou sítí, takže model může být trénován od konce do konce. Naši metodu hodnotíme s ohledem na úlohu kvalitativního řešení slovních problémů na dvou dostupných datových sadách (Quartz a Quarel). Náš systém dosahuje stejné přesnosti jako nejmodernější přesnosti na Quartz, překonává nejmodernější systém na QuaRelu a výrazně překonává tradiční systém založený na KR. Výsledky ukazují, že zaujatost zavedená řešením KR nebrání tomu, aby se zlepšila\npráce na konci úkolu. Naše metoda je navíc interpretovatelná díky zaujatosti zavedené KR přístupem.', 'az': "Təbiətli dil anlaşılması (NLU) içində Bilim Representation and Reasoning (KR) müvəffəqiyyəti istifadə etməsi çox qüvvətli və qüvvətli məqsəd təbiətli dil ayırıcısı ilə müəyyən edilmişdir. Əgər bir neçə layihə təşkil edilsə də universel anlama dilini təşkil etmək üçün başladılmışsa da, həqiqətən, üniversal ayırıcının varlığı həqiqətdən uzaqdır. Bu, NLP sahəsində bilgi göstəricisi və razılaşması (KR) uygulamasını çox çətinliyə qurtardı və buna da KR tabanlı NLU sistemlərinin düzgün değerlendirməsini engel etdi. Bizim məqsədimiz təbiətli dil anlama üçün KR tabanlı sistemlər inşa etməkdir. Biz Deeply İfadə edilən Bilim Representation & Reasoning (DeepEKR) adlı metodlarını təklif edirik ki, parçacıyı neural a ğ ilə əvəz edirik, simbolik göstəricisini yumşaltırıq ki, parçacılıq nöral ağ arasında deterministik mapping mövcud olmasın və sonunda simbolik çözücüyü eşit bir nöral ağ ilə əvəz edirik. modeli sona qədər təhsil edilə bilər. Biz öz metodumuzu QuaRTz və QuaRel işlərində iki faydalanıb verilən verilənlərin problemi çəkməsi haqqında qiymətləndiririk. Bizim sistemimiz QuaRTz'in qurbanlıq dəqiqliyi ilə eyni dəqiqliyinə nail edir, QuaRel'in qurbanlıq dəqiqliyinə səbəb istifadə edir və nəticəli KR sistemini çox üstün edir. Sonuçlar göstərir ki, KR çətinlikləri ilə tanınmış bias bunu daha yaxşı bir şey etməyə mane etməz.\nsonun işi. Əksinə, bizim metodumuz KR yaxınlığı ilə tanıdıqları təsirlərə görə yorumlaşdırılabilir.", 'bs': 'Uspješna primjena predstavljanja znanja i razloga (KR) u razumijevanju prirodnog jezika (NLU) u velikoj mjeri ograničava dostupnost robnog i općeg cilja prirodnog razmatrača jezika. Iako su pokrenuti nekoliko projekta u potrazi za razvojom univerzalnog jezika predstavljanja značaja, postojanje tačnog univerzalnog analizatora je daleko od stvarnosti. To je ozbiljno ograničilo primjenu zastupanja znanja i razmatranja (KR) na polju NLP-a i spriječilo je odgovarajuću procjenu sustava NLU-a na KR-u. Naš cilj je da izgradimo sisteme zasnovane na KR-u za razumijevanje prirodnog jezika bez oslanjanja na analizatora. Za to predlažemo metodu po imenu Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) gdje zamijenimo analizatora neuralnom mrežom, omekšamo simboličku predstavu tako da postoji deterministička mapa između analizatorske neuralne mreže i interpretabilnog logičkog oblika, i konačno zamijenimo simboličkog rješavača ekvivalentnom neuralnom mrežom, Dakle, model može biti obučen na kraju do kraja. Procjenjujemo našu metodu u odnosu na zadatak problema s kvalitativnim riječima rješavanja na dvije dostupne datasete (QuaRTz i QuaRel). Naš sistem postiže isto tačnost kao i tačnost države umjetnosti na QuaRTzu, iznosi stanje umjetnosti na QuaRelu i teško iznosi tradicionalni sistem temeljeni na KR-u. Rezultati pokazuju da predrasude koje je uvodila rješenje KR-a ne sprječava da radi bolje.\nPosao na kraju zadatka. Osim toga, naš metod je interpretabilan zbog pristrasnosti uvedenog pristupom KR-a.', 'fi': 'Knowledge Representation and Reasoning (KR) onnistuneen soveltamisen luonnollisen kielen ymmärtämisessä (NLU) rajoittaa pitkälti vankan ja yleiskäyttöisen luonnollisen kielen jäsentäjän saatavuus. Vaikka useita projekteja on käynnistetty universaalisen merkityksen esittämiskielen kehittämiseksi, tarkan universaalisen jäsentäjän olemassaolo on kaukana todellisuudesta. Tämä on rajoittanut merkittävästi tietämyksen esittämistä ja päättelyä NLP:n alalla ja estänyt myös KR:iin perustuvien NLP:n järjestelmien asianmukaisen arvioinnin. Tavoitteenamme on rakentaa KR-pohjaisia järjestelmiä luonnollisen kielen ymmärtämiseen ilman, että luotamme jäsentäjään. Tätä varten ehdotamme menetelmää nimeltä DeepEKR (DeepEKR), jossa parseri korvataan neuroverkolla, symbolista esitystä pehmennetään siten, että parserin neuroverkon ja tulkittavan loogisen muodon välillä on deterministinen kartoitus ja symbolinen ratkaisija korvataan vastaavalla neuroverkolla. Jotta malli voidaan kouluttaa päästä päähän. Arvioimme menetelmäämme laadullisen sanaongelmanratkaisun osalta kahdessa käytettävissä olevassa aineistossa (QuaRTz ja QuaRel). Järjestelmämme saavuttaa saman tarkkuuden kuin QuaRTz:n huipputekninen tarkkuus, ylittää QuaRelin huipputekniikan ja ylittää vakavasti perinteisen KR-pohjaisen järjestelmän. Tulokset osoittavat, että KR-ratkaisun aiheuttama harha ei estä sitä tekemästä parempaa\ntyö lopussa tehtävä. Lisäksi menetelmämme on tulkittavissa KR-menetelmän tuoman puolueellisuuden vuoksi.', 'et': 'Teadmiste esindamise ja mõistmise (KR) edukas rakendamine looduskeele mõistmises (NLU) on suures osas piiratud tugeva ja üldise otstarbega looduskeele parser olemasoluga. Kuigi universaalse tähenduseesinduskeele arendamiseks on käivitatud mitmeid projekte, on täpse universaalse parseri olemasolu kaugel reaalsusest. See on tõsiselt piiranud teadmiste esitamise ja arutluse rakendamist uue õppekava valdkonnas ning takistanud ka KR-põhiste uue õppekava süsteemide nõuetekohast hindamist. Meie eesmärk on luua KR-põhised süsteemid loodusliku keele mõistmiseks ilma parserile tuginemata. Selleks pakume välja meetodi nimega DeepEKR (DeepEKR), kus asendame parseri närvivõrguga, pehmendame sümboolset esitust nii, et parseri närvivõrgu ja tõlgendatava loogilise vormi vahel eksisteerib deterministlik kaardistamine ning lõpuks asendame sümboolse lahendaja samaväärse närvivõrguga. et mudelit saaks treenida otsast otsani. Hindame oma meetodit seoses sõnaprobleemide kvalitatiivse lahendamise ülesandega kahel olemasoleval andmekogumil (QuaRTz ja QuaRel). Meie süsteem saavutab sama täpsuse kui QuaRTz-i tipptasemel täpsus, ületab QuaRel-i tipptasemel ja tõsiselt ületab traditsioonilist KR-põhist süsteemi. Tulemused näitavad, et KR lahendusega kaasnev eelarvamus ei takista seda paremini saavutamast.\ntöö lõpus ülesanne. Lisaks on meie meetod tõlgendatav KR lähenemisviisi eelarvamuste tõttu.', 'af': "Suksesfol toepassing van kennis voorstel en redenings (KR) in Natuurlike Taal Verstaan (NLU) is groot beperk deur die beskikbaarheid van 'n kragtige en algemene doel natuurlike taal ontwerker te beskikbaar. Alhoewel veelvuldige projekte lanseer is in die volg van 'n universele betekening voorstelling taal te ontwikkel, is die bestaande van 'n presies universele ontwikkelder ver van die werklikheid. Hierdie het swaar beperk die toepassing van kennis voorsien en redering (KR) in die veld van NLP en ook voorgestel 'n regte evaluering van KR gebaseerde NLU stelsels. Ons doel is om KR gebaseerde stelsels vir Natuurlike Taal Verstaan te bou sonder om op 'n tolker te vertrou. Tussen hierdie voorstel ons 'n metode genaamd Deeply ingebedde kennis voorstelling en redekening (DeepEKR) waar ons die ontvanger vervang deur 'n neuralnetwerk, sag die simboliese voorstelling sodat 'n deterministiese kaart bestaan tussen die ontvanger neuralnetwerk en die uittelbare logiese vorm, en eindelik vervang die simboliese oplosser deur 'n gelykbare neuralnetwerk, So die model kan die einde-na-einde opgelei word. Ons evalueer ons metode met respek na die taak van Kwaliteit Woord Probleme Oplos op die twee beskikbare datastelle (QuaRTz en QuaRel). Ons stelsel bereik dieselfde presies as wat van die state-of-the-art-presies op QuaRTz, uitvoer die state-of-the-art op QuaRel en swaar uitvoer 'n tradisionele KR gebaseerde stelsel. @ info\nwerk by die einde taak. Ook, ons metode is uittelbaar vanweë die voorspoedige wat deur die KR toegang ingestel is.", 'ha': "@ item: inmenu Text Completion Haƙĩƙa, kuma kõ da an gabatar da wasu Projeko masu cikin bayan ya buɗe wata lugha mai fassara da fassara a universal, sai an ƙididdige wani parse na daidaita a universal, mai nĩsa ne daga gaskiya. Wannan ya ƙayyade shiryoyin shiryoyin shaidar da aka halatta da ilmi da inganci (KR) cikin field NLP kuma ya kange an ƙaddara wani hakki na tsarin KR-na'urar NLU. Gagonmu na sami tsarin KR masu basdar da KR wa Lugha Kiasala, Babu ƙayyade da wani mai salo. Ga wannan, Munã buɗa wani shiri wanda aka suna Reposition da Cilmi da Akwai da Akwai na Deƙash EKR (DeƙaseEKR) a inda muna bada Parser da shirin neural, yana buɗe gwargwadon shaidar, dõmin a ƙudura kartoni na ƙayyade tsakanin shirin Parser na neural da fomat mai fassarawa na fassara, kuma finally, za'a musanya solar alama da shirin neural da ke daidaita, Aka iya iya yin shirin motsi zuwa ƙarami. Tuna ƙaddara hanyonmu game da aikin Qualitative word Problem da Muke yarda a kan data-set biyu (QuaRTz da QuaRel). Yafukanmu yana sãmun taƙaitacce kamar da tsarin-halin-sanar ta kan QuaRTz, yana tafiyar da halin-sanar kan QuaRel kuma yana samar da wata na'urar KR wanda ya ƙayyade. Mataimakin ya nuna cewa rabo da aka introduce na KR ba ya hana ta daga samar da mafi alhẽri\nQUnicodeControlCharacterMenu Kayya, hanyoyinmu za'a fassara game da misãlai da aka fara na KR.", 'sk': 'Uspešno uporabo predstavljanja znanja in razumevanja (KR) v razumevanju naravnega jezika (NLU) je v veliki meri omejena z razpoložljivostjo robustnega in splošnega razčlenjevalnika naravnega jezika. Čeprav je bilo začetih več projektov za razvoj univerzalnega jezika predstavitve pomena, obstoj natančnega univerzalnega razčlenjevalnika daleč od realnosti. To je močno omejilo uporabo zastopanja znanja in razmišljanja (KR) na področju NLP in preprečilo tudi ustrezno ocenjevanje sistemov NLP, ki temeljijo na KR. Naš cilj je graditi KR osnovane sisteme za razumevanje naravnega jezika brez zanašanja na razčlenjevalnik. V ta namen predlagamo metodo z imenom Deeply Embedded Knowledge Representation & Reasoning (DeepEKR), kjer razčlenjevalnik zamenjamo z nevronskim omrežjem, mehčamo simbolno reprezentacijo tako, da obstaja deterministično kartiranje med razčlenjevalnim nevronskim omrežjem in razložljivo logično obliko, in na koncu simbolni reševalnik zamenjamo z enakovrednim nevronskim omrežjem, Tako da je model mogoče trenirati od konca do konca. Našo metodo ocenjujemo glede na nalogo kvalitativnega reševanja besednih problemov na dveh razpoložljivih naborih podatkov (QuaRTz in QuaRel). Naš sistem dosega enako natančnost kot najsodobnejša natančnost na QuaRTz, presega najsodobnejše na QuaRelu in močno presega tradicionalni sistem KR. Rezultati kažejo, da pristranskost, ki jo uvaja KR rešitev, ne preprečuje boljšega delovanja\nSlužba na koncu naloge. Poleg tega je naša metoda razložljiva zaradi pristranskosti, ki jo uvaja pristop KR.', 'jv': 'Aplikasi layang kanggo ngerasai Rayakno Pangan (Kr) ning langgambar Daerah Sampeyan (NLU) dumadhi kapan nguasai kapan pawaran saka nggo ndelokipe pawaran oleh tur ayula sing ngendalikno saben. Singang sampeyan akeh proyek sing sampeyan nang nggawe akeh lan nguasai tresnaning ing ing sakjane universel, sampeyan karo panelusuran akeh barang sampeyan kuwi kudu berlaké. Iki wis nggawe akeh sistem sing dipunangé awakdhéwé éntuk karo perusahaan (Kr) ing kapan NLP lan nganggep kuwi nggawe sistem NLU kang dipunangé nggawe barang apik. Kita goal punika nggawe sistem sing paling Kre dumadhi kanggo langkang Nalikar Sampeyan kuwi nggawe layang. Awak dhéwé siweh gunakake method nganggo deep pliy embeded knownès representation and ratiosuning Laptop" and "Desktop Awakdhéwé éntuk dhéwé nggawe barang nggambar cara-cara sing luwih Gambar uwong gawe dataset sing dibutuhke Sistem awak dhéwé iso nggawe barang karo hal-karang sing perusahaan karo Kjart, iso nggawe barang-perusahaan karo Kjart, iso nggawe sistem sing basa perusahaan karo Kjart. Rejali wong pada bias diuntingi kanggo ngerasakno kang aksi kr ora bisa ngubah sing luwih apik.\nbookmarks Laptop" and "Desktop', 'bo': 'མཐའ སྤྱི་ཚོགས་ཀྱི་གནས་ཚུལ་མང་པོ་ཞིག་འགོ་འཛུགས་ཡོད་ནའང་མིན་པར་སྤྱི་ཚོགས འདིས་NLP་གི་སྒེའུ་ཁུང་ནང་དུ་ཆ་རྟོགས་དང་བསམ་བློ་གཏོང་གི་ཉེར་སྤྱོད་ཀྱི་ཚད་གཞི་ཡོད་པ་དང་ཉེར་སྤྱོད་ཀྱི་ཉེར་སྤྱོད་ལ་ཉེན་ཁ་ཡོད ང་ཚོའི་དམིགས་ཡུལ་ནི། KR རིགས་ལ་རྨས་གཞི་བྱས་པའི་སྐད་ཡིག་ཆ་རྟོགས་ཀྱི་མ་ལག་ཅིག་བཟོ་རྒྱུ་ཡིན། Towards this we propose a method named Deeply Embedded Knowledge Representation & Reasoning (DeepEKR) where we replace the parser by a neural network, soften the symbolic representation so that a deterministic mapping exists between the parser neural network and the interpretable logical form, and finally replace the symbolic solver by an equivalent neural network, དེར་བརྟེན། མ་དབྱིབས་ལྟར་མཇུག་མཇུག་བསྡུ་འགྲོ་བ་ཡོད། ང་ཚོས་ཐབས་ལམ་ལ་སྤྱོད་པའི་གནད་སྡུད་གཞུང་ཚབ་གཉིས་ཀྱིས་མཐུན་སྐྱོང་བའི་ཚིག་གནད་དོན་གྱི་བྱ་རིམ་དང་གདོང་ལེན་ག ང་ཚོའི་མ་ལག་གིས་QuaRTz་ལ་གནས་སྐབས་གཟུགས་འགྱུར Results show that the bias introduced by a KR solution does not prevent it from doing better.\nམཇུག་མམ་གྱི་ལས་ཀ་རེད། ད་དུང་། ང་ཚོའི་ཐབས་ལམ་དེ་སྔོན་སྒྲིག་དབྱིབས་གཟུགས་རིས་བྱུང་བ་རེད།', 'he': 'שימוש מוצלח של מייצג ומגיון ידע (KR) בהבנת שפת טבעית (NLU) מוגבל בעיקר על ידי זמינות של מעבד שפת טבעית חזק ומטרה כללית. למרות שפרוייקטים רבים הופעלו במרדף לפיתוח שפת ייצוג של משמעות אוניברסלית, הקיום של חוקר אוניברסלי מדויק הוא רחוק מהמציאות. זה הגביל באופן רציני את השימוש של מייצג ידע וההגיון (KR) בשטח NLP וגם מנע מערכות NLU המבוססות על KR. המטרה שלנו היא לבנות מערכות מבוססות על KR כדי להבין שפה טבעית בלי לסמוך על מעבד. לעבר זה אנו מציעים שיטה בשם מייצג ומגיון ידע עמוק מעורבים (DeepEKR) שבו אנו מחליפים את המחקר על ידי רשת עצבית, לרפק את הייצג הסימבוליסטי כך שיקיים מפה קבועה בין רשת עצבית המחקר והצורה הגיונית הנפורסמת, ולבסוף להחליף את המפתר הסימבוליסטי על ידי רשת עצבית שווה, כדי שהמודל יוכל להיות מאומן סוף-סוף. We evaluate our method with respect to the task of Qualitative Word Problem Solving on the two available datasets (QuaRTz and QuaRel).  המערכת שלנו משיגה את אותה מדויקת כמו של מדויקת המדינה של קווארץ, מובילה את המדינה המדינה על קווארל ומביאה בצורה רצינית מערכת מסורתית מבוססת KR. התוצאות מראות שהההתמחות המוציאה על ידי פתרון KR לא מונעת ממנו לעשות\nעבודה בסוף המשימה. Moreover, our method is interpretable due to the bias introduced by the KR approach.'}
