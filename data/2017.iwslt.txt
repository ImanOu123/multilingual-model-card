{'en': 'Overview of the IWSLT 2017 Evaluation Campaign IWSLT  2017 Evaluation Campaign', 'ar': 'نظرة عامة على حملة تقييم IWSLT 2017', 'fr': "Aperçu de la campagne d'évaluation IWSLT 2017", 'es': 'Descripción general de la campaña de evaluación de IWSLT 2017', 'pt': 'Visão geral da campanha de avaliação do IWSLT 2017', 'ja': 'IWSLT 2017評価キャンペーンの概要', 'zh': 'IWSLT 2017 估概述', 'hi': 'IWSLT 2017 मूल्यांकन अभियान का अवलोकन', 'ru': 'Обзор Кампании по оценке IWSLT 2017', 'ga': 'Forbhreathnú ar Fheachtas Measúnaithe 2017 IWSLT', 'el': 'Επισκόπηση της εκστρατείας αξιολόγησης του IWSLT 2017', 'hu': 'Az IWSLT 2017 értékelési kampányának áttekintése', 'ka': 'IWSLT 2017-ის განსაზღვრება კამპანიის დანახვა', 'kk': '2017 IWSLT бағалау кампаниясының қарау', 'it': 'Panoramica della campagna di valutazione IWSLT 2017', 'ml': 'IWSLT 2017 പ്രവര്\u200dത്തിപ്പിക്കുന്ന ക്യാമ്പെയിന്\u200dറെ നിരീക്ഷിക്കുക', 'mt': 'Ħarsa ġenerali lejn il-Kampanja ta’ Evalwazzjoni tal-IWSLT 2017', 'lt': '2017 m. IWSLT vertinimo kampanijos apžvalga', 'mk': 'Преглед на кампањата за евалуација на IWSLT 2017', 'ms': 'Pandangan ringkasan Kampanye Evaluasi IWSLT 2017', 'mn': '2017 оны IWSLT-ын үнэлгээ дэвшүүлэх кампания', 'no': 'Oversyning av evalueringskampanjen IWSLT 2017', 'pl': 'Przegląd kampanii ewaluacyjnej IWSLT 2017', 'ro': 'Prezentare generală a campaniei de evaluare IWSLT 2017', 'si': 'IWSLT 2017 විශ්ලේෂණ කැමැන්ප්යාන්ස් ගැන බලන්න', 'sr': 'Overview of the IWSLT 2017 Evaluation Campaign', 'so': 'Overview of the IWSLT 2017 campaign evaluation', 'sv': 'Översikt över utvärderingskampanjen för IWSLT 2017', 'ta': 'Overview of the IWSLT 2017 Evaluation Campaign', 'ur': 'IWSLT 2017 ارزیابی کمپین کی اوورویز', 'uz': 'IWSLT 2017 tasdiqlash campaigni koʻchirish', 'vi': 'Ghi chép chiến dịch đánh giá IWSLT 87', 'nl': 'Overzicht van de IWSLT 2017 Evaluatiecampagne', 'da': 'Oversigt over evalueringskampagnen IWSLT 2017', 'bg': 'Преглед на кампанията за оценка на ИВСЛТ 2017', 'hr': 'Pregled kampanje za procjenu IWSLT 2017.', 'de': 'Überblick über die IWSLT 2017 Evaluationskampagne', 'id': 'Penampilan dari Kampanye Evaluasi IWSLT 2017', 'ko': 'IWSLT 2017년 평가 활동 개요', 'sw': 'Mapitio ya kampeni ya Uthibitisho ya IWSLT 2017', 'tr': 'IWSLT 2017 Taýýarlama Kampanyasynyň Görnöşi', 'fa': 'Overview of the IWSLT 2017 Evaluation Campaign', 'sq': 'Shfaqja e përgjithshme e fushatës së vlerësimit të IWSLT 2017', 'af': 'Oorskou van die IWSLT 2017 Evalueringskampanie', 'am': 'የIWSLT 2017 ማስታወቂያ ዘመቻ ላይ ማየት', 'hy': '2017-ի IW-ՍԼT-ի գնահատման քարոզարշավի վերաբերյալ', 'bn': 'IWSLT ২০১৭ সালের প্রচারাভিযানের উপর দেখা যাচ্ছে', 'az': 'IWSLT 2017 Yaxınlıq Kampanyası', 'ca': "Vista general de la campanya d'evaluació IWSLT 2017", 'et': 'Ülevaade IWSLT 2017 hindamiskampaaniast', 'cs': 'Přehled hodnotící kampaně IWSLT 2017', 'bs': 'Pregled kampanje za procjenu IWSLT 2017', 'fi': 'Yleiskatsaus IWSLT 2017 -arviointikampanjaan', 'jv': 'Tarjamah ning kampanyen IWSLT 1997', 'he': 'Overview of the IWSLT 2017 Evaluation Campaign', 'sk': 'Pregled ocenjevalne kampanje IWSLT 2017', 'ha': 'QUnicodeControlCharacterMenu', 'bo': 'IWSLT 2017 རྩིས་ལྟར་བསྡུས་གོང་གི་བསྡུས་ལྟ་བ་'}
{'en': 'The IWSLT 2017 evaluation campaign has organised three tasks. The Multilingual task, which is about training  machine translation systems  handling many-to-many language directions, including so-called zero-shot directions. The Dialogue task, which calls for the integration of context information in  machine translation , in order to resolve  anaphoric references  that typically occur in human-human dialogue turns. And, finally, the Lecture task, which offers the challenge of automatically transcribing and translating real-life university lectures. Following the tradition of these reports, we will described all tasks in detail and present the results of all runs submitted by their participants.', 'ar': 'نظمت حملة التقييم IWSLT 2017 ثلاث مهام. المهمة متعددة اللغات ، والتي تتعلق بتدريب أنظمة الترجمة الآلية على التعامل مع اتجاهات اللغة من عدة إلى عدة ، بما في ذلك ما يسمى اتجاهات بدون طلقة. مهمة الحوار ، التي تدعو إلى تكامل معلومات السياق في الترجمة الآلية ، من أجل حل الإشارات الجذابة التي تحدث عادةً في المنعطفات بين الإنسان والبشر. وأخيرًا ، مهمة المحاضرة ، والتي تمثل تحديًا لكتابة وترجمة المحاضرات الجامعية الواقعية تلقائيًا. وفقًا لتقليد هذه التقارير ، سنصف جميع المهام بالتفصيل ونقدم نتائج جميع الدورات التي قدمها المشاركون.', 'es': 'La campaña de evaluación de IWSLT 2017 ha organizado tres tareas. La tarea multilingüe, que consiste en capacitar a los sistemas de traducción automática que manejan instrucciones de muchos a muchos idiomas, incluidas las llamadas direcciones cero. La tarea Diálogo, que exige la integración de la información de contexto en la traducción automática, con el fin de resolver las referencias anafóricas que suelen ocurrir en los turnos de diálogo humano-humano. Y, por último, la tarea Lecture, que ofrece el desafío de transcribir y traducir automáticamente las conferencias universitarias de la vida real. Siguiendo la tradición de estos informes, describiremos todas las tareas en detalle y presentaremos los resultados de todas las corridas enviadas por sus participantes.', 'pt': 'A campanha de avaliação do IWSLT 2017 organizou três tarefas. A tarefa multilíngue, que trata do treinamento de sistemas de tradução automática que lidam com direções de muitos para muitos idiomas, incluindo as chamadas direções de tiro zero. A tarefa de Diálogo, que exige a integração de informações de contexto na tradução automática, a fim de resolver as referências anafóricas que normalmente ocorrem em turnos de diálogo humano-humano. E, finalmente, a tarefa Lecture, que oferece o desafio de transcrever e traduzir automaticamente palestras universitárias da vida real. Seguindo a tradição desses relatórios, descreveremos detalhadamente todas as tarefas e apresentaremos os resultados de todas as corridas enviadas por seus participantes.', 'fr': "La campagne d'évaluation IWSLT 2017 a organisé trois tâches. La tâche multilingue, qui consiste à former des systèmes de traduction automatique prenant en charge des directions de plusieurs langues vers plusieurs, y compris les directions dites «\xa0zero-shot\xa0». La tâche Dialogue, qui demande l'intégration d'informations contextuelles dans la traduction automatique, afin de résoudre les références anaphoriques qui se produisent généralement dans les tournures de dialogue homme-homme. Enfin, la tâche Lecture, qui présente le défi de transcrire et de traduire automatiquement des cours universitaires réels. Conformément à la tradition de ces rapports, nous décrirons toutes les tâches en détail et présenterons les résultats de toutes les courses soumises par leurs participants.", 'ja': 'IWSLT 2017評価キャンペーンは、3つのタスクを組織しています。マルチリンガルタスクは、いわゆるゼロショットの方向性を含む、多言語から多言語の方向性を扱う機械翻訳システムのトレーニングに関するものです。機械翻訳における文脈情報の統合を要求するDialogueタスクは、人間と人間の対話で典型的に発生する類推的な参照を解決するために行われる。そして最後に、講義タスクは、現実の大学の講義を自動的に文字起こしして翻訳するという課題を提供します。これらのレポートの伝統に従って、すべてのタスクを詳細に説明し、参加者が提出したすべての実行の結果を提示します。', 'hi': 'IWSLT 2017 मूल्यांकन अभियान ने तीन कार्यों का आयोजन किया है। बहुभाषी कार्य, जो प्रशिक्षण मशीन अनुवाद प्रणालियों के बारे में है जो तथाकथित शून्य-शॉट दिशाओं सहित कई-से-कई भाषा दिशाओं को संभालता है। संवाद कार्य, जो मशीन अनुवाद में संदर्भ जानकारी के एकीकरण के लिए कहता है, ताकि आमतौर पर मानव-मानव संवाद मोड़ में होने वाले एनाफोरिक संदर्भों को हल किया जा सके। और, अंत में, व्याख्यान कार्य, जो वास्तविक जीवन विश्वविद्यालय व्याख्यानों को स्वचालित रूप से अनुवाद करने और अनुवाद करने की चुनौती प्रदान करता है। इन रिपोर्टों की परंपरा का पालन करते हुए, हम सभी कार्यों का विस्तार से वर्णन करेंगे और उनके प्रतिभागियों द्वारा प्रस्तुत किए गए सभी रनों के परिणाम प्रस्तुत करेंगे।', 'ru': 'Кампания по оценке IWSLT 2017 организовала три задачи. Многоязычная задача, которая заключается в обучении систем машинного перевода, работающих со многими языковыми направлениями, включая так называемые направления с нулевым выстрелом. Задача Диалог, которая призывает к интеграции контекстной информации в машинный перевод, чтобы разрешить анафорические ссылки, которые обычно происходят в человеко-человеческом диалоге. И, наконец, задача Лекции, которая предлагает задачу автоматического транскрибирования и перевода реальных университетских лекций. Следуя традиции этих отчетов, мы подробно опишем все задачи и представим результаты всех прогонов, представленных их участниками.', 'zh': 'IWSLT 2017估三务。 多言事,此机器翻译统多方,所谓零射者也。 言事,求机器翻译上下文息,以解常人之厌用。 最后讲座务,给自转录及译现实生活中大学讲座挑战。 循其故事,详细描述其职事,以展参与者行之终。', 'ga': 'Tá trí thasc eagraithe ag feachtas meastóireachta IWSLT 2017. Is é an tasc Ilteangach, a bhaineann le córais aistriúcháin meaisín a oiliúint a láimhseálann treoracha teanga go leor, lena n-áirítear treoracha nialasacha mar a thugtar orthu. An tasc Idirphlé, a éilíonn comhtháthú faisnéise comhthéacs san aistriúchán meaisín, d’fhonn a réiteach tagairtí anaiféaracha a tharlaíonn go hiondúil i casadh idirphlé daonna-daonna. Agus, ar deireadh, tasc an Léachta, a thugann an dúshlán a bhaineann le fíor-léachtaí ollscoile a thras-scríobh agus a aistriú go huathoibríoch. De réir thraidisiún na dtuarascálacha seo, cuirfimid síos go mion ar na tascanna go léir agus cuirfimid i láthair torthaí na ritheanna go léir a chuir a rannpháirtithe isteach.', 'ka': 'IWSLT 2017-ის განსაზღვრების კამპანია სამი დავალების ორგანიზაცია. Multilingual task, which is about training machines translation systems handling many to-many languages directions, including so-called zero-shot directions. დიალოგის დავალება, რომელიც მაქსინური შეტყობინებაში კონტექსტური ინფორმაციის ინტერგურაციას მითხოვს, ანაფორიკური რეფენციების გარეშე, რომელიც ადამიანის-ადამიანის დი და საბოლოოდ, ლექტურაციის დავალება, რომელიც ავტომატურად გადაწერა და გადაწერა რეალური სიცოცხლე სუნივერტის ლექტურების გამოსახულება. ოჲჟლვ რპაეთუთწრა ნა რპაეთუთწრა ნა რვჱთ პვოჲპრაუთთ ღვ ჲბწჟნთმ გჟთფკთ პაბჲრთ თ ღვ ოპვეჟრაგთმ გჟთფკთ პვჱსლრართ ნა გჟთფკთ პაბჲრთ, კჲთრჲ ჟა ოპვეჟრ', 'hu': 'Az IWSLT 2017 értékelő kampánya három feladatot szervezett. A többnyelvű feladat, amely a gépi fordító rendszerek képzéséről szól, amelyek sok-sok nyelvi irányt kezelnek, beleértve az úgynevezett nulla-lövés irányt is. A Párbeszéd feladat, amely a kontextusinformációk gépi fordításba történő integrálását igényli, hogy megoldják az ember-ember párbeszédben jellemzően előforduló anafórikus referenciákat. És végül a Lecture feladat, amely a valós életű egyetemi előadások automatikus átírását és fordítását jelenti. A jelentések hagyományait követve részletesen ismertetjük az összes feladatot, és bemutatjuk a résztvevők által benyújtott összes futás eredményét.', 'el': 'Η εκστρατεία αξιολόγησης του IWSLT 2017 διοργάνωσε τρία καθήκοντα. Το πολυγλωσσικό έργο, το οποίο αφορά την εκπαίδευση συστημάτων μηχανικής μετάφρασης που χειρίζονται πολλές προς πολλές γλωσσικές κατευθύνσεις, συμπεριλαμβανομένων των λεγόμενων κατευθύνσεων μηδενικού πυροβολισμού. Το έργο Διάλογος, το οποίο απαιτεί την ενσωμάτωση πληροφοριών περιβάλλοντος στη μηχανική μετάφραση, προκειμένου να επιλυθούν αναφορικές αναφορές που συνήθως εμφανίζονται στον ανθρώπινο-ανθρώπινο διάλογο στροφές. Και, τέλος, το έργο Διάλεξης, το οποίο προσφέρει την πρόκληση της αυτόματης μεταγραφής και μετάφρασης πραγματικών πανεπιστημιακών διαλέξεων. Ακολουθώντας την παράδοση των εκθέσεων αυτών, θα περιγράψουμε λεπτομερώς όλες τις εργασίες και θα παρουσιάσουμε τα αποτελέσματα όλων των διαδρομών που υποβλήθηκαν από τους συμμετέχοντες τους.', 'it': "La campagna di valutazione IWSLT 2017 ha organizzato tre compiti. Il compito multilingue, che consiste nella formazione di sistemi di traduzione automatica che gestiscono molte direzioni linguistiche, comprese le cosiddette direzioni zero-shot. Il compito Dialogo, che richiede l'integrazione delle informazioni di contesto nella traduzione automatica, al fine di risolvere i riferimenti anaforici che tipicamente si verificano nel dialogo umano-umano gira. E, infine, il compito Lecture, che offre la sfida di trascrivere e tradurre automaticamente le lezioni universitarie reali. Seguendo la tradizione di questi report, descriveremo tutti i compiti in dettaglio e presenteremo i risultati di tutte le prove presentate dai loro partecipanti.", 'lt': '2017 m. IWSLT vertinimo kampanijoje buvo surengtos trys užduotys. Daugiakalbė užduotis, susijusi su mašinų vertimo sistemų mokymu, valdančiu daugelį ar daugelį kalbų krypčių, įskaitant vadinamąsias nulinės nuotraukos kryptis. Dialogo užduotis, kuria raginama integruoti kontekstinę informaciją į vertimą mašinomis, siekiant išspręsti anaforines nuorodas, kurios paprastai atsiranda žmogaus ir žmogaus dialoge. And, finally, the Lecture task, which offers the challenge of automatically transcribing and translating real-life university lectures.  Atsižvelgdami į šių pranešimų tradiciją, išsamiai aprašysime visas užduotis ir pristatysime visų jų dalyvių pateiktų rezultatus.', 'ml': 'IWSLT 2017 വിലാസപ്രകാരം മൂന്നു ജോലികള്\u200d സംഘടിപ്പിച്ചിരിക്കുന്നു. ഒരുപാട് ഭാഷകളുടെ നേര്\u200dവഴികള്\u200d കൈകാര്യം ചെയ്യുന്ന പല ഭാഷകളുടെയും പരിശീലനത്തെക്കുറിച്ചാണ് പല ഭാഷകളുടെയും പരിശീലനത്തെക മെഷിന്\u200d പരിഭാഷയിലെ വിവരങ്ങള്\u200d ഒരുമിച്ചുകൂട്ടുവാന്\u200d വിളിക്കുന്ന ഡയലോഗ് ജോലിയുടെ പ്രവര്\u200dത്തനം മനുഷ്യന്\u200d ഡയലോഗില്\u200d സാധാരണ സംഭവിക്കുന അവസാനം, ലെക്ട്രൂര്\u200d ജോലിയുടെ പ്രഭാഷണങ്ങള്\u200d സ്വയമായി വിവരിച്ചുകൊടുക്കുന്നതിന്\u200dറെയും വിലാസങ്ങള്\u200dക്ക് നല്\u200dകുന്നു. ഈ റിപ്പോര്\u200dട്ടുകളുടെ പാരമ്പര്യം പിന്നീട് നമ്മള്\u200d എല്ലാ ജോലികളെയും വിശദീകരിച്ചു അവരുടെ പങ്കുകാളികളായ എല്ലാ ഓട്ടുകളു', 'ms': 'Kampanye penilaian IWSLT 2017 telah mengatur tiga tugas. Tugas berbilang bahasa, yang berkaitan dengan latihan sistem terjemahan mesin yang mengendalikan arah-arah bahasa-banyak, termasuk arah-arah yang dipanggil 0-shot. Tugas Dialog, yang meminta integrasi maklumat konteks dalam terjemahan mesin, untuk menyelesaikan rujukan anafora yang biasanya berlaku dalam dialog manusia-manusia berubah. And, finally, the Lecture task, which offers the challenge of automatically transcribing and translating real-life university lectures.  Menurut tradisi laporan ini, kami akan menjelaskan semua tugas secara terperinci dan memperkenalkan hasil semua jalanan yang dihantar oleh peserta mereka.', 'mk': 'Кампањата за проценка на IWSLT 2017 организираше три задачи. Мултијазичната задача, која е во врска со обуката на машинските преведувачки системи кои се справуваат со многу до многу јазички насоки, вклучувајќи ги и таканаречените нула-стрелачки насоки. Работата на Дијалогот, која повикува на интеграција на контекстни информации во машинскиот превод, со цел решавање на анафорските референции кои обично се случуваат во дијалогот помеѓу луѓето и луѓето. And, finally, the Lecture task, which offers the challenge of automatically transcribing and translating real-life university lectures.  Following the tradition of these reports, we will described all tasks in detail and present the results of all runs submitted by their participants.', 'no': 'IWSLT-evalueringskampanjen 2017 har organisert tre oppgåver. Den fleirspråksoppgåva som er om opplæringssystemet for omsetjing av maskinen som handterar mange til mange språksretningar, inkludert så kalla null- retningar. Dialogoppgåva som kallar for integrering av kontekstinformasjon i maskineomsetjinga, for å løysa anaforiske referanser som vanlegvis skjer i menneskelig-menneskelig dialogvindauge. Og til slutt, læringsoppgåva, som tilbyr utfordringen av automatisk overskriving og omsetjing av universitetsleksjonar frå verkeleg liv. Etter tradisjonen av desse rapportene vil vi beskrive alle oppgåver detaljert og vise resultatet av alle køyringar som deltakarane sine sender.', 'mn': 'IWSLT 2017 оны оюутнуудын кампанийг 3 даалгавар зохион байгуулсан. Ихэнх хэлний даалгавар нь машины хөгжүүлэх системийн тухай олон хэл хэлний захиралд зохион байгуулагддаг. Энэ диалогын ажил нь машины хөрөнгө оруулалт дээр орчин үеийн мэдээллийг нэгтгэхэд хүн-хүн төрөлхтний диалог дээр ихэвчлэн тохиолддог анафорын холбоо шийдэхэд шаардлагатай. Эцэст нь сургалтын даалгаврыг автоматаар бичиж, жинхэнэ амьдралын сургуулийн лекцийг орлуулах зорилго өгдөг. Эдгээр мэдээллийн уламжлалын дараа бид бүх үйлдлийг нарийвчлан тайлбарлаж, оролцогчдын дамжуулагдсан бүх үйлдлийн үр дүн гаргах болно.', 'mt': "The IWSLT 2017 evaluation campaign has organised three tasks.  The Multilingual task, which is about training machine translation systems handling many-to-many language directions, including so-called zero-shot directions.  Il-kompitu tad-Djalogu, li jitlob l-integrazzjoni tal-informazzjoni tal-kuntest fit-traduzzjoni bil-magni, sabiex jiġu solvuti referenzi anaforiċi li tipikament iseħħu fid-djalogu bejn il-bniedem u l-bniedem. And, finally, the Lecture task, which offers the challenge of automatically transcribing and translating real-life university lectures.  Wara t-tradizzjoni ta' dawn ir-rapporti, se niddeskrivu l-kompiti kollha fid-dettall u nippreżentaw ir-riżultati tal-eżerċizzji kollha ppreżentati mill-parteċipanti tagħhom.", 'pl': 'W ramach kampanii ewaluacyjnej IWSLT 2017 zorganizowano trzy zadania. Zadanie wielojęzyczne, które polega na szkoleniu systemów tłumaczenia maszynowego obsługujących wiele do wielu kierunków językowych, w tym tzw. kierunki zero-shot. Zadanie Dialog, które wymaga integracji informacji kontekstowych w tłumaczeniu maszynowym, w celu rozwiązania anaforycznych odniesień, które zazwyczaj występują w zwrotach dialogu człowiek-człowiek. I wreszcie zadanie Wykładu, które stanowi wyzwanie automatycznego transkrypcji i tłumaczenia rzeczywistych wykładów uniwersyteckich. Zgodnie z tradycją tych raportów, szczegółowo opisujemy wszystkie zadania i przedstawimy wyniki wszystkich biegów zgłoszonych przez ich uczestników.', 'sr': 'Kampanja za procjenu IWSLT 2017 organizovala je tri zadatka. Mnogjezički zadatak, koji se radi o sistemima prevođenja mašina koji se bave mnogim na mnogim jezičkim uputama, uključujući takozvane upute na nulu pucnjavu. Диалог задача, који позволи да интеграцију контекста информација у преводу машина, за решање анафоричких референција који се обикновено дешају у реду људского-људского диалога. I konačno, zadatak lekcije, koji nudi izazov automatskog prepisanja i prevodenja lekcija u univerzitetu. Nakon tradicije ovih izveštaja, detaljno ćemo opisati sve zadatke i predstaviti rezultate svih pokreta podanih njihovim sudionicima.', 'ro': 'Campania de evaluare IWSLT 2017 a organizat trei sarcini. Sarcina multilingvă, care este despre instruirea sistemelor de traducere automată care gestionează direcții lingvistice multiple, inclusiv așa-numitele direcții zero-shot. Sarcina Dialog, care solicită integrarea informațiilor contextuale în traducerea automată, pentru a rezolva referințele anaforice care apar în mod obișnuit în dialogul om-om, se transformă. Și, în cele din urmă, sarcina Lecture, care oferă provocarea transcrierii și traducerii automate a prelegerilor universitare din viața reală. Urmând tradiția acestor rapoarte, vom descrie toate sarcinile în detaliu și vom prezenta rezultatele tuturor curselor depuse de participanții lor.', 'kk': 'IWSLT 2017 бағалау кампаниясы үш тапсырманы орнатады. Бірнеше тілдерді аудару жүйелерінде бірнеше тіл бағыттарын қамтамасыз ететін көптеген тапсырма, бұл үшін «zero-shot» бағыттары бар. Адам- адамдар диалогында әдетте болатын анафориялық сілтемелерді шешу үшін машина аудармасында контексті мәліметті біріктіру үшін шақыру диалогы. Соңғы жағдай, оқыту тапсырмасы, ол автоматты түрде университеттік лекцияларын аудару және аудару үшін көмек береді. Бұл хабарламалардың дәстүрінен кейін, бүкіл тапсырмаларды егжей- тегжейлі таңдап, олардың қатысушыларының жұмысының нәтижесін көрсетеді.', 'si': 'IWSLT 2017 විශ්ලේෂණ ක්\u200dරියාපනය වැඩි තුනක් සැකසුම් කරලා තියෙනවා. ගොඩක් භාෂාවක් වැඩය, ඒක තමයි පරීක්ෂණ පද්ධතියේ පද්ධතියේ භාෂාවක් ගොඩක් ප්\u200dරතිකාර කරනවා, ඒ වගේම සුන්ධ සංවාදය වැඩය, මිනිස්සු-මිනිස් සංවාදයේ සාමාන්\u200dය තොරතුරු සම්බන්ධය සම්බන්ධය කරන්න, මිනිස්සු වාර්තාවයෙන් සාමාන අන්තිමේදී, ලෙක්ස්කුර් වැඩය, ඒකෙන් ස්වයංක්\u200dරියාවිත විද්\u200dයාපිත විද්\u200dයාපිත විද්\u200dයාපිත විද්\u200dයාපිත විද්\u200d මේ වාර්තාවේ සාමාන්\u200dයය පස්සේ අපි හැම වැඩක්ම විස්තර කරනවා ඒ වගේම ඔවුන්ගේ අංශාවකයන්ගේ ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200d', 'sv': 'Utvärderingskampanjen IWSLT 2017 har organiserat tre uppgifter. Den flerspråkiga uppgiften, som handlar om att utbilda maskinöversättningssystem som hanterar många-till-många språkriktningar, inklusive så kallade noll-skottriktningar. Dialoguppgiften, som kräver integrering av kontextinformation i maskinöversättning, för att lösa anaforiska referenser som vanligtvis förekommer i mänsklig-mänsklig dialog vänder sig. Och slutligen föreläsningsuppgiften, som erbjuder utmaningen att automatiskt transkribera och översätta verkliga universitetsföreläsningar. I enlighet med traditionen av dessa rapporter kommer vi att beskriva alla uppgifter i detalj och presentera resultaten av alla löpningar som lämnats in av deras deltagare.', 'ta': 'IWSLT 2017 ஆய்வு விளையாட்டு மூன்று பணிகளை அமைத்துள்ளது. பல மொழிமொழியின் செயல், இது இயந்திர மொழிமாற்றி மொழிமாற்றும் அமைப்புகளை பயன்படுத்தும் முறைமைகளைப் பற்றி பல மொழிகள் திசைகள Name மேலும், இறுதியாக, விரிவாக்க செயல், அது தன்னியக்கமாக எழுதும் மற்றும் மொழிபெயர்ப்பு மற்றும் உண்மையான வாழ்க்கை கல்விகளை  இந்த அறிக்கைகளின் மாதிரி பின்னர், நாம் எல்லா பணிகளையும் விவரமாக விவரிக்க வேண்டும் மற்றும் அவர்கள் பங்கீடுபவர்களால் அனைத', 'ur': 'IWSLT 2017 ارزیابی کمپین نے تین کاموں کا سازمان کیا ہے۔ Multilingual task, which is about training machine translation systems handling many to-many languages directions, including so-called zero-shot directions. دیالوگ کا کام، جو ماشین ترجمہ میں کنٹیکس معلومات کی تفریق کے لئے دعوت دیتا ہے، اس لئے کہ انسان-انسان کے دیالوگ میں معلومات کی تفریق حل کریں۔ اور بالاخره، لکھنے کے کام، جو اپنے ساتھ دکھانے اور ترجمہ کرنے کی چال دیتا ہے، حقیقی زندگی والیونیوریٹی لکھانے کی۔ ان راپوروں کی پیدائش کے پیچھے ہم سب کاموں کو تفصیل کے ساتھ بیان کریں گے اور ان کے حضوروں کے ذریعہ ان کے حضور پیش کئے ہوئے تمام کاموں کا نتیجہ پیش کریں گے.', 'so': 'Campaiino qiimeynta ee IWSLT 2017 ayaa qabanqaabiyey saddex shaqo. Shaqada luuqadaha kala duduwan ee ku saabsan waxbarashada machine-turjumista oo qabanqaabinaya hagitaan luuqado badan, kuwaas oo ku qoran hagitaan zero-shot. Shaqada dialogue, kaasoo u yeedhaya la qabsashada macluumaadka dabiicadda ee turjumidda machine, si uu u xalliyo macluumaadka kaleemeysiga ee ku qoran qoraalka dadweynaha. Ugu dambaysta, shaqada Lecture, taas oo bixiya dhibaatada iskuul-qoritaanka iyo turjumidda jaamacadaha nololeedka ee halis ah. Taariikhda warqadahan kadib waxan si cad ah u qoraynaa shuqullada oo dhan, waxaana soo bandhigaynaa dhamaantood dhamaan waxyaabaha ay ka qeybqaaday.', 'uz': "The IWSLT 2017 evaluation campaign has organised three tasks.  Bir necha tillar vazifasi, bu haqida bir necha tildan ko'p tizimni boshqarish uchun mashina tarjima qilish tizimi haqida o'rganadi, shunday qo'llangan tizim nuqta yoʻnalishi mumkin. Name Va oxirida, Lektur vazifani avtomatik o'rganish va hayot universitetlarni o'rganish va tarjima qilish qismini anglatadi. Bu haqida maʼlumotlar taqdimotidan keyin biz hamma vazifalarni faqat bajaramiz va participlarning hamma ishlatilgan natijalarini hosil qilamiz.", 'vi': 'Lễ đánh giá IWSLT 87 đã tổ chức ba nhiệm vụ. Việc đa ngôn ngữ, vấn đề là các hệ thống đào tạo dịch cỗ máy xử lý nhiều hướng ngôn ngữ, bao gồm các hướng không bắn. Nhiệm vụ đối thoại, yêu cầu sự hoà nhập thông tin ngữ cảnh trong phiên dịch máy, để giải quyết các di chúc gốc mạo thường xảy ra trong cuộc đối thoại người-người. Và, cuối cùng, nhiệm vụ giảng dạy, đề nghị mở tự động biên tập và dịch những bài giảng đại học thật sự. Theo truyền thống của những báo cáo này, chúng tôi sẽ mô tả chi tiết mọi nhiệm vụ và trình bày kết quả của mọi hoạt động được gửi đi bởi những người tham gia.', 'bg': 'Кампанията за оценка организира три задачи. Многоезичната задача, която е свързана с обучението на системи за машинен превод, обработващи много към много езикови посоки, включително така наречените нулеви посоки. Задачата Диалог, която изисква интегриране на контекстната информация в машинния превод, за да се разрешат анафоричните препратки, които обикновено се появяват в диалога между човек и човек. И накрая, лекционната задача, която предлага предизвикателството автоматично да транскрибира и превежда реални университетски лекции. Следвайки традицията на тези доклади, ще опишем подробно всички задачи и ще представим резултатите от всички състезания, представени от участниците им.', 'hr': 'Kampanja za procjenu IWSLT 2017 organizirala je tri zadatka. Mnogjezički zadatak, koji se radi o sustavima prevođenja strojeva koji se bave mnogim na mnogim jezičkim smjerovima, uključujući takozvane upute za nulu pucnjavu. Zapovjednik dijaloga koji poziva na integraciju kontekstskih informacija u prevodu strojeva kako bi riješio anaforske referencije koje se obično događaju u okviru dijaloga ljudskih i ljudskih ljudi. I konačno, zadatak lekcije, koji nudi izazov automatskog prepisanja i prevodenja lekcija u univerzitetu. Nakon tradicije tih izvješća, detaljno ćemo opisati sve zadatke i predstaviti rezultate svih trčanja podanih njihovim učesnicima.', 'nl': 'De IWSLT 2017 evaluatiecampagne heeft drie taken georganiseerd. De meertalige taak, die gaat over het trainen van machinevertaalsystemen die veel-naar-vele taalrichtingen hanteren, waaronder zogenaamde zero-shot richtingen. De dialoogtaak, die vraagt om de integratie van contextinformatie in machinevertaling, om anaforische verwijzingen op te lossen die typisch voorkomen in de mens-mens dialoog draait. En tot slot de Lecture taak, die de uitdaging biedt om real-life university colleges automatisch te transcriberen en vertalen. In navolging van de traditie van deze rapporten beschrijven we alle taken gedetailleerd en presenteren we de resultaten van alle runs die door hun deelnemers zijn ingediend.', 'da': 'Evalueringskampagnen IWSLT 2017 har tilrettelagt tre opgaver. Flersproget opgave, der handler om at træne maskinoversættelsessystemer, der håndterer mange-til-mange sprogretninger, herunder såkaldte zero-shot retninger. Dialogopgaven, som kræver integration af kontekstinformation i maskinoversættelse, for at løse anaforiske referencer, der typisk forekommer i menneskelig-menneskelig dialog vender. Og endelig foredrag opgaven, som giver udfordringen til automatisk at transkribe og oversætte virkelige universitetsforedrag. I overensstemmelse med traditionen i disse rapporter vil vi beskrive alle opgaver i detaljer og præsentere resultaterne af alle løb, som deres deltagere har indsendt.', 'de': 'Die Evaluationskampagne IWSLT 2017 hat drei Aufgaben organisiert. Die Mehrsprachige Aufgabe, bei der es darum geht, maschinelle Übersetzungssysteme zu trainieren, die viele-zu-viele Sprachrichtungen handhaben, einschließlich sogenannter Null-Schuss-Richtungen. Die Dialogaufgabe, die die Integration von Kontextinformationen in die maschinelle Übersetzung erfordert, um anaphorische Referenzen aufzulösen, die typischerweise im Mensch-Mensch-Dialog auftreten. Und schließlich die Lecture-Aufgabe, die die Herausforderung bietet, reale Universitätsvorlesungen automatisch zu transkribieren und zu übersetzen. In Anlehnung an die Tradition dieser Berichte werden wir alle Aufgaben detailliert beschreiben und die Ergebnisse aller von den Teilnehmern eingereichten Läufe präsentieren.', 'sw': 'Kampeni ya Tathmini ya IWSLT 2017 imeandaa kazi tatu. Kazi ya lugha nyingi, ambayo ni kuhusu mafunzo ya mfumo wa kutafsiri mashine yanayohusu maelekezo mengi ya lugha, ikiwa ni pamoja na maelekezo yanayoitwa sifuri. Kazi ya Dialog, ambayo inatoa wito wa kuunganisha taarifa za muktadha katika tafsiri ya mashine, ili kutatua maoni yanayotokea kwa kawaida katika mazungumzo ya binadamu yanageuka. Na hatimaye, kazi ya Lecture, ambayo inatoa changamoto ya kuandika na kutafsiri masomo ya vyuo vikuu vya maisha halisi. Kufuatia utamaduni wa taarifa hizi, tutaelezea kazi zote kwa kina na tutatoa matokeo ya matokeo yote yaliyowasilishwa na washiriki wao.', 'fa': 'کمپین ارزیابی IWSLT ۲۰۱۷ سه کار را سازمان کرده است. وظیفه بسیاری زبان، که در مورد سیستم\u200cهای ترجمه\u200cهای ماشین آموزش می\u200cباشد که به طریق بسیاری به بسیاری زبان مدیریت می\u200cکند، شامل به عنوان طریق\u200cهای صفر شلیک می\u200cشود. وظیفه محاورۀ مشاورۀ ، که برای ترجمه کردن اطلاعات محیط در ترجمه ماشین می\u200cخواد ، برای حل رسانه\u200cهای نافوریک که معمولاً در محاورۀ محاورۀ انسان و انسان اتفاق می\u200cافتد. و بالاخره، وظیفه آموزشی که چالش تحویل دادن و ترجمه کردن سخنرانی دانشگاه واقعی را پیشنهاد می\u200cدهد. بعد از سنت این گزارش، همه کارها را به جزئیات توصیف می\u200cکنیم و نتیجه\u200cهای همه راه\u200cها را که توسط شرکت\u200cکنندگان آنها ارائه می\u200cدهند، بیان می\u200cکنیم.', 'tr': 'IWSLT 2017-nji ýygnam kampanyasy üç zady guruldy. Birnäçe dilli zady, maşynyň terjime sistemalary üçin köp dilden köp görnüşler we şol diýip atlandyrylýan, zero-shot görnüşleri bilen meýilleşdirýär. Makina terjime etmek üçin Kontekst maglumatynyň integrasyny çalýar Soňunda, Lectura zady, uniwersitet dermanlaryny awtomatik terjime etmek we terjime etmek üçin kynçylygy toldurýar. Bu raporlardan geleneklerine göre, bütün görevlerini detaylardan tanımlayacağız we iştirakçilerinin tarapyndan berilen tüm işlerin netijesini çykaracağız.', 'af': 'Die IWSLT 2017-evalueringskampanie het drie taak organiseer. Die veelvuldige taak, wat oor oefening van masjien vertalingsstelsels wat baie na veel taal rigtings behandel, insluitend so genoem zero-shot rigtings. Die dialoog taak, wat roep vir die integrasie van konteks inligting in masjien vertaling, om anaforiese verwysings te oplos wat tipies in die mens-mens-dialoog draai. En, eindelik, die Lektuur taak, wat die uitdaging van outomaties oorskryf en vertaling van regte lewens universiteit leksies aanbied. Volgens die tradisionele van hierdie raporte sal ons al die opdragte in detail beskrywe en die resultate van alle uitloope wat deur hulle deelnaders onderskrywe is.', 'id': 'Kampanye evaluasi IWSLT 2017 telah mengatur tiga tugas. The Multilingual task, which is about training machine translation systems handling many-to-many language directions, including so-called zero-shot directions.  Tugas Dialog, yang meminta integrasi informasi konteks dalam terjemahan mesin, untuk menyelesaikan referensi anafora yang biasanya terjadi dalam dialog manusia-manusia berubah. Dan akhirnya, tugas Lecture, yang menawarkan tantangan untuk secara otomatis menerjemahkan dan menerjemahkan kuliah universitas nyata. Menurut tradisi laporan ini, kami akan menjelaskan semua tugas secara rinci dan memperlihatkan hasil dari semua jalanan yang dikirim oleh peserta mereka.', 'sq': 'Fushata e vlerësimit IWSLT 2017 ka organizuar tre detyra. The Multilingual task, which is about training machine translation systems handling many-to-many language directions, including so-called zero-shot directions.  Detyra e Dialogut, e cila kërkon integrimin e informacionit të kontekstit në përkthimin e makinave, me qëllim që të zgjidhen referencat anaforike që zakonisht ndodhin në dialogun njerëzor-njerëzor. Dhe më në fund, detyra e Leksionit, e cila ofron sfidën e transkriptimit dhe përkthimit automatik të leksioneve universitare të jetës reale. Pas traditës së këtyre raporteve, ne do të përshkruajmë të gjitha detyrat në hollësi dhe do të paraqesim rezultatet e të gjitha kandidatave të paraqitura nga pjesëmarrësit e tyre.', 'hy': '2017 թվականի IwSMT-ի գնահատման քարոզարշավը կազմակերպեց երեք խնդիր: The Multilingual task, which is about training machine translation systems handling many-to-many language directions, including so-called zero-shot directions.  Խաղախոսության խնդիրը, որը պահանջում է ինտեգրել կոնտեքստի տեղեկատվությունը մեքենայի թարգմանման մեջ, որպեսզի լուծվի անաֆորական հաղորդակցությունները, որոնք սովորաբար տեղի են ունենում մարդկային-մարդկային խմբախոսության շրջա Եվ վերջապես, դասախոսությունը, որը առաջարկում է մարտահրավեր՝ ինքնագրել և թարգմանել իրական համալսարանի դասախոսությունները: Այս զեկույցների ավանդույթներին հետևելով, մենք մանրամասն կնկարագրենք բոլոր առաջադրանքները և կներկայացնենք նրանց մասնակիցների ներկայացված բոլոր գործողությունների արդյունքները:', 'am': 'የIWSLT 2017 ማስታወቂያ ዘመቻ ሦስት ስራ አሰራጅቷል፡፡ የቋንቋ ቋንቋ ስራ፣ እንደተጠራጠሩ zero-shot መንገዶችን እና ብዙ ቋንቋ-ቋንቋዎች መግለጫ ሲስተማርና ነው፡፡ የመስኮት ማህበራዊ ትርጉም ውስጥ ማኅበራዊ መረጃዎችን ለመጠቀም የሚጠይቅ የመስኮት ማህበራዊ ማህበረሰብ በተቃውሞ በሰው-አካባቢ ማህበራዊ ማህበረሰብ ውስጥ የሚደረገውን መልዕክቶች ለመፍታት ይጠራቸዋል፡፡ በመጨረሻም የራሱ የሕይወት የዩንቨርስቲ ትምህርት ትርጓሜዎችን በመፍጠር እና በመተርጓም የሚያደርገውን የሌክትራር ሥራ ነው፡፡ እነዚህን ሪፖርት ወግ ከተደረገ በኋላ ሁሉን ስራዎችን በተለይተን እናሳውራለን በተካፋሪዎቻቸውም የሁሉን ፈተና እናቀርባታለን፡፡', 'az': 'IWSLT 2017 değerlendirmə kampanyası üç işləri organize etdi. Çoxlu dil işləri, maşına çevirim sistemlərinin çoxlu dil yönlərini idarə edir, buna görə də "0-shot yönlərini" deyirlər. Makina çevirilməsində kontekst məlumatının integrasiyasını çağıran Dialoog işi, insan-insan dialoğunun dönüşündə olaraq anaforik referans çəkmək üçün. Və sonunda, təhsil işləri, həqiqi həyat üniversitələrinin təhsil edilməsini və təhsil etməsini təklif edir. Bu xəbərlərin nəticəsindən sonra, bütün işləri detalıqla tanımlayacağıq və onların iştirakçıları təyin etdikləri bütün işlərin sonuçlarını göstərəcəyik.', 'ca': "The IWSLT 2017 evaluation campaign has organised three tasks.  La tasca multilingüe, que consisteix en formar sistemes de traducció màquina que manejen direccions de moltes a moltes llengües, incloent les anomenades direccions de zero. La tasca del Diàleg, que demana la integració de la informació contextual en la traducció màquina, per resoldre les referències anafòriques que normalment ocorren en el diàleg humà-humà. I finalment, la tasca de la Llectura, que ofereix el repte de transcriure i traduir automàticament les lectures universitàries de vida real. Segons la tradició d'aquests informes, descriurem totes les tasques en detall i presentarem els resultats de totes les execucions presentades pels seus participants.", 'bs': 'Kampanja za procjenu IWSLT 2017 organizovala je tri zadatka. Mnogjezički zadatak, koji se radi o sustavima prevođenja mašina koji se bave mnogim na mnogim jezičkim uputama, uključujući takozvane upute nula uputa. Zadatak dijaloga, koji poziva na integraciju kontekstskih informacija u prevodu strojeva, kako bi riješio anaforske referencije koje se obično dešavaju u okviru ljudskog-ljudskog dijaloga. I konačno, zadatak lekcije, koji nudi izazov automatskog prepisanja i prevodenja lekcija u univerzitetu. Nakon tradicije ovih izvješća, detaljno ćemo opisati sve zadatke i predstaviti rezultate svih trčanja podanih njihovim učesnicima.', 'bn': 'আইউএসএলটি ২০১৭ সালের মূল্য প্রচারণা তিনটি কাজের আয়োজন করেছে। মাল্টিভাষার কাজ, যা প্রশিক্ষণ মেশিন অনুবাদ সিস্টেমের ব্যাপারে অনেক ভাষার নির্দেশ নিয়ে যাচ্ছে, যার মধ্যে অন্যান্য ন ডায়ালগ কাজ, যা মেশিন অনুবাদের মধ্যে প্রেক্ষাপট তথ্য যুক্ত করার আহ্বান জানাচ্ছে, যাতে মানুষ-মানব ডায়ালগে সাধারণত মানুষের ডা এবং অবশেষে, লেক্টার কাজ, যা স্বয়ংক্রিয়ভাবে বিশ্ববিদ্যালয়ের ভাষণ লেখা এবং অনুবাদ করার চ্যালেঞ্জ প্রদান করে। এই প্রতিবেদনের ঐতিহ্য অনুসারে, আমরা সব কাজ বিস্তারিত বর্ণনা করব এবং তাদের অংশগ্রহণকারীদের দ্বারা প্রতিষ্ঠিত সকল চালানো', 'ko': 'IWSLT 2017 평가 활동은 세 가지 임무를 구성했다.다국어 임무는 기계 번역 시스템이 다국어 방향을 처리하는 훈련에 관한 것으로 이른바 영포 방향을 포함한다.대화 임무는 기계 번역에서 언어 환경 정보를 통합시켜 인류 대화에서 흔히 볼 수 있는 반지 현상을 해결해야 한다.마지막으로 강좌 임무는 현실 생활에서 대학 강좌를 자동으로 베껴쓰고 번역하는 도전을 제공한다.이 보고서의 전통에 따라 우리는 모든 임무를 상세하게 묘사하고 참가자들이 제출한 모든 달리기 결과를 보여줄 것이다.', 'cs': 'Hodnotící kampaň IWSLT 2017 uspořádala tři úkoly. Vícejazyčný úkol, který je o školení strojových překladatelských systémů, které zpracovávají mnoho až mnoho jazykových směrů, včetně tzv. nulových směrů. Úkol Dialog, který vyžaduje integraci kontextových informací do strojového překladu s cílem řešit anaforické odkazy, které se obvykle vyskytují v dialogu člověk-člověk. A konečně úkol Přednášky, který nabízí výzvu automatického přepisu a překladu reálných univerzitních přednášek. V souladu s tradicí těchto zpráv podrobně popíšeme všechny úkoly a prezentujeme výsledky všech jízd předložených jejich účastníky.', 'et': 'IWSLT 2017 hindamiskampaania raames on korraldatud kolm ülesannet. Mitmekeelne ülesanne, mis seisneb masintõlkesüsteemide koolitamises, mis käsitlevad mitmesuguseid keelelisi suundi, sealhulgas nn null-shot suundi. Dialoog ülesanne, mis nõuab kontekstiteabe integreerimist masintõlkesse, et lahendada anafoorsed viited, mis tavaliselt esinevad inimese-inimese dialoogi pööretes. Ja lõpuks loengu ülesanne, mis pakub väljakutset automaatselt transkribeerida ja tõlkida reaalseid ülikooliloenguid. Nende aruannete traditsiooni järgi kirjeldame kõiki ülesandeid üksikasjalikult ja esitame kõikide osalejate esitatud jooksude tulemused.', 'fi': 'IWSLT 2017 -arviointikampanjassa on järjestetty kolme tehtävää. Monikielinen tehtävä, jossa koulutetaan konekäännösjärjestelmiä, jotka käsittelevät monista moniin kielisuuntiin, mukaan lukien ns. nolla-shot-suuntia. Dialogue-tehtävä, joka vaatii kontekstitietojen integroimista konekäännökseen, jotta voidaan ratkaista anaforiset viittaukset, joita tyypillisesti esiintyy ihmisen ja ihmisen välisessä vuoropuhelussa. Ja lopuksi luentotehtävä, joka tarjoaa haasteen kirjoittaa ja kääntää automaattisesti todellisia yliopistoluentoja. Raporttien perinteen mukaisesti kuvaamme kaikki tehtävät yksityiskohtaisesti ja esittelemme osallistujien jättämien ajojen tulokset.', 'jv': 'kampanjenengan IWSLT kang dipunangé tanggal maneh, sing wis arep telu nggawe Multilenguang task, sing paling nggambar sistem tarjamahan ing sampeyan, nggawe akeh-akeh tarjamahan, sithik gambar tarjamahan 0-ot. Wulangan Dialog, kang dipoleh kanggo ntedekaan informasi kontext kanggo tarjamahan ingan Nyong, ayo mulai, Lektur uwong, sing berarti ngebudhakan kang automatik bukal nggawe tarjamah lan mulai perusahaan layanan universite. Nyong-bengkane dolanan sing beraksi alat iki, awak dhéwé bakal terus nggawe gerakan ning daftar karo nganggo barang bakal terus ono wektu nggawe ngubah gambaran ning acara iki bakal terus tambah nggawe ngubah', 'ha': "Kamcampin da aka yi evaluation na IWSLT 2017 ya organize aikin uku. Kayan mulki-linguin, wanda ke sami na'urar da tsarin masu fassarar mashine da ke aiki shiryarwa masu yawa-zuwa-yawa, da kuma shirin da aka faɗa sifo-shot. Kayan aikin zauren akwatin bayanin, wanda ke kira a haɗi da information na mazaɓa cikin birarin akwatin bayani, dõmin ya buƙata wa masu motsi da kwamfyutan tebur kamar an faɗi a ɗabi'a cikin zauren mutum. Kuma da ƙarshen, Lecture aikin da ke gauraya wa'anar fassarori na farat-rubutu da fassarar universitei masu gaskiya. Ga bayan sharrin wannan da muka faɗa, zã mu bayyana kowanin aikin su daki-daki, kuma Mu gabatar da matsalar duk tafiyarsu da mushirikai waɗanda suka gabatar da shi.", 'sk': 'V okviru ocenjevalne kampanje IWSLT 2017 so bile organizirane tri naloge. Večjezična naloga, ki se nanaša na usposabljanje sistemov strojnega prevajanja, ki upravljajo jezikovne smeri, vključno s tako imenovanimi smeri brez strela. Dialog naloga, ki zahteva vključitev kontekstnih informacij v strojno prevajanje, da bi rešili anaforične reference, ki se običajno pojavijo v dialogu med človekom in človekom. In končno naloga predavanja, ki ponuja izziv avtomatičnega prepisovanja in prevajanja resničnih univerzitetnih predavanj. V skladu s tradicijo teh poročil bomo podrobno opisali vse naloge in predstavili rezultate vseh tekmovanj, ki jih bodo predložili udeleženci.', 'he': 'The IWSLT 2017 evaluation campaign has organised three tasks.  המשימה הרב-שפותית, שהיא בנוגע לאימון מערכות התרגום מכונות שמטפלות בכיוונים שפותיים רבים עד רבים, כולל הכיוונים שנקראים אפס-ירי. המשימה של הדיולוג, שמבקשת לאינטגרציה של מידע הקשר בתרגום מכונות, כדי לפתור התייחסות אנפוריות שמתרחשות בדרך כלל בדיולוג אנושי-אנושי. ולבסוף, המשימה של ההרצאות, שמציעה את האתגר של שינוי אוטומטי ותרגם הרצאות באוניברסיטת חיים אמיתיים. לאחר המסורת של הדו"חות האלה, נתאר את כל המשימות בפרטים ונציג את התוצאות של כל הריצוים שנשלחו על ידי השתתפים שלהם.', 'bo': 'IWSLT 2017 རྒྱབ་སྐྱོར་ལྡན་གྱི་ལས་འགན་གསུམ་དེ་སྒྲིག་ཡོད་པ་ཡིན། སྐད་ཡིག་གི་ལྟ་བུའི་དོན་ལྡན་མང་ཆེ་ཤོས་ཀྱི་ལས་འགན་བློ་གཏད་ཆས་ཤིག་ཡོད་པ་དེ་ནི་ལག་འཁྱེར་གྱི་སྣ་ཚོགས་ལ་མང་ཙམ་ཞ དངོས་ཐོག་གི་ཌའི་ལོག་ལས་ཀླད་སྒྲོམ་གྱི་ནང་དུ་ཡིག་ཆའི་གནས་ཚུལ་གསལ་བཤད་ཀྱི་integrོ་ གཏོང་དགོས་མིན་འདུག མཐའ་མར་དེ། སྒྲིག་འཛིན་གྱི་ལས་འགུལ་ནི། དེ་གིས་རང་འགུལ་གྱིས་ཚོར་ཡིག འུ་ཚོས་གསར་བརྗོད་ཀྱི་ལམ་སྲོལ་འདི་དག་གི་རྗེས་སུ་ང་ཚོས་བྱ་རིམ་ཡོངས་གསལ་བཤད་བྱས་ན་ཁོང་ཚོའི་ཞུགས་སྐྱེས་པ་ཚོར་'}
{'en': 'Going beyond zero-shot MT : combining phonological, morphological and semantic factors. The UdS-DFKI System at IWSLT 2017 MT : combining phonological, morphological and semantic factors. The  U d S - DFKI  System at  IWSLT  2017', 'ar': 'تجاوز MT بدون طلقة: الجمع بين العوامل الصوتية والمورفولوجية والدلالية. نظام UdS-DFKI في IWSLT 2017', 'pt': 'Indo além da TM zero-shot: combinando fatores fonológicos, morfológicos e semânticos. O Sistema UdS-DFKI na IWSLT 2017', 'fr': "Au-delà de la magnétoscopie à jet zéro\xa0: combinaison de facteurs phonologiques, morphologiques et sémantiques. Le système UDS-DFI à l'IWSLT 2017", 'es': 'Ir más allá de la MT de tiro cero: combinar factores fonológicos, morfológicos y semánticos. El sistema UDS-DFKI en IWSLT 2017', 'ja': 'ゼロショットMTを超えて：音声学的、形態学的、意味論的要因を組み合わせています。IWSLT 2017のUdS - DFKIシステム', 'zh': '越零镜头MT:合音,形语义素。 UdS-DFKI系IWSLT 2017', 'hi': 'शून्य-शॉट एमटी से परे जा रहा है: ध्वन्यात्मक, रूपात्मक और शब्दार्थ कारकों का संयोजन। IWSLT 2017 में UdS-DFKI सिस्टम', 'ru': 'Выход за рамки нулевого МТ: объединение фонологических, морфологических и семантических факторов. Система UDS-DFKI на IWSLT 2017', 'ga': 'Ag dul níos faide ná náid MT: fachtóirí fóineolaíochta, moirfeolaíocha agus shéimeantacha a chomhcheangal. Córas UdS-DFKI ag IWSLT 2017', 'hu': 'Túllépve a nulla-shot MT-n: fonológiai, morfológiai és szemantikai tényezők kombinációja. Az UdS-DFKI rendszer az IWSLT 2017-en', 'it': 'Andare oltre la MT zero-shot: combinare fattori fonologici, morfologici e semantici. Il sistema UdS-DFKI a IWSLT 2017', 'kk': 'Нөл- шоттың үстінен ауысу MT: фонологиялық, морфологиялық және семантикалық факторларды біріктіру. IWSLT 2017 жылы UdS- DFKI жүйесі', 'lt': 'Going beyond zero-shot MT: combining phonological, morphological and semantic factors.  The UdS-DFKI System at IWSLT 2017', 'el': 'Πέρα από το μηδέν-πυροβολισμό ΜΤ: συνδυασμός φωνολογικών, μορφολογικών και σημασιολογικών παραγόντων. Το σύστημα UdS-DFKI στο IWSLT 2017', 'ms': 'Melewati MT 0-shot: menggabungkan faktor fonologi, morfologi dan semantik. The UdS-DFKI System at IWSLT 2017', 'ka': '0-სტარტი MT-ის გარეშე: ფონოლოგიური, მორფოლოგიური და სმენტიური ფაქტორის შეერთება. UdS-DFKI სისტემა IWSLT 2017-ში', 'mt': 'Li jmorru lil hinn mill-MT b’zero shot: li jikkombinaw fatturi fonoloġiċi, morfoloġiċi u semantiċi. Is-Sistema UdS-DFKI fl-IWSLT 2017', 'no': 'Går over nullsatt MT: kombinerer fonologiske, morfologiske og semantiske faktorer. UdS-DFKI-systemet på IWSLT 2017', 'pl': 'Wychodząc poza zero-shot MT: łącząc czynniki fonologiczne, morfologiczne i semantyczne. System UdS-DFKI na IWSLT 2017', 'mk': 'Одиме надвор од нула-стрела МТ: комбинација на фонолошки, морфолошки и семантични фактори. УДС-ДФКИ системот на IWSLT 2017', 'sr': 'Prelazi preko nulog metara MT: kombinacija fonoloških, morfoloških i semantičkih faktora. UdS-DFKI sistem na IWSLT 2017.', 'si': 'සුන්ධ වෙඩි MT වලින් යන්න: ෆෝනොලෝජික, මෝර්ෆෝලෝජික සහ සෙමැන්ටික් විශේෂය සම්බන්ධ 2017 IWSLT වල UdS-DFKI පද්ධතිය', 'so': 'Halkaas ka badnaan MT: soo wadashada foonological, morphological iyo semantic. UdS-DFKI nidaamka IWSLT 2017', 'ml': 'പൂജ്യത്തിനു മുകളിലേക്ക് പോകുന്ന MT: ഫോളോളജിക്കല്\u200d, മോര്\u200dഫോളജിക്കല്\u200d, സെമാന്റിക് ഫാക്ടറുകള്\u200d. IWSLT 2017-ലെ UdS-DFKI സിസ്റ്റം', 'sv': 'Att gå bortom noll-skott MT: kombinera fonologiska, morfologiska och semantiska faktorer. UdS-DFKI System på IWSLT 2017', 'mn': 'Нэг шалтгааны MT-ээс гадна: фонологик, морфологик, семантик хүчин зүйлсийг нэгтгэдэг. 2017 оны IWSLT-д UdS-DFKI систем', 'ro': 'Mergând dincolo de MT zero-shot: combinarea factorilor fonologici, morfologici și semantici. Sistemul UdS-DFKI la IWSLT 2017', 'ur': 'صفر-شٹ MT سے زیادہ جاتا ہے: فانولوژیکوں، مورفولوژیکوں اور سیمانٹیکوں فکتوروں کو جمع کرتا ہے۔ IWSLT 2017 میں UdS-DFKI سیسٹم', 'ta': 'பூஜ்ஜியத்திற்கு மேல் செல்கிறது MT: தொலைநோயியல் மற்றும் காரணிகளை ஒன்று சேர்க்கிறது. IWSLT 2017 யில் UdS- DFKI அமைப்பு', 'uz': 'Nuqta oʻtgan MT: fonologi, morfologik va semantik faktorelarini birlashtirish. IWSLT 2017 da UdS- DFKI tizimi', 'vi': 'Vượt xa con số không điểm MTV: kết hợp các yếu tố ngôn ngữ, morphology và semantics. Hệ thống UdS-DFS Tại IWSLT Aw7', 'da': 'At gå ud over zero-shot MT: kombinere fonologiske, morfologiske og semantiske faktorer. UdS-DFKI System på IWSLT 2017', 'bg': 'Излизане отвъд нулевата МТ: комбиниране на фонологични, морфологични и семантични фактори. Системата УДС-ДФКИ на ИВСЛТ 2017', 'nl': 'Verder gaan dan zero-shot MT: het combineren van fonologische, morfologische en semantische factoren. Het UdS-DFKI-systeem op IWSLT 2017', 'hr': 'Prelazi preko nulog metara MT: kombiniranje fonoloških, morfoloških i semantičkih faktora. UdS-DFKI sustav na IWSLT 2017.', 'de': 'Über Zero-Shot MT hinausgehen: phonologische, morphologische und semantische Faktoren kombinieren. Das UdS-DFKI System auf der IWSLT 2017', 'id': 'Melewati MT 0-shot: menggabungkan faktor fonologi, morfologi dan semantis. Sistem UdS-DFKI di IWSLT 2017', 'ko': '초월 제로 박자 기계 번역: 음성, 형태와 의미 요소를 결합한다.2017년 IWSLT의 UdS DFKI 시스템', 'sw': 'Kutokea zaidi ya MT yenye risasi sifuri: kuunganisha sababu za kifolojia, vifolojia na sekunde. Mfumo wa UdS-DFKI katika IWSLT 2017', 'af': 'Gaan buite nul-skoot MT: kombineer fonologiese, morfologiese en semantiese faktore. Die UdS-DFKI stelsel op IWSLT 2017', 'tr': "Sıfır atış MT'den ötesinde: fonolojik, morfolojik ve semantik faktörleri birleştirmek. UdS-DFKI System IWSLT 2017-nji ýylda", 'sq': 'Përtej MT zero-shot: kombinimi i faktorëve fonologjikë, morfologjik dhe semantik. Sistemi UdS-DFKI në IWSLT 2017', 'fa': 'فراتر از شلیک صفر MT: تعداد فنالوژیک، مورفولوژیک و سیمانتیک. سیستم UdS-DFKI در IWSLT 2017', 'am': 'ከ 0-shot በላይ የሚሄድ MT: ፎሎጂ፣ ሞሮፎሎጂ እና የsemantic ውጤቶች በመቀላቀል ነው፡፡ The UdS-DFKI System at IWSLT 2017', 'hy': 'Գնալով զրոյական ՄԹ-ից դուրս՝ համադրելով ֆոնոլոգիական, մորֆոլոգիական և սեմանտիկ գործոններ: 2017 թվականին IwSMT-ի UDS-DFKI համակարգը', 'az': 'Sıfır-atlı MT-dən ötrü: fonolojik, morfolojik və semantik faktörləri birləşdirir. IWSLT 2017-də UdS-DFKI Sistemi', 'bn': 'Going beyond zero-shot MT: combining phonological, morphological and semantic factors.  ইউএসএলটি ২০১৭-এ UdS-DFKI সিস্টেম', 'ca': 'Allà més enllà de la MT zero: combinar factors fonològics, morfològics i semàntics. El sistema UdS-DFKI a IWSLT 2017', 'bs': 'Prelazi preko nulog metara MT: kombinacija fonoloških, morfoloških i semantičkih faktora. UdS-DFKI sistem na IWSLT 2017.', 'cs': 'Přesahující nulové MT: kombinace fonologických, morfologických a sémantických faktorů. Systém UdS-DFKI na IWSLT 2017', 'et': 'Lähenemine kaugemale null-shot MT: fonoloogiliste, morfoloogiliste ja semantiliste tegurite kombineerimine. UDS-DFKI süsteem IWSLT 2017. aastal', 'fi': 'Nollashot MT: fonologisten, morfologisten ja semanttisten tekijöiden yhdistäminen. UdS-DFKI-järjestelmä IWSLT 2017 -tapahtumassa', 'jv': 'Going öwo nul-shot MT: combining telelogic, modorologic and semanti. Sistem udaS-DFKi nang IWSLT 1997', 'ha': 'Yana ƙara bayan sifo-shot MT: kombinta folojical, morfological da sakanti. The UdS-DFKI system at IWSLT 2017', 'sk': 'Nadaljšanje brezposelnega MT: združevanje fonoloških, morfoloških in semantičnih dejavnikov. Sistem UdS-DFKI na IWSLT 2017', 'bo': 'Going beyond zero-shot MT: combining phonological, morphological and semantic factors. UdS-DFKI མ་ལག་ནི་ IWSLT 2017', 'he': 'עובר מעבר לאפס-ירי MT: שילוב גורמים פונולוגיים, מורפולוגיים וסמנטיים. The UdS-DFKI System at IWSLT 2017'}
{'en': 'This paper describes the UdS-DFKI participation to the multilingual task of the IWSLT Evaluation 2017. Our approach is based on factored multilingual neural translation systems following the small data and zero-shot training conditions. Our systems are designed to fully exploit multilinguality by including factors that increase the number of common elements among languages such as phonetic coarse encodings and synsets, besides shallow part-of-speech tags,  stems  and  lemmas . Document level information is also considered by including the topic of every document. This approach improves a  baseline  without any additional factor for all the language pairs and even allows beyond-zero-shot translation. That is, the translation from unseen languages is possible thanks to the common elements especially  synsets  in our  models  among languages.', 'ar': 'تصف هذه الورقة مشاركة UdS-DFKI في المهمة متعددة اللغات لتقييم IWSLT لعام 2017. ويستند نهجنا على أنظمة الترجمة العصبية متعددة اللغات التي تم تحليلها في أعقاب البيانات الصغيرة وظروف التدريب الصفرية. تم تصميم أنظمتنا للاستفادة الكاملة من تعدد اللغات من خلال تضمين العوامل التي تزيد من عدد العناصر المشتركة بين اللغات مثل الترميزات والتزامن الصوتي الخشن ، بالإضافة إلى علامات الجزء الضحل من الكلام والسيقان والليمس. يتم أيضًا أخذ معلومات مستوى المستند في الاعتبار من خلال تضمين موضوع كل مستند. يعمل هذا النهج على تحسين خط الأساس دون أي عامل إضافي لجميع أزواج اللغات ويسمح أيضًا بترجمة تتجاوز الصفر. وهذا يعني أن الترجمة من اللغات غير المرئية ممكنة بفضل العناصر المشتركة - خاصةً synsets في نماذجنا - بين اللغات.', 'fr': "Cet article décrit la participation de l'UDS-DFKI à la tâche multilingue de l'évaluation IWSLT 2017. Notre approche est basée sur des systèmes de traduction neuronale multilingues factorisés qui suivent les petites données et les conditions d'entraînement zero-shot. Nos systèmes sont conçus pour exploiter pleinement le multilinguisme en incluant des facteurs qui augmentent le nombre d'éléments communs entre les langues, tels que les codages phonétiques grossiers et les synsets, en plus des balises partielles superficielles, des stems et des lemmes. Les informations au niveau du document sont également prises en compte en incluant le sujet de chaque document. Cette approche améliore une base de référence sans aucun facteur supplémentaire pour toutes les paires de langues et permet même une traduction au-delà de zéro. En d'autres termes, la traduction à partir de langues invisibles est possible grâce aux éléments communs, en particulier les synsets dans nos modèles, entre les langues.", 'pt': 'Este artigo descreve a participação do UdS-DFKI na tarefa multilíngue da Avaliação IWSLT 2017. Nossa abordagem é baseada em sistemas de tradução neural multilíngue fatorada seguindo as condições de treinamento de dados pequenos e zero-shot. Nossos sistemas são projetados para explorar plenamente a multilinguagem, incluindo fatores que aumentam o número de elementos comuns entre os idiomas, como codificações fonéticas grosseiras e synsets, além de tags, radicais e lemas de partes de fala rasas. As informações no nível do documento também são consideradas, incluindo o tópico de cada documento. Essa abordagem melhora uma linha de base sem nenhum fator adicional para todos os pares de idiomas e até permite uma tradução além do zero. Ou seja, a tradução de idiomas invisíveis é possível graças aos elementos comuns —especialmente synsets em nossos modelos— entre os idiomas.', 'es': 'Este artículo describe la participación de UDS-DFKI en la tarea multilingüe de la Evaluación IWSLT 2017. Nuestro enfoque se basa en sistemas de traducción neuronal multilingüe factorizados que siguen las condiciones de entrenamiento de datos pequeños y cero disparos. Nuestros sistemas están diseñados para aprovechar al máximo el multilingüismo mediante la inclusión de factores que aumentan el número de elementos comunes entre los idiomas, como las codificaciones fonéticas gruesas y los synsets, además de etiquetas de parte superficial del discurso, plicas y lemas. La información a nivel de documento también se considera al incluir el tema de cada documento. Este enfoque mejora una línea de base sin ningún factor adicional para todos los pares de idiomas e incluso permite una traducción más allá de cero. Es decir, la traducción de idiomas inéditos es posible gracias a los elementos comunes —especialmente los synsets en nuestros modelos— entre los idiomas.', 'ja': '本稿では、IWSLT評価2017の多言語タスクへのUdS - DFKIの参加について説明する。私たちのアプローチは、小さなデータとゼロショットのトレーニング条件に従った因子多言語ニューラル翻訳システムに基づいています。当社のシステムは、浅い音声部分タグ、ステム、レマに加えて、音声の粗いエンコーディングやシンセットなどの言語間の共通要素の数を増加させる要因を含めることによって、多言語性を完全に利用するように設計されています。ドキュメントレベルの情報は、すべてのドキュメントのトピックを含めることによっても検討されます。このアプローチは、すべての言語ペアのための追加の要因なしのベースラインを改善し、ゼロショットを超えた翻訳さえ可能にします。つまり、見えない言語からの翻訳は、共通の要素、特に私たちのモデルのシンセット、言語間のおかげで可能になります。', 'zh': '本文引UdS-DFKI参2017年IWSLT评估多言事。 吾法率由小数及零次训练因子化多语言神经译系统。 臣等统旨充用多语言性,除浅词性词性标签、词干、引理之外,兼增言常见元素数之素,如语音粗粝编码合成集。 因文档之题以虑文档级。 其法改善基线,而无须为诸语言添加一切额外因素,至许超越转换。 亦言言语同元素(尤是模形中同义词集),不见语言译可也。', 'hi': 'यह पेपर IWSLT मूल्यांकन 2017 के बहुभाषी कार्य के लिए UdS-DFKI भागीदारी का वर्णन करता है। हमारा दृष्टिकोण छोटे डेटा और शून्य-शॉट प्रशिक्षण स्थितियों के बाद फैक्टर किए गए बहुभाषी तंत्रिका अनुवाद प्रणालियों पर आधारित है। हमारे सिस्टम को उन कारकों को शामिल करके बहुभाषीता का पूरी तरह से शोषण करने के लिए डिज़ाइन किया गया है जो उथले पार्ट-ऑफ-स्पीच टैग, उपजी और लेमा के अलावा ध्वन्यात्मक मोटे एन्कोडिंग और सिंसेट जैसी भाषाओं के बीच सामान्य तत्वों की संख्या में वृद्धि करते हैं। दस्तावेज़ स्तर की जानकारी को प्रत्येक दस्तावेज़ के विषय को शामिल करके भी माना जाता है। यह दृष्टिकोण सभी भाषा जोड़े के लिए किसी भी अतिरिक्त कारक के बिना एक आधार रेखा में सुधार करता है और यहां तक कि शून्य-शॉट अनुवाद से परे की अनुमति देता है। यही है, अनदेखी भाषाओं से अनुवाद सामान्य तत्वों के लिए संभव है - विशेष रूप से हमारे मॉडल में synsets - भाषाओं के बीच।', 'ru': 'В этой статье описывается участие UDS-DFKI в многоязычной задаче оценки IWSLT 2017. Наш подход основан на факторизованных многоязычных нейронных системах перевода с учетом небольших данных и условий обучения с нулевым выстрелом. Наши системы разработаны для полного использования многоязычия путем включения факторов, которые увеличивают количество общих элементов среди языков, таких как фонетические грубые кодировки и синсеты, помимо неглубоких тегов части речи, стеблей и леммы. Информация на уровне документа также учитывается путем включения темы каждого документа. Этот подход улучшает базовую линию без какого-либо дополнительного фактора для всех языковых пар и даже позволяет выполнять перевод сверх нулевого снимка. То есть перевод с невидимых языков возможен благодаря общим элементам - особенно синсетам в наших моделях - среди языков.', 'ga': 'Déanann an páipéar seo cur síos ar rannpháirtíocht UdS-DFKI i dtasc ilteangach Mheastóireacht IWSLT 2017. Tá ár gcur chuige bunaithe ar chórais néar-aistrithe ilteangacha fachtóirithe tar éis na gcoinníollacha oiliúna sonraí beaga agus náid. Tá ár gcórais deartha chun lántairbhe a bhaint as an ilteangachas trí fhachtóirí a chur san áireamh a mhéadaíonn líon na ngnéithe coitianta i measc teangacha mar ionchóduithe garbha foghraíochta agus sinsets, seachas clibeanna éadomhaine cuid cainte, gais agus leamaí. Déantar faisnéis ar leibhéal doiciméad a mheas freisin trí ábhar gach doiciméid a áireamh. Feabhsaíonn an cur chuige seo bunlíne gan aon fhachtóir breise do na péirí teanga go léir agus ceadaíonn sé fiú aistriúchán thar-náid. Is é sin le rá, is féidir aistriúchán ó theangacha nach bhfacthas riamh roimhe a bhuíochas leis na gnéithe coitianta — go háirithe na synsets inár múnlaí — i measc teangacha.', 'it': "Questo articolo descrive la partecipazione UdS-DFKI al compito multilingue della valutazione IWSLT 2017. Il nostro approccio si basa su sistemi di traduzione neurale multilingue fattorizzati che seguono i piccoli dati e le condizioni di allenamento zero-shot. I nostri sistemi sono progettati per sfruttare appieno la multilinguità includendo fattori che aumentano il numero di elementi comuni tra le lingue come le codifiche fonetiche grossolane e i sinonimi, oltre a tag poco profondi, steli e lemmi. Le informazioni a livello di documento sono considerate anche includendo l'argomento di ogni documento. Questo approccio migliora una linea di base senza alcun fattore aggiuntivo per tutte le coppie linguistiche e consente anche una traduzione oltre lo zero shot. Cioè, la traduzione da lingue invisibili è possibile grazie agli elementi comuni -soprattutto i sinonimi nei nostri modelli- tra le lingue.", 'el': 'Η παρούσα εργασία περιγράφει τη συμμετοχή του στο πολύγλωσσο καθήκον της Αξιολόγησης του IWSLT 2017. Η προσέγγισή μας βασίζεται σε πολυγλωσσικά συστήματα νευρολογικής μετάφρασης που ακολουθούν τα μικρά δεδομένα και τις συνθήκες εκπαίδευσης μηδενικού πυροβολισμού. Τα συστήματά μας είναι σχεδιασμένα για να αξιοποιούν πλήρως την πολυγλωσσία, συμπεριλαμβανόμενοι παράγοντες που αυξάνουν τον αριθμό κοινών στοιχείων μεταξύ γλωσσών όπως φωνητικές χονδρές κωδικοποιήσεις και σύνσετ, εκτός από ρηχά σήματα, στελέχη και λεμώματα. Οι πληροφορίες σε επίπεδο εγγράφου λαμβάνονται επίσης υπόψη συμπεριλαμβάνοντας το θέμα κάθε εγγράφου. Αυτή η προσέγγιση βελτιώνει μια γραμμή βάσης χωρίς πρόσθετο παράγοντα για όλα τα γλωσσικά ζεύγη και επιτρέπει ακόμη και μετάφραση πέρα από το μηδέν. Δηλαδή, η μετάφραση από αόρατες γλώσσες είναι δυνατή χάρη στα κοινά στοιχεία -ειδικά τα σύνθετα στα μοντέλα μας- μεταξύ των γλωσσών.', 'lt': 'Šiame dokumente aprašomas UdS-DFKI dalyvavimas daugiakalbėje IWSLT vertinimo 2017 m. užduotyje. Our approach is based on factored multilingual neural translation systems following the small data and zero-shot training conditions.  Our systems are designed to fully exploit multilinguality by including factors that increase the number of common elements among languages such as phonetic coarse encodings and synsets, besides shallow part-of-speech tags, stems and lemmas.  Dokumentų lygmens informacija taip pat svarstoma įtraukiant kiekvieno dokumento temą. Šis metodas pagerina pradinę vertimą be jokio papildomo veiksnio visoms kalbų poroms ir net leidžia išversti daugiau nei nulinę vertimą. Tai reiškia, kad neįstebimų kalbų vertimas yra įmanomas dėl bendrų elementų - ypač mūsų modelių sintezių - tarp kalbų.', 'mk': 'This paper describes the UdS-DFKI participation to the multilingual task of the IWSLT Evaluation 2017.  Нашиот пристап е базиран на факторирани мултијазични невропски преведувачки системи по малите податоци и услови за тренинг со нула. Our systems are designed to fully exploit multilinguality by including factors that increase the number of common elements among languages such as phonetic coarse encodings and synsets, besides shallow part-of-speech tags, stems and lemmas.  Document level information is also considered by including the topic of every document.  Овој пристап ја подобрува основната вредност без никаков дополнителен фактор за сите парови јазици и дури и овозможува преку нула-снимка превод. Тоа е, преводот од невидени јазици е можно благодарение на заедничките елементи - особено синсети во нашите модели - меѓу јазиците.', 'ka': 'ამ დოკუმენტი აღწერს UdS-DFKI-ს მონაცემულობას 2017 წლის IWSLT განსაზღვრების მრავალენგური დავალებისთვის. ჩვენი პროგრამა მრავალური ნეიროლის გარგება სისტემის ფექტურაცია, რომელიც შემდეგ პატარა მონაცემები და ნულის გარგება. ჩვენი სისტემები განაზღვრებულია მრავალენგულობის გამოყენება, რომელიც ფექტურების საერთო ელემენტების რაოდენობას, როგორც ფონეტიკური კონექტური კონექტურები და სინსექტები, მაგრამ მცი დოკუმენტის დოკუმენტის ინფორმაცია ასევე იყოს ყოველ დოკუმენტის თემაზე. ეს პროგრამა უფრო მეტად ახალგაზრდება ყველა ენის ზოგრამის დამატებული ფაქტორის გარეშე და უფრო უფრო მეტად ახალგაზრდება. ეს არის, შეცვალობა არაჩვენებული ენებიდან შესაძლებელია საერთო ელემენტების გამოყენება - განსაკუთრებით ჩვენი მოდელში - ენების შორის სინქესტებით.', 'hu': 'Ez a tanulmány bemutatja az UdS-DFKI részvételét a 2017-es IWSLT Értékelés többnyelvű feladatában. Megközelítésünk a faktorozott, többnyelvű neurális fordítási rendszereken alapul, amelyek a kis adatokat és a nulla lövéses edzési körülményeket követik. Rendszereinket úgy terveztük, hogy teljes mértékben kihasználják a többnyelvűséget, hogy olyan tényezőket tartalmaznak, amelyek növelik a nyelvek közös elemeinek számát, mint a fonetikus durva kódolások és szinonimák, a sekély beszédrész címkék, szárak és lemmák. A dokumentumszintű információkat úgy is figyelembe veszik, hogy minden dokumentum témáját tartalmazza. Ez a megközelítés további tényezők nélkül javítja az alapokat az összes nyelvpár esetében, és lehetővé teszi a nulla felvételen túli fordítást. Vagyis a láthatatlan nyelvekről történő fordítás a nyelvek közötti közös elemeknek köszönhetően lehetséges, különösen modelljeink szinkronkészleteinek.', 'kk': 'Бұл қағаз 2017 жылы IWSLT оқиғасының көп тілдік тапсырмасына UdS-DFKI қатынасын анықтайды. Біздің көпшілігіміз көптілік невралдық аудару жүйелеріне негізделген кішкентай деректерді және нөл- шарт оқыту шарттарынан кейін. Біздің жүйелеріміз көптілік тілдерді қолдану үшін фонетикалық кодтамасы мен синзеттерді қолданатын факторларды қолдану үшін құрылған, сөйлеу тегтері, стемдер және лиммалардың көпшілік элементтерін қолдану үші Құжаттың деңгейінің мәліметі әрбір құжаттың нақышын қоса да қалады. Бұл тәсілі барлық тіл екеуінің қосымша факторсыз негізгі сызығын жақсартады, сондай-ақ нөл түрлендіруден артық аударуға мүмкіндік береді. Бұл - көрсетілмеген тілдердің аудармасы жалпы элементтердің көмегімен - өзіміздің үлгілерімізде - тілдер арасындағы қадамдастыру мүмкін.', 'mt': 'Dan id-dokument jiddeskrivi l-parteċipazzjoni tal-UdS-DFKI fil-kompitu multilingwi tal-Evalwazzjoni tal-IWSLT 2017. L-approċċ tagħna huwa bbażat fuq sistemi ta’ traduzzjoni newrali multilingwi fatturati skont id-dejta żgħira u l-kundizzjonijiet ta’ taħriġ mingħajr skop. Is-sistemi tagħna huma mfassla biex jisfruttaw bis-sħiħ il-multilingwiċità billi jinkludu fatturi li jżidu n-numru ta’ elementi komuni fost il-lingwi bħall-kodifikazzjonijiet fonetiċi grossi u s-sinsetts, minbarra tikketti ta’ parti baxxa tad-diskors, stems u limmi. Informazzjoni fil-livell tad-dokument hija kkunsidrata wkoll billi jiġi inkluż is-suġġett ta’ kull dokument. Dan l-approċċ itejjeb il-linja bażi mingħajr ebda fattur addizzjonali għall-pari kollha tal-lingwi u saħansitra jippermetti traduzzjoni lil hinn minn zero-shot. Dan ifisser li t-traduzzjoni minn lingwi mhux viżibbli hija possibbli bis-saħħa tal-elementi komuni - speċjalment sinsetti fil-mudelli tagħna - fost il-lingwi.', 'mn': 'Энэ цаас 2017 оны IWSLT Evaluation-ын олон хэлний ажил дээр UdS-DFKI оролцоог тайлбарладаг. Бидний ойлголт нь хэл хэлний мэдрэлийн хөрөнгө оруулах системээс бага өгөгдлийн болон 0 шат сургалтын нөхцөл байдлын дараа үүсгэгдсэн олон хэлний мэдрэлийн хөрөнгө Бидний систем олон хэл хэлний хэрэглээнийг бүрэн ашиглах боломжтой. Хүмүүсийн хэл, стэмс, лимм зэрэг фоннетик кодлог, синсетүүдийг нэмэгдүүлдэг хүчин зүйлүүдийг нэмэгдүүлдэг. Документын түвшинд мэдээллийг баримт бүрийн сэдвийг нэмж авч үздэг. Энэ арга баримтууд бүх хэл хоёрын нэмэлт хүчин зүйлгүй суурь шугамыг сайжруулж, тэгш шугам хураагдах боломжтой болгодог. Энэ бол бидний загварын хоорондох ерөнхий элементүүдэд боломжтой. Ялангуяа бидний загварын хоорондох хэлнүүдийн хоорондох шинжилгээ.', 'pl': 'Niniejszy artykuł opisuje udział UdS-DFKI w wielojęzycznym zadaniu oceny IWSLT 2017. Nasze podejście opiera się na wielojęzycznych systemach translacji neuronowych opartych na małych danych i warunkach treningowych zero-shot. Nasze systemy zostały zaprojektowane tak, aby w pełni wykorzystać wielojęzyczność poprzez uwzględnienie czynników zwiększających liczbę wspólnych elementów wśród języków, takich jak fonetyczne szorstkie kodowanie i synsety, oprócz płytkich tagów części mowy, łodyg i lemmy. Informacje na poziomie dokumentu są również uwzględniane poprzez uwzględnienie tematu każdego dokumentu. Takie podejście poprawia bazę bazową bez dodatkowego czynnika dla wszystkich par językowych, a nawet umożliwia tłumaczenie ponad zero-shot. Oznacza to, że tłumaczenie z niewidzialnych języków jest możliwe dzięki wspólnym elementom – szczególnie zestawom w naszych modelach – między językami.', 'ro': 'Această lucrare descrie participarea UdS-DFKI la sarcina multilingvă a evaluării IWSLT 2017. Abordarea noastră se bazează pe sisteme de traducere neurală multilingvă factorate care urmează datele mici și condițiile de antrenament zero-shot. Sistemele noastre sunt concepute pentru a exploata pe deplin multilingvitatea, incluzând factori care crește numărul de elemente comune în rândul limbilor, cum ar fi codările fonetice grosiere și sintetele, pe lângă etichetele cu parte superficială a vorbirii, tulpinile și lemmele. Informațiile la nivelul documentului sunt, de asemenea, luate în considerare prin includerea subiectului fiecărui document. Această abordare îmbunătățește o bază de referință fără niciun factor suplimentar pentru toate perechile de limbi și permite chiar traducerea dincolo de zero-shot. Adică traducerea din limbi nevăzute este posibilă datorită elementelor comune -în special a sintezelor din modelele noastre- printre limbi.', 'sr': 'Ovaj papir opisuje sudjelovanje UdS-DFKI u multijezičkom zadatku IWSLT Evaluacije 2017. Naš pristup je baziran na faktoriranim multijezičkim sistemima neurološkog prevoda nakon malih podataka i uslova obuke na nulu. Naši sistemi su dizajnirani da potpuno iskoriste multijezičnost uključujući faktore koji povećavaju broj zajedničkih elementa među jezicima poput fonetičkih kodiranja i sinteza, osim plitkih dijelogovornih znakova, matičnih i limuna. Informacije o nivou dokumenta se takođe razmatraju uključujući temu svakog dokumenta. Ovaj pristup poboljšava početnu liniju bez dodatnog faktora svih jezičkih parova i čak omogućava prevod bez nule. To je, prevod iz nevidljivih jezika moguće zahvaljujući zajedničkim elementima - posebno sinkronizacijama u našim modelima - među jezicima.', 'si': 'මේ පත්තුව UdS-DFKI සම්බන්ධතාව 2017 IWSLT විශ්ලේෂණයේ බොහොම භාෂාවක් වැඩකට විස්තර කරනවා. අපේ ප්\u200dරවේශනය අධාරණය විශාල භාෂාවක් විශාල පද්ධතියේ පුංචි දත්ත සහ ශූර්ණ විශාල පද්ධත අපේ පද්ධතිය සම්පූර්ණයෙන්ම ගොඩක් භාෂාව ප්\u200dරයෝජනය කරන්න සාමාන්\u200dය භාෂාවක් වලින් ප්\u200dරයෝජනය කරලා තියෙන්නේ භාෂාවක් වලින් සමාන්\u200dය ලිපින්ත තැනක් තොරතුරු හැම ලිපින්තේ ප්\u200dරශ්නය සම්බන්ධ කරනවා. මේ විදියට සියළු භාෂා ජෝඩුවට විශේෂයක් නැති විශේෂයක් වැඩ කරනවා ඒ වගේම ශූන්ය- ශෝන්ය විද ඒක තමයි, නොදන්න බැරි භාෂාවල් වලින් අවවාද කරන්න පුළුවන් සාමාන්\u200dය භාෂාවට ස්තූතියි - විශේෂයෙන', 'so': 'Warqadan waxaa ku qoran qayb ka qaata UdS-DFKI ee shaqada luuqadaha kala duduwan ee qiimeynta IWSLT 2017. Dhaqdhaheenna waxay ku saleysan tahay nidaamka tarjumidda luuqadaha kala duduwan ee kooxaha neurada ah, taas oo ku socota macluumaadka yar iyo shuruudaha waxbarashada nooca ah. nidaamkayaga waxaa loo qoray inay si buuxsamaan u isticmaalaan qalabka kala duduwan, kuwaas oo ku jira waxyaabaha ku kordhiya tirada waxyaabaha caadiga ah oo luuqadaha ku jira, sida kooxaha telefonetka ah oo kooxaha la qorayo iyo sindiyada, oo aan ku jirin qeyb shallo oo hadal ah, heerar iyo luuqad. Macluumaadka heerka dukumentiga waxaa sidoo kale looga fikiraa maadooyinka dukumenti kasta. Xaalku waa horumariyaa qoraalka aasaasiga ah oo aan sabab dheeri u lahayn labada labo oo luqada ah islamarkaasna wuxuu sidoo kale u ogolaan karaa turjumaadda wax ka badan zero-shot. Taasi waa, turjumidda luuqadaha aan la garanayn waxaa suurtogal ah in lagu takooro alaabta caadiga ah - si gaar ah muuqashada noocyada- luuqadaha dhexdooda.', 'ms': 'This paper describes the UdS-DFKI participation to the multilingual task of the IWSLT Evaluation 2017.  pendekatan kita berdasarkan sistem terjemahan saraf berbilang bahasa yang berkaitan mengikut data kecil dan keadaan latihan senjata. Sistem kita dirancang untuk mengeksploitasi penuh kebanyakan bahasa dengan termasuk faktor yang meningkatkan bilangan unsur umum di antara bahasa seperti pengekodan fonetik kasar dan sinset, selain sebahagian rendah tag-ucapan, stem dan lemma. Maklumat aras dokumen juga dianggap dengan termasuk topik setiap dokumen. Pendekatan ini memperbaiki dasar tanpa sebarang faktor tambahan untuk semua pasangan bahasa dan bahkan membenarkan terjemahan diluar-sifar-shot. That is, the translation from unseen languages is possible thanks to the common elements -especially synsets in our models- among languages.', 'ta': 'இந்த தாள் IWSLT Evaluation 2017 யின் பல மொழி பணிக்கு UdS-DFKI பகிர்ந்ததை விளக்குகிறது. சிறிய தரவுகள் மற்றும் சூழ்நிலையான பயிற்சி நிலைகளைப் பின்பற்றி பல மொழிகளின் பாதுகாரண மொழிமாற்றும் அமைப்பு போன்டிக் குறியீடுகள் மற்றும் ஒத்திசைவுகள் போன்டிக் குறியீடுகள் மற்றும் பேச்சு குறியீடுகள் மற்றும் முழு பேச்சு ஒத்திசைவுகள் போன்ற பேச்சு ஒட் ஒவ்வொரு ஆவணத்தின் தலைப்பை சேர்த்து ஆவண நிலை தகவலும் கருதப்படுகிறது. @ info அதாவது, மறைக்கப்படாத மொழிகளில் இருந்து மொழிமாற்றி பொதுவான உறுப்புகளுக்கு நன்றி - குறிப்பாக எங்கள் மாதிரிகள', 'ml': 'ഈ പത്രത്തില്\u200d യുഡ്എസ്-ഡിഫ്കിയുടെ പങ്കെടുപ്പിനെ വിശദീകരിക്കുന്നു. ഐഡഎസ്\u200cഎസ്\u200cഎംഎസ്\u200cഎംഎസ് എവില്യൂഷന്\u200d  ഈ ചെറിയ വിവരങ്ങളും പൂജ്യത്തിന്റെ പരിശീലിപ്പിനുള്ള അവസ്ഥയും പിന്നീട് നമ്മുടെ പരിശീലനത്തിന്റെ അടിസ്ഥാനമാണ്. നമ്മുടെ സിസ്റ്റത്തിന്റെ സംവിധാനങ്ങള്\u200d പൂര്\u200dണ്ണമായും അഭിപ്രായം ഉപയോഗിക്കാനാണ് നിര്\u200dമ്മിക്കുന്നത്. ഫോണെറ്റിക് കോര്\u200dസ് കോര്\u200dഡിങ്ങും സിനിസ്റ്റുകളും  എല്ലാ രേഖയുടെയും പ്രമേയത്തിലുള്ള രേഖയുടെയും വിവരങ്ങളും കാണിക്കുന്നു. എല്ലാ ഭാഷകളുടെയും ജോണികള്\u200dക്കും കൂടുതല്\u200d കാരണമില്ലാത്ത ഒരു ബേസ്ലൈന്\u200d മെച്ചപ്പെടുത്തുന്ന ഈ നടപടി മുന്\u200dകൂട്ടു അതായത്, അദൃശ്യമായ ഭാഷകളില്\u200d നിന്നുള്ള പരിഭാഷപ്പെടുത്താന്\u200d സാധ്യതയുള്ള മൂലകങ്ങള്\u200dക്ക് നന്ദി- പ്രത്യേകിച്', 'no': 'Denne papiret beskriver UdS-DFKI-deltaket i multispråk oppgåva av IWSLT-evalueringa 2017. Tilnærminga vårt er basert på faktorerte fleirspråksomsetjingssystemer etter dei små data og nullstøvingsvilkår. Sistemet våre er utforma for å fullstendig bruka fleirspråk ved å inkludere faktorer som aukar talet på felles elementer blant språk, som fonetiske koding og synsett, blant sålmålt del av taletaggar, stemmer og lemmar. Informasjon om dokumentnivået blir også vurdert ved å inkludere emnet på kvar dokument. Denne tilnærminga forbetrar ein grunnlinje utan nokon tilleggsfaktor for alle språkopla og tillat til å oversette nullsatt omsetjing. Dette er at omsetjinga frå ugjennomsiktige språk er mogleg takk av dei felles elementa - spesielt synkroniserer i våre modeller - blant språk.', 'sv': 'Denna uppsats beskriver UdS-DFKI deltagande i den flerspråkiga uppgiften för IWSLT Evaluation 2017. Vårt tillvägagångssätt bygger på faktorerade flerspråkiga neurala översättningssystem som följer små data och noll-skott träningsförhållanden. Våra system är utformade för att fullt ut utnyttja flerspråkighet genom att inkludera faktorer som ökar antalet gemensamma element bland språk såsom fonetiska grova kodningar och synsets, förutom ytliga delar av tal taggar, stammar och lemmer. Information på dokumentnivå beaktas också genom att inkludera ämnet i varje dokument. Detta tillvägagångssätt förbättrar en baslinje utan ytterligare faktor för alla språkpar och tillåter till och med översättning bortom nollskott. Det vill säga översättningen från osynliga språk är möjlig tack vare de gemensamma elementen -särskilt synuppsättningar i våra modeller- bland språk.', 'ur': 'This paper describes the UdS-DFKI participation in the multilingual task of the IWSLT Evaluation 2017. ہمارا طریقہ بہت سی زبان کی تعلیم سیستموں پر بنیاد ہے جو چھوٹے ڈاٹے اور صفر-شٹ کی ترکین شرایط کے پیچھے ہیں. ہماری سیستموں کی طراحی کی گئی ہے کہ بہت سی زبانوں کی تعداد میں فکتوروں کے ساتھ مزید استعمال کریں جو زبانوں میں بہت سی اتحادیوں کی تعداد زیادہ کریں جیسے فونیٹ کورس اکڈینڈ اور سینسٹ، بہت سی بات ٹاگ، استمز اور لیمز کے علاوہ. دفتر سطح معلومات بھی ہر دفتر کے موضوع میں شامل ہوتے ہیں۔ یہ طریقہ تمام زبان جوڑوں کے لئے بغیر کسی زیادہ فکتوری کے برابر بنیس لین کو بہتر کرتا ہے اور حتی صفر-شٹ کے بعد بھی ترجمہ کرنے کی اجازت دیتا ہے. یہی ہے، غیب کی زبانوں سے ترجمہ امکان ہے، مخصوصاً ہمارے مدلکوں میں، زبانوں کے درمیان سینسٹ کے شکریہ.', 'vi': 'Tờ giấy này mô tả sự tham gia của UdS-DFS KIM vào nhiệm vụ đa dạng của IWSLT BẢbây giờ. Cách tiếp cận của chúng tôi dựa trên các hệ thống dịch chuyển thần kinh đa số nhân tạo theo các dữ liệu nhỏ và trường huấn luyện không phát. Hệ thống của chúng tôi được thiết kế để khai thác đầy đủ các yếu tố có thể gia tăng số yếu tố phổ biến trong các ngôn ngữ như các cấu tạo và cấu tạo chữ liên, ngoài những thẻ giọng phát âm nông, các dòng và các loại vượn. Thông tin cấp tài liệu cũng được xem xét bằng cách bao gồm chủ đề của mỗi tài liệu. Cách tiếp cận này cải thiện cơ sở cơ bản mà không có thêm nhân tố cho mọi cặp ngôn ngữ và thậm chí cho phép dịch không bắn. Tức là, dịch từ ngôn ngữ vô hình có thể được thực hiện nhờ vào các yếu tố chung, đặc biệt là các cấu tạo trong các mô hình.', 'uz': "Bu qogʻoz IWSLT Evaluation 2017 bir necha tildagi vazifaning UdS-DFKI qismlarini anglatadi. Bizning tilimiz bir necha tilda neyron tarjima tizimi asosida yaratilgan kichkina maʼlumot va nuqta o'rganish holatlaridan keyin bir necha tilda tarjima qilish tizimi asosida. Bizning tizimlarimiz bir necha tillikni butunlay ishlab chiqarish mumkin. Fonetga koʻpaytirish va bir qismi bilan gapirish taglari, vositalar va lema bilan bir qanchalik tarkibini oshirish mumkin. Ҳужжат даражаси маълумот ҳар бир ҳужжатнинг мавзуси билан ҳисобланади. @ info That is, the translation from unseen languages is possible thanks to the common elements -especially synsets in our models- among languages.", 'bg': 'Настоящата статия описва участието на УДС-ДФКИ в многоезичната задача на Оценката на международен транспорт 2017. Нашият подход се основава на факторирани многоезични невронни преводни системи, следващи условията за малки данни и нулеви тренировки. Нашите системи са проектирани да използват пълноценно многоезичието чрез включване на фактори, които увеличават броя на общите елементи сред езиците като фонетични груби кодировки и синсети, освен плитки тагове за част от речта, стъбла и леми. Информацията за ниво документ също се разглежда чрез включване на темата на всеки документ. Този подход подобрява базовата линия без допълнителен фактор за всички езикови двойки и дори позволява превод извън нулата. Тоест преводът от невидими езици е възможен благодарение на общите елементи – особено синсетите в нашите модели – между езиците.', 'da': 'Denne artikel beskriver UdS-DFKI deltagelse i den flersprogede opgave af IWSLT Evaluation 2017. Vores tilgang er baseret på faktorerede flersprogede neurale oversættelsessystemer, der følger de små data og zero-shot træningsforhold. Vores systemer er designet til fuldt ud at udnytte flersprogethed ved at inkludere faktorer, der øger antallet af fælles elementer blandt sprog, såsom fonetiske grove kodninger og synsets, udover overfladiske dele af tale tags, stængler og lemmer. Oplysninger på dokumentniveau overvejes også ved at inkludere emnet i hvert dokument. Denne fremgangsmåde forbedrer en baseline uden yderligere faktor for alle sprogpar og tillader endda oversættelse uden nul skud. Det vil sige, at oversættelse fra usynlige sprog er mulig takket være de fælles elementer - især synsets i vores modeller - blandt sprog.', 'nl': "Dit artikel beschrijft de deelname van UdS-DFKI aan de meertalige taak van de IWSLT Evaluatie 2017. Onze aanpak is gebaseerd op meertalige neurale vertaalsystemen die de kleine data en zero-shot trainingsomstandigheden volgen. Onze systemen zijn ontworpen om meertaligheid volledig te benutten door factoren op te nemen die het aantal gemeenschappelijke elementen tussen talen verhogen, zoals fonetische grove coderingen en synsets, naast ondiepe part-of-speech tags, stengels en lemma's. Informatie op documentniveau wordt ook overwogen door het onderwerp van elk document op te nemen. Deze aanpak verbetert een baseline zonder extra factor voor alle taalparen en maakt zelfs vertaling zonder nul-shot mogelijk. Dat wil zeggen, de vertaling uit onzichtbare talen is mogelijk dankzij de gemeenschappelijke elementen -vooral synsets in onze modellen- tussen talen.", 'hr': 'Ovaj papir opisuje sudjelovanje UdS-DFKI u multijezičkom zadatku procjene IWSLT 2017. Naš pristup se temelji na faktoriranim multijezičkim sustavima neurološkog prevoda nakon malih podataka i uslova obuke nule pucnjave. Naši sustavi su dizajnirani da potpuno iskoriste multijezičnost uključujući faktore koji povećavaju broj zajedničkih elementa među jezicima poput fonetičkih kodiranja i sinteza, osim plitkih dijelogovornih znakova, matičnih i limenata. Informacije o razini dokumenta također se razmatraju uključujući temu svakog dokumenta. Ovaj pristup poboljšava početnu liniju bez dodatnog faktora svih jezičkih parova i čak omogućava prevod preko nula. To je, prevod iz nevidljivih jezika moguće zahvaljujući zajedničkim elementima - posebno sinkronizacijama u našim modelima - među jezicima.', 'de': 'Dieser Beitrag beschreibt die Beteiligung des UdS-DFKI an der mehrsprachigen Aufgabe der IWSLT Evaluation 2017. Unser Ansatz basiert auf faktorierten mehrsprachigen neuronalen Translationssystemen, die den kleinen Daten und Zero-Shot Trainingsbedingungen folgen. Unsere Systeme sind so konzipiert, dass sie die Mehrsprachigkeit voll ausnutzen, indem sie Faktoren berücksichtigen, die die Anzahl gemeinsamer Elemente zwischen Sprachen erhöhen, wie phonetische grobe Codierungen und Synsets, sowie flache Teile der Sprache Tags, Stems und Lemmas. Informationen auf Dokumentenebene werden auch berücksichtigt, indem das Thema jedes Dokuments berücksichtigt wird. Dieser Ansatz verbessert eine Baseline ohne zusätzlichen Faktor für alle Sprachpaare und ermöglicht sogar Übersetzungen über Null hinaus. Das heißt, die Übersetzung aus unsichtbaren Sprachen ist dank der gemeinsamen Elemente – insbesondere Synsets in unseren Modellen – unter den Sprachen möglich.', 'id': 'Kertas ini menjelaskan UdS-DFKI berpartisipasi dalam tugas multibahasa Evaluasi IWSLT 2017. pendekatan kita berdasarkan faktor sistem terjemahan saraf multibahasa mengikuti data kecil dan kondisi latihan nol. Sistem kita dirancang untuk mengeksploitasi penuh multilingualisme dengan termasuk faktor yang meningkatkan jumlah elemen umum di antara bahasa seperti kode fonetik kasar dan sinset, selain bagian rendah-dari-pidato tag, stem dan lemma. Informasi tingkat dokumen juga dipertimbangkan dengan termasuk topik setiap dokumen. This approach improves a baseline without any additional factor for all the language pairs and even allows beyond-zero-shot translation.  That is, the translation from unseen languages is possible thanks to the common elements -especially synsets in our models- among languages.', 'ko': '본고는 UdS DFKI가 2017년 국제여자직업기술대학의 평가에 참여한 다국어 임무를 묘사한다.우리의 방법은 소형 데이터와 0회 훈련 조건에서의 다언어 신경 번역 시스템을 바탕으로 한다.우리의 시스템은 여러 가지 언어를 충분히 이용하는 데 목적을 둔다. 얕은 단어성 표기, 어간과 인용 외에 언어 간의 공공 요소 수량을 증가시키는 요소, 예를 들어 음성 굵은 인코딩과 문법집도 포함한다.문서 수준의 정보도 각 문서가 포함된 주제를 통해 고려할 수 있다.이런 방법은 모든 언어의 기준을 개선했고, 어떠한 추가 요소도 없었으며, 심지어는 제로 렌즈 번역도 허용했다.즉, 언어 간의 공통 요소, 특히 우리 모델 중의 문법집 때문에 보이지 않는 언어에서 번역할 수 있다.', 'sw': 'Gazeti hili linaelezea ushiriki wa UdS-DFKI katika kazi ya lugha mbalimbali ya UWSLT 2017. Hatua yetu inatokana na mfumo wa utafsiri wa lugha mbalimbali kufuatia takwimu ndogo na mazingira ya mafunzo yasiyo na sifuri. Mifumo yetu imetengenezwa kutumia viungo vingi kwa pamoja na sababu ambazo zinaongezea idadi ya vipengele vya kawaida miongoni mwa lugha kama vile kodi za mkononi na syntetezo, pamoja na sehemu mbaya ya za hotuba, viungo na viungo. Taarifa za kiwango cha nyaraka pia zinachukuliwa na pamoja na mada ya kila nyaraka. Hatua hii inaboresha mstari wa msingi bila sababu yoyote ya wanandoa wa lugha zote na hata inaruhusu tafsiri isiyo na sifuri. Hiyo ndiyo, tafsiri kutoka lugha isiyo na mambo yanawezekana shukrani kwa vipengele vya kawaida –hususani kwenye mifano yetu- miongoni mwa lugha zetu.', 'fa': 'این کاغذ مشارکت UdS-DFKI را توصیف می\u200cکند در کار بسیاری زبان\u200cهای ارزیابی IWSLT ۲۰۱۷. روش ما بر اساس سیستم\u200cهای ترجمه\u200cهای عصبی چندین زبان\u200cهای فعال شده است که در دنبال اطلاعات کوچک و شرایط آموزش\u200cهای صفر صفر است. سیستم\u200cهای ما برای کامل استفاده از زبان\u200cهای زیادی طراحی شده\u200cاند، با جمله faktورها که تعداد عناصر معمولی بین زبانها، مانند رمزبندی\u200cهای تلفنی و سینست\u200cهای زیادی، به جز برچسب\u200cهای زیادی سخنرانی، استیمز و لیمز افزایش می\u200cدهند. اطلاعات سطح سند همچنین با توجه به عنوان موضوع هر سند به نظر می رسد. این روش یک خط پایین را بدون هیچ faktor اضافی برای همه جفت زبان بهتر می\u200cکند و حتی اجازه می\u200cدهد که فراتر از صفر ترجمه کنید. این است که ترجمه از زبان غیر قابل توجه به عنوان عناصر مشترک ممکن است، مخصوصا در مدل\u200cهای ما، بین زبان\u200cها.', 'tr': 'Bu kagyz UdS-DFKI 2017-nji IWSLT Taýýarlama täblisasynda gatnaşmagy bardyr. Biziň ýaryşymyz kiçi maglumatlar we zero-atly eğitim şartlaryna görä çevirili multidilli näyral terjime sistemalaryna daýanýar. Biziň sistemamyz dillerden, fonetik kodlemeler we synsetler ýaly köp dillerden, süzmeleriň we limmalaryň arasynda köp dillerini ulanan faktörler bilen doly ulanmak üçin tasarlanýar. Senediň ehli maglumaty hem her senediň meýdançasynda görkezilsin. Bu ýaryşy ähli dil çiftleri üçin esasy çyzgylygy bejermek üçin üýtgedýär we hatda 0-atdan öňünde terjime edip bilýär. Bärde, gaýd edilmedik dillerden terjime edilen orta elementleriň üçin mümkin däldir -iň özellikle biziň modellerimizde- dillerimiz arasynda syntaklaýar.', 'af': "Hierdie papier beskrywe die UdS-DFKI deelnadering na die multilinglike taak van die IWSLT Evaluering 2017. Ons toegang is gebaseer op faktoreerde multitaalske neurale vertalingsstelsels volgens die klein data en nul-skoot onderwerp voorwaardes. Ons stelsels is ontwerp om volledig multilingaliteit te gebruik deur die insluitende faktore wat die aantal gemeenskaplike elemente onder tale vermeerder soos fonetiese kodering en sinkronisasies, behalwe skaal deel van spraak etikette, stamme en lemmas. Dokumentvlak inligting word ook aangeneem deur insluitend die onderwerp van elke dokument. Hierdie toegang verbeter 'n basislien sonder enige addisionele faktor vir al die taal pare en selfs toelaat buite-nul-skoot vertaling. Dit is, die vertaling van onverskynlike tale is moontlik dank aan die gemeenskare elemente - veral sinkroniseer in ons modele- onder tale.", 'az': 'Bu kağıt 2017-ci IWSLT değerlendirməsinin çoxlu dil işinin UdS-DFKI iştirağını təsdiq edir. Bizim tərzimiz küçük məlumatlar və sıfır fərqli təhsil şartları ilə fəxr edilmiş çoxlu dilli nöral tercümə sistemlərinə dayanılır. Bizim sistemlərimiz çoxlu dilləri istifadə etmək üçün, sözlərin bir parças ı etiketlərin, stems və limlərin arasındakı çoxlu elementlərin sayını artıran faktörlərlə birlikdə təşkil edilmişdir. Hər dökümənin məsələsini də dahil edərək belə səhifə məlumatı düşünürlər. Bu tərzim bütün dil çiftləri üçün heç bir əlavə faktör olmadan əsas çizgini yaxşılaşdırır və hətta sıfır çəkməyə imkan verir. Bu, qeyb dillərin çeviri ortaq elementlərin tərzində mümkün olar. Özellikle modellərimizdə sinqutlar - dillərin arasında.', 'am': 'ይህ ገጽ የዩድS-DFKI የIWSLT አካላት 2017 በብዙ ቋንቋ ስራ ላይ ተግባር ይናገራል፡፡ የብዙ ቋንቋዎች የደዌብ ትርጉም ሥርዓት ከታናሹ ዳታዎች እና የzero-shot ተማሪ ግንኙነት ተከትሎ ነው፡፡ ሲስተምሮቻችን ብዙዎችን ልዩ ልዩ ቋንቋዎች ለመፍጠር ይደረጋሉ፡፡ የሰነዱ ደረጃዎች መረጃ ከሁሉም ሰነዱን በመጨመር ይጨመርበታል። የቋንቋ ቋንቋዎች ሁኔታ ሳይኖር ይህ ሥርዓት የbaseline ጥያቄን ያሳድጋል፡፡ That is, the translation from unseen languages is possible thanks to the common elements -especially synsets in our models- among languages.', 'sq': 'Ky dokument përshkruan pjesëmarrjen e UdS-DFKI në detyrën shumëgjuhëse të vlerësimit të IWSLT 2017. Përqafimi ynë është bazuar në sistemet e përkthimit nervor shumëgjuhës faktorizuar pas të dhënave të vogla dhe kushteve të trajnimit zero-shot. Sistemet tona janë dizajnuar për të shfrytëzuar plotësisht shumëgjuhësinë duke përfshirë faktorë që rritin numrin e elementeve të përbashkëta midis gjuhëve të tilla si kodifikimet fonetike të mëdha dhe sinsettet, përveç etiketave të sipërfaqe të fjalimit, shtyllave dhe limave. Informacioni mbi nivelin e dokumentit konsiderohet gjithashtu duke përfshirë temën e çdo dokumenti. Ky qasje përmirëson një bazë pa ndonjë faktor shtesë për të gjitha çiftet e gjuhës dhe madje lejon përkthimin përtej zero-shot. Kjo është, përkthimi nga gjuhët e padukshme është i mundur falë elementeve të përbashkëta - veçanërisht sintetit në modelet tona - midis gjuhëve.', 'bn': 'এই পত্রিকাটি ইউডিএসএলটি ইমানুয়েশন ২০১৭ এর বহুভাষার কাজে ইউডএস-ডিএফকির অংশগ্রহণের ব্যাখ্যা করছে। Our approach is based on factored multilingual neural translation systems following the small data and zero-shot training conditions.  আমাদের সিস্টেম সম্পূর্ণ ভাষার মাধ্যমে বিভিন্ন ভাষার মধ্যে সাধারণ উপাদান বৃদ্ধি করা হয়েছে, যেমন ফোনেটিক কোর্স এনকোডিং এবং সিনেটগুলোর সংখ্যা বৃদ্ধি করে, য প্রত্যেক নথির বিষয়বস্তু সহ ডকুমেন্ট স্তরের তথ্য বিবেচনা করা হয়। এই পদ্ধতি সকল ভাষার জোড়ার জন্য বেসেলাইনের কোনো কারণ ছাড়া একটি বেসেলাইন উন্নত করে এবং এমনকি এমনকি শুধুমাত্র শুটের বাইরে  এটাই, অদৃশ্য ভাষার অনুবাদ সম্ভব, সাধারণ উপাদানের জন্য ধন্যবাদ - বিশেষ করে আমাদের মডেল- ভাষার মধ্যে সিনেট।', 'bs': 'Ovaj papir opisuje sudjelovanje UdS-DFKI na višejezički zadatak procjene IWSLT 2017. Naš pristup je baziran na faktoriranim multijezičkim sustavima neurološkog prevoda nakon malih podataka i uslova obuke nule pucnjave. Naši sustavi su dizajnirani da potpuno iskoriste multijezičnost uključujući faktore koji povećavaju broj zajedničkih elementa među jezicima poput fonetičkih kodiranja i sinteza, osim plitkih dijelogovornih znakova, matičnih i limenata. Informacije o nivou dokumenta se također razmatraju uključujući temu svakog dokumenta. Ovaj pristup poboljšava početnu liniju bez ikakvog dodatnog faktora svih jezičkih parova i čak omogućava prevod bez nula. To je, prevod iz nevidljivih jezika moguće zahvaljujući zajedničkim elementima - posebno sinkronizacijama u našim modelima - među jezicima.', 'ca': "Aquest paper descriu la participació de l'UdS-DFKI a la tasca multilingüe de l'Evaluació IWSLT 2017. El nostre enfocament està basat en sistemes de traducció neural multilingüe fets segons les petites condicions d'entrenament de dades i dispars zero. Els nostres sistemes estan dissenyats per explotar plenament la multilingüalitat incloent factors que augmenten el nombre d'elements comuns entre les llengües, com ara codificacions fonètiques grosses i sinsets, a part de les etiquetes de fala, troncs i lemmes. Document level information is also considered by including the topic of every document.  Aquest enfocament millora la base sense cap factor adicional per a tots els parells de llengües i fins i tot permet una traducció més enllà de zero. És a dir, la traducció de llengües invisibles és possible gràcies a els elements comuns -especialment sintetitzats en els nostres models- entre les llengües.", 'cs': 'Tento článek popisuje účast UdS-DFKI na vícejazyčném úkolu hodnocení IWSLT 2017. Náš přístup je založen na faktorovaných vícejazyčných neuronových translačních systémech, které sledují malá data a nulové tréninkové podmínky. Naše systémy jsou navrženy tak, aby plně využívaly mnohojazyčnost zahrnutím faktorů, které zvyšují počet společných prvků mezi jazyky, jako jsou fonetické hrubé kódování a synsety, kromě mělkých značek části řeči, stonků a lemmat. Informace na úrovni dokumentu jsou také zohledněny zahrnutím tématu každého dokumentu. Tento přístup zlepšuje základní hodnotu bez jakéhokoli dalšího faktoru pro všechny jazykové páry a dokonce umožňuje překlad mimo nulu. To znamená, že překlad z neviditelných jazyků je možný díky společným prvkům -zejména synsetům v našich modelech- mezi jazyky.', 'fi': 'TÃĊssÃĊ artikkelissa kuvataan UdS-DFKI:n osallistumista IWSLT Evaluation 2017 monikieliseen tehtÃĊvÃĊÃĊn. LÃĊhestymistapamme perustuu faktoituihin monikielisiin neurokÃĊÃĊnnÃ¶sjÃĊrjestelmiin pienen datan ja nollalaukauksen harjoitteluolosuhteiden mukaisesti. JÃĊrjestelmÃĊmme on suunniteltu hyÃ¶dyntÃĊmÃĊÃĊn monikielisyyttÃĊ tÃĊysimÃĊÃĊrÃĊisesti sisÃĊllyttÃĊmÃĊllÃĊ tekijÃ¶itÃĊ, jotka lisÃĊÃĊvÃĊt yhteisten elementtien mÃĊÃĊrÃĊÃĊ kielissÃĊ, kuten foneettiset karkeat koodaukset ja synsetit, sekÃĊ matalat puheen osatunnisteet, varret ja lemmat. Asiakirjatason tiedot otetaan huomioon myÃ¶s sisÃĊllyttÃĊmÃĊllÃĊ kunkin asiakirjan aihe. TÃĊmÃĊ lÃĊhestymistapa parantaa perusaikataulua ilman lisÃĊtekijÃ¶itÃĊ kaikille kielipareille ja mahdollistaa jopa nollakuvan ylittÃĊmisen. Toisin sanoen kÃĊÃĊntÃĊminen nÃĊkymÃĊttÃ¶mistÃĊ kielistÃĊ on mahdollista kielien yhteisten elementtien â\x80\x93 erityisesti mallien synzettien â\x80\x93 ansiosta.', 'et': 'Käesolevas dokumendis kirjeldatakse UDS-DFKI osalemist 2017. aasta IWSLT hindamise mitmekeelses ülesandes. Meie lähenemine põhineb faktooritud mitmekeelsetel neurotõlkesüsteemidel, mis järgivad väikeseid andmeid ja null-shot treeningtingimusi. Meie süsteemid on loodud mitmekeelsuse täielikuks ärakasutamiseks, lisades kõnede madalate osade siltide, varrede ja lemmade kõrval tegureid, mis suurendavad keelte ühiste elementide arvu, nagu foneetilised jämedad kodeeringud ja sünkroonid. Dokumenditaseme teavet arvestatakse ka iga dokumendi teema lisamisega. Selline lähenemisviis parandab lähtetaset ilma lisategurita kõigi keelepaaride puhul ja võimaldab isegi nullplaadi tõlkimist. See tähendab, et tõlkimine nähtamatutest keeltest on võimalik tänu keelte ühistele elementidele, eriti meie mudelites olevatele sünkroonidele.', 'hy': 'This paper describes the UdS-DFKI participation to the multilingual task of the IWSLT Evaluation 2017.  Our approach is based on factored multilingual neural translation systems following the small data and zero-shot training conditions.  Մեր համակարգերը նախագծված են, որպեսզի ամբողջովին օգտագործեն բազմալեզվությունը, ներառելով գործոններ, որոնք բարձրացնում են տարրերի թիվը լեզուների մեջ, ինչպիսիք են ֆոնետիկ խիստ կոդավորումները և սինսետները, բացի խոսքի մակերեսային մասերից, պարաններից Տեղեկատվությունը փաստաթղթի մակարդակի վրա համարվում է նաև ներառելով յուրաքանչյուր փաստաթղթի թեման: Այս մոտեցումը բարելավում է հիմքը առանց որևէ ավելացնող գործոնի բոլոր լեզվի զույգերի համար և նույնիսկ թույլ է տալիս ոչ զրոյական թարգմանություն: Այսինքն, անտեսանելի լեզուներից թարգմանությունը հնարավոր է շնորհիվ ընդհանուր տարրերին, հատկապես մեր մոդելների սինսետներին, լեզուների միջև:', 'he': 'העיתון הזה מתאר את השתתפות של UdS-DFKI למשימה רבת שפותית של הערכת IWSLT 2017. הגישה שלנו מבוססת על מערכות תרגומות עצביות רבות שפות מפורסמות בעקבות הנתונים הקטנים והתנאי אימון אפס. המערכות שלנו נועדות כדי לנצל באופן מלא את המון שפויות בכלל גורמים שגודלים את מספר אלמנטים משותפים בין שפות כמו קודמות פונטיות עצומות וסינסטים, חוץ מהתגים של חלק גבוה של דיבור, גזעים ולימות. מידע רמת המסמכים נחשב גם על ידי כולל נושא של כל מסמכים. הגישה הזאת משפר את רמז הבסיס ללא שום גורם נוסף לכל זוגות השפה ואפילו מאפשר מעבר לתרגום אפס. That is, the translation from unseen languages is possible thanks to the common elements -especially synsets in our models- among languages.', 'sk': 'V prispevku je opisano sodelovanje UdS-DFKI pri večjezični nalogi IWSLT Evaluation 2017. Naš pristop temelji na faktoriranih večjezičnih nevronskih prevajalskih sistemih, ki sledijo pogojem majhnih podatkov in treninga brez strela. Naši sistemi so zasnovani tako, da v celoti izkoristijo večjezičnost z vključitvijo dejavnikov, ki povečujejo število skupnih elementov med jeziki, kot so fonetične grobe kodiranje in sinzeti, poleg plitvih oznak, stebel in lem. Informacije na ravni dokumenta se upoštevajo tudi z vključitvijo teme vsakega dokumenta. Ta pristop izboljšuje osnovno izhodišče brez dodatnih dejavnikov za vse jezikovne pare in celo omogoča prevajanje nad ničlo. To pomeni, da je prevod iz nevidnih jezikov mogoč zahvaljujoč skupnim elementom med jeziki, zlasti sinovom v naših modelih.', 'ha': "Wannan takardan na bayyana ushurin UdS-DFKI zuwa ga aikin mulki-lingui na IWSLT Bayance 2017. Mataimakinmu ne a kan da aka ƙayyade tsarin tarjibu na'urar mulki-lingui da ke ƙaranci da mazaɓa na tsari da sifiri. Ana designar ayukanmu dõmin a cika amfani da masu mulki, ko da matsalari waɗanda ke ƙara ƙidãyar ƙanshi na ɗabi'a cikin lugha kamar kodi-kodi da na'urar mutane, da kuma da sunayen sauri, ko kuma sauri. Ana iya ƙayyade maɓallin takardar aiki da aka ƙunsa da kimar duk takardar. This approach improves a baseline without any additional factor for all the language pairs and even allows beyond-zero-shot translation.  Wancan, fassarar daga harshen waɗanda ba'a sani ba, za'a iya son gõde wa ƙanshi na ɗayan -mai ƙayyade sigogi cikin misalinmu- daga harshen.", 'jv': 'Peraké iki rambarang nggambar urip udS-DFKi kanggo ngilangno langkung sampeyan kanggo ngilangno IWSLT invalusi 1997 Ndheke awak dhéwé digawe sistem sing nyebutaké karo nggambar barang langkung urip, dadi, ingkang 0. Sistem-sistem sing dibenakno kanggo nggawe akeh luwih-luwih akeh pisan karo ingkang sing katêpakan karo pernik kuwi tindakan, kaya nguasai Where\'s Mixed Language politenessoffpolite"), and when there is a change ("assertivepoliteness Ndoleh iki ditambah sing songon sing paling dong lan tambah kuwi nggawe geranggap karo nganggo kesempatan kanggo langgar sapa pangan lan tambah kuwi nggawe tarjamahan 0. Bilih, terjamahan ing langgambar obah-obahan sing bisa nggambar nggo eleman kapun -supoyo iso nggambar tarjamahan ing model- sing bisa langgambar.', 'bo': 'ཤོག་བྱང་འདིས་UdS-DFKI སྤྱི་ཚོགས་ཀྱི་སྐད་རིགས་ཀྱི་ལས་འགན་སྣ་མང་ཆེ་བ་ལ་བཤད་ཀྱི་ཡོད། ང་ཚོའི་ཐབས་ལམ་ལུགས་འདིས་སྐྱེལ་བ་མང་ཆེ་ཤོས་སྐད་ཀྱི་ནུས་པ་དབྱིབས་ཆགས་ཀྱི་རྒྱུ་དངོས་གཞི་རྟེན་ཏེ། ང་ཚོའི་མ་ལག་ནི་སྐད་ཡིག ཡིག་གེའི་ནང་དུ་ཡིག་གེའི་གནད་དོན་ཡིག་གི་གནས་ཚུལ་ཡང་བསམ་བློ་གཏད་པ སྐད་རིགས་གཉིས་ཀྱི་ཆ་རྐྱེན་འདི་ལ་གཞི་རྩིས་གཞི་ཞིག་ཡར་རྒྱས་གཏོང་བ་མེད་པར་གཞི་རྩིས་གཞི་ཞིག་དང་ཡང་བསླབ་པའ འདི་ནི་སྣང་མེད་པའི་སྐད་རིགས་ལས་ཡིག'}
{'en': 'The Samsung and University of Edinburgh’s submission to IWSLT17 S amsung and  U niversity of  E dinburgh’s submission to  IWSLT 17', 'fr': "Soumission de Samsung et de l'Université d'Édimbourg à IWSLT17", 'ar': 'تقديم سامسونج وجامعة إدنبرة إلى IWSLT17', 'pt': 'A submissão da Samsung e da Universidade de Edimburgo ao IWSLT17', 'es': 'La presentación de Samsung y la Universidad de Edimburgo a IWSLT17', 'hi': 'सैमसंग और एडिनबर्ग विश्वविद्यालय के IWSLT17 के लिए प्रस्तुत', 'ja': 'サムスンとエディンバラ大学のIWSLT 17への提出', 'zh': '三星及爱丁堡大学 IWSLT17 文', 'ru': 'Представление Samsung и Эдинбургского университета на IWSLT17', 'ga': 'Aighneacht Samsung agus Ollscoil Dhún Éideann chuig IWSLT17', 'hu': 'A Samsung és az Edinburgh-i Egyetem beadványa az IWSLT17-hez', 'it': "La presentazione di Samsung e dell'Università di Edimburgo a IWSLT17", 'ka': 'ვებინდონის სამსონგონ და სუნივერსიტი IWSLT17', 'el': 'Η υποβολή της Samsung και του Πανεπιστημίου του Εδιμβούργου στο IWSLT17', 'kk': 'Самбунг және Эдинбург университетінің IWSLT17-ке', 'mk': 'Предложението на Самсунг и Универзитетот во Единбург на IWSLT17', 'ms': 'Pemberian Samsung dan Universiti Edinburgh kepada IWSLT17', 'lt': 'Samsung ir Edinburgo universiteto pareiškimas IWSLT17', 'mt': "Is-sottomissjoni ta' Samsung u l-Università ta' Edinburgh lill-IWSLT17", 'mn': 'Самбун болон Эдинбургийн Их Сургуулийн IWSLT17-д', 'ml': 'എഡിന്\u200dബര്\u200dഗിന്\u200dറെ സാംസംഗിന്\u200dറെയും യൂണിവേഴ്സിറ്റിയിലേയ്ക്കും IWSLT17 ക്രമീകരിക്കുന്നു', 'no': 'Den Samsung og Universiteten av Edimburg s øket til IWSLT17', 'pl': 'Zgłoszenie Samsung i Uniwersytetu Edynburga do IWSLT17', 'sr': 'Predloženje Samsung i Univerziteta Edinburga IWSLT17', 'ro': 'Transmiterea Samsung și Universitatea din Edinburgh la IWSLT17', 'si': 'සැම්සන්ග් සහ එඩින්බින්ග් විශ්වාසිත්තාව IWSLT17 වලට පිළිගන්න', 'so': 'Samsung iyo Jaamacadda Edinburgh warqadiisa IWSLT17', 'sv': "Samsung och University of Edinburgh's bidrag till IWSLT17", 'ta': 'சாம்சாங்க் மற்றும் எடின்பர்க் கல்வியூரியில் IWSLT17 க்கு அனுப்புகிறது', 'ur': 'سامبنگ اور ایڈینبرنگ یونیوریوسٹ کی IWSLT17 کے تحویل', 'uz': 'Samsung va Edinburg universitetet IWSLT17 ga', 'vi': 'Công trình của Samsung và Đại học Edinburgh đã nộp cho IWSLT17', 'da': "Samsung og University of Edinburgh's indlæg til IWSLT17", 'hr': 'Predloženje Samsung i Univerziteta Edinburga IWSLT17', 'bg': 'Представянето на Samsung и Университета в Единбург за IWSLT17', 'nl': 'De inzending van Samsung en de Universiteit van Edinburgh aan IWSLT17', 'id': 'Samsung dan Universitas Edinburgh menyerahkan kepada IWSLT17', 'de': 'Die Einreichung von Samsung und der Universität Edinburgh an IWSLT17', 'fa': 'سپاسگز و دانشگاه ادینبورگ به IWSLT17', 'sw': 'Tamko la Samsung na Chuo Kikuu cha Edinburgh kwa IWSLT17', 'tr': 'Edimburgyň IWSLT17 Uniwersiteti Samsung we Uniwersiteti', 'sq': 'Subjektimi i Samsung dhe Universitetit të Edinburgut në IWSLT17', 'af': 'Die Samsung en Universiteit van Edinburger se ondersoek aan IWSLT17', 'am': 'የሳምስጋን እና የኤዲንቡር ዩኒቨርስቲ IWSLT17 አዋጅ', 'hy': 'Սամսունգը և Էդինբուրգի համալսարանը IW-ՍԼԹ17', 'az': 'Edinburgon Üniversitesi Samsung və IWSLT17', 'ko': '삼성과 에든버러대가 IWSLT17에 제출', 'bn': 'এডিনবার্গ বিশ্ববিদ্যালয়ের স্যামসংগ এবং বিশ্ববিদ্যালয় ইউডএসএলটি১৭ এর প্রতি প্রদান করেছেন', 'bs': 'Predloženje Samsung i Univerziteta Edinburga IWSLT17', 'ca': "La presentació de Samsung i de la Universitat d'Edimburgu17 a l'IWSLT17", 'cs': 'Podání Samsung a University of Edinburgh IWSLT17', 'et': 'Samsungi ja Edinburghi Ülikooli esitatud taotlus IWSLT17-le', 'fi': 'Samsungin ja Edinburghin yliopiston esitys IWSLT17:lle', 'jv': 'Rasané neng Saturdayg karo Universite dumadhining IWSLT7', 'he': 'ההעברה של סמסונג ואוניברסיטת אדינבורג ל IWSLT17', 'ha': "The Samsung and University of Edinburgh's submission to IWSLT17", 'sk': 'Predložitev Samsung in Univerze v Edinburgu za IWSLT17', 'bo': "Edimburg's IWSLT17 ལ་བཤད་པ་ཡིན་ཆེན་སོགས་དང་ཆེན་ཕྱོགས་ཀྱི་ཆེན་ཡུལ།"}
{'en': 'This paper describes the joint submission of Samsung Research and Development, Warsaw, Poland and the University of Edinburgh team to the IWSLT MT task for TED talks. We took part in two translation directions, en-de and de-en. We also participated in the en-de and de-en lectures SLT task. The models have been trained with an attentional encoder-decoder model using the BiDeep model in  Nematus . We filtered the training data to reduce the problem of  noisy data , and we use back-translated monolingual data for domain-adaptation. We demonstrate the effectiveness of the different techniques that we applied via  ablation studies . Our submission system outperforms our  baseline , and last year’s University of Edinburgh submission to IWSLT, by more than 5  BLEU .', 'pt': 'Este artigo descreve a apresentação conjunta da Samsung Research and Development, Varsóvia, Polônia e da equipe da Universidade de Edimburgo para a tarefa IWSLT MT para palestras TED. Participamos de duas direções de tradução, en-de e de-en. Também participamos da tarefa SLT de palestras en-de e de-en. Os modelos foram treinados com um modelo de codificador-decodificador de atenção usando o modelo BiDeep no Nematus. Filtramos os dados de treinamento para reduzir o problema de dados ruidosos e usamos dados monolíngues retrotraduzidos para adaptação ao domínio. Demonstramos a eficácia das diferentes técnicas que aplicamos por meio de estudos de ablação. Nosso sistema de submissão supera nossa linha de base, e a submissão da Universidade de Edimburgo do ano passado ao IWSLT, em mais de 5 BLEU.', 'es': 'Este artículo describe la presentación conjunta de Samsung Research and Development, Varsovia, Polonia, y el equipo de la Universidad de Edimburgo a la tarea IWSLT MT para las charlas TED. Participamos en dos direcciones de traducción, en-de y de-en. También participamos en la tarea SLT de conferencias en-de y de-en. Los modelos se han entrenado con un modelo de codificador-decodificador atencional que utiliza el modelo BiDeep en Nematus. Filtramos los datos de entrenamiento para reducir el problema de los datos ruidosos y utilizamos datos monolingües retrotraducidos para la adaptación al dominio. Demostramos la eficacia de las diferentes técnicas que aplicamos mediante estudios de ablación. Nuestro sistema de presentación supera nuestra línea de base, y la presentación de la Universidad de Edimburgo del año pasado a IWSLT, en más de 5 BLEU.', 'ar': 'تصف هذه الورقة التقديم المشترك لـ Samsung Research and Development ، وارسو ، بولندا وفريق جامعة إدنبرة إلى مهمة IWSLT MT لمحادثات TED. شاركنا في اتجاهين للترجمة ، en-de و de-en. شاركنا أيضًا في مهمة SLT للمحاضرات en-de and de-en. تم تدريب النماذج باستخدام نموذج وحدة فك ترميز متعمد باستخدام نموذج BiDeep في Nematus. قمنا بتصفية بيانات التدريب لتقليل مشكلة البيانات الصاخبة ، ونستخدم بيانات أحادية اللغة مترجمة مرة أخرى لتكييف المجال. نثبت فعالية التقنيات المختلفة التي طبقناها من خلال دراسات الاجتثاث. يتفوق نظام التقديم لدينا على خط الأساس لدينا ، وتقديم جامعة إدنبرة العام الماضي إلى IWSLT ، بأكثر من 5 BLEU.', 'fr': "Ce document décrit la soumission conjointe de Samsung Research and Development, Varsovie, Pologne et l'équipe de l'Université d'Edimbourg à la tâche IWSLT MT pour les conférences TED. Nous avons participé à deux directions de traduction, en-de et de-en. Nous avons également participé à la tâche SLT des conférences en-de et de-en. Les modèles ont été entraînés avec un modèle codeur-décodeur attentionnel utilisant le modèle BiDeep dans Nematus. Nous avons filtré les données d'entraînement afin de réduire le problème des données bruyantes, et nous utilisons des données monolingues rétro-traduites pour l'adaptation au domaine. Nous démontrons l'efficacité des différentes techniques que nous avons appliquées par le biais d'études d'ablation. Notre système de soumission surpasse notre niveau de référence, et la soumission de l'Université d'Édimbourg à l'IWSLT de l'année dernière, de plus de 5 UEBL.", 'ja': '本稿では、サムスン研究開発、ポーランド、ワルシャワ、エディンバラ大学チームによるTED講演のためのIWSLT MTタスクへの共同提出について説明します。私たちは、en - deとde - enの2つの翻訳ディレクションに参加しました。また、en - deとde - enの講義SLTタスクにも参加した。モデルは、NematusのBiDeepモデルを使用して、注意深いエンコーダデコーダモデルでトレーニングされています。ノイズの多いデータの問題を軽減するためにトレーニングデータをフィルタリングし、ドメイン適応のために逆翻訳されたモノリンガルデータを使用しました。焼灼療法を通じて適用したさまざまな手法の有効性を実証しています。当社の提出システムは、ベースラインを上回り、昨年のエディンバラ大学のIWSLTへの提出は、5 BLEUを超えました。', 'zh': '本文引波兰华沙三星研发中心与爱丁堡大学团队合 IWSLT MT T TED 讲。 二译者,en-dede-en也。 与en-de、de-en讲座SLT事。 既NematusBiDeep用意编码器 - 解码器习之。 过练数以减噪声数,并用反译单语数以应之。 融证异术之有效性。 我们的提交系统比我们的基线和去年爱丁堡大学向IWSLT提交的申请高出5 BLEU以上。', 'hi': 'यह पेपर सैमसंग रिसर्च एंड डेवलपमेंट, वारसॉ, पोलैंड और एडिनबर्ग विश्वविद्यालय की टीम को टेड वार्ता के लिए IWSLT एमटी कार्य के लिए संयुक्त प्रस्तुत करने का वर्णन करता है। हमने दो अनुवाद दिशाओं में भाग लिया, एन-डी और डी-एन। हमने एन-डी और डी-एन व्याख्यान एसएलटी कार्य में भी भाग लिया। मॉडल को नेमेटस में BiDeep मॉडल का उपयोग करके एक चौकस एन्कोडर-डिकोडर मॉडल के साथ प्रशिक्षित किया गया है। हमने शोर डेटा की समस्या को कम करने के लिए प्रशिक्षण डेटा को फ़िल्टर किया, और हम डोमेन-अनुकूलन के लिए बैक-अनूदित मोनोलिंगुअल डेटा का उपयोग करते हैं। हम विभिन्न तकनीकों की प्रभावशीलता का प्रदर्शन करते हैं जिन्हें हमने एब्लेशन अध्ययनों के माध्यम से लागू किया था। हमारी सबमिशन सिस्टम हमारी बेसलाइन को मात देती है, और पिछले साल एडिनबर्ग विश्वविद्यालय ने IWSLT को प्रस्तुत किया था, 5 से अधिक BLEU द्वारा।', 'ru': 'Этот документ описывает совместное представление Samsung Research and Development, Варшава, Польша и команда Эдинбургского университета к задаче MT IWSLT для переговоров TED. Мы приняли участие в двух направлениях перевода, en-de и de-en. Мы также приняли участие в лекциях по СЛТ. Модели были обучены с помощью модели внимательного кодера-декодера с использованием модели BiDeep в Nematus. Мы отфильтровали обучающие данные, чтобы уменьшить проблему шумных данных, и используем одноязычные данные с обратным переводом для адаптации домена. Мы демонстрируем эффективность различных методик, которые мы применяли с помощью исследований абляции. Наша система подачи заявок превосходит нашу базовую линию, а прошлогодняя подача заявок в IWSLT в Эдинбургский университет более чем на 5 БЛЮ.', 'ga': 'Déanann an páipéar seo cur síos ar chomh-aighneacht fhoireann Taighde agus Forbartha Samsung, Vársá, an Pholainn agus Ollscoil Dhún Éideann chuig tasc IWSLT MT le haghaidh cainteanna TED. Ghlacamar páirt i dhá threo aistriúcháin, en-de agus de-en. Ghlacamar páirt freisin i dtasc SLT léachtaí en-de agus de-en. Cuireadh oiliúint ar na samhlacha le samhail ionchódóra-aireach a úsáideann an tsamhail BiDeep in Nematus. Rinneamar na sonraí oiliúna a scagadh chun fadhb na sonraí callánacha a laghdú, agus úsáidimid sonraí aonteangacha aisaistrithe le haghaidh oiriúnú fearainn. Léirímid éifeachtacht na dteicníochtaí éagsúla a chuireamar i bhfeidhm trí staidéir eisiblithe. Tá feidhmíocht ár gcóras aighneachta níos fearr ná ár mbunlíne, agus aighneacht Ollscoil Dhún Éideann anuraidh chuig IWSLT, níos mó ná 5 BLEU.', 'el': 'Η παρούσα εργασία περιγράφει την κοινή υποβολή της έρευνας και ανάπτυξης της Samsung, Βαρσοβίας, Πολωνίας και της ομάδας του Πανεπιστημίου του Εδιμβούργου στο έργο MT IWSLT για συνομιλίες TED. Πήραμε μέρος σε δύο μεταφραστικές κατευθύνσεις, en-de και de-en. Συμμετείχαμε επίσης στις διαλέξεις και τις διαλέξεις. Τα μοντέλα έχουν εκπαιδευτεί με ένα μοντέλο κωδικοποιητή-αποκωδικοποιητή προσοχής χρησιμοποιώντας το μοντέλο BiDeep στη Νεμάτο. Φιλτραρίσαμε τα δεδομένα εκπαίδευσης για να μειώσουμε το πρόβλημα των θορυβωδών δεδομένων, και χρησιμοποιούμε πίσω μεταφρασμένα μονογλωσσικά δεδομένα για προσαρμογή τομέα. Επιδεικνύουμε την αποτελεσματικότητα των διαφόρων τεχνικών που εφαρμόσαμε μέσω μελετών αφαίρεσης. Το σύστημα υποβολής μας ξεπερνά την αρχική μας βάση, και την περσινή υποβολή του Πανεπιστημίου του Εδιμβούργου στο IWSLT, κατά περισσότερο από 5 BLEU.', 'hu': 'Ez a tanulmány bemutatja a Samsung Kutatás és Fejlesztés, Varsó, Lengyelország és az Edinburgh-i Egyetem csapata közös benyújtását az IWSLT MT feladatra TED előadásokra. Két fordítási irányban vettünk részt: en-de és de-en. Részt vettünk az en-de és de-en előadásokon SLT feladaton is. A modelleket figyelmes útmérő-dekódoló modellel képeztük, a Nematus BiDeep modelljével. A képzési adatokat a zajos adatok problémájának csökkentése érdekében szűrtük, és visszafordított, egynyelvű adatokat használunk domain-adaptációhoz. Az általunk alkalmazott különböző technikák hatékonyságát ablációs vizsgálatokkal mutatjuk be. A benyújtási rendszerünk több mint 5 BLEU-val felülmúlja alapvető adatainkat, és a tavalyi Edinburgh-i Egyetem IWSLT-re történő benyújtását.', 'it': "Questo articolo descrive la presentazione congiunta di Samsung Research and Development, Varsavia, Polonia e del team dell'Università di Edimburgo al compito IWSLT MT per TED talks. Abbiamo partecipato a due direzioni di traduzione, en-de e de-en. Abbiamo anche partecipato alle lezioni en-de e de-en SLT task. I modelli sono stati addestrati con un modello encoder-decoder attento utilizzando il modello BiDeep di Nematus. Abbiamo filtrato i dati di formazione per ridurre il problema dei dati rumorosi e utilizziamo dati monolingue tradotti indietro per l'adattamento del dominio. Dimostriamo l'efficacia delle diverse tecniche che abbiamo applicato attraverso studi di ablazione. Il nostro sistema di presentazione supera la nostra base di riferimento, e la presentazione dell'anno scorso all'Università di Edimburgo a IWSLT, di più di 5 BLEU.", 'ka': 'ეს დოკუმენტი აღწერს სამსონგონის სწორედ განვითარებას და განვითარებას საერთო გასაღება, ვარსონი, პოლინდი და ვებინდონის სუნივერტური სამუშაო საუბრილობას I ჩვენ დავწყოეთ ორი გადაწყვეტილება, en-de და de-en. ჩვენ ასევე დავწევეთ en-de და de-en ლექციების SLT დავალებაში. მოდელები მოწყობილობულია მოდელზე, რომელიც გამოიყენება BiDeep მოდელს Nematus-ში. ჩვენ ტრიქტრებით მონაცემების მონაცემების პრობლემების შემცირებისთვის, და ჩვენ მონაცემების მონაცემების გამოყენება მონოლენგური მონაცემების მონა ჩვენ აჩვენებთ განსხვავებული ტექნოგიების ეფექტიურობას, რომლებიც ჩვენ აყენებდით აჩვენება აჩვენებული სწავლებებით. ჩვენი წინასწარმატების სისტემა უფრო გავაკეთება ჩვენი ბაზილინი და წინასწარმატების ინვებინდონისტემა IWSLT-ში, უფრო მეტი 5 BLEU-ზე.', 'mk': 'Овој весник го опишува заедничкото поднесување на Samsung Research and Development, Варшава, Полска и тимот на Универзитетот во Единбург на IWSLT MT задачата за разговорите на TED. Ние учествувавме во две насоки на превод, en-de и de-en. Ние, исто така, учествувавме во предавањата en-de и de-en SLT задача. Моделите беа обучени со внимателен модел на кодер-декодер користејќи го моделот BiDeep во Нематус. Ги филтриравме податоците за обука за намалување на проблемот со бучните податоци, и ги користиме монојазичните податоци преведени назад за адаптација на доменот. Демонстрираме ефикасност на различните техники кои ги применивме преку студии за аблација. Нашиот систем на поднесување го надминува нашиот основен резултат, а минатогодишниот универзитет во Единбург поднесувањето на IWSLT за повеќе од 5 БЛЕ.', 'lt': 'Šiame dokumente apibūdinamas bendras Samsung mokslinių tyrimų ir plėtros, Varšuvos, Lenkijos ir Edinburgo universiteto komandos pristatymas į MT užduotį, skirtą TED deryboms. Dalyvavome dviejose vertimo kryptimis: en-de ir de-en. Mes taip pat dalyvavome SLT uždaviniuose „en-de“ ir „de-en“. Modeliai buvo parengti naudojant atidų kodavimo kodavimo model į naudojant BiDeep modelį Nematuse. Mes filtruojome mokymo duomenis, kad sumažintume triukšmingų duomenų problem ą, ir naudojame grįžtamai išverstus monokalbinius duomenis domeno pritaikymui. Mes parodome įvairių metodų, kurie buvo taikomi atliekant abliacijos tyrimus, veiksmingumą. Mūs ų paraiškų teikimo sistema viršija mūsų pradinę ir praėjusių metų Edinburgo universiteto paraišką IWSLT daugiau kaip 5 BLEU.', 'kk': 'Бұл қағаз Самбунг зерттеу және жасау, Варшава, Польша және Эдинбург университетінің TED сөйлемелер үшін IWSLT MT тапсырмасына біріктірілген жұмысын таңдайды. Біз екі аудару бағыттарына, en-de және de-en бөлікті. Сонымен қатар, en-de және de-en лекцияларының SLT тапсырмасына қатынасыз. Бұл үлгілер Нематтың BiDeep үлгісін қолдану үлгісімен қарапайым кодерлеу үлгісімен оқылған. Біз дауыс деректерінің мәселесін азайту үшін оқыту деректерін сүзгіледік, доменге адаптациялау үшін қайта аударылған монолингі деректерді қолданамыз. Біз жұмыс істеу зерттеулері арқылы қолданатын әртүрлі техникалардың ефективнігін көрсетедік. Біздің келтіру жүйесіміз негізгі жолымызды жасайды. Өткен жылдың Эдинбург университеті IWSLT-ге 5 BLEU-дан артық келтірілген.', 'ml': 'ഈ പത്രത്തില്\u200d സാംസംഗ് നിരീക്ഷിക്കുന്നത് വാര്\u200dസോ, പോളണ്ട് യൂണിറ്റിയില്\u200d നിന്നും എഡിന്\u200dബര്\u200dഗ് യൂണിറ്റിയിലേക്കും വിവരിക്കുന്നത് ട നമ്മള്\u200d രണ്ട് പരിഭാഷത്തിന്\u200dറെ ദിശയങ്ങളില്\u200d പങ്കെടുത്തു. ഞങ്ങള്\u200d എന്\u200dഡിയും ഡി എംഎല്\u200dടിയും പ്രഭാഷണങ്ങളില്\u200d പങ്കുചേര്\u200dന്നു. നെമോട്ടിലെ ബൈഡിപ്പ് മോഡല്\u200d ഉപയോഗിച്ച് ശ്രദ്ധിക്കുന്ന ഒരു കോഡെര്\u200d ഡെക്കോഡെര്\u200d മോഡല്\u200d ഉപയോഗിച്ച് മോഡലുകള്\u200d പരി ശബ്ദവിവരങ്ങളുടെ പ്രശ്നം കുറവാക്കാനായി ഞങ്ങള്\u200d പരിശീലിക്കുന്ന പരിശീല ഡേറ്റാ ഫില്\u200dറ്രെയിന്\u200dറ് ചെയ്തിരിക്കുന്ന ഞങ്ങള്\u200d പ്രയോഗിച്ച വ്യത്യസ്ത വിദ്യാഭാഗ്യങ്ങളുടെ ഫലം കാണിച്ചുകൊടുക്കുന്നു. നമ്മുടെ കീഴ്പെടുത്തുന്ന സിസ്റ്റം നമ്മുടെ ബെസ്ലൈന്\u200d പ്രവര്\u200dത്തിപ്പിക്കുന്നു, കഴിഞ്ഞ വര്\u200dഷം എഡിന്\u200dബര്\u200dഗ് യൂണിവേഴ്', 'ms': 'This paper describes the joint submission of Samsung Research and Development, Warsaw, Poland and the University of Edinburgh team to the IWSLT MT task for TED talks.  Kami mengambil bahagian dalam dua arah terjemahan, en-de dan de-en. We also participated in the en-de and de-en lectures SLT task.  Model telah dilatih dengan model pengekod-penyahkod perhatian menggunakan model BiDeep dalam Nematus. Kami menapis data latihan untuk mengurangi masalah data bunyi, dan kami menggunakan data monobahasa terjemahan kembali untuk penyesuaian domain. Kami menunjukkan kegunaan teknik yang berbeza yang kami gunakan melalui kajian ablasi. Sistem penghantaran kami melebihi dasar dasar kami, dan tahun lalu Universiti Edinburgh penghantaran kepada IWSLT, dengan lebih dari 5 BLEU.', 'pl': 'Niniejszy artykuł opisuje wspólne zgłoszenie zespołu Samsung Research and Development z Warszawy i Uniwersytetu Edynburga do zadania IWSLT MT na wykłady TED. Braliśmy udział w dwóch kierunkach tłumaczenia, en-de i de-en. Braliśmy również udział w wykładach en-de i de-en SLT. Modele zostały przeszkolone za pomocą modelu kodera-dekodera uwagi przy użyciu modelu BiDeep w Nematus. Przefiltrowaliśmy dane szkoleniowe, aby zmniejszyć problem hałaśliwych danych, a do adaptacji domeny używamy tylnie tłumaczonych danych jednojęzycznych. Wykazujemy skuteczność różnych technik, które zastosowaliśmy poprzez badania ablacji. Nasz system składania zgłoszeń przewyższa naszą bazę podstawową i ubiegłoroczną zgłoszenie Uniwersytetu Edynburga do IWSLT, o ponad pięć BLEU.', 'mt': 'Dan id-dokument jiddeskrivi s-sottomissjoni konġunta tar-Riċerka u l-Iżvilupp ta’ Samsung, Varsavja, il-Polonja u t-tim tal-Università ta’ Edinburgh lill-kompitu MT tal-IWSLT għat-taħditiet ta’ TED. Ħu sehem f’żewġ direzzjonijiet ta’ traduzzjoni, en-de u de-en. Parteċipajna wkoll fil-ħidma SLT ta’ lezzjonijiet en-de u de-en. Il-mudelli ġew imħarrġa b’mudell attent ta’ kodifikatur-dekoder bl-użu tal-mudell BiDeep f’Nematus. Filtrajna d-dejta tat-taħriġ biex tnaqqas il-problema tad-dejta storbjuża, u nużaw dejta monolingwistika tradotta lura għall-adattament tad-dominju. We demonstrate the effectiveness of the different techniques that we applied via ablation studies.  Is-sistema ta’ sottomissjoni tagħna taqbeż il-linja bażi tagħna, u s-sottomissjoni tal-Università ta’ Edinburgh tas-sena l-oħra lill-IWSLT, b’aktar minn 5 BLEU.', 'mn': 'Энэ цаас Самбургийн судалгаа, хөгжүүлэлт, Варшава, Польша, Эдинбургийн Их Сургуулийн баг TED-ийн ярилцлагын IWSLT MT ажлын нийлбэр тайлбарладаг. Бид хоёр орчуулах замаар, энд болон де-энд оролцсон. Бид мөн en-de болон de-en лекцийн SLT даалгаварт оролцсон. Загвар нь Ниматусын BiDeep загварыг ашиглан ухаантай коддогч загвартай сургалтын загвар өгсөн. Бид чанга өгөгдлийн асуудлыг багасгахад сургалтын өгөгдлийг шинжилгээ хийсэн бөгөөд бид холбоотой адилтгалын тулд буцаад орчуулсан ганц хэлний өгөгдлийг ашиглаж байна. Бид өөр өөр техникуудын үр дүнг үзүүлдэг. Өнгөрсөн жилийн Эдинбургийн Их Сургууль IWSLT-д 5 гаруй BLEU-ээс илүү хүргэж чадна.', 'ro': 'Această lucrare descrie prezentarea comună a echipei Samsung de Cercetare și Dezvoltare, Varșovia, Polonia și Universitatea din Edinburgh la sarcina IWSLT MT pentru discuțiile TED. Am luat parte la două direcții de traducere, en-de și de-en. De asemenea, am participat la prelegerile en-de și de-en SLT. Modelele au fost instruite cu un model de encoder-decoder atențional folosind modelul BiDeep din Nematus. Am filtrat datele de instruire pentru a reduce problema datelor zgomotoase și folosim date monolingve traduse înapoi pentru adaptarea domeniului. Demonstrăm eficacitatea diferitelor tehnici pe care le-am aplicat prin studii de ablație. Sistemul nostru de depunere depășește baza noastră de referință, și depunerea de anul trecut la Universitatea din Edinburgh la IWSLT, cu mai mult de 5 BLEU.', 'no': 'Denne papiret beskriver den samanlige oppføringa av Samsung Research and Development, Warsaw, Poland og University of Edinburgh til IWSLT MT-oppgåva for TED talar. Vi har delt i to oversettelsretningar, en-de og de-en. Vi delta også i en-de og de-en-leksjonen SLT-oppgåva. Modellene er trent med eit oppmerksområde koderingsmodell med BiDeep-modellen i Nematus. Vi filtererte opplæringsdata for å redusera problemet med støydata, og vi bruker tilbakeomsette monospråk-data for domeneadaptasjon. Vi demonstrerer effektiviteten av dei ulike teknikka vi brukte via aktiveringsstudiar. Vårt s økingssystemet utfører vårt baseline, og siste år i Edinburgh-Universiteten søker til IWSLT med fleire enn 5 BLEU.', 'si': 'මේ පත්තේ සැමස්මින්ග් පරීක්ෂණය සහ විකාශය, වාර්ෂෝව, පෝලෑන්ඩ් සහ එන්ඩින්බුර්න් විශ්වාස කණ්ඩායම් IWSLT MT කණ අපි වාර්ථාව දෙකක් තියෙනවා, en-de හා de-en. අපි සමහරවිට en-de සහ de-en ප්\u200dරශ්නයක් SLT වැඩේ සම්බන්ධ වුනා. මොඩේල් එක අවධානයෙන් අවධානයක් තියෙන්නේ නෙමාටුස් වල BiDeep මොඩේල් භාවිත කරන්න. අපි ප්\u200dරශ්නය දත්ත අඩු කරන්න ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් පරික්ෂා කරලා තියෙන්නේ, අපි ප්\u200dරශ්නය කරලා තියෙන්නේ ප අපි ප්\u200dරදර්ශනය කරනවා වෙනස් තාක්ෂණික විදියට අපි ප්\u200dරයෝජනය කරනවා කියලා අපේ පිළිගන්න පද්ධතිය අපේ ප්\u200dරධාන පද්ධතිය නිර්මාණය කරනවා, ගිය අවුරුද්දේ ඉඩින්බිර්ගුන් විශ්වාසිය IWSLT ව', 'so': 'Warsaw, Poland iyo jaamacadda Edinburgh waxaa looga soo dhiibaa shaqada IWSLT MT ee la hadlo TED. Waxaynu qeyb ka dhignay laba hago turjuma, en-de iyo de-en. Waxaynu sidoo kale ka qeybqaadanay shaqada SLT-da iyo de-en. Tusaalada waxaa lagu baray tusaale ahaan codcoder-codcoder oo ku isticmaalaya modelka BiDeep ee Nematus. We filtered the training data to reduce the problem of noisy data, and we use back-translated monolingual data for domain-adaptation.  Waxaynu muujinnaa waxyaabaha ay leedahay xirfadaha kala duduwan ee aan ku dalbannay waxbarashada tababarka. nidaamka soo qabsashada wuxuu ka muujiyaa saldhiggeenna hoose, sanadkii hore jaamacadda Edinburgh wuxuu IWSLT u dhiibaa wax ka badan 5 BLEU.', 'sr': 'Ovaj papir opisuje zajedničku predstavu Samsung istraživanja i razvoja, Varšave, Poljske i Univerzitetske timove Edinburga u zadatku IWSLT MT za TED pregovore. Pridružili smo se u dva smjera prevođenja, en-de i de-en. Takođe smo sudjelovali u zadatku SLT-a na predavanjima en-de i de-en. Modeli su obučeni sa pažnjom modelom kodera-dekodera koristeći BiDeep model u Nematu. Filtrirali smo podatke o obuci da smanjimo problem bukovih podataka, i koristimo ponovno prevedene monojezičke podatke za adaptaciju domena. Pokazujemo učinkovitost različitih tehnika koje smo primjenjivali putem proučavanja ablacije. Naš sistem podnošenja iznosi početnu liniju, a prošlogodišnji Univerzitet Edinburga podnožava IWSLT-u više od 5 BLEU-a.', 'sv': 'Denna uppsats beskriver den gemensamma inlämningen av Samsungs forsknings- och utvecklingsteam, Warszawa, Polen och University of Edinburgh till IWSLT MT-uppgiften för TED-samtal. Vi deltog i två översättningsriktningar, en-de och de-en. Vi deltog även i en-de- och de-en-föreläsningarna SLT-uppgiften. Modellerna har tränats med en uppmärksam encoder-dekoder modell med hjälp av BiDeep modellen i Nematus. Vi filtrerade träningsdata för att minska problemet med bullriga data, och vi använder bakåtöversatta enspråkiga data för domänanpassning. Vi visar effektiviteten hos de olika tekniker som vi tillämpat genom ablationsstudier. Vårt inlämningssystem överträffar vår baslinje, och förra årets University of Edinburgh inlämning till IWSLT, med mer än 5 BLEU.', 'ta': 'இந்த காகிதம் சாம்சாங் ஆராய்ச்சி மற்றும் உருவாக்கம், வார்சா, போலான்டு மற்றும் எடின்பர்க் கல்லூரிக்கழகத்திற்கு IWSLT MT பணியை குற நாங்கள் இரண்டு மொழிபெயர்ப்பு திசைகளில் பகிர்ந்து எடுத்தோம். நாங்கள் ஒர்-டி மற்றும் de-en SLT பணியில் பங்கிட்டோம். Name நாங்கள் பயிற்சி தரவை அலங்காரம் செய்தோம் சப்தத்தின் பிரச்சனையை குறைக்க, மற்றும் நாம் மீண்டும் மொழிமொழிபெயர் We demonstrate the effectiveness of the different techniques that we applied via ablation studies.  எங்கள் கட்டளை அமைப்பு எங்கள் அடிப்படை வரிசையை வெளியேற்றுகிறது, மற்றும் கடந்த வருடத்தின் Edinburgh University IWSLT க்கு, 5 பிலியுக்கு மேற', 'ur': 'This paper describes the joint submission of Samsung Research and Development, Warsaw, Poland and the University of Edinburgh team to the IWSLT MT task for TED talks. ہم نے دو ترجمہ طریقوں میں حصہ لیا، ن-د اور د-ن. ہم نے بھی en-de اور de-en lectures SLT کے کام میں مشارک کیا۔ نمڈلوں کو نیماتوس میں بیڈیپ نمڈل کے مطابق دکھانے والی آنکوڈر ڈیکوڈر مدل کے ساتھ تعلیم دی گئی ہے. ہم نے روشن ڈیٹا کے مسئلہ کو کم کرنے کے لئے تدریس ڈیٹا فیلٹر کیا ہے اور ہم ڈیمین-اڈیٹ کے لئے واپس ترجمہ ایک زبان ڈیٹا استعمال کرتے ہیں۔ ہم ان مختلف ٹیکنیک کے مطابقت کو دکھاتے ہیں جن کو ہم نے آبلیٹ تحقیقات کے ذریعہ استعمال کیا تھا۔ ہماری اطلاعات سیستم ہماری بنسس لین سے زیادہ کام کرتی ہے، اور پہلے سال کے ادینڈینبور کی یونیورڈیس IWSLT کی اطلاعات سے پانچ بلیوس سے زیادہ کام کرتی ہے۔', 'uz': "Бу саҳифа Samsung Research ва Development, Warsaw, Poland va Edinburgh Universitetet TED talablari учун IWSLT MT вазифасига тўпланган қисмни айтиб беради. Biz ikkita tarjima tarjimalarga bir tarjima qilamiz, en-de va de-en. Biz SLT vazifalarini o'rganishga bajardik. Name Tovush maʼlumotini kamaytirish uchun taʼminlovchi maʼlumotni filterlashdik, va biz domen-adaptation uchun qayta tarjima qilingan monolingual maʼlumotdan foydalanamiz. Biz tashkilotni o'rganish orqali boshqa teknologizning effektini ko'rsatdik. Bizning qabul qilish tizimmiz bizning asosiy satrini bajaradi, yetgi yil Edinburg University IWSLT ga 5 dan ortiq BLEU yordamida.", 'vi': 'Tờ giấy này mô tả sự đệ trình chung của Samsung Research and Development, Warsaw, Ba Lan và đội University of Edinburgh vào công việc trao đổi đường sắt IWSLT cho buổi thảo luận Ted. Chúng tôi có tham gia hai hướng dịch, một-de và de-en. Chúng tôi cũng tham gia vào công việc giảng dạy trượt tuyết. Các mẫu đã được huấn luyện với một mô hình Cụ thể mã hóa tập trung, sử dụng BiDeep model ở Nemo. Chúng tôi đã lọc dữ liệu đào tạo để giảm vấn đề về dữ liệu ồn ào, và chúng tôi sử dụng dữ liệu ngôn ngữ riêng để sửa chữa miền. Chúng tôi chứng minh hiệu quả của các kỹ thuật khác nhau mà chúng tôi đã áp dụng qua nghiên cứu rửa. Hệ thống đệ trình của chúng tôi hoàn thiện cơ s ở cơ bản, và năm ngoái Đại học Edinburgh đã đệ trình I.WSLT, với hơn cả 5bíp.', 'bg': 'Настоящата статия описва съвместното представяне на екипа на Самсунг за научни изследвания и развитие, Варшава, Полша и Университета в Единбург на задачата за ТЕД разговори. Участвахме в две направления за превод - ен-де и де-ен. Участвахме и в занятията и занятията. Моделите са обучени с модел на внимателен кодер-декодер, използвайки модела в Нематус. Филтрирахме данните от обучението, за да намалим проблема с шумните данни и използваме назад преведени моноезични данни за адаптация на домейна. Демонстрираме ефективността на различните техники, които прилагаме чрез аблационни проучвания. Нашата система за подаване на заявки превъзхожда базовата ни база, а миналата година Университетът в Единбург подава заявки за IWSLT с повече от 5 БЛЕУ.', 'nl': 'Dit document beschrijft de gezamenlijke indiening van Samsung Research and Development, Warschau, Polen en het team van de Universiteit van Edinburgh aan de IWSLT MT taak voor TED talks. We namen deel aan twee vertaalrichtingen, en-de en. We hebben ook deelgenomen aan de en-de en-en lezingen SLT taak. De modellen zijn getraind met een attentional encoder-decoder model met behulp van het BiDeep model in Nematus. We hebben de trainingsgegevens gefilterd om het probleem van lawaaierige gegevens te verminderen, en we gebruiken back-vertaalde eentalige gegevens voor domeinaanpassing. We demonstreren de effectiviteit van de verschillende technieken die we toepasten via ablatiestudies. Ons indieningssysteem overtreft onze basislijn, en vorig jaar University of Edinburgh indiening aan IWSLT, met meer dan 5 BLEU.', 'da': 'Denne artikel beskriver den fælles indsendelse af Samsung Research and Development, Warszawa, Polen og University of Edinburgh team til IWSLT MT opgave for TED talks. Vi deltog i to oversættelsesretninger, en-de og de-en. Vi deltog også i en-de og de-en foredrag SLT opgaven. Modellerne er uddannet med en opmærksom encoder-dekoder model ved hjælp af BiDeep modellen i Nematus. Vi filtrerede træningsdataene for at reducere problemet med støjende data, og vi bruger tilbage-oversatte ensprogede data til domænetilpasning. Vi demonstrerer effektiviteten af de forskellige teknikker, vi anvendte via ablationsundersøgelser. Vores indsendelsessystem overstiger vores baseline, og sidste års University of Edinburgh indsendelse til IWSLT, med mere end 5 BLEU.', 'de': 'Dieses Papier beschreibt die gemeinsame Einreichung von Samsung Research and Development, Warschau, Polen und dem Team der Universität Edinburgh für die IWSLT MT-Aufgabe für TED-Vorträge. Wir nahmen an zwei Übersetzungsrichtungen teil, en-de und de-en. Wir haben auch an den en-de und de-en Vorlesungen SLT-Aufgabe teilgenommen. Die Modelle wurden mit einem Aufmerksamkeits-Encoder-Decoder-Modell unter Verwendung des BiDeep-Modells in Nematus trainiert. Wir haben die Trainingsdaten gefiltert, um das Problem der verrauschten Daten zu reduzieren, und wir verwenden rückübersetzte einsprachige Daten für die Domain-Anpassung. Wir demonstrieren die Wirksamkeit der verschiedenen Techniken, die wir in Ablationsstudien angewendet haben. Unser Einreichungssystem übertrifft unsere Ausgangsbasis und die letztjährige Einreichung der Universität Edinburgh bei IWSLT um mehr als fünf BLEU.', 'hr': 'Ovaj papir opisuje zajedničku predstavu tima Samsung istraživanja i razvoja, Warsaw, Poljske i Univerziteta Edinburga na zadatak IWSLT MT-a za TED pregovore. Pridružili smo se u dva smjera prevođenja, en-de i de-en. Također smo sudjelovali u zadatku SLT-a na predavanjima en-de i de-en. Modeli su obučeni s uzornim modelom kodera-dekodera koristeći BiDeep model u Nematu. Filtrirali smo podatke o obuci kako bi smanjili problem bučnih podataka, i koristili smo natrag prevedene monojezičke podatke za adaptaciju domena. Pokazujemo učinkovitost različitih tehnika koje smo primjenjivali putem ispitivanja ablacije. Naš sustav podnošenja iznosi početnu liniju, a prošlogodišnji Univerzitet Edinburga podnožava IWSLT-u više od 5 BLEU-a.', 'id': 'Kertas ini menjelaskan pengiriman bersama Samsung Research and Development, Warsaw, Polandia dan tim Universitas Edinburgh untuk tugas MT IWSLT untuk pembicaraan TED. We took part in two translation directions, en-de and de-en.  Kami juga berpartisipasi dalam pelajaran en-de dan de-en SLT tugas. Modelnya telah dilatih dengan model pengekode perhatian menggunakan model BiDeep di Nematus. Kami menginfilter data latihan untuk mengurangi masalah data berisik, dan kami menggunakan data monobahasa terjemahan kembali untuk adaptasi domain. Kami menunjukkan efektivitas dari teknik yang berbeda yang kami gunakan melalui studi ablasi. Sistem pengiriman kami melebihi dasar kami, dan tahun lalu Universitas Edinburgh pengiriman ke IWSLT, dengan lebih dari 5 BLEU.', 'ko': '본고는 삼성연구개발부, 폴란드 바르샤바와 에든버러대학팀이 공동으로 TED 회담에 제출한 IWSLT MT 임무를 묘사한다.우리는 두 가지 번역 방향, 은-덕과 덕-은에 참가했다.우리는 또 은-덕과 덕-은 강좌 SLT 임무에 참가했다.Nematus의 BiDeep 모형을 사용하고 주의 코딩-디코딩 모형을 사용하여 모형을 훈련합니다.우리는 훈련 데이터에 대해 필터를 해서 소음 데이터의 문제를 줄이고 반역 단어 데이터로 역적응을 한다.우리는 융해 연구를 통해 서로 다른 기술의 유효성을 증명하였다.우리의 제출 시스템은 우리의 기선과 작년 에든버러대학이 IWSLT에 제출한 제출 시스템보다 5배 이상 성능이 좋다.', 'fa': 'این کاغذ تعریف مشترک تحقیقات و توسعه سامبینگ، وارشاو، لهستان و دانشگاه ادینبرگ را برای صحبت TED به کار IWSLT MT توسعه می دهد. ما در دو مسیر ترجمه، en-de و de-en شرکت کردیم. ما همچنین در مطالعه\u200cهای "en-de" و "de-en" شرکت کردیم. مدل\u200cها با یک مدل رمزبندی\u200cکننده\u200cی رمزبندی به توجه آموزش داده شده\u200cاند با استفاده از مدل BiDeep در نیماتوس. ما داده\u200cهای آموزش را برای کاهش مشکل داده\u200cهای صوتی فیلتر دادیم، و از داده\u200cهای تکنولوژی عقب برای تغییر دومین استفاده می\u200cکنیم. ما فعالیت تکنیک\u200cهای مختلف را نشان می\u200cدهیم که از طریق مطالعه\u200cهای فعالیت استفاده کردیم. سیستم تسلیم کردن ما از خط پایین ما برتر است، و دانشگاه سال گذشته ادینبورگ به IWSLT تسلیم کردن بیشتر از ۵ BLEU است.', 'sw': 'This paper describes the joint submission of Samsung Research and Development, Warsaw, Poland and the University of Edinburgh team to the IWSLT MT task for TED talks.  Tumeshiriki katika maelekezo mawili ya kutafsiri, en-de na de-en. Pia tulishiriki katika mazungumzo ya en-de na de-en SLT. Mfano huo umefundishwa na modeli ya kudhibiti mfumo wa ufuatiliaji kwa kutumia Mradi wa Bidep huko Nematus. Tulifanya filamu za takwimu za mafunzo ili kupunguza tatizo la taarifa za sauti, na tunatumia taarifa za lugha zilizotafsiriwa kwa mara nyingine kwa ajili ya kuboresha kwa ajili ya maeneo ya ndani. Tunaonyesha ufanisi wa mbinu tofauti ambazo tulitumia kwa njia ya utafiti wa mafuta. Mfumo wetu wa kujitolea unaonyesha msingi wetu, na Chuo Kikuu cha mwaka jana cha Edinburgh kinawasilisha IWSLT, na zaidi ya 5 BLEU.', 'tr': "Bu kagyz Samsung Araştyrymy we Geliştirme, Warsaw, Polşa we Edinburgyň Uniwersiteti IWSLT MT görüşmeleri üçin ýerleşýän zadyny tassyklaýar. Biz iki tercüme yönteminde, en-de ve de-en yönteminde bölük aldık. Biz hem SLT sanatynda en-de we de-en okuwçylary üçin goşuldyk. Modeller Nematus'da BiDeep nusgasyny ulanan göz önüne kodeç modeli bilen eğlentiler. Sesli maglumatyň meselesini azaltmak üçin bilim maglumatyny filtreşdirdik we domenýäplikeýmek üçin yzyna terjime edilen monolingüň maglumatyny ulandyk. Biz ablasyon öwrenmelerinde uygulanan farklı teknikleriň täsirini görkeýäris. Biziň rugsat sistemamyz biziň baselinimizi ýok edip, geçen ýylyň Edimburgyň Uniwersitetimizi IWSLT'a 5-den köp bolan IWSLT'a teslim etýär.", 'sq': 'Ky dokument përshkruan paraqitjen e përbashkët të Samsung Research and Development, Varshavës, Polonisë dhe ekipit të Universitetit të Edinburgut në detyrën e IWSLT MT për bisedimet e TED. Ne morëm pjesë në dy drejtime përkthimi, en-de dhe de-en. Ne gjithashtu morëm pjesë në leksionet e en-de dhe de-en SLT. Modelet janë trajnuar me një model të vëmendshëm kodues-dekoder duke përdorur model in BiDeep në Nematus. Ne filtruam të dhënat e trajnimit për të reduktuar problemin e të dhënave të zhurmshme, dhe përdorim të dhënat monogjuhësore të përkthyera prapa për përshtatjen në domeni. Ne demonstrojmë efektshmërinë e teknikave të ndryshme që kemi aplikuar nëpërmjet studimeve të ablacionit. Sistemi ynë i paraqitjes kalon bazën tonë dhe paraqitjen e vitit të kaluar të Universitetit të Edinburgut në IWSLT, me më shumë se 5 BLEU.', 'af': 'Hierdie papier beskrywe die joint submisiëring van Samsung Research and Development, Warsaw, Poland en die Universiteit van Edimburg by die IWSLT MT-taak vir TED praat. Ons het deel in twee vertaling rigtings geneem, en-de en de-en. Ons het ook gedeel in die en-de en de-en leksies SLT taak. Name Ons het die onderwerp data filtereer om die probleem van geluide data te verminder, en ons gebruik terug-oorselde monolinglike data vir domein-aanpassing. Ons wys die effektiviteit van die verskillende teknike wat ons deur aktivering studies aanwend het. Ons ondersteuning stelsel uitvoer ons basisline, en laaste jaar se Universiteit van Edinburger se ondersteuning aan IWSLT deur meer as 5 BLEU.', 'hy': "This paper describes the joint submission of Samsung Research and Development, Warsaw, Poland and the University of Edinburgh team to the IWSLT MT task for TED talks.  Մենք մասնակցեցինք թարգմանման երկու ուղղությամբ, en-de և de-en: Մենք մասնակցեցինք նաև en-de և de-en դասընթացների Մոդելները վարժեցվել են ուշադիր կոդեր-կոդեր մոդելի միջոցով, օգտագործելով Նեմատոսի «ԲիDeep» մոդելը: Մենք ֆիլտրեցինք ուսուցման տվյալները աղմկոտ տվյալների խնդիրը նվազեցնելու համար, և մենք օգտագործում ենք վերադարձ թարգմանված միալեզու տվյալներ տիեզերական ադապտացիայի համար: We demonstrate the effectiveness of the different techniques that we applied via ablation studies.  Մեր ներկայացման համակարգը գերազանցում է մեր հիմնական արտահայտությունը, և անցյալ տարվա Էդինբուրգի համալսարանի ներկայացումը IwSLT-ին' ավելի քան հինգ ԲԼԵՎ-ով:", 'am': 'ይህ ገጽ ሳምሶን ምርምር እና ግንኙነት፣ Warsaw፣ ፖላንድ እና የኤዲንቡር ዩንቨርስቲ የቴድ ንግግር አካባቢዎች የኢዩአቲ አሜስቴር ስራ ለመጠየቅ ያሳያል፡፡ በሁለት ትርጉም መንገድ አንዳንድ ዲ እና ዲ እና ከ-ዲ እና ከ-የ-ቴን ትምህርት ጋር ተጋሪዎች ነበርን፡፡ ሞዴሎቹ በኔmatus የBiDeep ሞዴል በመጠቀም በአስማማቂነት የኮድድ-ዴድድ model ተማርተዋል፡፡ የድምፅ ድምፅ ዳታዎችን ለማጎድልበት የድምፅ መረጃዎችን አጣይተን ነበር፣ ለዶሜን-አቀማመጥ የተዘጋጀውን የሞሎልጋል ዳታዎችን እንጠቅሳለን፡፡ በጥያቄ ትምህርት የተጠቀምነውን የልዩ ልዩ ልዩና ጥያቄን እናሳያቸዋለን፡፡ የሙግሊታችን ስርዓት መደገፊያውን እና የዓመታቱ የኤዲቡር ዩንቨርስቲ በ5 ቢልዩን የበለጠ IWSLT ሰጥቷል፡፡', 'az': "Bu kağıt, Samsung Araştırma və Geliştirme, Warsaw, Polşa və Edinburger Üniversitesi TED görüşmələri üçün IWSLT MT işinin birlikdə təsdiqlənməsini təsdiqləyir. Biz iki tercümə tərəfində, en-de-en və de-en bölümünü aldıq. Biz də en-de və de-en lektorların SLT işin ə katıldıq. Modellər Nematus'da BiDeep modeli vasitəsilə dikkatli kodlayıcı modeli ilə təhsil edilmişdir. Biz səs verilənlərin problemini azaltmaq üçün təhsil verilənləri filtreşdirdik və domain-adaptasyonu üçün geri çevirilmiş monodil verilənləri istifadə edirik. Biz fərqli tekniklərlə uyguladığımız fərqli təqdirlərin etkinliğini göstəririk. Bizim tətbiq sistemimiz başlangıçımızı üstün edir. Geçən il Edinburg Universitetinin IWSLT-ə 5 BLEU-dən artıq tətbiqi edir.", 'bn': 'এই পত্রিকাটি স্যামসংঙ গবেষণা এবং উন্নয়ন, ওয়ার্সও, পোল্যান্ড এবং এডিনবার্গ বিশ্ববিদ্যালয় টেডি আলোচনার জন্য আইডিওএডএমএলটি এমটি কাজ আমরা দুই অনুবাদের দিকে অংশ নিয়েছিলাম, এন-ডি আর ডি-এন। আমরা এনএলটি কাজে অংশগ্রহণ করেছি। নেমুডাসের বাইডিপ মডেল ব্যবহার করে প্রশিক্ষণ প্রদান করা হয়েছে। আমরা আওয়াজের তথ্য কমানোর জন্য প্রশিক্ষণের তথ্য ফিল্টার করেছি এবং আমরা ডোমেইন-অ্যাডাপেশনের জন্য পুনরাবৃত্ত-অনুবাদের মোনো আমরা বিভিন্ন প্রযুক্তির কার্যক্রম প্রদর্শন করি যা আমরা আগুনের গবেষণার মাধ্যমে প্রয়োগ করেছি। আমাদের আত্মসমর্পণ ব্যবস্থা আমাদের বেসাইলাইনের বিশ্ববিদ্যালয় এবং গত বছরের এডিনবার্গের বিশ্ববিদ্যালয়ের আইডউএসএলটিকে', 'bs': 'Ovaj papir opisuje zajedničku predstavu Samsung istraživanja i razvoja, Varšave, Poljske i Univerzitetske timove Edinburga u zadatku IWSLT MT-a za TED pregovore. Pridružili smo se u dva smjera prevođenja, en-de i de-en. Također smo sudjelovali u zadatku SLT-a predavanja en-de i de-en. Modeli su obučeni s uzornim modelom kodera-dekodera koristeći BiDeep model u Nematu. Filtrirali smo podatke o obuci da smanjimo problem bukovih podataka, i koristimo natrag prevedene monojezičke podatke za adaptaciju domena. Pokazujemo učinkovitost različitih tehnika koje smo primjenjivali putem proučavanja ablacije. Naš sustav podnošenja iznosi našu početnu liniju, a prošlogodišnji Univerzitet Edinburga podnožava IWSLT-u više od 5 BLEU-a.', 'ca': "Aquest article descriu la presentació conjunta de Samsung Research and Development, Varsovia, Polònia i l'equip de la Universitat d'Edimburgu a la tasca MT de l'IWSLT per les conferències TED. Vam participar en dues direccions de traducció, en-de-en i de-en. També vam participar en les conferències SLT. Els models han estat entrenats amb un model atent de codificador amb el model BiDeep de Nematus. Vam filtrar les dades d'entrenament per reduir el problema de les dades sorolloses i vam utilitzar dades monollengües tornades per adaptar-se al domini. We demonstrate the effectiveness of the different techniques that we applied via ablation studies.  El nostre sistema de subministració supera la nostra base de referència, i la subministració de l'any passat a la Universitat d'Edimburgo a l'IWSLT, en més de 5 BLEU.", 'cs': 'Tento článek popisuje společné předložení týmu Samsung Research and Development, Varšava, Polsko a Univerzity v Edinburghu k úkolu IWSLT MT pro TED přednášky. Účastnili jsme se dvou směrů překladu, en-de a de-en. Podíleli jsme se také na en-de a de-en přednáškách SLT úkolu. Modely byly trénovány s pozornostním kodérem-dekodérem modelu pomocí modelu BiDeep v Nematus. Filtrovali jsme tréninková data, abychom snížili problém hlučných dat, a pro adaptaci domény používáme zpětně přeložená jednojjazyčná data. Účinnost různých technik, které jsme aplikovali, demonstrujeme prostřednictvím ablačních studií. Náš systém podávání předčí naši základní hodnotu a loňské univerzity v Edinburghu předložení IWSLT o více než pět BLEU.', 'et': 'Käesolevas artiklis kirjeldatakse Samsungi teadus- ja arendustegevuse, Varssavi, Poola ja Edinburghi Ülikooli meeskonna ühist esitamist IWSLT MT ülesandele TED kõnelusteks. Osalesime kahes tõlkesuunas, en-de ja de-en. Osalesime ka en-de ja de-en loengutes SLT ülesandes. Mudelid on treenitud tähelepanukooder-dekooder mudeliga, kasutades BiDeep mudelit Nematuses. Filtreerisime koolitusandmeid, et vähendada mürakate andmete probleemi ning kasutame domeeni kohandamiseks tagasitõlgitud ühekeelseid andmeid. Näitame ablatsiooniuuringute kaudu rakendatud erinevate tehnikate efektiivsust. Meie esitamissüsteem ületab meie baasi ja eelmise aasta Edinburghi Ülikooli esitamise IWSLT rohkem kui 5 BLEU.', 'fi': 'Tämä artikkeli kuvaa Samsungin tutkimus- ja kehitystyön, Varsovan, Puolan ja Edinburghin yliopiston tiimin yhteistä toimittamista IWSLT MT -tehtävään TED-keskusteluissa. Osallistuimme kahteen käännössuuntaan, en-de ja de-en. Osallistuimme myös en-de- ja de-en luentojen SLT-tehtävään. Mallit on koulutettu huomiokooderi-dekooderimallilla käyttäen BiDeep-mallia Nematuksessa. Suodatimme harjoitustiedot meluisan datan vähentämiseksi ja käytämme käännettyä monikielistä dataa domain-sopeutumiseen. Osoitamme ablaatiotutkimusten avulla käyttämiemme eri tekniikoiden tehokkuuden. Lähettämisjärjestelmämme ylittää perusaikamme ja viime vuoden Edinburghin yliopiston IWSLT-lähetyksen yli 5 BLEU:lla.', 'jv': 'Perintah iki rambarang nggawe ngupakan bakal ning sampek urip lan nggawe Panjenengan Tambudhakan karo Rasané Panjenengan, Warsawi, Pulan lan karo Ngombi Universite di Singpèratan kanggo ngilanggar IWSLT MT kanggo ngilanggar Tom Awak dhéwé wis pating durung tarjamahan, en-de lan de-en. Awak dhéwé éntuk kiper karo langgambar en-de lan de-en nggawe SLT cara nggawe model dumaten ono dumaten karo model an Attential We filed the tutorial data to cut the question of sound data, and we use back-translation Monlanguage data for domain-modification. Awak dhéwé éntuk efekat kanggo ngerasakno teknik sing wis dipulangan nyelarane Sistem sing beraksi maneh sing nggawe barang-barang kanggo awak dhéwé, lan neng Universite sing dumadhakan kanggo IWSLT, sing sembarang 5 CLUE.', 'ha': "Wannan karatun na describe the Jordanic Submit of Samsung Research and Development, Warsen, Poland and and University of edinburg to the IWSLT MT job for TeD talks. Mun sami shirin fassarar biyu, en-de da-en. Tuna samu sami zuwa ga aikin SLT. An sanar da shiryoyin ayuka da wata motel mai taƙaita kode-koda da da aka yi amfani da misalin BiDeDepe cikin Nematus. Mun filter data na ƙidãya dõmin mu ƙara matsalar data na saurari, kuma munã amfani da data na fassarwa da aka sake-fassar na-nau'in-sauri. Tuna nũna aikin mafiya-akati masu turbuɗe da muka samun mutane da aka samu karatun karatun. Shirin sallama na ƙara batar da bazalinmu, kuma a shekarar ta shekarar da aka shigar da IWSLT, na ƙari 5 BLEU.", 'he': "העיתון הזה מתאר את ההצגה המשותפת של מחקר ופיתוח סמסונג, וורשה, פולין וצוות האוניברסיטת אדינבורג למשימת MT IWSLT לשיחות TED. השתתפנו בשני כיוונים התרגום, en-de ו de-en. השתתפנו גם בהרצאות ב-en-de-en וה-en SLT משימה. הדוגמנים הוכשרו עם דוגמנית קודור-קידור תשומת לב בשימוש בדוגמנית BiDeep בנמטוס. סיננו את נתוני האימונים כדי להפחית את הבעיה של נתונים רעשים, ואנחנו משתמשים בנתונים מונושפתיים מתרגמים בחזרה אנחנו מראים את היעילות של הטכניקות השונים שהשתמשנו באמצעות מחקרי האבלציה. Our submission system outperforms our baseline, and last year's University of Edinburgh submission to IWSLT, by more than 5 BLEU.", 'sk': 'Ta prispevek opisuje skupno predložitev Samsung Research and Development, Varšava, Poljska in ekipe Univerze v Edinburgu na nalogo IWSLT MT za TED pogovore. Sodelovali smo v dveh prevajalskih smereh, en-de in de-en. Sodelovali smo tudi pri en-de in de-en predavanjih SLT. Modeli so bili usposobljeni z uporabo modela pozornosti kodirnika-dekodirnika z uporabo modela BiDeep v Nematusu. Podatke o usposabljanju smo filtrirali, da bi zmanjšali problem hrupnih podatkov, za prilagajanje domene pa uporabljamo nazaj prevedene enojezične podatke. Prikazujemo učinkovitost različnih tehnik, ki smo jih uporabili preko študij ablacije. Naš sistem za oddajo predložitev presega našo osnovno izhodišče, lanskoletno pa za več kot 5 BLEU predložitev Univerze v Edinburghu.', 'bo': 'ཤོག ང་ཚོས་ཚོའི་སྐད་ཡིག་གཟུགས་ཕྱོགས་གཉིས་ཀྱི་ནང་དུ་བླངས་པ་ཡིན།en-de དང་de-en། ང་ཚོས་SLT སློབ་ཚུལ་གྱི་དོན་ལས་(en-de)དང་de-en(de-en)ནང་དུ་ཞུགས་བྱས་པ་ཡིན། Models have been trained with an attentional encoder-decoder model using the BiDeep model in Nematus. We filtered the training data to reduce the problem of noisy data, and we use back-translated monolingual data for domain-adaptation. ང་ཚོས་ཤེས་ལྡན་བཟོ་བྱེད་པའི་བཟོ་རྩོལ་བ་དག་གི་ནུས་པ་སྟོན་ཡོད། ང་ཚོའི་འཆར་བཀོད་གྱི་མ་ལག་གིས་ང་ཚོའི་རིམ་གཞི་རྩལ་བ་སྒྱུར་མེད། འདས་པའི་ལོ་ངོས་ཐོག་གི་ཆ་ཕྲན་ལ་ཞིག་གི་སློབ་'}
{'en': 'The RWTH Aachen Machine Translation Systems for IWSLT 2017 RWTH   A achen Machine Translation Systems for  IWSLT  2017', 'fr': 'Les systèmes de traduction automatique RWTH Aix-la-Chapelle pour IWSLT 2017', 'pt': 'Os sistemas de tradução automática RWTH Aachen para IWSLT 2017', 'ar': 'أنظمة الترجمة الآلية RWTH Aachen لـ IWSLT 2017', 'es': 'Los sistemas de traducción automática de RWTH Aachen para IWSLT 2017', 'ja': 'IWSLT 2017のためのRWTH Aachen機械翻訳システム', 'zh': '亚琛工业大学2017年亚琛机器翻译统展览会', 'hi': 'IWSLT 2017 के लिए RWTH Aachen मशीन अनुवाद प्रणाली', 'ru': 'Системы машинного перевода RWTH Aachen для IWSLT 2017', 'ga': 'Córais Aistriúcháin Meaisín RWTH Aachen do IWSLT 2017', 'ka': 'RWTH Aachen Machine Translation Systems for IWSLT 2017', 'el': 'Τα συστήματα μηχανικής μετάφρασης της RWTH Aachen για το IWSLT 2017', 'hu': 'Az RWTH Aachen gépi fordító rendszerek az IWSLT 2017 számára', 'kk': 'IWSLT 2017 үшін RWTH Aachen Machine Translation Systems', 'it': 'I sistemi di traduzione automatica RWTH Aachen per IWSLT 2017', 'lt': '2017 m. IWSLT RWTH Aachen mašinų vertimo sistemos', 'mk': 'РWTH Aachen Machine Translation Systems за IWSLT 2017', 'ml': 'IWSLT 2017-നുള്ള RWTH Aachen Mashine Translation Systems', 'ms': 'Sistem Terjemahan Mesin RWTH Aachen untuk IWSLT 2017', 'mt': 'The RWTH Aachen Machine Translation Systems for IWSLT 2017', 'pl': 'Systemy tłumaczeń maszynowych RWTH Aachen dla IWSLT 2017', 'no': 'RWTH Aachen Machine Translation Systems for IWSLT 2017', 'ro': 'RWTH Aachen Machine Translation Systems pentru IWSLT 2017', 'mn': 'IWSLT 2017 оны RWTH Aachen Machine Translation Systems', 'sr': 'RWTH Aachen Machine Translation Systems for IWSLT 2017', 'sv': 'RWTH Aachen maskinĂ¶versĂ¤ttningssystem fĂ¶r IWSLT 2017', 'ta': 'RWTH Aachen Machine Translation Systems for IWSLT 2017', 'ur': 'IWSLT 2017 کے لئے RWTH Aachen Machine Translation Systems', 'si': 'IWSLT 2017 වෙනුවෙන් RWTH Aaching machine translation systems', 'so': 'RWTH Aachen Machine Translation Systems for IWSLT 2017', 'uz': 'Comment', 'vi': 'The RWTH Aachen Machine Translation Systems for IWSLT bây giờ 7.', 'nl': 'De machinevertaalsystemen van de RWTH Aachen voor IWSLT 2017', 'bg': 'Системите за машинен превод в Аахен за 2017 г.', 'hr': 'RWTH Aachen Machine Translation Systems for IWSLT 2017', 'da': 'RWTH Aachen maskinoversættelsessystemer til IWSLT 2017', 'id': 'RWTH Aachen Machine Translation Systems untuk IWSLT 2017', 'ko': '2017년 IWSLT의 RWTH 아분 기계 번역 시스템', 'de': 'Die maschinellen Übersetzungssysteme der RWTH Aachen für IWSLT 2017', 'sw': 'Mfumo wa Tafsiri wa Mashine ya RWTH Aachen kwa ajili ya IWSLT 2017', 'af': 'Die RWTH Aachen Masjien Vertaling Systeme vir IWSLT 2017', 'tr': 'IWSLT 2017 üçin IWTH Aachen Maşynyň terjime sistemleri', 'fa': 'سیستم ترجمه ماشین RWTH Aachen برای IWSLT 2017', 'sq': 'RWTH Aachen Machine Translation Systems for IWSLT 2017', 'am': 'RWTH Aachen machine translation systems for IWSLT 2017', 'hy': '2017-ի IW-ի Ախենի մեքենայի թարգմանման համակարգերը', 'az': 'RWTH Aachen Machine Translation Systems for IWSLT 2017', 'bn': 'The RWTH Aachen Machine Translation Systems for IWSLT 2017', 'bs': 'RWTH Aachen Machine Translation Systems for IWSLT 2017', 'ca': 'The RWTH Aachen Machine Translation Systems for IWSLT 2017', 'cs': 'Strojové překladatelské systémy RWTH Aachen pro IWSLT 2017', 'et': 'RWTH Aachen masintõlke süsteemid IWSLT 2017 jaoks', 'fi': 'RWTH Aachen Machine Translation Systems IWSLT 2017: lle', 'jv': 'RWtho Aaken Mahine Terusan Sistem kanggo IWSLT 1997', 'ha': 'KCharselect unicode block name', 'he': 'מערכות התרגום מכונות RWTH Aachen עבור IWSLT 2017', 'sk': 'Sistemi strojnega prevajanja RWTH Aachen za IWSLT 2017', 'bo': 'RWTH Aachen Machine Translation Systems for IWSLT 2017'}
{'en': 'This work describes the Neural Machine Translation (NMT) system of the RWTH Aachen University developed for the English$German tracks of the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2017. We use NMT systems which are augmented by state-of-the-art  extensions . Furthermore, we experiment with techniques that include data filtering, a larger vocabulary, two extensions to the  attention mechanism  and  domain adaptation . Using these methods, we can show considerable improvements over the respective baseline systems and our IWSLT 2016 submission.', 'ar': 'يصف هذا العمل نظام الترجمة الآلية العصبية (NMT) التابع لجامعة RWTH Aachen الذي تم تطويره للمسارات الإنجليزية $ الألمانية لحملة التقييم الخاصة بورشة العمل الدولية حول ترجمة اللغة المنطوقة (IWSLT) 2017. نحن نستخدم أنظمة NMT التي يتم تعزيزها من قبل الدولة- ملحقات من الفن. علاوة على ذلك ، نجرب التقنيات التي تشمل تصفية البيانات ، ومفردات أكبر ، وامتدادان لآلية الانتباه وتكييف المجال. باستخدام هذه الأساليب ، يمكننا إظهار تحسينات كبيرة على أنظمة خط الأساس المعنية وتقديم IWSLT 2016 الخاص بنا.', 'fr': "Ce travail décrit le système de traduction automatique neuronale (NMT) de l'Université RWTH d'Aix-la-Chapelle développé pour les pistes anglais$allemand de la campagne d'évaluation de l'International Workshop on Spoken Language Translation (IWSLT) 2017. Nous utilisons des systèmes NMT qui sont complétés par des extensions de pointe. De plus, nous expérimentons des techniques qui incluent le filtrage des données, un vocabulaire plus large, deux extensions du mécanisme de l'attention et l'adaptation du domaine. En utilisant ces méthodes, nous pouvons montrer des améliorations considérables par rapport aux systèmes de base respectifs et à notre soumission IWSLT 2016.", 'es': 'Este trabajo describe el sistema Neural Machine Translation (NMT) de la Universidad RWTH de Aquisgrán desarrollado para las secciones inglés/alemán de la campaña de evaluación del International Workshop on Spoken Language Translation (IWSLT) 2017. Utilizamos sistemas NMT que se complementan con extensiones de última generación. Además, experimentamos con técnicas que incluyen el filtrado de datos, un vocabulario más amplio, dos extensiones del mecanismo de atención y la adaptación de dominios. Con estos métodos, podemos mostrar mejoras considerables con respecto a los respectivos sistemas de referencia y nuestra presentación de IWSLT 2016.', 'pt': 'Este trabalho descreve o sistema Neural Machine Translation (NMT) da RWTH Aachen University desenvolvido para as trilhas inglês-alemão da campanha de avaliação do International Workshop on Spoken Language Translation (IWSLT) 2017. Usamos sistemas NMT que são aumentados por estados- extensões de última geração. Além disso, experimentamos técnicas que incluem filtragem de dados, vocabulário maior, duas extensões ao mecanismo de atenção e adaptação de domínio. Usando esses métodos, podemos mostrar melhorias consideráveis em relação aos respectivos sistemas de linha de base e nossa submissão ao IWSLT 2016.', 'ja': 'この研究では、2017年の口語翻訳国際ワークショップ（ IWSLT ）の評価キャンペーンの英語$ドイツ語トラックのために開発されたRWTHアーヘン大学のニューラル・マシン・トランスレーション（ NMT ）システムについて説明します。最先端の拡張機能で拡張されたNMTシステムを使用しています。さらに、データフィルタリング、より大きなボキャブラリー、注意メカニズムとドメイン適応の2つの拡張を含む技術を実験しました。これらの方法を使用すると、それぞれのベースラインシステムとIWSLT 2016の提出物に対してかなりの改善を示すことができます。', 'zh': '其言亚琛工大学之神经机器翻译(NMT)统,当为2017年国际口语译研讨会(IWSLT)估英语德语轨道开发也。 我用NMT统,先进益强。 试数漉,大词汇量,注意二广,应于内者也。 用此者,各以其基线之统,与IWSLT 2016交之大者也。', 'ru': 'В данной работе описывается система нейронного машинного перевода (NMT) Аахенского университета RWTH, разработанная для английских$ немецких треков оценочной кампании Международного семинара по устному переводу (IWSLT) 2017 года. Мы используем системы NMT, которые дополняются новейшими расширениями. Кроме того, мы экспериментируем с методами, которые включают фильтрацию данных, больший словарный запас, два расширения механизма внимания и адаптацию домена. Используя эти методы, мы можем продемонстрировать значительные улучшения по сравнению с соответствующими базовыми системами и нашим представлением IWSLT 2016.', 'hi': 'यह काम RWTH Aachen University की तंत्रिका मशीन अनुवाद (NMT) प्रणाली का वर्णन करता है जो बोली जाने वाली भाषा अनुवाद (IWSLT) 2017 पर अंतर्राष्ट्रीय कार्यशाला के मूल्यांकन अभियान के अंग्रेजी $ जर्मन पटरियों के लिए विकसित किया गया है। हम एनएमटी प्रणालियों का उपयोग करते हैं जो अत्याधुनिक एक्सटेंशन द्वारा संवर्धित होते हैं। इसके अलावा, हम उन तकनीकों के साथ प्रयोग करते हैं जिनमें डेटा फ़िल्टरिंग, एक बड़ी शब्दावली, ध्यान तंत्र और डोमेन अनुकूलन के लिए दो एक्सटेंशन शामिल हैं। इन तरीकों का उपयोग करके, हम संबंधित बेसलाइन सिस्टम और हमारे IWSLT 2016 सबमिशन पर काफी सुधार दिखा सकते हैं।', 'ga': 'Déanann an obair seo cur síos ar an gcóras Néar-Aistriúcháin Meaisín (NMT) de chuid Ollscoil RWTH Aachen a forbraíodh le haghaidh rianta Béarla-Gearmáinis d’fheachtas meastóireachta na Ceardlainne Idirnáisiúnta ar Aistriúchán Teanga Labhartha (IWSLT) 2017. Bainimid úsáid as córais NMT atá méadaithe ag an stát-. síntí den scoth. Ina theannta sin, déanaimid tástáil le teicnící a chuimsíonn scagadh sonraí, stór focal níos mó, dhá shíneadh ar an meicníocht aird agus oiriúnú fearainn. Agus na modhanna seo á n-úsáid againn, is féidir linn feabhsuithe suntasacha a léiriú ar na córais bhunlíne faoi seach agus ar ár n-aighneacht IWSLT 2016.', 'hu': 'Ez a munka bemutatja az RWTH Aachen Egyetem Neural Machine Translation (NMT) rendszerét, amelyet a 2017. évi International Workshop on Beszélt Nyelvi Fordítás (IWSLT) értékelő kampányának angol$német nyelvű pályáira fejlesztettek ki. NMT rendszereket használunk, amelyeket a legkorszerűbb kiterjesztésekkel bővítettünk. Továbbá olyan technikákkal kísérletezünk, amelyek magukban foglalják az adatszűrést, a nagyobb szókincset, a figyelem mechanizmus két kiterjesztését és a domain adaptációt. Ezekkel a módszerekkel jelentős javulást tudunk mutatni a vonatkozó alapvető rendszerekhez és az IWSLT 2016-os benyújtásunkhoz képest.', 'ka': 'ეს სამუშაო აღნივერტის ნეიროლური მაქინის განსაგულისხმების სისტემის განსაგულისხმება, რომელიც განვითარებულია 2017 წლის სამუშაო ენის განსაგულისხმების (IWSLT) ინგლისური$გერმანური მონაცემებ ჩვენ გამოყენებთ NMT სისტემები, რომლებიც სტატის გაფარდებით. დამატებით, ჩვენ ექსპერიმენტირებთ ტექნექციებით, რომლებიც მონაცემების ფილტრირება, დიდი სიტყვებულია, ორი გაფართლება მონაცემების მექანიზმის და დე ჩვენ შეგვიძლია გამოიყენოთ ეს მეტი, რომ ჩვენ შეგვიძლია აჩვენოთ მნიშვნელოვანი შესაძლებლობების შესაძლებლობების შესაძლებლობების შესაძლებლობების შესაძლებლობ', 'it': "Questo lavoro descrive il sistema Neural Machine Translation (NMT) della RWTH Aachen University sviluppato per le tracce English$German della campagna di valutazione del Workshop internazionale sulla traduzione linguistica parlata (IWSLT) 2017. Utilizziamo sistemi NMT che sono arricchiti da estensioni all'avanguardia. Inoltre, sperimentiamo tecniche che includono il filtraggio dei dati, un vocabolario più ampio, due estensioni al meccanismo di attenzione e adattamento del dominio. Utilizzando questi metodi, possiamo mostrare notevoli miglioramenti rispetto ai rispettivi sistemi di base e alla nostra presentazione IWSLT 2016.", 'lt': 'Šiame darbe aprašoma RWTH Aacheno universiteto „Neural Machine Translation“ (NMT) sistema, sukurta anglų dolerių vokiečių kalbos vertimo raštu (angl. Naudojame NMT sistemas, kurios yra papildytos naujausiais išplėtimais. Be to, eksperimentuojame su metodais, kurie apima duomenų filtravimą, didesnį žodyną, du dėmesio mechanizmo išplėtimus ir srities pritaikymą. Taikant šiuos metodus galime parodyti gerokai patobulinimus, palyginti su atitinkamomis bazinėmis sistemomis ir mūsų IWSLT 2016 pateikimu.', 'kk': 'Бұл жұмыс 2017 жылы Спокен тіл аудармасындағы халықаралық жұмыс істеу кампаниясының ағылшынша$Неміс тілдерінде жасалған RWTH Ахен университетінің нейралық машинаның аудармасының (NMT) жүйесін анықтайды Біз NMT жүйелерін қолданамыз, олар күй- жай кеңейтулерімен көтерілген. Қосымша, біз деректерді сүзгілеу, үлкен сөз сөздері, қарау механизмінің екі кеңейтуі мен доменнің адаптациясының техникаларымен тәжірибелеміз. Бұл әдістерді қолдану үшін, 2016 жылдағы IWSLT жүйелеріміздің негізгі жүйелеріміздің және жұмыс істеуіміздің көп жақсартылығын көрсетеді.', 'mk': 'Оваа работа го опишува системот на Неурална машина за превод (НМТ) на Универзитетот РВТ Ачен развиен за англиските песни од германската кампања за евалуација на Меѓународниот работилник за разговорен јазик превод (IWSLT) 2017 година. Ние користиме НМТ системи кои се зголемени со најсовремени проширувања. Покрај тоа, експериментираме со техники кои вклучуваат филтрирање на податоци, поголем речник, две проширувања на механизмот на внимание и адаптација на доменот. Користејќи ги овие методи, можеме да покажеме значителни подобрувања во однос на соодветните основни системи и нашето поднесување на IWSLT 2016.', 'ms': 'Kerja ini menggambarkan sistem Perjemahan Mesin Neural (NMT) Universiti RWTH Aachen dikembangkan untuk trek Inggeris$Jerman kempen penilaian Workshop International on Spoken Language Translation (IWSLT) 2017. Kami menggunakan sistem NMT yang ditambah oleh sambungan state-of-the-art. Selain itu, kami eksperimen dengan teknik yang termasuk penapisan data, vokbulari yang lebih besar, dua sambungan ke mekanisme perhatian dan penyesuaian domain. Using these methods, we can show considerable improvements over the respective baseline systems and our IWSLT 2016 submission.', 'ml': 'ഈ പ്രവര്\u200dത്തിക്കുന്നത് RWTH ആഷെന്\u200d യൂണിവേഴ്സിറ്റിയുടെ നെയുറല്\u200d മെഷീന്\u200d പരിഭാഷ (NMT) സിസ്റ്റം വിവരിച്ചുകൊടുക്കുന്നു. ഇംഗ്ലീഷ് ഡോളര്\u200d ജര്\u200dമ്മന്\u200d ഡോളര്\u200d പ നമ്മള്\u200d NMT സിസ്റ്റമുകള്\u200d ഉപയോഗിക്കുന്നു. അവസ്ഥ-ഓഫ് ആര്\u200dട്ട് വികസിപ്പിക്കുന്നു. അതിനുശേഷം, ഡേറ്റാ ഫില്\u200dറ്റര്\u200d ചെയ്യുന്ന സാങ്കേതികവിദ്യകളുമായി നമ്മള്\u200d പരീക്ഷിക്കുന്നുണ്ട്, വലിയ വാക്കുകള്\u200d, രണ്ട് വികസ ഈ രീതികള്\u200d ഉപയോഗിക്കുന്നത് നമുക്ക് വിശിഷ്ടമായ ബെസ്ലൈന്\u200d സിസ്റ്റത്തിനെക്കുറിച്ച് വളരെ മെച്ചപ്പെടുത്താന', 'el': 'Η παρούσα εργασία περιγράφει το σύστημα Νευρικής Μηχανικής Μετάφρασης (NMT) του Πανεπιστημίου Aachen που αναπτύχθηκε για τα Αγγλικά-Γερμανικά κομμάτια της καμπάνιας αξιολόγησης του Διεθνούς Εργαστηρίου για τη Μετάφραση Μιλημένης Γλώσσας (IWSLT) 2017. Χρησιμοποιούμε συστήματα τα οποία συμπληρώνονται από υπερσύγχρονες επεκτάσεις. Επιπλέον, πειραματιζόμαστε με τεχνικές που περιλαμβάνουν φιλτράρισμα δεδομένων, μεγαλύτερο λεξιλόγιο, δύο επεκτάσεις στον μηχανισμό προσοχής και προσαρμογή τομέα. Χρησιμοποιώντας αυτές τις μεθόδους, μπορούμε να δείξουμε σημαντικές βελτιώσεις σε σχέση με τα αντίστοιχα συστήματα βάσης και την υποβολή μας.', 'mt': 'Din il-ħidma tiddeskrivi s-sistema tat-Traduzzjoni tal-Magni Newrali (NMT) tal-Università RWTH Aachen żviluppata għall-binarji Ingliż$Ġermaniżi tal-kampanja ta’ evalwazzjoni tal-Workshop Internazzjonali dwar it-Traduzzjoni tal-Lingwa Kkellma (IWSLT) 2017. Aħna nużaw sistemi NMT li huma miżjuda minn estensjonijiet l-aktar avvanzati. Barra minn hekk, a ħna ninsperimentaw b’tekniki li jinkludu l-filtrazzjoni tad-dejta, vokabulari akbar, żewġ estensjonijiet għall-mekkaniżmu ta’ attenzjoni u l-adattament tad-dominju. Permezz ta’ dawn il-metodi, nistgħu nuru titjib konsiderevoli fuq is-sistemi ta’ bażi rispettivi u s-sottomissjoni tagħna tal-IWSLT 2016.', 'pl': 'W niniejszej pracy opisano system neuronowego tłumaczenia maszynowego (NMT) Uniwersytetu RWTH Aachen opracowany dla utworów angielsko-niemieckiej kampanii ewaluacyjnej Międzynarodowego Warsztatu Tłumaczenia Języka Mówionego (IWSLT) 2017. Korzystamy z systemów NMT, które są rozszerzone o najnowocześniejsze rozszerzenia. Ponadto eksperymentujemy z technikami, które obejmują filtrowanie danych, większe słownictwo, dwa rozszerzenia mechanizmu uwagi oraz adaptację domeny. Korzystając z tych metod, możemy wykazać znaczne ulepszenia w stosunku do odpowiednich systemów bazowych i naszego wniosku IWSLT 2016.', 'mn': 'Энэ ажил 2017 оны РWTH Ахен Их Сургуулийн мэдрэлийн машины хөгжүүлэлтийн системийг Германы долларын үнэлгээний кампанийг дэвшүүлсэн. Бид NMT системийг ашиглаж, урлагийн төвшинд нэмэгдүүлсэн. Мөн бид өгөгдлийн сүзүүлэлт, том үг, анхаарлын механизм болон зохион байгуулалттай хоёр нэмэгдүүлэлт хийдэг техникуудыг туршиж үзнэ. Эдгээр арга замыг ашиглан бид үндсэн систем болон 2016 оны IWSLT-ийн сургалтын талаар маш их сайжруулах боломжтой болно.', 'no': 'Dette arbeidet beskriver systemet for Neural Machine Translation (NMT) for RWTH Aachen University utvikla for engelsk$tysk spor av evalueringskampanjen for den internasjonale arbeidsområdet på Spoken Language Translation (IWSLT) 2017. Vi bruker NMT-systemer som er augmentert av tilstanden av kunstutvidingar. I tillegg eksperimenterer vi med teknikk som inkluderer filtrering av data, ein større ordbok, to utvidingar til oppmerksmekanismen og domeneadaptasjonen. Bruk desse metodane kan vi visa mykje forbetringar over dei tilhøyrande grunnlinjesystemene og våre IWSLT 2016-tillegg.', 'ro': 'Această lucrare descrie sistemul Neural Machine Translation (NMT) al Universității RWTH Aachen dezvoltat pentru urmele English$German ale campaniei de evaluare a Atelierului Internațional de Traducere a Limbilor Vorbite (IWSLT) 2017. Folosim sisteme NMT care sunt amplificate de extensii de ultimă generație. Mai mult, experimentăm tehnici care includ filtrarea datelor, un vocabular mai larg, două extensii ale mecanismului de atenție și adaptarea domeniului. Folosind aceste metode, putem arăta îmbunătățiri considerabile față de sistemele de referință respective și prezentarea noastră IWSLT 2016.', 'sr': 'Ovaj rad opisuje sistem Neuralnog prevoda mašine (NMT) Univerziteta RWTH Aachen razvijen za njemačke tragove engleskog dolara za procjenu kampanje Međunarodnog radionica o prevodu jezika spomenika (IWSLT) 2017. Koristimo NMT sisteme koje se povećavaju državnim proširenjem umjetnosti. Osim toga, eksperimentiramo sa tehnikama koja uključuju filtriranje podataka, veći rečnik, dve proširenje pažnje na mehanizam i adaptaciju domena. Koristeći te metode, možemo pokazati značajne poboljšanje u odnosu na odgovarajuće početne sisteme i naše podnošenje IWSLT 2016.', 'si': 'මේ වැඩේ කියන්නේ RWTH Aachන් විශ්වාසයේ න්\u200dයූරාල මැෂින් අවවාදය (NMT) පද්ධතියේ ඉංග්\u200dරීසිය $ජර්මාන් ප්\u200dරකාශයේ අන්තර්ජාත්\u200dය වැඩසටහ අපි NMT පද්ධතිය පාවිච්චි කරනවා ඒ වගේම ස්ථානයේ ක්\u200dරියාත්මක විස්තරයෙන් වැඩි කරනවා. තවත්, අපි පරීක්ෂණය කරන්නේ දත්ත පරීක්ෂණය සමග දත්ත පරීක්ෂණය, ලොකු භාෂාවක්, අවස්ථාවක් දෙකක් අවස්ථාවක් ස මේ විධානය භාවිතා කරන්න, අපිට පුළුවන් පුළුවන් විශේෂ පද්ධතිය සහ අපේ IWSLT 2016 පිළිගන්න.', 'sv': 'Detta arbete beskriver det Neural Machine Translation (NMT) system vid RWTH Aachen University utvecklat för de engelska$tyska spåren av utvärderingskampanjen för International Workshop on Spoken Language Translation (IWSLT) 2017. Vi använder NMT-system som förstärks med toppmoderna tillägg. Dessutom experimenterar vi med tekniker som inkluderar datafiltrering, ett större ordförråd, två tillägg till uppmärksamhetsmekanismen och domänanpassning. Med hjälp av dessa metoder kan vi visa betydande förbättringar jämfört med respektive baslinje system och vår IWSLT 2016 inlämning.', 'ta': 'இந்த வேலை RWTH ஆசென் கல்வியூரிக்காக உருவாக்கப்பட்ட நெருக்கர் இயந்திர மொழிமொழிபெயர்ப்பின் (IWSLT) அமைப்பை குறிப்பிடுகிறது. பேசும் மொழிமொழிமொழிபெயர்ப் நாம் NMT அமைப்புகளை பயன்படுத்துகிறோம் அது மாநிலை - கலை விரிவாக்கம் மூலம் அதிகரிக்கப்படுகிறது. மேலும், தகவல் வடிகட்டி, பெரிய சொல்வளம், இரண்டு விரிவாக்கம் கவனத்தின் முறைமையை மற்றும் களத்தின் ஒதுக்குப்பொருளை சேர்க்கும்  இந்த முறைமைகளை பயன்படுத்தி, நாம் சிறப்பான முன்னேற்றங்களை காட்ட முடியும் அடிப்படை முறைமைகள் மற்றும் எங்கள் IWSL', 'ur': 'اس کام نے RWTH Aachen University کے نیورال ماشین ترجمہ (NMT) سیستم کی توصیف کی ہے جو 2017 Spoken Language Translation (IWSLT) میں انگلیسی ڈالر ڈالر کے جرمن ٹریک کے ارزیابی کمپینٹ کی ارزیابی کمپینٹ کے لئے توصیف کی ہے. ہم NMT سیستموں کو استعمال کرتے ہیں جن کو ایٹیٹ کے اضافہ سے زیادہ کیا جاتا ہے۔ اور اس کے علاوہ، ہم نے ڈیٹا فیلٹرینگ، بڑی آواز والی، توجه کے مکانیسم اور ڈومین اضافہ کے دو پھیلانے کی تخصیلیں کے ساتھ آزمائش کی۔ یہ طریقے استعمال کر رہے ہیں، ہم اس طریقے پر بہت اچھے بہترین سوداگری دکھا سکتے ہیں جن کے بارے میں اور ہمارے IWSLT 2016 تحویل کے بارے میں.', 'so': 'Shuqulkaas wuxuu ku qoran yahay nidaamka tarjumaadda Neural Machine (NMT) ee jaamacadda RWTH Aachen oo loo horumariyey habbooyinka Ingiriis$Jarmalka ee qiimeynta campaignka caalamiga ah ee warqadda turjumaadka luqada lagu hadlo (IWSLT) 2017. Waxaynu isticmaalnaa nidaamka NMT, kuwaas oo lagu kordhiyey wadamada farshaxanka. Furthermore, waxaynu ku baaraandegaynaa qalabka ku saabsan baaritaanka macluumaadka, hadalka aad u weyn, labada sii-socda oo u bedelanaya macquulka caafimaadka iyo hagitaanka gudaha. Isku isticmaalka qaababkan, waxaan ka muujin karaa horumarin aad u fiican nidaamka aasaasiga ah iyo soo dirista IWSLT 2016.', 'uz': "Name @ info: status Ko'rsatganda, biz ma'lumotni filterlash, maxsus soʻzni, ikkita kengaytmasi mekanisme va domen adaptlash uchun ikkita kengaytmalar bilan tajriba qilamiz. Bu usuldan foydalanish bilan biz har xil asosiy tizimlari va IWSLT 2016 loyihasimizni ko'rsatishimiz mumkin.", 'vi': 'Bài làm này mô tả hệ thống dịch về máy thần kinh (NMB) của trường đại học RWTH Aachen phát triển cho quân đội Anh-Đức của chiến dịch đánh giá của Tổ chức Chứng Quốc tế về ngôn ngữ (IWSLT) trê K7. Chúng tôi sử dụng hệ thống NMT được tăng cường bởi những phần mở rộng mới Hơn nữa, chúng tôi thí nghiệm với các kỹ thuật bao gồm bộ lọc dữ liệu, một từ vựng lớn hơn, hai phần mở rộng của cơ chế tập trung và sửa chữa miền. Sử dụng các phương pháp này, chúng ta có thể thấy những cải tiến đáng kể về hệ cơ bản và sự cung cấp IWSLT trê.', 'da': 'Dette arbejde beskriver det Neural Machine Translation (NMT) system fra RWTH Aachen Universitet, der er udviklet til de engelsk-tyske spor af evalueringskampagnen for International Workshop on Talen Language Translation (IWSLT) 2017. Vi bruger NMT-systemer, som er forstærket med state-of-the-art udvidelser. Desuden eksperimenterer vi med teknikker, der omfatter datafiltrering, et større ordforråd, to udvidelser af opmærksomhedsmekanismen og domænetilpasning. Ved hjælp af disse metoder kan vi vise betydelige forbedringer i forhold til de respektive basissystemer og vores IWSLT 2016 indsendelse.', 'bg': 'Настоящата работа описва системата за неврален машинен превод (НМТ) на Аахенския университет разработена за песните по английски долар от кампанията за оценка на Международния семинар по говорен език (2017). Ние използваме системи, които са разширени с най-съвременни разширения. Освен това експериментираме с техники, които включват филтриране на данни, по-голям речник, две разширения на механизма за внимание и адаптация на домейна. Използвайки тези методи, можем да покажем значителни подобрения спрямо съответните базови системи и нашето представяне за 2016 г.', 'nl': 'Dit werk beschrijft het Neural Machine Translation (NMT) systeem van de RWTH Aachen University ontwikkeld voor de Engelstalige tracks van de evaluatiecampagne van de International Workshop on Spoken Language Translation (IWSLT) 2017. Wij maken gebruik van NMT systemen die worden aangevuld met state-of-the-art extensies. Verder experimenteren we met technieken zoals datafiltering, een grotere woordenschat, twee uitbreidingen van het aandachtsmechanisme en domeinaanpassing. Met behulp van deze methoden kunnen we aanzienlijke verbeteringen laten zien ten opzichte van de respectievelijke baseline systemen en onze IWSLT 2016 indiening.', 'hr': 'Ovaj rad opisuje sustav Neuralnog prevoda strojeva (NMT) Univerziteta RWTH Aachen razvijen za njemačke tragove engleskog dolara za procjenu kampanje Međunarodnog radionica o prevodu govornog jezika (IWSLT) 2017. Koristimo NMT sustave koje se povećavaju proširenjem stanja umjetnosti. Osim toga, eksperimentiramo tehnike koje uključuju filtriranje podataka, veći rečnik, dvije proširenje pažnje na mehanizam i adaptaciju domena. Koristeći te metode, možemo pokazati značajne poboljšanje u odnosu na odgovarajuće početne sustave i podnošenje IWSLT 2016.', 'id': 'Kerja ini menggambarkan sistem Translation Machine Neural (NMT) dari Universitas RWTH Aachen yang dikembangkan untuk jejak Inggris$Jerman dari kampanye evaluasi Workshop Internasional on Spoken Language Translation (IWSLT) 2017. Kami menggunakan sistem NMT yang ditambah oleh ekstensi state-of-the-art. Furthermore, we experiment with techniques that include data filtering, a larger vocabulary, two extensions to the attention mechanism and domain adaptation.  Dengan menggunakan metode-metode ini, kita dapat menunjukkan peningkatan yang konsiderel atas sistem dasar yang sesuai dan pengiriman IWSLT 2016 kami.', 'de': 'Diese Arbeit beschreibt das System der Neural Machine Translation (NMT) der RWTH Aachen, das für die englisch$German Tracks der Evaluationskampagne des Internationalen Workshops zur Sprachübersetzung (IWSLT) 2017 entwickelt wurde. Wir verwenden NMT-Systeme, die durch modernste Erweiterungen ergänzt werden. Darüber hinaus experimentieren wir mit Techniken, die Datenfilterung, ein größeres Vokabular, zwei Erweiterungen des Aufmerksamkeitsmechanismus und Domänenanpassung beinhalten. Mit diesen Methoden können wir erhebliche Verbesserungen gegenüber den jeweiligen Basissystemen und unserer IWSLT 2016-Einreichung zeigen.', 'ko': '이 사업은 RWTH 아흐레대학이 2017년 국제말하기번역세미나(IWSLT) 평가 활동을 위한 영어와 독일어 궤도를 위해 개발한 신경기계번역(NMT) 시스템을 기술했다.우리는 NMT 시스템을 사용하는데, 이러한 시스템은 최첨단 확장을 통해 증강되었다.그 밖에 우리는 데이터 필터, 더 큰 어휘량, 주의 메커니즘의 두 가지 확장과 영역 적응을 포함한 기술도 시도했다.이러한 방법을 사용하면 우리는 각자의 기선 시스템과 2016년에 제출한 IWSLT에 비해 현저한 개선을 나타낼 수 있다.', 'sw': 'Kazi hii inaelezea mfumo wa Tafsiri ya Mashine ya Neural (NMT) wa Chuo Kikuu cha RWTH Aachen ulioandaliwa kwa ajili ya kufuatiliwa kwa dola za Kiingereza za kwa ajili ya kampeni ya tathmini ya Warsha ya Kimataifa ya Utafsiri wa Lugha (IWSLT) 2017. Tunatumia mfumo wa NMT ambao unaongezeka na maendeleo ya hali ya sanaa. Zaidi ya hayo, tunajaribu kwa mbinu ambazo zinajumuisha kuchuja taarifa, lugha kubwa zaidi, maendeleo mawili kwa mfumo wa kusikiliza na kuboreshwa kwa ndani. Kwa kutumia mbinu hizi, tunaweza kuonyesha maendeleo makubwa kuhusu mifumo ya msingi na ujumbe wetu wa IWSLT 2016.', 'tr': 'Bu işe 2017-nji ýyldan RWTH Aachen Uniwersitetiniň Nural Makina Terjime sistemini (NMT) Iňlisçe$Almança dilinde Halkara dil terjime etmäge kampanýasynyň çykyş kampanyny bejerýär. Biz NMT sistemalary çapda möhüm döwletlere görä gelişýäris Munuň üçin data filtrelemesi, uly sözlük, üns meýdançasyna we domeny adaptasyny dahil eden tekniklerle synanyşýarys. Bu yönlerden ullanýarys, biz 2016-nji IWSLT-yň baýram sistemalarynda möhüm gelişmeleri görkezip bileris.', 'sq': 'Ky punë përshkruan sistemin e Translacionit të Makinës Neurale (NMT) të Universitetit RWTH Aachen të zhvilluar për gjurmët angleze$gjermane të fushatës së vlerësimit të Workshop Ndërkombëtar mbi Translacionin e gjuhës së folur (IWSLT) 2017. Ne përdorim sisteme NMT që janë rritur nga zgjerimet më të larta. Përveç kësaj, ne eksperimentojmë me teknika që përfshijnë filtrimin e të dhënave, një fjalor më të madh, dy zgjatje në mekanizmin e vëmendjes dhe përshtatjen e domenit. Duke përdorur këto metoda, ne mund të tregojmë përmirësime të konsiderueshme lidhur me sistemet bazë respektive dhe paraqitjen tonë të IWSLT 2016.', 'am': 'ይህ ሥራ የነዌብ መኪን ትርጉም (NMT) የኢንንግሊዝኛ ডল የጀርመን ትርጉም የዓለምአቀፍ የንግግር ቋንቋ ትርጉም ጉዳይ ዘመቻ (IWSLT) በዓለምአቀፍ ትርጉም ላይ የተዘጋጀውን የኢንተርኔት ዩንቨርስቲ (አሜሪካዊ አሜሪካ) የረኀብ መኪን ትርጉም (አ በሀገር-የ-አርእስት ስፋት የተጨማሪውን የNMT ስርዓቶች እናጠቃለን፡፡ ከዚህም በላይ የዳታ አጣራ፣ ትልቅ ቃላት፣ ሁለት ስፋት ማሰናከያ እና የውይይት አካባቢ እና አካባቢ ማድረግ የሚችሉትን እናሞክራለን፡፡ እነዚህን ሥርዓቶች በመጠቀም፣ በአካላዊ መደገፊያ ስርዓቶች እና የIWSLT 2016 ስልጣንን እናሳየን፡፡', 'hy': 'Այս աշխատանքը նկարագրում է RW THH Ախենի համալսարանի Նեյրալ Մեքենայի թարգմանման (NMT) համակարգը, որը զարգացել է 2017 թվականի Անգլերենի$գերմանացի թարգմանման միջազգային աշխատասենյակի գնահատման քարոզարշավի համար: Մենք օգտագործում ենք NMT համակարգեր, որոնք աճում են ամենաբարձր տարածություններով: Ավելին, մենք փորձում ենք այն տեխնիկաների հետ, որոնք ներառում են տվյալների ֆիլտրումը, ավելի մեծ բառարանը, ուշադրության մեխանիզմի երկու ընդլայնումը և բնագավառի ադապտացիան: Օգտագործելով այս մեթոդները, մենք կարող ենք ցույց տալ նշանակալի բարելավումներ համեմատական հիմնական համակարգերի և մեր IW-ՍԼT 2016-ի ներկայացման հարցում:', 'fa': 'این کار سیستم ترجمه ماشین عصبی (NMT) دانشگاه RWTH Aachen را توصیف می\u200cکند که برای نقشه\u200cهای آلمانی انگلیسی در کمپین ارزیابی کارگاه بین\u200cالمللی در ترجمه زبان اسپانیایی (IWSLT) ۲۰۰۷ توسعه داده شده است. ما از سیستم\u200cهای NMT استفاده می\u200cکنیم که توسط استفاده\u200cهای ایالت هنر افزایش می\u200cشوند. علاوه بر این، ما با تکنیک\u200cها آزمایش می\u200cکنیم که شامل فیلتر داده\u200cها، یک کلمه بزرگ، دو شاخه\u200cای برای مکانیسم توجه و تعادل دامنی است. با استفاده از این روش\u200cها، می\u200cتوانیم بر روی سیستم\u200cهای پایین\u200cخط و تسلیم IWSLT ۲۰۱۶ بهترین\u200cها را نشان دهیم.', 'af': "Hierdie werk beskryf die Neurale Masjien Vertaling (NMT) stelsel van die RWTH Aachen Universiteit wat ontwikkel het vir die Engels$Duitse snitte van die evaluering kampanie van die Internasionale Werkshop op Spoken Taal Vertaling (IWSLT) 2017. Ons gebruik NMT stelsels wat deur state-of-the-art uitbreidings vergroot word. Ons eksperimenteer ook met teknike wat data filtering insluit, 'n groter woordeboek, twee uitbreidings tot die aandag mekanisme en domein aanpassing. Deur hierdie metodes te gebruik, kan ons betekeurige verbeteringe vertoon oor die respektiewe basis stelsels en ons IWSLT 2016-onderwerp.", 'bn': 'এই কাজের ব্যাখ্যা করা হচ্ছে RWTH আচেন বিশ্ববিদ্যালয়ের নেউরাল মেশিন অনুবাদ (এনএমটি) ব্যবস্থা (এনএমটি) যা ইংরেজী ডলার জার্মানির ট্র্যাকের জন্য উন্নয়ন করেছে  আমরা এনএমটি সিস্টেম ব্যবহার করি যা রাষ্ট্র-অফ-শিল্প বিস্তারিত ব্যবহার করে। এছাড়াও, আমরা প্রযুক্তি দিয়ে পরীক্ষা করছি যার মধ্যে তথ্য ফিল্টারিং, বৃহত্তর শব্দভাণ্ডার, দুটি বিস্তারিত ব্যবস্থা এবং ডোমেইন এই পদ্ধতিগুলো ব্যবহার করে আমরা নিজেদের বেসাইলাইন সিস্টেমের ব্যাপারে বিশেষ উন্নতি দেখাতে পারি এবং আমাদের আইউএসএলটি', 'bs': 'Ovaj rad opisuje sistem Neuralnog prevoda mašine (NMT) Univerziteta RWTH Aachen razvijen za njemačke tragove engleskog dolara za procjenu kampanje Međunarodnog radionica o prevodu jezika spomenika (IWSLT) 2017. Koristimo NMT sisteme koje se povećavaju državnim proširenjem umjetnosti. Osim toga, eksperimentiramo sa tehnikama koja uključuju filtriranje podataka, veći rečnik, dvije proširenje na mehanizam pažnje i adaptaciju domena. Koristeći te metode, možemo pokazati značajne poboljšanje u odnosu na odgovarajuće početne sustave i predstavu IWSLT 2016.', 'cs': 'Tato práce popisuje systém neuronového strojového překladu (NMT) RWTH Aachen University vyvinutý pro německé stopy hodnotící kampaně Mezinárodního workshopu pro překlad mluvených jazyků (IWSLT) 2017. Používáme NMT systémy, které jsou rozšířeny o nejmodernější rozšíření. Dále experimentujeme s technikami, které zahrnují filtrování dat, větší slovní zásobu, dvě rozšíření mechanismu pozornosti a adaptaci domény. Pomocí těchto metod můžeme prokázat značná zlepšení oproti příslušným základním systémům a našemu předložení IWSLT 2016.', 'ca': "Aquesta obra descriu el sistema de traducció de màquines neurals (NMT) de la Universitat RWTH Aachen desenvolupat per a les pistes angleses$alemanes de la campanya d'evaluació de l'Atelier Internacional sobre traducció de llenguatge parlat (IWSLT) 2017. Utilitzem sistemes NMT que estan augmentats per estensions més modernes. A més, experimentem amb tècniques que inclouen filtrar dades, un vocabulari més gran, dues extensions al mecanisme d'atenció i l'adaptació a dominis. Utilitzant aquests mètodes, podem mostrar millors consideràries en relació amb els respectius sistemes de referència i la nostra presentació IWSLT 2016.", 'fi': 'T채ss채 ty철ss채 kuvataan RWTH Aachenin yliopiston neurokonek채채nn철sj채rjestelm채채 (NMT), joka on kehitetty englannin$saksan kielen k채채nn철ksen kansainv채lisen ty철pajan (IWSLT) 2017 arviointikampanjan arviointikampanjaan. K채yt채mme NMT-j채rjestelmi채, joita t채ydennet채채n uusimmilla laajennuksilla. Lis채ksi kokeilemme tekniikoita, jotka sis채lt채v채t tiedon suodatuksen, suuremman sanaston, kaksi huomiomekanismin laajennusta ja verkkotunnuksen mukauttamisen. N채iden menetelmien avulla voimme n채ytt채채 merkitt채vi채 parannuksia vastaaviin perusj채rjestelmiin ja IWSLT 2016 -julkaisuihimme verrattuna.', 'az': 'Bu iŇüin 2017-ci RWTH Aachen Universitetinin Neural Machine Translation (NMT) sistemini ńįngilizce$Almanca dill…ôrinin International Workshop on Spoken Language Translation (IWSLT) kampanyasńĪnńĪn deńüerlendirm…ô kampanyasńĪnńĪn ńįngilizce √ľ√ß√ľn hazńĪrlanmńĪŇüdńĪr. Biz NMT sisteml…ôrini istifad…ô edirik ki, art ńĪq artńĪqlarńĪ il…ô artńĪrńĪlńĪr. Daha sonra, m…ôlumat filtrl…ôm…ôsi, daha b√∂y√ľk s√∂zl…ôr, dikkat mehanizmisinin v…ô domeinin adaptasiyasńĪnńĪn iki geniŇüliyi il…ô t…ôcr√ľb…ô edirik. Bu metodlarńĪ istifad…ô ed…ôr…ôk, biz m√ľqayis…ôd…ô baz sisteml…ôrimiz v…ô 2016-ci IWSLT t…ôsdiql…ônm…ôl…ôrimiz √ľz…ôrind…ô √ßox yaxŇüńĪlńĪqlarńĪ g√∂st…ôr…ô bil…ôrik.', 'et': 'Käesolevas töös kirjeldatakse RWTH Aacheni Ülikooli neuraalse masintõlke (NMT) süsteemi, mis on välja töötatud 2017. aasta rahvusvahelise rääkiva keele tõlke seminari (IWSLT) hindamiskampaania inglise dollari kaupa. Kasutame NMT süsteeme, mida täiendavad kaasaegsed laiendused. Lisaks katsetame tehnikaid, mis hõlmavad andmete filtreerimist, suuremat sõnavara, kahte tähelepanumehhanismi laiendust ja domeeni kohandamist. Nende meetodite abil saame näidata märkimisväärseid edusamme võrreldes vastavate baassüsteemidega ja IWSLT 2016 esitatud esitustega.', 'sk': 'V delu je opisan sistem nevralnega strojnega prevajanja (NMT) Univerze RWTH Aachen, razvit za angleško-nemške sledi evalvacijske kampanje Mednarodne delavnice o govorjenem jeziku (IWSLT) 2017. Uporabljamo NMT sisteme, ki so dopolnjeni z najsodobnejšimi razširitvami. Poleg tega eksperimentiramo s tehnikami, ki vključujejo filtriranje podatkov, večji besednjak, dve razširitvi mehanizma pozornosti in prilagajanje domene. Z uporabo teh metod lahko pokažemo znatne izboljšave v primerjavi z ustreznimi osnovnimi sistemi in našo predložitev IWSLT 2016.', 'ha': "Wannan aikin yana describe the Neural Machine Translate (NMT) system of the RWthth Aachen University developed for the English$jeruman traces of the evaluation campfire of the International workload on Talken Lugha (IWSLT) 2017. We use NMT systems which are augmented by state-of-the-art extensions.  Furan haka, munã jarraba masu akan takwai waɗand a ke cikin fililurin data, mai girma wa maganar, watau faɗi biyu zuwa mazaɓa-muhalli da adadin ayuka. Yin amfani da waɗannan hanyõyin, za mu iya nũna mafiya kyau a kanzu masu ƙari ga tsarin basuɓani da kuma ma'anar IWSLT 2016.", 'jv': 'Wuhan iki oleh nggambaran sistem Noral Manus Terjamahan @item:listbox Label Ngawe Perintah sing dipoleh iki, kéné iso ngéwangi nggawe barang-barang kanggo sistem sing titimbang nggawe barang nggawe tarjamahan kanggo nggawe IWSLT-2013.', 'he': 'העבודה הזו מתארת את מערכת המכונה הנוירולית (NMT) של אוניברסיטת RWTH Aachen שפותחה עבור המסלולים הגרמניים של קמפיין הערכה של הספר הבינלאומי על התרגום לשפה מדברת (IWSLT) 2017. אנו משתמשים במערכות NMT שגודלות על ידי התרחבות מצוינות. חוץ מזה, אנו מנסים עם טכניקות שכלולות פילטר נתונים, מילון גדול יותר, שתי התרחבות למנגנון תשומת לב והשימוש לתחום. בשימוש בשיטות אלה, אנחנו יכולים להראות שיפורים משמעותיים מעל מערכות הבסיסים המתאימות המתאימות והשליחה שלנו IWSLT 2016.', 'bo': 'This work describes the Neural Machine Translation (NMT) system of the RWTH Aachen University developed for the English$German tracks of the evaluation campaign of the International Workshop on the Spoken Language Translation (IWSLT) 2017. ང་ཚོས་རྒྱལ་ཁབ་ཀྱི་འཛམ་གླིང་ལ་སྤྱོད་པའི་NMT་ལ་དུས་འཕགས་རིས་ཕར་བསྐྱེད་ཡོད། Furthermore, we experiment with techniques that include data filtering, a larger vocabulary, two extensions to the attention mechanism and domain adaptation. ང་ཚོས་ཐབས་ལམ་འདི་དག་སྤྱད་ནས་གནད་གཟུགས་རིས་ལ་རང་ཉིད་ཀྱི་རྨན་གཞུང་གི་མ་ལག་དང་ང་ཚོའི་IWSLT(2016)ཆ་འཕྲིན་ག'}
{'en': 'FBK’s Multilingual Neural Machine Translation System for IWSLT 2017 FBK ’s Multilingual Neural Machine Translation System for  IWSLT  2017', 'ar': 'نظام الترجمة الآلية العصبية متعدد اللغات من FBK لـ IWSLT 2017', 'fr': 'Le système de traduction automatique neuronale multilingue de FBK pour IWSLT 2017', 'es': 'Sistema de traducción automática neuronal multilingüe de FBK para IWSLT 2017', 'pt': 'Sistema de tradução automática neural multilíngue da FBK para IWSLT 2017', 'ja': 'IWSLT 2017のためのFBKの多言語ニューラルマシン翻訳システム', 'zh': 'FBK 以 IWSLT 2017 多言神经机器翻译统', 'hi': 'IWSLT 2017 के लिए FBK की बहुभाषी तंत्रिका मशीन अनुवाद प्रणाली', 'ru': 'Многоязычная система нейронного машинного перевода FBK для IWSLT 2017', 'ga': 'Córas Aistriúcháin Meaisín Néarach Ilteangach FBK do IWSLT 2017', 'ka': 'FBK multilingual Neural Machine Translation System for IWSLT 2017', 'el': 'Πολυγλωσσικό Νευρικό Μηχανολογικό Μεταφραστικό Σύστημα της FBK για το IWSLT 2017', 'hu': 'FBK többnyelvű idegi fordító rendszere IWSLT 2017-hez', 'it': 'Sistema di traduzione automatica neurale multilingue di FBK per IWSLT 2017', 'kk': 'FBK- ның IWSLT 2017 жылы көптілік нейрал машинаның аудару жүйесі', 'lt': 'FBK 2017 m. daugiakalbės neurologinės mašinos vertimo sistema', 'ms': 'Sistem Terjemahan Mesin Neural Berbahasa FBK untuk IWSLT 2017', 'mk': 'Мултијазичен систем за преведување на неврални машини на ФБК за IWSLT 2017', 'ml': "FBK's Multilingual Neural Machine Translation System for IWSLT 2017", 'mt': 'Is-Sistema Multilingwi tat-Traduzzjoni tal-Makkinarju Newrali tal-FBK għall-IWSLT 2017', 'pl': 'Wielojęzyczny system maszynowego tłumaczenia neuronowego FBK dla IWSLT 2017', 'mn': 'FBK-ын IWSLT 2017 оны олон хэлний мэдрэлийн машин хөгжүүлэх систем', 'no': 'FBK sin fleirspråk neuralmaskinsomsetjingssystem for IWSLT 2017', 'ro': 'Sistemul multilingv de traducere a mașinii neurale FBK pentru IWSLT 2017', 'sr': "FBK's Multilingual Neural Machine Translation System for IWSLT 2017", 'si': "FBK's Multilanguage neural machine translation system for IWSLT 2017", 'so': "FBK's Multilingual Neural machine Translation System for IWSLT 2017", 'sv': 'FBK:s flersprĂ¥kiga neurala maskinĂ¶versĂ¤ttningssystem fĂ¶r IWSLT 2017', 'ta': 'IWSLT 2017 க்கான FBK பல மொழி நெயுரல் இயந்திரம் மொழிபெயர்ப்பு அமைப்பு', 'ur': 'FBK کی Multilingual Neural Machine Translation System for IWSLT 2017', 'uz': 'Comment', 'vi': 'Hệ thống dịch đa ngôn ngữ thần kinh cho IWSLT bây giờ', 'bg': 'Многоезичната система за неврален машинен превод на ФБК за 2017', 'da': "FBK's flersprogede neural maskinoversættelsessystem til IWSLT 2017", 'hr': 'FBK Multilingual Neural Machine Translation System for IWSLT 2017', 'nl': 'Het meertalige neurale machinevertaalsysteem van FBK voor IWSLT 2017', 'de': 'Das mehrsprachige neuronale maschinelle Übersetzungssystem von FBK für IWSLT 2017', 'fa': 'سیستم ترجمه ماشین عصبی چندین زبان FBK برای IWSLT 2017', 'id': 'Sistem Translasi Mesin Neural Multibahasa FBK untuk IWSLT 2017', 'sw': 'Mfumo wa Tafsiri wa Mashine ya Kifaransa ya FBK kwa ajili ya IWSLT 2017', 'ko': 'FBK의 다국어 신경 기계 번역 시스템, IWSLT 2017에 사용', 'sq': 'Sistemi Multilingual Neural Machine Translation i FBK për IWSLT 2017', 'am': "FBK's Multilingual Neural Machine Translation System for IWSLT 2017", 'hy': 'FBK-ի բազլեզու նյարդային մեքենայի թարգմանման համակարգը 2017-ին', 'af': 'FBK se Multilingual Neurale Masjien Vertalingsstelsel vir IWSLT 2017', 'az': "FBK's Multilingual Neural Machine Translation System for IWSLT 2017", 'bn': 'এফবিকের মাল্টিভাষায় নিউরাল মেশিন অনুবাদ সিস্টেম ইউএসএলটি ২০১৭', 'bs': "FBK's Multilingual Neural Machine Translation System for IWSLT 2017", 'ca': 'El Sistema de Traducció Multilingüe de Màquines Neural s de FBK per IWSLT 2017', 'cs': 'Vícejazyčný neuronový strojový překlad FBK pro IWSLT 2017', 'et': 'FBK mitmekeelne neuraalne masintõlke süsteem IWSLT 2017 jaoks', 'fi': 'FBK: n monikielinen hermojen konekäännösjärjestelmä IWSLT 2017: lle', 'tr': "FBK'yň IWSLT 2017-nji ýyllar dili", 'jv': "FBK's Multilanguage Neral Device translation System for IWSLT 1997", 'sk': 'FBK-jezični nevralni strojni prevajalski sistem za IWSLT 2017', 'he': 'מערכת התרגום של מכונות נוירויות רבות של FBK עבור IWSLT 2017', 'ha': 'KCharselect unicode block name', 'bo': "FBK's Multilingual Neural Machine Translation System for IWSLT 2017"}
{'en': 'Neural Machine Translation has been shown to enable  inference  and cross-lingual knowledge transfer across multiple language directions using a single multilingual model. Focusing on this multilingual translation scenario, this work summarizes FBK’s participation in the IWSLT 2017 shared task. Our submissions rely on two multilingual systems trained on five languages (English,  Dutch ,  German ,  Italian , and Romanian). The first one is a 20 language direction model, which handles all possible combinations of the five languages. The second multilingual system is trained only on 16 directions, leaving the others as zero-shot translation directions (i.e representing a more complex inference task on language pairs not seen at training time). More specifically, our zero-shot directions are  Dutch$German  and  Italian$Romanian  (resulting in four language combinations). Despite the small amount of parallel data used for training these systems, the resulting multilingual models are effective, even in comparison with  models  trained separately for every language pair (i.e. in more favorable conditions). We compare and show the results of the two multilingual models against a baseline single language pair systems. Particularly, we focus on the four zero-shot directions and show how a multilingual model trained with small data can provide reasonable results. Furthermore, we investigate how pivoting (i.e using a bridge / pivot language for  inference  in a source!pivot!target translations) using a multilingual model can be an alternative to enable zero-shot translation in a low resource setting.', 'ar': 'لقد ثبت أن الترجمة الآلية العصبية تتيح الاستدلال ونقل المعرفة عبر اللغات عبر اتجاهات لغة متعددة باستخدام نموذج واحد متعدد اللغات. بالتركيز على سيناريو الترجمة متعدد اللغات هذا ، يلخص هذا العمل مشاركة FBK في المهمة المشتركة IWSLT 2017. تعتمد عروضنا على نظامين متعددي اللغات مدربين على خمس لغات (الإنجليزية والهولندية والألمانية والإيطالية والرومانية). الأول هو نموذج اتجاه يضم 20 لغة ، والذي يتعامل مع جميع التركيبات الممكنة للغات الخمس. يتم تدريب النظام متعدد اللغات الثاني فقط على 16 اتجاهًا ، تاركًا الآخرين كتوجيهات ترجمة بدون طلقة (أي يمثل مهمة استدلال أكثر تعقيدًا على أزواج اللغات التي لم تتم رؤيتها في وقت التدريب). وبشكل أكثر تحديدًا ، فإن اتجاهاتنا الخالية من الضربات هي الهولندية $ German و Italian $ Romanian (مما أدى إلى أربع مجموعات لغوية). على الرغم من قلة البيانات المتوازية المستخدمة لتدريب هذه الأنظمة ، فإن النماذج متعددة اللغات الناتجة فعالة ، حتى بالمقارنة مع النماذج المدربة بشكل منفصل لكل زوج لغوي (أي في ظروف أكثر ملاءمة). نحن نقارن ونعرض نتائج النموذجين متعددي اللغات مقابل أنظمة أزواج لغوية واحدة أساسية. على وجه الخصوص ، نركز على الاتجاهات الأربعة الخالية من الضربات ونبين كيف يمكن لنموذج متعدد اللغات مدرب ببيانات صغيرة أن يوفر نتائج معقولة. علاوة على ذلك ، نحن نتحرى كيفية التمحور (أي استخدام لغة جسر / محورية للاستدلال في ترجمات المصدر! المحور! الهدف) باستخدام\nيمكن أن يكون النموذج متعدد اللغات بديلاً لتمكين الترجمة بدون طائل في بيئة منخفضة الموارد.', 'fr': "Il a été démontré que la traduction automatique neuronale permet l'inférence et le transfert de connaissances multilingues dans plusieurs directions linguistiques à l'aide d'un seul modèle multilingue. En se concentrant sur ce scénario de traduction multilingue, ce travail résume la participation de FBK à la tâche partagée IWSLT 2017. Nos soumissions s'appuient sur deux systèmes multilingues formés en cinq langues (anglais, néerlandais, allemand, italien et roumain). Le premier est un modèle à 20 directions linguistiques, qui gère toutes les combinaisons possibles des cinq langues. Le deuxième système multilingue n'est entraîné que dans 16 directions, les autres étant des directions de traduction zéro plan (c'est-à-dire représentant une tâche d'inférence plus complexe sur des paires de langues non vues au moment de la formation). Plus précisément, nos directions zero-shot sont le néerlandais$allemand et l'italien$roumain (ce qui donne quatre combinaisons de langues). Malgré la faible quantité de données parallèles utilisées pour la formation de ces systèmes, les modèles multilingues qui en résultent sont efficaces, même en comparaison avec des modèles formés séparément pour chaque paire de langues (c'est-à-dire dans des conditions plus favorables). Nous comparons et montrons les résultats des deux modèles multilingues par rapport à des systèmes de paires de langues uniques de référence. En particulier, nous nous concentrons sur les quatre directions de tir zéro et montrons comment un modèle multilingue entraîné avec de petites données peut fournir des résultats raisonnables. En outre, nous étudions comment le pivotement (c'est-à-dire l'utilisation d'un langage pont/pivot pour l'inférence dans une source\xa0! Pivot\xa0! traductions cibles) à l'aide d'un\nLe modèle multilingue peut être une alternative pour permettre la traduction zero-shot dans un environnement à faibles ressources.", 'ja': 'ニューラル・マシン・トランスレーションは、単一の多言語モデルを使用して、複数の言語方向にわたる推論とクロスリンガルの知識伝達を可能にすることが示されている。 この多言語翻訳シナリオに焦点を当てたこの研究では、FBKのIWSLT 2017共有タスクへの参加をまとめています。 当社の提出物は、5つの言語（英語、オランダ語、ドイツ語、イタリア語、ルーマニア語）で訓練された2つの多言語システムに依存しています。 1つ目は、5つの言語のすべての可能な組み合わせを処理する20言語方向モデルです。 第2の多言語システムは、16の方向のみでトレーニングされ、他はゼロショット翻訳方向として残されます（つまり、トレーニング時には見られない言語ペアでのより複雑な推論タスクを表します）。 具体的には、ゼロショットのルートはオランダ語$ドイツ語とイタリア語$ルーマニア語（ 4つの言語の組み合わせになります）です。 これらのシステムを訓練するために使用される並列データの量が少ないにもかかわらず、結果として生じる多言語モデルは、（すなわち、より有利な条件で）すべての言語ペアに対して別々に訓練されたモデルと比較しても効果的である。 ベースライン単一言語ペアシステムと比較し、２つの多言語モデルの結果を示す。 特に、4つのゼロショット方向に焦点を当て、小さなデータで訓練された多言語モデルが合理的な結果を提供する方法を示します。 さらに、ソース!ピボット!ターゲット翻訳での推論のためのブリッジ/ピボット言語の使用方法を調べます。\n多言語モデルは、低リソース設定でゼロショット翻訳を可能にする代替手段となり得る。', 'es': 'Se ha demostrado que la traducción automática neuronal permite la inferencia y la transferencia de conocimientos multilingües a través de múltiples direcciones lingüísticas utilizando un único modelo multilingüe. Centrándose en este escenario de traducción multilingüe, este trabajo resume la participación de FBK en la tarea compartida de IWSLT 2017. Nuestras presentaciones se basan en dos sistemas multilingües capacitados en cinco idiomas (inglés, holandés, alemán, italiano y rumano). El primero es un modelo de dirección de 20 idiomas, que maneja todas las combinaciones posibles de los cinco idiomas. El segundo sistema multilingüe se entrena solo en 16 direcciones, dejando las otras como direcciones de traducción cero (es decir, representan una tarea de inferencia más compleja en pares de idiomas que no se ven en el momento del entrenamiento). Más específicamente, nuestras direcciones cero son Holandés$Alemán e Italiano$Rumano (lo que resulta en cuatro combinaciones de idiomas). A pesar de la pequeña cantidad de datos paralelos utilizados para entrenar estos sistemas, los modelos multilingües resultantes son eficaces, incluso en comparación con los modelos entrenados por separado para cada combinación de idiomas (es decir, en condiciones más favorables). Comparamos y mostramos los resultados de los dos modelos multilingües con los sistemas de pares de un solo idioma de referencia. En particular, nos centramos en las cuatro direcciones de tiro cero y mostramos cómo un modelo multilingüe entrenado con datos pequeños puede proporcionar resultados razonables. Además, investigamos cómo pivotante (es decir, usar un lenguaje puente/pivote para la inferencia en una fuente! ¡pivote! traducciones objetivo) mediante un\nel modelo multilingüe puede ser una alternativa para permitir la traducción cero en un entorno de recursos bajos.', 'pt': 'A tradução automática neural demonstrou permitir a inferência e a transferência de conhecimento em vários idiomas em várias direções de idioma usando um único modelo multilíngue. Com foco nesse cenário de tradução multilíngue, este trabalho resume a participação do FBK na tarefa compartilhada do IWSLT 2017. Nossos envios contam com dois sistemas multilíngues treinados em cinco idiomas (inglês, holandês, alemão, italiano e romeno). O primeiro é um modelo de direção de 20 idiomas, que lida com todas as combinações possíveis dos cinco idiomas. O segundo sistema multilíngue é treinado apenas em 16 direções, deixando os outros como direções de tradução de tiro zero (ou seja, representando uma tarefa de inferência mais complexa em pares de idiomas não vistos no momento do treinamento). Mais especificamente, nossas rotas de tiro zero são holandês$alemão e italiano$romeno (resultando em quatro combinações de idiomas). Apesar da pequena quantidade de dados paralelos usados para treinar esses sistemas, os modelos multilíngues resultantes são eficazes, mesmo em comparação com modelos treinados separadamente para cada par de idiomas (ou seja, em condições mais favoráveis). Comparamos e mostramos os resultados dos dois modelos multilíngues em relação a sistemas de par de idiomas únicos de linha de base. Particularmente, focamos nas quatro direções de tiro zero e mostramos como um modelo multilíngue treinado com dados pequenos pode fornecer resultados razoáveis. Além disso, investigamos como pivotar (ou seja, usar uma linguagem de ponte/pivot para inferência em traduções de origem!pivot!destino) usando um\nO modelo multilíngue pode ser uma alternativa para habilitar a tradução de tiro zero em uma configuração de poucos recursos.', 'zh': '神经机器翻译已验可用单多语言模样跨数语推理及跨语知识转移。 周旋多言译,此事FBK参IWSLT 2017共同任务。 所交赖两多言语系统,经五语(英语,荷兰语,德语,意大利语与罗马尼亚语)培训。 一曰20言向形,其治五言者可得而合也。 第二多语言系统只在16个向上训练,而他向则为零次译向(即示训练时未见的语言对更杂的推理)。 更具体地说,荷兰语德语与意大利语罗马尼亚语(四语合)。 虽与言为独教(利之所在)。 以两多言模形,与基线单语对系统较显。 专于四零射,示小数之多言。 此外,我们研究了怎么用\n多言模形可于资源较少者设中零次译。', 'ru': 'Было показано, что нейронный машинный перевод позволяет делать выводы и передавать знания на нескольких языках, используя одну многоязычную модель. Ориентируясь на этот многоязычный сценарий перевода, эта работа суммирует участие ФБК в совместной задаче IWSLT 2017. Наши представления основаны на двух многоязычных системах, подготовленных на пяти языках (английском, голландском, немецком, итальянском и румынском). Первый - это модель направления языка 20, которая обрабатывает все возможные комбинации пяти языков. Вторая многоязычная система обучается только по 16 направлениям, оставив остальные как нулевые направления перевода (то есть представляя собой более сложную задачу вывода по языковым парам, которые не видны во время обучения). Более конкретно, нашими направлениями с нулевым выстрелом являются голландский$ немецкий и итальянский$ румынский (что приводит к четырем языковым комбинациям). Несмотря на небольшое количество параллельных данных, используемых для обучения этих систем, полученные многоязычные модели эффективны, даже по сравнению с моделями, обученными отдельно для каждой языковой пары (т.е. в более благоприятных условиях). Мы сравниваем и показываем результаты двух многоязычных моделей с базовыми системами одной языковой пары. В частности, мы сосредоточимся на четырех направлениях с нулевым выстрелом и покажем, как многоязычная модель, обученная с небольшими данными, может обеспечить разумные результаты. Кроме того, мы исследуем, как поворот (т.е. использование языка моста/поворота для вывода в источнике!pivot!целевых переводов) с использованием\nмногоязычная модель может быть альтернативой для обеспечения перевода с нулевым выстрелом в условиях низкого ресурса.', 'hi': 'न्यूरल मशीन अनुवाद को एक एकल बहुभाषी मॉडल का उपयोग करके कई भाषा दिशाओं में अनुमान और क्रॉस-लिंगुअल ज्ञान हस्तांतरण को सक्षम करने के लिए दिखाया गया है। इस बहुभाषी अनुवाद परिदृश्य पर ध्यान केंद्रित करते हुए, यह कार्य IWSLT 2017 साझा कार्य में FBK की भागीदारी को सारांशित करता है। हमारी प्रस्तुतियाँ पांच भाषाओं (अंग्रेजी, डच, जर्मन, इतालवी और रोमानियाई) पर प्रशिक्षित दो बहुभाषी प्रणालियों पर निर्भर करती हैं। पहला एक 20 भाषा दिशा मॉडल है, जो पांच भाषाओं के सभी संभावित संयोजनों को संभालता है। दूसरी बहुभाषी प्रणाली को केवल 16 दिशाओं पर प्रशिक्षित किया जाता है, दूसरों को शून्य-शॉट अनुवाद दिशाओं के रूप में छोड़ दिया जाता है (यानी प्रशिक्षण के समय नहीं देखे गए भाषा जोड़े पर अधिक जटिल अनुमान कार्य का प्रतिनिधित्व करता है)। अधिक विशेष रूप से, हमारे शून्य-शॉट दिशाएं डच $ जर्मन और इतालवी $ रोमानियाई (जिसके परिणामस्वरूप चार भाषा संयोजन हैं)। इन प्रणालियों के प्रशिक्षण के लिए उपयोग किए जाने वाले समानांतर डेटा की छोटी मात्रा के बावजूद, परिणामी बहुभाषी मॉडल प्रभावी होते हैं, यहां तक कि प्रत्येक भाषा जोड़ी (यानी अधिक अनुकूल परिस्थितियों में) के लिए अलग से प्रशिक्षित मॉडल की तुलना में भी। हम एक बेसलाइन एकल भाषा जोड़ी प्रणालियों के खिलाफ दो बहुभाषी मॉडल के परिणामों की तुलना करते हैं और दिखाते हैं। विशेष रूप से, हम चार शून्य-शॉट दिशाओं पर ध्यान केंद्रित करते हैं और दिखाते हैं कि छोटे डेटा के साथ प्रशिक्षित एक बहुभाषी मॉडल उचित परिणाम कैसे प्रदान कर सकता है। इसके अलावा, हम जांच करते हैं कि कैसे pivoting (यानी एक स्रोत में अनुमान के लिए एक पुल / पिवट भाषा का उपयोग करना!pivot!target translations) का उपयोग करके\nबहुभाषी मॉडल एक कम संसाधन सेटिंग में शून्य-शॉट अनुवाद को सक्षम करने के लिए एक विकल्प हो सकता है।', 'ga': 'Léiríodh go n-éascaíonn Neural Machine Translation tátal agus aistriú eolais tras-teanga thar threoracha iolracha ag baint úsáide as múnla ilteangach amháin. Ag díriú ar an gcás aistriúcháin ilteangach seo, déanann an obair seo achoimre ar rannpháirtíocht FBK i dtasc comhroinnte IWSLT 2017. Bíonn ár n-aighneachtaí ag brath ar dhá chóras ilteangacha atá oilte ar chúig theanga (Béarla, Ollainnis, Gearmáinis, Iodáilis agus Rómáinis). Múnla treo 20 teanga atá sa chéad cheann, a láimhseálann gach teaglaim fhéideartha de na cúig theanga. Ní dhéantar oiliúint ar an dara córas ilteangach ach ar 16 threoir, rud a fhágann na cinn eile mar threoracha aistrithe nialasacha (i.e. tasc tátail níos casta ar phéirí teanga nach bhfeictear ag am oiliúna a léiriú). Go sonrach, is iad an Ollainnis-Gearmáinis agus an Iodáilis$Rómáinis ár dtreoracha náid-shotalach (agus ceithre theaglaim teanga mar thoradh air). D’ainneoin an méid beag sonraí comhthreomhara a úsáidtear chun na córais seo a oiliúint, tá na samhlacha ilteangacha a eascraíonn as seo éifeachtach, fiú i gcomparáid le samhlacha arna n-oiliúint ar leithligh do gach péire teanga (i.e. i gcoinníollacha níos fabhraí). Déanaimid comparáid agus taispeántar torthaí an dá mhúnla ilteangach i gcoinne córais bhunlíne péire teanga amháin. Go háirithe, dírímid ar na ceithre threoir náid-lámhaigh agus léirímid conas is féidir le múnla ilteangach atá oilte le sonraí beaga torthaí réasúnta a sholáthar. Ina theannta sin, déanaimid iniúchadh ar an gcaoi a bhfuil maighdeog (i.\nis féidir le samhail ilteangach a bheith ina rogha eile chun aistriúchán nialasach a chumasú i suíomh acmhainní ísle.', 'it': "Neural Machine Translation è stato dimostrato che consente l'inferenza e il trasferimento di conoscenze cross-lingual attraverso più direzioni linguistiche utilizzando un unico modello multilingue. Concentrandosi su questo scenario di traduzione multilingue, questo lavoro riassume la partecipazione di FBK al compito condiviso IWSLT 2017. I nostri contributi si basano su due sistemi multilingui formati su cinque lingue (inglese, olandese, tedesco, italiano e rumeno). Il primo è un modello di direzione in 20 lingue, che gestisce tutte le combinazioni possibili delle cinque lingue. Il secondo sistema multilingue è formato solo su 16 direzioni, lasciando le altre come direzioni di traduzione zero-shot (cioè rappresentano un compito di inferenza più complesso sulle coppie linguistiche non viste al momento dell'allenamento). Più specificamente, le nostre indicazioni zero-shot sono Dutch$German e Italian$Roman (con conseguente quattro combinazioni linguistiche). Nonostante la piccola quantità di dati paralleli utilizzati per la formazione di questi sistemi, i modelli multilingui risultanti sono efficaci, anche rispetto ai modelli formati separatamente per ogni coppia linguistica (cioè in condizioni più favorevoli). Confrontiamo e mostriamo i risultati dei due modelli multilingue rispetto a sistemi di base a coppie di lingue singole. In particolare, ci concentriamo sulle quattro direzioni zero-shot e mostriamo come un modello multilingue addestrato con piccoli dati può fornire risultati ragionevoli. Inoltre, esaminiamo come pivoting (cioè utilizzando un linguaggio bridge/pivot per inferenza in una traduzione source!pivot!target) utilizzando un\nIl modello multilingue può essere un'alternativa per abilitare la traduzione a scatto zero in un'impostazione a basso contenuto di risorse.", 'hu': 'A Neural Machine Translation kimutatták, hogy egyetlen többnyelvű modell segítségével lehetővé teszi a következtetést és a többnyelvű tudás átadását több nyelvű irányban. A többnyelvű fordítási forgatókönyvre összpontosítva a munka összefoglalja az FBK részvételét az IWSLT 2017 megosztott feladatában. Pályázataink öt nyelven (angol, holland, német, olasz és román) képzett többnyelvű rendszeren alapulnak. Az első egy 20 nyelvi iránymodell, amely kezeli az öt nyelv összes lehetséges kombinációját. A második többnyelvű rendszer csak 16 irányban képzésre kerül, a többieket zéró fordítási irányként hagyják (azaz bonyolultabb következtetési feladatot képviselnek a képzési időben nem látott nyelvpárokra). Pontosabban, a nulla lövéses irányzatunk a holland$német és olasz$román (ami négy nyelvkombinációt eredményez). A rendszerek képzéséhez használt kis mennyiségű párhuzamos adat ellenére az eredményekből származó többnyelvű modellek hatékonyak, még az egyes nyelvpárokra külön képzett modellekhez képest is (azaz kedvezőbb körülmények között). A két többnyelvű modell eredményeit összehasonlítjuk és bemutatjuk egy alapvető egynyelvpáros rendszerrel. Különösen a négy nulla lövési irányra összpontosítunk, és megmutatjuk, hogy egy kis adatokkal képzett többnyelvű modell hogyan tud ésszerű eredményeket nyújtani. Továbbá azt vizsgáljuk, hogy a forrás!pivot!target fordításokban hogyan lehet forgatni (vagyis bridge/pivot nyelvet használni a következtetéshez)\nA többnyelvű modell alternatívája lehet a nullás fordítás engedélyezéséhez alacsony erőforrás-beállítás mellett.', 'ka': 'Name ამ მრავალენგური გადაწყვეტის სენარიოში, ეს სამუშაო სამუშაო დაწყვეტილება FBK-ის გადაწყვეტილება IWSLT 2017 სამუშაო სამუშაო სამუშაოში. ჩვენი მხოლოდ ენერგიური სისტემაში გამოყენებული ორი მრავალური ენერგიური სისტემაში (ანგლისური, ჰოლდენური, გერმანური, თრალიანი და პრომინური). პირველი არის 20 ენის მიერ მოდელი, რომელიც ყველაფერი შესაძლებელი ხუთი ენების კომბიზაციების შესაძლებელება. მეორე მრავალენგური სისტემა მხოლოდ 16 მხარეს დაწყებულია, სხვების გადასვლა როგორც 0-სტრიქტური დაწყება (მაგალითად, უფრო კომპლექტური ინფრენციის რაოდენობა ენგური ზო უფრო განსაკუთრებულია, ჩვენი ნულ სტრიქტის დაწყება დონდენური და იტალიანური დონდენური დონდენური და პრომინური დონდენური დონდენური დ ამ სისტემებში გამოყენებული პარალელური მონაცემების პარალელი რაოდენობაც, შემდეგ მრავალური მოდელები ეფექტიურია, მაგრამ მრავალური მოდელები განსხვავებული მოდელთან განსხვავებულად განსხვავებული ყოველ ჩვენ შემდგომარებით და ჩვენ გამოჩვენებთ ორი მრავალენგური მოდელების შედეგები ერთი ენგური ზოგური სისტემებისთვის. განსაკუთრებით, ჩვენ ფონსკურებთ ოთხი ნულ სტრიქტის მიღებზე და ჩვენ გამოჩვენებთ, როგორ მრავალენგური მოდელი, როგორ მცირე მონაცემებით განაკეთებული დამატებით, ჩვენ განსხვავებთ თუ როგორ კონფიგურაციის გამოყენება (მაგალითად, კონფიგურაციის/pivot ენერგიის გამოყენება ინფრენციაში!pivot!target translations) გამოყენება\nმრავალენგური მოდელი შეიძლება იყოს ალტენტიფიკაცია, რომელიც 0- სტრიქტის გადაწყვეტილების შესაძლებელია მარტივი რესურსის შესაძლებელად.', 'kk': 'Нейрондық машинаның аудармасы бір көп тіл үлгісін қолдану үшін бірнеше тіл бағыттарына аудару және бірнеше тіл бағыттарына аудару мүмкіндігін қолдануға болады. Бұл көптілік аудармалардың сценариясына көздейтін жұмыс жұмысын 2017 жылдың IWSLT тапсырмасында FBK қатынасын ортақтастырады. Біздің келтіріміз бес тілде оқылған екі көп тіл жүйесіне тәуелді (ағылшын, немецтер, немецтер, итальян және руманша). Біріншісі - 20 тіл бағыттау үлгісі. Бұл бес тілдердің барлық мүмкін жинақтарын басқару үлгісі. Екінші көптілік жүйесі тек 16 бағыттарда оқылған, басқаларды нөл түрінде аудару бағыттары ретінде қалдырылады (мысалы, тіл екеуінде көрілмеген көпшілік тапсырмасы болады). Біздің нөл түрлі бағыттарымыз немецтер және итальян$Румынша (төрт тіл комбинациясы болады). Бұл жүйелерді оқыту үшін қолданылатын параллель деректердің кішкентай саны қарамастан, көптілік үлгілері әрбір тіл жиі үлгілеріне салыстырылған үлгілерімен әртүрлі салыстырылады (мысалы, көп жақсы шарт Біз екі көп тіл моделінің нәтижесін негізгі тіл жүйелеріне қарсы салыстырып көрсетедік. Мәселен, біз төрт нөл түрлі бағыттарына назар аударып, кішкентай деректердің бірнеше тілдік үлгісін қалай оқытуға мүмкін нәтижелерді көрсетуге болады. Қосымша, біз көпшілік көпшілік/pivot тілді көпшілік көпшілігін көпшілік көпшілігін қолданатын (мысалы, көпшілік көпшілігін қолданатын кө\nкөп тілдік үлгісі нөл түрлі ресурстарды төмен орнату үшін альтернативті болуы мүмкін.', 'lt': 'Nustatyta, kad neurologinis mašinų vertimas leidžia daryti išvadas ir tarpkalbinius žinių perdavimus įvairiomis kalbomis naudojant vieną daugiakalbį model į. Atsižvelgiant į šį daugiakalbį vertimo scenarijų, šiame darbe apibendrinamas FBK dalyvavimas bendroje 2017 m. IWSLT užduotyje. Mūsų pastabos grindžiamos dviem daugiakalbėmis sistemomis, mokomomis penkiomis kalbomis (anglų, olandų, vokiečių, italų ir rumunų kalbomis). Pirmasis yra 20 kalbų krypties model is, kuris apima visus galimus penkių kalbų derinius. Antroji daugiakalbė sistema mokoma tik 16 krypči ų, o kitos – nulinio vertimo krypčių (t. y. yra sudėtingesnė išvadų užduotis kalbų poroms, kurios mokymo metu nenustatytos). More specifically, our zero-shot directions are Dutch$German and Italian$Romanian (resulting in four language combinations).  Nepaisant nedidelio lygiagreči ų duomenų, naudojamų mokymui šiose sistemose, gaunami daugiakalbiai modeliai yra veiksmingi, net palyginti su modeliais, parengtais atskirai kiekvienai kalbų porai (t. y. palankesnėmis sąlygomis). Palyginame ir parodysime dviejų daugiakalbių modelių rezultatus su pradinėmis vienos kalbos poros sistemomis. Ypač daugiausia dėmesio skiriame keturioms nulinės nuotraukos kryptims ir parodomi, kaip daugiakalbis modelis, parengtas naudojant mažus duomenis, gali duoti pagrįstų rezultatų. Be to, mes tiriame, kaip apsisukti (t. y. naudoti tilto ir (arba) apsisukti kalbą, kad būtų galima daryti i švadą source!pivot!target vertimuose) naudojant\ndaugiakalbis modelis gali būti alternatyva, leidžianti nulinį vertimą mažai išteklių.', 'el': 'Η νευρωνική μηχανική μετάφραση έχει αποδειχθεί ότι επιτρέπει την εξαγωγή συμπερασμάτων και τη διασυνοριακή μεταφορά γνώσης σε πολλαπλές γλωσσικές κατευθύνσεις χρησιμοποιώντας ένα μόνο πολύγλωσσο μοντέλο. Εστιάζοντας σε αυτό το πολυγλωσσικό σενάριο μετάφρασης, η παρούσα εργασία συνοψίζει τη συμμετοχή του στο κοινό έργο IWSLT 2017. Οι αιτήσεις μας βασίζονται σε δύο πολύγλωσσα συστήματα εκπαιδευμένα σε πέντε γλώσσες (αγγλικά, ολλανδικά, γερμανικά, ιταλικά και ρουμανικά). Το πρώτο είναι ένα 20γλωσσικό μοντέλο κατεύθυνσης, το οποίο χειρίζεται όλους τους πιθανούς συνδυασμούς των πέντε γλωσσών. Το δεύτερο πολύγλωσσο σύστημα εκπαιδεύεται μόνο σε 16 κατευθύνσεις, αφήνοντας τις άλλες ως κατευθύνσεις μετάφρασης μηδενικής εμβέλειας (δηλαδή αντιπροσωπεύουν μια πιο σύνθετη εργασία συμπερασμάτων σε γλωσσικά ζεύγη που δεν παρατηρούνται κατά τη διάρκεια της εκπαίδευσης). Πιο συγκεκριμένα, οι κατευθύνσεις μηδενικής λήψης είναι ολλανδικά$γερμανικά και ιταλικά$ρουμανικά (με αποτέλεσμα τέσσερις γλωσσικούς συνδυασμούς). Παρά τη μικρή ποσότητα παράλληλων δεδομένων που χρησιμοποιούνται για την εκπαίδευση αυτών των συστημάτων, τα προκύπτουσα πολύγλωσσα μοντέλα είναι αποτελεσματικά, ακόμη και σε σύγκριση με μοντέλα που εκπαιδεύονται ξεχωριστά για κάθε γλωσσικό ζευγάρι (δηλαδή σε ευνοϊκότερες συνθήκες). Συγκρίνουμε και παρουσιάζουμε τα αποτελέσματα των δύο πολύγλωσσων μοντέλων σε σχέση με ένα βασικό σύστημα μονογλωσσικών ζευγαριών. Ειδικότερα, εστιάζουμε στις τέσσερις κατευθύνσεις μηδενικής λήψης και δείχνουμε πώς ένα πολύγλωσσο μοντέλο εκπαιδευμένο με μικρά δεδομένα μπορεί να προσφέρει λογικά αποτελέσματα. Επιπλέον, διερευνούμε πώς η περιστροφή (δηλ. χρήση μιας γλώσσας γεφυρών/περιστροφών για συμπέρασμα σε μεταφράσεις πηγής!\nΤο πολύγλωσσο μοντέλο μπορεί να αποτελέσει μια εναλλακτική λύση για να επιτρέψει τη μετάφραση μηδενικού πυροβολισμού σε μια χαμηλή ρύθμιση πόρων.', 'mt': 'Neural Machine Translation has been shown to enable inference and cross-lingual knowledge transfer across multiple language directions using a single multilingual model.  Filwaqt li tiffoka fuq dan ix-xenarju ta’ traduzzjoni multilingwi, dan ix-xogħol jagħti sommarju tal-parteċipazzjoni tal-FBK fil-kompitu kondiviż tal-IWSLT 2017. Is-sottomissjonijiet tagħna jiddependu fuq żewġ sistemi multilingwi mħarrġa fuq ħames lingwi (Ingliż, Olandiż, Ġermaniż, Taljan u Rumen). L-ewwel wieħed huwa mudell ta’ direzzjoni ta’ 20 lingwa, li jimmaniġġja l-kombinazzjonijiet kollha possibbli tal-ħames lingwi. It-tieni sistema multilingwi hija mħarrġa biss f’16-il direzzjoni, u l-oħrajn jitħallew bħala direzzjonijiet ta’ traduzzjoni mingħajr skop (jiġifieri jirrappreżentaw kompitu ta’ inferenza aktar kumpless fuq pari lingwistiċi li ma jidhrux fil-ħin tat-taħriġ). B’mod aktar speċifiku, id-direzzjonijiet tagħna mingħajr skop huma Olandiżi$Ġermaniżi u$Rumeni$Taljani (li jirriżultaw f’erba’ kombinazzjonijiet lingwistiċi). Minkejja l-ammont żgħir ta’ dejta parallela użata għat-taħriġ ta’ dawn is-sistemi, il-mudelli multilingwi li jirriżultaw huma effettivi, anki meta mqabbla ma’ mudelli mħarrġa separatament għal kull par lingwistiku (jiġifieri f’kundizzjonijiet aktar favorevoli). Aħna nqabblu u nuru r-riżultati taż-żewġ mudelli multilingwi ma’ sistemi ta’ pari ta’ lingwa waħda fil-linja bażi. B’mod partikolari, a ħna niffokaw fuq l-erba’ direzzjonijiet mingħajr skop u nuru kif mudell multilingwi mħarreġ b’dejta żgħira jista’ jipprovdi riżultati raġonevoli. Barra minn hekk, ninvestigaw kif il-pivot (jiġifieri l-użu ta’ lingwa ta’ pont/pivot għall-inferenza f’traduzzjonijiet ta’ sors!pivot!target) juża\nmudell multilingwi jista’ jkun alternattiva biex jippermetti traduzzjoni mingħajr skop f’ambjent baxx ta’ riżorsi.', 'mk': 'Се покажа дека неуралната машинска превод овозможува конференција и трансференција на меѓујазичното знаење преку повеќе јазички насоки користејќи еден мултијазичен модел. Со фокус на овој мултијазичен сценарио за превод, оваа работа го резимира учеството на ФБК во заедничката задача на IWSLT 2017 година. Нашите поднесувања зависат од два мултијазични системи обучени на пет јазици (англиски, холандски, германски, италијански и романски). Првиот е модел на насока на 20 јазици, кој ги води сите можни комбинации на петте јазици. Вториот мултијазичен систем е обучен само во 16 насоки, оставајќи ги другите како нуларни насоки за превод (т.е. претставувајќи покомплексна задача за конференција на јазичките парови кои не се гледаат во време на обуката). Поконкретно, нашите нуларни насоки се холандски долари германски и италијански долари романски (што резултира со четири комбинации на јазик). И покрај малата количина паралелни податоци кои се користат за обуката на овие системи, резултатите на мултијазичните модели се ефикасни, дури и во споредба со моделите обучени одделно за секој јазички пар (т.е. во поголеми услови). We compare and show the results of the two multilingual models against a baseline single language pair systems.  Особено, се фокусираме на четирите нулта насоки и покажуваме како мултијазичен модел обучен со мали податоци може да обезбеди разумни резултати. Покрај тоа, истражуваме како вртење (т.е. користење јазик на мост/вртење за конференција во извор!pivot!target преводи) користење на\nмултијазичкиот модел може да биде алтернатива за овозможување на превод со нула снимка во ниско поставување на ресурси.', 'ms': 'Terjemahan Mesin Neural telah dipaparkan untuk membolehkan kesimpulan dan pemindahan pengetahuan saling bahasa melalui arah berbilang bahasa menggunakan model berbilang bahasa tunggal. Berfokus pada skenario terjemahan berbilang bahasa ini, kerja ini mengungkapkan perkongsian FBK dalam tugas kongsi IWSLT 2017. Penghantaran kami bergantung pada dua sistem berbilang bahasa yang dilatih dalam lima bahasa (Bahasa Inggeris, Belanda, Jerman, Itali, dan Romani). Yang pertama adalah model arah 20 bahasa, yang mengendalikan semua kombinasi yang mungkin dari lima bahasa. Sistem berbilang bahasa kedua hanya dilatih pada 16 arah, meninggalkan yang lain sebagai arah terjemahan-sifar (iaitu mewakili tugas kesimpulan yang lebih kompleks pada pasangan bahasa yang tidak dilihat pada masa latihan). Lebih khusus, arah 0-shot kami adalah Dutch$German dan Italian$Romanian (yang menghasilkan empat kombinasi bahasa). Walaupun jumlah kecil data selari yang digunakan untuk melatih sistem in i, model berbilang bahasa yang menghasilkan adalah berkesan, walaupun dibandingkan dengan model yang dilatih secara terpisah untuk setiap pasangan bahasa (iaitu dalam keadaan yang lebih baik). Kami membandingkan dan menunjukkan hasil dua model berbilang bahasa melawan sistem asas pasangan bahasa tunggal. Terutama, kita fokus pada empat arah tembakan sifar dan menunjukkan bagaimana model berbilang bahasa dilatih dengan data kecil boleh memberikan keputusan yang masuk akal. Selain itu, kami menyelidiki bagaimana pivot (iaitu menggunakan bahasa jambatan/pivot untuk kesimpulan dalam sumber!pivot!target terjemahan) menggunakan\nmodel berbilang bahasa boleh menjadi alternatif untuk benarkan terjemahan-tembakan sifar dalam tetapan sumber rendah.', 'ml': 'ന്യൂറല്\u200d മെഷീന്\u200d പരിഭാഷപ്പെടുത്തിയിരിക്കുന്നു. ഒരു പല ഭാഷ മോഡല്\u200d ഉപയോഗിക്കുന്നതിനുമുള്ള അപരിഹാരം പ്രാവര്\u200dത്തികമാക്കു ഈ പല ഭാഷകങ്ങളുടെ പരിഭാഷയുടെ കാര്യത്തില്\u200d ശ്രദ്ധിക്കുന്നു. ഈ ജോലി എഫ്ബിക്കിന്റെ പങ്കെടുക്കുന്നത് ഐഡബിഎസ്എല്\u200dടി 2017 പങ അഞ്ചു ഭാഷകളില്\u200d പഠിപ്പിക്കപ്പെട്ട രണ്ടു പല ഭാഷ സിസ്റ്റം നമ്മുടെ കീഴ്പ്പെടുത്തിയിരിക്കുന്നു. ഇംഗ്ലീഷ്,  The first one is a 20 language direction model, which handles all possible combinations of the five languages.  രണ്ടാമത്തെ മള്\u200dഭാഷാ സിസ്റ്റം 16 ദിശയില്\u200d മാത്രമേ പരിശീലിക്കപ്പെടുന്നുള്ളൂ, മറ്റുള്ളവരെ പൂജ്യത്തില്\u200d വെടിവെക്കുന്ന വഴികളായി ഉപേക്ഷ കൂടുതല്\u200d പ്രത്യേകിച്ച്, ഞങ്ങളുടെ പൂജ്യ വെടിവെക്കപ്പെട്ട വഴികള്\u200d ഡച്ച് ഡോളര്\u200d ജര്\u200dമ്മന്\u200d ഡോളര്\u200dമാനിയനും ഇറ്റാലിയന്\u200d ഡ ഈ സിസ്റ്റം പരിശീലനത്തിനായി ഉപയോഗിക്കുന്ന അളവിലുള്ള ഡേറ്റായിട്ടും സംഭവിച്ചാലും അതിന്റെ ഫലമായി പല ഭാഷ മോഡലുകള്\u200d പ്രാവര്\u200dത്തമാകുന്നു. എല്ലാ ഭാഷ ജോ നമ്മള്\u200d ഒരു ബേസ്ലൈനിലെ ഒരു ഭാഷ ജോടി സിസ്റ്റമുകള്\u200dക്കെതിരായി രണ്ട് മാള്\u200dട്ടില്\u200d ഭാഷ മോഡലുകളുടെ ഫലങ്ങള്\u200d താല പ്രത്യേകിച്ച്, നമ്മള്\u200d നാല് പൂജ്യത്തിലെ വെടിവെക്കപ്പെട്ട വഴികളിലേക്ക് ശ്രദ്ധിക്കുകയും, ചെറിയ വിവരങ്ങള്\u200d കൊണ്ട് പഠിപ് Furthermore, we investigate how pivoting (i.e using a bridge/pivot language for inference in a source!pivot!target translations) using a\nഒരു കുറഞ്ഞ വിഭവങ്ങളുടെ സജ്ജീകരണത്തില്\u200d സൂക്ഷിപ്പിക്കുന്നതിനുള്ള ഒരു മാറ്റം പല ഭാഷ മാതൃകയാകുന്നു.', 'pl': 'Wykazano, że neuronalne tłumaczenie maszynowe umożliwia wnioskowanie i transfer wiedzy między językami w wielu kierunkach językowych za pomocą jednego wielojęzycznego modelu. Koncentrując się na tym wielojęzycznym scenariuszu tłumaczenia, niniejsza praca podsumowuje udział FBK w wspólnym zadaniu IWSLT 2017. Nasze zgłoszenia opierają się na dwóch wielojęzycznych systemach przeszkolonych w pięciu językach (angielskim, holenderskim, niemieckim, włoskim i rumuńskim). Pierwszym z nich jest 20-sty model kierunku językowego, który obsługuje wszystkie możliwe kombinacje pięciu języków. Drugi wielojęzyczny system jest trenowany tylko na 16-kierunkach, pozostawiając pozostałe jako kierunki tłumaczenia zero-shot (tj. reprezentujące bardziej złożone zadanie wnioskowania na pary językowe niewidziane w czasie szkolenia). Dokładniej, nasze kierunki zero-shot to holenderski$niemiecki i włoski$rumuński (skutkując czterema kombinacjami językowymi). Pomimo niewielkiej ilości danych równoległych wykorzystywanych do szkolenia tych systemów, powstałe modele wielojęzyczne są skuteczne, nawet w porównaniu z modelami szkolonymi oddzielnie dla każdej pary językowej (tj. w bardziej korzystnych warunkach). Porównujemy i pokazujemy wyniki dwóch modeli wielojęzycznych z podstawowymi systemami pary jednoosobowej. W szczególności skupiamy się na czterech kierunkach zero-shot i pokazujemy, jak wielojęzyczny model trenowany z małymi danymi może zapewnić uzasadnione rezultaty. Ponadto badamy, jak pivoting (tj. używanie języka bridge/pivot do wnioskowania w tłumaczeniach source!pivot!target) przy użyciu\nModel wielojęzyczny może być alternatywą umożliwiającą tłumaczenie zero-shot przy niskim poziomie zasobów.', 'mn': 'Цөмийн машины хөгжлийн хөгжлийг олон хэл загварыг ашиглан халдвар болон олон хэл загвар руу шилжүүлэх боломжтой болгодог. Энэ олон хэлний орчуулалтын хувилбарын тухай анхаарлаа, энэ ажил 2017 оны IWSLT-д FBK-ын оролцоог хуваалцах ажлыг илтгэдэг. Бидний сургалтыг таван хэл дээр сургалтын хоёр олон хэл системээр үнэлдэг. Эхний нэг бол 20 хэл чиглэлийн загвар юм. Энэ нь таван хэлний бүх боломжтой нийлүүлэлтийг удирддаг. Хоёр дахь олон хэлний систем нь зөвхөн 16 хэлбэрээр суралцагдсан, бусад хүмүүсийг тэгш хэлбэрээр хөрөнгө оруулах загвар болгож байна (яг л суралцах үед харагдахгүй хэлний хоёр дээр илүү нарийн төвөгтэй Илүү тодорхой нь, бидний 0-р зураг нь Герман болон Италиан доллар румын доллар юм. Эдгээр системүүд сургалтын тулд хэрэглэгддэг жижиг параллел өгөгдлийн хэмжээсүүд ч үр дүнтэй олон хэл загварууд нь үр дүнтэй. Хэдийгээр хэл хоёр бүрийн хувьд сургалтын загваруудыг харьцуулахад (яг л илүү сайн нөх Бид олон хэл загварын үр дүнг нэг хэл хоёр системтэй харьцуулж, харуулж байна. Ялангуяа бид дөрвөн зураг зураг дээр анхаарлаа төвлөрүүлж, жижиг өгөгдлийн сургалтын олон хэл загвар хэрхэн боловсруулж чадна гэдгийг харуулж байна. Дараа нь, бид хэрхэн шийдвэрлэхийг шалгаж байна (яг эх үүсвэрт халдварын тулд көп/pivot хэл хэрхэн ашиглаж байгааг шалгаж байна)\nолон хэл загварын загвар нь бага баялаг боловсруулах боломжтой болгодог.', 'no': 'Nøyralmaskineomsetjing er vist for å slå på infeksjon og krysspråk kunnskap over fleire språkkretningar med ein enkelt multispråk modell. Ved å fokusera på denne fleirspråksomsetjingsformasjonen, samanserer dette arbeidet FBK med deltaka i delt oppgåva IWSLT 2017. Våre søknader er er på to fleirspråksystemet trengte på fem språk (engelsk, nederlandsk, tysk, italsk og rumensk). Den første er eit 20 språk retningsmodul, som handterar alle moglege kombinasjonane av fem språk. Den andre fleirspråkssystemet er berre trengd på 16 retningar, og dei andre vert laga som retningar for omsetjingar med null-shot (t.d. som representerer e i meir komplisert inferens oppgåve på språkspar som ikkje er sett på treningstid). Spesifikke er at våre null- retningar er nederlandsk dollar og italiansk dollar romnisk (som følgjer med fire språkkkombinasjonar). Til tross på det lite mengda parallelle data som vert brukte for å trenja desse systema, er det resulterte fleirspråk modelane effektiv, sjølv i sammengd med modelane som vert trenta separe for kvar språk par (dvs. i meir favorittlege vilkåra). Vi sammenliknar og viser resultatet av dei to fleirspråk modelane mot ein grunnlinje enkelt språk-par-systemet. I særskilt kan vi fokusere på fire retningane med nullsatt, og vise korleis ein fleirspråk modell trent med små data kan gje riktige resultat. I tillegg undersøker vi korleis pivoting (t.d. brukar e in britt/pivotspråk for inferens i ein kjelde!pivot!målsetting) med ein\nFleirspråk modell kan vera ein alternativ for å slå på omsetjing av null- bilde i ei låg ressursinnstilling.', 'ro': 'S-a demonstrat că traducerea automată neurală permite deducerea și transferul de cunoștințe între limbi în mai multe direcții lingvistice folosind un singur model multilingv. Concentrându-se pe acest scenariu de traducere multilingvă, această lucrare rezumă participarea FBK la sarcina comună IWSLT 2017. Cererile noastre se bazează pe două sisteme multilingve instruite pe cinci limbi (engleză, olandeză, germană, italiană și română). Primul este un model de direcție în 20 de limbi, care gestionează toate combinațiile posibile ale celor cinci limbi. Al doilea sistem multilingv este instruit numai pe 16 direcții, lăsând celelalte ca direcții de traducere zero-shot (adică reprezintă o sarcină de deducere mai complexă asupra perechilor de limbi care nu au fost văzute în timpul antrenamentului). Mai exact, direcțiile noastre zero-shot sunt olandeză$germană și italiană$română (rezultând în patru combinații de limbi). În ciuda cantității mici de date paralele utilizate pentru instruirea acestor sisteme, modelele multilingve rezultate sunt eficiente, chiar și în comparație cu modelele instruite separat pentru fiecare pereche de limbi (adică în condiții mai favorabile). Comparăm și prezentăm rezultatele celor două modele multilingve în raport cu sistemele de bază unice perechi de limbi. În special, ne concentrăm pe cele patru direcții zero-shot și arătăm cum un model multilingv instruit cu date mici poate oferi rezultate rezonabile. Mai mult, investigăm modul în care pivotarea (adică folosirea unui limbaj bridge/pivot pentru inferență într-o traducere sursă!pivot!target) folosind un\nModelul multilingv poate fi o alternativă pentru a activa traducerea zero-shot într-o setare cu resurse reduse.', 'sv': 'Neural Machine Translation har visat sig möjliggöra inferens och tvärspråklig kunskapsöverföring över flera språkriktningar med hjälp av en enda flerspråkig modell. Med fokus på detta flerspråkiga översättningsscenario sammanfattar detta arbete FBK:s deltagande i IWSLT 2017 delade uppgift. Våra bidrag bygger på två flerspråkiga system utbildade på fem språk (engelska, nederländska, tyska, italienska och rumänska). Den första är en 20 språkriktning modell, som hanterar alla möjliga kombinationer av de fem språken. Det andra flerspråkiga systemet utbildas endast i 16 riktningar, vilket lämnar de andra som noll-skott översättningsriktningar (dvs. representerar en mer komplex inferensuppgift för språkpar som inte ses vid utbildningstiden). Mer specifikt är våra nollskottsriktningar nederländska$tyska och italienska$rumänska (vilket resulterar i fyra språkkombinationer). Trots den lilla mängden parallella data som används för att träna dessa system, är de resulterande flerspråkiga modellerna effektiva, även i jämförelse med modeller som utbildas separat för varje språkpar (dvs under mer gynnsamma förhållanden). Vi jämför och visar resultaten av de två flerspråkiga modellerna mot ett baslinjesystem för enkelspråkspar. Särskilt fokuserar vi på de fyra nollskottsriktningarna och visar hur en flerspråkig modell utbildad med små data kan ge rimliga resultat. Vidare undersöker vi hur pivoting (dvs att använda ett bridge/pivot språk för inferens i en source!pivot!target översättningar) med hjälp av en\nFlerspråkig modell kan vara ett alternativ för att aktivera noll-shot översättning med en låg resursinställning.', 'ta': 'மொழி மொழிமாற்றத்தை பயன்படுத்தி பல மொழிகளின் மாதிரி வழிகளில் மாற்றும் மொழியின் மொழிமாற்றங்களை செயல்படுத்த முடியும் இந்த பல மொழி மொழி மொழிபெயர்ப்பு காட்சியில் கவனம் செலுத்துகிறது, இந்த வேலை பிபிக்கின் பங்கிட்ட பணியில் பங்கிட்ட FBK பக எங்கள் கட்டளைகள் ஐந்து மொழிகளில் பயிற்சி செய்யப்பட்ட இரண்டு பல மொழிமாற்று அமைப்புகளை நம்புகிறது (ஆங்கிலம், டுந்து, ஜெர் முதல் ஒரு 20 மொழி திசை மாதிரி ஆகும், அது ஐந்து மொழிகளின் அனைத்து சாத்தியமான இணைப்புகளையும் கையாளும். இரண்டாவது பல மொழி அமைப்பு 16 திசைகளில் மட்டுமே பயிற்சி செய்யப்படுகிறது, மற்றவர்களை பூஜ்ஜியமாக- ஷாட் மொழிமாற்றி தேர்வுகளாக விட்டுவிடுகிறத மேலும் குறிப்பிட்டு, எங்கள் பூஜ்ஜியம் சுட்ட திசைகள் டெச்ச் $ஜெர்மன் மற்றும் இத்தாலியன் $ரோமானியன் (நான்கு மொழ இந்த அமைப்புகளை பயிற்சிக்க பயன்படுத்தப்படும் இணைப்பு தரவுகள் சிறிய அளவும், முடிவு பல மொழி மாதிரிகள் செயல்படுகிறது, ஒவ்வொரு மொழி ஜோடிக்கும் தனியாக பயிற்ச நாம் ஒரு மொழி ஜோடி அமைப்புகளுக்கு எதிராக இரு பல மொழி மாதிரிகளின் முடிவுகளை ஒப்பிட்டு காட்டுகிறோம். குறிப்பாக, நாம் நான்கு பூஜ்ஜியத்தின் தேர்வுகளை கவனம் செலுத்தி சிறிய தரவுடன் பயிற்சி மாதிரி எவ்வாறு கொடுக்கப்பட்டு மேலும், நாம் எவ்வாறு பைவோட்டின் மொழி (அதாவது மூலத்தில் பிரஜ்/பிவாட் மொழியை உபயோகிக்கும் மூலம்! pivot!target மொழிபெயர்ப்புகள\nமொழிமூலத்தை குறைந்த மூலத்தின் அமைப்பில் பூஜ்ஜியமாக மொழிமாற்றம் செயல்படுத்த ஒரு மாற்றாக இருக்கலாம்.', 'so': 'Turjumidda baabuurta naafada ah waxaa lagu muujiyey si uu u fududeeyo in aqoonta luuqadaha lagu soo wareejiyo hagitaanka luuqadaha kala duduwan oo lagu isticmaalo tusaale keliya oo luuqad kala duduwan. Focus on scenarimadan turjumista luuqadaha kala duduwan, kaasi shaqadu wuxuu ku summariyaa qayb-qaadashada FBK ee ka qayb-qaadashada shaqada IWSLT 2017. Suuriyadayadu waxay ku xiran yihiin laba nidaam oo luuqado kala duduwan oo lagu tababaray shan luuqadood (Ingiriis, Dutch, Jarmal, Talyaani iyo Romaniyan). Kii ugu horreeya waa tilmaamo ku hagista 20 luuqadood, kaas oo maamula dhammaan isku xirka shanta luqadood oo suurtagal ah. Tirada labaad oo luuqadaha kala duduwan waxaa lagu tababaraa 16 hagis oo kaliya, kuwa kalena waxaa lagu hagaa hagitaanka turjumidda nuqulka (waa tusaale ahaan shaqo ka kooban oo ku saabsan labo luuqadeed oo aan xilliga waxbarashada lagu arag). Si gaar ah, hagitaanadeenna aan zero lagu dhuftay waa Yuhuud Jarmal iyo Talyaani (sababtoo ah afar luuqadood). Inta kastoo ay tirada u yar yihiin macluumaadka lambarka ah oo lagu isticmaali karo waxbarashada nidaamkan, tusaalaha luuqadaha kala duduwan ayaa faa’iido leh, xitaa isbarbarta modellada loo baro si gooni ah labada luqad oo dhan (tusaale ahaan xaaladaha aad u jecel tahay). Waxaynu isbarbardhignaa oo tusnaynaa resultimaha labada tusaale oo luuqad kala duduwan oo ka gees ah nidaamka labada luqad oo kaliya. Si gaar ah, waxaynu ku kalsoonaynaa afartii hagitaan oo noocyo ah, waxaana tusaynaa siduu qaab u baran karo tusaale luuqad kala duduwan oo macluumaad yaru u sameyn karo resulto macquul ah. Furthermore, waxaynu baaritaan sidoo kale codeynta (tusaale ahaan luqada sawirka/pivot-ka-isticmaalka cudurka!pivot!target turjumaan)\nTusaale luuqado kala duduwan waxay noqon kartaa mid bedel ah oo ku habboon tarjumaadda nooca ah oo ku qoran saxda nooca.', 'sr': 'Pokazano je neuronski prevod mašine kako bi omogućio infekciju i prijenos međujezičkih znanja preko višestrukih jezičkih uputa koristeći jednog multijezičkog model a. Fokusirajući se na ovaj scenario multijezičkog prevoda, ovaj rad sažetuje učešće FBK-a u zajedničkom zadatku IWSLT 2017. Naši podaci se oslanjaju na dva multijezička sistema obučena na pet jezika (engleski, holandski, nemački, italijanski i rumunski). Prvi je 20 jezika usmjeren model, koji vodi sve moguće kombinacije pet jezika. Drugi multijezički sistem je obučen samo na 16 smjera, ostavljajući ostale kao upute za prevođenje nula uputa (tj. predstavljajući kompleksniji zadatak infekcije na jezičke parove koje nisu videle u treningu). Posebnije, naša uputa za nulu pucnjavu su njemački i italijanski rumunski dolari (rezultat četiri jezičke kombinacije). Uprkos malam količinom paralelnih podataka koje se koriste za obuku tih sustava, rezultativni multijezički modeli su efikasni, čak i u usporedbi sa modelima obučenim odvojeno za svaki jezički par (tj. u favorilnijim uslovima). Uspoređujemo i pokazujemo rezultate dva multijezičkog modela protiv početnog jedinstvenog jezičkog parova. Posebno, fokusiramo se na četiri uputstva nula i pokažemo kako multijezički model obučen sa malim podacima može pružiti razumne rezultate. Osim toga, istražujemo kako glasanje (tj. koristeći most/pivot jezik za infekciju u izvoru!pivot!meta prevode) koristeći\nMnogi jezički model može biti alternativa da omogućava prevod nula pucnjava u niskom nastavu resursa.', 'si': 'න්\u200dයූරල් මැෂින් පරිවර්තනය පෙන්වන්න පුළුවන් විදියට වැඩි භාෂාවක් පරිවර්තනය සහ වැඩි භාෂාවක් දන මේ ගොඩක් භාෂාවක් වාර්තාවක් සිනාරියෝ, මේ වැඩේ FBK ගේ සම්බන්ධ වැඩක් IWSLT 2017 වාර්තාවේ සම්බන්ධ වැඩක් වෙන් අපේ පිළිබඳුම් විශ්වාස කරන්නේ බොහොම භාෂාවක් පහත් භාෂාවට ප්\u200dරධානය කරලා තියෙන්නේ (ඉංග්\u200dරීසි මුලින්ම භාෂාව ප්\u200dරමාණයක් 20 ක්, ඒකෙන් භාෂාව පහයේ සියළුම සම්බන්ධය කරනවා. දෙවෙනි විශේෂ භාෂාවක් පද්ධතිය ප්\u200dරධාන 16 විතරයි ප්\u200dරධාන කරලා තියෙන්නේ, අනිත් අනිත් අනිවාර්යාත්මක ප්\u200dරධානයක් සුරූ තව විශේෂයෙන්, අපේ ශූර්ණ ශූර්ණ ප්\u200dරතික්\u200dරියාත්මක ප්\u200dරතික්\u200dරියාත්මක ජර්මන් ඩොලර්මාන් සහ ඉතාල මේ පද්ධතිය ප්\u200dරධානය කරන්න පුළුවන් සාමාන්\u200dය දත්ත ප්\u200dරමාණය කරලා තියෙන්නේ නැත්නම්, ප්\u200dරතිචාර භාෂාවික මොඩේල් ප්\u200dරශ්ණයි, හැම භාෂ අපි මුළු භාෂාවක් දෙකක් ගැන ප්\u200dරතිචාරයක් පෙන්වන්න සහ ප්\u200dරතිචාරයක් පෙන්වන්න. විශේෂයෙන්ම, අපි සුන්ධ වෙඩි තියෙන පැත්තේ හතරක් අවධානය කරනවා ඒ වගේම පෙන්වන්න පුළුවන් පොඩි භාෂාවක්  තවත්, අපි පරීක්ෂණය කරනවා කොච්චර පිවෝට් කරන්නේ කොහොමද කියලා\nගොඩක් භාෂාවක් මොඩල් වෙනස් වෙන්න පුළුවන් ශූන්ය- ෂෝට් වාර්තාවක් අවශ්\u200dය කරන්න.', 'ur': 'نئورل ماشین ترجمہ دکھائی گئی ہے کہ ایک متعدد زبان موڈل کے مطابق ضرورت اور کروس زبان علم ترجمہ کو بہت سی زبان کی طرف سے فعال کرے۔ اس multilingual translation scenario پر تمرکز کیا جاتا ہے، اس کام نے FBK کا حصہ IWSLT 2017 میں شریک کام کی ہے. ہمارے مسلمانوں نے پانچ زبانوں پر تعلیم کی دو چند زبان کی سیستم پر اعتماد کیا ہے۔ سب سے پہلے ایک 20 زبان دئیرسی موڈل ہے، جو پانچ زبانوں کی تمام امکانات کی جمع کرتی ہے۔ دوسری ملتی زبان سیسٹم صرف 16 دقیق پر آموزش کی جاتی ہے اور دوسروں کو صفر-شٹ ترجمہ دقیق کے طور پر چھوڑ دیتا ہے اور زیادہ مشخص ہے، ہماری صفر-شٹ کی طرف ڈچ ڈالر ڈالر اور ایتالیایی ڈالر رومین ہیں (چار زبان اتحادیوں کے نتیجہ میں) ۔ ان سیسٹموں کی تعلیم کے لئے استعمال کیے جاتے ہیں، بہت سی زبان کی مدلکوں کے مطابق بھی موجود ہیں، اگرچہ ہر زبان جوڑے کے لئے مختلف طریقے کی آموزش کی مدلکوں کے مطابق بھی موجود ہیں (یعنی بہت اچھی شرایط میں). ہم ان دو ملتی زبان مدل کے نتیجے کو بنیاس لین کی ایک زبان جوڑی سیستم سے دکھاتے ہیں۔ مخصوصاً ہم چار صفر-شٹ کی طرف متمرکز ہوتے ہیں اور دکھاتے ہیں کہ چھوٹے ڈاٹے کے ساتھ تعلیم کی ایک بہت سی زبان مدل کس طرح قابل نتیجے دے سکتا ہے۔ اور اس کے بعد ہم تحقیق کرتے ہیں کہ کس طرح pivoting (i.e. using a bridge/pivot language for inference in a source!pivot!target translations) using a\nبہت سی زبان موڈل کم رسورس تنظیمات میں صفر-شٹ کی ترجمہ کو فعال کرنے کے لئے ایک alternativa ہو سکتا ہے.', 'vi': 'Dịch về máy thần kinh đã được cho thấy có khả năng gây ra và truyền thông ngôn ngữ khác nhau truyền qua nhiều hướng ngôn ngữ bằng một mô hình chung. Tập trung vào kịch bản dịch đa dạng này, công việc này tổng hợp FBK vào công việc chia s ẻ IWSLT Des7. Phụ đề của chúng tôi dựa vào hai hệ thống đa dạng được đào tạo bằng năm ngôn ngữ (Anh, Dutch, Đức, Ý, Rumani). Cái thứ nhất là một mô hình hướng ngôn ngữ 20, nó giải quyết mọi cách kết hợp có thể của năm ngôn ngữ. Các hệ thống ngôn ngữ thứ hai được đào tạo chỉ theo hướng 16, để các phương pháp khác lại thành hướng dịch chuyển không phát (đại diện cho một nhiệm vụ ám ảnh phức tạp hơn về các cặp ngôn ngữ không được nhìn thấy trong thời gian huấn luyện). Cụ thể hơn, phương pháp bắn không là Dutch-German and Italian., Rumani (kết quả là bốn hợp ngôn ngữ). Mặc dù lượng nhỏ các dữ liệu song song được dùng để đào tạo các hệ thống này, các mô-đun đa dạng đều có hiệu quả, thậm chí so với các mô hình được đào tạo riêng cho mỗi cặp ngôn ngữ (tức thì trong đi ều kiện thích hợp hơn). Chúng tôi so sánh và hiển thị kết quả của hai mô- đun đa dạng với một hệ thống đôi ngôn ngữ đơn cơ bản. Đặc biệt, chúng tôi tập trung vào bốn hướng bắn không và cho thấy cách một mô hình đa dạng được đào tạo với dữ liệu nhỏ có thể cung cấp kết quả hợp lý. Hơn nữa, chúng tôi đi ều tra làm thế nào xoay chuyển (dùng ngôn ngữ cầu/pivot để nhận biết nguồn dịch!pivot!mu)\nLoại máy phát ngôn có thể là một cách thay đổi để dịch bằng bắn không trong thiết lập ít tài nguyên.', 'uz': "Name Bu bir necha tillar tarjima scenarida fokuslash mumkin, bu ishni IWSLT 2017 bilan birlashgan vazifani FBK'ning qismlariga qisqaradi. Bizning imkoniyatlarimiz besh tillar tilida o'rganish ikkita tillar tizimga ishlatadi (Inglizcha, Dutch, Olmoncha, Italiya va Rumincha). Birinchi so'zning 20 tilning retori modeli, bu besh tillarda hamma muhim bir birlashtirishni boshqaradi. Ikkinchi xil tillar tizimi faqat 16 yordamda o'rganadi, boshqalarni zero-shog'i tarjima qilish yordamlari sifatida qoladi (balki tilning vaqtda ko'rinadigan vazifani ko'proq murakkab o'zgartiradi). Ko'pchilik, bizning noksiz yordamlarimiz Dutch dollari va Italyancha dollari (to'rt tillar birlashtirilganligi sababli). Bu tizimlarni tahrirlash uchun ishlatilgan parallel maʼlumotlarning kichik qismi bo'lsa, natijada ko'plab tillar modellari effektiv, xato har bir tillar uchun alohida o'rnatilgan modellar bilan birga o'rganadi (balki ko'proq holatda). Biz bir necha til modellarning natijalarini ko'rsamiz va bir necha tilning bir xil tizimi bilan bir xil tizimga ko'ra olamiz. Ko'pchilik, biz to'rtta nuqta yordamlarga foydalanamiz va kichkina maʼlumot bilan bir tildagi model qanday o'rganish mumkin natijalarini yaratadi. Furthermore, we investigate how pivoting (i.e using a bridge/pivot language for inference in a source!pivot!target translations) using a\nName", 'nl': 'Het is aangetoond dat neuronale machinevertaling inferentie en cross-linguale kennisoverdracht in meerdere taalrichtingen mogelijk maakt met behulp van één meertalig model. Dit werk richt zich op dit meertalige vertaalscenario en vat de deelname van FBK aan de gedeelde taak IWSLT 2017 samen. Onze inzendingen zijn gebaseerd op twee meertalige systemen die zijn opgeleid in vijf talen (Engels, Nederlands, Duits, Italiaans en Roemeens). De eerste is een 20-talenrichtingsmodel, dat alle mogelijke combinaties van de vijf talen behandelt. Het tweede meertalige systeem wordt alleen getraind op 16-richtingen, waarbij de andere als nul-shot vertaalrichtingen worden gelaten (d.w.z. een complexere inferentietaak voor taalparen die tijdens de training niet worden gezien). Meer specifiek zijn onze zero-shot richtingen Nederlands$Duits en Italiaans$Roemeens (resulterend in vier taalcombinaties). Ondanks de geringe hoeveelheid parallelle gegevens die voor de training van deze systemen worden gebruikt, zijn de resulterende meertalige modellen effectief, zelfs in vergelijking met modellen die afzonderlijk voor elk taalpaar worden getraind (d.w.z. in gunstigere omstandigheden). We vergelijken en tonen de resultaten van de twee meertalige modellen met een baseline single language pair systemen. We richten ons vooral op de vier zero-shot richtingen en laten zien hoe een meertalig model getraind met kleine data redelijke resultaten kan opleveren. Verder onderzoeken we hoe pivoting (d.w.z. gebruik maken van een bridge/pivot taal voor inferentie in een source!pivot!target vertalingen) met behulp van een\nEen meertalig model kan een alternatief zijn om zero-shot vertaling mogelijk te maken in een lage resource setting.', 'de': 'Die neuronale maschinelle Übersetzung ermöglicht die Inferenz und den sprachübergreifenden Wissenstransfer über mehrere Sprachrichtungen hinweg mit einem einzigen mehrsprachigen Modell. Diese Arbeit konzentriert sich auf dieses mehrsprachige Übersetzungsszenario und fasst die Beteiligung des FBK an der gemeinsamen Aufgabe IWSLT 2017 zusammen. Unsere Einreichungen basieren auf zwei mehrsprachigen Systemen, die in fünf Sprachen trainiert sind (Englisch, Niederländisch, Deutsch, Italienisch und Rumänisch). Das erste ist ein 20-Sprachrichtungsmodell, das alle möglichen Kombinationen der fünf Sprachen behandelt. Das zweite mehrsprachige System wird nur auf 16-Richtungen trainiert, wobei die anderen als Null-Schuss-Übersetzungsrichtungen (d.h. eine komplexere Inferenz-Aufgabe für Sprachpaare darstellen, die zum Zeitpunkt des Trainings nicht gesehen werden). Genauer gesagt sind unsere Null-Schuss-Richtungen Niederländisch$Deutsch und Italienisch$Rumänisch (was zu vier Sprachkombinationen führt). Trotz der geringen Menge an parallelen Daten, die für das Training dieser Systeme verwendet werden, sind die resultierenden mehrsprachigen Modelle effektiv, auch im Vergleich zu Modellen, die für jedes Sprachpaar separat trainiert werden (d.h. unter günstigeren Bedingungen). Wir vergleichen und zeigen die Ergebnisse der beiden mehrsprachigen Modelle mit einem Basissystem für Einzelsprachenpaare. Insbesondere konzentrieren wir uns auf die vier Null-Schuss-Richtungen und zeigen, wie ein mehrsprachiges Modell, das mit kleinen Daten trainiert wird, vernünftige Ergebnisse liefern kann. Des Weiteren untersuchen wir, wie Pivoting (d.h. Verwendung einer Bridge/Pivot-Sprache für Inferenz in einer source!pivot!target-Übersetzung) mittels eines\nEin mehrsprachiges Modell kann eine Alternative sein, um Zero-Shot-Übersetzungen in einer ressourcenarmen Umgebung zu ermöglichen.', 'da': "Neural Machine Translation har vist sig at muliggøre inference og tværsproget vidensoverførsel på tværs af flere sprogretninger ved hjælp af en enkelt flersproget model. Med fokus på dette flersprogede oversættelsesscenario opsummerer dette arbejde FBK's deltagelse i IWSLT 2017 delte opgave. Vores indlæg bygger på to flersprogede systemer uddannet på fem sprog (engelsk, hollandsk, tysk, italiensk og rumænsk). Den første er en 20 sprogretningsmodel, der håndterer alle mulige kombinationer af de fem sprog. Det andet flersprogede system trænes kun i 16 retninger, hvilket efterlader de andre som nulskudsoversættelsesvejledninger (dvs. repræsenterer en mere kompleks slutopgave på sprogpar, der ikke ses på træningstidspunktet). Mere specifikt er vores nulskud retninger hollandsk$tysk og italiensk$rumænsk (hvilket resulterer i fire sprogkombinationer). På trods af den lille mængde parallelle data, der anvendes til at træne disse systemer, er de resulterende flersprogede modeller effektive, selv i sammenligning med modeller, der trænes separat for hvert sprogpar (dvs. under mere gunstige forhold). Vi sammenligner og viser resultaterne af de to flersprogede modeller i forhold til et basissprogepar systemer. Især fokuserer vi på de fire zero-shot retninger og viser, hvordan en flersproget model trænet med små data kan give rimelige resultater. Desuden undersøger vi, hvordan pivoting (dvs. brug af et bridge/pivot sprog til inference i en source!pivot!target oversættelser) ved hjælp af en\nFlersproget model kan være et alternativ til at aktivere nulskudsoversættelse i en indstilling med lav ressource.", 'hr': 'Pokazano je prijevod neuroloških strojeva kako bi omogućio prijenos infekcije i preko jezika znanja u višestrukim jezičkim smjerovima koristeći jednog multijezičkog model a. Fokusirajući se na ovaj multijezički scenario prevoda, ovaj rad sažetuje učešće FBK-a u zajedničkom zadatku IWSLT 2017. Naši podaci se oslanjaju na dva multijezička sustava obučena na pet jezika (engleski, holandski, njemački, italijanski i i rumunski). Prvi je 20 jezika usmjeren model, koji vodi sve moguće kombinacije pet jezika. Drugi multijezički sustav obučen je samo na 16 smjera, ostavljajući ostale kao upute za prevođenje nula uputa (tj. predstavljajući kompleksniji zadatak infekcije na jezičke parove koje nisu vidjele u treningu). Posebnije, naše upute za nulu pucanja su njemački i italijanski rumunski dolari (što je rezultiralo četiri jezičke kombinacije). Uprkos malam količinom paralelnih podataka koji se koriste za obuku tih sustava, rezultacije multijezičkih modela su učinkoviti, čak i u usporedbi s modelima obučenim odvojenim za svaki jezički par (tj. u favorilnijim uvjetima). Uspoređujemo i pokazujemo rezultate dva multijezičkog modela protiv početnog jedinstvenog jezičkog sustava. Posebno, usredotočili smo se na četiri uputa na nulu i pokazali kako multijezički model obučen s malim podacima može pružiti razumne rezultate. Osim toga, istražujemo kako glasanje (tj. koristeći most/pivot jezik za infekciju u izvoru!pivot!meta prevode) koristeći\nmultijezički model može biti alternativa za omogućavanje prevođenja nula pucnjave u niskom nastavu resursa.', 'fa': 'ترجمه ماشین عصبی نشان داده شده است که با استفاده از یک مدل متعدد زبان قابل تغییر ورزش و تغییر علم متعدد زبان باشد. با تمرکز روی این سناریو ترجمه\u200cهای زیادی زبان، این کار مشارکت FBK در کار مشترک IWSLT 2017 را جمع می\u200cکند. تسلیم\u200cهای ما بر دو سیستم\u200cهای متعدد زبان آموزش یافته به پنج زبان (انگلیسی، هلندی، آلمان، ایتالیایی و رومانی) بستگی دارند. اولین یک مدل راهنمایی ۲۰ زبان است که تمام ترکیب\u200cهای ممکن پنج زبان را کنترل می\u200cکند. سیستم دوم چندین زبان تنها در ۱۶ مسیر آموزش داده می\u200cشود و بقیه را به عنوان مسیر ترجمه\u200cهای صفر (یعنی نشان دادن یک کار پیچیده\u200cتری بر جفت زبان که در زمان آموزش ندیده\u200cاند) ترجمه می\u200cکند. دقیقاً مسیرات صفر صفر ما آلمان و ایتالیایی دلار رومانی هستند (به نتیجه ۴ ترکیب زبانی) که به وجود آورده است. با وجود تعداد کوچک داده\u200cهای متفاوتی که برای آموزش این سیستم استفاده می\u200cشود، مدل\u200cهای متعدد زبان\u200cها موثر می\u200cشوند، حتی در مقایسه با مدل\u200cهای متفاوتی که برای هر جفت زبان آموزش داده می\u200cشوند، حتی در مقایسه با مدل\u200cهای متفاوتی برای هر جفت زبان ما نتایج دو مدل متعدد زبان را با یک سیستم جفت زبان بنیادی مقایسه می کنیم و نشان می دهیم. به خصوص، ما روی چهار مسیر صفر تمرکز می کنیم و نشان می دهیم که چگونه یک مدل چند زبان آموزش شده با داده های کوچک می تواند نتایج منطقی را پیشنهاد کند. علاوه بر این، ما تحقیق می\u200cکنیم که چقدر با استفاده از زبان پل/pivot برای آلودگی در یک منبع!pivot!ترجمه هدف استفاده می\u200cکنیم\nمدل\u200cهای زیادی زبان می\u200cتواند جایگزینی باشد که در تنظیمات منابع کم ترجمه\u200cهای صفر را فعال کند.', 'ko': '신경기계번역은 단일한 다언어 모델을 사용하여 여러 언어의 방향에서 추리와 다언어 지식의 이동을 할 수 있다는 것이 증명되었다.이 작업은 이런 다국어 번역 장면에 중점을 두고 FBK가 IWSLT 2017 공유 임무에 참여한 상황을 정리했다.우리가 제출한 문서는 두 개의 다중 언어 시스템에 의존하는데, 각각 다섯 가지 언어 (영어, 네덜란드어, 독일어, 이탈리아어, 루마니아어) 의 교육을 받았다.첫 번째는 20개 언어의 방향 모델로 다섯 개 언어의 모든 가능한 조합을 처리한다.두 번째 다중 언어 시스템은 16개 방향에서만 훈련을 하고 다른 방향은 0렌즈 번역 방향(즉 훈련할 때 보지 못한 언어 대조에서 더욱 복잡한 추리 임무를 나타낸다)이다.더 구체적으로 말하자면, 우리의 제로 포 방향은 네덜란드어 $독일어 $이탈리아어 $루마니아어 (네 가지 언어의 조합을 초래한 것) 이다.비록 이러한 시스템을 훈련하는 데 사용되는 병행 데이터량은 매우 작지만 얻은 다중 언어 모델은 효과적이며, 각 언어를 대상으로 각각 훈련하는 모델에 비해 (즉 더욱 유리한 조건에서) 효과적이다.우리는 두 가지 다중 언어 모델과 기본 단일 언어가 시스템에 대한 결과를 비교하고 보여 주었다.특히 우리는 네 개의 영포 방향을 주목하고 작은 데이터로 훈련된 다국어 모델이 합리적인 결과를 어떻게 제공하는지 보여주었다.그 밖에 우리는 어떻게 사용하는지 연구했다\n다중 언어 모델은 저자원 환경에서 제로 렌즈 번역을 실현하는 대체 방안이 될 수 있다.', 'bg': 'Доказано е, че невралният машинен превод позволява заключения и междуезичен трансфер на знания в няколко езикови посоки, използвайки един многоезичен модел. Фокусирайки се върху този многоезичен сценарий за превод, тази работа обобщава участието на ФБК в споделената задача. Нашите предложения разчитат на две многоезични системи, обучени на пет езика (английски, холандски, немски, италиански и румънски). Първият е модел на посока 20 езика, който обработва всички възможни комбинации от петте езика. Втората многоезична система се обучава само в 16 посоки, оставяйки останалите като нулеви насоки за превод (т.е. представлява по-сложна задача за заключение на езикови двойки, които не се виждат по време на обучение). По-конкретно, нашите нулеви направления са холандски$немски и италиански$румънски (което води до четири езикови комбинации). Въпреки малкото количество паралелни данни, използвани за обучение на тези системи, получените многоезични модели са ефективни дори в сравнение с модели, обучени отделно за всяка езикова двойка (т.е. при по-благоприятни условия). Сравняваме и показваме резултатите от двата многоезични модела с базови системи за единична езикова двойка. По-специално, ние се фокусираме върху четирите нулеви направления и показваме как един многоезичен модел, обучен с малки данни, може да осигури разумни резултати. Освен това, ние изследваме как въртене (т.е. използване на мост / пивот език за заключение в преводи на източник!пивот!таргет) с помощта на\nмногоезичният модел може да бъде алтернатива за разрешаване на нулев превод в настройка на нисък ресурс.', 'id': 'Terjemahan Mesin Neural telah ditunjukkan untuk memungkinkan kesimpulan dan transfer pengetahuan saling bahasa melalui berbagai arah bahasa menggunakan model multibahasa tunggal. Fokus pada skenario terjemahan berbagai bahasa ini, pekerjaan ini mengungkapkan partisipasi FBK dalam tugas bersama IWSLT 2017. Pemberian kami bergantung pada dua sistem berbagai bahasa yang dilatih dalam lima bahasa (Inggris, Belanda, Jerman, Italia, dan Rumania). Yang pertama adalah model arah 20 bahasa, yang menangani semua kombinasi mungkin dari lima bahasa. Sistem multibahasa kedua hanya dilatih pada 16 arah, meninggalkan yang lainnya sebagai arah terjemahan nol (i.e. mewakili tugas inferensi yang lebih kompleks pada pasangan bahasa yang tidak terlihat pada waktu latihan). Lebih spesifik, arah zero-shot kami adalah Dutch$German dan Italian$Romanian (yang berasal dari empat kombinasi bahasa). Meskipun jumlah kecil data paralel yang digunakan untuk melatih sistem-sistem in i, model berbagai bahasa hasilnya efektif, bahkan dibandingkan dengan model yang dilatih secara terpisah untuk setiap pasangan bahasa (i.e. dalam kondisi yang lebih baik). Kami membandingkan dan menunjukkan hasil dari dua model berbagai bahasa melawan sistem dasar sepasang bahasa tunggal. Terutama, kita fokus pada empat arah zero-shot dan menunjukkan bagaimana model berbagai bahasa dilatih dengan data kecil dapat memberikan hasil yang masuk akal. Selain itu, kami menyelidiki bagaimana pivoting (i.e. menggunakan bahasa jembatan/pivot untuk kesimpulan dalam sumber!pivot!target terjemahan) menggunakan\nmodel berbeda bahasa dapat menjadi alternatif untuk memungkinkan terjemahan nol dalam pengaturan sumber daya rendah.', 'sw': 'Tafsiri ya Mashine ya Kijerumani imeonyesha kuwezesha kuhamisha maarifa yanayotokana na lugha mbalimbali kwa kutumia mifano moja ya lugha. Akiangalia tukio hili la kutafsiri lugha mbalimbali, kazi hii inatoa muhtasari wa ushiriki wa FBK katika juhudi la IWSLT 2017. Makala yetu yanategemea mifumo miwili ya lugha mbalimbali yanayofundishwa kwa lugha tano (Kiingereza, Dutch, Ujerumani, Kiitalia, na KiRomania). Kitu cha kwanza ni mtindo wa mwelekeo wa lugha 20, ambao unakabiliana na muunganiko wa lugha tano. Mfumo wa pili wa lugha wa lugha unafundishwa pekee kwa njia 16, na kuwaacha wengine kama maelekezo ya kutafsiri sifuri (yaani wakiwakiwakilisha kazi ya maambukizi magumu juu ya ndoa za lugha ambazo hazikuonekana wakati wa mafunzo). Zaidi zaidi, maelekezo yetu yasiyo na sifuri ni dola za Kiholanzi na Kiitalia (yanasababisha makusanyiko ya lugha nne). Pamoja na kiasi kidogo cha data tofauti kinachotumiwa kwa ajili ya mafunzo ya mfumo huu, mifano ya lugha mbalimbali ni yenye ufanisi, hata ukilinganishwa na mifano inayofundishwa tofauti kwa kila namna ya lugha (yaani katika hali nzuri zaidi). Tunawalinganisha na kuonyesha matokeo ya mifano miwili ya lugha mbalimbali dhidi ya mfumo wa mbili wa lugha moja. hasa, tunajikita kwenye maelekezo manne yasiyo na sifuri na kuonyesha jinsi mtindo wa lugha ulivyofundishwa na data ndogo unavyoweza kutoa matokeo mazuri. Zaidi ya hayo, tunachunguza namna ya upigaji kura (yaani kwa kutumia lugha ya upigaji kura ya daraja/pivo kwa ajili ya uchunguzi katika chanzo!pivot!target) kwa kutumia tafsiri\nMfano wa lugha mbalimbali unaweza kuwa mbadala wa kuwezesha tafsiri yenye risasi sifuri katika mfumo wa rasilimali chini.', 'am': 'የኔural machine ትርጉም በአንድ ብልልቋንቋ ምሳሌ በመጠቀም የቋንቋ ቋንቋዎች እውቀትን ማቀናቀል እና የቋንቋ ቋንቋዎች እውቀት ማቀናቀል ያስችላል፡፡ ይህ በብዙ ቋንቋ ትርጉም ማዕከላዊ ቦታ ላይ በመጠንቀቅ፣ ይህ ሥራ የFBK ትግባር በ2017 በተካፈለው ስራ ላይ ያሳያል፡፡ መልዕክታችን በአምስት ቋንቋዎች (እንግሊዝኛ፣ ድል፣ ጀርመን፣ ጣሊያንኛ እና ሮማኒያን) በሚያስተማሩበት ሁለት የቋንቋ ቋንቋዎች ስርዓቶች ላይ ነው፡፡ የመጀመሪያው የ20 ቋንቋ ምሳሌ ነው፣ የ5 ቋንቋዎች የሚችሉትን ሁሉ የሚቆርጥ ነው፡፡ ሁለተኛው የቋንቋ ቋንቋዎች ስርዓት በ16 መንገዶች ብቻ ያስተምርበታል፤ ሌሎችንም በ0-shot ትርጉም ማቀናጃ ትተዋል (ምናልባት በቋንቋው ሁኔታ ላይ የተጨነቀ ስራ በተማሪ ጊዜ ያልታየው የቋንቋ ሁኔታ ላይ ነው፡፡ በተጨማሪም፣ የzero-shot መንገዳችን የዳርክ ዶላር እና የጣሊያንኛ ዶላር (አራት ቋንቋ ጥያቄ) ነው፡፡ ምንም እንኳን ለቋንቋዎች ሁለት ዓይነቶች በተለየ ተማሪዎች የሚጠቀሙት አካላቢ ዳታ ትንሽ ቢሆን፣ ፍላጎቱ የቋንቋ ቋንቋዎች ዓይነቶች በተለየ ዓይነቶች በተስተያየት እና ለሁሉም ዓይነቶች ተማርተዋል (ምናልባት በመልካም ጉዳይ) የሁለቱን ብዙልቋንቋ ምሳሌዎች በጥያቄ አንዲት ቋንቋ ሁለትን ሥርዓቶች ላይ እናሳየዋለን እናሳየዋለን፡፡ በተለይም፣ በአራቱ የzero-shot መንገዶች ላይ እናሳየዋለን እና ትንሽ ዳታዎችን እንዴት የሚያስተምር የብዙልቋንቋ ሞዴል ያሳየናል፡፡ በተጨማሪም፣ እንዴት እንደተጨማሪው ድምፅ (አዲስ bridge/pivot ቋንቋ በመጠቀም በsource!pivot!target ትርጓሜዎችን) በመጠቀም\nየቋንቋ ቋንቋ ሞዴል በዝቅተኛ resource ማዘጋጀት የzero-shot ትርጉም ለማስችል የሚችል alternative ሆኖ ይችላል።', 'hy': 'Պարզվել է, որ նյարդային մեքենայի թարգմանությունը հնարավորություն է տալիս հետևանքների և լեզվի միջև տեղափոխվող գիտելիքների փոխանցումը բազմաթիվ լեզվի ուղղություններով՝ օգտագործելով մեկ բազլեզվի մո Այս աշխատանքը, կենտրոնացնելով այս բազլեզու թարգմանման սցենարին, համառոտագրում է FBK-ի մասնակցությունը 2017 թվականի IwSMT-ի ընդհանուր խնդրի մեջ: Մեր ներկայացումները հիմնված են երկու բազլեզու համակարգի վրա, որոնք վարժեցվում են հինգ լեզուներով (անգլերեն, հոլանդերեն, գերմաներեն, իտալանդերեն և ռոմաներեն): Առաջինը 20 լեզու ուղղության մոդել է, որը վերահսկում է հինգ լեզուների բոլոր հնարավոր համադրությունները: Երկրորդ բազմալեզու համակարգը պատրաստվում է միայն 16 ուղղությամբ, ուրիշներին թողնելով զրոյական թարգմանման ուղղությամբ (այսինքն՝ ավելի բարդ եզրակացություն ներկայացնելով լեզվի զույգերի համար, որոնք չեն տեսնում պատրաստման ժամանակ): Ավելի հատկապես, մեր զրոյի ուղղությունները հոլանդացի$գերմանացի և իտալանդացի$ռոմանացի են (ինչը հանգեցնում է չորս լեզվի համադրումներին): Չնայած այս համակարգերի ուսումնասիրության համար օգտագործվող զուգահեռ տվյալների փոքր քանակությանը, արդյունքում ունեցող բազմալեզու մոդելները արդյունավետ են, նույնիսկ համեմատելով առանձին ուսումնասիրել մոդելների հետ յուրաքանչյուր լեզվի զուգավորի We compare and show the results of the two multilingual models against a baseline single language pair systems.  Հատկապես, մենք կենտրոնանում ենք չորս զրոյական ուղղությունների վրա և ցույց ենք տալիս, թե ինչպես շատ լեզվով մոդելը, որն ուսուցանված է փոքր տվյալներով, կարող է տրամադրել խելամիտ արդյունքներ: Ավելին, մենք ուսումնասիրում ենք, թե ինչպես է պտտվում (այսինքն՝ կամուրջի և պտտվող լեզու օգտագործելը որպես հետևանք աղբյուրի, պտտվող, նպատակային թարգմանությունների մեջ) օգտագործելով\nբազմալեզու մոդելը կարող է լինել այլընտրանք, որը թույլ կտա զրոյական թարգմանություն ցածր ռեսուրսների միջոցով:', 'bn': 'নিউরেল মেশিন অনুবাদ প্রদর্শন করা হয়েছে একটি মাল্টিভাষা মডেল ব্যবহার করে অনেক ভাষার নির্দেশ দিয়ে অসুসংক্রান্ত এবং ক্রাশ-ভাষায় বি এই বহুভাষায় অনুবাদের দৃষ্টিভঙ্গি নিয়ে মনোযোগ দিচ্ছে, এই কাজের সারসংক্ষিপ্ত করেছে যে এফবিকের অংশগ্রহণ করা হয়েছে ইউএসএলটি  Our submissions rely on two multilingual systems trained on five languages (English, Dutch, German, Italian, and Romanian).  প্রথমটি হচ্ছে ২০ ভাষার দিকের মডেল, যা পাঁচ ভাষার সম্ভাব্য সম্ভাব্য সংযোগের মাধ্যমে কাজ করে। দ্বিতীয় বহুভাষার ব্যবস্থা শুধুমাত্র ১৬টি দিকে প্রশিক্ষণ প্রদান করা হয়, যাতে অন্যদেরকে শুধুমাত্র শুট-গুলি অনুবাদ হিসেবে রেখে যায় (যে আরো বিশেষ করে, আমাদের শূন্যগুলো নির্দেশ হল ডাচ ডলার জার্মান এবং ইতালিয়ান ডলার (যার ফলে চার ভাষার সংযোগ সৃষ্টি)। এই সিস্টেমের প্রশিক্ষণের জন্য ব্যবহৃত সামান্য পারালেল ডাটা সত্ত্বেও, এর ফলে বহুভাষার মডেল কার্যকর, এমনকি প্রত্যেক ভাষার জোড়ার জন্য প্রশিক্ষণ প্রশিক্ষণ আমরা তুলনা করি এবং দুটি মাল্টিভাষার মডেলের ফলাফল দেখাই একটি বেসেলাইনের এক ভাষা জোড়া সিস্টেমের বিরুদ্ধে। বিশেষ করে, আমরা চারটি শুটের দিকে মনোযোগ দিচ্ছি এবং দেখাচ্ছি কিভাবে একটি মাল্টিভাষার মডেল প্রশিক্ষিত ছোট তথ্যের সাথে যুক এছাড়াও, আমরা তদন্ত করি কিভাবে ভোট দিচ্ছি (যেমন একটি সূত্রে আক্রান্ত ব্রিজ/পিভোট ভাষা ব্যবহার করে!\nমাল্টিভাষার মডেল একটি বিকল্প হতে পারে একটি কম সম্পদ সংক্রান্ত অনুবাদ সক্রিয় করার জন্য।', 'az': "Nöral Makina Çevirməsi bir çoxlu dil modeli ilə çoxlu dil tərəflərində infeksiya və çoxlu dil bilgi tərəflərini fəallaşdırmaq üçün göstərildi. Bu çoxlu dil tercümə senaryosuna təsirləndirəcək, bu işin FBK'nin 2017-ci IWSLT vəzifəs in ə bölünməsini təmizləyir. Bizim göndərilərimiz beş dildə təhsil edilən iki çox dil sisteminə təvəkkül edir. İlk kişi, beş dillərin mümkün kombinasyonlarını idarə edir. İkinci çoxlu dil sistemi yalnız 16 tərəflərdə təhsil edilir, digərini sıfır-shot tərcüm tərəflərinə təhsil edir (həm də dil çiftlərinin təhsil vaxtında görmədiyini daha kompleks təsirli təhsil edir). Əksinə, sıfır-fəsad tərəflərimiz Hollandi Dolar və İtalyan Dolar Romalıdır (dörd dil kombinatsiyası olaraq). Bu sistemləri təhsil etmək üçün istifadə edilən kiçik paralel məlumatlar istisna olmasına rağmen, bu məlumatlar çoxlu dil modelləri təhsil edirlər, hətta hər dil çift üçün ayrı-ayrı təhsil edilmiş modellərlə qarşılaşdırılırlar. Biz iki çoxlu dil modelinin sonuçlarını təklif bir dil sistemi ilə qarşılaşdırırıq və göstəririk. Özellikle, biz dörd sıfır tərəflərinə odaklanırıq və küçük məlumatlarla təhsil edilmiş çoxlu dil modeli necə münasibətli sonuçlar verir. Daha əlavə, biz nəyə pivot dilini təşkil edirik (i.e. köprü/pivot dilini təşkil edirik ki, mənbədə!pivot!məqsəd tercümələrini təşkil edirik)\nçoxlu dil modeli düşük ressurs qurğusunda 0-shot tercüməsini qabilleştirmək üçün alternativ olar.", 'tr': "Nural Maşynyň terjimesini birnäçe dil modinde çykyş we owadan dil bilgi transferlerini etkinleştirmek üçin görkezildi. Bu multi dilli terjime senaryýasyna üns bermek üçin bu işi FBK'iň 2017 IWSLT zadynda paýlaşýan işi goşulýar. Biziň görnüşlerimiz beş dilde bilim sistemalara (iňlisçe, holandiýa, nemes, italiýa we rumança) guruldy. Ilkinji adam 20 dil görniş nusgasy, beş dilleriň mümkin kombinasyonlaryny çykýar. Ikinji köp dilli sistemasy diňe 16 görnüşde bilinýär, başgalaryny 0-görnüş terjime edilýän çykyşlar ýaly terjime edilýär (öýdýän, okuwçylyk wagtynda görülmeýän dil çiftlerde has karmaşık täsiri täsir edip biler). Diňe belli bolsa, biziň 0-atly görnüşlerimiz Holländçe$Almança we italiýa$rumança (dört dil birleşişmäne mejbur). Bu sistemalary okuw etmek üçin ullanýan kiçi parallel maglumaty rağmen, netijeli köp dil nusgalary etkinleýär, hatda her dil çift üçin aýratyn bir nusga bilen guruldyrylýan nusgalar bilen (diýip gowy görýän şertlerde). Biz iki çoklu dil nusgynyň netijesini boýunça çykarýarys we görkez. Aýratyn bolsa, biz dört sany zero görnüş yönlerine üns berýäris we kiçi bir maglumat bilen bilim alýan bir multi dilli modeli nähili makul netijeleri berip biler görkezip bileris. Ynha, birnähili pivot dilini ulanmak üçin esasy çykyşyrylýarys\nÇoklu dilli nusga çykyş etmek üçin bir seçenek bolup biler.", 'af': "Name As gefokus op hierdie multitaalse vertaling scenario, hierdie werk opsomming FBK se deelheid in die IWSLT 2017 deel taak. Ons ondersteunings vertrou op twee multitaalse stelsels wat op vyf tale opgelei is (Engels, Dutch, Duits, Italiese en Rumänees). Die eerste een is 'n 20 taal rigting model wat alle moontlike kombinasies van die vyf tale hanteer. Die tweede multitaalske stelsel is slegs op 16 rigtings onderwerp, wat die ander as nulskoot vertaling rigtings verlaat word (i.e. verteenwoordig 'n meer kompleks inferensie taak op taal paar wat nie op onderwerp tyd gesien is nie). More specifically, our zero-shot directions is Dutch$German and Italian$Romanian (resulting in four language combinations). Terwyl die klein hoeveelheid parallel e data gebruik word vir die onderwerp van hierdie stelsels, is die resulteerde multitaal modele effektief, selfs in vergelyking met modele onderwerp aparte vir elke taal paar (bv. in meer gunstelike voorwaardes). Ons vergelyk en wys die resultate van die twee multitaal modele teen 'n basisline enkele taal paar stelsels. Spesiaal, ons fokus op die vier nul-skoot rigtings en wys hoe 'n multi-tale model opgelei met klein data kan redelike resultate verskaf. Ons ondersoek ook hoe pivoting (i.e. gebruik 'n brig/pivottaal vir inferensie in 'n bron!pivot!target translations) gebruik 'n\nmultilingual model kan 'n alternatief wees om nul- skoot vertaling in' n lae hulpbron instelling te aktiveer.", 'sq': 'Përkthimi i Makinës Neurale është treguar për të mundësuar përfundimin dhe transferimin e njohurive ndërgjuhësore nëpërmjet drejtimeve të shumëgjuhës duke përdorur një model të vetëm shumëgjuhës. Duke u përqëndruar në këtë skenar përkthimi shumëgjuhës, ky punë përmbledh pjesëmarrjen e FBK në detyrën e përbashkët të IWSLT 2017. Paraqitjet tona mbështeten në dy sisteme shumëgjuhësore të trajnuar në pesë gjuhë (anglisht, hollandez, gjerman, italian dhe rumun). E para është një model drejtimi 20 gjuhësh, i cili trajton të gjitha kombinimet e mundshme të pesë gjuhëve. Sistemi i dytë shumëgjuhës është trajnuar vetëm në 16 drejtime, duke i lënë të tjerët si drejtime përkthimi zero-shot (pra përfaqëson një detyrë më komplekse inference në çiftet gjuhësh që nuk janë parë në kohën e trajnimit). More specifically, our zero-shot directions are Dutch$German and Italian$Romanian (resulting in four language combinations).  Pavarësisht nga sasia e vogël e të dhënave paralele të përdorura për trajnimin e këtyre sistemeve, modelet shumëgjuhësore që rezultojnë janë efektive, edhe në krahasim me modelet e trajnuara veçanërisht për çdo çift gjuhësh (pra në kushte më të favorshme). Ne krahasojmë dhe tregojmë rezultatet e dy modeleve shumëgjuhësore me një sistem bazë të një çifti gjuhësh të vetme. Veçanërisht, ne përqëndrohemi në katër drejtimet zero-shot dhe tregojmë se si një model shumëgjuhës i trajnuar me të dhëna të vogla mund të ofrojë rezultate të arsyeshme. Përveç kësaj, ne hetojmë se si pivoting (i.e. duke përdorur një gjuhë urë/pivot për përfundim në një burim!pivot!target përkthime) duke përdorur një\nmodeli shumëgjuhës mund të jetë një alternativë për të mundësuar përkthimin zero-shot në një rregullim të ulët burimesh.', 'fi': 'Neuraalisen konekäännöksen on osoitettu mahdollistavan päättelyn ja monikielisen tiedonsiirron eri kielisuuntiin käyttäen yhtä monikielistä mallia. Tässä työssä keskitytään monikieliseen käännöskenaarioon ja tiivistetään FBK:n osallistumista IWSLT 2017:n yhteiseen tehtävään. Tekstimme perustuvat kahteen monikieliseen järjestelmään, jotka on koulutettu viidellä kielellä (englanti, hollanti, saksa, italia ja romania). Ensimmäinen on 20 kielen suuntamalli, joka käsittelee kaikki viiden kielen mahdolliset yhdistelmät. Toinen monikielinen järjestelmä on koulutettu vain 16 suuntaan, jolloin muut jäävät nolla-shot käännössuunnaksi (eli se edustaa monimutkaisempaa päättelytehtävää kielipareille, joita ei ole nähty harjoitushetkellä). Tarkemmin sanottuna nollalaukauksen suuntamme ovat Dutch$German ja Italian$Romanian (tuloksena on neljä kieliyhdistelmää). Huolimatta siitä, että näiden järjestelmien koulutuksessa käytetään vähän rinnakkaistietoja, tuloksena olevat monikieliset mallit ovat tehokkaita, jopa verrattuna malleihin, jotka on koulutettu erikseen jokaiselle kieliparille (eli suotuisammissa olosuhteissa). Vertailemme ja näytämme kahden monikielisen mallin tuloksia yhden kieliparin perustason järjestelmiin. Erityisesti keskitymme neljään nollakuvaussuuntaan ja näytämme, miten pienellä datalla koulutettu monikielinen malli voi tuottaa kohtuullisia tuloksia. Lisäksi tutkimme, miten pivoting (eli bridge/pivot kieli päättelyyn source!pivot!target käännöksissä) käyttäen\nmonikielinen malli voi olla vaihtoehto nolla-shot käännöksen mahdollistamiseen alhaisella resurssilla.', 'et': 'On tõestatud, et neuromasintõlge võimaldab ühe mitmekeelse mudeli abil järeldada ja keeleülest teadmiste edastamist mitmes keelesuunas. Sellele mitmekeelsele tõlketsenaariumile keskendudes võtab töö kokku FBK osalemise IWSLT 2017 jagatud ülesandes. Meie esitused tuginevad kahele mitmekeelsele süsteemile, mis on koolitatud viies keeles (inglise, hollandi, saksa, itaalia ja rumeenia keeles). Esimene on 20 keele suuna mudel, mis käsitleb kõiki võimalikke kombinatsioone viiest keelest. Teist mitmekeelset süsteemi koolitatakse ainult 16 suunas, jättes teised null-shot tõlkesuunadeks (st kujutab endast keerulisemat järeldusülesannet keelepaaride puhul, mida koolituse ajal ei nähta). Täpsemalt, meie null-shot suunad on hollandi$saksa ja itaalia$rumeenia (tulemuseks on neli keelekombinatsiooni). Vaatamata nende süsteemide koolitamiseks kasutatavate paralleelsete andmete väikesele hulgale on tulemuslikud mitmekeelsed mudelid isegi võrreldes iga keelepaari jaoks eraldi koolitatud mudelitega (st soodsamates tingimustes). Me võrdleme ja näitame kahe mitmekeelse mudeli tulemusi ühe keelepaari baassüsteemidega. Eelkõige keskendume neljale null-shot suunale ja näitame, kuidas mitmekeelne mudel, mida koolitatakse väikeste andmetega, võib anda mõistlikke tulemusi. Lisaks uurime, kuidas pöörlemine (st bridge/pivot keele kasutamine järelduseks source!pivot!target tõlgetes) kasutades\nmitmekeelne mudel võib olla alternatiiv nullkatse tõlkimise võimaldamiseks vähese ressursi seadetes.', 'bs': 'Pokazano je prevod neuroloških strojeva kako bi omogućio prijenos infekcije i prijenos međujezičkih znanja preko višestrukih jezičkih uputa koristeći jednog multijezičkog model a. Fokusirajući se na ovaj scenario multijezičkog prevoda, ovaj rad sažetuje učešće FBK-a u zajedničkom zadatku IWSLT 2017. Naši podaci se oslanjaju na dva multijezička sustava obučena na pet jezika (engleski, holandski, njemački, italijanski i i rumunski). Prvi je 20 jezika usmjeren model, koji vodi sve moguće kombinacije pet jezika. Drugi multijezički sistem je obučen samo na 16 smjera, ostavljajući ostale kao upute za prevođenje nula uputa (tj. predstavljajući kompleksniji zadatak infekcije na jezički par koji nije viđen u treningu). Posebnije, naša uputa za nulu pucnjavu su njemački i italijanski rumunski dolari (rezultat četiri kombinacije jezika). Uprkos malam količinom paralelnih podataka koje se koriste za obuku tih sustava, rezultacije multijezičkih modela su efikasne, čak i u usporedbi s modelima obučenim odvojeno za svaki jezički par (tj. u favorilnijim uvjetima). Uspoređujemo i pokazujemo rezultate dva multijezičkog modela protiv početnog jedinstvenog jezičkog parova. Posebno, fokusiramo se na četiri uputstva nula i pokažemo kako multijezički model obučen sa malim podacima može pružiti razumne rezultate. Osim toga, istražujemo kako glasanje (tj. koristeći most/pivot jezik za infekciju u izvoru!pivot!meta prevode) koristeći\nmultijezički model može biti alternativa da omogućava prevod nula uputa u niskom nastavu resursa.', 'ca': "Neural Machine Translation has been shown to enable inference and cross-lingual knowledge transfer across multiple language directions using a single multilingual model.  En centrar-se en aquest escenari de traducció multilingüe, aquesta feina resume la participació de la FBK en la tasca compartida IWSLT 2017. Les nostres proposicions es basan en dos sistemes multilingües entrenats en cinc llengües (anglès, holandès, alemany, italià i rumà). El primer és un model de direcció de 20 llengües, que maneja totes les combinacions possibles de les cinc llengües. El segon sistema multilingüe només està entrenat en 16 direccions, deixant les altres com direccions de traducció a zero tiros (és a dir, representant una tasca de inferència més complexa en parelles de llenguatges no vistes en el temps d'entrenament). Més concretament, les nostres direccions de dispar zero són holandeses,alemanyes i italianes,rumuneses (resultant en quatre combinacions de llenguatges). Malgrat la petita quantitat de dades paralleles que s'utilitzen per formar aquests sistemes, els models multilingües resultants són efectius, fins i tot en comparació amb models formats separadament per cada parell de llengües (i.e. en condicions més favorables). Comparem i ensenyem els resultats dels dos models multilingües amb un sistema basal d'un parell de llenguatges. En especial, ens concentrem en les quatre direccions de zero-fotografia i mostram com un model multilingüe entrenat amb petites dades pot proporcionar resultats raonables. A més, investigam com el pivot (és a dir, utilitzar un llenguatge de pont/pivot per inferència en una traducció source!pivot!target) utilitzant una\nel model multilingüe pot ser una alternativa per permetre una traducció de zero en un entorn de baix recursos.", 'cs': 'Bylo prokázáno, že neuronový strojový překlad umožňuje inferenci a přenos znalostí napříč více jazykovými směry pomocí jediného vícejazyčného modelu. Tato práce se zaměřuje na tento vícejazyčný překladový scénář a shrnuje účast FBK na sdíleném úkolu IWSLT 2017. Naše příspěvky jsou založeny na dvou vícejazyčných systémech školených v pěti jazycích (angličtině, nizozemštině, němčině, italštině a rumunštině). Prvním z nich je 20jazykový směrový model, který zpracovává všechny možné kombinace pěti jazyků. Druhý vícejazyčný systém je trénován pouze na šestnácti směrech, ostatní ponechávají jako nulové směry překladu (tj. představují složitější úlohu inference u jazykových párů, které nebyly viděny v době tréninku). Konkrétněji, naše pokyny nulového snímku jsou nizozemsky$německy a italsky$rumunsky (výsledkem jsou čtyři jazykové kombinace). Navzdory malému množství paralelních dat použitých pro výcvik těchto systémů jsou výsledné vícejazyčné modely efektivní i ve srovnání s modely trénovanými samostatně pro každý jazykový pár (tj. za příznivějších podmínek). Porovnáváme a ukazujeme výsledky dvou vícejazyčných modelů s jednotlivými jazykovými párovými systémy. Zaměřujeme se zejména na čtyři směry nulového snímku a ukazujeme, jak může vícejazyčný model trénovaný s malými daty poskytnout rozumné výsledky. Dále zkoumáme, jak pivotování (tj. použití bridge/pivot jazyka pro inferenci v překladu source!pivot!target) pomocí překladu\nVícejazyčný model může být alternativou k umožnění nulového překladu v nízkém nastavení zdrojů.', 'jv': 'Nyural Mas Terjamahan kang diputamong karo akeh bantuan karo pak-pakan kelas kuwi tindakan karo alêran langkung sampek uga tindakan Ngawe Perintah kanggo ngerasakno multilengkang seneng nggambar, akeh gunakake diputasane FBK ning mulai nggawe IWSLT 1997. Rasané awak dhéwé kuwi sistem sing karo akeh langkung sampeyan sing tukang limo (Inggris, Pak-alaman, Itilyan, lan rumani). Awak dhéwé éntuk sistem sing sampeyan dudu, sing wis ngerasakno ngono akeh basa luwih. Tulung multilengkang sistem punika dipoleh kanggo 16 sampeyan Wuhanjenengan langkung weruh-wae, kita sampeyan 0$Kemerupan karo Gampang Dolanan karo Itilyan (sing paling upat karo akeh basa). Ngayon ngrebut sistem sing paling-perusahaan nganggo akeh dadi, dadi nggawe sistem iki, dadi supoyo model multilenguang sing dirangkap, dadi kapan karo model sing bisa seperakan kanggo sabên langa sampeyan (ta.d. sakjane kapan supoyo kapan). Awak dhéwé nggerarané karo sistem sing nyeasakno karo sistem sing sampeyan karo perusahaan langgar sampeyan. maneh YB\nmodel sing paling-lengkang iso dianggo oleh operasi kanggo ngabah tarjamahan nul', 'he': 'הוראה התרגום של מכונת נוירואלית מאפשר למסקנה ועברת ידע בין שפות לאורך כיוונים רבים בשפה באמצעות מודל רב שפות אחד. התמקדות בתרחיש התרגום המספר שפוי זה, העבודה הזו מסכם את השתתפות של FBK במשימה המשותפת IWSLT 2017. השיחות שלנו תלויות בשני מערכות רבות שפות מאומנות בחמש שפות (אנגלית, הולנדית, גרמנית, איטלקית, רומנית). הראשון הוא דוגמן כיוון של 20 שפות, שמטפל בכל שילובים אפשריים של חמש שפות. המערכת הרב-שפותית השנייה מאומנת רק על 16 כיוונים, משאירה את האחרים כיוונים התרגום אפס (כלומר מייצג משימה מסובכת יותר על זוגות שפות שלא נראות בזמן האימון). במיוחד יותר, הכיוונים האפסים שלנו הם הולנדים גרמנים ואיטלקים רומנים (מה שהוביל בארבעה שילובים שפות). Despite the small amount of parallel data used for training these systems, the resulting multilingual models are effective, even in comparison with models trained separately for every language pair (i.e. in more favorable conditions).  אנחנו משוותים ולהראות את התוצאות של שני הדוגמנים הרב-שפותיים נגד מערכות זוג שפות אחת בסיסית. במיוחד, אנחנו מתמקדים בארבעה הכיוונים של אפס יריות ולהראות איך מודל רב-שפתי מאומן עם נתונים קטנים יכול לספק תוצאות סבירות. חוץ מזה, אנו חוקרים כיצד לפיוט (כלומר להשתמש בשפה גשר/פיוט למסקנה במקור!pivot!target תרגומות) באמצעות\nmultilingual model can be an alternative to enable zero-shot translation in a low resource setting.', 'sk': 'Pokazano je, da nevralni strojni prevod omogoča sklepanje in medjezični prenos znanja v več jezikovnih smereh z uporabo enega večjezičnega modela. S poudarkom na tem večjezičnem prevajalskem scenariju povzema sodelovanje FBK v skupni nalogi IWSLT 2017. Naše prijave temeljijo na dveh večjezičnih sistemih, usposobljenih v petih jezikih (angleščina, nizozemščina, nemščina, italijanščina in romunščina). Prvi je model 20 jezikovnih smeri, ki obravnava vse možne kombinacije petih jezikov. Drugi večjezični sistem se usposablja samo na 16 smereh, ostali pa ostanejo brez smeri prevajanja (tj. predstavlja bolj zapleteno nalogo sklepanja o jezikovnih parih, ki jih v času treninga niso videli). Natančneje, naša ničelna smer sta nizozemski$nemški in italijanski$romunski (kar pomeni štiri jezikovne kombinacije). Kljub majhni količini vzporednih podatkov, ki se uporabljajo za usposabljanje teh sistemov, so nastali večjezični modeli učinkoviti tudi v primerjavi z modeli, ki se usposabljajo ločeno za vsak jezikovni par (tj. v ugodnejših pogojih). Rezultate dveh večjezičnih modelov primerjamo in prikazujemo z osnovnimi sistemi enojnega jezikovnega pare. Osredotočamo se zlasti na štiri smeri brez strela in pokažemo, kako lahko večjezični model, usposobljen z majhnimi podatki, zagotovi razumne rezultate. Poleg tega raziskujemo, kako pivoting (tj. uporaba bridge/pivot jezika za sklepanje v prevodih source!pivot!target) z uporabo\nVečjezični model je lahko alternativa za omogočanje ničelnega prevajanja v nastavitvi z nizkimi viri.', 'ha': "An nuna Translate ɗin Kikake na Naural don ya iya amfani da motsi guda na multi-lingui. Kuran fassarar fassarar multilingular, wannan aikin yana ƙarfafa da shirin FBK a cikin aikin IWSLT 2017 wanda aka raba shi. Salutarmu na dõgara a kan wasu'ummõmi biyu na'ura wanda aka sanar da su a cikin harshen shan lugha (Ingiriya, Dushen, Jarman, Italian, da Rumani). Kida ya farko yana da wata misalin shiryarwa 20 cikin harshe, wanda ke karɓi kullum masu yiwu da ake iya cikin lugha shan. An sanar da na'urar mulki-lingui na farko kawai a kan shiryarwa 16an, kuma yana barin wasu kamar shirin tarjifani na sifuri (misali, yana wakin wani aikin muraja da ya fi tsanani a kan mazaɓan harshe wanda ba su gan a lokacin da za'a yi wa lõkaci). More specifically, our zero-shot directions are Dutch$German and Italian$Romanian (resulting in four language combinations).  Babu ƙarami da yawan data masu daidaita da aka yi amfani da wa wa'anar wannan na'urar, misãlai masu ƙaranci masu amfani da su yana da amfani da, kuma kõ da sami da misãlai wanda aka yi wa shirin su rarraba kowace ga duk harshe (misali, cikin hali masu son zafi). Kana samfani da kuma Muke nũna matsalan misalin biyu masu mulki-lingui don a sami'ura guda da harshe guda. Kayyai, muna kiyãye shiryoyin huɗu na sifiri kuma mu nuna yadda an sanar da misalin multilusi da ƙarami masu da data ƙarami za ta bãyar da matsalar da inganci. Furan haka, Munã yin ƙidãya yadda pivot ke yi (misali da za'a yi amfani da harshen tsohon/pivot wa kasancẽwa cikin wani source!pivot!goat fassarar) na amfani da a\n@ info: whatsthis", 'bo': 'ནུས་ཡིག་ལག་འཁྱེར་གྱི་སྐད་ཡིག སྐད་རིགས་ཀྱི་སྐད་རིགས་འདིའི་དཔེ་བསྐྲུན་འདི་ལ་མཉམ་དུ་བསྡུར་ན། ལས་ཀ་འདིས་མཇུག་བསྡུས་ཡོད་པའི་FBK མཉམ་སྤྱོད་ཀྱི་ལས་ཀ་བ ང་ཚོའི་བསམ་འཆར་ཡོད་པའི་སྐད་ཡིག་ཆའི་མ་ལག་ཆ་གཉིས་ཀྱིས་སྐད་རིགས་ལྔའི་ནང་བསྟུན་ནས་གཙོ་རིམ། དང་པོ་དེ་སྐད་རིགས་གཤམ་ཀྱི་དཔེ་དབྱིབས་༢༠་ཡིན། དེ་ནི་སྐད་རིགས་ཀྱི་ཆ་ཚང་མཉམ་དུ་གཏོང་བྱེད་ཀྱི་ཡོད། སྐད་ཡིག་ཐབས་ལམ་གཉིས་པ་དེ་འདྲ་བའི་འགྲེལ་བཤད་ཀྱི་ཕྱོགས་སྤྱིར་བཏང་བ་ཅིན་16དང་། དེ་ལས་ཀྱང་གཞན་ཚོ་རྣམས་ལྟ་ཀློག་སྐད་ཡིག་གནད་དོན་དག་ཙམ་ ང་ཚོའི་གནས་སྟངས་ལ་ཁྱད་པར་མིན་ཐག་གཟུགས་རིས་ནི་ཇར་མན་$དང་ཨེ་ཊ་ལི་ཡཱན་$རོམ་ཡིག་རེད། མ་ལག་འདི་དག་གི་སྒེར་གྱི་ཚད་ཆུང་ཉུང་བའི་ཚད་ཆུང ང་ཚོས་སྐད་རིགས་དབྱེ་བ་གཉིས་མཉམ་བཟོ་བ་དང་བསྟུན་ནས་དབྱེ་བ་གཉིས་དབྱེ་བ་དག་རེད། དམིགས་བསལ་ན། ང་ཚོས་བློ་རིམ་འདི་༤་གཡིས་སྟོན་པ་ལས་སྐད་རིགས་ཀྱི་མ་དབྱིབས་བཏུབ་པའི་གནས་ཚུལ་ཆ་རྐྱེན་བྱས་ནས་རྐྱེ འོན་ཀྱང་། ང་ཚོས་ཁྱད་པར་གཏོང་ནི་ཇི་ལྟར་ཞིབ་བྱེད་པའི་སྐད་རིགས་ཞིག་ཏུ་བལྟ་ཞིབ་བྱེད།\nསྐད་རིགས་ཀྱི་མ་དབྱིབས་མང་ཆེ་བ་དེ་ཁྱད་པར'}
{'en': 'KIT’s Multilingual Neural Machine Translation systems for IWSLT 2017 KIT ’s Multilingual Neural Machine Translation systems for  IWSLT  2017', 'ar': 'أنظمة الترجمة الآلية العصبية متعددة اللغات لـ KIT لـ IWSLT 2017', 'fr': "Les systèmes de traduction automatique neuronale multilingues de KIT pour l'IWSLT 2017", 'es': 'Sistemas de traducción automática neuronal multilingüe de KIT para IWSLT 2017', 'pt': 'Sistemas de tradução automática neural multilíngue da KIT para IWSLT 2017', 'ja': 'IWSLT 2017のためのKITの多言語ニューラルマシン翻訳システム', 'hi': 'IWSLT 2017 के लिए KIT की बहुभाषी तंत्रिका मशीन अनुवाद प्रणाली', 'ru': 'Набор многоязычных нейронных систем машинного перевода для IWSLT 2017', 'zh': 'KIT 向 IWSLT 2017 多言神经机器翻译系统', 'ga': 'Córais Aistriúcháin Inneall Néaracha Ilteangacha KIT do IWSLT 2017', 'el': 'Πολυγλωσσικά νευρωνικά συστήματα μηχανικής μετάφρασης του KIT για το IWSLT 2017', 'hu': 'KIT többnyelvű idegfordító rendszerei IWSLT 2017-hez', 'ka': 'KIT- ის მრავალენგური ნეირალური მაქსინის განსაგულისხმების სისტემი IWSLT 2017', 'kk': 'KIT- дің IWSLT 2017 жылы көптілік нейрал машинаны аудару жүйелері', 'it': 'Sistemi di traduzione automatica neurale multilingue KIT per IWSLT 2017', 'ml': "KIT's Multilingual Neural Machine Translation System for IWSLT 2017", 'mt': 'Sistemi ta’ Traduzzjoni Multilingwi ta’ Magni Newrali tal-KIT għall-IWSLT 2017', 'mn': 'KIT-ын IWSLT 2017 оны олон хэлний мэдрэлийн машин хөгжүүлэх системүүд', 'lt': 'KIT daugiakalbės neurologinių mašinų vertimo sistemos IWSLT 2017 m.', 'mk': "KIT's Multilingual Neural Machine Translation systems for IWSLT 2017", 'pl': 'Wielojęzyczne systemy tłumaczenia maszynowego KIT dla IWSLT 2017', 'no': 'KIT sin fleirspråk neuralmaskinsomsetjingssystem for IWSLT 2017', 'ms': "KIT's Multilingual Neural Machine Translation systems for IWSLT 2017", 'so': "KIT's Neural machine turjumista systems for IWSLT 2017", 'ro': 'Sisteme de traducere automată neurală multilingvă KIT pentru IWSLT 2017', 'sr': "KIT's Multilingual Neural Machine Translation Systems for IWSLT 2017", 'si': "KIT's Multilanguage neural machine translation systems for IWSLT 2017", 'ta': 'Comment', 'ur': 'IWSLT 2017 کے لئے KIT کی Multilingual Neural Machine Translation System', 'sv': 'KIT:s flersprĂ¥kiga neurala maskinĂ¶versĂ¤ttningssystem fĂ¶r IWSLT 2017', 'uz': 'Comment', 'vi': 'Hệ thống đa ngôn ngữ thần kinh cho IWSLT bây giờ', 'nl': 'Meertalige neurale machinevertaalsystemen van het KIT voor IWSLT 2017', 'da': "KIT's flersprogede neurale maskinoversættelsessystemer til IWSLT 2017", 'hr': "KIT's Multilingual Neural Machine Translation Systems for IWSLT 2017", 'bg': 'Многоезични системи за неврален машинен превод на КИТ за 2017', 'de': 'Mehrsprachige neuronale maschinelle Übersetzung des KIT für IWSLT 2017', 'id': "KIT's Multilingual Neural Machine Translation systems for IWSLT 2017", 'ko': 'KIT IWSLT 2017을 위한 다국어 신경계 번역 시스템', 'sw': 'Mfumo wa Tafsiri wa Mashine ya Kitamaduni wa KiKIT kwa ajili ya IWSLT 2017', 'fa': 'سیستم ترجمه ماشین عصبی Multilingual KIT برای IWSLT 2017', 'tr': "KIT'iň IWSLT 2017-nji ýylyň çoklu dilli Neural Makine terjime sistemleri", 'af': 'KIT se Multilingual Neural Masjien Vertaling stelsels vir IWSLT 2017', 'sq': "KIT's Multilingual Machine Translation Systems for IWSLT 2017", 'am': 'የKIT Multilingual Neural machine Translation systems for IWSLT 2017', 'hy': "KIT's Multilingual Neural Machine Translation systems for IWSLT 2017", 'bn': "KIT'র মাল্টিভাষার নিউরাল মেশিন অনুবাদ সিস্টেম ইউএসএলটি ২০১৭", 'bs': "KIT's Multilingual Neural Machine Translation Systems for IWSLT 2017", 'ca': 'Els sistemes de traducció multilingüe de màquines neuronals de KIT per IWSLT 2017', 'et': 'KIT mitmekeelsed neuroaalse masintõlke süsteemid IWSLT 2017 jaoks', 'az': "KIT'in Multilingual Neural Machine Translation Systems for IWSLT 2017", 'cs': 'Vícejazyčné neuronové strojové překlady KIT pro IWSLT 2017', 'fi': 'KIT:n monikieliset hermojen konekäännösjärjestelmät IWSLT 2017:lle', 'jv': "KiT's Multilanguage Neral Mas Terjamahan kanggo IWSLT 1997", 'he': 'מערכות התרגום של מכונות נוירויות רבות של KIT עבור IWSLT 2017', 'ha': 'KCharselect unicode block name', 'sk': 'KIT-ovi večjezični nevralni strojni prevajalski sistemi za IWSLT 2017', 'bo': "KIT's Multilingual Neural Machine Translation System for IWSLT 2017"}
{'en': 'In this paper, we present KIT’s multilingual neural machine translation (NMT) systems for the IWSLT 2017 evaluation campaign machine translation (MT) and spoken language translation (SLT) tasks. For our MT task submissions, we used our multi-task system, modified from a standard attentional neural machine translation framework, instead of building 20 individual NMT systems. We investigated different  architectures  as well as different  data corpora  in training such a multilingual system. We also suggested an effective adaptation scheme for  multilingual systems  which brings great improvements compared to  monolingual systems . For the SLT track, in addition to a monolingual neural translation system used to generate correct punctuations and true cases of the data prior to training our multilingual system, we introduced a noise model in order to make our system more robust. Results show that our novel modifications improved our  systems  considerably on all  tasks .', 'ar': 'في هذا البحث ، نقدم أنظمة الترجمة الآلية العصبية متعددة اللغات (NMT) الخاصة بـ KIT لمهام الترجمة الآلية لحملة التقييم IWSLT 2017 وترجمة اللغة المنطوقة (SLT). بالنسبة لعمليات إرسال مهام MT الخاصة بنا ، استخدمنا نظامنا متعدد المهام ، المعدل من إطار عمل ترجمة آلي عصبي معياري ، بدلاً من بناء 20 نظام NMT فردي. لقد بحثنا في بنى مختلفة وكذلك مجموعة بيانات مختلفة في تدريب مثل هذا النظام متعدد اللغات. لقد اقترحنا أيضًا خطة تكيف فعالة للأنظمة متعددة اللغات والتي تحقق تحسينات كبيرة مقارنة بالأنظمة أحادية اللغة. بالنسبة لمسار SLT ، بالإضافة إلى نظام الترجمة العصبية أحادي اللغة المستخدم لإنشاء علامات الترقيم الصحيحة والحالات الحقيقية للبيانات قبل تدريب نظامنا متعدد اللغات ، قدمنا نموذج الضوضاء من أجل جعل نظامنا أكثر قوة. تظهر النتائج أن التعديلات الجديدة التي أجريناها حسّنت أنظمتنا بشكل كبير في جميع المهام.', 'pt': 'Neste artigo, apresentamos os sistemas de tradução automática neural (NMT) multilíngue do KIT para as tarefas de tradução automática (MT) e tradução de linguagem falada (SLT) da campanha de avaliação do IWSLT 2017. Para nossos envios de tarefas de MT, usamos nosso sistema multitarefa, modificado a partir de uma estrutura de tradução automática neural de atenção padrão, em vez de construir 20 sistemas NMT individuais. Investigamos diferentes arquiteturas, bem como diferentes corpora de dados no treinamento de um sistema multilíngue. Sugerimos também um esquema de adaptação eficaz para sistemas multilíngues que traz grandes melhorias em relação aos sistemas monolíngues. Para a trilha SLT, além de um sistema de tradução neural monolíngue usado para gerar pontuações corretas e casos verdadeiros dos dados antes de treinar nosso sistema multilíngue, introduzimos um modelo de ruído para tornar nosso sistema mais robusto. Os resultados mostram que nossas novas modificações melhoraram consideravelmente nossos sistemas em todas as tarefas.', 'fr': "Dans cet article, nous présentons les systèmes de traduction automatique neuronale (NMT) multilingues de KIT pour les tâches de traduction automatique (MT) et de traduction parlée (SLT) de la campagne d'évaluation IWSLT 2017. Pour nos soumissions de tâches de TA, nous avons utilisé notre système multitâche, modifié à partir d'un framework de traduction automatique neuronale attentionnelle standard, au lieu de créer 20 systèmes NMT individuels. Nous avons étudié différentes architectures ainsi que différents corpus de données lors de la formation d'un tel système multilingue. Nous avons également suggéré un système d'adaptation efficace pour les systèmes multilingues qui apporte de grandes améliorations par rapport aux systèmes monolingues. Pour la piste SLT, en plus d'un système de traduction neuronale monolingue utilisé pour générer des ponctuations correctes et des cas réels des données avant d'entraîner notre système multilingue, nous avons introduit un modèle de bruit afin de rendre notre système plus robuste. Les résultats montrent que nos nouvelles modifications ont considérablement amélioré nos systèmes pour toutes les tâches.", 'es': 'En este artículo, presentamos los sistemas de traducción automática neuronal (NMT) multilingüe de KIT para las tareas de traducción automática (MT) y traducción del lenguaje hablado (SLT) de la campaña de evaluación IWSLT 2017. Para nuestros envíos de tareas de MT, utilizamos nuestro sistema multitarea, modificado a partir de un marco de traducción automática neuronal atencional estándar, en lugar de construir 20 sistemas NMT individuales. Investigamos diferentes arquitecturas, así como diferentes cuerpos de datos en la formación de un sistema multilingüe de este tipo. También sugerimos un esquema de adaptación eficaz para los sistemas multilingües que aporta grandes mejoras en comparación con los sistemas monolingües. Para la pista SLT, además de un sistema de traducción neuronal monolingüe que se utiliza para generar puntuaciones correctas y casos reales de los datos antes de entrenar nuestro sistema multilingüe, introdujimos un modelo de ruido para hacer que nuestro sistema sea más robusto. Los resultados muestran que nuestras modificaciones novedosas mejoraron considerablemente nuestros sistemas en todas las tareas.', 'ja': '本稿では， IWSLT 2017評価キャンペーン機械翻訳（ MT ）及び口語翻訳（ SLT ）タスクのためのKITの多言語ニューラル機械翻訳（ NMT ）システムを紹介する． MTタスクの提出には、20の個別のNMTシステムを構築する代わりに、標準的な注意神経機械翻訳フレームワークから修正されたマルチタスクシステムを使用しました。このような多言語システムのトレーニングにおいて、さまざまなアーキテクチャとさまざまなデータ組織を調査しました。また、単一言語システムと比較して大幅な改善をもたらす多言語システムの効果的な適応スキームを提案した。SLTトラックでは、多言語システムをトレーニングする前に、データの正しい句読点と真のケースを生成するために使用される単語ニューラル翻訳システムに加えて、システムをより堅牢にするためにノイズモデルを導入しました。結果は、当社の新規改造により、すべてのタスクでシステムが大幅に改善されたことを示しています。', 'zh': '本文引KIT多语言神经机器翻译(NMT)系统,以IWSLT 2017评估机器翻译(MT)口语译(SLT)。 于我机器翻译务,我用我多任务统,于我神经机器翻译框架改,非构20独NMT也。 臣等考练此多语言系统时不同架构及不同数语料库。 又立有效者多言系统适方,比于单语系统,其大改也。 其于SLT轨道,除于训练我们的多言语系统之前生成正标点符号和真数例的单语神经译系统之外,我们还引入了一个噪声模样,以使我们的系统更加健壮。 结果表明,新改大改。', 'hi': 'इस पेपर में, हम आईडब्ल्यूएसएलटी 2017 मूल्यांकन अभियान मशीन अनुवाद (एमटी) और बोली जाने वाली भाषा अनुवाद (एसएलटी) कार्यों के लिए किट के बहुभाषी तंत्रिका मशीन अनुवाद (एनएमटी) सिस्टम पेश करते हैं। हमारे एमटी कार्य प्रस्तुतियों के लिए, हमने अपने बहु-कार्य प्रणाली का उपयोग किया, जो 20 व्यक्तिगत एनएमटी सिस्टम के निर्माण के बजाय एक मानक चौकस तंत्रिका मशीन अनुवाद ढांचे से संशोधित किया गया था। हमने इस तरह के बहुभाषी प्रणाली को प्रशिक्षित करने में विभिन्न आर्किटेक्चर के साथ-साथ विभिन्न डेटा कॉर्पोरेट की जांच की। हमने बहुभाषी प्रणालियों के लिए एक प्रभावी अनुकूलन योजना का भी सुझाव दिया जो मोनोलिंगुअल सिस्टम की तुलना में महान सुधार लाता है। एसएलटी ट्रैक के लिए, हमारे बहुभाषी सिस्टम को प्रशिक्षित करने से पहले डेटा के सही विराम चिह्नों और सच्चे मामलों को उत्पन्न करने के लिए उपयोग की जाने वाली एक मोनोलिंगुअल तंत्रिका अनुवाद प्रणाली के अलावा, हमने अपने सिस्टम को और अधिक मजबूत बनाने के लिए एक शोर मॉडल पेश किया। परिणाम बताते हैं कि हमारे उपन्यास संशोधनों ने सभी कार्यों पर हमारे सिस्टम में काफी सुधार किया है।', 'ru': 'В этой статье мы представляем системы многоязычного нейронного машинного перевода (NMT) KIT для задач машинного перевода (MT) и перевода на устный язык (SLT) оценочной кампании IWSLT 2017. Для представления наших задач MT мы использовали нашу многозадачную систему, модифицированную из стандартной структуры нейронного машинного перевода внимания, вместо того, чтобы строить 20 отдельных систем NMT. Мы исследовали различные архитектуры, а также различные корпуса данных при обучении такой многоязычной системе. Мы также предложили эффективную схему адаптации для многоязычных систем, которая приносит значительные улучшения по сравнению с одноязычными системами. Для трека SLT, в дополнение к одноязычной нейронной системе перевода, используемой для генерации правильных знаков препинания и истинных случаев данных до обучения нашей многоязычной системы, мы ввели модель шума, чтобы сделать нашу систему более надежной. Результаты показывают, что наши новые модификации значительно улучшили наши системы по всем задачам.', 'ga': "Sa pháipéar seo, cuirimid i láthair córais ilteangacha néaraistriúcháin meaisín (NMT) KIT le haghaidh tascanna aistriúcháin meaisín (MT) agus aistriúcháin teanga labhartha (SLT) feachtas meastóireachta IWSLT 2017. Le haghaidh ár n-aighneachtaí tasc MT, d'úsáideamar ár gcóras il-tasc, arna mhodhnú ó chreat caighdeánach aistriúcháin meaisín néarachais aird, in ionad 20 córas NMT aonair a thógáil. Rinneamar imscrúdú ar ailtireachtaí éagsúla chomh maith le corpora sonraí difriúla chun córas ilteangach den sórt sin a oiliúint. Mholamar freisin scéim oiriúnaithe éifeachtach do chórais ilteangacha a thugann feabhsuithe móra i gcomparáid le córais aonteangacha. Maidir leis an rian SLT, chomh maith le córas néar-aistriúcháin aonteangach a úsáidtear chun poncanna cearta agus cásanna fíora de na sonraí a ghiniúint sular cuireadh oiliúint ar ár gcóras ilteangach, thugamar isteach múnla torainn chun ár gcóras a dhéanamh níos láidre. Léiríonn torthaí gur chuir ár modhnuithe úra feabhas mór ar ár gcórais ar gach tasc.", 'ka': 'ამ დომენტში ჩვენ KIT-ის მრავალენგური ნეიროლური მანქანის გაგრძელება (NMT) სისტემებისთვის IWSLT 2017 წარმოადგენის მანქანის გაგრძელება (MT) და წარმოადგენული ენის გაგრძელება ჩვენი MT სამუშაო სამუშაო დამუშაობისთვის ჩვენი მრავალ სამუშაო სისტემის გამოყენება, სტანდარტატური ნეიროლური სამუშაო დამუშაობისთვის შეცვლა ჩვენ განსხვავებული არქტიქტურების შესახებ და განსხვავებული მონაცემების კოპორაციაში ასეთი მრავალენგური სისტემის შესახებ. ჩვენ შეგიძლიათ მრავალენგური სისტემებისთვის ეფექტიური აკაპტიფიკაციის სქემი, რომელიც მონოლენგური სისტემებისთვის დიდი უფრო მეტად SLT სისტემაში, მონოლენგური ნეირალური გადარგზავნა სისტემას დამატებით, რომელიც გამოყენებულია მონაცემების მსგავსი სისტემას შემდეგ მრავალენგური სისტემას შემდეგ, ჩვენ ჩვენი სისტემას უფრო ძალ წარმოდგენები ჩვენი პრომენტის შესახებ ჩვენი სისტემის შესახებ ყველა სამუშაოში უფრო მნიშვნელოვანია.', 'hu': 'Ebben a tanulmányban bemutatjuk a KIT többnyelvű neurális gépi fordító (NMT) rendszereit az IWSLT 2017 értékelési kampány gépi fordítási (MT) és beszélt nyelvű fordítási (SLT) feladataihoz. MT feladatok benyújtásához 20 egyedi NMT rendszer építése helyett egy standard figyelemmel kísérő neurális gépi fordítási keretrendszerből módosított multi-task rendszert használtunk. Egy ilyen többnyelvű rendszer képzése során különböző architektúrákat és különböző adatcsoportokat vizsgáltunk. Javasoltuk továbbá a többnyelvű rendszerek hatékony alkalmazkodási rendszerét, amely jelentős javulást hoz az egynyelvű rendszerekhez képest. Az SLT sáv esetében a többnyelvű rendszerünk kiképzése előtt a helyes írásjelek és az adatok valós esetei generálására használt egynyelvű neurális fordítási rendszer mellett zajmodellt vezettünk be a rendszerünk robusztusabbá tétele érdekében. Az eredmények azt mutatják, hogy új módosításaink minden feladatban jelentősen javították rendszereinket.', 'el': 'Στην παρούσα εργασία, παρουσιάζουμε τα πολύγλωσσα συστήματα νευρωνικής μηχανικής μετάφρασης (ΝΜΤ) του KIT για τις εργασίες της καμπάνιας αξιολόγησης της μηχανής μετάφρασης (ΜΤ) και της προφορικής γλώσσας (SLT). Για τις υποβολές εργασιών μας, χρησιμοποιήσαμε το σύστημα πολλαπλών εργασιών μας, τροποποιημένο από ένα τυποποιημένο πλαίσιο νευρολογικής μηχανικής μετάφρασης προσοχής, αντί να χτίσουμε 20 μεμονωμένα συστήματα NMT. Ερευνήσαμε διαφορετικές αρχιτεκτονικές καθώς και διαφορετικά σώματα δεδομένων στην εκπαίδευση ενός τέτοιου πολυγλωσσικού συστήματος. Προτείναμε επίσης ένα αποτελεσματικό πρόγραμμα προσαρμογής για πολυγλωσσικά συστήματα το οποίο επιφέρει μεγάλες βελτιώσεις σε σύγκριση με τα μονογλωσσικά συστήματα. Για το κομμάτι εκτός από ένα μονογλωσσικό σύστημα νευρωνικής μετάφρασης που χρησιμοποιείται για την παραγωγή σωστών στίξεων και πραγματικών περιπτώσεων των δεδομένων πριν από την εκπαίδευση του πολύγλωσσου συστήματός μας, εισαγάγαμε ένα μοντέλο θορύβου προκειμένου να καταστήσουμε το σύστημά μας πιο ανθεκτικό. Τα αποτελέσματα δείχνουν ότι οι νέες τροποποιήσεις μας βελτίωσαν σημαντικά τα συστήματά μας σε όλες τις εργασίες.', 'it': 'In questo articolo presentiamo i sistemi di traduzione automatica neurale multilingue (NMT) di KIT per le attività di traduzione automatica (MT) e traduzione in lingua parlata (SLT) della campagna di valutazione IWSLT 2017. Per i nostri compiti MT abbiamo utilizzato il nostro sistema multi-task, modificato da un framework di traduzione automatica neurale di attenzione standard, invece di costruire 20 sistemi NMT individuali. Abbiamo studiato diverse architetture e diversi data corpora nella formazione di un sistema multilingue. Abbiamo anche proposto un efficace sistema di adattamento dei sistemi multilingue che apporta notevoli miglioramenti rispetto ai sistemi monolingue. Per la traccia SLT, oltre a un sistema di traduzione neurale monolingue utilizzato per generare punteggiature corrette e casi reali dei dati prima di formare il nostro sistema multilingue, abbiamo introdotto un modello di rumore per rendere il nostro sistema più robusto. I risultati dimostrano che le nostre nuove modifiche hanno migliorato notevolmente i nostri sistemi in tutte le attività.', 'kk': 'Бұл қағазда, біз IWSLT 2017 бағалау кампаниясының (MT) және сөйлейтін тіл аудару (SLT) тапсырмаларының KIT көптеген невралдық машинаның аудару жүйелерін көрсетедік. MT тапсырмаларымызды жіберу үшін біз көптеген тапсырмаларды жүйеңізді қолдандық, стандартты невралдық компьютердің аудару фреймінен өзгертілген, 20 жеке NMT жүйесін құру орнына. Біз бірнеше архитектураларды және бірнеше деректер корпорасын бірнеше тілдік жүйесінде зерттедік. Мұндай-ақ біз бірнеше тілді жүйелер үшін ең жақсы жақсарту сұлбаны таңдадық. Бұл бірнеше тілді жүйелерде қалыпты жақсартуларды салыстырады. SLT жолында, бірнеше тілдік жүйеңізді оқыту алдында, бірнеше тілдік жүйеңізді оқыту үшін, жүйеңізді дұрыс түрлендіру үшін бірнеше тілдік аудару жүйесіне қолданылатын невралдық аудару жүйесіне қосымша Нәтижелер біздің романдық өзгертулеріміз жүйелерімізді барлық тапсырмаларда жақсы жасады.', 'ml': 'ഈ പത്രത്തില്\u200d നമ്മള്\u200d കിട്ടിയിരിക്കുന്നത് ഐഡഎസ്എല്\u200dടി 2017 പ്രകാരം മെഷീന്\u200d പരിഭാഷ (എംടി) എന്നിട്ടും സംസാരിക്കുന്ന ഭാഷ പരിഭാഷകള്\u200dക്കും കിട്ടുന്നു. ഞങ്ങളുടെ MT ജോലിയുടെ അടിസ്ഥാനങ്ങള്\u200dക്ക്, ഞങ്ങള്\u200d ഞങ്ങളുടെ പല-ജോലി സിസ്റ്റം ഉപയോഗിച്ചു, സാധാരണ ശ്രദ്ധിക്കുന്ന ന്യൂറല്\u200d മെഷീന്\u200d പരിശോധി വ്യത്യസ്ത്രീകങ്ങളും വ്യത്യസ്ത ഡേറ്റാ കോര്\u200dപ്പോറ്ററുമായി ഞങ്ങള്\u200d അന്വേഷിച്ചുകൊണ്ടിരുന്നു. ഒരു പല ഭാ പല ഭാഷകങ്ങളുടെ സിസ്റ്റങ്ങള്\u200dക്കുള്ള ഒരു പ്രായോഗികമായ ആഡ്പാപ്റ്റേഷന്\u200d പദ്ധതിയും ഞങ്ങള്\u200d നിര്\u200dദേശിച്ചിരുന്നു.  For the SLT track, in addition to a monolingual neural translation system used to generate correct punctuations and true cases of the data prior to training our multilingual system, we introduced a noise model in order to make our system more robust.  ഫലങ്ങള്\u200d കാണിക്കുന്നത് നമ്മുടെ നോവല്\u200d മാറ്റങ്ങള്\u200d നമ്മുടെ സിസ്റ്റത്തിന്റെ മുന്\u200dഗണന മാറ്റങ്ങള്\u200d എല്ലാ', 'mt': "F’dan id-dokument, qed nippreżentaw is-sistemi multilingwi tat-traduzzjoni tal-magni newrali (NMT) tal-KIT għall-kompiti tat-traduzzjoni tal-magni tal-kampanja ta’ evalwazzjoni tal-IWSLT 2017 (MT) u tat-traduzzjoni tal-lingwa mitkellma (SLT). Għas-sottomissjonijiet tal-kompiti MT tagħna, użajna s-sistema multikompiti tagħna, modifikata minn qafas standard ta’ attenzjoni għat-traduzzjoni tal-magni newrali, minflok bini ta’ 20 sistema individwali NMT. Investigajna arkitetturi differenti kif ukoll korpi tad-dejta differenti fit-taħriġ ta’ sistema multilingwi bħal din. Aħna ssuġġeriejna wkoll skema effettiva ta' adattament għas-sistemi multilingwi li ġġib titjib kbir meta mqabbel mas-sistemi monolingwi. Għall-binarju SLT, minbarra sistema ta’ traduzzjoni newrali monolingwi użata biex tiġġenera punti korretti u każijiet veri tad-dejta qabel it-taħriġ tas-sistema multilingwi tagħna, introduċejna mudell ta’ storbju sabiex is-sistema tagħna ssir aktar robust a. Ir-riżultati juru li l-modifiki l-ġodda tagħna tejbu s-sistemi tagħna konsiderevolment fuq il-kompiti kollha.", 'ms': 'Dalam kertas ini, kami memperkenalkan sistem penerjemah mesin saraf berbilang bahasa (NMT) KIT untuk tugas penerjemah mesin kempen penilaian IWSLT 2017 (MT) dan penerjemah bahasa bercakap (SLT). Untuk penghantaran tugas MT kami, kami menggunakan sistem multi-tugas kami, diubahsuai dari kerangka penerjemah mesin saraf perhatian piawai, selain daripada membina 20 sistem NMT individu. Kami menyelidiki arkitektur yang berbeza serta korpra data yang berbeza dalam latihan seperti sistem berbilang bahasa. Kami juga menyarankan skema penyesuaian yang berkesan untuk sistem berbilang bahasa yang membawa peningkatan yang besar dibandingkan dengan sistem monobahasa. Untuk trek SLT, selain daripada sistem terjemahan saraf monobahasa yang digunakan untuk menghasilkan titik yang betul dan kes yang betul data sebelum melatih sistem berbilang bahasa kami, kami memperkenalkan model bunyi untuk membuat sistem kami lebih kuat. Results show that our novel modifications improved our systems considerably on all tasks.', 'mn': 'Энэ цаасан дээр бид KIT-ийн олон хэлний мэдрэлийн машины хөгжүүлэлтийг (NMT) IWSLT 2017 оны оюутнуудын машины хөгжүүлэлтийг (MT) болон хэлний хөгжүүлэлтийг (SLT) ашиглаж байна. MT-ийн ажлын хэвлэлийн тулд бид олон ажлын системийг ашиглаж, стандарт сонирхолтой мэдрэлийн механикийн хэвлэлийн хэлбэрээс өөрчлөгдсөн. НMT-ийн системийг бүтээх оронд 20 хүн байдаг. Бид өөр архитектуруудыг, мөн олон хэл системийн сургалтын тулд өөр өөр өгөгдлийн корпора судалсан. Бид мөн олон хэл системийн эффективны адилтгал загварыг санал болгосон. Энэ нь нэг хэл системтэй харьцуулахад маш сайжруулалт гаргадаг. Олон хэлний системийг суралцахаас өмнө зөв тохиолдол болон үнэн тохиолдол бүтээхэд нэг хэлний мэдрэлийн хөгжүүлэлтийн системийн нэмэлт бид системийг илүү хүчтэй болгодог. Үр дүнд бидний шинэ өөрчлөлт бидний системийг бүх ажил дээр маш их сайжруулсан гэдгийг харуулж байна.', 'mk': 'Во овој весник, ги претставуваме мултијазичните системи на КИТ за превод на невројални машини (НМТ) за задачите на кампањата за проценка IWSLT 2017 (MT) и превод на говорен јазик (SLT). For our MT task submissions, we used our multi-task system, modified from a standard attentional neural machine translation framework, instead of building 20 individual NMT systems.  We investigated different architectures as well as different data corpora in training such a multilingual system.  We also suggested an effective adaptation scheme for multilingual systems which brings great improvements compared to monolingual systems.  За SLT трагата, покрај монојазичкиот нервен преведувачки систем кој се користи за генерирање на точни точки и вистински случаи на податоците пред обуката на нашиот мултијазичен систем, воведовме модел на бука со цел да го направиме нашиот систем посилен. Резултатите покажуваат дека нашите нови модификации значително ги подобрија нашите системи на сите задачи.', 'lt': 'Šiame dokumente pristatome KIT daugiakalbes neurologinio vertimo (NMT) sistemas, skirtas 2017 m. IWSLT vertinimo kampanijos vertimo (MT) ir kalbos vertimo (SLT) užduotims. Mūsų MT užduočių teikimui mes naudojome savo daugiafunkcinę sistemą, modifikuotą pagal standartinę atidžiaus nervinių mašinų vertimo sistemą, užuot statę 20 atskirų NMT sistemų. We investigated different architectures as well as different data corpora in training such a multilingual system.  Mes taip pat pasiūlėme veiksmingą daugiakalbių sistemų pritaikymo sistemą, kuri padėtų gerokai pagerinti, palyginti su vienakalbėmis sistemomis. SLT bėgių atveju, be vienokalbės neurologinio vertimo sistemos, naudojamos teisingoms taškoms ir tikriems duomenų atvejams gauti prieš mokymą mūsų daugiakalbėje sistemoje, sukūrėme triukšmo model į, kad mūsų sistema taptų patikimesnė. Rezultatai rodo, kad mūsų nauji pakeitimai gerokai pagerino mūsų sistemas visose užduotyse.', 'pl': 'W artykule przedstawiamy wielojęzyczne systemy tłumaczenia maszynowego (NMT) KIT dla zadań kampanii ewaluacyjnej IWSLT 2017 (MT) i tłumaczenia języka mówionego (SLT). Do składania zadań MT użyliśmy naszego wielozadaniowego systemu, zmodyfikowanego ze standardowego systemu tłumaczenia neuronowego, zamiast budowania 20-ciu indywidualnych systemów NMT. Badaliśmy różne architektury oraz korpusy danych podczas szkolenia takiego systemu wielojęzycznego. Zaproponowaliśmy również skuteczny system adaptacji systemów wielojęzycznych, który przynosi znaczne usprawnienia w porównaniu z systemami jednojęzycznymi. Dla ścieżki SLT, oprócz jednojęzycznego systemu tłumaczenia neuronowego wykorzystywanego do generowania poprawnych interpunkcji i prawdziwych przypadków danych przed treningiem naszego wielojęzycznego systemu, wprowadziliśmy model hałasu, aby uczynić nasz system bardziej solidny. Wyniki pokazują, że nasze nowatorskie modyfikacje znacznie ulepszyły nasze systemy we wszystkich zadaniach.', 'ro': 'În această lucrare, prezentăm sistemele multilingve de traducere automată neurală (NMT) ale KIT pentru activitățile de evaluare a campaniei IWSLT 2017 de traducere automată (MT) și traducere vorbită (SLT). Pentru depunerea sarcinilor MT, am folosit sistemul nostru multi-task, modificat dintr-un cadru standard de traducere automată neurală atențională, în loc să construim 20 de sisteme NMT individuale. Am investigat diferite arhitecturi, precum și diverse corpore de date în formarea unui astfel de sistem multilingv. De asemenea, am sugerat o schemă eficientă de adaptare a sistemelor multilingve, care aduce mari îmbunătățiri în comparație cu sistemele monolingve. Pentru pista SLT, pe lângă un sistem de traducere neurală monolingvă utilizat pentru a genera punctuații corecte și cazuri reale ale datelor înainte de instruirea sistemului nostru multilingv, am introdus un model de zgomot pentru a face sistemul nostru mai robust. Rezultatele arată că noile noastre modificări au îmbunătățit considerabil sistemele noastre în toate sarcinile.', 'so': 'Qoraalkan waxan ku qoran qoraalka, waxaynu soo bandhignaynaa tarjumaadka qoraalka qoraalka ah ee KIT (NMT) ee qoraalka neurada oo kala duduwan (IWSLT) ee IWSLT 2017 (MT) iyo tarjumaadda luuqada lagu hadlo (SLT). Waxaynu u isticmaalnay nidaamka shaqaalaha badan ee MT, si aan uga beddelno qashinka tarjumaadda maskaxda ee caadiga ah oo aan dhisno 20 nidaam oo gaar ah ee NMT. Waxaannu baaritay dhismaha kala duduwan iyo shirkado macluumaad kala duduwan waxbarashada nidaamka luuqadaha kala duduwan. Sidoo kale waxaan kaloo soo jeedinnay qorshaha isbedelka oo faa’iido leh ee nidaamka luuqadaha kala duduwan, kaas oo keenaya beddelmo badan oo la barbarbarto nidaamka afka ah. Jidadka SLT, waxaa dheer oo ah nidaam tarjumaadda neurada ah oo loo isticmaalay inuu sameeyo meelo saxda ah iyo xaalada runta ah oo macluumaadka ah ka hor intaan tababarinno nidaamka luuqadaha kala duduwan, waxaan sameynay model cod ah si aan systemkeenna uga dhigno wax ka badan. Resultiyada waxay muuqataa in isbedeshiintayada warqada ah ay si weyn ugu bedeshay nidaamka shaqada oo dhan.', 'si': 'මේ පැත්තට, අපි KIT ගේ විශාල භාෂාවක් න්\u200dයූරල් මැෂින් අවවාදය (NMT) පද්ධතිය පෙන්වන්නේ IWSLT 2017 විශාල පද්ධතිය අවවාදය (MT) සහ ක අපේ MT කාර්යාලය සම්බන්ධ වෙනුවෙන්, අපි අපේ ගොඩක් කාර්යාලය පද්ධතිය පාවිච්චි කරනවා, ප්\u200dරමාණික අවධානයක් නිර්මාණය අපි වෙනස් ස්ථාපනයක් පරීක්ෂා කරලා තියෙන්නේ වෙනස් දත්ත කොර්පෝරා වගේම වඩා භාෂාවක් පද්ධ අපි වගේම ප්\u200dරශ්නයක් තියෙන්නේ වැඩි භාෂාත්මක පද්ධතිය සඳහා ප්\u200dරශ්නයක් සම්බන්ධ විදිහට සම්බන SLT ට්\u200dරෑක් වෙනුවෙන්, එක භාෂාවක් න්\u200dයූරල් පද්ධතියක් සඳහා ප්\u200dරයෝජනය කරන්න ප්\u200dරයෝජනය කරන්න ප්\u200dරයෝජනය කරන්න ප්\u200dරයෝජනය කරන්න ප්\u200dරයෝජනය සහ හ ප්\u200dරතිචාරයක් පෙන්වන්නේ අපේ නියම වෙනස් වෙන්න අපේ පද්ධතිය සියලුම් විශේෂ කරලා හැම වැඩේ වි', 'sv': 'I denna uppsats presenterar vi KIT:s flerspr책kiga neurala maskin철vers채ttningssystem (NMT) f철r IWSLT 2017 utv채rderingskampanjen maskin철vers채ttning (MT) och spr책k철vers채ttning (SLT) uppgifter. F철r v책ra inl채mningar av MT-uppgifter anv채nde vi v책rt multi-task system, modifierat fr책n en standard uppm채rksamhets neural maskin철vers채ttning ramverk, ist채llet f철r att bygga 20 individuella NMT-system. Vi unders철kte olika arkitekturer samt olika datakorpora i utbildningen av ett s책dant flerspr책kigt system. Vi f철reslog ocks책 ett effektivt anpassningssystem f철r flerspr책kiga system, vilket inneb채r stora f철rb채ttringar j채mf철rt med enspr책kiga system. F철r SLT-sp책ret, f철rutom ett enspr책kigt neuralt 철vers채ttningssystem som anv채nds f철r att generera korrekta skiljetecken och verkliga fall av data innan vi tr채nade v책rt flerspr책kiga system, introducerade vi en bullermodell f철r att g철ra v책rt system mer robust. Resultaten visar att v책ra nya modifieringar f철rb채ttrade v책ra system avsev채rt p책 alla uppgifter.', 'no': 'I denne papiret presenterer vi KIT sin fleirspråk neuralmaskinsomsetjing (NMT) for IWSLT 2017 evalueringskampanjonar for omsetjing av maskinen (MT) og språk omsetjing (SLT). For våre MT-oppgåver, brukte vi våre fleire oppgåver-systemet, endra frå eit standard attentional neuralmaskinsomsetjingsframme i staden for bygging av 20 individuelle NMT-systemer. Vi undersøkte ulike arkitekturar og ulike datakorporar i opplæring slik ein fleirspråk system. Vi har også foreslått eit effektivt tilpassingsskjema for fleirspråkssystemer som fører stor forbedringar samanlikna med monospråkssystemer. For SLT-sporet, i tillegg til eit monospråksomsetjingssystem brukt for å laga rette punktpunkt og sanne tilfeller av data før opplæring av multispråksystemet vår, introdusere vi ein støymodell for å gjera systemet vårt mer sterkt. Resultater viser at novelendringane våra forbetra systemet våre i alle oppgåver.', 'ur': 'ہم اس کاغذ میں KIT کی multilingual neural machine translation (NMT) سیستموں کو IWSLT 2017 ارزیابی کمپین ماشین ترجمہ (MT) اور زبان ترجمہ (SLT) کے کاموں کے لئے پیش کرتے ہیں. ہم نے اپنے مٹی ٹی ٹی ٹی ٹی ٹی ٹی ٹی ٹی ٹی سیسٹم کو استعمال کیا تھا، ایک استاندارڈ توجه کی نیورل ماشین ترجمہ فرم سے بدل دیا تھا، 20 شخصی NMT سیسٹم بنانے کے بدلے۔ ہم نے مختلف معماروں کی تحقیق کی اور مختلف ڈیٹا کورپورا کی تعلیم میں ایسی بہت سی زبان سیستم کی۔ ہم نے بہت سی زبان سیستموں کے لئے ایک مثبت تدبیر سیستموں کو بھی پیش کیا ہے جو ایک زبان سیستموں کے مقابلے میں بہت سی تدبیر لاتے ہیں۔ SLT ٹراک کے لئے، ایک ہی زبان نورول ترجمہ سیستم کے علاوہ جو ہمارے بہت سی زبان سیستم کی آموزش سے پہلے دکھانے کے لئے استعمال کیا گیا تھا، ہم نے اپنے سیستم کو زیادہ طاقتور بنانے کے لئے ایک صدا مدل پہنچایا۔ نتائج دکھاتے ہیں کہ ہماری روانی بدلوں نے ہماری سیستموں کو ہر کام پر اچھی طرح بہتر کر دیا۔', 'ta': 'இந்த காக்கியத்தில், நாம் குறிப்பிடுகிறோம் IWSLT 2017 முறைமைகளுக்கான KIT பல மொழி புதிய பாதுகாப்பு மொழிமொழிமொழிமாற்றி (SLT) பணிகளுக்கும். எங்கள் MT செயல் ஒப்புக்களுக்கு, நாங்கள் எங்கள் பல பணி அமைப்பை பயன்படுத்தி, நிலையான புதிய கணினி மொழிபெயர்ப்பு சட்டத்திலிருந்து மாற்றப்பட் நாங்கள் பல மொழி அமைப்பில் பயிற்சி செய்ய வேறு பாகங்களையும் மற்றும் வேறு தரவு நிறுவனத்தையும் ஆய்வு செய We also suggested an effective adaptation scheme for multilingual systems which brings great improvements compared to monolingual systems.  SLT தடத்திற்கு, ஒரு மொழிமொழி புதிய மொழிமாற்றும் அமைப்புக்கும் தவிர, சரியான பாங்குகள் மற்றும் உண்மையான நிகழ்வுகளை உருவாக்க பயன்படுத்தப்பட்டுள்ளது, எங்கள் பல மொழ முடிவுகள் எங்கள் புதிய மாற்றங்கள் அனைத்து பணிகளிலும் மேம்படுத்தப்பட்டது என்பதை காட்டுகிறது.', 'sr': 'U ovom papiru predstavljamo KIT-ove multijezičke neuralne mašine prevode (NMT) za zadatak IWSLT 2017. za prevod mašine za procjenu (MT) i prevod jezika (SLT). Za podnošenje zadataka MT-a, koristili smo naš multizadatačni sistem, modifikovan iz standardnog pažnjovnog okvira prevoda neuralnih mašina umjesto izgradnje 20 pojedinih NMT sistema. Istražili smo različite arhitekture i različite podatke korporacije u obuci takvog multijezičkog sistema. Takođe smo predložili efikasnu adaptaciju za višejezičke sisteme koji donosi velike poboljšanje u usporedbi sa monojezičkim sistemima. Za SLT trag, uz monojezički neuralni sustav prevoda koji se koristi da bi stvorili prave tačke i prave slučajeve podataka prije obuke našeg multijezičkog sistema, uveli smo model buke kako bi naš sistem bio jači. Rezultati pokazuju da su naše izmjene romana značajno poboljšale naše sisteme na svim zadacima.', 'uz': "Bu qogʻozda, biz IWSLT 2017'ning IWSLT kompyuterning bir nechta neyron tarjima tizimini (MT) va gapiradigan tillar tarjima qilish (SLT) vazifalarini ko'proq qilamiz. Vazifaning MT tashkilotlarimiz uchun biz bir necha vazifa tizimimizdan foydalanamiz, andoza taʼminlovchi neyron tarjima freymidan o'zgartirildi, 20 shaxsiy NMT tizimlarini yaratishdan oʻzgartirish. Biz bir necha tillar tizimini o'rganish uchun boshqa arxivlar va boshqa maʼlumot kompaniyalarini o'rganimiz. Biz bir necha tillar tizimlari uchun katta taʼminlov qolipini taqdim qildik. Bu bir necha tizim tizimga kamaytirilgan juda katta yaxshi yaxshilar beradi. Name Natijalar esa novel o'zgarishlarimiz bizning tizimmizni hamma vazifalarda katta yaxshi ko'rsatadi.", 'vi': 'Trong tờ giấy này, chúng tôi giới thiệu hệ thống dịch chuyển máy thần kinh đa dạng của KIT (NMB) cho các công việc dịch chuyển máy chiến dịch dịch dịch dịch dịch trong chiến dịch IWWSLT (MTV) và dịch ngôn ngữ đã nói (SLT). Chúng tôi sử dụng hệ thống đa nhiệm vụ của chúng tôi, được thay đổi từ một bộ phận dịch chuyển thiết bị thần kinh cao tiêu chuẩn thay vì xây dựng máy NMT riêng đôi. Chúng tôi đã điều tra các kiến trúc khác nhau và nhóm dữ liệu khác nhau trong việc đào tạo một hệ thống đa dạng. Chúng tôi cũng đề nghị một kế hoạch thích ứng hiệu quả cho hệ thống đa dạng mang lại nhiều cải tiến lớn so với hệ thống ngôn ngữ. Với đường trượt tuyết, ngoài một hệ thống dịch chuyển thần kinh đơn ngôn ngữ được sử dụng để tạo ra các dấu chấm đúng và trường hợp thực sự của dữ liệu trước khi huấn luyện hệ thống đa dạng, chúng tôi đã tạo ra một mô hình tiếng ồn để làm hệ thống của chúng tôi vững mạnh hơn. Kết quả cho thấy những thay đổi mới của chúng ta đã cải tiến đáng kể hệ thống.', 'da': "I denne artikel præsenterer vi KIT's flersprogede neurale maskinoversættelsessystemer (NMT) til IWSLT 2017 evalueringskampagnen maskinoversættelse (MT) og talesproget oversættelse (SLT) opgaver. Til vores MT-opgave indsendelse brugte vi vores multi-task system, modificeret fra en standard opmærksomheds neural maskinoversættelse ramme, i stedet for at bygge 20 individuelle NMT-systemer. Vi undersøgte forskellige arkitekturer samt forskellige datakorpora i træningen af et sådant flersproget system. Vi foreslog også en effektiv tilpasningsordning for flersprogede systemer, som medfører store forbedringer i forhold til ensprogede systemer. For SLT sporet introducerede vi ud over et ensproget neuralt oversættelsessystem, der bruges til at generere korrekte tegnsætninger og sande tilfælde af data inden træningen af vores flersprogede system, en støjmodel for at gøre vores system mere robust. Resultaterne viser, at vores nye ændringer forbedrede vores systemer betydeligt på alle opgaver.", 'bg': 'В настоящата статия представяме многоезичните системи за невронен машинен превод (НМТ) на КИТ за задачите на кампанията за оценка на машинен превод (МТ) и говорен език (СЛТ). За нашите МТ задачи, ние използвахме нашата многозадача система, модифицирана от стандартна рамка за невронен машинен превод на вниманието, вместо изграждане на 20 отделни НМТ системи. Проучихме различни архитектури, както и различни корпуси от данни при обучението на такава многоезична система. Предложихме и ефективна схема за адаптиране на многоезичните системи, която води до големи подобрения в сравнение с едноезичните системи. В допълнение към едноезичната невронна преводна система, използвана за генериране на правилни пунктуации и реални случаи на данните преди обучението на многоезичната ни система, ние въведохме модел на шума, за да направим нашата система по-здрава. Резултатите показват, че новите ни модификации значително подобряват системите ни при всички задачи.', 'nl': "In dit artikel presenteren we de meertalige neural machine translation (NMT) systemen van het KIT voor de IWSLT 2017 evaluatiecampagne machine translation (MT) en gesproken taal vertaling (SLT). Voor onze MT-opdrachten gebruikten we ons multi-task systeem, aangepast van een standaard attentional neural machine translation framework, in plaats van 20 individuele NMT systemen te bouwen. Bij de training van zo'n meertalig systeem hebben we verschillende architecturen en datacorpora onderzocht. Wij hebben ook voorgesteld een doeltreffend aanpassingssysteem voor meertalige systemen te ontwikkelen dat grote verbeteringen oplevert ten opzichte van eentalige systemen. Voor het SLT-spoor hebben we naast een eentalig neuraal vertaalsysteem dat wordt gebruikt om correcte interpuncties en echte gevallen van de gegevens te genereren voordat we ons meertalig systeem trainen, een ruismodel geïntroduceerd om ons systeem robuuster te maken. De resultaten tonen aan dat onze nieuwe modificaties onze systemen aanzienlijk verbeterd hebben bij alle taken.", 'hr': 'U ovom papiru predstavljamo KIT-ove višejezičke neuralne uređaje prevode (NMT) za zadatke IWSLT 2017. za prevod uređaja za procjenu mašine (MT) i prevod jezika (SLT). Za podnošenje zadataka MT-a, koristili smo naš višezadatačni sustav, modificirani iz standardnog uvjerljivog okvira prevoda neuralnih strojeva umjesto izgradnje 20 pojedinih NMT sustava. Istražili smo različite arhitekture i različite podatke korporacije u obuci takvog multijezičkog sustava. Također smo predložili učinkoviti sustav adaptacije za multijezičke sustave koji donosi velike poboljšanje u usporedbi s monojezičkim sustavima. Za SLT trag, osim jednojezičkog sustava neuralnog prevoda koji se koristi kako bi stvorili ispravne tačke i prave slučajeve podataka prije obuke našeg multijezičkog sustava, uvodili smo model buke kako bi naš sustav bio jači. Rezultati pokazuju da su naše izmjene romana značajno poboljšale naše sustave na svim zadacima.', 'id': 'Dalam kertas ini, kami mempersembahkan sistem penerjemah mesin saraf multibahasa KIT (NMT) untuk tugas kampanye evaluasi IWSLT 2017 mesin penerjemah (MT) dan penerjemah bahasa berbicara (SLT). Untuk pengiriman tugas MT kami, kami menggunakan sistem multi-tugas kami, diubah dari standar perhatian sistem terjemahan mesin saraf, bukannya membangun 20 sistem NMT individu. Kami menyelidiki arsitektur yang berbeda dan korpora data yang berbeda dalam latihan seperti sistem multibahasa. Kami juga menyarankan skema adaptasi efektif untuk sistem multibahasa yang membawa perkembangan besar dibandingkan dengan sistem monobahasa. Untuk pelacak SLT, selain sistem terjemahan saraf monobahasa yang digunakan untuk menghasilkan punctuasi yang benar dan kasus yang benar dari data sebelum melatih sistem multibahasa kita, kami memperkenalkan model suara untuk membuat sistem kita lebih kuat. Hasil menunjukkan bahwa modifikasi novel kami meningkatkan sistem kami dengan konsiderasi pada semua tugas.', 'de': 'In diesem Beitrag stellen wir die mehrsprachigen neuronalen maschinellen Übersetzungssysteme (NMT) des KIT für die IWSLT 2017 Evaluationskampagne Maschinenübersetzung (MT) und gesprochene Übersetzung (SLT) vor. Für die Einreichung von MT-Aufgaben verwendeten wir unser Multi-Task-System, das von einem Standard-attentional neuronal machine translation Framework modifiziert wurde, anstatt 20-einzelne NMT-Systeme zu bauen. Im Training eines solchen mehrsprachigen Systems untersuchten wir verschiedene Architekturen sowie verschiedene Datenkorpora. Wir schlugen auch ein wirksames Anpassungssystem für mehrsprachige Systeme vor, das im Vergleich zu einsprachigen Systemen große Verbesserungen bringt. Für die SLT-Spur haben wir zusätzlich zu einem einsprachigen neuronalen Übersetzungssystem, das verwendet wird, um korrekte Interpunktionen und wahre Fälle der Daten vor dem Training unseres mehrsprachigen Systems zu generieren, ein Rauschmodell eingeführt, um unser System robuster zu machen. Die Ergebnisse zeigen, dass unsere neuartigen Modifikationen unsere Systeme bei allen Aufgaben erheblich verbessert haben.', 'fa': 'در این کاغذ، ما سیستم\u200cهای ترجمه ماشین عصبی (NMT) چندین زبان KIT را برای ترجمه ماشین ارزیابی کامپیوتر IWSLT ۲۰۰۷ (MT) و ترجمه زبان صحبت می\u200cکنیم. برای تحویل دادن کار MT ما از سیستم چندین کار ما استفاده کردیم که از یک چهارچوب ترجمه ماشین عصبی استاندارد تغییر داده شده، به جای ساختن سیستم NMT ۲۰ فردی. ما به معماری های مختلف و شرکت داده های مختلف در تمرین چنین سیستم متعدد زبان تحقیق کردیم. ما همچنین پیشنهاد دادیم یک برنامه تغییرات موثری برای سیستم\u200cهای زیادی زبان که در مقابل سیستم\u200cهای یک زبان بهترین\u200cها را می\u200cآورد. برای نقاشی SLT، در addition to a single language neuronal translation system used to generate correct points and true cases of the data prior to training our multilingual system, we introduced a noise model to make our system more robust. نتیجه\u200cها نشان می\u200cدهند که تغییرات رمانی ما سیستم\u200cهای ما را به طور زیادی در همه کارها بهتر کرده است.', 'ko': '본고에서 KIT의 다국어 신경기계번역(NMT) 시스템을 소개했는데 IWSLT 2017 평가활동기계번역(MT)과 구어번역(SLT) 임무에 사용된다.기계 번역 임무를 제출할 때, 우리는 우리의 다중 임무 시스템을 사용했다. 이 시스템은 20개의 단독 NMT 시스템을 구축하는 것이 아니라 표준적인 주의 신경 기계 번역 프레임워크를 수정한 것이다.우리는 서로 다른 체계 구조와 서로 다른 데이터 자료 라이브러리를 연구하여 이런 다중 언어 시스템을 훈련시켰다.우리는 또한 효과적인 다언어 시스템 자체 적응 방안을 제시했는데 단어 시스템에 비해 이 방안은 매우 큰 개선이 있었다.SLT 궤도에 대해 우리의 다중 언어 시스템을 훈련하기 전에 정확한 문장부호와 데이터의 실제 상황을 형성하는 데 사용되는 단어 신경 번역 시스템 외에 우리는 소음 모델을 도입하여 우리의 시스템을 더욱 건장하게 한다.결과적으로, 우리의 새로운 수정은 우리의 시스템이 모든 임무에서 크게 개선되었다는 것을 나타냈다.', 'sw': 'Katika karatasi hii, tunaonyesha tafsiri ya mashine ya asili ya lugha mbalimbali ya KIT (NMT) kwa mfumo wa utafiti wa mashine ya uchunguzi wa kampeni ya IWSLT 2017 (MT) na tafsiri ya lugha (SLT) inayozungumzwa. Kwa ujumbe wetu wa kazi za MT, tulitumia mfumo wetu wa kazi nyingi, ulibadilishwa kutoka katika mfumo wa utafsiri wa mashine ya kawaida wa kawaida, badala ya kujenga mfumo wa NMT binafsi 20. Tulichunguza majengo tofauti pamoja na kampuni mbalimbali ya data katika mafunzo ya mfumo wa lugha mbalimbali. Pia tulipendekeza mpango wa kuboresha ufanisi wa mfumo wa lugha mbalimbali ambao unaleta maendeleo makubwa yanayolinganishwa na mfumo wa lugha. Kwa njia ya SLT, pamoja na mfumo wa utafsiri wa neura wa kiutamaduni uliotumika kutengeneza punguzo sahihi na matukio ya kweli ya data kabla ya kufundisha mfumo wetu wa lugha za lugha, tulianzisha sauti ili kuifanya mfumo wetu kuwa mbaya zaidi. Matokeo yanaonyesha kwamba mabadiliko ya riwaya yetu yaliboresha mifumo yetu kwa kiasi kikubwa kwenye kazi zote.', 'sq': 'Në këtë letër, ne paraqesim sistemet shumëgjuhësore të përkthimit të makinave nervore (NMT) të KIT për detyrat e përkthimit të makinave të fushatës s ë vlerësimit IWSLT 2017 (MT) dhe të përkthimit të gjuhës së folur (SLT). Për paraqitjen e detyrave tona MT, ne përdorëm sistemin tonë shumëdetyror, të modifikuar nga një kuadër standart i përkthimit të makinave nervore, në vend të ndërtimit të 20 sistemeve individuale NMT. Ne hetuam arkitektura të ndryshme si dhe trupa të ndryshme të të dhënave në trajnimin e një sistemi të tillë shumëgjuhës. Ne gjithashtu sugjeruam një skemë të efektshme përshtatjeje për sistemet shumëgjuhësore që sjell përmirësime të mëdha krahasuar me sistemet monogjuhësore. Për gjurmën SLT, përveç një sistemi përkthimi nervor monogjuhës që përdoret për të gjeneruar piksione të sakta dhe raste të vërteta të të dhënave përpara trajnimit të sistemit tonë shumëgjuhës, ne futëm një model zhurme për të bërë sistemin tonë më të fortë. Rezultatet tregojnë se modifikimet tona të reja përmirësuan sistemet tona konsiderueshëm në të gjitha detyrat.', 'tr': "Bu kagyzda biz KIT'iň IWSLT 2017-nji ýyldaky çykyş kampanyasynyň (MT) we gürleýän dil terjimeleri (SLT) sistemlerini belleýäris. MT täzim göndermelerimiz üçin, biziň köp-täzim sistemamyzy ulandyk, NMT sistemalaryny bejermek yerine standart näral maşynyň terjimesinden üýtgedildi. Biz beýleki arhitektura we beýleki data korporasyny bir multi dil sistemasynda okuw etmäge bardyk. Biz hem bir multi dil sistemalary üçin täsirli bir adaptasyon taslaýyny teklip etdik. Bu da mono dil sistemalary bilen görä örän gelişmeler getirir. SLT çizgi üçin, sistemimizi daha güçlü şekilde düzgün noktalar we gerçek durumları oluşturmak için kullanılan bir monodil nöral terjime sisteminde birleştirdik. Netijenler biziň täze täzeliklerimiziň sistemamyzyň hemme işiňde has gowy görkezilýändigini görkezýär.", 'af': "In hierdie papier, ons voorsien Kít se multitaal neurale masjien vertaling (NMT) stelsels vir die IWSLT 2017 evaluering kampanie masjien vertaling (MT) en praat taal vertaling (SLT) taak. Vir ons MT-taak onderwerp het ons ons multi-taak stelsel gebruik, gewysig van 'n standaard aandaglike neurale masjien vertalingsraamwerk, in plaas van 20 individuele NMT stelsels bou. Ons het verskillende arkitektuure en verskillende data korpora onderwerp in die onderwerp soos 'n multitaalske stelsel. Ons het ook 'n effektief aanpassingsskema voorgestel vir multitaalske stelsels wat groot verbeteringe bring in vergelyking met monotaalske stelsels. Vir die SLT snit, in addition to a monolingual neural translation system used to generate correct punctuations and true cases of the data prior to training our multilingual system, we introduced a noise model in order to make our system more robust. Resultate wys dat ons novel veranderinge ons stelsels aansienlik op alle werke verbeter het.", 'am': "In this paper, we present KIT's multilingual neural machine translation (NMT) systems for the IWSLT 2017 evaluation campaign machine translation (MT) and spoken language translation (SLT) tasks.  ለMT ስራታችን መልዕክቶች፣ ብዙ-ስራ-ስርዓታችንን፣ 20 የተለየ የNMT ስርዓቶችን ከመሠረት ፋንታ የተደላደለ የናውራዊ የሜክራዊ ትርጉም ፍሬም ተለወጠን፡፡ ብዙ የቋንቋ ቋንቋ ሲስተማርን እና ልዩ የዳታ ኮርፖርት ላይ የተለያየን የመዝገብ መሠረት ጠይተናል፡፡ እናም ከአንደኛ ቋንቋዎች ሲስተካከል ብዙ ቋንቋዎች ተሟጋቾችን የሚያሳልፍ የጥቅም አካባቢ ፕሮግራም አግኝተናል፡፡ ለSLT ግንኙነት፣ ከሞሎ ቋንቋ የኔural ትርጉም ስርዓት በቀር እውነተኛ የዳታ ጉዳዮች እና እውነተኞች ጉዳዮች መፍጠር በተጠቃሚ የቋንቋ ቋንቋዎች ስርዓታችንን ከመጠቀም በፊት የድምፅ ምሳሌ አግኝተናል፡፡ ፍሬዎቹ የኖሪያችን ለውጦችን ስርዓታችንን በሁሉም ስራ ላይ ያበዛል፡፡", 'bn': 'এই কাগজটিতে আমরা কিআইটির মাল্টিভাষার নিউরেল মেশিন অনুবাদ (এনএমটি) সিস্টেমগুলোকে উপস্থাপন করছি আইউএসএলটি ২০১৭ সালের জন্য প্রচারাভিযান মেশিন অনুবা For our MT task submissions, we used our multi-task system, modified from a standard attentional neural machine translation framework, instead of building 20 individual NMT systems.  আমরা বিভিন্ন প্রতিষ্ঠান এবং বিভিন্ন তথ্য কোর্পোরাও তদন্ত করেছি যেমন একটি বহুভাষাভাষী সিস্টেম প্রশিক্ষণে আমরা অনেক ভাষার সিস্টেমের জন্য কার্যকর পরামর্শ প্রদান করেছি যা মোনোলিভাষী সিস্টেমের তুলনায় অনেক উন্নতি প্রদান করে। এসএলটি ট্র্যাকের জন্য, একটি মোনোলিভাল নিউরেল অনুবাদ সিস্টেম ছাড়াও, যা আমাদের বহুভাষাভাষী সিস্টেম প্রশিক্ষণের আগে সঠিক পরিমাণ এবং সত্যিকারের মামলা তৈরি কর ফলাফল দেখা যাচ্ছে যে আমাদের উপন্যাস পরিবর্তন আমাদের সিস্টেম ব্যবস্থা সব কাজে বেশী ভালো করেছে।', 'hy': 'Այս աշխատանքում մենք ներկայացնում ենք KIT-ի բազլեզու նյարդային մեքենայի թարգմանման (NMT) համակարգերը 2017 թվականի IW-ի գնահատման արշավի մեքենայի թարգմանման (MT) և խոսված լեզվի թարգմանման (PLT) խնդիրների համար: Մեր MT խնդիրների ներկայացման համար մենք օգտագործեցինք մեր բազմախնդիրների համակարգը, որը փոխվել է ստանդարտ ուշադրության նյարդային մեքենայի թարգմանման շրջանակից, 20 անհատական NMT համակարգ ստեղծելու փոխարեն: Մենք ուսումնասիրեցինք տարբեր ճարտարապետություններ, ինչպես նաև տարբեր տվյալների կառուցվածքներ այդպիսի բազլեզու համակարգի ուսումնասիրության մեջ: Մենք նաև առաջարկեցինք մի արդյունավետ ադապտացիոն ծրագիր բազլեզու համակարգերի համար, որը մեծ բարելավումներ է բերում միալեզու համակարգերի հետ համեմատած: ՍԼԹ-ի համար, բացի միալեզու նյարդային թարգմանման համակարգից, որը օգտագործվում է տեղեկատվության ճիշտ կետերի և իրական դեպքերի ստեղծման համար նախքան մեր բազլեզու համակարգը, մենք ներկայացրեցինք աղմուկի մոդել, որպեսզի մեր համակարգը ավելի ուժեղ դարձնի: Results show that our novel modifications improved our systems considerably on all tasks.', 'az': "Bu kağızda, biz KIT'nin çoxlu dil nöral maşın çevirimi (NMT) sistemlərini IWSLT 2017-ci ilə değerlendirmə kampanyası maşın çevirimi (MT) və danışan dil çevirimi (SLT) işlərini göstəririk. MT vəzifələrimiz üçün çoxlu işlər sistemimizi istifadə etdik, 20 nəfər NMT sistemlərini inşa etmək yerinə standart təhlükəsizli nöral maşın çeviri frameworkindən dəyişdirildik. Biz müxtəlif arhitektarları və müxtəlif məlumat korporasını çoxlu dil sistemi təhsil etdik. Biz də çoxlu dil sistemlərinin müvəffəqiyyəti tərzini təbliğ etdik ki, monodil sistemləri ilə müvəffəqiyyətlər gətirir. SLT yolu üçün, çoxlu dil sistemimizi təhsil etmədən əvvəl, sistemimizi daha qüvvətli təhsil etmək üçün, monodil nöral tercümə sistemi istifadə etmək üçün istifadə edilən məlumatları və həqiqət məlumatları yaratmaq üçün istifadə etdik. Sonuçlarımız yeni dəyişikliklərimiz sistemimizi bütün işlərdə çox yüksəltdi.", 'bs': 'U ovom papiru predstavljamo KIT-ove multijezičke neuralne mašine prevode (NMT) za zadatke IWSLT 2017. za prevod mašine za procjenu (MT) i prevod jezika (SLT). Za podatke MT-ovih zadataka, koristili smo naš multizadatačni sistem, modifikovan iz standardnog pažnjnog okvira prevoda neuralnih strojeva umjesto izgradnje 20 pojedinačnih NMT-ovih sustava. Istražili smo različite arhitekture i različite podatke korporacije u obuci takvog multijezičkog sistema. Također smo predložili efikasnu adaptaciju za multijezičke sisteme koji donosi velike poboljšanje u usporedbi sa monojezičkim sistemima. Za SLT trag, osim jednojezičkog nervnog prevodnog sustava koji se koristi kako bi stvorili ispravne tačke i prave slučajeve podataka prije obuke našeg multijezičkog sistema, predstavili smo model buke kako bi naš sistem bio jači. Rezultati pokazuju da su naše izmjene romana značajno poboljšale naše sisteme na svim zadacima.', 'ca': "En aquest article presentem els sistemes multilingües de traducció neural (NMT) del KIT per a les tasques de la campanya d'evaluació IWSLT 2017 de traducció automàtica (MT) i traducció de llenguatges parlats (SLT). Per a les nostres submissions de tasques MT, vam utilitzar el nostre sistema de multitasques, modificat d'un marc de traducció neural d'atenció estándar, en lloc de construir 20 sistemes NMT individuals. Vam investigar arquitectures diferents i corpores de dades diferents en capacitar un sistema multilingüe així. També vam suggerir un esquema d'adaptació efectiu per als sistemes multilingües que porta grans millors comparats amb els sistemes monolingües. Per a la pista SLT, a més d'un sistema de traducció neural monolingüe utilitzat per generar puntuacions correctes i casos veritables de les dades abans d'entrenar el nostre sistema multilingüe, vam introduir un model de soroll per fer el nostre sistema més robust. Els resultats mostren que les noves modificacions milloraren considerablement els nostres sistemes en totes les tasques.", 'cs': 'V tomto článku představujeme vícejazyčné systémy neuronového strojového překladu (NMT) KIT pro úlohy hodnotící kampaně IWSLT 2017 strojového překladu (MT) a mluveného jazyka (SLT). Pro předložení MT úkolů jsme použili náš multi-task systém modifikovaný ze standardního pozornostního neuronového strojového překladu frameworku namísto budování 20 individuálních NMT systémů. Při výcviku takového vícejazyčného systému jsme zkoumali různé architektury i různé datové korpusy. Navrhli jsme také efektivní adaptační systém pro vícejazyčné systémy, který přináší velké zlepšení ve srovnání s jednojjazyčnými systémy. Pro SLT trať jsme kromě monojazyčného neuronového překladového systému používaného k generování správných interpunkcí a skutečných případů dat před výcvikem našeho vícejazyčného systému zavedli model šumu, aby byl náš systém robustnější. Výsledky ukazují, že naše nové modifikace výrazně zlepšily naše systémy při všech úkolech.', 'et': 'Käesolevas töös tutvustame KIT mitmekeelseid neuraalse masintõlke (NMT) süsteeme IWSLT 2017 hindamiskampaania masintõlke (MT) ja rääkiva keele tõlke (SLT) ülesannete jaoks. Meie MT ülesannete esitamiseks kasutasime 20 individuaalse NMT süsteemi ehitamise asemel mitme ülesandega süsteemi, mida on modifitseeritud standardsest tähelepanuväärsest neuro masintõlke raamistikust. Sellise mitmekeelse süsteemi koolitamisel uurisime nii erinevaid arhitektuure kui ka erinevaid andmekorporeid. Samuti tegime ettepaneku mitmekeelsete süsteemide tõhusaks kohandamiseks, mis toob võrreldes ühekeelsete süsteemidega kaasa märkimisväärseid parandusi. Lisaks ühekeelsele närvitõlkesüsteemile, mida kasutatakse õigete punktide ja andmete tõeliste juhtumite loomiseks enne mitmekeelse süsteemi koolitamist, võtsime SLT-rajale kasutusele müramudeli, et muuta süsteem tugevamaks. Tulemused näitavad, et meie uudsed muudatused parandasid meie süsteeme märkimisväärselt kõigis ülesannetes.', 'fi': 'T채ss채 artikkelissa esitell채채n KIT:n monikielisi채 neurokonek채채nn철sj채rjestelmi채 IWSLT 2017 -arviointikampanjan konek채채nn철s (MT) ja puhekielen k채채nn철s (SLT). MT-teht채v채l채hetyksiss채mme k채ytimme moniteht채v채j채rjestelm채채, jota muokattiin standardista tarkkaavaisen neuron konek채채nn철skehyksest채, sen sijaan, ett채 rakensimme 20 yksitt채ist채 NMT-j채rjestelm채채. Tutkimme erilaisia arkkitehtuureja ja datakorpusia t채llaisen monikielisen j채rjestelm채n koulutuksessa. Ehdotimme my철s monikielisi채 j채rjestelmi채 koskevaa tehokasta sopeutumisj채rjestelm채채, joka tuo suuria parannuksia yksikielisiin j채rjestelmiin verrattuna. SLT-radalle otettiin k채ytt철철n yksikielinen neurok채채nn철sj채rjestelm채, jota k채ytet채채n oikeiden v채limerkkien ja todellisten tapausten tuottamiseen ennen monikielisen j채rjestelm채mme koulutusta, kohinamalli, jotta j채rjestelm채mme olisi vankempi. Tulokset osoittavat, ett채 uudet muutokset paransivat j채rjestelmi채mme huomattavasti kaikissa teht채viss채.', 'jv': 'Nang pepulan iki, kita kang nggawe sistem itlanjut ning kelangan plural (NMT) ning kelangan kanggo kowe nggawe kampanjenengan capong maning (MT) lan kelangan langgambar terjamahan (SLT) nggo nganggo cara nggawe barang IWSLT" IWSLT " Rasané MT karo mulai, kita gambar sistem multi-task gawe ngubah sistem multi-task gawe ngubah kang sampeyan tanggal sistem nyong pada Jejaring, sanes sistem NMT sing wis ana Awak dhéwé énsegalih akeh barêng-barêng lan tambah kayané perusahaan dadi nang nggawe sistem multilanggar. Awak dhéwé éntuk suggeréné sak aplikasi kanggo sistem sing luwih akeh basa kanggo nggawe sistem sing di luwih nggawe gerakan karo sistem sing mungkin-ingkang. SLT track nganggo sistem sing takon nggambar penting nggo oleh puntuan karo iso nggawe bagian sing diranggunaké sing diranggunaké karo pertualangan kanggo ngubah sistem multilenguang, kita nambah utawa model sing takon nggawe sistem sing bisa pasar awak dhéwé. Pametuné mungkin ajeng-ajeng kanggo ngubah anyar tentang kanggo mbanjurakno ning sistem anyar sing dikarepaké ning sabên negoro.', 'he': 'בעיתון הזה, אנחנו מציגים את מערכות ההתרשמות המכונות העצביות המורבות של KIT (NMT) למשימות קמפיין הערכה IWSLT 2017 למשימות המשימה MT שלנו, השתמשנו במערכת המשימה רבה שלנו, שינוי ממסגרת התרגום של מכונות עצביות סטנדרטית תשומת לב, במקום לבנות 20 מערכות NMT בודדות. חקרנו ארכיטקטורות שונות, כמו גם גופו נתונים שונים באימון מערכת רבות שפות כזו. הצענו גם מערכת התאמה יעילה למערכות רבות שפות שמביאה שיפורים גדולים בהשוואה למערכות מונושפות. למסלול SLT, בנוסף למערכת תרגום עצבי מונושפת שמשתמשת לייצר נקודות נכונות ומקרים אמיתיים של הנתונים לפני האימון מערכת רבת-שפת שלנו, הכרנו מודל רעש כדי לגרום למערכת שלנו יותר חזקה. התוצאות מראות שהשינויים החדשים שלנו שיתפרו את המערכות שלנו באופן משמעותי בכל המשימות.', 'ha': "Daga wannan takardan, munã halatar da KIT's multiziman tarjima na neural na'urar (NMT) na'urar-tarjima na IWSLT 2017 Ga ƙanƙan da aikin MT, mun yi amfani da matsayinmu masu yawa, an canza daga firam ɗin fassarar masu aikin neural na daidaita, musamman da za'a samar da wasu na'urar NMT-na'ura. Haƙĩƙa, mun yi ƙidãya masu matsayi daban-daban da samun makampuni na danganta dabam-dabam ga wa'adin wata na'urar-mulki. Kayya, Mun buɗa wani na'urar adaptori mai amfani da wa'ura na'urar mulki-wasu, wanda ke zo da mafiya kyauta masu kyau sami'a da tsari na'urar farko. @ item: inmenu Mataimakin na nuna cewa musanya na yangonmu sun kyautata matsayinmu mai girma a kan duk aikin.", 'sk': 'V prispevku predstavljamo večjezične sisteme nevronskega strojnega prevajanja (NMT) KIT za naloge strojnega prevajanja (MT) in govorjenega jezika (SLT) IWSLT 2017. Za oddajo nalog MT smo namesto gradnje 20 posameznih NMT sistemov uporabili naš večopravilni sistem, spremenjen iz standardnega ogrodja za strojno prevajanje pozornosti. Pri usposabljanju takšnega večjezičnega sistema smo raziskovali različne arhitekture in različne podatkovne korpuse. Predlagali smo tudi učinkovit program prilagajanja za večjezične sisteme, ki prinaša velike izboljšave v primerjavi z enojezičnimi sistemi. Za sled SLT smo poleg enojezičnega nevronskega prevajalskega sistema, ki se uporablja za ustvarjanje pravilnih točk in resničnih primerov podatkov pred usposabljanjem našega večjezičnega sistema, uvedli model hrupa, da bi naš sistem postal robustnejši. Rezultati kažejo, da so naše nove spremembe bistveno izboljšale naše sisteme pri vseh nalogah.', 'bo': "In this paper, we present KIT's multilingual neural machine translation (NMT)systems for the IWSLT 2017 evaluation campaign machine translation (MT) and spoken language translation (SLT) tasks. For our MT task submissions, we used our multi-task system, modified from a standard attentional neural machine translation framework, instead of building 20 individual NMT systems. ང་ཚོས་སྒྲིག་ཆ་མ་འདྲ་བ་དང་སྦྲེལ་མཐུད་གཙོ་ཆོས་ཡིན་པ་མ་འདྲ་བར་བསླབས་ཏེ། ང་ཚོས་སྐད་ཡིག SLT དྲ་རྒྱའི་ལྟ་བུའི་དོན་ལས། ང་ཚོའི་རིམ་ལུགས་ཀྱི་དཔེ་དབྱིབས་བདེ་བཤད་དང་བདེ་བཤད་ཀྱི་གནས་ཚུལ་འདི་གསར་བསྐྲུན་འབད་བར་སྤྱོད་ཡོད་པ་ཧ་གོ་མ་ཤེས་པའི་སྐད་ཡིག གྲུབ་འབྲས་བྱ་ཚུལ་བཟོ་བཅོས་དུས་ང་ཚོའི་མ་ལག་གི་ལག་སྟངས་ལ་རྒྱས་སྐྱོར་ཡོད་པ་རེད།"}
{'en': 'Towards better translation performance on  spoken language', 'ar': 'نحو أداء ترجمة أفضل في اللغة المنطوقة', 'pt': 'Para um melhor desempenho de tradução na linguagem falada', 'fr': 'Vers de meilleures performances de traduction dans la langue parlée', 'es': 'Hacia un mejor rendimiento de la traducción en el idioma hablado', 'ja': '口語の翻訳パフォーマンスの向上に向けて', 'hi': 'बोली जाने वाली भाषा पर बेहतर अनुवाद प्रदर्शन की ओर', 'ru': 'Повышение эффективности перевода на разговорный язык', 'zh': '口语译性能', 'ga': 'I dtreo feidhmíocht aistriúcháin níos fearr ar an teanga labhartha', 'ka': 'საუკეთესო წარმატების შემდეგ', 'hu': 'A beszélt nyelven történő jobb fordítási teljesítmény felé', 'el': 'Προς καλύτερη απόδοση μετάφρασης της προφορικής γλώσσας', 'lt': 'Geresnio vertimo kalba rezultatų link', 'it': 'Verso migliori prestazioni di traduzione nella lingua parlata', 'mk': 'За подобри преведувања на говорниот јазик', 'mt': 'Lejn prestazzjoni aħjar tat-traduzzjoni fil-lingwa mitkellma', 'ml': 'സംസാരിക്കുന്ന ഭാഷയില്\u200d മുകളിലേക്ക് നല്ല പരിഭാഷയുടെ പ്രദര്\u200dശനം', 'ms': 'Ke arah prestasi terjemahan yang lebih baik pada bahasa bercakap', 'mn': 'Холбоотой хэл дээр илүү сайн орчуулах үйл ажиллагаанд', 'pl': 'W kierunku lepszej wydajności tłumaczenia języka mówionego', 'no': 'Til bedre omsetjingspråk', 'ro': 'Către o mai bună performanță de traducere în limba vorbită', 'sr': 'Na bolji izvod prevođenja na govornom jeziku', 'kk': 'Орындалған тілде жақсы аудару әрекетін', 'so': 'Towards better translation performance on spoken language', 'sv': 'Mot bättre översättningsresultat på talat språk', 'ta': 'பேச்சு மொழியில் மொழிமாற்றியின் மேல் சிறந்த மொழிமாற்றும் செயல்பாடு', 'si': 'කතා කරපු භාෂාව ගැන හොඳ වාර්තාවක් වෙනුවෙන්.', 'ur': 'بات کی زبان پر بہترین ترجمہ کرنا کی طرف', 'uz': '@ info: whatsthis', 'vi': 'Hướng tới hiệu quả dịch thuật tốt hơn trên ngôn ngữ dùng', 'bg': 'Към по-добри резултати в превода на говоримия език', 'nl': 'Naar betere vertaalprestaties in gesproken taal', 'da': 'På vej mod bedre oversættelsesevne på talt sprog', 'hr': 'Prema boljim izvodima prevoda na govornom jeziku', 'id': 'Ke arah prestasi terjemahan yang lebih baik dalam bahasa berbicara', 'de': 'Bessere Übersetzungsleistung in gesprochener Sprache', 'ko': '구어 번역 수준을 향상시키다.', 'sw': 'Utafiti bora wa tafsiri katika lugha inayozungumzwa', 'tr': 'Gürleýän diller barada gowy terjime etmek üçin', 'af': 'Gaan na beter vertaling op praat taal', 'sq': 'Për shfaqje më të mirë përkthimi në gjuhën e folur', 'am': 'በተናገረ ቋንቋ ላይ የተሻለ ትርጉም performance', 'hy': 'Խոսվող լեզվի թարգմանման ավելի լավ արդյունք', 'az': 'Sözlü dildə daha yaxşı tərcümə göstərmək üçün', 'bn': 'ভাষায় ভাষার ভাষায় ভালো অনুবাদ প্রদর্শন', 'bs': 'Prema boljim izvodima prevoda na govornom jeziku', 'ca': 'Towards better translation performance on spoken language', 'fa': 'به پیشرفت بهتر ترجمه روی زبان صحبت می\u200cکند', 'cs': 'K lepšímu překladu mluveného jazyka', 'et': 'Parema tõlketootluse suunas kõnelevas keeles', 'fi': 'Kohti puhuttua kieltä koskevien käännösten parempaa suorituskykyä', 'jv': 'Tulung bantuan sing luwih jarang langkung', 'sk': 'K boljši učinkovitosti prevajanja govorjenega jezika', 'ha': '@ action', 'he': 'לכיוון מופע תרגום טוב יותר בשפה מדברת', 'bo': 'སྐད་རིགས་འདིའི་ནང་དུ་ཡིག་སྐད་ཡིག་དང་ལེགས་ཤིག་འགྲོ་བཞིན་པ་དང་།'}
{'en': 'In this paper, we describe GTCOM’s neural machine translation(NMT) systems for the International Workshop on Spoken Language Translation(IWSLT) 2017. We participated in the English-to-Chinese and Chinese-to-English tracks in the small data condition of the  bilingual task  and the zero-shot condition of the  multilingual task . Our  systems  are based on the encoder-decoder architecture with attention mechanism. We build byte pair encoding (BPE) models in parallel data and back-translated monolingual training data provided in the small data condition. Other techniques we explored in our system include two deep architectures, layer nomalization, weight normalization and training models with annealing Adam, etc. The official scores of  English-to-Chinese , Chinese-to-English are 28.13 and 21.35 on test set 2016 and 28.30 and 22.16 on test set 2017. The official scores on German-to-Dutch, Dutch-to-German, Italian-to-Romanian and Romanian-to-Italian are 19.59, 17.95, 18.62 and 20.39 respectively.', 'ar': 'في هذه الورقة ، نصف أنظمة الترجمة الآلية العصبية (NMT) الخاصة بـ GTCOM لورشة العمل الدولية حول ترجمة اللغة المنطوقة (IWSLT) 2017. شاركنا في المسارات من الإنجليزية إلى الصينية ومن الصينية إلى الإنجليزية في حالة البيانات الصغيرة الخاصة بـ مهمة ثنائية اللغة وحالة اللقطة الصفرية للمهمة متعددة اللغات. تعتمد أنظمتنا على بنية وحدة فك التشفير مع آلية الانتباه. نحن نبني نماذج تشفير زوج البايت (BPE) في بيانات متوازية وبيانات تدريب أحادية اللغة مترجمة للخلف مقدمة في حالة البيانات الصغيرة. تشتمل التقنيات الأخرى التي اكتشفناها في نظامنا على بنيتين عميقتين ، وترشيح الطبقة ، وتطبيع الوزن ، ونماذج التدريب مع تلدين آدم ، وما إلى ذلك. الدرجات الرسمية للغة الإنجليزية إلى الصينية ، ومن الصينية إلى الإنجليزية هي 28.13 و 21.35 في مجموعة الاختبار 2016 و 28.30 و 22.16 في مجموعة الاختبار 2017. الدرجات الرسمية على الألمانية إلى الهولندية ومن الهولندية إلى الألمانية ومن الإيطالية إلى الرومانية ومن الرومانية إلى الإيطالية هي 19.59 و 17.95 و 18.62 و 20.39 على التوالي.', 'pt': 'Neste artigo, descrevemos os sistemas de tradução automática neural (NMT) do GTCOM para o International Workshop on Spoken Language Translation (IWSLT) 2017. Participamos das trilhas de inglês para chinês e chinês para inglês na condição de dados pequenos do tarefa bilíngue e a condição de tiro zero da tarefa multilíngue. Nossos sistemas são baseados na arquitetura codificador-decodificador com mecanismo de atenção. Construímos modelos de codificação de pares de bytes (BPE) em dados paralelos e dados de treinamento monolíngues retrotraduzidos fornecidos na condição de dados pequenos. Outras técnicas que exploramos em nosso sistema incluem duas arquiteturas profundas, nomalização de camadas, normalização de peso e modelos de treinamento com recozimento de Adam, etc. As pontuações oficiais de inglês para chinês, chinês para inglês são 28,13 e 21,35 no conjunto de testes de 2016 e 28h30 e 22h16 no conjunto de testes de 2017. As pontuações oficiais de alemão para holandês, holandês para alemão, italiano para romeno e romeno para italiano são 19,59, 17,95, 18,62 e 20,39, respectivamente.', 'es': 'En este artículo, describimos los sistemas de traducción automática neuronal (NMT) de GTCOM para el International Workshop on Spoken Language Translation (IWSLT) 2017. Participamos en las pistas de inglés a chino y de chino a inglés en la condición de datos pequeños de la tarea bilingüe y la condición de tiro cero de la tarea multilingüe. Nuestros sistemas se basan en la arquitectura de codificador-decodificador con mecanismo de atención. Creamos modelos de codificación de pares de bytes (BPE) en datos paralelos y traducimos de forma inversa datos de entrenamiento monolingües proporcionados en condiciones de datos pequeños. Otras técnicas que exploramos en nuestro sistema incluyen dos arquitecturas profundas, nominación de capas, normalización de peso y modelos de entrenamiento con recocido Adam, etc. Las puntuaciones oficiales de inglés a chino, chino a inglés son 28,13 y 21,35 en el conjunto de pruebas de 2016 y 28,30 y 22,16 en el conjunto de pruebas de 2017. Los puntajes oficiales de alemán a holandés, holandés a alemán, italiano a rumano y rumano a italiano son 19,59, 17,95, 18,62 y 20,39 respectivamente.', 'fr': "Dans cet article, nous décrivons les systèmes de traduction automatique neuronale (NMT) de GTCOM pour l'International Workshop on Spoken Language Translation (IWSLT) 2017. Nous avons participé aux pistes de l'anglais vers le chinois et du chinois vers l'anglais dans la condition de petites données de la tâche bilingue et la condition zéro de la tâche multilingue. Nos systèmes sont basés sur l'architecture encodeur-décodeur avec mécanisme d'attention. Nous construisons des modèles de codage par paires d'octets (BPE) dans des données parallèles et des données d'apprentissage monolingues rétro-traduites fournies dans la condition de petites données. Les autres techniques que nous avons explorées dans notre système incluent deux architectures profondes, la nomalisation des couches, la normalisation du poids et les modèles d'entraînement avec recuit Adam, etc. Les scores officiels de l'anglais vers le chinois, du chinois vers l'anglais sont de 28,13 et 21,35 sur le jeu de test 2016 et de 28h30 et 22,16 sur le jeu de test 2017. Les scores officiels de l'allemand vers le néerlandais, du néerlandais vers l'allemand, de l'italien vers le roumain et du roumain vers l'italien sont respectivement de 19,59, 17,95, 18,62 et 20,39.", 'ja': '本稿では、2017年の口語翻訳国際ワークショップ（ IWSLT ）のためのGTCOMの神経機械翻訳（ NMT ）システムについて説明する。バイリンガルタスクの小さなデータ条件と多言語タスクのゼロショット条件で、英語から中国語、中国語から英語のトラックに参加しました。当社のシステムは、注意メカニズムを備えたエンコーダデコーダアーキテクチャに基づいています。私たちは、並列データと小さなデータ条件で提供される逆翻訳されたモノリンガルトレーニングデータでバイトペア符号化（ BPE ）モデルを構築します。私たちのシステムで探索した他の技術には、2つの深いアーキテクチャ、層のノマライゼーション、重量正規化、およびアニーリングアダムを使用したトレーニングモデルなどがあります。英語から中国語、中国語から英語の公式スコアは、2016年のテストセットで28.13と21.35、2017年のテストセットで28.30と22.16です。ドイツ対オランダ、オランダ対ドイツ、イタリア対ルーマニア、ルーマニア対イタリアの公式スコアはそれぞれ19.59、17.95、18.62、20.39です。', 'zh': '本文GTCOM为2017年国际口语译研讨会(IWSLT)神经机器翻译(NMT)系统。 参双语事之小数,多言之零镜头英语中文英语轨道。 系统基于编码器-解码器架构,有注意机制。 并行数中字节编码(BPE)模形,于小数下供反向译单语训练数。 其他术兼两深度架构,重名之,权归一化用退火亚当。 2016年试者英语中文、中文英语官为28.13、21.35,2017年试组者为28.30、22.16。 德语对荷兰语,荷兰语对德语,意大利语对罗马尼亚语与罗马尼亚语对意大利语官得分分为19.59,17.95,18.62与20.39。', 'hi': 'इस पेपर में, हम बोली जाने वाली भाषा अनुवाद (IWSLT) 2017 पर अंतर्राष्ट्रीय कार्यशाला के लिए GTCOM के तंत्रिका मशीन अनुवाद (NMT) प्रणालियों का वर्णन करते हैं। हमने द्विभाषी कार्य की छोटी डेटा स्थिति और बहुभाषी कार्य की शून्य-शॉट स्थिति में अंग्रेजी-से-चीनी और चीनी-से-अंग्रेजी पटरियों में भाग लिया। हमारे सिस्टम ध्यान तंत्र के साथ एनकोडर-डिकोडर आर्किटेक्चर पर आधारित हैं। हम समानांतर डेटा में बाइट जोड़ी एन्कोडिंग (बीपीई) मॉडल और छोटे डेटा की स्थिति में प्रदान किए गए बैक-अनूदित मोनोलिंगुअल प्रशिक्षण डेटा का निर्माण करते हैं। अन्य तकनीकों हम अपने सिस्टम में पता लगाया दो गहरी आर्किटेक्चर, परत nomalization, वजन सामान्यीकरण और annealing एडम, आदि के साथ प्रशिक्षण मॉडल शामिल हैं. अंग्रेजी-से-चीनी, चीनी-से-अंग्रेजी के आधिकारिक स्कोर परीक्षण सेट 2016 पर 28.13 और 21.35 और परीक्षण सेट 2017 पर 28.30 और 22.16 हैं। जर्मन-से-डच, डच-से-जर्मन, इतालवी-से-रोमानियाई और रोमानियाई-से-इतालवी पर आधिकारिक स्कोर क्रमशः 19.59, 17.95, 18.62 और 20.39 हैं।', 'ru': 'В этой статье мы описываем системы нейронного машинного перевода(НМП) GTCOM для Международного семинара по переводу на устный язык (IWSLT) 2017 года. Мы участвовали в треках английский-китайский и китайско-английский в условиях малых данных двуязычной задачи и в условиях нулевого выстрела многоязычной задачи. Наши системы основаны на архитектуре кодировщик-декодер с механизмом внимания. Мы строим модели кодирования пары байтов (BPE) в параллельных данных и обратно-переведенных одноязычных обучающих данных, предоставляемых в условиях малых данных. Другие методы, которые мы исследовали в нашей системе, включают две глубокие архитектуры, номализацию слоев, нормализацию веса и модели обучения с отжигом Адама и т. д. Официальные оценки с английского на китайский, с китайского на английский - 28,13 и 21,35 на тестовом наборе 2016 и 28,30 и 22,16 на тестовом наборе 2017. Официальные баллы на немецко-голландском, голландско-немецком, итальянско-румынском и румынско-итальянском языках составляют 19,59, 17,95, 18,62 и 20,39 соответственно.', 'ga': 'Sa pháipéar seo, déanaimid cur síos ar chórais néar-aistriúcháin meaisín (NMT) GTCOM don Cheardlann Idirnáisiúnta ar Aistriúchán Teanga Labhartha (IWSLT) 2017. Ghlacamar páirt sna rianta Béarla-go-Síneach agus Sínis-go-Béarla i riocht sonraí beaga an Fhorais. tasc dátheangach agus riocht náid an tasc ilteangach. Tá ár gcórais bunaithe ar an ailtireacht ionchódóra-díchódóra le meicníocht aird. Tógann muid samhlacha ionchódaithe beart péire (BPE) i sonraí comhthreomhara agus sonraí oiliúna aonteangacha aisaistrithe a sholáthraítear sa riocht sonraí beaga. I measc na dteicnící eile a rinneamar iniúchadh inár gcóras tá dhá ailtireacht dhomhain, ainmníocht ciseal, normalú meáchain agus samhlacha oiliúna le Adam annealing, etc. Is iad na scóir oifigiúla Béarla-go-Síneach, Sínis-go-Béarla ná 28.13 agus 21.35 ar shraith tástála 2016 agus 28.30 agus 22.16 ar thacar tástála 2017. Is iad na scóir oifigiúla ar Gearmáinis-go-Ollainnis, Ollainnis-go-Gearmáinis, Iodáilis-go-Rómáinis agus Rómáinis-go-Iodáilis ná 19.59, 17.95, 18.62 agus 20.39 faoi seach.', 'hu': 'Ebben a tanulmányban bemutatjuk a GTCOM neurális gépi fordítási rendszereit a Beszélt Nyelvi Fordítás Nemzetközi Workshop (IWSLT) 2017-ben. Az angol-kínai és kínai-angol nyelvű pályákon vettünk részt a kétnyelvű feladat kisméretű adatok állapotában és a többnyelvű feladat nulla-shot állapotában. Rendszereink az útmérő-dekóder architektúrán alapulnak, figyelem mechanizmussal. Bájtpáros kódolási (BPE) modelleket építünk párhuzamos adatokból és visszafordított, egynyelvű képzési adatokból, amelyeket kis adatok esetén biztosítunk. A rendszerünkben feltárt további technikák közé tartozik a két mély architektúra, a réteg nomalizáció, a súlynormalizáció és az edzési modellek lágyító Adam, stb. Az angol-kínai, kínai-angol nyelvű hivatalos pontszámok 28.13 és 21.35 a 2016-os tesztkészleten, valamint 28.30 és 22.16 a 2017-es tesztkészleten. A hivatalos eredmények német-holland, holland-német, olasz-román és román-olasz arányban 19,59, 17,95, 18,62 és 20,39.', 'el': 'Στην παρούσα εργασία, περιγράφουμε τα συστήματα νευρωνικής μηχανικής μετάφρασης (NMT) της GTCOM για το Διεθνές Εργαστήριο Μεταφρασής Μιλημένης Γλώσσας (IWSLT) 2017. Συμμετείχαμε στις διαδρομές Αγγλικά-Κινέζικα και Κινέζικα-Αγγλικά στην κατάσταση μικρών δεδομένων της δίγλωσσης εργασίας και στην κατάσταση μηδενικής λήψης της πολυγλωσσικής εργασίας. Τα συστήματά μας βασίζονται στην αρχιτεκτονική κωδικοποιητή-αποκωδικοποιητή με μηχανισμό προσοχής. Κατασκευάζουμε μοντέλα κωδικοποίησης ζεύγους Byte (σε παράλληλα δεδομένα και πίσω μεταφρασμένα μονογλωσσικά δεδομένα εκπαίδευσης που παρέχονται στην κατάσταση μικρών δεδομένων. Άλλες τεχνικές που διερευνήσαμε στο σύστημά μας περιλαμβάνουν δύο βαθιές αρχιτεκτονικές, ονοματολογία στρώματος, ομαλοποίηση βάρους και μοντέλα προπόνησης με ανόπτηση κ.λπ. Οι επίσημες βαθμολογίες των Αγγλικών-στα-Κινέζικα, Κινέζικα-στα-Αγγλικά είναι 28.13 και 21.35 στο δοκιμαστικό σύνολο 2016 και 28.30 και 22.16 στο δοκιμαστικό σύνολο 2017. Οι επίσημες βαθμολογίες στα γερμανικά-ολλανδικά, ολλανδικά-γερμανικά, ιταλικά-ρουμανικά και ρουμανικά-ιταλικά είναι 19.59, 17.95, 18.62 και 20.39 αντίστοιχα.', 'it': "In questo articolo, descriviamo i sistemi neurali di traduzione automatica (NMT) di GTCOM per il Workshop Internazionale sulla traduzione linguistica parlata (IWSLT) 2017. Abbiamo partecipato alle tracce inglese-cinese e cinese-inglese nella condizione dei piccoli dati del compito bilingue e nella condizione zero-shot del compito multilingue. I nostri sistemi si basano sull'architettura encoder-decoder con meccanismo di attenzione. Costruiamo modelli di codifica byte pair (BPE) in dati paralleli e dati di formazione monolingue tradotti indietro forniti in condizioni di dati piccoli. Altre tecniche che abbiamo esplorato nel nostro sistema includono due architetture profonde, la nomalizzazione degli strati, la normalizzazione del peso e modelli di allenamento con ricottura Adam, ecc. I punteggi ufficiali di inglese-cinese, cinese-inglese sono 28.13 e 21.35 sul set di test 2016 e 28.30 e 22.16 sul set di test 2017. I punteggi ufficiali su tedesco-olandese, olandese-tedesco, italiano-rumeno e rumeno-italiano sono rispettivamente 19.59, 17.95, 18.62 e 20.39.", 'lt': 'Šiame dokumente apibūdinamos GTCOM nervinių mašinų vertimo (NMT) sistemos, skirtos 2017 m. Tarptautiniam seminarui dėl kalbos vertimo (IWSLT). Dalyvavome anglų–kinų–anglų ir kinų–anglų keliuose nedidelėje dvikalbės užduoties duomenų būklėje ir daugiakalbės užduoties nulinėje būklėje. Mūsų sistemos grindžiamos kodavimo ir dekoderio architektūra su dėmesio mechanizmu. Mes kuriame baitų porų kodavimo (BPE) modelius lygiagrečiais duomenimis ir grįžtamaisiais vertimais pateikiamus vienkalbinius mokymo duomenis mažomis duomenų sąlygomis. Kiti metodai, kuriuos ištyrėme mūsų sistemoje, yra dvi gilios architektūros, sluoksnių nomalizacija, svorio normalizavimas ir mokymo modeliai su Adamo priedavimu ir t. t. Oficialūs anglų–kinų, kinų–anglų rezultatai yra 28,13 ir 21,35 2016 m. bandymų rinkinyje ir 28,30 ir 22,16 2017 m. bandymų rinkinyje. Oficialūs Vokietijos–Nyderlandų, Nyderlandų–Vokietijos, Italijos–Rumunijos ir Rumunijos–Italijos rezultatai atitinkamai yra 19,59, 17,95, 18,62 ir 20,39.', 'kk': 'Бұл қағазда, 2017 жылдағы халықаралық тіл аудармасының GTCOM невралдық машинаның аудармасы (NMT) жүйелерін анықтап береміз. Біз ағылшын тілінен қытайлық және қытайлық тілінен ағылшын тілінен қатысушыларға қатысушылар мен көптілік тапсырманың шағын деректер жағдайында қатысушылар болдық. Біздің жүйелеріміз кодерлеу архитектурасына негізделген. Біз байт кодтамасы (BPE) үлгілерін параллелі деректерде және қайта аударылған монолингі оқыту деректерінде құрамыз. Жүйемізде зерттеген басқа техникаларда екі түсті архитектура, қабатты номализациялау, қабатты нормализациялау және оқыту үлгілері Адамды анализациялау үлгілері және т. б. 2016 және 28.30 және 22.16 сынақтағы ағылшын-қытайша, қытайша-а Немістан-голландша, голландша-немістан, итальян-руман және руман-ті-итальян-ті-итальяндағы офистикалық сандары 19,59, 17,95, 18,62 және 20,39.', 'mk': 'Во овој весник, ги опишуваме системите на ГТКОМ за нервен машински превод (НМТ) за Меѓународниот работилник за говорен јазик превод (IWSLT) 2017 година. Ние учествувавме на англиско-кинески и кинески-англиски траги во малата состојба на податоците на двојјазичката задача и состојбата со нула-снимка на мултијазичката задача. Нашите системи се базирани на архитектурата на кодерот со механизам на внимание. Создаваме модели за кодирање пар бајти (BPE) во паралелни податоци и монојазични податоци за обука кои се обезбедуваат во малата состојба на податоци. Другите техники кои ги истражувавме во нашиот систем вклучуваат две длабоки архитектури, номализација на слоевите, нормализација на тежината и тренинг модели со анализација на Адам итн. Официјалните резултати на англиско-кинески, кинески-англиски се 28.13 и 21.35 на тестот 2016 и 28.30 и 22.16 на тестот The official scores on German-to-Dutch, Dutch-to-German, Italian-to-Romanian and Romanian-to-Italian are 19.59, 17.95, 18.62 and 20.39 respectively.', 'ms': "In this paper, we describe GTCOM's neural machine translation(NMT) systems for the International Workshop on Spoken Language Translation(IWSLT) 2017.  Kami berpartisipasi dalam trek Inggeris-ke-Cina dan Cina-ke-Inggeris dalam keadaan data kecil tugas dua bahasa dan keadaan zero-shot tugas berbilang bahasa. Sistem kita berdasarkan arkitektur pengekod dengan mekanisme perhatian. Kami membina model pengekodan pasangan bait (BPE) dalam data selari dan data latihan monobahasa terjemahan-belakang disediakan dalam keadaan data kecil. Teknik lain yang kami pelajari dalam sistem kami termasuk dua arkitektur dalam, nomalizasi lapisan, normalisasi berat dan model latihan dengan aneling Adam, dll. skor rasmi dari Inggeris-ke-Cina, Cina-ke-Inggeris adalah 28.13 dan 21.35 pada set ujian 2016 dan 28.30 dan 22.16 pada set ujian 2017. Skor rasmi pada Jerman-ke-Belanda, Belanda-ke-Jerman, Itali-ke-Roman dan Romania-ke-Italia adalah 19.59, 17.95, 18.62 dan 20.39 respectively.", 'mt': "In this paper, we describe GTCOM's neural machine translation(NMT) systems for the International Workshop on Spoken Language Translation(IWSLT) 2017.  Parteċipajna fil-binarji Ingliż-Ċiniż-Ingliż u Ċiniż-Ingliż fil-kundizzjoni żgħira tad-dejta tal-kompitu bilingwi u l-kundizzjoni żero-shot tal-kompitu multilingwi. Is-sistemi tagħna huma bbażati fuq l-arkitettura tad-dekoder tal-kodiċi b’mekkaniżmu ta’ attenzjoni. Aħna nibnu mudelli ta’ kodifikazzjoni ta’ par ta’ bytes (BPE) f’dejta parallela u dejta ta’ taħriġ monolingwi tradotta lura pprovduta fil-kundizzjoni ta’ dejta żgħira. Tekniki oħra li esplorajna fis-sistema tagħna jinkludu żewġ arkitetturi profondi, nomalizzazzjoni tas-saffi, normalizzazzjoni tal-piż u mudelli ta’ taħriġ bl-annellar Adam, eċċ. Il-punteġġi uffiċjali ta’ Ingliż-Ċiniż-Ingliż, Ċiniż-Ingliż huma 28.13 u 21.35 fis-sett tat-test 2016 u 28.30 u 22.16 fis-sett tat-test 2017. Il-punteġġi uffiċjali fuq il-Ġermaniż sal-Olandiż, il-Olandiż sal-Ġermaniż, it-Taljan sal-Rumen u r-Rumen sal-Italjan huma 19.59, 17.95, 18.62 u 20.39 rispettivament.", 'ml': "In this paper, we describe GTCOM's neural machine translation(NMT) systems for the International Workshop on Spoken Language Translation(IWSLT) 2017.  ഞങ്ങള്\u200d ഇംഗ്ലീഷില്\u200d നിന്നും ചൈനീസില്\u200d നിന്നും ഇംഗ്ലീഷില്\u200d നിന്നും പങ്കുചേര്\u200dന്നിരുന്നു. രണ്ടു ഭാഷ ജോലിയുടെ ചെറിയ ഡാറ്റാ സ് നമ്മുടെ സിസ്റ്റത്തിന്റെ കോഡെര്\u200d ഡെക്കോഡേര്\u200d ആര്\u200dക്കിട്ടറിന്റെ അടിസ്ഥാനത്താണ്. നമ്മള്\u200d ബൈറ്റ് ജോടി കോഡിങ്ങ് ഉണ്ടാക്കുന്നു (BPE) മോഡല്\u200d ഉണ്ടാക്കുന്നു. പാരാളില്\u200d ഡാറ്റായും പിന്നില്\u200d പരിഭാഷിക്കപ് ഞങ്ങളുടെ സിസ്റ്റത്തില്\u200d നാം പരിശോധിച്ച മറ്റു സാങ്കേതികങ്ങള്\u200d രണ്ടു ആഴത്തില്\u200d ആദമിനെ പരീക്ഷിക്കുമ്പോള്\u200d ആദമിന്റെ പേരില്\u200d ആഗ്രഹിക്കുന്ന രണ്ട് ആഴത്തിലെ ആഴത്തിലെ ആഴത്തില്\u200d ആഴ ജര്\u200dമ്മന്\u200d മുതല്\u200d ഡച്ച്, ഡച്ച്-മുതല്\u200d ജര്\u200dമ്മന്\u200d, ഇറ്റാലിയന്\u200d മുതല്\u200d റോമാനിയനും റോമാനിയന്\u200d മുതല്\u200d ഇറ്റാലിയനും സ്കോര്\u200dട്ടുകള്\u200d ഏകദേശം 19. 59, 17. 95, 18. 62, 20.39", 'ka': 'ჩვენ 2017 წლის ინტერსონაციონალური სამუშაო სამუშაო სამუშაო სამუშაო სისტემაში GTCOM-ის ნეიროლური მანქანის გაგრძელება(NMT) სისტემაში აღწერეთ. ჩვენ ინგლისურ-კინესური და კინესური-ანგლისური მონაცემების პატარა მონაცემების შემდეგ და მრავალენგური მონაცემების ნულ-სურათის შემდეგ ჩვენ გავაკეთეთ. ჩვენი სისტემები კოდირების აქტიქტურაზე დაბაზიან ახალგაზრულება მექანიზმისთვის. ჩვენ პარალელური მონაცემებში ბაიტების კოდირება (BPE) მოდელების შექმნა და მონაცემების მონაცემებში მონაცემები მონაცემებში, რომლებიც პატარა მონაცემების შე სხვა ტექნოგიები, რომლებიც ჩვენ სისტემაში გავკვირდით, ჩვენი სისტემაში იქნება ორი ძალიან აქტიქტიკური, ნომალიზაცია, სიმაღლე ნორმალიზაცია და განაკვირდის მოდელები, ანგალიზაცია ადამიანის ანგალიზაცია და განაკვირდილება და განმავლობა ჲტთუთალნთრვ პაჱსმვნთ ნა დვპმანჟკთ-ეჲლევნჟკთ, დვპმანჟკთ-ეჲლევნჟკთ, თრალჟკთ-ჲრ-პსმანჟკთ თ პსმანჟკთ-ჲრ-თრალჟკთ ჟა 19,59, 17,95, 18,62 თ 20,39.', 'pl': 'W niniejszym artykule opisujemy systemy neuronowego tłumaczenia maszynowego (NMT) GTCOM dla Międzynarodowego Warsztatu Tłumaczenia Języka Mówionego (IWSLT) 2017. Uczestniczyliśmy w ścieżkach angielsko-chińskich i chińsko-angielskich w stanie małych danych zadania dwujęzycznego oraz warunku zero-shot zadania wielojęzycznego. Nasze systemy oparte są na architekturze kodera-dekodera z mechanizmem uwagi. Budujemy modele kodowania par bajtów (BPE) w danych równoległych i wstecznie tłumaczonych jednojęzycznych danych treningowych dostarczanych w warunkach małych danych. Inne techniki, które zbadaliśmy w naszym systemie to dwie głębokie architektury, nomalizacja warstw, normalizacja wagi i modele treningowe z wyżarzaniem Adama itp. Oficjalne wyniki angielsko-chińskiego, chińskiego-angielskiego to 28.13 i 21.35 na zestawie testowym 2016 oraz 28.30 i 22.16 na zestawie testowym 2017. Oficjalne wyniki dla niemieckiego-holenderskiego, holenderskiego-niemieckiego, włoskiego-rumuńskiego i rumuńskiego-włoskiego wynoszą odpowiednio 19.59, 17.95, 18.62 i 20.39.', 'ro': 'În această lucrare, descriem sistemele neuronale de traducere automată (NMT) ale GTCOM pentru Atelierul Internațional de Traducere a Limbilor Vorbite (IWSLT) 2017. Am participat la piesele din engleză-chineză și chineză-engleză în condițiile mici de date ale sarcinii bilingve și condiția zero-shot a sarcinii multilingve. Sistemele noastre se bazează pe arhitectura encoder-decoder cu mecanism de atenție. Construim modele de codificare a perechilor de byte (BPE) în date paralele și date de instruire monolingvă traduse înapoi furnizate în condițiile de date mici. Alte tehnici pe care le-am explorat în sistemul nostru includ două arhitecturi profunde, nomazarea straturilor, normalizarea greutății și modele de antrenament cu recoacerea Adam, etc. Scorurile oficiale de limba engleză-chineză, chineză-engleză sunt 28.13 și 21.35 pe setul de test 2016 și 28.30 și 22.16 pe setul de test 2017. Scorurile oficiale pe germană-olandeză, olandeză-germană, italiană-română și română-italiană sunt 19.59, 17.95, 18.62 și respectiv 20.39.', 'sr': 'U ovom papiru opisujemo GTCOMov sistem za prevod neuralne mašine (NMT) za međunarodno radionico o prevodu jezika govornog jezika (IWSLT) 2017. Mi smo sudjelovali u tragovima engleskog na kineskom i kineskom na engleskom jeziku u malom stanju podataka dvojezičkog zadatka i u stanju nula snimanja multijezičkog zadatka. Naši sistemi su zasnovani na arhitekturi kodera sa pažnjom. Napravili smo modele kodiranja bajtova par (BPE) u paralelnim podacima i podacima o monojezičkoj obuci koje su predložene u malom stanju podataka. Druge tehnike koje smo istražili u našem sistemu uključuju dve duboke arhitekture, nomalizaciju slojeva, normalizaciju težine i obuku modela sa priključenjem Adama itd. Zvanični rezultati engleskog na kineskog, kineskog na engleskom, 28,13 i 21,35 na testu setu 2016. i 28,30 i 22,16 na testu setu 2017. Zvanični rezultati o njemačkom-na-holandskom, holandskom-na-njemačkom, italijanskom-na-rumunskom i rumunskom-na-italijanskom su attiecīgo 19,59, 17,95, 18,62 i 20,39.', 'si': 'මේ පත්තරේ අපි GTCOM ගේ න්\u200dයුරල් මැෂින් පද්ධතිය භාෂාව (IWSLT) 2017 සඳහා අන්ත්\u200dරාජ්\u200dය වැඩස්කම් පද්ධතිය සඳහා විවෘත කරන්නේ අපි ඉංග්\u200dරීසියාවෙන් චීනියාවෙන් සහ චීනියාවෙන් ඉංග්\u200dරීසියාවෙන් පැත්තට සම්බන්ධ වෙලා තියෙන්නේ දෙවල් භාෂ අපේ පද්ධතිය ස්ථාපනය අධිකාරණ පද්ධතියෙන් ඇන්කෝඩර්-ඩිකෝඩර් ස්ථාපනය සඳහා. අපි බායිට් ජෝඩි සංකේතනය (BPE) මොඩේල් හදන්නේ සමාන්\u200dය දත්ත සහ පස්සේ භාෂාවික සංකේතයේ පොඩි දත්ත ස්ථානය අපේ පද්ධතියේ අපි පරීක්ෂණය කරලා තියෙන අනිත් තාක්ෂණාවක් තියෙන්නේ ගොඩක් ස්ථානය, පරීක්ෂණය, බලය සාමාන්\u200dයය සහ ප්\u200dරශ්නය සඳහා අඩාමන් අඩාමන් එක්ක, ඉතින්. අංග්\u200d ජර්මන් වලින්-ඩොච්ච්, ඩච්ච්-වලින්-ජර්මන්, ඉතාලියාන් වලින් රෝමානියාන් වලින් රෝමානියාන් වලින් ඉතාලියාන් වලින් 19', 'so': "Qoraalkan waxaynu ku qornaa qoraalka GTCOM's neural machine turjuman (NMT) systems for the International Workshop on Talk Language (IWSLT) 2017. Waxaannu ka qeybqaadanay waddooyinka Ingiriiska-Shiino-iyo-Shiino-ka-Ingiriis-ka-Ingiriis, xaalada macluumaadka yar ee shaqada labada luqadood iyo xaalada nooca ah ee shaqada luuqadaha kala duduwan. nidaamkayagu waxay ku saleysan yihiin dhismaha qodeynta, waxayna ku jirtaan qalabka aad u jeedid. Waxaannu dhisnaa noocyada koodeynta (BPE) oo lambarka ah iyo macluumaadka waxbarashada afka noocyada ah oo dib loo turjumay oo ku qoran xaalada macluumaadka yar. Teqooyin kale oo aan nidaamka ka baaraynay waxay ka mid yihiin labo meelo aad u dheer, nidaamka qashinka, qaabilaadda miisaanka iyo waxbarashada, tusaale ahaan imtixaanka afka ingiriisiga iyo Shiino-ilaa-Ingiriiska waxaa ka mid ah 28.13 iyo 21.35, imtixaanka 2016 iyo 28.30 iyo 22.16. Jarmal-to-Dutch, Dutch-to-Jarmal, Talyaani-to-Romaniyan iyo Talyaani waa 19.59, 17.95, 18.62 iyo 20.39.", 'sv': 'I denna uppsats beskriver vi GTCOM:s neurala maskinﾃｶversﾃ､ttningssystem (NMT) fﾃｶr International Workshop on Spoken Language Translation (IWSLT) 2017. Vi deltog i de engelska-till-kinesiska och kinesiska-till-engelska spﾃ･ren i smﾃ･datatillstﾃ･ndet fﾃｶr tvﾃ･sprﾃ･kig uppgift och nollskottet fﾃｶr den flersprﾃ･kiga uppgiften. Vﾃ･ra system ﾃ､r baserade pﾃ･ encoder-dekoder arkitektur med uppmﾃ､rksamhetsmekanism. Vi bygger BPE-modeller (byte pair encoding) i parallella data och bakﾃ･tﾃｶversatta ensprﾃ･kiga trﾃ､ningsdata som tillhandahﾃ･lls i smﾃ･datatillstﾃ･ndet. Andra tekniker vi utforskat i vﾃ･rt system inkluderar tvﾃ･ djupa arkitekturer, lagernomalisering, viktnormalisering och trﾃ､ningsmodeller med glﾃｶdgning Adam, etc. De officiella poﾃ､ngen fﾃｶr engelska-till-kinesiska, kinesiska-till-engelska ﾃ､r 28.13 och 21.35 pﾃ･ testset 2016 och 28.30 och 22.16 pﾃ･ testset 2017. De officiella poﾃ､ngen pﾃ･ tysk-till-nederlﾃ､ndska, hollﾃ､ndsk-till-tyska, italiensk-till-rumﾃ､nska och rumﾃ､nsk-till-italienska ﾃ､r 19,59, 17,95, 18,62 respektive 20,39.', 'mn': 'Энэ цаасан дээр бид 2017 оны Олон улсын Спокен хэл хөрөнгө оруулах (IWSLT) ажлын мэдрэлийн машин хөрөнгө оруулах системийг тайлбарлаж байна. Бид англи, Хятад, Хятад, Англи хэлний хоёр хэлний ажлын жижиг өгөгдлийн нөхцөлд оролцсон ба олон хэлний ажлын 0 шат нөхцөлд оролцсон. Бидний систем анхаарал төвлөрүүлэх механизм дээр кодлогч-онгоцлогч архитектур дээр суурилсан. Бид байт хоёр кодлог (BPE) загваруудыг параллел өгөгдлийн болон буцаагдах нэг хэл дасгал сургалт өгөгдлийг жижиг өгөгдлийн нөхцөлд өгсөн. Бидний системд судалсан өөр өөр техникууд нь хоёр гүн гүнзгий архитектур, давхар хувилбар, жин нормализаци, сургалтын загварууд Адам гэх мэт. Герман-хот, Дач-хот, Герман-хот, Итали-хот, Руман-хот, Руман-хот-Итали-хоёр нэртэй оюутнууд нь 19.59, 17.95, 18.62, 20.39.', 'no': 'I denne papiret beskriver vi GTCOM sin neuralmaskinsomsetjing (NMT) for internasjonale arbeidskop på Spoken Language Translation (IWSLT) 2017. Vi deltok i engelsk-til-kinesisk og kinesisk-til-engelsk spor i den lille data-vilkårene for det bilingåle oppgåva og forhold til null-bildet for det fleire språksoppgåva. Sistemet våre er basert på koderingsarkitekturen med oppmerksmekanismen. Vi bygger byte- par- koding (BPE) modeller i parallelle data og tilbakeomsette monospråk- treningsdata oppgjevne i den lille dataforhold. Andre teknikk som vi utforska i systemet vår inneheld to dype arkitektur, lag nomalizering, vektsnormalisering og treningsmodeller med annealing av Adam osv. Offisielle poeng av engelsk-til-kinesisk, kinesisk-til-engelsk er 28,13 og 21,35 på testsett 2016 og 28,30 og 22,16 på testsett 2017. Offisielle poeng på tysk-til-nederlandsk, nederlandsk-til-tysk, italiansk-til-Romansk og Romansk-til-italiansk er 19,59, 17,95, 18,62 og 20,39 respectively.', 'ur': 'اس کاغذ میں ہم نے GTCOM کی نیورال ماشین ترجمہ (NMT) سیستموں کو اسپاک زبان ترجمہ (IWSLT) 2017 کے لئے بین المللی کارشاپ کے لئے توصیح دیتے ہیں. ہم انگلیسی سے چینی اور چینی سے انگلیسی ٹراکیوں میں مشارکت کرتے تھے دو زبان کے کام کی چھوٹی ڈیٹ شرط میں اور بہت سی زبان کے کام کی صفر شٹ شرط میں۔ ہماری سیستموں کو توجه کے مکانیسم کے ساتھ کوڈر ڈیکوڈر معماری پر بنیاد ہے۔ ہم بائیٹ جوڑ کا اکنوڈینگ (BPE) موڈل بناتے ہیں parallel data اور پیچھے ترجمہ ایک زبان کی آموزش دیٹے جو چھوٹی ڈیٹ شرط میں حاصل کیے جاتے ہیں. اور دوسری تکنیک جنہیں ہم نے اپنے سیسٹم میں تحقیق کی ہے دو عمیق معماری، لائر بدلنے، وزن عاملی اور تعلیم موڈل ہیں جو آدام کے ساتھ پیدا کرنے کے ساتھ ہے، اور اس کے ساتھ. انگلیسی سے چینی، چینی سے انگلیسی کے رسمی اسکور ہیں 28.13 اور 21.35 تست سٹ میں 2016 اور 28.30 اور 22.16 تست سٹ پر۔ جرمن-سے-ڈچ، ڈچ-سے-جرمن، ایتالیایی-سے-رومانی اور رومانی-سے-ایتالیایی-سے-ایتالیایی پر رسمی اسکائٹ 19.59، 17.95، 18.62 اور 20.39 ہیں.', 'ta': "இந்த காகிதத்தில், நாம் GTCOM's நரம்பு இயந்திர மொழிமொழிபெயர்ப்புகளை குறிப்பிடுகிறோம். நாங்கள் ஆங்கிலத்தில் இருந்து சீனா மற்றும் சீனா- முதல் ஆங்கிலத்திலிருந்து பாதைகளில் பங்கிட்டோம் இரு மொழி பணியின் சிறிய தகவல் ந எங்கள் அமைப்புகள் கவனத்துடன் குறியீட்டாளர் அடிப்படையில் உள்ளது. நாம் பைட்ட் ஜோட் குறியீட்டு (BPE) மாதிரிகளை ஒப்பிட்ட தரவுகளில் உருவாக்குகிறோம் மற்றும் பின் மொழிமொழிபெயர்ப்பு  எங்கள் கணினியில் நாம் தேடிய மற்ற தொழில்நுட்பங்கள் இரு ஆழமான கட்டுக்கூறுகள், அடுக்கு நோமாற்றம், எடை வழக்கமாக்கம் மற்றும் பயிற்சி மாதிரிகள் உள்ளன. ஆங்கிலத்தில் இருந்து சீன- மொழிக் ஜெர்மன் முதல்-டச்சு, டுச்- முதல் ஜெர்மன், இத்தாலியன் இருந்து ரோமேனியன் மற்றும் ரோமானியன் இருந்த இத்தாலியன் மீது அரசியல் மதிப்புகள் 19. 59, 17. 95, 18. 62", 'vi': 'Trong tờ giấy này, chúng tôi mô tả hệ thống dịch chuyển máy thần kinh của GTCOM (NMB) của Bộ Giao dịch ngôn ngữ quốc tế về ngôn ngữ) Chúng tôi đã tham gia vào đường ray Anh-Trung-Trung-Trung-Anh trong tình trạng dữ liệu nhỏ của nhiệm vụ hai thứ tiếng và điều kiện bắn không phát của nhiệm vụ đa dạng. Hệ thống của chúng tôi dựa trên cấu trúc mã hóa với cơ chế tập trung. Chúng tôi xây dựng các mẫu mã hai byte pair (BPE) trong dữ liệu song song và dữ liệu đào tạo một ngôn ngữ ngược cung cấp trong điều kiện dữ liệu nhỏ. Các kỹ thuật khác mà chúng tôi khám phá được trong hệ thống gồm hai kiến trúc sâu, biến đổi lớp lớp, phục hồi trọng lượng và các mô hình huấn luyện với tính kế hoạch Adam, v.v. những điểm đánh dấu chính của Anh-sang-Trung Quốc, Trung Quốc-Anh là 8.13 và 21.35. trên thử thách thách thách thách được đặt là A6 và 2.30 và 22.16. Các điểm đánh giá chính thức về Đức-Dutch-Dutch, Dutch-German, Italian-or-Rumani-Italian là 19.59, 17.95, 18.62 và 20.39.', 'uz': "In this paper, we describe GTCOM's neural machine translation(NMT) systems for the International Workshop on Spoken Language Translation(IWSLT) 2017.  Biz ingliz tildan Xitoycha va Xitoycha va ingliz tilidan bir tillar vazifaning kichkina maʼlumot holatida va ko'plab tillar vazifasidagi nuqta holatiga ega bo'lgan. Bizning tizimlarimiz foydalanuvchi mekanisme bilan kod-dekoder arxituvchiga asoslangan. Biz bitta kodlash usulini (BPE) parametrlar va kichkina maʼlumot holatida qayta tarjima qiladigan monolingual taʼminlovchi maʼlumotlarni yaratishmiz. Biz tizimimizda qidirilgan boshqa texnologiyalar esa ikki eng yuqori maktablar, qanday nomalization, vekt normalisiyatsiya va taʼminlovchi modellar va o'xshash modellarda mavjud emas. Engliz tilidan Xitoycha va Inglizchaga 28.13 va 21.35 ta'lim 2016 va 2017 ta'sirida 28.30 va 22.16 tugmalar birikmasida. Olmoncha-Dutch, Dutch-Olmonchadan-Olmonchaga, Italyanga-Romaniya va Ruminccha-Italiyaga rasm soni 19.59, 17.95, 18.62 va 20.39 darajasi.", 'da': "I denne artikel beskriver vi GTCOM's neurale maskinoversættelsessystemer (NMT) til International Workshop on Spoken Language Translation (IWSLT) 2017. Vi deltog i de engelsk-til-kinesiske og kinesisk-til-engelske spor i den lille data tilstand af den tosprogede opgave og nul-shot tilstand af den flersprogede opgave. Vores systemer er baseret på encoder-dekoder arkitektur med opmærksomhedsmekanisme. Vi bygger byte pair encoding (BPE) modeller i parallelle data og back-translated ensprogede træningsdata leveret i små data tilstand. Andre teknikker, vi udforskede i vores system, omfatter to dybe arkitekturer, lagnomalisering, vægtnormalisering og træningsmodeller med udglødning Adam osv. De officielle scorer af engelsk-kinesisk, kinesisk-til-engelsk er 28.13 og 21.35 på testsæt 2016 og 28.30 og 22.16 på testsæt 2017. De officielle resultater på tysk-til-hollandsk, hollandsk-til-tysk, italiensk-til-rumænsk og rumænsk-til-italiensk er henholdsvis 19,59, 17,95, 18,62 og 20,39.", 'nl': "In dit artikel beschrijven we GTCOM's neural machine translation (NMT) systemen voor de International Workshop on Spoken Language Translation (IWSLT) 2017. We namen deel aan de Engels-naar-Chinees en Chinees-naar-Engels tracks in de kleine dataconditie van de tweetalige taak en de zero-shot conditie van de meertalige taak. Onze systemen zijn gebaseerd op de encoder-decoder architectuur met aandachtsmechanisme. We bouwen byte pair encoding (BPE) modellen in parallelle data en back-vertaalde eentalige trainingsgegevens verstrekt in de kleine data conditie. Andere technieken die we hebben onderzocht in ons systeem omvatten twee diepe architecturen, laagnomalisatie, gewichtsnormalisatie en trainingsmodellen met gloeien Adam, enz. De officiële scores van Engels-naar-Chinees, Chinees-naar-Engels zijn 28.13 en 21.35 op testset 2016 en 28.30 en 22.16 op testset 2017. De officiële scores op Duits-naar-Nederlands, Nederlands-naar-Duits, Italiaans-naar-Roemeens en Roemeens-naar-Italiaans zijn respectievelijk 19.59, 17.95, 18.62 en 20.39.", 'bg': 'В настоящата статия описваме системите за невронен машинен превод (НМТ) на Международната работна среща по говорен език (2017). Участвахме в песните английско-китайски и китайско-английски в състоянието на малки данни на двуезичната задача и състоянието на нулев изстрел на многоезичната задача. Нашите системи са базирани на архитектурата на кодер-декодер с механизъм за внимание. Създаваме модели за кодиране на байт двойка (БПЕ) в паралелни данни и обратно преведени едноезични данни за обучение, предоставени в условията на малки данни. Други техники, които изследвахме в нашата система, включват две дълбоки архитектури, слоеве номализация, нормализиране на теглото и модели за обучение с отгряване на Адам и др. Официалните оценки на английски-китайски, китайски-английски са 28.13 и 21.35 на тестовия комплект 2016 и 28.30 и 22.16 на тестовия комплект 2017. Официалните оценки на немски-холандски, холандски-немски, италиански-румънски и румънски-италиански са съответно 19,59, 17,95, 18,62 и 20,39.', 'de': 'In diesem Beitrag beschreiben wir die neuronalen maschinellen Übersetzungssysteme (NMT) von GTCOM für den International Workshop on Spoken Language Translation (IWSLT) 2017. Wir beteiligten uns an den englisch-to-Chinese und chinesisch-to-English Tracks in der kleinen Datenbedingung der zweisprachigen Aufgabe und der zero-shot Bedingung der mehrsprachigen Aufgabe. Unsere Systeme basieren auf der Encoder-Decoder-Architektur mit Aufmerksamkeitsmechanismus. Wir bauen Byte Pair Encoding (BPE)-Modelle in parallelen Daten und rückübersetzten monolingualen Trainingsdaten, die in der kleinen Datenbedingung bereitgestellt werden. Weitere Techniken, die wir in unserem System erforscht haben, umfassen zwei tiefe Architekturen, Schichtnomalisierung, Gewichtsnormalisierung und Trainingsmodelle mit Glühen Adam, etc. Die offiziellen Ergebnisse von Englisch-Chinesisch, Chinesisch-Englisch sind 28.13 und 21.35 auf Testsatz 2016 und 28.30 und 22.16 auf Testsatz 2017. Die offiziellen Noten auf Deutsch-Niederländisch, Niederländisch-Deutsch, Italienisch-Rumänisch und Rumänisch-Italienisch liegen bei 19.59, 17.95, 18.62 und 20.39.', 'id': 'Dalam kertas ini, kami menggambarkan sistem mesin terjemahan saraf GTCOM (NMT) untuk Workshop Internasional tentang Perjemahan Bahasa Bicara (IWSLT) 2017. Kami berpartisipasi di jalur Inggris-ke-Cina dan Cina-ke-Inggris dalam kondisi data kecil dari tugas dua bahasa dan kondisi zero-shot dari tugas multibahasa. Sistem kita berdasarkan arsitektur pengekode dengan mekanisme perhatian. Kami membangun model kode pasangan byte (BPE) dalam data paralel dan data latihan monobahasa terterjemahan kembali yang diberikan dalam kondisi data kecil. Teknik lainnya yang kami eksplorikan dalam sistem kami termasuk dua arsitektur dalam, nomalizasi lapisan, normalisasi berat badan dan model latihan dengan aneling Adam, dll. skor resmi dari Inggris-ke-Cina, Cina-ke-Inggris adalah 28.13 dan 21.35 pada set tes 2016 dan 28.30 dan 22.16 pada set tes 2017. Nilai resmi dari Jerman-ke-Belanda, Belanda-ke-Jerman, Italia-ke-Rumania dan Rumania-ke-Italia adalah 19,59, 17,95, 18,62 dan 20,39 respectively.', 'fa': 'در این کاغذ، ما سیستم ترجمه\u200cهای ماشین عصبی GTCOM را برای کارگاه بین المللی در ترجمه زبان Spoken (IWSLT) ۲۰۰۷ توصیف می\u200cکنیم. ما در نقشه های انگلیسی و چینی و چینی و انگلیسی در شرایط داده های کوچک کار دو زبان و شرایط شلیک صفر از کار بسیاری زبان شرکت کردیم. سیستم\u200cهای ما بر روی معماری\u200cسازی\u200cهای رمزبندی با مکانیسم توجه بنیاد دارند. ما مدل\u200cهای رمزبندی بایت\u200cها (BPE) را در داده\u200cهای متفاوت و داده\u200cهای آموزش تک زبان\u200cهای پشت ترجمه می\u200cکنیم که در شرایط داده\u200cهای کوچک ارائه می\u200cدهند. فناوری دیگر که در سیستم ما کشف کرده\u200cایم شامل دو معماری عمیق، تغییر تغییر طبقه، سنگین\u200cسازی و آموزش مدل\u200cهای سنگین با آدام، و غیر از آن است. امتیاز رسمی انگلیسی به چینی، چینی-به انگلیسی 28.13 و 21.35 در مجموعه آزمایش 2016 و 28.30 و 22.16 در مجموعه آ امتياز رسمي در مورد آلماني-به-هلندي، هلندي-به-آلماني، ايتاليايي-به-روماني و روماني-به-ايتاليايي-به-ايتاليايي، 19.59، 17.95، 18.62 و 20.39 هستند.', 'sw': 'Katika gazeti hili, tunaelezea mfumo wa kutafsiri mashine ya asili ya GTCOM (NMT) kwa ajili ya Warsha ya Kimataifa ya Utafsiri wa Lugha ya Utetezi (IWSLT) 2017. Tumeshiriki katika njia ndogo ya takwimu za kazi za lugha mbili na hali isiyo na sifa ya kazi za lugha nyingine. Mifumo yetu inatokana na ujenzi wa kodi na mfumo wa kusikiliza. Tunajenga mifano ya kodi mbili (BPE) katika takwimu tofauti na taarifa za mafunzo ya lugha za kimapenzi zilizotolewa katika hali ndogo ya data. Teknolojia nyingine tulizotafuta katika mfumo wetu ni pamoja na majengo mawili ya ndani, utekelezaji wa vipande vya juu, utaratibu wa uzito na mafunzo yanayofanana na Adam, etc. Takwimu rasmi zinazohusu Ujerumani-hadi Dutch, Ujerumani-hadi Ujerumani, Kiitalia-hadi-Romania na wa-Italia ni asilimia 19.59, 17.95, 18.62 na 20.39.', 'tr': "Bu kagyzda, 2017-nji ýylda GTCOM'iň näral maşynyň terjime(NMT) sistemlerini Halkara IWSLT dilinde IWSLT'i belleýäris. Biz iňlisçe-Çin çe we Çinçe-dan-Iňlisçe çykyşlarynda iki dil täblisasynyň kiçi maglumatyň durumynda ýaşadyk we olaryň birnäçe dil täblisasynyň durumynda bölýärdik. Biziň sistemamyz ködleme-kodlemek arhitektegimyza üns berip başlaýar. Biz parallel maglumatlarda byt çift kodleme (BPE) nusgalary we yzyna terjime edilen monodil eğleme maglumatlaryny kiçi maglumatlarda berilýäris Biziň sistemamyzda gözleýän başga tekniklerimizde iki derin arhitektura, katy nomalizasy, günit normalizasyon we okuwçylyk nusgalary Adymy pievlaşdyrmak bilen meňzeýär. Iňlisçe-Çin çe, Çinçe-we-Iňlisçe resmi sanlary 28.13 we 21.35 test düzeninde 2016 we 28.30 we 22.16-nji ýylda test düzeninde. Resmi sany nemesçe-we-hollança, hollança-we-nemesçe, italiýa-we-rumança-we-rumança-we-italiýa sany 19.59, 17.95, 18.62 we 20.39.", 'af': 'In hierdie papier, beskryf on s GTCOM se neurale masjien vertaling( NMT) stelsels vir die Internasionale Werkshop op Spoken Taal Vertaling( IWSLT) 2017. Ons het gedeel in die Engels-na-Sinees en Sinees-na-Engels snitte in die klein data voorwaarde van die twee tale taak en die nul-skoot voorwaarde van die veelvuldige taak. Ons stelsels is gebaseer op die enkoder-dekoder-arkitektuur met aandag mekanisme. Ons bou byte paar enkodering (BPE) modele in parallele data en terugvertaal monolinglike onderwerking data verskaf in die klein data voorwaardes. Ander teknike wat ons in ons stelsel uitgesoek het, inkluit twee diep arkitektuur, laag nomalisering, gewig normalisering en onderwerp modele met aansluiting van Adam, ensfh. Die offisiele telling van Engels-na-Sjinese, Sjinese-na-Engels is 28.13 en 21.35 op toets stel 2016 en 28.30 en 22.16 op toets stel 2017. Die offisiele telling op Duits-na-Dutch, Dutch-to-Duits, Italian-to-Romanian en Romanian-to-Italian is respectively 19.59, 17.95, 18.62 en 20.39.', 'sq': 'Në këtë letër, ne përshkruajmë sistemet e përkthimit nervor të makinave të GTCOM-it (NMT) për Workshop Ndërkombëtar për Trakthimin e gjuhës s ë folur (IWSLT) 2017. Ne morëm pjesë në gjurmët angleze-kineze dhe kineze-angleze në gjendjen e vogël të të dhënave të detyrës dygjuhëse dhe gjendjen zero-shot të detyrës shumëgjuhëse. Sistemet tona janë bazuar në arkitekturën e koduesit-dekoderit me mekanizëm vëmendje. Ne ndërtojmë modele të kodimit të çiftit byte (BPE) në të dhëna paralele dhe të dhëna të trajnimit monogjuhësor të përkthyer mbrapa, të dhëna të dhëna të vogla. Teknikë të tjera që ne eksploruam në sistemin tonë përfshijnë dy arkitektura të thella, nomalizim të nivelit, normalizim të peshës dhe modele trajnimi me anelizimin e Adamit, etj. Rezultatet zyrtare të anglisht-në-kinez, kinez-në-anglisht janë 28.13 dhe 21.35 në grupin e testimeve 2016 dhe 28.30 dhe 22.16 në grupin e testimeve 2017. Rezultatet zyrtare në gjerman-në-hollandez, hollandez-në-gjerman, italian-në-rumun dhe rumun-në-italian janë respektivisht 19.59, 17.95, 18.62 dhe 20.39.', 'hr': 'U ovom papiru opisujemo GTCOMov sustav neurostrojnog prevoda (NMT) za međunarodno radionico o prevodu govornog jezika (IWSLT) 2017. Učestvovali smo u tragovima engleskog-kineskog i kineskog-na-engleskog u malom podatku stanja dvojezičkog zadatka i nula-snimanja multijezičkog zadatka. Naši sustavi su temeljeni na arhitekturi kodera-dekodera sa pažnjom. Napravili smo modele kodiranja bajtova par (BPE) u paralelnim podacima i podacima o monojezičkoj obuci natrag koji su predstavljeni u malom stanju podataka. Druge tehnike koje smo istražili u našem sustavu uključuju dvije duboke arhitekture, nomalizaciju slojeva, normalizaciju težine i obuku modela s priključenjem Adama itd. Zvanični rezultati engleskog na kineski, kineski na engleski, 28,13 i 21,35 na testu setu 2016. i 28,30 i 22,16 na testu setu 2017. Zvanični rezultati o njemačkom-na-holandskom, holandskom-na-njemačkom, italijanskom-na-rumunskom i rumunskom-na-italijanskom su attiecīgo 19,59, 17,95, 18,62 i 20,39.', 'hy': 'Այս թղթի մեջ մենք նկարագրում ենք GTcom-ի նյարդային մեքենայի թարգմանման (NMT) համակարգերը 2017 թվականի միջազգային լեզվի թարգմանման աշխատասենյակի համար: Մենք մասնակցեցինք անգլերեն-չինարեն և չինարեն-անգլերեն հետքերին երկլեզու խնդրի փոքր տվյալների վիճակում և բազլեզու խնդրի զրոյի վիճակում: Մեր համակարգերը հիմնված են կոդեր-կոդեր ճարտարապետության վրա, որը ունի ուշադրության մեխանիզմ: Մենք կառուցում ենք բայտի զույգ կոդավորման (BP) մոդելներ զուգահեռ տվյալներում և վերադարձ թարգմանվող միալեզվի ուսուցման տվյալներում, որոնք տրամադրվում են փոքր տվյալների պայմաններում: Մեր համակարգում ուսումնասիրեցինք նաև երկու խորը ճարտարապետություն, շերտի նորմալիզացիա, կշիռի նորմալիզացիա և ուսուցման մոդելներ, որոնք օգտագործում են Ադամը, և այլն: Անգլերեն-չինարեն, չինարեն-անգլերեն պաշտոնական գնահատականները 28.13 և 21.35 են 2016 թվականի փորձարկումների ընթա Գերմաներեն-հոլանդերեն, հոլանդերեն-գերմաներեն, իտալանդերեն-ռոմաներեն-իտալերեն և ռոմաներեն-իտալերեն-իտալերեն պաշտոնական գնահատականները 19.59, 17.95, 18.62 և 20.39 են:', 'az': "Bu kağızda, 2017-ci Sözlü Dil Çeviri(IWSLT) ilə Uluslararası İş Hopu üçün GTCOM'un nöral Makina Çeviri(NMT) sistemlərini tanımırıq. Biz İngilizə-Çinlə-Çinlə-İngilizə və Çinlə-İngilizə tərəfində iki dil işlərinin küçük məlumatların şəkilində və çoxlu dil işlərinin sıfır-shot şəkilində iştirak etdik. Sistemlərimiz kodlayıcı-dekoder arhitektarına dikkatimiz mehanizması ilə dayanılır. Biz byt çift kodlama (BPE) modellərini parallel verilər və yenidən tərcümə edilmiş monodil təhsil məlumatlarını kiçik məlumatlarda təhsil edirik. Sistemimizdə keşif etdiyimiz başqa tekniklərdə iki derin arhitektür, sütun nomalizasyonu, ağırlı normalizasyonu və təhsil modelləri Adəmi qovuşdurmaq və ya.b. Almanca-Holandiya, Holandiya-Almanca, İtalyan-Rumunya-dan-İtalyan-dan-İtalyan olaraq resmi nöqtələri 19.59, 17.95, 18.62 və 20.39 idilər.", 'ko': '본고에서 GTCOM이 2017년 국제구어번역세미나(IWSLT)를 위해 개발한 신경기계번역(NMT) 시스템을 소개한다.이중 언어 임무의 작은 데이터 조건과 다중 언어 임무의 제로 포 조건에서 우리는 영어가 중국어와 중국어에 대한 영어 추적에 참여했다.우리 시스템은 주의 메커니즘을 가진 인코더 - 디코더 체계 구조를 바탕으로 한다.우리는 병렬 데이터에서 바이트 대코딩(BPE) 모델을 구축하고 작은 데이터 조건에서 반역 단어 트레이닝 데이터를 제공한다.우리가 시스템에서 탐색한 다른 기술은 두 가지 심층 구조, 층 규범화, 권중 규범화와 퇴화 아담 훈련 모델 등을 포함한다. 2016년 시험집의 영어 대 중국어, 중국어 대 영어의 공식 점수는 각각 28.13과 21.35 2017년 시험집의 공식 점수는 각각 28.30과 22.16이다.독일의 네덜란드, 네덜란드의 독일, 이탈리아의 루마니아, 루마니아의 이탈리아에 대한 공식 점수는 각각 19.59, 17.95, 18.62와 20.39이다.', 'am': 'በዚህ ገጽ የGTCOM የኔural machine ትርጉም (NMT) ስርዓቶችን ለዓለምአቀፍ የቋንቋ ትርጉም ጉዳይ (IWSLT) 2017 እናብራራለን፡፡ በሁለቱ ቋንቋዎች ስራ እና በብዙልቋንቋ ቋንቋ ስራ ላይ ከታናሽ ዳታዎችን እና በንግግሊዘኛ እና በቻይና-ወደ ኢንግሊዝኛ መንገዶች ተካፈልን፡፡ ስርዓታችን የድምፅ አካባቢ መሠረት ላይ ነው፡፡ በአካባቢው ዳታ እና በተለየ ትርጉም የተዘጋጀው የብጤት ሁለት የኢኮድ (BPE) ሞዴላዎችን እናደርጋለን፡፡ Other techniques we explored in our system include two deep architectures, layer nomalization, weight normalization and training models with annealing Adam, etc. The official scores of English-to-Chinese, Chinese-to-English are 28.13 and 21.35 on test set 2016 and 28.30 and 22.16 on test set 2017.  የፖለቲካ ደረጃዎች የጀርመን-ጀርመን፣ ዳርክ-ጀርመን፣ ጣሊያንኛ-ጀምሮ-ሮማኒያን እና ጀምሮ-ጣሊያንኛ የተባለው ደረጃዎች እንደ 19.59፣ 17.95፣ 18.62 እና 20.39 ናቸው።', 'bs': 'U ovom papiru opisujemo GTCOMov sistem za prevod neuralnih strojeva (NMT) za međunarodno radionico o prevodu jezika govornog jezika (IWSLT) 2017. Mi smo sudjelovali u tragovima engleskog na kineskom i kineskom na engleskom jeziku u malom stanju podataka dvojezičkog zadatka i u stanju nula snimanja multijezičkog zadatka. Naši sistemi su temeljeni na arhitekturi kodera sa pažnjom. Mi gradimo modele kodiranja bajtova par (BPE) u paralelnim podacima i podacima o monojezičkoj obuci natrag predviđenim u malom stanju podataka. Druge tehnike koje smo istražili u našem sustavu uključuju dvije duboke arhitekture, nomalizaciju slojeva, normalizaciju težine i obuku modela s priključenjem Adama itd. Zvanični rezultati engleskog na kineski, kineski na engleski, 28,13 i 21,35 na testu setu 2016. i 28,30 i 22,16 na testu setu 2017. Zvanični rezultati o njemačkom-na-holandskom, holandskom-na-njemačkom, italijanskom-na-rumunskom i rumunskom-na-italijanskom su attiecīgo 19,59, 17,95, 18,62 i 20,39.', 'ca': "En aquest article, descrivim els sistemes de traducció neural de la màquina (NMT) de GTCOM per a l'International Workshop on Spoken Language Translation (IWSLT) 2017. Vam participar en les pistes anglès-xinès-anglès i xinès-anglès en la petita condició de dades de la tasca bilingüe i la condició de fotografia zero de la tasca multilingüe. Els nostres sistemes estan basats en l'arquitectura del codificador-decoder amb mecanisme d'atenció. Construim models de codificació de parell de bytes (BPE) en dades paralleles i dades d'entrenament monolingüe traduïdes de nou proporcionades en la petita condició de dades. D'altres tècniques que vam explorar al nostre sistema són dues arquitectures profundes, nomalització de capes, normalització de pes i models d'entrenament amb Adam, etc. Les puntuacions oficials d'anglès-a-xinès, xinès-a-anglès són 28,13 i 21,35 en el conjunt de provas 2016 i 28,30 i 22,16 en el conjunt de provas 2017. Les puntuacions oficials en alemanya-holandesa-holandesa-alemana, italiana-rumena-rumena-a-italiana són 19,59, 17,95, 18,62 i 20,39 respectivament.", 'bn': 'এই কাগজটিতে আমরা জিটিকোমের নিউরুল মেশিন অনুবাদ (এনএমটি) সিস্টেম ব্যাখ্যা করছি যেখানে কথোপকথন ভাষা অনুবাদের (IWSLT) ২০১৭ সালে আন্ We participated in the English-to-Chinese and Chinese-to-English tracks in the small data condition of the bilingual task and the zero-shot condition of the multilingual task.  আমাদের সিস্টেম এনকোডার-ডেকোডার আর্কিকেটের ভিত্তিতে রয়েছে মনোযোগ দিয়ে। আমরা বাইট জোড়া এনকোডিং (বিপেই) মডেল তৈরি করি সামান্য তথ্য এবং পিছনে অনুবাদ করা মোনোলিভাল ভাষার প্রশিক্ষণের তথ্য যা ছোট তথ্য আমাদের সিস্টেমে আমরা অন্যান্য প্রযুক্তিগুলোর মধ্যে দুটি গভীর প্রতিষ্ঠান, স্তর নোমাজেশন, ওজন স্বাভাবিক এবং প্রশিক্ষণ মডেল ইত্যাদি রয়েছে যার মধ্যে দুটি গুরুত্বপূর্ণ প্রতিষ্ঠান,  জার্মান-থেকে ডাচ, ডাচ-থেকে জার্মান, ইতালিয়ান-থেকে রোমানিয়ান এবং রোমানিয়ান-থেকে ইতালিয়ানের সরকারি স্কোর ১৯. ৫৯, ১৭. ৯৫, ১৮. 62', 'cs': 'V tomto článku popisujeme systémy neuronového strojového překladu (NMT) GTCOM pro Mezinárodní workshop mluveného jazyka překladu (IWSLT) 2017. Podíleli jsme se na anglicko-čínských a čínsko-anglických tratích v malém datovém stavu dvojjazyčného úkolu a nulovém záběru vícejazyčného úkolu. Naše systémy jsou založeny na architektuře kodéru-dekodéru s mechanismem pozornosti. Vytváříme modely kódování bajtových párů (BPE) v paralelních datech a zpětně přeložená jednojjazyčná tréninková data poskytovaná v podmínkách malých dat. Další techniky, které jsme zkoumali v našem systému, zahrnují dvě hluboké architektury, nomalizaci vrstev, normalizaci hmotnosti a tréninkové modely s žíháním Adama atd. Oficiální skóre z angličtiny do čínštiny, čínštiny do angličtiny jsou 28.13 a 21.35 na testovací sadě 2016 a 28.30 a 22.16 na testovací sadě 2017. Oficiální skóre v německo-nizozemském, nizozemském-německém, italském-rumunském a rumunském-italském jsou 19.59, 17.95, 18.62 a 20.39.', 'et': 'Käesolevas töös kirjeldame GTCOM neuraalseid masintõlke (NMT) süsteeme rahvusvahelise rääkiva keele tõlke seminari (IWSLT) 2017 jaoks. Osalesime inglise-hiina ja hiina-inglise rajadel kakskeelse ülesande väikeste andmete tingimuses ja mitmekeelse ülesande null-shot tingimuses. Meie süsteemid põhinevad kodeerija-dekooderi arhitektuuril tähelepanumehhanismiga. Ehitame baidipaari kodeerimise (BPE) mudeleid paralleelsetes andmetes ja tagasitõlgitud ühekeelsetes koolitusandmetes väikeste andmete tingimustes. Teised meie süsteemis uuritud tehnikad hõlmavad kahte sügavat arhitektuuri, kihi nomaliseerimist, kaalu normaliseerimist ja treeningmudelit lõõmutava Adamiga jne. Ametlikud hinded inglise-hiina, hiina-inglise-inglise-keele kohta on 28.13 ja 21.35 testikomplektis 2016 ja 28.30 ja 22.16 testikomplektis 2017. Ametlikud tulemused Saksa-Hollandi, Hollandi-Saksa, Itaalia-Rumeenia ja Rumeenia-Itaalia vahel on vastavalt 19,59, 17,95, 18,62 ja 20,39.', 'fi': 'Tﾃ､ssﾃ､ artikkelissa kuvataan GTCOM:n neurokonekﾃ､ﾃ､nnﾃｶsjﾃ､rjestelmﾃ､t (NMT) International Workshop on Spoken Language Translation (IWSLT) 2017. Osallistuimme englanti-kiina- ja kiina-englanti-raitoihin kaksikielisen tehtﾃ､vﾃ､n pienen datan tilassa ja monikielisen tehtﾃ､vﾃ､n nollashot-tilassa. Jﾃ､rjestelmﾃ､mme perustuvat kooderi-dekooderiarkkitehtuuriin huomiomekanismilla. Rakennamme byte pair encoding (BPE) -malleja rinnakkaisdataan ja taaksekﾃ､ﾃ､nnettyyn monikieliseen harjoitteludataan pienissﾃ､ dataolosuhteissa. Muita tekniikoita, joita tutkimme jﾃ､rjestelmﾃ､ssﾃ､mme, ovat kaksi syvﾃ､arkkitehtuuria, kerroksen nomalisointi, painon normalisointi ja harjoitusmallit hehkutuksella Adamilla jne. Englanti-kiina, kiina-englanti viralliset pisteet ovat 28.13 ja 21.35 testisarjassa 2016 ja 28.30 ja 22.16 testisarjassa 2017. Viralliset pisteet saksasta-hollantiin, hollantista-saksaan, italiasta-romaniaan ja romaniasta-italiaan ovat 19,59, 17,95, 18,62 ja 20,39.', 'jv': 'Nang pepulan iki, kéné ngerasakno sistem tarjamahan (NMT) ning GTTOm karo Gambar IWSLT, nggo langgambar IWSLT, nggawe Kasunyatan Jejaring Awak dhéwé ngejaraké Inggris-Catane karo Cainan-karo-Inggris kuwi nggambar obah sing apik dhéwé kuwi nggawe barang tanggal gawe ngubah dhéwé lan ora iso nggawe balé sing oleh nggawe luwih. Sistem awak dhéwé digawesi uwong karo akeh koder-decoder karo mekanistik. Awak dhéwé nggawe model sing dibenalke mrogram Awak dhéwé éntuk teknik sing beraksi ning sistèm dhéwé mulai gawe architectures, layer Nomalization, ndèhku nggawe barang nggawe sistem sing dibenakno karo annearing man, itlan. Punika offisisi sing dibenakaké Inggris-to-Chinese, Chinese-to Punika dipoleh sing nganggo alaman-nganggo-alaman, dadi-nganggo-alaman, italian-nganggo-rumani karo-italian-nganggo-italian sing 19.', 'sk': 'V prispevku opisujemo sisteme nevronskega strojnega prevajanja (NMT) GTCOM za mednarodno delavnico o govorjenem jeziku (IWSLT) 2017. Sodelovali smo na sledeh angleško-kitajsko-kitajsko-kitajsko in kitajsko-angleško-angleško v stanju majhnih podatkov dvojezične naloge in stanju ničelnega strela večjezične naloge. Naši sistemi temeljijo na arhitekturi kodirnika-dekoderja z mehanizmom pozornosti. Modele kodiranja bajtnih parov (BPE) gradimo v vzporednih podatkih in v nazaj prevedenih enojezičnih podatkih o usposabljanju, zagotovljenih v stanju majhnih podatkov. Druge tehnike, ki smo jih raziskovali v našem sistemu, vključujejo dve globoki arhitekturi, slojno nomalizacijo, normalizacijo teže in modele treninga z žarjenjem Adama itd Uradni rezultati angleško-kitajsko, kitajsko-angleško-angleško so 28,13 in 21,35 na testnem kompletu 2016 ter 28,30 in 22,16 na testnem kompletu 2017. Uradni rezultati za nemško-nizozemsko, nizozemsko-nemško, italijansko-romunsko in romunsko-italijansko so 19,59, 17,95, 18,62 oziroma 20,39.', 'he': 'בעיתון הזה, אנחנו מתארים את מערכות התרגום המכונה העצבית (NMT) של GTCOM עבור הספר הבינלאומי על התרגום לשפה מדברת (IWSLT) 2017. השתתפנו במסלולות אנגלית-לסינית וסינית-לאנגלית במצב הנתונים הקטן של המשימה השולשית והמצב האפס של המשימה השולשית. המערכות שלנו מבוססות על הארכיטקטורה של הקודר-מפענח עם מנגנון תשומת לב. אנחנו בונים דוגמנים של קידוד זוג בייטים (BPE) במידע מקביל ומידע אימון מונושפתי מתרגם מאחור שנוסף במצב הנתונים הקטן. טכניקות אחרות שחקרנו במערכת שלנו כוללות שני ארכיטקטורות עמוקות, נומליזציה של שכבות, נורמליזציה במשקל ומודלים אימונים עם אנגלית-לסינית, סינית-לאנגלית הם 28.13 ו-21.35 בסט מבחן 2016 ו-28.30 ו-22.16 בסט מבחן 2017. התוצאות הרשמיות בגרמנית-להולנדית-לגרמנית, הולנדית-לגרמנית-איטלקית-לרומנית-לרומנית-לאיטלקית הן 19.59, 17.95, 18.62 ו-20.39.', 'ha': "Daga wannan takardan, Munã bayyana GTCOM's neural translation system (NMT) system for the International workspace on Talk language Translate (IWSLT) 2017. Mun yi shirin hanyõyin English-to-China-to-English in the small data state of the aikin lugha biyu and the no-shot state of the multilinglanguage. Systemyinmu ne an daidaita matsayin kode-koda da da masu zane-zane. Tuna samar da bayte kodi biyu (BLE) misalin mutane da ke daidaita data kuma data masu tsari da bakin-translate monoli-language wanda aka bãyar da cikin ƙarami data. Tsamman masu aikin matsayin da muka samu a cikin tsarin mu, yana da matsakalakin masu ƙaranci biyu, nomalisiya ga wurãre, masu shirya nau'i da shiryoyin tababar da aka kama da Ãdam, etc. Ana karatun rasmi a kan jeruman-zuwa-Ducha, Ducha-zuwa-jeruman, Italian-zuwa-Rumoniyan kuma-Italian-zuwa-Talyaaniya henyu 19.59, 17.95, 18.63 da 20.39.", 'bo': "In this paper, we describe GTCOM's neural machine translation(NMT) systems for the International Workshop on Spoken Language Translation(IWSLT) 2017. ང་ཚོ་སྐད་ཡིག་ཆ་ལ་རྒྱ་ནག་དང་རྒྱ་ནག་ལ་དབྱིན་ཡིག་གི་གླེང་སྒྲུང་ནང་བཙུགས་བྱས། ང་ཚོའི་མ་ལག་གིས་གསང་ཨང་བྱེད་ཀྱི་སྒྲིག་ཆས་གཞི་རྟེན་འདུག་གི་གནས་སྟངས་ཉམས་འཇོག་བྱེད་ཀྱི་ཡོད། ང་ཚོས་བདུན་ཕྱོགས་ཀྱི་གསལ་བཤད་ཀྱི་ཆ་byte pair་encoding (BPE)མིང་སྒྲིག་ཆ་འཕྲིན་དང་back-translated monolingual training data་སྣང་ཚུལ་ཆུང་ཀུ་བྱིན་ཡོད། Other techniques we explored in our system include two deep architectures, layer nomalization, weight normalization and training models with annealing Adam, etc. The official scores of English-to-Chinese, Chinese-to-English are 28.13 and 21.35 on test set 2016 and 28.30 and 22.16 on test set 2017. གཞུང་འབྲེལ་གྱི་སྐོར་གྲངས་མིན་ཐག་(German-to-Dutch), Dutch-to-German, Italian-to-Romanian-to-Italian) ཡིན་པ་ཚུ་respectively 19.59, 17.95, 18.62 སོར་20.39"}
{'en': 'Kyoto University MT System Description for IWSLT 2017 K yoto  U niversity  MT  System Description for  IWSLT  2017', 'ar': 'وصف نظام MT لجامعة كيوتو لـ IWSLT 2017', 'fr': "Description du système MT de l'Université de Kyoto pour IWSLT 2017", 'pt': 'Descrição do Sistema MT da Universidade de Kyoto para IWSLT 2017', 'es': 'Descripción del sistema MT de la Universidad de Kyoto para IWSLT 2017', 'ja': 'IWSLT 2017の京都大学MTシステム説明', 'ru': 'Описание системы MT Киотского университета для IWSLT 2017', 'zh': '京都大学机器翻译统明 IWSLT 2017', 'hi': 'IWSLT 2017 के लिए क्योटो विश्वविद्यालय एमटी सिस्टम विवरण', 'ga': 'Cur síos ar Chóras MT Ollscoil Kyoto do IWSLT 2017', 'ka': 'კიოტო სუნივერტის MT სისტემის გამოსახულება IWSLT 2017', 'el': 'Περιγραφή συστήματος του Πανεπιστημίου Κιότο για το IWSLT 2017', 'hu': 'Kiotói Egyetem MT System Description for IWSLT 2017', 'it': "Descrizione del sistema MT dell'Università di Kyoto per IWSLT 2017", 'kk': 'Киото университетінің MT жүйесінің IWSLT 2017 сипаттамасы', 'lt': 'Kioto universiteto MT sistemos aprašymas IWSLT 2017 m.', 'ml': 'ക്യോട്ടോ യൂണിവേര്\u200dട്ടിയിലെ എംടി സിസ്റ്റം വിവരണം IWSLT 2017', 'mk': 'Description of the Kyoto University MT System for IWSLT 2017', 'ms': 'Huraian Sistem MT Universiti Kyoto untuk IWSLT 2017', 'mn': 'IWSLT 2017 оны Киото сургуулийн MT системийн тодорхойлолт', 'no': 'Kyoto University MT System- skildring for IWSLT 2017', 'ro': 'Kyoto University MT System Descriere pentru IWSLT 2017', 'sr': 'Описане Киотског университета MT система за IWSLT 2017', 'si': 'IWSLT 2017 සඳහා කියෝටෝ විශ්වාසය MT පද්ධති විස්තර', 'so': 'Kyoto Jaamacadda MT Description for IWSLT 2017', 'sv': 'Kyoto University MT System Beskrivning för IWSLT 2017', 'pl': 'Opis systemu MT na Uniwersytecie Kyoto dla IWSLT 2017', 'mt': 'Deskrizzjoni tas-Sistema MT tal-Università ta’ Kjoto għall-IWSLT 2017', 'ur': 'IWSLT 2017 کے لئے کیوٹو یونیوریسٹ MT سیسٹم سپٹم کا سپٹرانی', 'ta': 'IWSLT 2017 க்கான Kyoto University MT அமைப்பு விவரம்', 'uz': 'IWSLT 2017 uchun Kyoto University MT tizim taʼrifi', 'vi': 'Mô tả hệ thống giao thoa IWSLT bây giờ', 'hr': 'Kioto Univerzitet MT sustav opisa za IWSLT 2017', 'da': 'Kyoto University MT System Beskrivelse for IWSLT 2017', 'nl': 'Kyoto University MT Systeembeschrijving voor IWSLT 2017', 'bg': 'Описание на системата за МТ на Университета Киото 2017', 'id': 'Deskripsi Sistem MT Universitas Kyoto untuk IWSLT 2017', 'de': 'Kyoto University MT Systembeschreibung für IWSLT 2017', 'ko': '2017년 IWSLT 교토대학 MT 시스템 설명', 'fa': 'Description of the MT University of Kyoto System for IWSLT 2017', 'af': 'Kyoto Universiteit MT Stelsel Beskrywing vir IWSLT 2017', 'sw': 'Maelezo ya Mfumo wa MT wa Chuo Kikuu cha Kyoto kwa ajili ya IWSLT 2017', 'tr': 'Kioto Uniwersitet MT Sistem Wasp IWSLT 2017 üçin', 'sq': 'Përshkrimi i Sistemit MT të Universitetit të Kioto për IWSLT 2017', 'am': 'Kyoto University MT System Description for IWSLT 2017', 'hy': 'Կիոտո համալսարանի MT համակարգի նկարագրությունը 2017-ին', 'az': 'IWSLT 2017 칲칞칲n Kioto Universiteti MT Sistemi T톛rc칲m톛', 'bn': 'IWSLT ২০১৭ এর জন্য কিয়োটো বিশ্ববিদ্যালয় এমটি সিস্টেম বিবরণ', 'bs': 'Kioto Univerzitet MT Sistemski opis za IWSLT 2017', 'cs': 'Kyoto University MT Popis systému pro IWSLT 2017', 'ca': "Descripció del sistema MT de la Universitat de Kyoto per l'IWSLT 2017", 'et': 'Kyoto Ülikooli MT süsteemi kirjeldus IWSLT 2017', 'fi': 'Kyoto University MT System Description for IWSLT 2017', 'he': 'תיאור מערכת MT של אוניברסיטת קיוטו IWSLT 2017', 'jv': 'Kiotong Universite MT Sistem Keterangan kanggo IWSLT 1997', 'sk': 'Kyoto University MT Opis sistema za IWSLT 2017', 'ha': 'Description for IWSLT 2017', 'bo': 'Kyoto University MT System Description for IWSLT 2017'}
{'en': 'We describe here our Machine Translation (MT) model and the results we obtained for the IWSLT 2017 Multilingual Shared Task. Motivated by Zero Shot NMT [ 1 ] we trained a Multilingual Neural Machine Translation by combining all the training data into one single collection by appending the tokens to the source sentences in order to indicate the target language they should be translated to. We observed that even in a low resource situation we were able to get translations whose quality surpass the quality of those obtained by Phrase Based Statistical Machine Translation by several BLEU points. The most surprising result we obtained was in the zero shot setting for Dutch-German and Italian-Romanian where we observed that despite using no parallel corpora between these language pairs, the NMT model was able to translate between these languages and the translations were either as good as or better (in terms of BLEU) than the non zero resource setting. We also verify that the NMT models that use feed forward layers and self attention instead of recurrent layers are extremely fast in terms of training which is useful in a NMT experimental setting.', 'ar': 'نصف هنا نموذج الترجمة الآلية (MT) والنتائج التي حصلنا عليها للمهمة المشتركة متعددة اللغات IWSLT 2017. بدافع من Zero Shot NMT [1] ، قمنا بتدريب الترجمة الآلية العصبية متعددة اللغات من خلال دمج جميع بيانات التدريب في مجموعة واحدة من خلال إلحاق الرموز المميزة بجمل المصدر للإشارة إلى اللغة الهدف التي يجب الترجمة إليها. لاحظنا أنه حتى في حالة الموارد المنخفضة ، تمكنا من الحصول على ترجمات تفوق جودتها جودة تلك التي حصلت عليها الترجمة الآلية الإحصائية القائمة على العبارات من خلال العديد من نقاط BLEU. كانت النتيجة الأكثر إثارة للدهشة التي حصلنا عليها في إعداد اللقطة الصفرية للهولندية الألمانية والإيطالية الرومانية حيث لاحظنا أنه على الرغم من عدم استخدام مجموعات موازية بين أزواج اللغات هذه ، فإن نموذج NMT كان قادرًا على الترجمة بين هذه اللغات وكانت الترجمات إما جيد أو أفضل (من حيث BLEU) من إعداد الموارد غير الصفرية. نتحقق أيضًا من أن نماذج NMT التي تستخدم طبقات التغذية الأمامية والاهتمام الذاتي بدلاً من الطبقات المتكررة سريعة للغاية من حيث التدريب وهو أمر مفيد في إعداد تجريبي لـ NMT.', 'fr': "Nous décrivons ici notre modèle de traduction automatique (TA) et les résultats que nous avons obtenus pour la tâche partagée multilingue IWSLT 2017. Motivés par Zero Shot NMT [1], nous avons formé une traduction automatique neuronale multilingue en combinant toutes les données d'entraînement en une seule collection en ajoutant les jetons aux phrases sources afin d'indiquer la langue cible vers laquelle elles doivent être traduites. Nous avons constaté que même dans une situation de ressources limitées, nous pouvions obtenir des traductions dont la qualité surpassait la qualité de celles obtenues par la traduction automatique statistique basée sur des phrases de plusieurs points BLEU. Le résultat le plus surprenant que nous avons obtenu a été le réglage zéro pour le néerlandais-allemand et l'italo-roumain, où nous avons observé que, malgré l'absence de corpus parallèles entre ces paires de langues, le modèle NMT était capable de traduire entre ces langues et les traductions étaient aussi bonnes ou meilleures (dans termes de l'UEBL) que le paramètre de ressources différent de zéro. Nous vérifions également que les modèles NMT qui utilisent des couches d'avance et une attention personnelle au lieu de couches récurrentes sont extrêmement rapides en termes d'entraînement, ce qui est utile dans un contexte expérimental de NMT.", 'es': 'Describimos aquí nuestro modelo de traducción automática (MT) y los resultados que obtuvimos para la tarea compartida multilingüe IWSLT 2017. Motivados por Zero Shot NMT [1], entrenamos una traducción automática neuronal multilingüe combinando todos los datos de entrenamiento en una sola colección añadiendo los tokens a las oraciones fuente para indicar el idioma de destino al que deben traducirse. Observamos que incluso en una situación de pocos recursos, pudimos obtener traducciones cuya calidad superaba la calidad de las obtenidas por la traducción automática estadística basada en frases en varios puntos BLEU. El resultado más sorprendente que obtuvimos fue en la configuración de tiro cero para el neerlandés-alemán y el italo-rumano, donde observamos que, a pesar de no utilizar corpus paralelos entre estos pares de idiomas, el modelo NMT podía traducir entre estos idiomas y las traducciones eran tan buenas o mejores (en términos de BLEU) que la configuración de recursos distinta de cero. También verificamos que los modelos de NMT que utilizan capas de alimentación hacia adelante y autoatención en lugar de capas recurrentes son extremadamente rápidos en términos de entrenamiento, lo que es útil en un entorno experimental de NMT.', 'pt': 'Descrevemos aqui nosso modelo de tradução automática (MT) e os resultados que obtivemos para a tarefa compartilhada multilíngue IWSLT 2017. Motivados pelo Zero Shot NMT [1], treinamos uma Tradução Automática Neural Multilíngue combinando todos os dados de treinamento em uma única coleção anexando os tokens às sentenças de origem para indicar o idioma de destino para o qual deveriam ser traduzidos. Observamos que mesmo em uma situação de poucos recursos conseguimos obter traduções cuja qualidade supera em vários pontos a qualidade das obtidas pela Tradução Automática Estatística Baseada em Frases. O resultado mais surpreendente que obtivemos foi na configuração de tiro zero para holandês-alemão e ítalo-romeno, onde observamos que, apesar de não usar corpora paralelos entre esses pares de idiomas, o modelo NMT foi capaz de traduzir entre esses idiomas e as traduções foram tão bom ou melhor (em termos de BLEU) do que a configuração de recurso diferente de zero. Verificamos também que os modelos NMT que utilizam camadas feed forward e autoatenção ao invés de camadas recorrentes são extremamente rápidos em termos de treinamento o que é útil em um cenário experimental NMT.', 'hi': 'हम यहां हमारे मशीन अनुवाद (एमटी) मॉडल और आईडब्ल्यूएसएलटी 2017 बहुभाषी साझा कार्य के लिए प्राप्त परिणामों का वर्णन करते हैं। जीरो शॉट एनएमटी [1] से प्रेरित होकर हमने सभी प्रशिक्षण डेटा को एक एकल संग्रह में जोड़कर एक बहुभाषी तंत्रिका मशीन अनुवाद को प्रशिक्षित किया, ताकि लक्ष्य भाषा को इंगित करने के लिए स्रोत वाक्यों में टोकन जोड़कर उन्हें अनुवादित किया जा सके। हमने देखा कि यहां तक कि एक कम संसाधन की स्थिति में भी हम अनुवाद प्राप्त करने में सक्षम थे जिनकी गुणवत्ता कई BLEU बिंदुओं द्वारा वाक्यांश आधारित सांख्यिकीय मशीन अनुवाद द्वारा प्राप्त उन लोगों की गुणवत्ता को पार कर जाती है। सबसे आश्चर्यजनक परिणाम जो हमने प्राप्त किया वह डच-जर्मन और इतालवी-रोमानियाई के लिए शून्य शॉट सेटिंग में था, जहां हमने देखा कि इन भाषा जोड़े के बीच कोई समानांतर निगम का उपयोग करने के बावजूद, एनएमटी मॉडल इन भाषाओं के बीच अनुवाद करने में सक्षम था और अनुवाद या तो गैर शून्य संसाधन सेटिंग की तुलना में अच्छे या बेहतर (BLEU के संदर्भ में) थे। हम यह भी सत्यापित करते हैं कि एनएमटी मॉडल जो आवर्तक परतों के बजाय फ़ीड फॉरवर्ड परतों और आत्म-ध्यान का उपयोग करते हैं, प्रशिक्षण के मामले में बेहद तेज हैं जो एनएमटी प्रयोगात्मक सेटिंग में उपयोगी है।', 'ja': 'ここでは、機械翻訳（ MT ）モデルと、IWSLT 2017多言語共有タスクの結果について説明します。 ゼロショットNMT [1]に動機づけられて、私たちは、翻訳すべきターゲット言語を示すために、トークンをソース文に追加することによって、すべてのトレーニングデータを1つのコレクションに統合することによって、多言語ニューラルマシン翻訳を訓練しました。 私たちは、リソースの少ない状況でも、フレーズベースの統計機械翻訳で得られた品質を上回る品質の翻訳を、いくつかのBLEUポイントで得ることができたことを観察しました。 最も驚くべき結果は、オランダ語-ドイツ語とイタリア語-ルーマニア語のゼロショット設定で得られたもので、これらの言語ペア間で平行コーパスを使用していないにもかかわらず、NMTモデルはこれらの言語間で翻訳することができ、翻訳は非ゼロリソース設定と同等か、それよりも優れていることが観察されました。 また、繰り返しレイヤーの代わりにフィードフォワードレイヤーと自己注目を使用するNMTモデルは、NMT実験環境で役立つトレーニングの点で非常に高速であることも確認します。', 'zh': '吾于此见吾机器翻译(MT)模形及吾为IWSLT 2017多言共功之效也。 零射 NMT [1] 之激劝,附之源句,合诸训练数于一会,以示翻译成言,而练多言神经机器翻译。 吾观之,虽匮于资源,吾得过短语之计机器翻译BLEU点之译。 吾得最可讶者,荷兰语 - 德语、意大利语 - 罗马尼亚语之零镜头置中,虽无并行语料库,然NMT能译于其间,而译与非零资设一善(BLEU而言)。 又验用前馈层自我注意而非循环层者NMT其形甚速,甚有用于NMT实验也。', 'ru': 'Здесь мы описываем нашу модель машинного перевода (MT) и результаты, полученные для многоязычной общей задачи IWSLT 2017. Основываясь на Zero Shot NMT [1], мы обучили многоязычный нейронный машинный перевод, объединив все обучающие данные в одну коллекцию, добавив маркеры к исходным предложениям, чтобы указать целевой язык, на который они должны быть переведены. Мы заметили, что даже при низкой ресурсной ситуации мы смогли получить переводы, качество которых превосходит качество, полученное с помощью статистического машинного перевода на основе фраз, на несколько пунктов BLEU. Самый удивительный результат, который мы получили, был в настройке нулевого снимка для голландско-немецкого и итальяно-румынского языков, где мы заметили, что, несмотря на использование параллельных тел между этими языковыми парами, модель NMT смогла перевести между этими языками, и переводы были либо так же хороши, либо лучше (с точки зрения BLEU), чем установка ненулевого ресурса. Мы также проверяем, что модели NMT, в которых используются прямые слои подачи и самовнимание вместо повторяющихся слоев, чрезвычайно быстры с точки зрения обучения, что полезно в экспериментальных условиях NMT.', 'ga': 'Déanaimid cur síos anseo ar ár múnla Aistriúcháin Meaisín (MT) agus na torthaí a fuaireamar do Thasc Comhroinnte Ilteangach 2017 IWSLT. Spreagtha ag Zero Shot NMT [1] chuireamar oiliúint ar Aistriúchán Inneall Néarach Ilteangach trí na sonraí oiliúna go léir a chomhcheangal in aon bhailiúchán amháin trí na comharthaí a chur i gceangal leis na habairtí foinseacha chun an sprioctheanga ar cheart iad a aistriú chuici a léiriú. Thugamar faoi deara, fiú amháin i gcás acmhainní ísle, go rabhamar in ann aistriúcháin a sháraíonn cáilíocht na n-aistriúchán Meaisín Staidrimh Fhrás-Bhunaithe roinnt pointí BLEU a fháil. Ba é an toradh is ionadh a fuaireamar ná sa socrú urchar nialasach don Ollainnis-Gearmáinis agus don Iodáilis-Rómáinis nuair a thugamar faoi deara, ainneoin nach raibh corpóra comhthreomhar á úsáid idir na péirí teangacha seo, go raibh samhail an NMT in ann aistriú idir na teangacha seo agus bhí na haistriúcháin mar a chéile. maith mar nó níos fearr (i dtéarmaí BLEU) ná an socrú acmhainní neamh-nialas. Deimhnímid freisin go bhfuil na samhlacha NMT a úsáideann sraitheanna beatha ar aghaidh agus féin-aird in ionad sraitheanna athfhillteacha thar a bheith tapa i dtéarmaí oiliúna atá úsáideach i suíomh turgnamhach NMT.', 'hu': 'Itt ismertetjük a Gépi Fordítás (MT) modellünket és az IWSLT 2017 többnyelvű megosztott feladat eredményeit. A Zero Shot NMT [1] motiválta a többnyelvű neurális fordítást, hogy az összes edzési adatot egyetlen gyűjteménybe ötvöztük, a tokeneket hozzáadtuk a forrás mondatokhoz annak érdekében, hogy jelezzük, milyen célnyelvre kell fordítani. Megfigyeltük, hogy még alacsony erőforrás esetén is sikerült olyan fordításokat kapnunk, amelyek minősége több BLEU ponttal meghaladja a Phrase Based Statisztikai Gépi Fordítás minőségét. A legmeglepőbb eredményünk a holland-német és olasz-román nulla lövésben volt, ahol megfigyeltük, hogy annak ellenére, hogy ezek között a nyelvpárok között nem használtak párhuzamos korpuszokat, az NMT modell képes volt lefordítani ezek között a nyelvek között, és a fordítások vagy olyan jók, vagy jobb (BLEU tekintetében), mint a nem nulla erőforrás beállítás. Ellenőrizzük továbbá, hogy a visszatérő rétegek helyett előrelátó és önfigyelmet használó NMT modellek rendkívül gyors az NMT kísérleti környezetben hasznos edzés szempontjából.', 'el': 'Περιγράφουμε εδώ το μοντέλο μηχανικής μετάφρασης (ΜΤ) και τα αποτελέσματα που λάβαμε για την Πολυγλωσσική Κοινή Εργασία. Με κίνητρο το εκπαιδεύσαμε μια Πολυγλωσσική Νευρική Μηχανική Μετάφραση συνδυάζοντας όλα τα δεδομένα εκπαίδευσης σε μία συλλογή προσθέτοντας τα σήματα στις προτάσεις προέλευσης για να υποδείξουμε τη γλώσσα-στόχο στην οποία θα πρέπει να μεταφραστούν. Παρατηρήσαμε ότι ακόμη και σε μια κατάσταση χαμηλού πόρου καταφέραμε να πάρουμε μεταφράσεις των οποίων η ποιότητα ξεπερνά την ποιότητα εκείνων που λαμβάνονται από τη Στατιστική Μηχανική Μετάφραση με βάση τις φράσεις κατά αρκετά σημεία. Το πιο εκπληκτικό αποτέλεσμα που λάβαμε ήταν το μηδενικό πλάνο για τα ολλανδικά-γερμανικά και τα ιταλικά-ρουμανικά όπου παρατηρήσαμε ότι παρά τη χρήση παράλληλων σωμάτων μεταξύ αυτών των γλωσσικών ζευγαριών, το μοντέλο NMT ήταν σε θέση να μεταφράσει μεταξύ αυτών των γλωσσών και οι μεταφράσεις ήταν είτε τόσο καλές όσο και καλύτερες (από την άποψη της BLEU) από τη μη μηδενική ρύθμιση πόρων. Επιβεβαιώνουμε επίσης ότι τα μοντέλα που χρησιμοποιούν στρώματα τροφοδοσίας και αυτοπροσοχή αντί για επαναλαμβανόμενα στρώματα είναι εξαιρετικά γρήγορα από την άποψη της εκπαίδευσης που είναι χρήσιμο σε ένα πειραματικό περιβάλλον NMT.', 'it': "Descriviamo qui il nostro modello di traduzione automatica (MT) e i risultati ottenuti per l'IWSLT 2017 Multilingual Shared Task. Motivati da Zero Shot NMT [1] abbiamo addestrato una traduzione automatica neurale multilingue combinando tutti i dati di allenamento in un'unica raccolta aggiungendo i token alle frasi di origine al fine di indicare la lingua di destinazione in cui devono essere tradotti. Abbiamo osservato che anche in una situazione di scarsa risorse siamo stati in grado di ottenere traduzioni la cui qualità supera quella ottenuta da Phrase Based Statistical Machine Translation di diversi punti BLEU. Il risultato più sorprendente che abbiamo ottenuto è stato l'impostazione zero shot per l'olandese-tedesco e l'italiano-rumeno dove abbiamo osservato che, pur non utilizzando corpi paralleli tra queste coppie linguistiche, il modello NMT era in grado di tradurre tra queste lingue e le traduzioni erano buone o migliori (in termini di BLEU) dell'impostazione non zero risorse. Verifichiamo inoltre che i modelli NMT che utilizzano strati feed forward e auto attenzione invece di strati ricorrenti sono estremamente veloci in termini di allenamento che è utile in un ambiente sperimentale NMT.", 'lt': 'Čia apibūdiname mūsų mašinų vertimo (MT) model į ir rezultatus, gautus 2017 m. IWSLT daugiakalbės bendros užduoties srityje. Motyvuota nulinio nuotraukos NMT [1] mes apmokėme daugiakalbį neurologinį mašin ų vertimą, sujungdami visus mokymo duomenis į vieną rinkinį, pridedant simbolius prie pradinių sakinių, kad būtų nurodyta tikslinė kalba, į kurią jie turėtų būti vertami. Mes pastebėjome, kad net esant mažai išteklių, mums pavyko gauti vertimus, kurių kokybė viršija kokybę, kurią gauna kai kuriais BLEU taškais atlikus frazėmis pagrįstą statistikos mašin ų vertimą. Labiausiai nustebinantis rezultatas buvo nulinis Dutch-German ir Italian-Romanian atžvilgiu, kai pastebėjome, kad nepaisant lygiagrečios korpros tarp šių kalbų porų, NMT modelis sugebėjo išversti tarp šių kalbų, o vertimai buvo tokie patys gerai kaip arba geriau (BLEU atžvilgiu) nei ne nulinis išteklių nustatymas. Taip pat tikriname, kad NMT modeliai, kurie naudoja pašarų pirmuosius sluoksnius ir savarankišką dėmesį vietoj pakartotinių sluoksnių, yra itin greiti mokymo, kuris yra naudingas NMT eksperimentinėje aplinkoje.', 'ka': 'ჩვენ აღწერეთ ჩვენი მაქსინური გადაწყვეტილება (MT) მოდელი და მივიღეთ შედეგი IWSLT 2017 მრავალენგური გაყოფილი დავალებისთვის. Zero Shot NMT-ის მოტივირებული [1] ჩვენ მრავალენგური ნეიროლური მაქინის გარგება შევცვალოთ, ყველა შემწყვეტის მონაცემები ერთი კოლექციაში ერთად გარგებით გამოყენებული სიმბოლოს წესების დამატებით, რომ მინ ჩვენ შევხედავთ, რომ ჩვენ კონფიგური რესურსის სიტუაციაში ჩვენ შეგვიძლია გავიღოთ განაცვლების საფუნქცია, რომელიც საფუნქცია უფრო მეტი BLEU წერტილებით მიიღებული ფრასის ბა ჩვენ მივიღეთ ყველაზე გასაკვირვებელი შედეგი იყო ნულ სტატის შენახვედში ჰონდენური-გერმანური და იტალიური-პომინური, სადაც ჩვენ დავხედავთ, რომ ამ ენის ზოგების შორის პარალელი კოპორა არ გამოყენება, NMT მოდელი შეუძლია ამ ენების შ ჩვენ ასევე დავწერეთ, რომ NMT მოდელები, რომლებიც გამოყენებენ წინასწარმოდგენების წინასწარმოდგენების და თავისწარმოდგენების ნაცვლად რეკურენტის წინასწარმოდგენების შეცვლად ძ', 'ms': 'Kami menggambarkan di sini model Terjemahan Mesin (MT) kami dan hasil yang kami dapatkan untuk Tugas Berberbagai Bahasa IWSLT 2017. Motivated by Zero Shot NMT [1] we trained a Multilingual Neural Machine Translation by combining all the training data in to one single collection by adding the tokens to the source sentences in order to indicate the target language they should be translated to. We observed that even in a low resource situation we were able to get translations whose quality surpass the quality of those obtained by Phrase Based Statistical Machine Translation by several BLEU points.  Hasil yang paling mengejutkan yang kami dapatkan adalah dalam tetapan tembakan sifar untuk Belanda-Jerman dan Italia-Romania di mana kami memperhatikan bahawa walaupun menggunakan corpora selari antara pasangan bahasa ini, model NMT mampu menerjemahkan antara bahasa ini dan terjemahan sama ada sebaik atau lebih baik (dalam terma BLEU) daripada tetapan sumber bukan sifar. Kami juga mengesahkan bahawa model NMT yang menggunakan lapisan hadapan sumber dan perhatian diri selain lapisan berulang sangat cepat dalam terma latihan yang berguna dalam seting percubaan NMT.', 'mk': 'Овде го опишуваме нашиот модел за машински превод (МТ) и резултатите кои ги добивме за IWSLT 2017 Multilingual Shared Task. Мотивирано од НМТ со нула пукање [1] трениравме мултијазичен неурален машински превед со комбинација на сите податоци за тренинг во една колекција со додавање на знаците на изворните реченици со цел да го индицираме јазикот на кој треба да се преведат. We observed that even in a low resource situation we were able to get translations whose quality surpass the quality of those obtained by Phrase Based Statistical Machine Translation by several BLEU points.  Најизненадувачкиот резултат што го добивме беше во поставувањето на нула пукање за холандско-германско и италијанско-романско каде забележавме дека и покрај тоа што не користеше паралелна корпора помеѓу овие јазички парови, моделот НМТ беше во можност да преведе помеѓу овие јазици и преводите биле или подобри (во поглед на Бле We also verify that the NMT models that use feed forward layers and self attention instead of recurrent layers are extremely fast in terms of training which is useful in a NMT experimental setting.', 'kk': 'Мұнда Машин аудару (MT) моделімізді және 2017 IWSLT көп тілді ортақтастырылған тапсырманың нәтижелерін таңдап береміз. Zero Shot NMT [1] арқылы жылжытылған бірнеше тілді нейралық машинаның аудармасын бір жинақтағы барлық оқыту деректерін бір жинақтағымен белгілерді көзінің сөйлемелеріне қосу үшін, олардың аударылатын тілді көрсетуге болады. Біз көп ресурс жағдайында да біз фраз негізінде статистикалық машинаның аудармаларының сапатын бірнеше BLEU нүктелерінен аударуға мүмкін болдық. Біз олардың ең қызықтық нәтижесі нәтижесінде олардың-немецтер және итальян-румандықтағы нөл шарт баптауларында болды. Бұл тілдер арасындағы параллелі корпора қолданбаға қарамастан, NMT моделі бұл тілдер мен аудармалардың арасындағы аудармалары нөл ресур Біз сондай-ақ NMT үлгілерін қайталанатын қабаттардың орнына қайталанатын қабаттарды және өзіңізді қайталанатын қабаттарды қолданатын NMT эксперименталдық параметрлерінде пайдалы оқыту үшін өте те', 'mt': "Hawnhekk niddeskrivu l-mudell tagħna tat-Traduzzjoni bil-Makkinarju (MT) u r-riżultati li nkisbu għall-IWSLT 2017 Multilingual Shared Task. Motivat minn Zero Shot NMT [1] a ħna mħarrġin Traduzzjoni Multilingwi ta’ Makkinarju Newrali billi kkombinaw id-dejta kollha tat-taħriġ f’ġabra waħda billi żiedu t-tokens mas-sentenzi tas-sors sabiex tindika l-lingwa fil-mira li għandhom jiġu tradotti għaliha. Aħna osservajna li anke f’sitwazzjoni ta’ riżorsi baxxi rnexxielna niksbu traduzzjonijiet li l-kwalità tagħhom taqbeż il-kwalità ta’ dawk miksuba mit-Traduzzjoni tal-Magna Statistika bbażata fuq il-Frażi b’diversi punti tal-BLEU. Ir-riżultat l-aktar sorprendenti li nkisbu kien fis-setting zero shot għall-Olandiż-Ġermaniż u t-Taljan-Rumen fejn osservajna li minkejja l-użu ta’ ebda korpora parallela bejn dawn il-pari lingwistiċi, il-mudell NMT kien kapaċi jittraduċi bejn dawn il-lingwi u t-traduzzjonijiet kienu jew tajbin daqs jew aħjar (f’termini ta’ BLEU) mill-setting ta’ riżorsi mhux żero. Aħna jivverifikaw ukoll li l-mudelli NMT li jużaw saffi ta' alimentazzjoni 'l quddiem u l-attenzjoni awtonoma minflok saffi rikorrenti huma estremament mgħa ġġla f'termini ta' taħriġ li huwa utli f'ambjent sperimentali NMT.", 'pl': 'Opisujemy tutaj nasz model tłumaczenia maszynowego (MT) oraz wyniki uzyskane w ramach wielojęzycznego wspólnego zadania IWSLT 2017. Motywowani przez Zero Shot NMT [1] trenowaliśmy wielojęzyczne neuronowe tłumaczenie maszynowe poprzez łączenie wszystkich danych treningowych w jedną kolekcję poprzez dołączenie tokenów do zdań źródłowych w celu wskazania języka docelowego, na który powinny zostać przetłumaczone. Zauważyliśmy, że nawet w sytuacji niskiej zasobów udało nam się uzyskać tłumaczenia, których jakość przewyższa jakość tych uzyskanych przez tłumaczenie maszynowe fraze based statystyczne o kilka punktów BLEU. Najbardziej zaskakującym rezultatem, jaki uzyskaliśmy, było ustawienie zerowego ujęcia dla holendersko-niemieckich i włosko-rumuńskich, gdzie zauważyliśmy, że pomimo braku równoległych korpusów między tymi parami językowymi, model NMT był w stanie tłumaczyć między tymi językami, a tłumaczenia były równie dobre lub lepsze (pod względem BLEU) niż ustawienie zasobów niezerowych. Weryfikujemy również, że modele NMT, które wykorzystują warstwy paszowe i samoobserwację zamiast powtarzających się warstw, są niezwykle szybkie pod względem treningu, co jest przydatne w środowisku eksperymentalnym NMT.', 'ml': 'ഞങ്ങള്\u200d ഇവിടെ നമ്മുടെ മെഷീന്\u200d പരിഭാഷ (എംടി) മോഡലിനെയും വിശദീകരിക്കുന്നു. ഞങ്ങള്\u200dക്ക് കിട്ടിയ ഫലങ്ങളും ഐഡബ്ലിഎസ്എല്\u200dടി 2017 മൊ സീറോ ഷോട്ട് NMT [1] നിര്\u200dമ്മിക്കപ്പെട്ടിരിക്കുന്നു. എല്ലാ ട്രെയിനിങ്ങളുടെയും വിവരങ്ങളും ഒരൊറ്റ സംഘത്തിലേക്ക് കൂട്ടിച്ചേര്\u200dക്കുന്നതിനാല്\u200d നമ്മള്\u200d ഒരു മണ് കുറഞ്ഞ വിഭവങ്ങളുടെ സ്ഥിതിയില്\u200d പോലും ഞങ്ങള്\u200dക്ക് വിവരങ്ങള്\u200d ലഭിക്കാന്\u200d കഴിയുമെന്ന് ഞങ്ങള്\u200d കണ്ടിട്ടുണ്ട്. അതിന്റെ വാക്യാസ്സ് ബെസ്റ് ഞങ്ങള്\u200dക്ക് ലഭിച്ചതില്\u200d ഏറ്റവും ആശ്ചര്യപ്പെട്ട ഫലമെന്തെന്നാല്\u200d ഡച്ച്-ജര്\u200dമ്മനിയും ഇറ്റാലിയനും റോമാനിയയുമായി വെടിവെക്കുന്നത് പൂജ്യത്തിലായിരുന്നു. ഈ ഭാഷകള്\u200dക്കിടയില്\u200d പാരാലില്ലാ ആവര്\u200dത്തിക്കുന്ന തലകള്\u200dക്ക് പകരം ആവര്\u200dത്തിക്കാന്\u200d സ്വയം ശ്രദ്ധിക്കുന്ന NMT മോഡലുകള്\u200d ഉപയോഗിക്കുന്നത് നിര്\u200dണ്ണയിക്കുന്നതിന് പകര', 'ro': 'Descriem aici modelul nostru Machine Translation (MT) și rezultatele obținute pentru IWSLT 2017 Multilingval Shared Task. Motivați de Zero Shot NMT [1] am instruit o traducere automată neurală multilingvă prin combinarea tuturor datelor de instruire într-o singură colecție prin adăugarea jetoanelor la propozițiile sursă pentru a indica limba țintă în care ar trebui traduse. Am observat că, chiar și într-o situație cu resurse reduse, am reușit să obținem traduceri a căror calitate depășește calitatea celor obținute prin traducerea automată statistică bazată pe fraze cu mai multe puncte BLEU. Cel mai surprinzător rezultat pe care l-am obținut a fost setarea zero shot pentru olandeză-germană și italian-română unde am observat că, în ciuda faptului că nu folosim corpore paralele între aceste perechi de limbi, modelul NMT a fost capabil să traducă între aceste limbi și traducerile au fost fie la fel de bune ca sau mai bune (în termeni de BLEU) decât setarea non zero resurse. De asemenea, verificăm faptul că modelele NMT care utilizează straturi de alimentare înainte și auto-atenție în loc de straturi recurente sunt extrem de rapide în ceea ce privește antrenamentul care este util într-un cadru experimental NMT.', 'mn': 'Бид энд машины орчуулалт (MT) загварыг тайлбарлаж, 2017 оны IWSLT-ийн олон хэлний хуваалтын ажлын үр дүнг авсан. Zero Shot NMT-ын хөдөлгөөн [1] нь бид олон хэлний мэдрэлийн машин хөдөлгөөнийг нэг цуглуулга руу бүх сургалтын мэдээллийг нэг цуглуулга рүү нэгтгэхэд тэмдэглэл эх үүсвэрийн өгүүлбэрт нэмж, тэдний зориулагдсан хэл нь илэрхийлэх хэрэгтэй. Бид бага нөөцийн байдалд хүртэл бид хэдэн BLEU цэгээр авсан фразийн үндсэн статистикийн машины хөгжүүлэлтийн чанарыг илүү өндөр хэмжээний хэмжээний хэмжээнд орлуулж чадна гэдгийг анзаарсан. Бидний олж авсан хамгийн гайхалтай үр дүнг нь Дач-Герман, Итали-Румын хоёр хоорондын параллел корпора ашиглахгүй ч NMT загвар нь эдгээр хэл болон орчуулалтын хоорондын хувьд тэгш хэлбэрээс илүү сайн эсвэл илүү сайн байдаг гэдгийг анзаарсан. Мөн бид NMT-ийн загварууд NMT-ийн туршилтын тулд хэрэглэх боломжтой болон өөрийгөө анхаарлаа дахин дахин дахин анхаарлаа ашигладаг гэдгийг баталж байна.', 'no': 'Vi beskriver her maskineoversettelsmodellen vår (MT) og resultatene vi har fått for multispråk delt oppgåve i IWSLT 2017. Motivirert av Zero Shot NMT [1] vi trenga ein fleirspråk neuralmaskinsomsetjing ved å kombinerera alle opplæringsdata til ei enkelt samling ved å leggja teikna til kjeldesetningane for å indisera målspråket dei skal omsetjast til. Vi observerte at selv i ein liten ressurssituasjon vi kunne få omsetjingar som kvaliteten overpassar kvaliteten til dei som er henta av Frassebaserte statistiske maskinsomsetjing av fleire BLEU-punkt. Det mest overraskende resultatet vi har fått var i nullstillinga for nederlandsk-tysk og italiansk-Romansk der vi observerte at selv om ingen parallell korpora mellom disse språkparene, var NMT-modellen i stand til å oversette mellom disse språka og omsetningane enten så godt som eller bedre (i høve til BLEU) enn ikkje-nullstillinga. Vi kontrollerer også at NMT-modellen som brukar fargelag framover og selvmerking i staden for gjentakingslag er ekstremt rask i uttrykking som er nyttig i ein NMT-eksperimentell innstilling.', 'sv': 'Här beskriver vi vår maskinöversättning (MT) modell och de resultat vi fick för IWSLT 2017 Multilingual Shared Task. Motiverade av Zero Shot NMT [1] tränade vi en flerspråkig neural maskinöversättning genom att kombinera alla träningsdata till en enda samling genom att lägga till tokens i källmeningarna för att indikera vilket målspråk de ska översättas till. Vi observerade att även i en situation med låg resurs kunde vi få översättningar vars kvalitet överträffar kvaliteten på dem som erhållits genom frasbaserad statistisk maskinöversättning med flera BLEU-poäng. Det mest överraskande resultatet vi fick var nollskottsinställningen för holländsk-tyska och italiensk-rumänska där vi observerade att NMT-modellen, trots att de inte använde några parallella korpora mellan dessa språkpar, kunde översätta mellan dessa språk och översättningarna var antingen lika bra som eller bättre (i termer av BLEU) än nollresursinställningen. Vi kontrollerar också att NMT-modellerna som använder foder framåt lager och självuppmärksamhet istället för återkommande lager är extremt snabba när det gäller träning vilket är användbart i en NMT experimentell miljö.', 'si': 'අපි මෙහෙ අපේ මැෂින් පරිවර්තන (MT) මොඩේල් විස්තර කරනවා ඒ වගේම අපි ලබාගත්ත ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරත Zero Shot NMT [1] නිර්මාණය කරලා, අපි ගොඩක් භාෂාවක් නිර්මාණය මෂ්\u200dයාන්ත්\u200dරය භාෂාවක් ප්\u200dරශ්නය කරලා සියලුම ප්\u200dරශ්නයක් දත්ත එක්ක සංග්නයකට එක්ක අපි බලාපොරොත්තු වුනා කියලා අපිට පුළුවන් විවාහයක් ලැබුනා කියලා කියලා ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් ලැබුනා කියලා ප්\u200dරශ්නයක අපිට පුළුවන් විශ්වාසිකම ප්\u200dරතිචාරයක් තියෙන්නේ ඩච්ච්-ජර්මාන් සහ ඉතාලියාන්-රෝමානියාන් වෙනුවෙන් ශෝට් සැකසුම් වෙනුවෙන් අපි බලන්නේ ඒ වගේ භාෂා ජාත අපි පරීක්ෂා කරනවා NMT මෝඩේල් එක්ක කියලා ප්\u200dරයෝජනය කරනවා කියලා සහ ආපහු ප්\u200dරයෝජනය සඳහා ආපහු අවධානය ප්\u200dරයෝජනය කරනවා කිය', 'so': 'Halkan waxaynu ku sawiraynaa Tusaale turjumista mashiinka (MT) iyo resultiyada aan u helnay shaqo luuqado badan oo IWSLT 2017. Waxqabadka Zero Shot NMT [1] waxaan ku tababarinnay tarjumaadda luuqadaha kala duduwan, kaasoo aan ku soo ururinnay macluumaadka waxbarashada oo dhan oo hal urur ah, waxaana ku daray calaamado ku qoran fursad sourceed si aan u muujinno luqada waxqabadka lagu turjumo. Waxaynu aragnay in xataa xaalad hoos u ah, aan heli karnay turjubaan, kaas oo ay qiimaheedu ka kor mari karto qiimaha kaalmeynta rasmiga ee lagu soo qoray tarjumaadka Statistical Machine ee BLEU. Midhaha ugu yaab badan ee aan helnay waxaa lagu qoray qoraalka jarmal-Jarmal iyo Talyaani-Romanian, halkaas oo aannu aragnay in kastoo aan isticmaalin korpora parallel oo u dhexeeya labadan luqada ah, modelka NMT ayaa awooday inuu turjumo between afkan iyo turjumaadyadaasu waxay ahaan lahaayeen wax wanaagsan ama ka wanaagsan (in terms of BLEU) than aan zero resource set. Sidoo kale waxaynu xaqiijinnaa in modellada NMT oo isticmaalaya inay cunaan qasnadaha hore iyo isfiirsashada, taasoo aan laga helin qasnadaha soo socda aad u dhaqdhaqaaqi, taas oo faa’iido u leh qabashada imtixaanka NMT.', 'ta': 'We describe here our Machine Translation (MT) model and the results we obtained for the IWSLT 2017 Multilingual Shared Task.  ஜீரோ ஷாட் NMT [1] நாம் பல மொழி நெருக்கல் இயந்திரம் மொழிபெயர்ப்பை பயிற்சி செய்தோம் அனைத்து பயிற்சிகளையும் ஒரே தொகுப்பில் சேர்த்து மூல வாக்கியங்களுக்கு கு நாங்கள் பார்த்தோம் கூட ஒரு குறைந்த மூலநிலையிலும் நாங்கள் மொழிபெயர்ப்புகளை பெற முடியவில்லை என்பதை நாம் பார்த்தோம். அதன் தரம் சொற்ற மிகவும் ஆச்சரியமான முடிவு நாம் கிடைத்தது என்பது டெச்- ஜெர்மன் மற்றும் இத்தாலியன்- ரோமானியன் அமைப்பில் இருந்தது என்பது தான் நாம் பார்த்தோம். இந்த மொழி ஜோடி இடையே இணையான நிறுவனத்தை பயன்படுத நாம் தெரிந்து கொள்கிறோம் நிகழ்வு அடுக்குப் பதிலாக NMT மாதிரிகள் முன் அடுக்குகள் மற்றும் தன்னை கவனத்தை பயன்படுத்துகின்றன என்பதை நிச', 'ur': 'ہم یہاں ہمارے ماشین ترجمہ (MT) موڈل اور نتیجے کو بیان کرتے ہیں جو ہم نے IWSLT 2017 Multilingual Shared Task کے لئے حاصل کیا ہے. Zero Shot NMT کے ذریعہ حرکت کی جاتی ہے [1] ہم نے ایک Multilingual Neural Machine Translation کی تعلیم دی ہے، اس طرح تمام تربین ڈاٹوں کو ایک ہی مجموعہ میں جمع کرتا ہے، تاکہ وہ موجود زبان کی تعلیم کریں جو ان کو تربیت کی جاتی ہے. ہم نے دیکھا تھا کہ اگرچہ ایک کم منطقی موقعیت میں بھی ہم تعلیم حاصل کرسکتے تھے جن کی کیفیت ان لوگوں کی کیفیت سے زیادہ زیادہ گزر سکتی تھی جو فریس Based Statistical Machine Translation کے ذریعے بہت سی BLEU پوینٹوں سے حاصل کئے گئے۔ ہم نے سب سے زیادہ عجیب نتیجہ حاصل کیا تھا کہ ڈچ-جرمن اور ایتالیایی-رومین کے لئے صفر شٹ کا سامان تھا جہاں ہم نے دیکھا تھا کہ ان زبان جوڑوں کے درمیان کوئی مشابہ کوپرا نہیں استعمال کرنا چاہے ان زبانوں کے درمیان NMT موڈل کو ان زبانوں کے درمیان ترجمہ کر سکتا تھا اور ترجمہ کیا گیا تھا یا صفر کے سامان سے ہم بھی تصدیق کرتے ہیں کہ NMT موڈل جو فائدہ آگے لہروں اور دوبارہ لہروں کے بدلے اپنی توجه استعمال کرتے ہیں ان کی تعلیم کے معاملہ میں بہت تیز ہیں جو NMT آزمائش کے معاملہ میں فائدہ ہے.', 'sr': 'Ovde opisujemo naš model za prevod mašine (MT) i rezultate koje smo dobili za višejezički zajednički zadatak IWSLT 2017. Motivirali smo Zero Shot NMT [1] vježbali smo multijezičku Neuralnu mašinu prevodom kombinacijom svih podataka obuke u jednu jedinu kolekciju dodajući znakove na izvornu rečenicu kako bi pokazali ciljni jezik na koji bi trebali biti prevedeni. Primijetili smo da smo čak i u manjoj situaciji resursa uspeli da dobijemo prevode čije kvalitete nadmašuju kvalitetu onih koji su dobili prevod statističkih mašin a na Frazu baziranih od nekoliko BLEU bodova. Najiznenađujući rezultat koji smo dobili je bio u nulom stanju snimanja za holandski-nemački i italijanski-rumunski, gde smo primetili da, uprkos korištenju paralelne korpore između tih jezičkih pare, model NMT je bio u mogućnosti da prevodi između tih jezika i prevoda bilo je dobar ili bolji (u smislu BLEU) nego nula resursa. Takođe potvrđujemo da su modeli NMT koji koriste korištenje korištenih slojeva i samopouzdanja umjesto rekonstruiranih slojeva izuzetno brzi u smislu obuke koja je korisna u eksperimentalnom postavljanju NMT-a.', 'uz': "Biz bu yerda Mashining tarjima modeli va IWSLT 2017 bir necha tillar bilan birlashtirilgan vazifaning natijalarini anglatamiz. @ info: whatsthis Biz shunday ko'rib turganimizni ko'p bir necha BLEU nuqta bilan imkoniyatlarning sifatida o'zgartirish imkoniyatini o'rganish mumkin. The most surprising result we obtained was in the zero shot setting for Dutch-German and Italian-Romanian where we observed that despite using no parallel corpora between these language pairs, the NMT model was able to translate between these languages and the translations were either as good as or better (in terms of BLEU) than the non zero resource setting.  Biz shunday tasdiqlash mumkin, qaytadan qatlamni oʻzgartirib, NMT modellarini oldingi qatlam va o'zimni ishlatishni tasdiqlash mumkin, bu NMT tajriba moslamada foydalanishi mumkin.", 'vi': 'Chúng tôi mô tả ở đây mô hình Dịch cỗ máy (MTV) và kết quả chúng tôi có được cho công việc chia sẻ đa ngôn ngữ IWSLT 2007. Động cơ bởi Zero Shot NMT [1] Chúng tôi đã đào tạo một phiên dịch đa ngôn ngữ thần kinh bằng cách kết hợp tất cả dữ liệu huấn luyện thành một bộ sưu tập bằng cách thêm vật trưng bày vào các ký hiệu từ đầu để chỉ ra ngôn ngữ mà chúng nên dịch sang. Chúng tôi quan sát rằng ngay cả trong một tình huống tài nguyên thấp chúng tôi đã có thể nhận được một bản dịch có chất cao hơn chất lượng của bản dịch Dịch Dịch Dịch bởi Cơ Quan âm học kiểu Xuất xứ. Kết quả đáng ngạc nhiên nhất mà chúng ta có được là tại trường quay không cho Dutch-German và Italian-Rumani, nơi chúng ta nhận thấy rằng mặc dù không sử dụng vật thể song song song nào giữa hai cặp ngôn ngữ này, mẫu NMT đã có thể dịch giữa hai ngôn ngữ này và bản dịch đã tốt hoặc tốt hơn (theo nghĩa tiếng bíp) so với thiết lập tài nguyên không. Chúng tôi cũng xác nhận rằng các mẫu NMT sử dụng thức ăn phía trước và sự chăm sóc bản thân thay vì lớp thường xuyên trở nên cực kỳ nhanh trong việc huấn luyện có ích trong một môi trường thử nghiệm NMT.', 'bg': 'Тук описваме модела ни за машинен превод (МТ) и резултатите, които получихме за многоезичната споделена задача. Мотивирани от Нулев изстрел НМТ [1], ние обучихме многоезичен неврален машинен превод чрез комбиниране на всички данни за обучение в една колекция, като добавихме символите към изходните изречения, за да посочим целевия език, на който трябва да бъдат преведени. Наблюдавахме, че дори и в ситуация с нисък ресурс успяхме да получим преводи, чието качество надхвърля качеството на получените чрез фразован статистически машинен превод с няколко точки. Най-изненадващият резултат, който получихме, беше в настройката за нулев изстрел за холандско-немски и италианско-румънски език, където забелязахме, че въпреки че не използвахме паралелни корпуси между тези езикови двойки, моделът успя да превежда между тези езици и преводите бяха толкова добри, колкото и по-добри (по отношение на Блеу) от настройката на ресурс, която не беше нулева. Също така проверяваме, че моделите на НМТ, които използват подхранващи слоеве и самовнимание вместо повтарящи се слоеве, са изключително бързи по отношение на обучението, което е полезно в експериментална обстановка на НМТ.', 'hr': 'Ovdje opisujemo naš model za prevod strojeva (MT) i rezultate koje smo dobili za višejezički zajednički zadatak IWSLT 2017. Motivirali smo Zero Shot NMT [1] vježbali smo multijezički neuronski prevod strojeva kombinirajući sve podatke obuke u jednu jedinu kolekciju dodajući znakove na izvornu rečenicu kako bi pokazali ciljni jezik na kojem bi trebali biti prevedeni. Primijetili smo da smo čak i u manjoj situaciji resursa uspjeli dobiti prevode čije kvalitete nadmašuju kvalitetu onih koji su dobili prevod statističkih strojeva na temelju Fraze u nekoliko BLEU bodova. Najiznenađujući rezultat koji smo dobili je bio u nulom stanju pucnjave za holandski-njemački i italijanski-rumunski, gdje smo primijetili da, uprkos korištenju paralelnog korporacije između tih jezičkih parova, model NMT-a je bio u stanju prevoditi između tih jezika i prevoda bilo je dobar ili bolji (u smislu BLEU) nego u stanju ne nule resursa. Također potvrđujemo da su modeli NMT-a koji koriste slojeve hrane naprijed i samopouzdanje umjesto rekonstruiranih slojeva izuzetno brzi u smislu obuke koja je korisna u eksperimentalnom postavljanju NMT-a.', 'da': 'Vi beskriver her vores maskinoversættelsesmodel (MT) og de resultater, vi opnåede for IWSLT 2017 Multilingual Shared Task. Motiveret af Zero Shot NMT [1] trænede vi en Multilingual Neural Machine Translation ved at kombinere alle træningsdata i én enkelt samling ved at tilføje tokens til kildesætningerne for at angive det målsprog, de skal oversættes til. Vi bemærkede, at selv i en situation med lave ressourcer kunne vi få oversættelser, hvis kvalitet overstiger kvaliteten af dem, der opnås ved Phrase Based Statistical Machine Translation med flere BLEU-punkter. Det mest overraskende resultat vi opnåede var i nulskudsindstillingen for hollandsk-tysk og italiensk-rumænsk, hvor vi observerede, at selvom vi ikke brugte parallelle korpora mellem disse sprogpar, var NMT modellen i stand til at oversætte mellem disse sprog, og oversættelserne var enten lige så gode som eller bedre (i form af BLEU) end ikke nul ressource indstillingen. Vi kontrollerer også, at de NMT modeller, der bruger feed forward lag og selvopmærksomhed i stedet for tilbagevendende lag, er ekstremt hurtige i form af træning, hvilket er nyttigt i en NMT eksperimentel miljø.', 'nl': 'We beschrijven hier ons Machine Translation (MT) model en de resultaten die we hebben verkregen voor de IWSLT 2017 Multilingual Shared Task. Gemotiveerd door Zero Shot NMT [1] hebben we een meertalige neurale machinevertaling getraind door alle trainingsgegevens te combineren in één verzameling door de tokens aan de bronzinnen toe te voegen om de doeltaal aan te geven waarnaar ze moeten worden vertaald. We constateerden dat zelfs in een lage resource situatie we in staat waren vertalingen te krijgen waarvan de kwaliteit overtreft de kwaliteit van die verkregen door Phrase Based Statistical Machine Translation met verschillende BLEU punten. Het meest verrassende resultaat dat we kregen was de nul shot instelling voor Nederlands-Duits en Italiaans-Roemeens waar we zagen dat ondanks het gebruik van geen parallelle corpora tussen deze taalparen, het NMT model in staat was om tussen deze talen te vertalen en de vertalingen waren ofwel even goed als of beter (in termen van BLEU) dan de non-nul resource setting. We verifiëren ook dat de NMT modellen die gebruik maken van feed forward lagen en zelfaandacht in plaats van terugkerende lagen extreem snel zijn in termen van training wat nuttig is in een NMT experimentele setting.', 'id': 'Kami menggambarkan di sini model Translation Mesin (MT) kami dan hasil yang kami dapatkan untuk Tugas Berberbagi Multibahasa IWSLT 2017. Motivated by Zero Shot NMT [1] we trained a Multilingual Neural Machine Translation by combining all the training data into one single collection by appending the tokens to the source sentences in order to indicate the target language they should be translated to.  Kami memperhatikan bahwa bahkan dalam situasi sumber daya rendah kami mampu mendapatkan terjemahan yang kualitas melebihi kualitas yang diperoleh oleh Perjemahan Mesin Statistik Berdasarkan Frasa dengan beberapa titik BLEU. Hasil yang paling mengejutkan yang kami dapatkan adalah dalam setting zero shot untuk Belanda-Jerman dan Italia-Romani di mana kami memperhatikan bahwa meskipun tidak menggunakan corpora paralel antara pasangan bahasa ini, model NMT mampu menerjemahkan antara bahasa ini dan terjemahan baik sebagai atau lebih baik (dalam termasuk BLEU) daripada setting sumber bukan nol. Kami juga memverifikasi bahwa model NMT yang menggunakan lapisan feed maju dan perhatian diri selain lapisan recurrent sangat cepat dalam terma latihan yang berguna dalam pengaturan eksperimental NMT.', 'de': 'Wir beschreiben hier unser Machine Translation (MT) Modell und die Ergebnisse, die wir für die IWSLT 2017 Multilingual Shared Task erhalten haben. Motiviert von Zero Shot NMT [1] trainierten wir eine mehrsprachige neuronale maschinelle Übersetzung, indem wir alle Trainingsdaten in einer einzigen Sammlung kombinieren, indem wir die Token an die Ausgangssätze anhängen, um die Zielsprache anzugeben, in die sie übersetzt werden sollen. Wir haben festgestellt, dass wir selbst in einer Situation mit geringen Ressourcen Übersetzungen erhalten konnten, deren Qualität die Qualität der Phrase Based Statistical Machine Translation um mehrere BLEU-Punkte übertrifft. Das überraschendste Ergebnis, das wir erzielten, war die Null-Aufnahme-Einstellung für Niederländisch-Deutsch und Italienisch-Rumänisch, wo wir beobachteten, dass das NMT-Modell trotz der Verwendung von parallelen Korpora zwischen diesen Sprachpaaren in der Lage war, zwischen diesen Sprachen zu übersetzen und die Übersetzungen waren entweder so gut wie oder besser (in Bezug auf BLEU) als die Nicht-Null-Ressourceneinstellung. Wir verifizieren auch, dass die NMT-Modelle, die Feed Forward Layers und Self Attention anstelle wiederkehrender Schichten verwenden, extrem schnell sind, was in einem NMT-Experiment nützlich ist.', 'fa': 'ما در اینجا مدل ترجمه ماشین ما (MT) را توصیف می\u200cکنیم و نتیجه\u200cهایی که برای کار مشترک زیادی زبان\u200cهای IWSLT ۲۰۰۷ به دست آوردیم. توسط Zero Shot NMT [1] حرکت کرده ایم، ما یک ترجمه ماشین عصبی چندین زبان آموزش دادیم با جمع کردن تمام داده های آموزش به یک مجموعه با جمع کردن نشانه ها به جمله منبع برای نشان دادن زبانی هدف که باید به آن ترجمه شوند. ما مشاهده کردیم که حتی در موقعیت منابع کم توانستیم ترجمه\u200cهای کیفیت آنها از کیفیت کسانی که توسط ترجمه ماشین آماری بر اساس فراز گرفته\u200cاند، با چند نقطه BLEU بیشتر از کیفیت آنها را فراتر کنیم. آخرین نتیجه تعجب\u200cکننده\u200cای که ما به دست آوردیم، در صفر تغییر تصاویر برای هلندی-آلمان و ایتالیایی-رومانی بود که ما مشاهده کردیم که با وجود استفاده از هیچ شرکت parallel بین این جفت زبان\u200cها، مدل NMT قادر بود بین این زبان\u200cها و ترجمه\u200cها یا به اندازه\u200cی صفر یا بهتر (به عنوان BLEU) بودند. ما همچنین تصدیق می\u200cکنیم که مدل NMT که از طبقه\u200cهای پیشینی و توجه خود استفاده می\u200cکنند به جای طبقه\u200cهای تکرار به طور کامل سریع است که در یک تنظیم آزمایشی NMT مفید است.', 'sw': 'Tunaelezea hapa mfano wetu wa Tafsiri ya Mashine (MT) na matokeo tuliyopata kwa ajili ya kazi ya lugha nyingine ya IWSLT 2017. Tulimetengenezwa na risasi ya NMT[1] tulifundisha Tafsiri ya Mashine ya Kilugha ya Neurali kwa kuunganisha taarifa zote za mafunzo katika mkusanyiko mmoja kwa kuongeza alama kwenye sentensi za vyanzo ili kuonyesha lugha ya lengo ambazo zinapaswa kutafsiriwa. Tuliona kwamba hata katika hali ya rasilimali ya chini tuliweza kupata tafsiri ambazo kiwango chake kinaweza kuona kiwango cha watu waliopatikana na Tafsiri ya Kitakwimu cha Habari cha msingi na baadhi ya pointi za BLEU. Matokeo ya kushangaza zaidi tuliyopata ilikuwa katika kitengo cha risasi sifuri kwa Kijerumani na Kiitalia ambapo tuliona kwamba pamoja na kutumia kampuni isiyofanana kati ya wanandoa wa lugha hizi, mtindo wa NMT uliweza kutafsiri kati ya lugha hizi na tafsiri zilikuwa vizuri kama au vizuri (kwa kiasi cha BLEU) kuliko kituo cha rasilimali sifuri. Pia tunathibitisha kuwa mifano ya NMT yanayotumia vipande vya mbele na kujitazama badala ya vipande vya kurejea ni haraka sana kwa sababu ya mafunzo yanayofaa katika mazingira ya NMT.', 'tr': 'Biz bu ýerde maşynyň terjimelerimizi (MT) nusgasyny we 2017-nji ýyldan IWSLT üçin alan netijelerimizi tassyýarys. Zero Shot NMT tarapyndan süýtgedilen [1] we köp dilli Neural Maşynyň terjimesini birleştirerek ähli bilim maglumatyny bir ýygnaga ekleýän çeşme sözlerine sembol ekleýän we olaryň terjime edilmelidigini belli etmek üçin bilim sistemasyna terjime etdik. Biz garaşdyrdyk ki, hatda iň az kaynakça ýagdaýda hem biz Frases Based Statistik Maşynyň terjimelerinden birnäçe BLEU noktalary tarapyndan alan adamlaryň kalitesinden üstünlik gazanabilerdik. Biziň alyp bilýän iň geň şaşırtýan netijelerimiz Höländçe-nemes we italiýa-rumança üçin 0 aty düzümlerinde bolupdyr. Bu diller arasynda parallel korpora ýok bolmasa, NMT modeli bu diller we terjimeler arasynda ýa gowy ýa-da gowy terjime edip bilýärdi (BLEU-a görä) 0 resurslar däldir. Biz hem NMT nusgalarynyň öňki katlaryny we özüne üns berjek nusgalarynyň NMT deneysel düzeninde ullanyşynda gaty çalt bolandygyny barlaýarys.', 'af': "Ons beskrywe hier ons Masjien Vertaling (MT) model en die resultate wat ons ontvang het vir die IWSLT 2017 Multilingual Gedeelde Taak. Gemotiveerde deur Zero Shot NMT [1] het ons 'n veelvuldige neurale masjien vertaling opgelei deur al die onderwerking data in een enkele versameling te kombinerer deur die tekens aan die bron setings byvoeg om die doel taal na te wys wat hulle moet vertaling word. Ons het opgemerk dat selfs in 'n lae hulpbron situasie ons kon ontvang van vertalings wie se kwaliteit die kwaliteit oor die kwaliteit van die wat deur Frase Based Statistiese Masjien Vertaling verkry het deur verskeie BLEU punte. Die mees verwonderende resultaat wat ons ontvang het was in die nul skoot instelling vir Nederland-Duits en Italiese-Rumänees waar ons aanhou dat nadat ons geen parallele korpora tussen hierdie taal paar gebruik het nie, was die NMT model in staat om tussen hierdie taal en die vertalings te vertaal was of so goed as of beter (in terms van BLEU) as die non nul hulpbron instelling. Ons bevestig ook dat die NMT-modele wat voer vorentoe laagte en self aandag gebruik in plaas van herhaalde laagte is ekstrem vinnig in terms van onderwerp wat gebruik is in 'n NMT-eksperimentaal instelling.", 'am': 'We describe here our Machine Translation (MT) model and the results we obtained for the IWSLT 2017 Multilingual Shared Task.  በZero Shot NMT [1] የተንቀሳቀሰው የቋንቋ ቋንቋዎች የኔural Machine ትርጉም በማስተካከል የተጠቃሚ ዳራዎችን ሁሉ በአንድ ሰብስብ በመጠቀም እና ምልክቶችን ወደ ምንጭ ቃላት በመጠቀም ለማስታወቂያው ቋንቋ እንዲትረጉም እናስተማርነው። በጥቂት የክፍለ ሀብት ጉዳይ እንኳ ምርርጓሜዎችን ለማግኘት እንችላለን፡፡ በጣም የበረታው ፍጻሜ በኩል፣ ጀርመን እና ጣሊያንኛ-ሮማኒያን በቁጥጥር፣ በዚህ ቋንቋዎች መካከል ተሳያየት ምንም እንኳ ምንም እንኳን በይፋ ቢሆን የNMT ሞዴል በዚህ ቋንቋዎች እና ትርጓሜዎች መካከል መተርጓሜ ይችላል፡፡ እንደዚህም እናረጋግጣለን፤ የNMT ተፈተና ማግኘት የሚጠቅመው አካባቢ እና ራሳቸውን በመስጠት የሚጠቅሙ የNMT ዓይነቶች በቁጥጥር ላይ የሚጠቅሙ የፍላጎት ደረጃዎች እና የራሳቸውን ማስታወቂያ እንደሚጠቅሙ ነው፡፡', 'hy': "Այստեղ մենք նկարագրում ենք մեր մեքենայի թարգմանման (MT) մոդելը և արդյունքները, որոնք մենք ստացանք 2017 թվականի IW-ի բազլեզու ընդհանուր առաջադրանքի համար: Ներոյի ՆՄԹ-ի միջոցով [1] մենք սովորեցրեցինք բազլեզու նյարդային մեքենայի թարգմանություն' միավորելով բոլոր ուսումնասիրության տվյալները մեկ հավաքածուի մեջ, ավելացնելով նշանները աղբյուր նախադասությունների մեջ, որպեսզի նշենք նպատակային լեզուն, որի վրա Մենք նկատեցինք, որ նույնիսկ ցածր ռեսուրսների իրավիճակում մենք կարողացանք ստանալ թարգմանություններ, որոնց որակը գերազանցում է արտահայտության հիմնված վիճակագրական մեքենայի թարգմանությունների որակը մի քանի կետով: Ամենազարմանալի արդյունքը, որ մենք ստացանք, այն էր, որ հոլանդացի-գերմանացի և իտալանդի-ռոմանացի դեպքում զրոյական պատկերացումն էր, որտեղ մենք նկատեցինք, որ չնայած այս լեզվի զույգերի միջև զուգահեռ կոպորա չօգտագործելու, NMT-ի մոդելը կարողացավ թարգմանել այս լեզուների միջև և և թարգ We also verify that the NMT models that use feed forward layers and self attention instead of recurrent layers are extremely fast in terms of training which is useful in a NMT experimental setting.", 'sq': 'Ne përshkruajmë këtu modelin tonë të përkthimit të makinave (MT) dhe rezultatet që kemi fituar për IWSLT 2017 Multilingual Shared Task. Motivuar nga Zero Shot NMT [1] ne trajnuam një Translation Multilingual Neural Machine duke kombinuar të gjitha të dhënat e trajnimit në një koleksion të vetëm duke shtuar tokenat në fjalët e burimit me qëllim që të tregojnë gjuhën objektiv në të cilën duhet të përkthyen. We observed that even in a low resource situation we were able to get translations whose quality surpass the quality of those obtained by Phrase Based Statistical Machine Translation by several BLEU points.  Rezultati më i befasueshëm që arritëm ishte në vendosjen zero për hollandez-gjerman dhe italian-rumun ku vëzhguam se pavarësisht nga përdorimi i asnjë korpore paralele midis këtyre çifteve gjuhësh, modeli NMT ishte në gjendje të përkthyej midis këtyre gjuhëve dhe përkthimet ishin ose aq të mira ose më të mirë (në terma të BLEU) sesa vendosja jo zero burimesh. Ne gjithashtu verifikojmë se modelet NMT që përdorin shtresa përpara dhe vetë vëmendje në vend të shtresave të përsëritura janë jashtëzakonisht të shpejtë në lidhje me trajnimin që është i dobishëm në një ambient eksperimental NMT.', 'az': 'Burada mašin Çeviri (MT) modelini və 2017 IWSLT-nin çoxlu dil paylaşılmış işlərimizin sonuçlarını təsbit edirik. Zero Shot NMT tarafından hərəkət edildi [1] Biz çoxlu dil NMT tərcümünü təhsil etdik, bütün təhsil məlumatlarını tək koleksiya birləşdirdik ki, məqsədilə tərcümə ediləcəkləri dili göstərmək üçün məlumatları mənbəzi cümlələrə əlavə etdik. Biz baxırdıq ki, hətta düşük ressurs durumu içində, bəzi BLEU nöqtələrindən, Fraz Based Statistik Makinelərin Tercüməsindən qazananların keyfiyyətini üstün edə bilərdik. Ən təəccüblü sonuçlarımız Hollandi-Alman və İtalyan-Rumun üçün sıfır atış qurmasında idi ki, bu dil çiftləri arasında paralel korpora istifadə etməyə rağmen, NMT modeli bu dillərin arasında və tercümələrin ya da sıfır kaynaq qurmasından daha yaxşıdır, ya da daha yaxşıdır. Biz həmçin in NMT modellərinin NMT eksperimenti təyin etməsində faydalanır.', 'bs': 'Ovdje opisujemo naš model za prevod mašine (MT) i rezultate koje smo dobili za višejezički zajednički zadatak IWSLT 2017. Motivirali smo Zero Shot NMT [1] vježbali smo multijezički neuronski prevod strojeva kombinirajući sve podatke obuke u jednu jedinu kolekciju dodajući znakove na izvorne rečenice kako bi pokazali ciljni jezik na kojem bi trebali biti prevedeni. Primijetili smo da smo čak i u situaciji niskog resursa uspjeli dobiti prevode čije kvalitete nadmašuju kvalitetu onih koji su dobili prevod statističkih mašin a na Frazi u nekoliko BLEU bodova. Najiznenađujući rezultat koji smo dobili je bio u nulom stanju pucnjave za holandski-njemački i italijanski-rumunski, gdje smo primijetili da, uprkos korporaciji paralelne korporacije između tih jezičkih parova, model NMT je bio u stanju prevoditi između tih jezika i prevoda bilo je dobar ili bolji (u smislu BLEU) nego što je postavljanje ne nula resursa. Također potvrđujemo da su modeli NMT koji koriste slojeve hrane i samopouzdanje umjesto rekonstruiranih slojeva izuzetno brzi u smislu obuke koje je korisno u eksperimentalnom postavljanju NMT-a.', 'bn': 'আমরা এখানে আমাদের মেশিন অনুবাদ (এমটি) মডেল এবং আইউএসএলটি ২০১৭ সালের মাল্টিভাষাভাষী ভাষায় শেয়ার কর্মসূচীর জন্য আমরা পেয়েছ জিরো শুট এনএমটি [১] আমরা একটি বহুভাষী নিউরাল মেশিন অনুবাদ প্রশিক্ষণ দিয়েছি একই সংগ্রহে সকল প্রশিক্ষণের তথ্য সংগ্রহের মাধ্যমে একটি সংগ্রহের মাধ্যমে সোর্সের বাক্যে প্ We observed that even in a low resource situation we were able to get translations whose quality surpass the quality of those obtained by Phrase Based Statistical Machine Translation by several BLEU points.  আমাদের সবচেয়ে বিস্ময়কর ফলাফল হল ডাচ-জার্মানী এবং ইতালিয়ান-রোমানীয় গুলি নির্ধারণের মাধ্যমে যেখানে আমরা দেখেছি যে এই ভাষার জোড়ার মধ্যে কোন পারালেল কর্পোরা ব্যবহার করেনি সত্ত্বেও এনএমটি মড আমরা একই সাথে নিশ্চিত করি যে এনএমটি মডেল যারা প্রত্যাবর্তনের পরিশেষ পরীক্ষায় এনএমটি পরীক্ষার পরীক্ষায় ব্যবহার করে প্রশিক্ষণের পরিবর্তে প', 'ko': '우리는 여기서 우리의 기계번역(MT) 모델과 우리가 IWSLT 2017 다중 언어 공유 임무에서 얻은 결과를 묘사했다.제로포 NMT[1]의 계발을 받아 우리는 모든 훈련 데이터를 하나의 집합으로 합쳐서 표기를 원시문에 추가하여 그들이 번역해야 할 목표 언어를 지시함으로써 다언어 신경기계 번역을 훈련시켰다.우리는 자원이 비교적 낮은 상황에서도 짧은 말에 기반한 통계기계 번역의 질이 여러 BLEU점보다 높은 번역을 얻을 수 있다는 것을 관찰했다.우리가 얻은 가장 놀라운 결과는 네덜란드-독일과 이탈리아-루마니아어의 제로 렌즈 설정에서 이 언어들이 평행어 자료 라이브러리를 사용하지 않았음에도 불구하고 NMT모델은 이 언어들 사이에서 번역할 수 있고 번역 효과가 제로 자원 설정과 같거나 더 좋다는 것을 관찰했다(BLEU의 경우).우리는 또한 중복층이 아닌 피드백층과 자기주의를 사용하는 NMT 모델이 훈련에 매우 빠르다는 것을 검증했는데 이것은 NMT 실험 환경에서 매우 유용하다.', 'ca': "Aquí descrivim el nostre model de traducció màquina (MT) i els resultats que vam obtenir per la IWSLT 2017 Multilingual Shared Task. Motivada per Zero Shot NMT [1] vam entrenar una traducció multilingüe de màquines neuronales combinant totes les dades d'entrenament en una única col·lecció afegint les fitxes a les frases d'origen per indicar la llengua a la que haurien de ser traduïdes. Vam observar que fins i tot en una situació de baix recursos vam poder obtenir traduccions cuja qualitat supera la qualitat dels obtinguts per la traducció de màquines estadístiques basades en frases en diversos punts BLEU. El resultat més sorprenent que vam aconseguir va ser en la configuració de fotografies zero per holandès-alemanès i italià-rumun on vam observar que malgrat no utilitzar cap corpora parallela entre aquests parells de llengües, el model NMT va poder traduir entre aquestes llengües i que les traduccions eren tant bones com o millors (en termes de BLEU) que la configuració de recursos no zero. També verificem que els models NMT que utilitzen capes avançades de alimentació i autoatenció en lloc de capes recurrents són extremadament ràpids en termes d'entrenament que és útil en un entorn experimental NMT.", 'cs': 'Popisujeme zde náš model strojového překladu (MT) a výsledky, které jsme získali pro IWSLT 2017 Multilingual Shared Task. Motivováni Zero Shot NMT [1] jsme trénovali vícejazyčný neuronový strojový překlad kombinací všech tréninkových dat do jedné sbírky připojením tokenů ke zdrojovým větám, abychom označili cílový jazyk, do kterého mají být přeloženy. Zjistili jsme, že i v situaci s nízkými zdroji jsme byli schopni získat překlady, jejichž kvalita převyšuje kvalitu těch získaných frázovým statistickým strojovým překladem o několik bodů BLEU. Nejpřekvapivějším výsledkem, který jsme dosáhli, bylo nastavení nulového snímku pro nizozemsko-německé a italsko-rumunské, kde jsme pozorovali, že navzdory použití paralelních korpusů mezi těmito jazykovými páry, NMT model byl schopen překládat mezi těmito jazyky a překlady byly buď stejně dobré nebo lepší (z hlediska BLEU) než nulové nastavení zdrojů. Dále ověřujeme, že NMT modely, které používají posuvné vrstvy a vlastní pozornost namísto recidivujících vrstev, jsou extrémně rychlé z hlediska tréninku, což je užitečné v experimentálním prostředí NMT.', 'et': 'Kirjeldame siin oma masintõlke mudelit ja IWSLT 2017 mitmekeelse jagatud ülesande tulemusi. Motivatsioonil Zero Shot NMT [1] koolitasime mitmekeelset neuraalset masintõlket, kombineerides kõik treeninguandmed ühte kogumisse, lisades märgid lähtelausetele, et näidata sihtkeelt, kuhu need tuleks tõlkida. Märgisime, et isegi madala ressursi olukorras suutsime saada tõlkeid, mille kvaliteet ületab fraasipõhise statistilise masintõlke kvaliteeti mitme BLEU punkti võrra. Kõige üllatavam tulemus, mida me saavutasime, oli null shot seadistus hollandi-saksa ja itaalia-rumeenia, kus me täheldasime, et vaatamata nende keelepaaride vahel paralleelseid korpuseid ei kasutatud, NMT mudel suutis tõlkida nende keelte vahel ja tõlked olid kas sama hea või parem (mõttes BLEU) kui mitte null ressursi seadistus. Samuti kontrollime, et NMT mudelid, mis kasutavad korduvate kihtide asemel söödavaid kihte ja enesetähelepanu, on äärmiselt kiired koolituse osas, mis on kasulik NMT eksperimentaalsetes tingimustes.', 'fi': 'Kuvaamme tässä konekäännösmalliamme ja IWSLT 2017 Multilingual Shared Task -projektin tuloksia. Zero Shot NMT:n [1] innoittamana koulutimme monikielisen neurokonekäännöksen yhdistämällä kaikki harjoitustiedot yhteen kokoelmaan liittämällä poletit lähdelauseisiin osoittaaksemme kohdekielen, jolle ne tulisi kääntää. Havaitsimme, että jopa vähäisessä resurssitapauksessa pystyimme saamaan käännöksiä, joiden laatu ylittää Phrase Based Statistical Machine Translation -käännöksen laadun useilla BLEU-pisteillä. Yllättävin tulos, jonka saimme, oli hollanti-saksa ja italia-romania nollahaun asetus, jossa huomasimme, että vaikka näiden kieliparien välillä ei käytetä rinnakkaiskorpusia, NMT-malli pystyi kääntämään näiden kielten välillä ja käännökset olivat joko yhtä hyviä tai parempia (suhteessa BLEU) kuin non zero resurssi asetus. Todistamme myös, että NMT-mallit, joissa käytetään syöttötasoja ja itsehuomiota toistuvien kerrosten sijaan, ovat erittäin nopeita harjoittelun kannalta, mikä on hyödyllistä NMT-kokeellisessa ympäristössä.', 'jv': 'Awak dhéwé rambaran karo model Device translation (MT) lan gambaran sing ditoleh kanggo IWSLT 1997 Multilanguage shared task. Motyabase karo 0 shot NMT Awak dhéwé éntuk aturan sing paling nêmên dengané kayané awak dhéwé iso nggawe tarjamahan kayuté surat kuwi nggawe kalité piyé sing ngebekeré ning acara Rase basa Statatistik Mask Terjamahan seng liyane sing nyelehake nggambar barang blo. Awak dhéwé luwih apik dhéwé butawak iki ning acara éwé nggawe gerakan kanggo nggawe barang ning alaman-alaman karo italian-rumane kuwi, ning awak dhéwé ngerasai perusahaan karo perusahaan karo perusahaan langkung iki, model NMT iso terjamah karo perusahaan langguna iki lan terjamahan iki dadi apik dhéwé (terus bener CLUE) kaya kedaulatan ora nul Awak dhéwé éntuk sistem NMT sing nambah alat-sangan sing bakal bantuan karo nganggep bantuan alat sing bakal terus kalah bantuan ingkang dipatengan alat sing apik dhéwé. iki dadi sing luwih apik dhéwé, sing paling dhéwé kuwi alat dhéwé manut NMT.', 'ha': "Tuna bayyana a nan misalin Tarjifanmu na Mashinki (MT) da matsala wanda muka sãmu wa IWSLT 2017 An Motsar da Zero Shat NMT[1] Mun Trainin wata Tarjifan na Mashin Naural na Littafin da Muka haɗa duk ma'anar aiki zuwa juma guda da kuma a ƙara ayukan ayuka zuwa maganar source, don a nuna lugha da za'a fassara su da shi. Mun gane cẽwa, kõ dã a cikin wani wuri mai rauni na resource, za mu iya iya motsar fassarar masu karatun wanda tsari ya kifi sifar da waɗanda aka mottar da su na Bayani Mataimin Statistical Machine Translate by several points BLEU. Babbar fassarar da muka sãmi shi shine cikin sifire-shot set for Dush-Jeruman da Italian-Romiyan inda muka gani that ingawa ba mu yi amfani da parallel Coropa a tsakanin waɗannan harshen biyu, shirin NMT ya iya fassara tsakanin harshen da fassararsu sun kasance kõ da alhẽri (in the words of BLEU) daga non-0 resource settings. We also verify that the NMT models that use feed forward layers and self attention instead of recurrent layers are extremely fast in terms of training which is useful in a NMT experimental setting.", 'sk': 'Tukaj opisujemo naš model strojnega prevajanja (MT) in rezultate, ki smo jih pridobili za večjezično skupno nalogo IWSLT 2017. Z motivom Zero Shot NMT [1] smo usposabljali večjezični strojni prevod živcev z združitvijo vseh podatkov o vadbi v eno zbirko z dodajanjem žetonov izvornim stavkom, da bi navedli ciljni jezik, v katerega bi jih bilo treba prevesti. Ugotovili smo, da smo tudi v razmerah z nizkimi viri uspeli dobiti prevode, katerih kakovost presega kakovost tistih, ki jih je pridobil statistični strojni prevod na podlagi fraze za več točk BLEU. Najbolj presenetljiv rezultat, ki smo ga dobili, je bil v nastavitvi ničelnega strela za nizozemsko-nemško in italijansko-romunsko, kjer smo opazili, da je model NMT kljub uporabi vzporednih korpusov med temi jezikovnimi pari uspel prevajati med temi jeziki, prevodi pa so bili bodisi tako dobri ali boljši (glede na BLEU) kot nastavitev ničelnega vira. Prav tako preverjamo, da so modeli NMT, ki namesto ponavljajočih se plasti uporabljajo podajalne plasti in samopozornost, izjemno hitri v smislu usposabljanja, kar je koristno v eksperimentalnem okolju NMT.', 'he': 'אנחנו מתארים כאן את מודל התרגום המכונית שלנו (MT) והתוצאות שנקבלנו עבור המשימה המשותפת המורבית IWSLT 2017. מוטיבציה על ידי 0 Shot NMT [1] אימנו תרגום מכונת נוירולית רבות שפות על ידי שילוב את כל נתוני האימונים לאוסף אחד שמנו לב שאפילו במצב משאבים נמוך הצלחנו לקבל תרגומות שאיכות שלהן מעלית את איכות אלה שנקבלו על ידי תרגומת מכונת סטטיסטיקה מבוססת בפרזיות על ידי מספר נקודות BLEU. The most surprising result we obtained was in the zero shot setting for Dutch-German and Italian-Romanian where we observed that despite using no parallel corpora between these language pairs, the NMT model was able to translate between these languages and the translations were either as good as or better (in terms of BLEU) than the non zero resource setting.  We also verify that the NMT models that use feed forward layers and self attention instead of recurrent layers are extremely fast in terms of training which is useful in a NMT experimental setting.', 'bo': 'ང་ཚོས་དེ་རིང་འདིར་ང་ཚོའི་མིང་འཁོར་གྱི་ལྗོངས་སྤྱོད་ཀྱི་རྣམ་པ(MT)མ་དཔེ་བརྗོད་བྱས་ན། འུ་ཚོས་ཀྱིས་IWSLT 2017 རྩ་བ་ འདྲ་བཤུ་Zero Shot NMT[1]གིས་སྤྱད་ནས་ང་ཚོའི་སྐད་ཡིག་ཆ་གྲངས་ཀྱི་Neural Machine Translation་ཤིག་གིས་སྦྲེལ་མཐུད་རྒྱབ་སྐྱོར་གྱི་གནས་སྟངས་ཡོངས་རྫོགས་གཅིག་ནང་དུ་སྒྲིག་འཇུག་བྱེད་ ང་ཚོས་རྒྱུ་དངོས་གནས་ཚུལ་ཉུང་བའི་གནས་ཚུལ་འདིའི་ནང་དུ་ང་ཚོས་རང་ཉིད་ཀྱི་རྩིས་ཐང་ཡིན་པས། ཡིག་ཕེར་གཞི་གཞི་རྟེན་ནས་གནས་ཚུལ་གྱི་འགྱུར་བ ང་ཚོའི་རེ་བ་འབྲས་ཡོད་པའི་མཐའ་མཚམས་འདི་ནི་(zero)རྣམ་གྲངས་ཀྱི་སྒྲིག་འགོད་ཀྱི་ཐལ་ཤིག་ནི་ཕར་རིས། NMT་གི་དཔེ་དབྱིབས་སྔོན་སྐྱོད་པའི་བང་རིམ་དང་རང་ཉིད་ཀྱི་ལྟ་སྟངས་སྤྱོད་པའི་རྣམ་པ་དེ་ཡང་བསྐྱར་འགྱུར་བའི་བང་རིམ་ཚོས་མཚམས'}
{'en': 'Neural Machine Translation Training in a Multi-Domain Scenario', 'ar': 'تدريب الترجمة الآلية العصبية في سيناريو متعدد المجالات', 'es': 'Entrenamiento de traducción automática neuronal en un escenario multidominio', 'pt': 'Treinamento de tradução automática neural em um cenário de vários domínios', 'fr': 'Formation à la traduction automatique neuronale dans un scénario multidomaine', 'ja': 'マルチドメインシナリオにおける神経機械翻訳トレーニング', 'ru': 'Тренинг по нейронному машинному переводу в многодоменном сценарии', 'zh': '多域场中神经机器翻译训练', 'hi': 'एक बहु डोमेन परिदृश्य में तंत्रिका मशीन अनुवाद प्रशिक्षण', 'ga': 'Oiliúint Néar-Aistriúcháin Meaisín i gCás Ilfhearainn', 'ka': 'Name', 'hu': 'Neurális gépi fordítási képzés egy többtartományos forgatókönyvben', 'it': 'Formazione di traduzione automatica neurale in uno scenario multi-dominio', 'kk': 'Көп- домен сценариясында нейрондық машинаның аудару оқытуы', 'lt': 'Neuralinių mašin ų vertimo mokymas daugiadominiame scenarijuje', 'ms': 'Latihan Terjemahan Mesin Neural dalam Skenario Berbanyak Domain', 'el': 'Εκπαίδευση νευρωνικής μηχανικής μετάφρασης σε ένα σενάριο πολλαπλών τομέων', 'ml': 'Multi- Domain Scenario-ല്\u200d ന്യൂറല്\u200d മെഷീന്\u200d പരിഭാഷപ്പെടുത്തുന്നത്', 'mt': 'Taħriġ fit-Traduzzjoni tal-Magni Newrali f’Xenarju Multidomestiku', 'mk': 'Тренинг за превод на неврални машини во многутромно сценарио', 'mn': 'Мөн олон Домены сценариод мэдрэлийн машин хөгжүүлэх сургалт', 'pl': 'Szkolenie w tłumaczeniu maszynowym neuronalnym w scenariuszu wieloodziennym', 'ro': 'Instruire de traducere automată neurală într-un scenariu multidomeniu', 'no': 'Neuralmaskineomsetjing i ein fleire domenescenario', 'so': 'Turjumista maskinenta Neural Training in a Scenario Multi-Domain', 'sv': 'Neural Machine Translation Utbildning i ett scenarie med flera domäner', 'sr': 'Neuralna mašin a za prevod u multidomenu scenariju', 'si': 'ගොඩක් ඩොමේන් සේනාරියෝ වල නිර්මාණ මැෂින් වාර්තාව ප්\u200dරධානය', 'ta': 'Multi- Domain Scenario', 'ur': 'Multi-Domain Scenario میں نیورال ماشین ترجمن ترکین', 'uz': 'Name', 'vi': 'Dịch về máy thần kinh trên đa miền Đào tạo', 'bg': 'Обучение за неврален машинен превод в многодомейн сценарий', 'hr': 'Vježba za prevod neuroloških strojeva u multidomenom scenariju', 'nl': 'Neuronale Machine Translation Training in een multi-domein scenario', 'da': 'Træning i Neural maskinoversættelse i et scenarie med flere domæner', 'de': 'Training für neuronale maschinelle Übersetzung in einem Multi-Domain-Szenario', 'id': 'Latihan Translasi Mesin Neural dalam Skenario Multi-Domain', 'fa': 'تعلیم ترجمه ماشین عصبی در سینماری چند دامنی', 'ko': '다영역 장면에서의 신경 기계 번역 훈련', 'sw': 'Tafsiri ya Mashine ya Kijerumani', 'af': 'Name', 'sq': 'Trenimin e përkthimit të makinave nervore në një skenar shumë-domini', 'tr': 'Birnäçe sahypa senarynyň içinde näral maşynyň terjime trenlemesi', 'am': 'ምርጫዎች', 'hy': 'Նյարդային մեքենայի թարգմանման փորձը բազմաբնական սցենարիում', 'az': 'Multi-Domain Scenario içində nöral Makinelərin Çeviri Eğitimi', 'bn': 'বহুডোমেইন স্কেনারিয়ায় নিউরেল মেশিন অনুবাদ প্রশিক্ষণ', 'bs': 'Vježba za prevod neuroloških strojeva u multidomenom scenariju', 'ca': 'Formació de traducció de màquines neuronals en un escenari multidominical', 'cs': 'Školení neuronového strojového překladu v multidoménovém scénáři', 'et': 'Neuraalse masintõlke koolitus mitmedomeenilises stsenaariumis', 'fi': 'Neuraalisen konekäännöksen koulutus usean toimialueen skenaariossa', 'jv': 'Nyural Majin Terjamahan Karo Multi-domain Senari', 'sk': 'Usposabljanje nevralnega strojnega prevajanja v scenariju več področij', 'ha': 'translation', 'he': 'אימון תרגום מכונות נוירות בסצנריו של תרגומים רבים', 'bo': 'སྣ་མང་གསོག་ཁྲམ་གྱི་ཤོག་ལེབ་ནང་དུ་Neural Machine Translation Training'}
{'en': 'In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and multi-model ensemble. Our findings show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the  model  is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality. A  weighted ensemble  of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning an already trained model.', 'ar': 'في هذه الورقة ، نستكشف طرقًا بديلة لتدريب نظام الترجمة الآلية العصبية في سيناريو متعدد المجالات. نحن نحقق في تسلسل البيانات (مع الضبط الدقيق) ، وتكديس النماذج (الضبط الدقيق متعدد المستويات) ، واختيار البيانات ومجموعة النماذج المتعددة. تُظهر النتائج التي توصلنا إليها أنه يمكن تحقيق أفضل جودة للترجمة من خلال بناء نظام أولي على سلسلة من البيانات المتاحة خارج المجال ثم ضبطها على البيانات الموجودة في المجال. يعمل تكديس النموذج بشكل أفضل عندما يبدأ التدريب بأبعد بيانات خارج المجال ويتم ضبط النموذج بشكل تدريجي مع النطاق الأبعد التالي وما إلى ذلك. لم يؤد اختيار البيانات إلى أفضل النتائج ، ولكن يمكن اعتباره بمثابة حل وسط لائق بين وقت التدريب وجودة الترجمة. كان أداء المجموعة الموزونة من النماذج الفردية المختلفة أفضل من اختيار البيانات. إنه مفيد في سيناريو عندما لا يكون هناك وقت لضبط نموذج مدرب بالفعل.', 'fr': "Dans cet article, nous explorons d'autres moyens d'entraîner un système de traduction automatique neuronale dans un scénario multidomaine. Nous étudions la concaténation des données (avec réglage fin), l'empilement de modèles (réglage fin multi-niveaux), la sélection des données et l'ensemble multi-modèles. Nos résultats montrent que la meilleure qualité de traduction peut être obtenue en créant un système initial basé sur une concaténation de données hors domaine disponibles, puis en l'ajustant avec précision sur les données internes au domaine. L'empilage de modèles fonctionne mieux lorsque la formation commence avec les données hors domaine les plus éloignées et que le modèle est progressivement affiné avec le domaine le plus éloigné suivant, etc. La sélection des données n'a pas donné les meilleurs résultats, mais peut être considérée comme un bon compromis entre le temps de formation et la qualité de la traduction. Un ensemble pondéré de différents modèles individuels a obtenu de meilleurs résultats que la sélection de données. Cela est bénéfique dans un scénario où il n'y a pas de temps pour peaufiner un modèle déjà entraîné.", 'pt': 'Neste artigo, exploramos formas alternativas de treinar um sistema de tradução automática neural em um cenário multidomínio. Investigamos a concatenação de dados (com ajuste fino), empilhamento de modelos (ajuste fino multinível), seleção de dados e ensemble multimodelo. Nossas descobertas mostram que a melhor qualidade de tradução pode ser alcançada construindo um sistema inicial em uma concatenação de dados disponíveis fora do domínio e, em seguida, ajustando-o nos dados no domínio. O empilhamento de modelos funciona melhor quando o treinamento começa com os dados fora do domínio mais distantes e o modelo é ajustado de forma incremental com o próximo domínio mais distante e assim por diante. A seleção de dados não deu os melhores resultados, mas pode ser considerada como um compromisso decente entre o tempo de treinamento e a qualidade da tradução. Um conjunto ponderado de diferentes modelos individuais teve um desempenho melhor do que a seleção de dados. É benéfico em um cenário em que não há tempo para ajustar um modelo já treinado.', 'es': 'En este artículo, exploramos formas alternativas de entrenar un sistema de traducción automática neuronal en un escenario multidominio. Investigamos la concatenación de datos (con ajuste fino), el apilamiento de modelos (ajuste fino multinivel), la selección de datos y el ensamblaje de varios modelos. Nuestros hallazgos muestran que se puede lograr la mejor calidad de traducción si se crea un sistema inicial a partir de una concatenación de datos disponibles fuera del dominio y, a continuación, se ajusta a los datos del dominio. El apilamiento de modelos funciona mejor cuando el entrenamiento comienza con los datos más alejados fuera del dominio y el modelo se ajusta gradualmente con el siguiente dominio más lejano, etc. La selección de datos no dio los mejores resultados, pero puede considerarse como un compromiso decente entre el tiempo de formación y la calidad de la traducción. Un conjunto ponderado de diferentes modelos individuales funcionó mejor que la selección de datos. Es beneficioso en un escenario en el que no hay tiempo para ajustar un modelo ya entrenado.', 'ja': '本稿では、マルチドメインシナリオにおけるニューラル機械翻訳システムを訓練するための代替的な方法を探求する。データの連結（微調整）、モデルの積み重ね（マルチレベルの微調整）、データの選択、マルチモデルアンサンブルを調査します。私たちの調査結果は、利用可能なドメイン外データの連結に初期システムを構築し、それをドメイン内データで微調整することによって、最高の翻訳品質を達成できることを示しています。モデルのスタッキングは、トレーニングが最も遠いドメイン外データで開始され、モデルが最も遠いドメインで増分的に微調整される場合に最適です。データの選択は最良の結果をもたらすものではありませんでしたが、トレーニング時間と翻訳品質の間のまともな妥協と見なすことができます。異なる個々のモデルの加重アンサンブルは、データ選択よりも優れたパフォーマンスを発揮しました。すでに訓練されたモデルを微調整する時間がないシナリオでは有益です。', 'zh': '本文中,探多域场中训练神经机器翻译系统之法。 臣等考数联(带微调),堆叠(多级),数与多模集成。 臣等考结果表明,于域外数串结初统,然后域内数调,可得最佳译质。 凡练自至远之数,而模用下一最远增量微调之时,积功最佳,依此类推。 数择未善,然可以为培训时译之间一折衷也。 异形者加权合于数也。 未暇微调于已练者,有益也。', 'hi': 'इस पेपर में, हम एक बहु-डोमेन परिदृश्य में एक तंत्रिका मशीन अनुवाद प्रणाली को प्रशिक्षित करने के लिए वैकल्पिक तरीकों का पता लगाते हैं। हम डेटा संयोजन (ठीक ट्यूनिंग के साथ), मॉडल स्टैकिंग (मल्टी-लेवल फाइन ट्यूनिंग), डेटा चयन और मल्टी-मॉडल एनसेंबल की जांच करते हैं। हमारे निष्कर्ष ों से पता चलता है कि सबसे अच्छी अनुवाद गुणवत्ता उपलब्ध आउट-ऑफ-डोमेन डेटा के संयोजन पर एक प्रारंभिक प्रणाली का निर्माण करके और फिर इसे इन-डोमेन डेटा पर ठीक-ट्यून करके प्राप्त की जा सकती है। मॉडल स्टैकिंग सबसे अच्छा काम करता है जब प्रशिक्षण सबसे दूर के आउट-ऑफ-डोमेन डेटा के साथ शुरू होता है और मॉडल अगले सबसे दूर डोमेन के साथ वृद्धिशील रूप से ठीक-ट्यून किया जाता है और इसी तरह। डेटा चयन ने सबसे अच्छा परिणाम नहीं दिया, लेकिन प्रशिक्षण समय और अनुवाद की गुणवत्ता के बीच एक सभ्य समझौता माना जा सकता है। विभिन्न व्यक्तिगत मॉडलों का एक भारित पहनावा डेटा चयन की तुलना में बेहतर प्रदर्शन किया। यह एक परिदृश्य में फायदेमंद है जब पहले से ही प्रशिक्षित मॉडल को ठीक करने के लिए कोई समय नहीं है।', 'ru': 'В этой статье мы исследуем альтернативные способы обучения нейронной системы машинного перевода в многодоменном сценарии. Мы исследуем конкатенацию данных (с точной настройкой), суммирование моделей (многоуровневая точная настройка), выбор данных и мультимодельный ансамбль. Наши результаты показывают, что наилучшее качество перевода может быть достигнуто путем построения начальной системы на объединении доступных внедоменных данных, а затем их точной настройки на внутридоменных данных. Стек модели лучше всего работает, когда обучение начинается с самых удаленных внедоменных данных, а модель постепенно настраивается со следующим самым удаленным доменом и так далее. Отбор данных не дал наилучших результатов, но может рассматриваться как достойный компромисс между временем обучения и качеством перевода. Взвешенный набор различных отдельных моделей показал лучшие результаты, чем выбор данных. Полезно в сценарии, когда нет времени на доработку уже обученной модели.', 'ga': "Sa pháipéar seo, déanaimid iniúchadh ar bhealaí eile chun córas néar-aistriúcháin meaisín a oiliúint i gcás ilfhearann. Déanaimid imscrúdú ar chomhchatú sonraí (le mionchoigeartú), cruachta samhlacha (míntiús illeibhéil), roghnú sonraí agus ensemble ilmhúnlaí. Léiríonn ár dtorthaí gur féidir an cháilíocht aistriúcháin is fearr a bhaint amach trí chóras tosaigh a thógáil ar chomhchaitheamh na sonraí atá ar fáil lasmuigh den fhearann agus ansin é a mhionchoigeartú ar shonraí san fhearann. Is fearr a oibríonn cruachta samhlacha nuair a thosaíonn an oiliúint leis na sonraí is faide amach as an bhfearann agus déantar an tsamhail a mhionchoigeartú go hincriminteach leis an gcéad réimse eile is faide amach agus mar sin de. Níor thug an roghnú sonraí na torthaí is fearr, ach is féidir é a mheas mar chomhréiteach réasúnta idir am oiliúna agus cáilíocht an aistriúcháin. D'fheidhmigh ensemble ualaithe de shamhlacha aonair éagsúla níos fearr ná roghnú sonraí. Tá sé tairbheach i gcás nach bhfuil aon am ann le mionchoigeartú a dhéanamh ar shamhail atá oilte cheana féin.", 'ka': 'ამ დომენში ჩვენ ვაკვირდებით ალტენტრუმენტური გზები, რომელიც ნეიროლური მანქანის გაგრძელების სისტემის მრავალური დიომინური სინარიოში. ჩვენ განსხვავებთ მონაცემების შეცდომარება (სწორი შეცდომარებით), მოდელური შეცდომარება (მრავალური შეცდომარება), მონაცემები და მრავალური მოდელური შეც ჩვენი შესაძლებლობები აჩვენებენ, რომ საუკეთესო წარმოწყობილობა შესაძლებელია გადაწყობილობა სისტემის შექმნა, რომელიც საშუალო დემომინის მონაცემების გარეშე და შემდეგ დაწყ Model stacking works best when training starts with the highest out-of-domain data and the model is incrementally fine tuned with the next farthest domain and so on. Data selection did not give the best results, but can be considered a decent compromise between training time and translation quality. განსხვავებული ინდეგუფიკალური მოდელების მარცხენა, რომლებიც მონაცემების არჩევაზე უკეთესია. რჲგა ვ ბვჱოჲლჱთრვლნჲ გ ჟვნაპთჲ, კჲდარჲ ნწმა გპვმვ ეა ნაოპაგთქ გვფვ რპვნთპან მჲევლ.', 'el': 'Σε αυτή την εργασία, διερευνούμε εναλλακτικούς τρόπους εκπαίδευσης ενός νευρικού συστήματος μηχανικής μετάφρασης σε ένα σενάριο πολλαπλών τομέων. Ερευνούμε τη συμπλήρωση δεδομένων (με λεπτή ρύθμιση), τη συσσώρευση μοντέλων (πολλαπλός συντονισμός), την επιλογή δεδομένων και το σύνολο πολλαπλών μοντέλων. Τα ευρήματά μας δείχνουν ότι η καλύτερη ποιότητα μετάφρασης μπορεί να επιτευχθεί με την οικοδόμηση ενός αρχικού συστήματος πάνω σε μια αλληλουχία διαθέσιμων δεδομένων εκτός πεδίου και στη συνέχεια τον συντονισμό με δεδομένα εντός πεδίου. Η συσσώρευση μοντέλων λειτουργεί καλύτερα όταν η εκπαίδευση αρχίζει με τα πιο απομακρυσμένα δεδομένα εκτός πεδίου και το μοντέλο προσαρμόζεται σταδιακά με τον επόμενο μακρύτερο τομέα και ούτω καθεξής. Η επιλογή δεδομένων δεν έδωσε τα καλύτερα αποτελέσματα, αλλά μπορεί να θεωρηθεί ως ένας αξιοπρεπής συμβιβασμός μεταξύ του χρόνου κατάρτισης και της ποιότητας της μετάφρασης. Ένα σταθμισμένο σύνολο διαφορετικών μεμονωμένων μοντέλων πέτυχε καλύτερα από την επιλογή δεδομένων. Είναι ευεργετικό σε ένα σενάριο όπου δεν υπάρχει χρόνος για την τελειοποίηση ενός ήδη εκπαιδευμένου μοντέλου.', 'hu': 'Ebben a tanulmányban alternatív módokat vizsgálunk egy neurális gépi fordító rendszer képzésére egy több domain forgatókönyvben. Vizsgáljuk az adatok összekapcsolását (finomhangolással), a modellek halmozását (többszintű finomhangolással), az adatok kiválasztását és a többmodelles együtteset. Eredményeink azt mutatják, hogy a lehető legjobb fordítási minőség elérhető egy kezdeti rendszer létrehozásával a rendelkezésre álló, domain kívüli adatok összekapcsolására, majd finomhangolásával a domain kívüli adatokra. A modellhalmozás akkor működik a legjobban, amikor a képzés a legtávolabbi tartományon kívüli adatokkal kezdődik, és a modellt fokozatosan finomhangolják a következő legtávolabbi tartományra és így tovább. Az adatok kiválasztása nem hozta a legjobb eredményt, de tisztességes kompromisszumnak tekinthető a képzési idő és a fordítás minősége között. A különböző modellek súlyozott együttese jobban teljesített, mint az adatok kiválasztása. Előnyös egy olyan forgatókönyv esetén, amikor nincs idő egy már képzett modell finomhangolására.', 'it': "In questo articolo, esploriamo modi alternativi per addestrare un sistema di traduzione automatica neurale in uno scenario multi-dominio. Investighiamo la concatenazione dei dati (con fine tuning), l'impilamento dei modelli (multi-level fine tuning), la selezione dei dati e l'insieme dei modelli multipli. I nostri risultati dimostrano che la migliore qualità di traduzione può essere raggiunta costruendo un sistema iniziale su una concatenazione di dati fuori dominio disponibili e poi regolandolo sui dati in-dominio. L'impilamento dei modelli funziona meglio quando l'allenamento inizia con i dati fuori dominio più lontani e il modello viene progressivamente perfezionato con il dominio successivo più lontano e così via. La selezione dei dati non ha dato i migliori risultati, ma può essere considerata un compromesso decente tra il tempo di formazione e la qualità della traduzione. Un insieme ponderato di diversi modelli individuali ha funzionato meglio della selezione dei dati. È utile in uno scenario in cui non c'è tempo per perfezionare un modello già addestrato.", 'lt': 'Šiame dokumente mes ieškome alternatyvių būdų mokyti nervinių mašin ų vertimo sistemą daugiadominiame scenarijuje. Mes tiriame duomenų sutapimą (su smulkiu pritaikymu), modelių sutvarkymą (daugiapakopį patikslinimą), duomenų atranką ir daugiapakopį rinkinį. Mūsų išvados rodo, kad geriausią vertimo kokybę galima pasiekti kuriant pradinę sistemą, kurioje būtų suderinti turimi ne domeno duomenys, o vėliau tiksliai pritaikyti prie domeno duomenų. Pavyzdžių rinkimas geriausiai veikia, kai mokymas prasideda iš tolimiausių ne srities duomenų, o model is palaipsniui patobulinamas su kita tolimiausia sritimi ir t. t. Duomenų atranka nepateikė geriausių rezultatų, tačiau gali būti laikoma deramu mokymo laiko ir vertimo kokybės kompromisu. Įvairių atskirų modelių svertinis rinkinys buvo geresnis už duomenų atranką. Tai naudinga scenarijuje, kai nėra laiko tobulinti jau parengtą model į.', 'mk': 'Во овој весник, истражуваме алтернативни начини за обука на систем за превод на невровни машини во повеќето домени сценарио. Истражуваме концентрација на податоците (со фино прилагодување), стапување на модели (мулти-ниво фино прилагодување), избор на податоци и мулти-моделен ансембл. Нашите откритија покажуваат дека најдобриот квалитет на превод може да се постигне со изградба на иницијален систем на концентрација на достапни податоци надвор од доменот и потоа финетизирање на податоците во доменот. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on.  Селекцијата на податоци не даде најдобри резултати, туку може да се смета за пристоен компромис помеѓу времето на обука и квалитетот на превод. Тежен ансембл на различни индивидуални модели изврши подобро од изборот на податоци. Тоа е корисно во сценарио кога нема време за подобрување на веќе обучен модел.', 'kk': 'Бұл қағазда, біз невралдық компьютерді аудару жүйесін көп доменнің сценариясында оқыту арқылы іздейміз. Біз деректерді баптау (дұрыс баптау), модель баптау (көп деңгейлі баптау), деректерді таңдау және көп үлгілерді қолданып тұрамыз. Біздің іздегеніміз ең жақсы аудармалардың сапасы бар доменге бар деректерді біріктіру үшін бастапқы жүйені құрып, доменге келтіріп, оны домен деректеріне жақсы түзетуге болады. Оқыту үлгісі доменге басталғанда, үлгісі келесі артық доменге жұмыс істейді. Деректерді таңдау үшін ең жақсы нәтижелер бермейді, бірақ оқыту уақыты мен аудармалардың сапасы арасындағы дұрыс сәйкес деп ойлауға болады. Деректерді таңдауынан артық түрлі үлгілердің тең символы. Бұл сценарияда, бірақ оқылған моделді түзету үшін уақыт жоқ.', 'ms': 'Dalam kertas ini, kami mengeksplorasi cara alternatif untuk melatih sistem terjemahan mesin saraf dalam skenario multi-domain. Kami menyelidiki penyesuaian data (dengan penyesuaian baik), penyesuaian model (penyesuaian baik berbilang-tahap), pemilihan data dan kumpulan berbilang-model. Penemuan kami menunjukkan kualiti terjemahan terbaik boleh dicapai dengan membina sistem awal pada persatuan data luar domain yang tersedia dan kemudian menyesuaikannya pada data dalam domain. Stacking model berfungsi terbaik bila latihan bermula dengan data luar domain yang paling jauh dan model secara bertambah ditetapkan dengan domain terjauh berikutnya dan sebagainya. Pemilihan data tidak memberikan keputusan terbaik, tetapi boleh dianggap sebagai kompromi yang layak antara masa latihan dan kualiti terjemahan. Satu kumpulan berat model individu yang berbeza dilakukan lebih baik daripada pemilihan data. Ia berguna dalam skenario apabila tiada masa untuk menyesuaikan model yang sudah dilatih.', 'mt': "F’dan id-dokument, a ħna nesploraw modi alternattivi biex in ħarrġu sistema ta’ traduzzjoni ta’ magni newrali f’xenarju multidomestiku. Aħna ninvestigaw il-konċentrazzjoni tad-dejta (b’aġġustament fin), l-istakkar tal-mudelli (aġġustament fin f’diversi livelli), l-għażla tad-dejta u ensemble ta’ mudelli multipli. Is-sejbiet tagħna juru li l-a ħjar kwalità tat-traduzzjoni tista’ tinkiseb billi tinbena sistema inizjali fuq konċentrazzjoni tad-dejta disponibbli barra d-dominju u mbagħad tiġi rfinuta fuq id-dejta fid-dominju. L-istakkar tal-mudell jaħdem bl-aħjar mod meta t-taħriġ jibda bl-aktar dejta 'l barra mid-dominju u l-mudell jiġi aġġustat inkrementalment bl-aktar dominju 'l bogħod li jmiss, eċċ. L-għa żla tad-dejta ma tat l-aħjar riżultati, iżda tista’ titqies bħala kompromess deċenti bejn iż-żmien tat-taħriġ u l-kwalità tat-traduzzjoni. Ensemble peżat ta’ mudelli individwali differenti mwettaq aħjar mill-għażla tad-dejta. Huwa ta’ benefiċċju f’xenarju fejn m’hemmx żmien biex jiġi rfinut mudell diġà mħarreġ.", 'mn': 'Энэ цаасан дээр бид мэдрэлийн дамжуулах системийг олон доторх хувилбарт суралцах өөр аргыг судалж байна. Бид өгөгдлийн тохиромжтой (сайн тохиромжтой), загварын тохиромжтой (олон түвшин тохиромжтой тохиромжтой), өгөгдлийн сонголт, олон загварын тохиромжтой судалгаа хийсэн. Бидний олж мэдсэн зүйлс нь хамгийн сайн орчуулагчийн чанарыг хүлээж чадна. Эхний системийг зохион байгуулж, холбоотой мэдээллийг багтааж, дараа нь холбоотой мэдээллээр тодорхойлж чадна. Багш суралцах нь хамгийн алсын холбоотой мэдээллээс эхлэх үед сайн ажилладаг. Загвар нь дараагийн хамгийн алсын холбоотой байдаг. Өгөгдлийн сонголт хамгийн сайн үр дүнг өгсөн биш, гэхдээ суралцах цаг болон орчуулах чадварын хоорондох сайн зөрчилдөөн гэж үзэж болно. Өөр төрлийн загваруудын жингийн тоо нь өгөгдлийн сонголтоос илүү сайн хийсэн. Энэ нь аль хэдийн сургалтын загварыг тодорхойлдох цаг байхгүй.', 'ml': 'ഈ പത്രത്തില്\u200d, നമ്മള്\u200d മറ്റൊരു വഴികള്\u200d പരിശീലിക്കുന്നു. ഒരു പുരുഷന്\u200d മെഷീന്\u200d പരിശീലിപ്പിക്കാന്\u200d വേണ്ടി മാറ്റുന്ന വഴ നമ്മള്\u200d ഡേറ്റാ കൂട്ടിക്കൊണ്ടിരിക്കുന്നത് അന്വേഷിക്കുന്നു (നല്ല മാതൃകയുമായി), മോഡല്\u200d സ്റ്റാക്കിങ്ങ് ചെയ്യുന്ന നമ്മുടെ കണ്ടുപിടികള്\u200d കാണിക്കുന്നത് നമ്മുടെ ഏറ്റവും മികച്ച വിവരങ്ങളുടെ ഗുണത്തില്\u200d എത്താന്\u200d സാധിക്കുന്നതാണ്. ഒരു ആദ്യ സിസ്റ്റം നിര്\u200d മോഡല്\u200d സ്റ്റാക്കിങ്ങ് പ്രവര്\u200dത്തിക്കുന്നത് ഏറ്റവും വിശിഷ്ടമായ ഡോമെയിന്\u200d ഡേറ്റായി തുടങ്ങുമ്പോള്\u200d ഏറ്റവും നല്ലതാണ്. പിന്നെ മോഡല്\u200d അട ഡേറ്റാ തെരഞ്ഞെടുക്കുന്നത് ഏറ്റവും നല്ല ഫലങ്ങള്\u200d നല്\u200dകിയിട്ടില്ല, പക്ഷെ പരിശീലന സമയത്തിനും പരിഭാഷക്കുള്ള സമയത്തി വ്യത്യസ്ത വ്യക്തിക മോഡലുകളുടെ ഭാരം തൂക്കപ്പെട്ട ഒരു എണ്\u200dസ്പെല്\u200d ഡേറ്റാ തെരഞ്ഞെടുക്കുന്നതി ഒരു പരിശീലന മോഡലിന് സമയമില്ലാത്ത ഒരു സിനേറ്ററിയോയില്\u200d അത് പ്രയോജനപ്പെടുന്നു.', 'no': 'I denne papiret utforskar vi alternativ måtar å trena eit neuralmaskinsomsetjingssystem i ein fleire domenescenario. Vi undersøker samsvaring av data (med fint innstilling), modellstaking (fleirnivå fint innstilling), datautval og fleirmodell. Finningane våre viser at den beste omsetjingskvaliteten kan oppnå ved å bygge eit opphavleg system på sammenlikning av tilgjengelege data frå avdomenet og så finn oppsett av data frå i domenet. Modellstakking fungerer best når opplæring startar med dei fjerste ute- domenedata og modellen er aukande fint med neste fjerste domenet og så videre. Data-utval gjev ikkje dei beste resultatene, men kan verta kalla som ein godt kompromis mellom treningstid og omsetjingskvalitet. Eit vektsmerke av ulike enkelte modeller som utførte bedre enn datautval. Det er nyttig i ein scenario når det ikkje er noko tid for å finna ein allereie trengt modell.', 'pl': 'W niniejszym artykule badamy alternatywne sposoby treningu neuronowego systemu tłumaczenia maszynowego w scenariuszu wieloodziennym. Badamy łączenie danych (z precyzyjnym dostrojeniem), układanie modeli (wielopoziomowe dostrojenie), wybór danych oraz zespół wielopoziomowy. Nasze wyniki pokazują, że najlepszą jakość tłumaczeń można osiągnąć poprzez zbudowanie wstępnego systemu opartego na łączeniu dostępnych danych poza domeną, a następnie dostosowanie go na danych wewnątrz domeny. Układanie modeli działa najlepiej, gdy trening rozpoczyna się od najdalszych danych poza domeną, a model jest stopniowo dostosowywany do następnej najdalszej domeny i tak dalej. Wybór danych nie dał najlepszych rezultatów, ale może być uznany za przyzwoity kompromis pomiędzy czasem szkolenia a jakością tłumaczenia. Ważony zespół różnych poszczególnych modeli sprawdził się lepiej niż wybór danych. Jest to korzystne w sytuacji, gdy nie ma czasu na dopracowanie już przeszkolonego modelu.', 'ro': 'În această lucrare, explorăm modalități alternative de a instrui un sistem de traducere automată neurală într-un scenariu multi-domeniu. Investigăm concatenarea datelor (cu ajustare fină), stivuirea modelelor (reglare fină pe mai multe niveluri), selectarea datelor și ansamblul multimodel. Rezultatele noastre arată că cea mai bună calitate a traducerii poate fi obținută prin construirea unui sistem inițial pe o concatenare a datelor disponibile din afara domeniului și apoi ajustarea acestuia pe datele din domeniu. Stivuirea modelelor funcționează cel mai bine atunci când instruirea începe cu cele mai îndepărtate date din afara domeniului și modelul este reglat progresiv cu următorul domeniu cel mai îndepărtat și așa mai departe. Selectarea datelor nu a dat cele mai bune rezultate, dar poate fi considerată un compromis decent între timpul de formare și calitatea traducerii. Un ansamblu ponderat de diferite modele individuale a performat mai bine decât selectarea datelor. Este benefic într-un scenariu în care nu există timp pentru reglarea fină a unui model deja instruit.', 'sr': 'U ovom papiru istražujemo alternativne načine da obučimo sistem neuronskog prevoda u multidomenu scenariju. Istražujemo potvrdu podataka (sa dobrim tuniranjem), model stacking (višeniverski fin tuning), selekciju podataka i multimodel ensemble. Naši nalazi pokazuju da se najbolji kvalitet prevoda može postići izgradnjem početnog sistema na usklađivanju dostupnih podataka izvan domena i zatim ga ispravljanjem podataka u domenu. Model stacking funkcioniše najbolje kada trening počne sa najdaljim podacima iz domena i model je veoma finalno prilagođen sledećim najdaljim domenom i tako daljim. Izbor podataka nije dao najbolje rezultate, ali može se smatrati pristojnim kompromisom između treninga i kvalitete prevoda. Težak ensember različitih individualnih modela izvršen je bolji od selekcije podataka. Korisno je u scenariju kada nema vremena za finaliziranje veæ obučenog model a.', 'so': 'Qoraalkan waxaynu ku baaraannaa waddooyin kala duwan oo aan ku tababarinno nidaamka tarjumaadda maskaxda neurada ah oo ku yaala muuqasho kala duduwan. Waxaannu baaraynaa kooxaha macluumaadka (sameynta qurxooyinka), sameynta qaabilaada (sameynta heerarka kala duduwan), doorashada macluumaadka iyo tusaale kale oo kala duduwan. Shaqooyinkayada waxaa muuqanaya in qiimaha turjumidda ugu wanaagsan lagu sameyn karo nidaam bilowga ah oo lagu sameynayo macluumaad a an degenaanshaha laga helin, kadibna si fiican looga sameyn karo macluumaadka gudaha. Isticmaalka muusikadu wuxuu si wanaagsan u shaqeeyaa marka waxbarashada laga bilaabo macluumaadka ka baxsan ee gudaha, modelkuna wuxuu si wanaagsan ugu shaqeeyaa domain ugu dambeeya. Dalbashada macluumaadku ma bixinin midhaha ugu wanaagsan, laakiin waxaa looga xisaabin karaa dhibaato wanaagsan oo u dhexeeya xilliga waxbarashada iyo takhasuska turjumidda. Noocyo kala duduwan oo shakhsi ah waxaa la sameeyay mid ka wanaagsan doorashada macluumaadka. Waxaa faa’iido ku leh muusikada marka aysan waqti ku jirin sameynta model la tababaray.', 'sv': 'I denna uppsats utforskar vi alternativa sﾃ､tt att trﾃ､na ett neuralt maskinﾃｶversﾃ､ttningssystem i ett flerdomﾃ､nsscenario. Vi undersﾃｶker datasammankoppling (med finjustering), modellstapling (flernivﾃ･finjustering), dataval och flermodellensemble. Vﾃ･ra resultat visar att bﾃ､sta ﾃｶversﾃ､ttningskvalitet kan uppnﾃ･s genom att bygga ett inledande system pﾃ･ en sammanslagning av tillgﾃ､ngliga data utanfﾃｶr domﾃ､nen och sedan finjustera det pﾃ･ data inom domﾃ､nen. Modellstapling fungerar bﾃ､st nﾃ､r trﾃ､ningen bﾃｶrjar med den lﾃ､ngst utanfﾃｶr domﾃ､nen data och modellen finjusteras stegvis med nﾃ､sta lﾃ､ngst domﾃ､n och sﾃ･ vidare. Datavalet gav inte de bﾃ､sta resultaten, men kan betraktas som en anstﾃ､ndig kompromiss mellan utbildningstid och ﾃｶversﾃ､ttningskvalitet. En viktad ensemble av olika individuella modeller presterade bﾃ､ttre ﾃ､n dataval. Det ﾃ､r fﾃｶrdelaktigt i ett scenario dﾃ､r det inte finns tid att finjustera en redan trﾃ､nad modell.', 'ta': 'இந்த காகிதத்தில், நாம் மாற்று வழிகளை கண்டுபிடிக்க முடியும் பல- டோமைன் காட்சியில் ஒரு புதிய இயந்திர மொழிமாற் நாம் தரவு கூட்டுதலை (நன்றாக துண்டிக்கொண்டு, மாதிரி தடுப்பு (பல- நிலையில் நல்ல துண்டுதல்), தரவு தேர்வு மற்றும் பல- மாதிரி ஒ எங்கள் கண்டுபிடிப்புகள் சிறந்த மொழிபெயர்ப்பு தரம் மூலம் ஒரு ஆரம்ப அமைப்பை உருவாக்கி காட்டுகிறது கிடைக்கும் தளத்தில் இருக்கும் தகவல்  மாதிரி துடுக்குதல் துவங்கும்போது பயிற்சி துவங்கும் போது மாதிரி சிறந்ததாக செயல்படுகிறது மற்றும் மாதிரி அடுத்த கடினமான களம் மூலம்  தரவு தேர்வு சிறந்த முடிவுகள் கொடுக்கவில்லை, ஆனால் பயிற்சி நேரத்திற்கும் மொழிபெயர்ப்பு தரமும் இடையே ஒரு சரிய தரவு தேர்ந்தெடுப்பை விட சிறப்பாக செயல்படுத்தப்பட்ட தனிப்பட்ட மாதிரிகளின் ஒரு நிறைவேறு ஒளிக்க இது ஏற்கனவே பயிற்சிக்கப்பட்ட மாதிரியில் நன்றாக பயிற்சி செய்ய நேரம் இல்லை என்று ஒரு காட்சியில் பயனுள்ளது.', 'si': 'මේ පැත්තේ අපි වෙනස් විදියට පරීක්ෂා කරන්නේ න්\u200dයූරාල් මැෂින් වාර්තාව පද්ධතියක් ගොඩක් ඩොමේන් සිනාර අපි දත්ත සංවිධානය (හොඳ සංවිධානය සමග) පරීක්ෂා කරනවා, මොඩේල් ස්ටැක් කරනවා (විශිෂ්ට සංවිධානය සමග, ද අපේ හොයාගන්න පෙන්වන්නේ හොඳම වාර්ථාව ප්\u200dරමාණයක් ලැබෙන්න පුළුවන් විදියට පද්ධතියක් නිර්මාණය කරන්න පුළුවන් විදියට ප්\u200d මොඩේල් ස්ටැකින් හොඳම වැඩ කරනවා ප්\u200dරේෂණය පටන් ගත්තොත් පුළුවන් පුළුවන් වෙලාවට සහ ප්\u200dරමාණය පටන් ගත්තොත් හොඳම වැඩ කරනවා  දත්ත තෝරණය හොඳම ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප Name ඒක ප්\u200dරයෝජනයක් තියෙන්නේ දැනටමත් ප්\u200dරයෝජනය කරලා තියෙන වෙලාවක් නෑ කියලා.', 'ur': 'اس کاغذ میں، ہم ایک نئورل ماشین ترجمہ سیسٹم کی تعلیم دینے کے لئے مختلف طریقے کی تلاش کرتے ہیں. ہم نے ڈاٹ کے سامنے (ٹھیک ٹونگ کے ساتھ) موڈل استاکنگ (متعدد سطح ٹھیک ٹونگ کے ساتھ) ڈاٹ کا انتخاب اور متعدد موڈل انتخاب کیا ہے۔ ہمارے نتیجے نشان دیتے ہیں کہ بہترین ترجمہ کیفیت کو پہنچا سکتی ہے ایک پہلی سیستم کے ذریعہ سے موجود دیومین خارج سے ڈاٹی کے مطابق اور اس کے بعد ڈومین ڈاٹی پر خوب تنظیم کر سکتی ہے. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. ڈاٹا انتخاب نے بہترین نتیجے نہیں دیے، لیکن ٹرینگ زمان اور ترجمہ کیفیت کے درمیان اچھا ترجمہ سمجھ سکتا ہے. ڈیٹا انتخاب سے بہتر کام کیے گئے مختلف فرقے موڈل کے ایک وزن انڈیل. یہ ایک سناریو میں فائدہ اٹھاتا ہے جبکہ ایک پہلے کی تعلیم کی مدل کی تعلیم کے لئے وقت نہیں ہے.', 'uz': "Bu qogʻozda biz bir necha domen scenarida neyrolik tarjima tizimni o'rganish uchun boshqa usullarni o'rganamiz. Biz maʼlumotlar birlashtirishni qidirimiz (yaxshi ko'proq tugmalar bilan, modelni saqlash (ko'p darajada yaxshi bogʻlash), maʼlumot tanlash va bir necha model foydalanuvchisi. Bizning murojaatlarimizni ko'rsatishimiz mumkin, eng yaxshi tarjima sifatida birinchi tizimni qo'yish mumkin, mavjud maʼlumotdan foydalanish mumkin va keyin domen maʼlumotlarida birga yaxshi ko'rsatish mumkin. Name @ info: whatsthis Name Ehtimol o'rganilgan modelni ajratish vaqt emas.", 'vi': 'Trong tờ giấy này, chúng tôi tìm cách khác để đào tạo một hệ thống dịch chuyển máy thần kinh trong một kịch bản đa miền. Chúng tôi điều tra kết nối dữ liệu (với độ chỉnh cuối), cấu trúc mô hình (độ chỉnh sửa nhiều cấp độ), việc chọn dữ liệu và kết hợp đa mẫu. Những phát hiện của chúng tôi cho thấy chất lượng dịch thuật tốt nhất có thể đạt được bằng việc xây dựng một hệ thống đầu tiên dựa trên việc kết hợp dữ liệu ngoài miền và sau đó sửa chữa dữ liệu nội bộ. Kiểu xếp kiểu có tác dụng tốt nhất khi huấn luyện bắt đầu với dữ liệu ngoài miền xa nhất và kiểu mẫu được đặt dần cẩn thận với miền xa nhất tiếp theo và v.v. Việc chọn dữ liệu không có kết quả tốt nhất, nhưng có thể được coi là một thỏa hiệp thỏa hiệp thỏa đáng giữa thời gian huấn luyện và chất lượng dịch. Một kết hợp với các mô hình cá nhân khác nhau được thực hiện tốt hơn việc chọn dữ liệu. Nó có lợi cho một kịch bản khi không có thời gian chỉnh sửa một mô hình đã được huấn luyện.', 'bg': 'В тази статия изследваме алтернативни начини за обучение на невронна система за машинен превод в многодомейн сценарий. Изследваме конкатенацията на данни (с фина настройка), подреждането на модели (фина настройка на няколко нива), подбора на данни и мултимоделен ансамбъл. Нашите констатации показват, че най-доброто качество на превода може да бъде постигнато чрез изграждане на първоначална система въз основа на конкатениране на налични извън домейна данни и след това фина настройка на данните в домейна. Подреждането на модела работи най-добре, когато обучението започва с най-отдалечените извън домейна данни и моделът е постепенно фино настроен със следващия най-отдалечен домейн и т.н. Изборът на данни не даде най-добри резултати, но може да се счита за достоен компромис между времето за обучение и качеството на превода. Теглен ансамбъл от различни индивидуални модели се представи по-добре от избора на данни. Полезно е в сценарий, когато няма време за фина настройка на вече обучен модел.', 'hr': 'U ovom papiru istražujemo alternativne načine za obuku sustava prevoda neuronskih strojeva u višedomenom scenariju. Istražujemo zaključavanje podataka (s dobrim napravom), model stacking (višeniverska fina naprava), izbor podataka i multimodel ensemble. Naši nalazi pokazuju da se najbolji kvalitet prevoda može postići izgradnjem početnog sustava o usklađivanju dostupnih podataka izvan domena i potom je dobro određivanju podataka u domenu. Model stacking radi najbolje kada trening počinje sa najdaljim podacima izvan domena, a model se povećavaju sa najdaljim domenom i tako dalje. Izbor podataka nije dao najbolje rezultate, već se može smatrati pristojnim kompromisom između treninga i kvalitete prevoda. Težina osiguranja različitih pojedinačnih modela provedena je bolja od selekcije podataka. Korisno je u scenariju kada nema vremena za finaliziranje već obučenog model a.', 'nl': 'In dit artikel onderzoeken we alternatieve manieren om een neuraal machine translation systeem te trainen in een multi-domein scenario. We onderzoeken data aaneenschakeling (met finetuning), model stacking (multi-level finetuning), data selectie en multi-model ensemble. Onze bevindingen tonen aan dat de beste vertaalkwaliteit bereikt kan worden door een initieel systeem te bouwen op een aaneenschakeling van beschikbare out-of-domain data en vervolgens af te stemmen op in-domain data. Model stacking werkt het beste wanneer de training begint met de verst buiten het domein gegevens en het model stapsgewijs wordt verfijnd met het volgende verste domein enzovoort. De selectie van gegevens leverde niet de beste resultaten op, maar kan worden beschouwd als een fatsoenlijk compromis tussen trainingstijd en vertaalkwaliteit. Een gewogen ensemble van verschillende individuele modellen presteerde beter dan dataselectie. Het is voordelig in een scenario waarin er geen tijd is voor finetuning van een reeds getraind model.', 'da': 'I denne artikel undersøger vi alternative måder at træne et neuralt maskinoversættelsessystem på i et multi-domæne scenarie. Vi undersøger datasammenkobling (med finjustering), modelstapning (multi-level finjustering), datavalg og multi-model ensemble. Vores resultater viser, at den bedste oversættelseskvalitet kan opnås ved at bygge et indledende system på en sammenkobling af tilgængelige data uden for domænet og derefter finjustere det på data inden for domænet. Model stabling fungerer bedst, når træningen begynder med de længste data uden for domænet, og modellen finjusteres trinvist med det næste længste domæne osv. Dataudvælgelsen gav ikke de bedste resultater, men kan betragtes som et anstændigt kompromis mellem uddannelsestid og oversættelseskvalitet. Et vægtet ensemble af forskellige individuelle modeller klarede sig bedre end datavalg. Det er gavnligt i et scenario, hvor der ikke er tid til at finjustere en allerede uddannet model.', 'fa': 'در این کاغذ، ما روش\u200cهای جایگزینی برای آموزش یک سیستم ترجمه ماشین عصبی در یک سناریو چندین دومین تحقیق می\u200cکنیم. ما تحقیق مشترک داده\u200cها (با تنظیمات خوب) را می\u200cکنیم، مدل تنظیمات (تنظیمات متعدد سطح) و انتخاب داده\u200cها و مدل\u200cهای متعدد را تحقیق می\u200cکنیم. نتیجه\u200cهای ما نشان می\u200cدهند که بهترین کیفیت ترجمه می\u200cتواند با ساختن یک سیستم اولیه بر روی هماهنگ داده\u200cهای خارج از دومین موجود باشد و سپس آن را در داده\u200cهای دامنه\u200cی دامنه\u200cای خوب تنظیم کنید. زمانی که تمرین با اطلاعات بیرون دومین شروع می\u200cشود، مدل بهترین کار می\u200cکند و مدل با دومین\u200cترین دومین بعدی بسیار خوب تنظیم می\u200cشود. انتخاب داده\u200cها بهترین نتیجه\u200cها را نداده\u200cاند، ولی می\u200cتوانند به عنوان یک توطئه\u200cای مناسب بین زمان آموزش و کیفیت ترجمه\u200cها به نظر بگیرند. یک نمودار وزن از مدل\u200cهای جدید بهتر از انتخاب داده انجام می\u200cدهد. وقتی زمانی برای تنظیم کردن یک مدل آموزش آموزش وجود ندارد، در یک سناریو منافع است.', 'id': 'Dalam kertas ini, kami mengeksplorasi cara alternatif untuk melatih sistem terjemahan mesin saraf dalam skenario multi-domain. Kami menyelidiki data concatenation (dengan penyesuaian baik), model stacking (penyesuaian baik multi-tingkat), data seleksi dan multimodel ensemble. Penemuan kami menunjukkan bahwa kualitas terjemahan terbaik dapat dicapai dengan membangun sistem awal pada konatenasi dari data luar domain yang tersedia dan kemudian memperbaikinya pada data dalam domain. Stacking model berfungsi terbaik ketika latihan dimulai dengan data luar domain yang paling jauh dan model secara incremental disesuaikan dengan domain terjauh berikutnya dan sebagainya. Seleksi data tidak memberikan hasil terbaik, tetapi dapat dianggap sebagai kompromi layak antara waktu latihan dan kualitas terjemahan. Sebuah ensemble berat dari model individu yang berbeda dilakukan lebih baik daripada pemilihan data. Ini berguna dalam skenario ketika tidak ada waktu untuk memperbaiki model yang sudah dilatih.', 'ko': '본고에서 우리는 여러 분야의 장면에서 신경기계 번역 시스템을 훈련하는 대체 방법을 모색했다.우리는 데이터 직렬(미조정), 모델 중첩(다단계 미조정), 데이터 선택과 다중 모델 집적을 연구했다.우리의 연구 결과에 따르면 사용 가능한 역외 데이터의 직렬을 통해 초기 시스템을 구축한 다음에 역내 데이터에 대해 마이크로스피커를 하면 가장 좋은 번역 품질을 얻을 수 있다.훈련이 가장 먼 역외 데이터로부터 시작될 때 모델 중첩 효과가 가장 좋고 모델은 다음 가장 먼 역에 따라 증가량을 미세하게 조정한다.데이터 선택이 가장 좋은 결과를 내놓지는 않았지만 교육 시간과 번역의 질 사이의 합리적인 절충으로 여겨질 수 있다.서로 다른 개체 모델의 가중 조합은 데이터 선택보다 우수하다.이미 훈련된 모형을 미세하게 조정할 시간이 없는 상황에서 유익하다.', 'de': 'In diesem Beitrag untersuchen wir alternative Möglichkeiten, ein neuronales maschinelles Übersetzungssystem in einem Multi-Domain-Szenario zu trainieren. Wir untersuchen Datenverkettung (mit Feinabstimmung), Modellstapelung (mehrstufige Feinabstimmung), Datenauswahl und Multimodellensemble. Unsere Ergebnisse zeigen, dass die beste Übersetzungsqualität erreicht werden kann, indem ein erstes System auf einer Verkettung verfügbarer Out-of-Domain-Daten aufgebaut und anschließend auf In-Domain-Daten abgestimmt wird. Das Modellstapeln funktioniert am besten, wenn das Training mit den am weitesten entfernten Daten beginnt und das Modell schrittweise auf die nächst am weitesten entfernte Domäne abgestimmt wird usw. Die Datenauswahl lieferte nicht die besten Ergebnisse, kann aber als angemessener Kompromiss zwischen Schulungszeit und Übersetzungsqualität betrachtet werden. Ein gewichtetes Ensemble verschiedener Einzelmodelle schneidet besser ab als die Datenauswahl. Es ist vorteilhaft in einem Szenario, in dem keine Zeit für die Feinabstimmung eines bereits trainierten Modells bleibt.', 'sw': 'Katika gazeti hili, tunatafuta njia mbadala ya kufundisha mfumo wa kutafsiri mashine ya asili katika eneo la ndani mbalimbali. Tunafanya uchunguzi wa kukusanyika kwa taarifa (kwa ujumbe mzuri), unyanyasaji wa mifano (mazungumzo mazuri ya kiwango kikubwa), uchaguzi wa data na vifaa vingi. Matokeo yetu yanaonyesha kuwa kiwango bora cha tafsiri kinaweza kupatikana kwa kujenga mfumo wa mwanzo wa pamoja wa taarifa zilizopatikana nje ya ndani na kisha kuvifanya vizuri kwenye taarifa za ndani. Utamaduni wa Model unafanya kazi vizuri pale mafunzo yanapoanza kwa takwimu za nje ya ndani na mtindo huu unaendelea kuwa mzuri zaidi na kwa hiyo. Uchaguzi wa taarifa haukutoa matokeo mazuri, lakini inaweza kuchukuliwa kama ni jambo zuri kati ya muda wa mafunzo na kiwango cha kutafsiri. Mfano wa mifano tofauti uliofanywa vizuri kuliko uchaguzi wa data. Ni faida sana katika tukio hilo ambapo hakuna muda wa kuwatunza vizuri mfano wa tayari ulioendeshwa.', 'af': "In hierdie papier, ons ondersoek alternatiewe maniere om 'n neurale masjien vertaling stelsel in' n multidomein scenario te trein. Ons ondersoek data samelegging (met fyn tuning), model stapeling (multi-vlak fyn tuning), data keuse en multi-model ensemble. Ons gevinde vertoon dat die beste vertaling kwaliteit kan bereik word deur 'n aanvanklike stelsel te bou op' n samelewing van beskikbaar uit-domein data en dan fin-tuning dit op in-domein data. Model stacking werk beste wanneer onderwerking begin met die hoogste van-domein data en die model is inkrementeer fin-tuned met die volgende hoogste domein en so op. Data keuse het nie die beste resultate gegee nie, maar kan wees aangesien as 'n goeie kompromis tussen oefening tyd en vertaling kwaliteit. Name Dit is nuttig in 'n scenario wanneer daar geen tyd is om 'n reeds opgelei model te fin-tuning nie.", 'sq': 'Në këtë letër, ne eksplorojmë mënyra alternative për të trajnuar një sistem përkthimi të makinave nervore në një skenar shumëdomenik. Ne hetojmë përcaktimin e të dhënave (me rregullim të hollë), përcaktimin e modeleve (rregullim të hollë në shumë nivele), zgjedhjen e të dhënave dhe ansamblin e shumëmodeleve. Zbulimet tona tregojnë se cilësia më e mirë e përkthimit mund të arrihet duke ndërtuar një sistem fillestar në një bashkëkuptim të të dhënave të disponueshme jashtë domenit dhe pastaj duke i përshtatur ato në të dhënat në domeni. Mbështetja e modelit funksionon më mirë kur trajnimi fillon me të dhënat më të largëta jashtë domenit dhe modeli është në mënyrë graduale i rregulluar me domenin tjetër më të largët dhe kështu me radhë. Zgjidhja e të dhënave nuk dha rezultatet më të mira, por mund të konsiderohet si një kompromis i denjë midis kohës së stërvitjes dhe cilësisë së përkthimit. Një grup i peshuar i modeleve të ndryshme individuale kryer më mirë se zgjedhja e të dhënave. Është e dobishme në një skenar kur nuk ka kohë për rregullimin e një modeli tashmë të stërvitur.', 'tr': 'Bu kağıt içinde, bir nöral makine terjime sistemini çoklu domenin senaryosunda öğrenmek için başka yollarla keşif ediyoruz. Biz maglumat samçylygyny (dogry döredilen bilen), modelleri takyklaýan, maglumat saýlamagyny we köp modelleri takyklaýan. Biziň tapylyklarymyz en gowy terjime kwaliteti başarmak sistemini domain maglumatynyň birleşmesine ýetip biljek bolandygyny görkez we soňra da domain maglumatynyň üstünde düzeltebilir. Model stacking işleýän zaman eğitim başlaýan iň öňki domenýäsiz maglumatlar bilen iň gowy işleýär we model indiki iň öňki domenýä we şeýle däl. Maglumat saýlawy iň gowy netijede bermedi, ýöne okuw wagty we terjime kwaliteti arasynda adalatly bir wezip bolup biler. Veri saýlawdan gowy edip biljek beýleki nusgalaryň yzarlanýan bir köşe. Bu sahypada eýýäm bilinmiş nusga düzeltmek üçin wagt ýok wagtynda faydaly.', 'am': 'በዚህ ፕሮግራም፣ የናቡር የሆኑት የሞክራዊ ትርጉም ሲስተማርን በብዙ ድምፅ በተለይ መንገድ እንፈልጋለን፡፡ የዳታ ክፍተት (በጥሩ ጉዳይ፣ ሞዴል መቆጣጠር (ብዙነኛ ክፍል), የዳታ ምርጫ እና ብዙዎችን ዓይነት ምሳሌ አነስተዋል፡፡ ፍጥረታችን የተሻለ ትርጉም ጥሩ መሆኑን ማግኘት ነው፡፡ ሞዴል መቆጣጠር ከውጭ ከዶሜን ዳታ ጋር ሲጀምር ይሻላል፤ ሞዴላውም በሚቀጥለው በጣም የበረታች ዶሜን እና እንደዚህ ነው፡፡ የዳታ ምርጫ የተሻለ ፍሬዎችን አያደርግም፥ ነገር ግን በተማሪዎች ጊዜ እና ትርጓሜ ጥሩ መካከለኛ ክፍል እንዲመስል ይችላል። ከዳታ ምርጫ የተሻለ የተለየ የተለየ የልዩ ዓይነቶች ምሳሌ በተመለከት ላይ የተጠቃሚ ሞዴል ለመጠቀም ጊዜ የለውም፡፡', 'hy': 'Այս թղթի մեջ մենք ուսումնասիրում ենք տարբեր միջոցներ, որոնց միջոցով պետք է վարժեցնենք նեյրոնային մեքենայի թարգմանման համակարգը բազմատիրային սցենարիայում: Մենք ուսումնասիրում ենք տվյալների համեմատությունը (նրբագեղ կարգավորումներով), մոդելների համեմատությունը (բազմամամակարդակ կարգավորումներով), տվյալների ընտրությունը և բազմամոդելների համակարգը: Մեր հայտնաբերությունները ցույց են տալիս, որ լավագույն թարգմանման որակը կարող է հասնել սկզբնական համակարգի կառուցման միջոցով, որը կառուցում է հնարավոր ոչ բնագավառ տվյալների համապատասխանատվության վրա և հետո կատարում է այն բնագավառի Մոդելների կառուցվածքը լավագույնն է աշխատում, երբ ուսուցումը սկսվում է ամենահեռու տվյալների հետ, և մոդելը դառնում է դասակարգված հաջորդ ամենահեռու տվյալների հետ և այլն: Տվյալների ընտրությունը լավագույն արդյունքներ չէր տալիս, սակայն կարելի է համարժեք հակամարտություն համարել կրթության ժամանակի և թարգմանման որակի միջև: Տարբեր անհատական մոդելների կշռված համակարգը ավելի լավ արդյունք ուներ, քան տվյալների ընտրությունը: Դա օգտակար է մի սցենարիայում, երբ ժամանակ չկա արդեն պատրաստված մոդելի բարելավման համար:', 'az': 'Bu kağızda, çoxlu domena senaryosunda nöral maşın çevirim sistemini təhsil etmək üçün alternatif yolları keşfetirik. Biz məlumatlar birlikləşdirilməsini (yaxşı tunlar ilə), modelləri birlikləşdirilməsini (çoxlu səviyyədə yaxşı tunlar), məlumatlar seçilməsini və çoxlu modelləri birlikləşdiririk. Bizim tapındıqlarımız ən yaxşı tercümə keyfiyyətini göstərir ki, faydalanmış domena verilənlərin birlikdə ilk sistemi in şa edərək, sonra da domena verilənlərin üstündə təmizlənməsi olar. Model stacking təhsil başladığında ən uzaq domena verilənlərlə başladığında ən yaxşı işlər edir. Model sonraki ən uzaq domena ilə çox yaxşı təhsil edilir. Data seçimi ən yaxşı sonuçlar verilmədi, amma təhsil vaxtı və tercümə keyfiyyəti arasında uyğun bir kompromis olar. Veri seçməsindən daha yaxşı işlədilən fərqli modellərin ağırlı bir ensembli. Bu, əvvəlcə təhsil edilmiş modeli düzəltmək üçün vaxtı yoxdur.', 'bn': 'এই কাগজটিতে আমরা বিকল্প উপায় খুঁজে বের করি একটি মাল্টিডোমেইনের দৃশ্যে নিউরেল মেশিন অনুবাদ সিস্টেম প্রশিক্ষণ করতে। আমরা তথ্য সংযোগের তথ্য অনুসন্ধান করি (ভালো টুনিং সহ), মডেল স্ট্যাকিং (বহুস্তরের ভালো টুনিং), তথ্য নির্বাচন এবং বহুমাত্র মড আমাদের আবিস্কার দেখা যাচ্ছে যে ভালো অনুবাদের মান অর্জন করা যাবে একটি প্রাথমিক সিস্টেম নির্মাণের মাধ্যমে যা পাওয়া যাবে ডোমেইনের তথ্য মোডেল স্ট্যাকিং শুরু করে যখন সবচেয়ে ভালো প্রশিক্ষণ শুরু হয় ডোমেইনের তথ্য দিয়ে এবং পরের সবচেয়ে ক্ষতিগ্রস্ত ডোমেইনের সাথে মডেল বেশি ভাল তথ্য নির্বাচনের সেরা ফলাফল দেয়নি, কিন্তু প্রশিক্ষণের সময় এবং অনুবাদের মানের মধ্যে ভালোভাবে বিবেচনা করা যায়। A weighted ensemble of different individual models performed better than data selection.  এটা একটি দৃশ্যে সুবিধাজনক যখন ইতোমধ্যে প্রশিক্ষিত মডেলের সুন্দর করার সময় নেই।', 'ca': "En aquest article explorem maneres alternatives d'entrenar un sistema de traducció neuronal en un escenari multidominiu. Investiguem la concatenació de dades (amb ajustes fins), la pila de models (ajustes fins a molts nivells), la selecció de dades i un conjunt multi model. Els nostres descobriments demostren que la millor qualitat de traducció es pot aconseguir construint un sistema inicial sobre una concatenació de dades disponibles fora de domini i després ajustar-la a dades en domini. La pila de models funciona millor quan l'entrenament comença amb les dades més llunyanes fora de domini i el model s'ajusta incrementalment al següent domini més lluny, etcètera. La selecció de dades no donava els millors resultats, però es pot considerar un compromís decent entre el temps d'entrenament i la qualitat de traducció. A weighted ensemble of different individual models performed better than data selection.  És beneficiós en un escenari quan no hi ha temps per ajustar un model ja entrenat.", 'bs': 'U ovom papiru istražujemo alternativne načine da obučimo sistem prevoda neuronskih strojeva u multidomeničkom scenariju. Istražujemo potvrdu podataka (sa dobrim tuniranjem), model stacking (višeniverski fin tuning), selekciju podataka i multimodel ensemble. Naši nalazi pokazuju da se najbolja kvaliteta prevoda može postići izgradnjem početnog sistema o usklađivanju dostupnih podataka izvan domena i potom je dobro određivanjem podataka u domenu. Model stacking najbolje funkcioniše kada trening počne sa najdaljim podacima iz domena i model je vrlo dobro sređen sa sljedećim najdaljim domenom i tako dalje. Izbor podataka nije dao najbolje rezultate, već se može smatrati pristojnim kompromisom između treninga i kvalitete prevoda. Težina osiguranja različitih modela provedena je bolje od selekcije podataka. Korisno je u scenariju kada nema vremena za finaliziranje već obučenog model a.', 'et': 'Käesolevas töös uurime alternatiivseid võimalusi neuromasintõlkesüsteemi koolitamiseks mitme valdkonna stsenaariumis. Uurime andmete konkatenatsiooni (peenhäälestusega), mudeli virnastamist (mitmetasandiline peenhäälestus), andmete valikut ja multimudeli ansamblit. Meie tulemused näitavad, et parima tõlkekvaliteedi saavutamiseks on võimalik luua esialgne süsteem olemasolevate domeeniväliste andmete ühendamisel ja seejärel täpsustada seda domeenisiseste andmete põhjal. Mudeli virnastamine toimib kõige paremini, kui treening algab kõige kaugemate domeenivälisete andmetega ja mudelit peenestatakse järk-järgult järgmise kaugema domeeniga jne. Andmete valik ei andnud parimaid tulemusi, kuid neid võib pidada korralikuks kompromissiks koolitusaja ja tõlkekvaliteedi vahel. Erinevate individuaalsete mudelite kaalutud komplekt oli parem kui andmete valik. See on kasulik stsenaariumis, kui juba väljaõpetatud mudeli täpsustamiseks ei ole aega.', 'cs': 'V tomto článku zkoumáme alternativní způsoby tréninku neuronového strojového překladu v multidoménovém scénáři. Zkoumáme řetězení dat (s jemným laděním), stohování modelů (víceúrovňové jemné ladění), výběr dat a vícemodelový soubor. Naše zjištění ukazují, že nejlepší kvality překladu lze dosáhnout vybudováním počátečního systému na řetězení dostupných dat mimo doménu a jeho jemným laděním na doménových datech. Stohování modelů funguje nejlépe, když trénink začíná s nejvzdálenějšími daty mimo doménu a model je postupně jemně laděn s další nejvzdálenější doménou atd. Výběr dat nepřinesl nejlepší výsledky, ale může být považován za slušný kompromis mezi dobou školení a kvalitou překladu. Vážený soubor různých jednotlivých modelů vedl lépe než výběr dat. Je to výhodné ve scénáři, kdy není čas na jemné ladění již vycvičeného modelu.', 'fi': 'Tﾃ､ssﾃ､ artikkelissa selvitﾃ､mme vaihtoehtoisia tapoja kouluttaa neurokonekﾃ､ﾃ､nnﾃｶsjﾃ､rjestelmﾃ､ﾃ､ moniulotteisessa skenaariossa. Tutkimme datan yhdistﾃ､mistﾃ､ (hienosﾃ､ﾃ､timellﾃ､), mallipinoamista (monitasoinen hienosﾃ､ﾃ､tﾃｶ), datan valintaa ja monimallikokoonpanoa. Tuloksemme osoittavat, ettﾃ､ paras kﾃ､ﾃ､nnﾃｶslaatu voidaan saavuttaa rakentamalla alkuperﾃ､inen jﾃ､rjestelmﾃ､ kﾃ､ytettﾃ､vissﾃ､ olevan verkkotunnuksen ulkopuolisen datan yhdistﾃ､misestﾃ､ ja hienosﾃ､ﾃ､tﾃ､mﾃ､llﾃ､ sitﾃ､ verkkotunnuksen sisﾃ､iseen dataan. Mallipinoaminen toimii parhaiten, kun harjoittelu alkaa kauimmasta verkkotunnuksen ulkopuolisesta datasta ja mallia hienosﾃ､ﾃ､tﾃ､ﾃ､ asteittain seuraavalla etﾃ､isimmﾃ､llﾃ､ verkkotunnuksella ja niin edelleen. Tietojen valinta ei tuottanut parhaita tuloksia, mutta sitﾃ､ voidaan pitﾃ､ﾃ､ kohtuullisena kompromissina koulutusajan ja kﾃ､ﾃ､nnﾃｶksen laadun vﾃ､lillﾃ､. Eri yksittﾃ､isten mallien painotettu kokonaisuus suoriutui paremmin kuin datan valinta. Se on hyﾃｶdyllistﾃ､ tilanteessa, jossa ei ole aikaa hienosﾃ､ﾃ､tﾃ､ﾃ､ jo koulutettua mallia.', 'sk': 'V prispevku raziskujemo alternativne načine usposabljanja nevronskega strojnega prevajanja v večdomenskem scenariju. Preučujemo konatenacijo podatkov (s finim tuning), zlaganje modelov (fino tuning na več ravneh), izbiro podatkov in večmodelni ansambel. Naše ugotovitve kažejo, da je mogoče najboljšo kakovost prevajanja doseči z izgradnjo začetnega sistema na podlagi združevanja razpoložljivih zunajdomenskih podatkov in nato z natančnim nastavitvijo na podatke znotraj domena. Najbolje deluje zlaganje modelov, ko se trening začne z najbolj oddaljenimi podatki zunaj domene, model pa je postopoma natančno nastavljen z naslednjo najbolj oddaljeno domeno in tako naprej. Izbira podatkov ni dala najboljših rezultatov, vendar se lahko šteje za dostojen kompromis med časom usposabljanja in kakovostjo prevajanja. Ponderiran ansambel različnih posameznih modelov je bil boljši od izbire podatkov. Koristno je v scenariju, ko ni časa za natančno nastavitev že usposobljenega modela.', 'jv': 'Nang this paper, we istrage Alternate Methods to vlake a Neral device translation System in a multi-domain scenaro. We istrage data concatenation (with Fintuning), model stabking (multi-evel Fining tuning), data select and multi-model ensembedle. We find show that the top translation quality can be accesed by rebuking an first System on a concatenation of available out-of-domain data and after Fine-tuning it on in-domain data. modem staking Works all right when cursing beginning with the FLAGS out-of-domain data and the model is expanmently Fine-tuned with the next FLAGS domain and so on. D Digawe Ora bener yèn iso nggo ngerasakno, kuwi ora ono wektu nggawe model terus-terus.', 'he': 'בעיתון הזה, אנו חוקרים דרכים אלטרנטיות לאמן מערכת תרגום מכונות עצביות בתרחיש רב-דומין. אנו חוקרים את התאמת נתונים (עם התאמה מצוינת), התאמת מודלים (התאמה מצוינת במרבה רמות), בחירת נתונים ואנסמבל מודלים רבים. הממצאים שלנו מראים כי איכות התרגום הטובה ביותר ניתן להשיג על ידי בניית מערכת ראשונה על שימוש של נתונים מחוץ לתחום זמינים ואז להתאים אותו על נתונים בתחום. הערכת מודלים עובדת הכי טוב כשהאימונים מתחילים עם נתונים מחוץ לתחום הרחוקים ביותר והמודל מתאים באופן נוסף עם התחום הרחוק הבא וכן הלאה. בחירת נתונים לא נתנה את התוצאות הטובות ביותר, אלא ניתן לשקול כמסכם הגון בין זמן האימונים לאיכות התרגום. סמל משקל של דוגמנים בודדים שונים שהופך טוב יותר מבחירת נתונים. זה מועיל בתרחיש כאשר אין זמן להתאים מודל כבר מאומן.', 'ha': "Daga wannan takardan, za mu nẽmi hanyõyi masu amfani da tsarin tarjibu na ƙarami cikin wani na'urar-Domen. Tuna ƙidãya samun data (da ke samun tunkuɗe mai kyau), mai motsi (tunkuɗe mai amfani da zane-zane-zane, zaɓen data da shirin misãlai masu yawa. FayiyinMu na nũna cewa, mafiya kyaun fassarar ta tsohon da za'a samar da tsari na farko a kan samun samun data masu iya iya fitarwa daga-Domin kuma ka sami-tunna shi kan data masu cikin-Domin. Halin mai amfani da Model yana aiki mafi alhẽri idan na fara yin shirin da data masu fitarwa daga-Domen na ƙara, kuma a yi amfani da motel guda na bayan haka. Zaɓi danne ba ya bãyar da mafi kyaun fassara ba, amma an iya ƙaddara shi kamar wata shawara mai kyau tsakanin zaman shawara da sifar fassarar. An sami wani ƙunci mai nau'i na wasu misãlai daban-daban da aka aikata mafi alhẽri daga zaɓaɓen data. Ina da amfani cikin wani fasa idan ba wani lokaci ya sami wani misalin wanda aka riga ya tsara.", 'bo': 'ང་ཚོས་ཤོག་བུ་འདིའི་ནང་དུ་ཡིག་ཆའི་ནང་དུ་གླེང་སྒྲུང་ནང་གི་ཐབས་ལམ་གཞན་ཡང་འཚོལ་ཞིབ་ལྟ་བུ་རེད། ང་ཚོས་བརྟག་ཞིབ་བྱས་པའི་ཆ་འཕྲིན་གསལ་བཤད་དང་། མིག་ཆས་བཀག་ཆས་བྱས་ཉེས། Our findings show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. གདམ་ཀ་ཆ་འཕྲིན་ཡིག་ཆ་ནི་ཕལ་འབྲས་མེད་པར། ཡིན་ནའང་དུས་ཚོད་དང་སྐར་ཡིག སྒེར་གྱི་མིག་དཔེ་དབྱིབས་མ་འདྲ་བ་གི་རྒྱ་ཚད་ཀྱི་མཚོན་རྟགས་ཀྱང་ཉེས་བདེན་བཤད་ཀྱི་ཡོད། འདི་ལྟ་བུའི་སྐོར་ལས་ཕན་འབེབས་ཅན་ཆེན་པོ་ཞིག་གིས་གནས་སྟངས་ལྡན་པའི་དུས་ཚོད་མེད་པའི་སྐབས་ལ།'}
{'en': 'Synthetic Data for Neural Machine Translation of Spoken-Dialects', 'ar': 'البيانات التركيبية للترجمة الآلية العصبية لللهجات المنطوقة', 'es': 'Datos sintéticos para la traducción automática neuronal de dialectos hablados', 'fr': 'Données synthétiques pour la traduction automatique neuronale des dialectes parlés', 'pt': 'Dados sintéticos para tradução automática neural de dialetos falados', 'ja': '口語方言の神経機械翻訳のための合成データ', 'zh': '口语方言神经机器翻译合成数', 'hi': 'बोले गए-बोलियों के तंत्रिका मशीन अनुवाद के लिए सिंथेटिक डेटा', 'ru': 'Синтетические данные для нейронного машинного перевода разговорных диалектов', 'ga': "Sonraí Sintéiseacha d'Aistriúchán Meaisín Néarach ar Chanúintí Labhartha", 'hu': 'Szintetikus adatok beszélt dialektusok idegi gépi fordításához', 'it': 'Dati sintetici per la traduzione automatica neurale dei dialetti vocali', 'kk': 'Сызық- диалектердің нейрондық машинаның аудармасының синтетикалық деректері', 'lt': 'Sintetiniai duomenys kalbėtųjų dialektų vertimui nervinėse mašinose', 'ka': 'Name', 'el': 'Συνθετικά δεδομένα για τη νευρωνική μηχανική μετάφραση των ομιλούμενων διαλέκτων', 'mk': 'Name', 'ms': 'Synthetic Data for Neural Machine Translation of Spoken-Dialects', 'mn': 'Спокен-Диалогчдын мэдрэлийн машин хөгжүүлэх синтетик өгөгдлийг', 'mt': 'Dejta Sintetika għat-Traduzzjoni ta’ Makkinarju Newrali ta’ Dijaletti Spoken', 'ml': 'സ്പോക്ക്- ഡയലക്ട്രെക്സിന്റെ നെയുറല്\u200d യന്ത്രത്തിനുള്ള സങ്കീറ്റിക് ഡേറ്റാName', 'pl': 'Dane syntetyczne dla neuronowego tłumaczenia maszynowego dialektów mówionych', 'sr': 'Sintetički podaci za Neuralnu mašinu prevod govornih dijalekata', 'ro': 'Date sintetice pentru traducerea automată neurală a dialectelor vorbite', 'so': 'Turjumidda Spoken-Dialects', 'no': 'Syntetiske data for neuralmaskinsomsetjing av spoken- dialektar', 'si': 'Name', 'ur': 'Name', 'sv': 'Syntetiska data för neural maskinöversättning av taldialekter', 'ta': 'Name', 'uz': 'Name', 'vi': 'Dữ liệu tổng hợp cho máy thần kinh Dịch ngôn ngữ', 'bg': 'Синтетични данни за неврален машинен превод на говорими диалекти', 'nl': 'Synthetische gegevens voor neuronale machinevertaling van gesproken dialecten', 'da': 'Syntetiske data til neural maskinoversættelse af talte dialekter', 'de': 'Synthetische Daten zur neuronalen maschinellen Übersetzung von gesprochenen Dialekten', 'hr': 'Sintetički podaci za neuronski prevod uređaja govornih dijalekata', 'id': 'Data Sintetis untuk Translation Mesin Neural dari Dialek-Bicara', 'ko': '구어 방언 신경 기계 번역의 합성 데이터', 'fa': 'داده\u200cهای سینتیک برای ترجمه ماشین\u200cهای عصبی\u200c', 'tr': 'Sintetik Mazmunlar Çevirmek üçin Sintetik Maglumaty', 'sw': 'Takwimu za Synthetic kwa ajili ya Tafsiri ya Mashiniki ya Njerumani', 'af': 'Sintetiese Data vir Nural Masjien Vertaling van Spoken- Dialeksies', 'hy': 'Սինտետիկ տվյալներ խոսված-դիալեկտների նյարդային մեքենայի թարգմանման համար', 'am': 'Spoken-Dialects', 'sq': 'Të dhënat sintetike për përkthimin e makinës nervore të dialekteve të folura', 'az': 'Spoken-Dialektların Nöral Makina Çeviri üçün Sintetik Verici', 'bn': 'Name', 'bs': 'Sintetički podaci za neuronski prevod uređaja govornih dijalekata', 'ca': 'Les dades sintètiques per a la traducció de diàlectes parlades a màquines neurones', 'cs': 'Syntetická data pro neuronový strojový překlad mluvených dialektů', 'et': 'Sünteetilised andmed kõnelevate dialektide neuraalse masintõlke jaoks', 'fi': 'Synteettiset tiedot puhuttujen dialektien hermojen konekääntämiseen', 'jv': 'Detil Senetik kanggo Perintah Panjenengan Neral', 'ha': 'KCharselect unicode block name', 'sk': 'Sintetični podatki za živčni strojni prevod govorjenih dialektov', 'he': 'נתונים סינתטיים לתרגום מכונות נוירות של דיאלקטים מדברים', 'bo': 'Spoken-Dialects ཡི་ལག་འཁྱེར་གྱི་མིའི་ལག་འཁྱེར་གྱི་ཚིག་སྒྲིག་ཆ་སྐྱེལ་བ'}
{'en': 'In this paper, we introduce a novel approach to generate synthetic data for training Neural Machine Translation systems. The proposed approach supports  language variants  and  dialects  with very limited parallel training data. This is achieved using a seed data to project words from a closely-related resource-rich language to an under-resourced language variant via word embedding representations. The proposed approach is based on localized embedding projection of distributed representations which utilizes monolingual embeddings and approximate nearest neighbors queries to transform parallel data across language variants. Our approach is language independent and can be used to generate data for any variant of the source language such as  slang  or spoken dialect or even for a different language that is related to the source language. We report experimental results on Levantine to English translation using  Neural Machine Translation . We show that the synthetic data can provide significant improvements over a very large scale system by more than 2.8 Bleu points and it can be used to provide a reliable translation system for a spoken dialect which does not have sufficient parallel data.', 'ar': 'في هذا البحث ، نقدم طريقة جديدة لتوليد بيانات تركيبية لتدريب أنظمة الترجمة الآلية العصبية. النهج المقترح يدعم المتغيرات اللغوية واللهجات مع بيانات تدريب متوازية محدودة للغاية. يتم تحقيق ذلك باستخدام بيانات أولية لعرض الكلمات من لغة غنية بالموارد وثيقة الصلة بالموارد إلى متغير لغة منخفض الموارد عبر تمثيلات تضمين الكلمات. يعتمد النهج المقترح على إسقاط التضمين المحلي للتمثيلات الموزعة التي تستخدم الزخارف أحادية اللغة وتقريب استعلامات الجيران الأقرب لتحويل البيانات المتوازية عبر متغيرات اللغة. نهجنا مستقل عن اللغة ويمكن استخدامه لإنشاء بيانات لأي متغير من اللغة المصدر مثل العامية أو اللهجة المنطوقة أو حتى للغة مختلفة مرتبطة باللغة المصدر. نقدم نتائج تجريبية حول الترجمة من اللغة الشامية إلى الإنجليزية باستخدام الترجمة الآلية العصبية. نظهر أن البيانات التركيبية يمكن أن توفر تحسينات كبيرة على نظام واسع النطاق بأكثر من 2.8 نقطة Bleu ويمكن استخدامها لتوفير نظام ترجمة موثوق لهجة منطوقة لا تحتوي على بيانات موازية كافية.', 'fr': "Dans cet article, nous présentons une nouvelle approche pour générer des données synthétiques pour l'entraînement des systèmes de traduction automatique neuronale. L'approche proposée prend en charge les variantes linguistiques et les dialectes avec des données d'apprentissage parallèles très limitées. Ceci est réalisé en utilisant des données de départ pour projeter des mots d'une langue étroitement liée riche en ressources vers une variante de langue sous-financée via des représentations d'intégration de mots. L'approche proposée est basée sur une projection d'intégration localisée de représentations distribuées qui utilise des intégrations monolingues et des requêtes approximatives des voisins les plus proches pour transformer des données parallèles entre les variantes linguistiques. Notre approche est indépendante de la langue et peut être utilisée pour générer des données pour n'importe quelle variante de la langue source, telle que l'argot ou le dialecte parlé, ou même pour une autre langue liée à la langue source. Nous présentons des résultats expérimentaux sur la traduction du levantin vers l'anglais utilisant la traduction automatique neuronale. Nous montrons que les données synthétiques peuvent apporter des améliorations significatives par rapport à un système à très grande échelle de plus de 2,8 points Bleu et qu'elles peuvent être utilisées pour fournir un système de traduction fiable pour un dialecte parlé qui ne dispose pas de suffisamment de données parallèles.", 'pt': 'Neste artigo, apresentamos uma nova abordagem para gerar dados sintéticos para treinamento de sistemas de tradução automática neural. A abordagem proposta suporta variantes de linguagem e dialetos com dados de treinamento paralelo muito limitados. Isso é alcançado usando dados de semente para projetar palavras de um idioma rico em recursos intimamente relacionado para uma variante de idioma com poucos recursos por meio de representações de incorporação de palavras. A abordagem proposta é baseada na projeção de embedding localizada de representações distribuídas que utiliza embeddings monolíngues e consultas aproximadas de vizinhos mais próximos para transformar dados paralelos entre variantes de idioma. Nossa abordagem é independente do idioma e pode ser usada para gerar dados para qualquer variante do idioma de origem, como gírias ou dialeto falado, ou mesmo para um idioma diferente relacionado ao idioma de origem. Relatamos resultados experimentais na tradução levantino para inglês usando a tradução automática neural. Mostramos que os dados sintéticos podem fornecer melhorias significativas em um sistema de escala muito grande em mais de 2,8 pontos Bleu e podem ser usados para fornecer um sistema de tradução confiável para um dialeto falado que não possui dados paralelos suficientes.', 'es': 'En este artículo, presentamos un enfoque novedoso para generar datos sintéticos para el entrenamiento de sistemas de traducción automática neuronal. El enfoque propuesto admite variantes lingüísticas y dialectos con datos de entrenamiento paralelos muy limitados. Esto se logra mediante el uso de datos semilla para proyectar palabras de un lenguaje rico en recursos estrechamente relacionado a una variante de idioma con pocos recursos a través de representaciones de incrustación de palabras. El enfoque propuesto se basa en la proyección de incrustación localizada de representaciones distribuidas que utiliza incrustaciones monolingües y consultas de vecinos más cercanos aproximados para transformar datos paralelos en las variantes del idioma. Nuestro enfoque es independiente del idioma y se puede utilizar para generar datos para cualquier variante del idioma de origen, como jerga o dialecto hablado, o incluso para un idioma diferente que esté relacionado con el idioma de origen. Reportamos los resultados experimentales de la traducción del levantino al inglés mediante la traducción automática neuronal. Demostramos que los datos sintéticos pueden proporcionar mejoras significativas con respecto a un sistema a gran escala en más de 2.8 puntos azules y pueden usarse para proporcionar un sistema de traducción confiable para un dialecto hablado que no tiene suficientes datos paralelos.', 'ja': '本稿では、神経機械翻訳システムをトレーニングするための合成データを生成するための新規アプローチを紹介する。 提案されたアプローチは、非常に限られた並列トレーニングデータで言語変種と方言をサポートします。 これは、シードデータを使用して、密接に関連するリソース豊富な言語から、単語埋め込み表現を介してリソース不足の言語バリアントに単語を投影することによって達成される。 提案されたアプローチは、単一言語の埋め込みと近似的な近隣のクエリを利用して、言語変数間で並列データを変換する、分散表現の局所的な埋め込み投影に基づいている。 当社のアプローチは言語に依存せず、スラングや口語方言などのソース言語のあらゆるバリアント、またはソース言語に関連する別の言語のデータを生成するために使用することができます。 私たちは、ニューラル・マシン・トランスレーションを使用して、レバンチンの実験結果を英訳に報告します。 合成データは、非常に大規模なシステムに対して2.8 BLEUポイント以上の大幅な改善を提供することができ、十分な並列データを持たない口語方言の信頼できる翻訳システムを提供するために使用することができることを示しています。', 'hi': 'इस पेपर में, हम न्यूरल मशीन ट्रांसलेशन सिस्टम के प्रशिक्षण के लिए सिंथेटिक डेटा उत्पन्न करने के लिए एक उपन्यास दृष्टिकोण पेश करते हैं। प्रस्तावित दृष्टिकोण बहुत सीमित समानांतर प्रशिक्षण डेटा के साथ भाषा वेरिएंट और बोलियों का समर्थन करता है। यह एक बीज डेटा का उपयोग करके प्राप्त किया जाता है ताकि शब्द एम्बेडिंग अभ्यावेदन के माध्यम से एक निकट-संबंधित संसाधन-समृद्ध भाषा से शब्दों को एक अंडर-रिसोर्स्ड भाषा संस्करण में प्रोजेक्ट किया जा सके। प्रस्तावित दृष्टिकोण वितरित अभ्यावेदन के स्थानीयकृत एम्बेडिंग प्रक्षेपण पर आधारित है जो मोनोलिंगुअल एम्बेडिंग और अनुमानित निकटतम पड़ोसियों के प्रश्नों का उपयोग भाषा के रूपों में समानांतर डेटा को बदलने के लिए करता है। हमारा दृष्टिकोण भाषा स्वतंत्र है और इसका उपयोग स्रोत भाषा के किसी भी संस्करण के लिए डेटा उत्पन्न करने के लिए किया जा सकता है जैसे कि स्लैंग या बोली बोली या यहां तक कि एक अलग भाषा के लिए भी जो स्रोत भाषा से संबंधित है। हम न्यूरल मशीन अनुवाद का उपयोग करके अंग्रेजी अनुवाद के लिए Levantine पर प्रयोगात्मक परिणामों की रिपोर्ट करते हैं। हम दिखाते हैं कि सिंथेटिक डेटा 2.8 Bleu बिंदुओं से अधिक एक बहुत बड़े पैमाने पर प्रणाली पर महत्वपूर्ण सुधार प्रदान कर सकता है और इसका उपयोग बोली बोली के लिए एक विश्वसनीय अनुवाद प्रणाली प्रदान करने के लिए किया जा सकता है जिसमें पर्याप्त समानांतर डेटा नहीं है।', 'zh': '本文,我们说了一种生成合成数据的新法,所以训练神经机器翻译系统。 所建法赞言变体及方言,并行训练数甚有限。 此用种数因词嵌单词以密相关多言投影于不足之言变体也。 其法基于分布式局嵌投影,因单语嵌近邻询以转跨语变体者并行数据。 吾法无关言语,可施于为源语变体(如俚语口语方言)生数,乃至他语生数。 臣等报用神经机器翻译将黎凡特语翻译成英语实验实。 合成数可以过2.8 Bleu,而可以并行数语方言可恃者译之统也。', 'ru': 'В данной работе мы вводим новый подход к генерации синтетических данных для обучения систем нейронного машинного перевода. Предлагаемый подход поддерживает языковые варианты и диалекты с очень ограниченными параллельными обучающими данными. Это достигается с помощью исходных данных для проецирования слов из тесно связанного языка, богатого ресурсами, в вариант языка, не обеспеченный достаточными ресурсами, через представления встраивания слов. Предлагаемый подход основан на локализованной проекции вложений распределенных представлений, которая использует одноязычные вложения и приблизительные запросы ближайших соседей для преобразования параллельных данных по языковым вариантам. Наш подход не зависит от языка и может быть использован для генерации данных для любого варианта исходного языка, такого как сленг или разговорный диалект, или даже для другого языка, который связан с исходным языком. Мы сообщаем о результатах эксперимента по переводу с левантийского на английский с использованием нейронного машинного перевода. Мы показываем, что синтетические данные могут обеспечить значительные улучшения по сравнению с очень крупномасштабной системой более чем на 2,8 балла Bleu, и они могут быть использованы для обеспечения надежной системы перевода для разговорного диалекта, который не имеет достаточных параллельных данных.', 'ga': 'Sa pháipéar seo, tugaimid isteach cur chuige nua chun sonraí sintéiseacha a ghiniúint chun córais Néar-Aistriúcháin Meaisín a oiliúint. Tacaíonn an cur chuige atá beartaithe le leaganacha teanga agus canúintí le sonraí oiliúna comhthreomhara an-teoranta. Baintear é seo amach trí úsáid a bhaint as sonraí síl chun focail a theilgean ó theanga shaibhir acmhainní atá dlúthghaolmhar go malairt teanga gann-acmhainní trí léirithe neadaithe focal. Tá an cur chuige atá beartaithe bunaithe ar theilgean leabaithe logánta ar uiríll dáilte a úsáideann leabaithe aonteangacha agus fiosruithe neasa ó na comharsana chun sonraí comhthreomhara a athrú thar leaganacha teanga. Tá an cur chuige atá againn neamhspleách ar theanga agus is féidir é a úsáid chun sonraí a ghiniúint d’aon leagan den teanga fhoinseach ar nós slang nó canúint labhartha nó fiú do theanga eile a bhaineann leis an teanga fhoinseach. Tuairiscímid torthaí turgnamhacha ar aistriúchán Levantine go Béarla ag baint úsáide as Neural Machine Translation. Léirímid gur féidir leis na sonraí sintéiseacha feabhsuithe suntasacha a sholáthar thar chóras an-mhór de níos mó ná 2.8 pointe Bleu agus is féidir é a úsáid chun córas iontaofa aistriúcháin a sholáthar do chanúint labhartha nach bhfuil go leor sonraí comhthreomhara ann.', 'ka': 'ამ დომენტში ჩვენ შევცვალოთ პრომენტიური პროგრამა სინტეტიკური მონაცემების შექმნა ნეიროლური მაქინის გარგუნების სისტემისთვის. პროგრამის შესაძლებელი პროგრამა ენის გარიანტების და დიალექტის მნიშვნელოვანი პარალელური განათლების მონაცემებით. ეს იქნება პროექტის სიტყვების გამოყენებაზე, რომელიც მხოლოდ დაკავშირებულია რესურსოსოსოსოსოსოსოსოსოსოსოსოსოსოსოსოსოსოსოსოსოსოს პროგრამის მიზეზი ლოკალურად გადაწყვეტილი პროგრამის დაბაზია, რომელიც მონოლენგიური გადაწყვეტილებების გამოყენება და მხოლოდ მხოლოდ საზოგადოებების კითხვების გამოყენება, რომ პოლა ჩვენი წარმოდგენა ენის განსაზღვრებულია და შეიძლება გამოყენება მონაცემებისთვის ყველა გარიანტირებისთვის, როგორც სლანდ ან საუბრალო დიალექტი ან სხვა ენის განსაზღვრებულია ჩვენ ექსპერიმენტის წარმოდგენება Levantine-ს ანგლისური გაგრძელებას გამოყენებთ ნეირალური მაქინის გაგრძელებას. ჩვენ ჩვენ გამოჩვენებთ, რომ სინტეტიკური მონაცემები შეუძლიათ ძალიან დიდი სისტემის შესაძლებლობად უფრო დიდი სისტემის შესაძლებლობა 2.8 წერტილებით და ეს შეიძლია გამოყენება, რომ საუკეთესო დიალექ', 'hu': 'Ebben a tanulmányban új megközelítést mutatunk be a Neural Machine Translation rendszerek képzéséhez szükséges szintetikus adatok generálására. A javasolt megközelítés nagyon korlátozott párhuzamos képzési adatokkal támogatja a nyelvi variánsokat és dialektusokat. Ez egy vetőadat segítségével érhető el, hogy a szavakat egy szorosan kapcsolódó erőforrásokban gazdag nyelvből egy alacsony erőforrásokkal rendelkező nyelvváltozatba vetítse a szóbeágyazási reprezentációk segítségével. A javasolt megközelítés az elosztott reprezentációk lokalizált beágyazási vetítésén alapul, amely egynyelvű beágyazásokat és a legközelebbi szomszédok lekérdezéseit használja a párhuzamos adatok nyelvi változatok közötti átalakítására. Megközelítésünk nyelvfüggetlen, és használható a forrásnyelv bármely változatára, például a szlengre vagy a beszélt dialektusra, vagy akár egy másik, a forrásnyelvvel kapcsolatos nyelvre vonatkozó adatok generálására. Kísérleti eredményeket jelentünk Levantine-ről angolra Neural Machine Translation segítségével. Megmutatjuk, hogy a szintetikus adatok több mint 2,8 Bleu ponttal jelentős fejlesztést tudnak nyújtani egy nagyon nagyszabású rendszerhez képest, és felhasználhatók egy olyan beszélt dialektus megbízható fordítási rendszerének biztosítására, amely nem rendelkezik elegendő párhuzamos adattal.', 'el': 'Σε αυτή την εργασία, εισάγουμε μια νέα προσέγγιση για την παραγωγή συνθετικών δεδομένων για την εκπαίδευση συστημάτων Νευρικής Μηχανικής Μετάφρασης. Η προτεινόμενη προσέγγιση υποστηρίζει γλωσσικές παραλλαγές και διαλέκτους με πολύ περιορισμένα δεδομένα παράλληλης κατάρτισης. Αυτό επιτυγχάνεται χρησιμοποιώντας δεδομένα για την προβολή λέξεων από μια γλώσσα πλούσια σε πόρους σε μια παραλλαγή γλώσσας που δεν διαθέτει πόρους μέσω αναπαραστάσεων ενσωμάτωσης λέξεων. Η προτεινόμενη προσέγγιση βασίζεται στην τοπική προβολή ενσωμάτωσης κατανεμημένων αναπαραστάσεων, η οποία χρησιμοποιεί μονογλωσσικές ενσωμάτωση και κατά προσέγγιση ερωτήματα κοντινών γειτόνων για τη μετατροπή παράλληλων δεδομένων σε διάφορες γλωσσικές παραλλαγές. Η προσέγγισή μας είναι ανεξάρτητη από τη γλώσσα και μπορεί να χρησιμοποιηθεί για την παραγωγή δεδομένων για οποιαδήποτε παραλλαγή της γλώσσας προέλευσης όπως αργκό ή προφορική διάλεκτο ή ακόμη και για μια διαφορετική γλώσσα που σχετίζεται με τη γλώσσα προέλευσης. Αναφέρουμε πειραματικά αποτελέσματα σχετικά με τη μετάφραση Λεβαντίνης στα Αγγλικά χρησιμοποιώντας Νευρική Μηχανική Μετάφραση. Δείχνουμε ότι τα συνθετικά δεδομένα μπορούν να προσφέρουν σημαντικές βελτιώσεις σε σχέση με ένα σύστημα μεγάλης κλίμακας με περισσότερα από 2.8 σημεία και μπορούν να χρησιμοποιηθούν για να παρέχουν ένα αξιόπιστο μεταφραστικό σύστημα για μια προφορική διάλεκτο που δεν διαθέτει επαρκή παράλληλα δεδομένα.', 'it': "In questo articolo, introduciamo un nuovo approccio per generare dati sintetici per la formazione di sistemi di traduzione automatica neurale. L'approccio proposto supporta varianti linguistiche e dialetti con dati di formazione parallela molto limitati. Ciò è ottenuto utilizzando un dato di seed per proiettare parole da un linguaggio ricco di risorse strettamente correlato a una variante linguistica con risorse insufficienti tramite rappresentazioni di incorporazione di parole. L'approccio proposto si basa sulla proiezione di embedding localizzata di rappresentazioni distribuite che utilizza embedding monolingue e query approssimative di vicini vicini per trasformare i dati paralleli tra le varianti linguistiche. Il nostro approccio è indipendente dalla lingua e può essere utilizzato per generare dati per qualsiasi variante della lingua di origine come lo slang o il dialetto parlato o anche per una lingua diversa che è correlata alla lingua di origine. Riportiamo risultati sperimentali sulla traduzione da levantino a italiano utilizzando Neural Machine Translation. Mostriamo che i dati sintetici possono fornire miglioramenti significativi rispetto a un sistema su larga scala con più di 2,8 punti Bleu e possono essere utilizzati per fornire un sistema di traduzione affidabile per un dialetto parlato che non dispone di dati paralleli sufficienti.", 'kk': 'Бұл қағазында, нейрондық машинаны аудару жүйелерінің синтетикалық деректерін құру үшін романдық тәртібін келтіреміз. Келтірілген тәсілі тілдердің варианттарын және диалекттарын өте шектелген параллель оқыту деректері қолдайды. Бұл өзгертілген ресурс- баяны тілден тілдерді сөздерді ендіру үшін баяндау деректерін қолдану керек. Келтірілген арқылы, бірнеше тілді ендіру және ең жақын соңғы сұрақтарды тіл варианттарына параллель деректерді аудару үшін жергілікті ендіру проектіне негізделген. Біздің тәсілдігіміз - тіл тәуелсіз және көзінің тілінің кез келген вариантын құру үшін қолданылады, мысалы, сланс не сөйленген диалекттік немесе көзінің тіліне қатынаған басқа тіл ү Біз Левантин туралы тәжірибелі нәтижелерді нейрал машинаны аударып ағылшын тіліне аударып береміз. Біз синтетикалық деректері 2.8 Bleu нүктелерінен артық үлкен масштабтағы жүйесінің үлкен жақсартуларын көрсетеді. Ол сөйлейтін диалект үшін сенімді аудару жүйесін қолдануға болады. Бұл параллель деректері же', 'lt': 'In this paper, we introduce a novel approach to generate synthetic data for training Neural Machine Translation systems.  Siūlomas metodas remia kalbų variantus ir dialektus su labai ribotais lygiagrečių mokymo duomenimis. Tai pasiekiama naudojant sėklos duomenis, kad būtų galima projektuoti žodžius iš glaudžiai susijusios kalbos, kuria daug išteklių, į nepakankamai išteklių turintį kalbos variant ą per žodžių įterpimo paveikslus. Siūlomas metodas grindžiamas vietine pasiskirstytų atstovybių įterpimo projekcija, kuria naudojami vienakalbiniai įterpimai ir apytiksliai artimiausių kaimynų prašymai, kad lygiagrečiai duomenys būtų keičiami įvairiais kalbų variantais. Mūsų požiūris yra nepriklausomas nuo kalbos ir gali būti naudojamas kuriant duomenis apie bet kokį šaltinio kalbos variant ą, pvz., slang arba kalbėtą dialektą, arba net apie kitą kalbą, susijusią su šaltinio kalba. Mes pranešame eksperimentinius Levantine rezultatus vertimui anglų kalba naudojant neurologinį mašinų vertimą. Mes parodome, kad sintetiniai duomenys gali reikšmingai pagerinti labai didelės apimties sistemą daugiau kaip 2,8 Bleu taško ir gali būti naudojami patikimai vertimo sistema kalbamajam dialektui, kuriame nėra pakankamai lygiagrečių duomenų.', 'mk': 'Во овој весник, воведуваме нов пристап за генерирање синтетички податоци за обука на системите за преведување на невралните машини. The proposed approach supports language variants and dialects with very limited parallel training data.  This is achieved using a seed data to project words from a closely-related resource-rich language to an under-resourced language variant via word embedding representations.  Предложениот пристап се базира на локализирана проекција на вградување на дистрибуирани претставувања, која користи монојазични вградувања и приближно прашања за најблиските соседи за трансформирање паралелни податоци низ јазичките варијанти. Нашиот пристап е независен од јазикот и може да се користи за генерирање на податоци за било кој варијант на изворниот јазик како што е сланг или говорен дијалект или дури и за различен јазик кој е поврзан со изворниот јазик. Ние известуваме експериментални резултати на Левантин на англиски превод користејќи неврална машина превод. Ние покажуваме дека синтетичките податоци можат да обезбедат значителни подобрувања преку многу голем систем за повеќе од 2,8 Блу поени и дека можат да се користат за обезбедување доверлив преводен систем за говорен дијалект кој нема доволни паралелни податоци.', 'ml': 'ഈ പത്രത്തില്\u200d, നെയുറല്\u200d മെഷീന്\u200d പരിഭാഷ സിസ്റ്റം പരിശീലിക്കുന്നതിനായി സിന്റെറ്റിക്ക് ഡേറ്റായുണ്ടാക്കാന്\u200d  പ്രൊദ്ദേശിക്കപ്പെട്ട നിര്\u200dദ്ദേശിക്കപ്പെട്ട ഭാഷ മാറ്റങ്ങളെ പിന്തുണയ്ക്കുന്നു, വളരെ പരാലിമാല വാക്കിന്റെ പ്രതിനിധികള്\u200d മൂലം അടുത്ത് ബന്ധപ്പെട്ട വിഭവഭാഷയില്\u200d നിന്നും സമ്പത്തുള്ള വാക്കുകളില്\u200d നിന്നും വിതൃത്തിയുടെ വ പ്രൊദ്ദേശിക്കപ്പെട്ട പ്രൊജക്ഷന്\u200d സ്ഥാനമായി വിതരണം ചെയ്യുന്ന പ്രൊജക്ടിങ്ങിന്\u200dറെ അടിസ്ഥാനത്താണ്. അത് മോണോളില്\u200dഭാഷയുടെ അയല്\u200dക്കാരുടെ വ നമ്മുടെ സമ്പ്രദായം ഭാഷ സ്വാതന്ത്ര്യതയാണ്. സ്രോതഭാഷയുടെ സ്രോതഭാഷയുടെ വ്യത്യാസങ്ങള്\u200dക്കും വേര്\u200dതിരിക്കാനും ഉപയോഗിക്കാനും സാധി നെയുറല്\u200d മെഷീന്\u200d പരിഭാഷപ്രകാരം ഉപയോഗിച്ച് ലെവാന്\u200dറിനിലേക്ക് ഇംഗ്ലീഷിലേക്ക് പരീക്ഷണ ഫലങ്ങള്\u200d നാം  2.8 ബ്ല്യൂ പോയിന്\u200dറുകള്\u200dക്ക് കൂടുതല്\u200d വലിയ സ്കേല്\u200d സിസ്റ്റത്തിന് മുന്\u200dഗണന മെച്ചപ്പെടുത്താന്\u200d സാധിക്കുന്ന സിന്\u200dറെതിക വിവരങ്ങള്\u200d കാണിക്കുന്നു.', 'ms': 'Dalam kertas ini, kami memperkenalkan pendekatan baru untuk menghasilkan data sintetik untuk melatih sistem Terjemahan Mesin Neural. pendekatan yang direncanakan menyokong varian bahasa dan dialekt dengan data latihan selari yang sangat terbatas. Ini dicapai dengan menggunakan data benih untuk projek perkataan dari bahasa kaya sumber yang berkaitan dekat ke varian bahasa yang berkaitan bawah sumber melalui perwakilan penyampilan perkataan. Pendekatan yang direncanakan berdasarkan projeksi penyembedding setempat perwakilan yang disebarkan yang menggunakan penyembedding monobahasa dan kira-kira pertanyaan tetangga terdekat untuk mengubah data selari melalui varian bahasa. pendekatan kita adalah bahasa yang bebas dan boleh digunakan untuk menghasilkan data untuk mana-mana varian bahasa sumber seperti slang atau dialekt bercakap atau bahkan untuk bahasa yang berbeza yang berkaitan dengan bahasa sumber. Kami laporkan hasil percubaan Levantine ke terjemahan Inggeris menggunakan Perjemahan Mesin Neural. Kami menunjukkan bahawa data sintetik boleh menyediakan peningkatan yang signifikan dalam sistem skala yang sangat besar dengan lebih dari 2.8 titik Bleu dan ia boleh digunakan untuk menyediakan sistem terjemahan yang boleh dipercayai untuk dialekt bercakap yang tidak mempunyai data selari yang mencukupi.', 'no': 'I denne papiret introduserer vi ein roman tilnærming for å laga syntetiske data for å trenga Neuralmaskinsomsetjingssystemer. Dette foreslått tilnærming støttar språk- variantar og dialektar med veldig begrenset parallelle opplæringsdata. Dette er oppnådd ved å bruka ei frødata til prosjektet ord frå ein nær relatert ressursryk språk til ein underressursert språk- variant via ord innebygde representasjonar. Foreslått tilnærming er basert på lokalisert innbygging av distribuerte representasjonar som brukar monospråk innbygging og omtrent næraste naboespørjingar for å transformera parallelle data over språk variasjonar. Tilnærming vårt er språk uavhengig og kan brukast for å laga data for kvar variant av kjeldespråket som slang eller taledialekt eller til og med ein annan språk som er relatert til kjeldespråket. Vi rapporterer eksperimentelt resultat på Levantine til engelsk omsetjing med Neural Machine Translation. Vi viser at syntetiske data kan gje signifikante forbedringar over eit veldig stor skalasjonssystem med fleire enn 2.8 Bleu-punkt, og det kan brukast til å gjera eit betre omsetjingssystem for eit talet dialekt som ikkje har nok parallelle data.', 'mt': 'In this paper, we introduce a novel approach to generate synthetic data for training Neural Machine Translation systems.  L-approċċ propost jappoġġja varjanti lingwistiċi u dijaletti b’dejta parallela limitata ħafna ta’ taħriġ. Dan jinkiseb bl-użu ta’ dejta taż-żerriegħa biex jiġu proġettati kliem minn lingwa b’ħafna riżorsi relatata mill-qrib għal varjant lingwistiku b’inqas riżorsi permezz ta’ rappreżentazzjonijiet ta’ inkorporazzjoni tal-kliem. L-approċċ propost huwa bbażat fuq projezzjoni ta’ inkorporazzjoni lokalizzata ta’ rappreżentazzjonijiet distribwiti li tuża inkorporazzjonijiet monolingwi u approssimazzjoni ta’ mistoqsijiet tal-eqreb ġirien biex tittrasforma dejta parallela bejn varjanti lingwistiċi. L-approċċ tagħna huwa indipendenti mil-lingwa u jista’ jintuża biex jiġġenera dejta għal kwalunkwe varjant tal-lingwa tas-sors bħal slang jew dijalekt mitkellem jew anke għal lingwa differenti li hija relatata mal-lingwa tas-sors. We report experimental results on Levantine to English translation using Neural Machine Translation.  Aħna nuru li d-dejta sintetika tista’ tipprovdi titjib sinifikanti fuq sistema fuq skala kbira ħafna b’aktar minn 2.8 punti Bleu u tista’ tintuża biex tipprovdi sistema ta’ traduzzjoni affidabbli għal dijalekt mitkellem li ma jkollhiex biżżejjed dejta parallela.', 'mn': 'Энэ цаасанд бид мэдрэлийн машин орчуулах системийн сургалтын синтетик өгөгдлийг бүтээх шинэ арга зам бий болгодог. Өөрчлөгдсөн арга нь хэлний хувилбаруудыг, диалектуудыг маш хязгаарлагдсан параллел суралцах өгөгдлийг дэмжиж байна. Энэ нь ойролцоогоор холбогдсон ресурс баян хэлээс багасгасан хэл дээр багасгасан хэл төлөвлөгөө ашиглаж байна. Өөрчлөгдсөн арга нь хэлний нэг хэлний нэвтрүүлэлтийг ашиглаж, ойролцоогоор ойролцоогоор нь хэлний хувьд параллел өгөгдлийг шилжүүлэх зорилготой байгууллагуудын төсөл дээр суурилсан юм. Бидний ойлголт бол хэл хамааралтай болон эх үүсвэрийн хэл, сланг, ярианы диалект, эх үүсвэртэй холбоотой өөр хэл дээр ч хэрэглэгддэг. Бид Левантин-ын туршилтын үр дүнг мэдрэлийн машины хөгжүүлэлтийг ашиглан Англи хэлний хөгжүүлэлт руу илтгэдэг. Бид синтетик өгөгдлийн мэдээллийг 2.8 Bleu цэгээс илүү том хэмжээний системээс илүү сайжруулж чадна гэдгийг харуулж чадна. Энэ нь хэлсэн диалектикийн хувьд хангалттай параллел өгөгдлийн тулд итгэлтэй орчуулах системийг хангах боломжгүй.', 'ro': 'În această lucrare, introducem o abordare nouă pentru generarea de date sintetice pentru instruirea sistemelor Neural Machine Translation. Abordarea propusă sprijină variantele lingvistice și dialectele cu date de formare paralelă foarte limitate. Acest lucru se realizează folosind date de semințe pentru a proiecta cuvinte dintr-un limbaj strâns legat de resurse bogat într-o variantă de limbaj insuficient resurse prin reprezentări de încorporare a cuvintelor. Abordarea propusă se bazează pe proiecția localizată de încorporare a reprezentărilor distribuite, care utilizează încorporări monolingve și interogări aproximative de vecini pentru a transforma date paralele între variante de limbă. Abordarea noastră este independentă de limbă și poate fi folosită pentru a genera date pentru orice variantă a limbii sursă, cum ar fi slangul sau dialectul vorbit sau chiar pentru o altă limbă care este legată de limba sursă. Raportăm rezultatele experimentale ale traducerii Levantine în limba engleză folosind Neural Machine Translation. Aratăm că datele sintetice pot oferi îmbunătățiri semnificative față de un sistem la scară foarte mare cu mai mult de 2,8 puncte Bleu și pot fi folosite pentru a oferi un sistem de traducere fiabil pentru un dialect vorbit care nu are suficiente date paralele.', 'pl': 'W niniejszym artykule przedstawiamy nowatorskie podejście do generowania syntetycznych danych dla szkolenia systemów neuronowego tłumaczenia maszynowego. Proponowane podejście wspiera warianty językowe i dialekty z bardzo ograniczonymi danymi równoległego treningu. Osiąga się to przy użyciu danych nasiennych do projektowania słów z blisko powiązanego języka bogatego w zasoby do niewyposażonego wariantu językowego za pomocą reprezentacji osadzania słów. Proponowane podejście opiera się na lokalizowanej projekcji osadzenia reprezentacji rozproszonych, która wykorzystuje jednojęzyczne osadzenia i przybliżone zapytania najbliższych sąsiadów do transformacji danych równoległych w wariantach językowych. Nasze podejście jest niezależne od języka i może być wykorzystane do generowania danych dla dowolnego wariantu języka źródłowego, takiego jak slang lub dialekt mówiony, a nawet dla innego języka, który jest związany z językiem źródłowym. Raportujemy wyniki eksperymentalne dotyczące tłumaczenia lewantyńskiego na angielski przy użyciu neuronowego tłumaczenia maszynowego. Pokazujemy, że dane syntetyczne mogą zapewnić znaczną poprawę w stosunku do bardzo dużej skali systemu o ponad 2.8 punkty Bleu i mogą być wykorzystane do zapewnienia wiarygodnego systemu tłumaczenia dialektu mówionego, który nie posiada wystarczającej ilości danych równoległych.', 'so': "Qoraalkan waxaynu ku soo bandhignaa habab warqad ah si aan u sameyno macluumaad la xiriira tababarida nidaamka tarjumaadda ee Neural machine. Dhaqdhaqaaqa la soo jeeday wuxuu kaalmeeyaa bedelayaasha luuqada iyo ku qoran macluumaad waxbarasho si aad u xadan. This is achieved using a seed data to project words from a closely-related resource-rich language to an under-resourced language variant via word embedding representations.  Dhaqdhaqaaqa la soo jeeday waxay ku saleysan tahay qorshaha meelaha lagu kala soocay oo ay isticmaalaan meelaha lagu soocay afka noocyada ah iyo su'aalaha deriska u dhow si uu u beddelo macluumaad lambarka ah oo kala duwan luuqadaha. Dhaqdhaqaalkayagu waa luqad xor ah, waxaana loo isticmaali karaa macluumaad u sameyn karo luuqadaha asalka ah oo kala duduwan, tusaale ahaan barashada ama luuqad kale oo la xiriira luqada asalka ah. Waxaan wargelinaynaa arimaha imtixaanka ah ee Levantine ku qoran turjumista Ingiriiska ee isticmaalka tarjumaadda maskinada ee Neural. Waxaynu muujinnaa in macluumaadka la xiriira waxay ka heli karaan hagaajin aad u weyn nidaamka aad u weyn oo ka badan 2.8 Bleu, waxaana loo isticmaali karaa in lagu siiyo nidaam turjuman oo aamin ah oo aan lahaynin macluumaad isku mid ah.", 'sr': 'U ovom papiru predstavljamo novi pristup stvaranju sintetičkih podataka za obuku neuronskih sustava prevoda mašina. Predloženi pristup podržava jezičke varijante i dijalekte sa vrlo ograničenim paralelnim podacima obuke. To je postignuto korištenjem podataka semena za projektiranje reèi iz bliskog jezika sa bogatim resursima na varijant jezika koji je manje resursa putem reèi ukljuèujuæih predstavljanja. Predloženi pristup je baziran na lokalizovanom ugrađenom projekciji distribucijskih predstavljanja koji koriste monojezičke ugrađenje i približavaju najbliže susjede pitanja za transformaciju paralelnih podataka širom jezičkih varianta. Naš pristup je jezik nezavisan i može se koristiti za generiranje podataka za bilo koji variant izvornog jezika kao što je slang ili govoren dijalekt ili čak i za drugi jezik koji se odnosi na izvorni jezik. Prijavljujemo eksperimentalne rezultate Levantina na engleski prevod koristeći neurološki prevod mašine. Pokazujemo da sintetički podaci mogu pružiti značajne poboljšanje u veoma velikoj skali sistema više od 2.8 Bleu to čki i koristiti se za pružanje pouzdanog sustava prevoda za govorni dijalekt koji nema dovoljno paralelnih podataka.', 'si': 'මේ පත්තරේ අපි ප්\u200dරවේශනය කරනවා න්\u200dයූරාල් මැෂින් වාර්තාව පද්ධතියට සංවේශනය දත්ත සිද්ධා කරන්න. ප්\u200dරතිශ්න විදිහට භාෂාව වෙනස් සහ ඩායිලෙක්ට් එක්ක ගොඩක් සීමාවිත සාමාන්\u200dය ප්\u200dරධානය දත් Name ප්\u200dරයෝජනය විදිහට ස්ථානික සංවිධානය සම්බන්ධ විදිහට ප්\u200dරයෝජනය සඳහා ස්ථානික සංවිධානය සඳහා ප්\u200dරයෝජනය සම්බන්ධ අපේ භාෂාව ස්ථාවත් වෙන්න පුළුවන් ඒ වගේම මුළු භාෂාවේ ස්ලෑන් නැත්තම් කතා කරපු භාෂාව සමග වෙනස් භාෂාව සම්බ අපි ලෙවැන්ටින් ගැන ප්\u200dරශ්නයක් ඉංග්\u200dරීසි භාවිතානයට ප්\u200dරශ්නයක් වෙනුවෙන් ඉංග්\u200dරීස අපිට පෙන්වන්න පුළුවන් විදිහට සංවිධානය දත්ත 2.8 බ්ලෙයුවට වඩා ගොඩක් ලොකු පද්ධතියෙන් විශාල පද්ධතිය ප්\u200dරවේශනය කරන්න පුළුවන් විදි', 'ta': 'இந்த காகிதத்தில், நாம் ஒரு புதிய முறைமையை குறிப்பிடுகிறோம் புதிய தகவலை உருவாக்குவதற்கு, நெயுரால் இயந்திரம் மொ மிகவும் வரையறுக்கப்பட்ட இணைப்பு பயிற்சி தரவுடன் மொழி மாறிகளை ஆதரிக்கிறது. Name பரிந்துரைக்கப்பட்ட செயல்பாடு தனிப்பட்ட உள்ளூர்ந்த நுழைவாக்கும் திட்டத்திற்கு அடிப்படையில் உள்ளது. அது மொழி மாறிகளை மாற்றுவதற்கு பயன்படுத்தும எங்கள் செயல்பாடு மொழி சுதந்திரமாகும் மற்றும் மூலத்தின் மொழிக்கு தகவலை உருவாக்க பயன்படுத்தலாம் மூலத்திற்கான மாறுபாடு அல்லது பேச் நியூரால் இயந்திரம் மொழிபெயர்ப்பை பயன்படுத்தி லெவான்டின் மீது ஆங்கிலத்தில் மொழிபெயர்ப்பில் சோதன நாம் காண்பிக்கிறோம் அந்த கூட்டிணைப்பு தகவல் 2.8 ப்லியு புள்ளிகள் மூலம் மிகப்பெரிய அளவு முறைமையில் முன்னேற்றங்களை வழங்க முடியும் மற்றும் பேசு', 'sv': 'I denna uppsats introducerar vi ett nytt tillvägagångssätt för att generera syntetiska data för utbildning av Neural Machine Translation system. Den föreslagna metoden stöder språkvarianter och dialekter med mycket begränsade parallella utbildningsdata. Detta uppnås med hjälp av en sådd data för att projicera ord från ett närbesläktat resursrikt språk till en underresurs språkvariant via ord inbäddning representationer. Den föreslagna metoden bygger på lokaliserad inbäddningsprojektion av distribuerade representationer som använder enspråkiga inbäddningar och ungefärliga närmaste grannfrågor för att omvandla parallella data över språkvarianter. Vårt tillvägagångssätt är språkoberoende och kan användas för att generera data för alla varianter av källspråket såsom slang eller talad dialekt eller till och med för ett annat språk som är relaterat till källspråket. Vi rapporterar experimentella resultat på Levantine till engelsk översättning med Neural Machine Translation. Vi visar att syntetiska data kan ge betydande förbättringar jämfört med ett mycket storskaligt system med mer än 2,8 Bleu punkter och kan användas för att tillhandahålla ett tillförlitligt översättningssystem för en talad dialekt som inte har tillräckligt med parallella data.', 'ur': 'اس کاغذ میں، ہم نے نورال ماشین ترجمہ سیستموں کے لئے سینٹیسی ڈیٹی پیدا کرنے کے لئے ایک نور طریقہ پیش کیا ہے۔ پیشنهاد کی طریقہ زبان کے بدلنے اور دیالکسٹ کی مدد کرتی ہے بہت محدودہ مشابہ ترکین دیٹے کے ساتھ. یہ ایک سیڈ ڈیٹا کے مطابق پہنچائی جاتی ہے کہ ایک نزدیک مرتبہ سراسر-ثروت زبان کی زبان سے ایک کم رسسورٹ زبان کی ویرائینٹ کے ذریعہ لکھی جاتی ہے. پیشنهاد کی طریقہ تقسیم کی تقسیم کی محلی طریقہ پر بنیاد ہے جو تقسیم ہوئی نمونات کی ایک زبان انبودینگ کا استعمال کرتا ہے اور تقریباً قریب ترین مسابقات کے سوال کرتا ہے کہ زبان ویرائینٹوں میں parallel data تبدیل کریں. ہمارا طریقہ صرف زبان ہے اور اسے سورج زبان کی جتنی فرق کے لئے ڈاٹ پیدا کرنے کے لئے استعمال کر سکتا ہے جیسے اسلانگ یا بول دیالکسٹ یا حتی ایک فرق زبان کے لئے جو سورج زبان کے معاملہ میں ہے۔ ہم نئورل ماشین ترجمہ کے مطابق لیوانٹین کے ذریعے انگلیسی ترجمہ پر آزمائش کے نتائج گزارتے ہیں. ہم نشان دیتے ہیں کہ سینٹیسی ڈیٹے ایک بڑے سیسٹم پر بہت بڑے سیسٹم کے ذریعہ اضافہ کر سکتے ہیں 2.8 بلئو پوینٹوں کے ذریعہ اور اس کا استعمال کر سکتا ہے ایک بات دیالکسٹ کے لئے ایک قابل ترجمہ سیسٹم کے لئے جو کافی parallel ڈیٹا نہیں ہے۔', 'uz': "Bu hujjatda biz Neural Mashine tarjima tizimini tahrirlash uchun tizimni yaratish uchun novel tarjima qilamiz. Name Name Aniqlanadigan usul, tarjima qilingan tashkilotlarni olib tashlash asosida asosida, ular monolingual embedonidan foydalanadi va eng karibu xil soʻrovlarini o'zgartirish uchun qo'shimcha maʼlumotni o'zgartirish mumkin. Bizning fikrimiz o'zgarishga o'xshash tili, va soʻzlar yoki gapiradigan boshqa tillar uchun maʼlumotni yaratish mumkin. @ info: status Biz buni ko'rayapmiz, synthetik maʼlumoti 2.8 Bleu nuqta bilan juda katta balandlik tizimdan juda katta yaxshi improvementlarni beradi va bu gapiradigan dialektika muammiy tarjima tizimni qoʻllash mumkin, bu to ʻgʻri taklif maʼlumot yoʻq.", 'vi': 'Trong tờ giấy này, chúng tôi giới thiệu một phương pháp mới để tạo ra dữ liệu tổng hợp để đào tạo hệ thống dịch về máy thần kinh. Cách tiếp cận đề nghị hỗ trợ biến dạng ngôn ngữ và phương ngữ với dữ liệu nghiên cứu song song song rất giới hạn. Việc này được thực hiện bằng một dữ liệu hạt giống để chuyển từ ngôn ngữ giàu tài nguyên có liên quan đến một biến thể ngôn ngữ thiếu nguồn thông qua các chỉ dẫn từ: Cách tiếp cận được đề nghị dựa trên hình ảnh ghép nội bộ cục bộ của các biểu tượng được phân phát sử dụng sự nhúng vào ngôn ngữ và ước lượng hàng xóm gần nhất để chuyển đổi dữ liệu song song song trong các biến thể ngôn ngữ. Cách tiếp cận của chúng ta là độc lập ngôn ngữ và có thể được dùng để tạo dữ liệu cho bất kỳ biến thể nào của ngôn ngữ gốc như tiếng lóng hay tiếng nói hay thậm chí cho một ngôn ngữ khác liên quan đến ngôn ngữ nguồn. Chúng tôi báo cáo kết quả thử nghiệm về dịch từ Levantine sang dịch tiếng Anh bằng Dịch lắp Thần máy. Chúng tôi cho thấy các dữ liệu tổng hợp có thể cải tiến đáng kể trên một hệ thống quy mô lớn với nhiều điểm hơn 2.8 Bleu và nó có thể được dùng để cung cấp một hệ thống dịch tin cậy cho một thổ ngữ nói không có đủ dữ liệu song song.', 'da': 'I denne artikel introducerer vi en ny tilgang til generering af syntetiske data til træning af Neural Machine Translation systemer. Den foreslåede fremgangsmåde understøtter sprogvarianter og dialekter med meget begrænsede parallelle træningsdata. Dette opnås ved hjælp af en frødata til at projicere ord fra et nært beslægtet ressourcerigt sprog til en underressourcebaseret sprogvariant via ordindlejring repræsentationer. Den foreslåede tilgang er baseret på lokaliseret indlejring projektion af distribuerede repræsentationer, der bruger ensprogede indlejringer og omtrentlige nærmeste naboers forespørgsler til at omdanne parallelle data på tværs af sprogvarianter. Vores tilgang er sproguafhængig og kan bruges til at generere data for enhver variant af kildesproget såsom slang eller talt dialekt eller endda for et andet sprog, der er relateret til kildesproget. Vi rapporterer eksperimentelle resultater på Levantine til dansk oversættelse ved hjælp af Neural Machine Translation. Vi viser, at de syntetiske data kan give betydelige forbedringer i forhold til et meget stort system med mere end 2,8 Bleu point, og de kan bruges til at give et pålideligt oversættelsessystem for en talt dialekt, der ikke har tilstrækkelige parallelle data.', 'hr': 'U ovom papiru predstavljamo novi pristup stvaranju sintetičkih podataka za obuku sustava neurološkog prevoda stroja. Predloženi pristup podržava jezičke varijante i dijalekte s vrlo ograničenim paralelnim podacima obuke. To se postigne korištenjem podataka o sjemenju za projektiranje riječi iz bliskog jezika s bogatim resursima na varijant jezika s manjim resursima putem predstavljanja riječi uključenih. Predloženi pristup se temelji na lokaliziranoj integraciji projekciji raspodjeljenih predstavljanja koji koriste monojezičke integracije i približavaju najbliže susjede pitanja za transformaciju paralelnih podataka u jezičkim varijantima. Naš pristup je jezik nezavisan i može se koristiti za proizvodnju podataka za bilo koju variantu izvornog jezika poput slang ili govornog dijalekta ili čak i za različit jezik koji se odnosi na izvorni jezik. Prijavljujemo eksperimentalne rezultate Levantina na engleski prevod koristeći neurološki prevod stroja. Pokazujemo da sintetički podaci mogu pružiti značajne poboljšanje preko veoma velikog sustava preko 2.8 Bleu to čki i koristiti se za pružanje pouzdanog sustava prevoda za govorni dijalekt koji nema dovoljno paralelnih podataka.', 'nl': 'In dit artikel introduceren we een nieuwe aanpak om synthetische data te genereren voor het trainen van Neural Machine Translation systemen. De voorgestelde aanpak ondersteunt taalvarianten en dialecten met zeer beperkte parallelle trainingsgegevens. Dit wordt bereikt met behulp van een zaaddata om woorden uit een nauw verwante bronrijke taal te projecteren naar een taalvariant met weinig middelen via woordinbedding representaties. De voorgestelde aanpak is gebaseerd op gelokaliseerde embedding projectie van gedistribueerde representaties die gebruik maakt van eentalige embeddingen en benaderende nabuurschapsqueries om parallelle gegevens over taalvarianten te transformeren. Onze aanpak is taalonafhankelijk en kan worden gebruikt om gegevens te genereren voor elke variant van de brontaal zoals slang of gesproken dialect of zelfs voor een andere taal die gerelateerd is aan de brontaal. We rapporteren experimentele resultaten op Levantine naar Engels vertaling met behulp van Neural Machine Translation. We tonen aan dat de synthetische gegevens aanzienlijke verbeteringen kunnen opleveren ten opzichte van een zeer grootschalig systeem met meer dan 2.8 Bleu punten en het kan worden gebruikt om een betrouwbaar vertaalsysteem te bieden voor een gesproken dialect dat niet voldoende parallelle gegevens heeft.', 'bg': 'В настоящата статия представяме нов подход за генериране на синтетични данни за обучение на системи за неврален машинен превод. Предложеният подход поддържа езикови варианти и диалекти с много ограничени паралелни данни за обучение. Това се постига с помощта на начални данни, за да се проектират думи от тясно свързан език, богат на ресурси, към езиков вариант с недостатъчно ресурси чрез представяне на думи. Предложеният подход се основава на локализирана проекция за вграждане на разпределени представи, която използва едноезични вграждания и приблизителни заявки за съседи, за да трансформира паралелни данни в езикови варианти. Нашият подход е езиков независим и може да се използва за генериране на данни за всеки вариант на изходния език като жаргон или говорим диалект или дори за различен език, който е свързан с изходния език. Докладваме експериментални резултати на левантин на английски превод с помощта на неврален машинен превод. Показваме, че синтетичните данни могат да осигурят значителни подобрения в сравнение с много мащабна система с повече от 2,8 точки Блу и могат да бъдат използвани за осигуряване на надеждна преводна система за говорен диалект, който няма достатъчно паралелни данни.', 'de': 'In diesem Beitrag stellen wir einen neuartigen Ansatz vor, um synthetische Daten für das Training von neuronalen maschinellen Übersetzungssystemen zu generieren. Der vorgeschlagene Ansatz unterstützt Sprachvarianten und Dialekte mit sehr begrenzten parallelen Trainingsdaten. Dies wird mithilfe von Seed-Daten erreicht, um Wörter aus einer eng verwandten ressourcenreichen Sprache über Worteinbettungsdarstellungen in eine unterressourcenreiche Sprachvariante zu projizieren. Der vorgeschlagene Ansatz basiert auf der lokalisierten Einbettungsprojektion verteilter Repräsentationen, die monolinguale Einbettungen und approximative Near Neighbors Abfragen verwendet, um parallele Daten über Sprachvarianten hinweg zu transformieren. Unser Ansatz ist sprachunabhängig und kann verwendet werden, um Daten für jede Variante der Ausgangssprache wie Slang oder gesprochenen Dialekt oder sogar für eine andere Sprache zu generieren, die mit der Ausgangssprache verwandt ist. Wir berichten über experimentelle Ergebnisse zur Levantin-Englisch-Übersetzung mittels neuronaler maschineller Übersetzung. Wir zeigen, dass die synthetischen Daten im Vergleich zu einem sehr großen System um mehr als 2.8 Bleu Punkte signifikante Verbesserungen bieten können, und sie können verwendet werden, um ein zuverlässiges Übersetzungssystem für einen gesprochenen Dialekt bereitzustellen, der nicht über ausreichende parallele Daten verfügt.', 'ko': '본고에서 우리는 신경기계 번역 시스템을 훈련하는 데 사용되는 합성 데이터를 생성하는 새로운 방법을 소개했다.이 방법이 지원하는 언어 변체와 사투리의 병행 훈련 데이터는 매우 제한적이다.이것은 피드 데이터를 사용하여 단어를 삽입함으로써 단어가 밀접한 관계를 가진 자원이 풍부한 언어에서 자원이 부족한 언어 변체에 투사되는 것을 나타낸다.이 방법은 분포식 표시의 국부 삽입 투영을 바탕으로 단어 삽입과 근접 조회를 이용하여 다중 언어 변체를 병행 데이터로 변환한다.우리의 방법은 언어와 상관없이 원시 언어의 어떠한 변체 데이터, 예를 들어 비속어나 구어 사투리, 심지어 원시 언어와 관련된 다른 언어의 데이터를 생성하는 데 사용할 수 있다.우리는 신경기계로 번역한 실험 결과를 보고했다.우리는 합성 데이터가 매우 큰 시스템보다 2.8개의 Bleu점을 넘는 현저한 개선을 제공할 수 있을 뿐만 아니라, 충분한 병행 데이터가 없는 구어 사투리에 믿을 만한 번역 시스템을 제공할 수 있다고 밝혔다.', 'id': 'Dalam kertas ini, kami memperkenalkan pendekatan baru untuk menghasilkan data sintetis untuk melatih sistem Translation Mesin Neural. pendekatan yang diusulkan mendukung varian bahasa dan dialekt dengan data latihan paralel yang sangat terbatas. Ini dicapai dengan menggunakan data benih untuk proyeksi kata-kata dari bahasa yang sangat terkait dengan sumber daya ke varian bahasa yang tidak terkait dengan sumber daya melalui reprezentasi penyampilan kata. pendekatan yang diusulkan berdasarkan proyeksi embedding lokalisasi dari representation yang didistribusikan yang menggunakan embedding monobahasa dan mendekati pertanyaan tetangga terdekat untuk mengubah data paralel melalui varian bahasa. pendekatan kita adalah bahasa yang bebas dan dapat digunakan untuk menghasilkan data untuk setiap varian bahasa sumber seperti slang atau dialekt berbicara atau bahkan untuk bahasa yang berbeda yang berhubungan dengan bahasa sumber. Kami melaporkan hasil percobaan Levantine ke terjemahan Inggris menggunakan Translation Neural Machine. Kami menunjukkan bahwa data sintetis dapat menyediakan peningkatan yang signifikan dalam sistem skala yang sangat besar dengan lebih dari 2,8 poin Bleu dan dapat digunakan untuk menyediakan sistem terjemahan yang dapat dipercaya untuk dialekt berbicara yang tidak memiliki data paralel yang cukup.', 'sw': 'Katika karatasi hii, tunaonyesha mbinu za riwaya za kutengeneza takwimu za pamoja kwa ajili ya mafunzo ya mfumo wa Tafsiri wa Mashine ya Kiurali. Hatua ya pendekezo inaunga mkono mabadiliko ya lugha na tofauti yenye taarifa za mafunzo mbalimbali. Hii imefanikiwa kwa kutumia taarifa za kizazi kwa ajili ya mradi wa maneno kutoka lugha inayohusiana na rasilimali yenye utajiri wa karibu kwenda tofauti ya lugha isiyo na rasilimali kupitia uwakilishi wa neno. Mtandao huu unapendekezwa umejikita na mradi wa kuwagawanya wakilishaji ambao unatumia mabango ya lugha za kidini na maswali ya jirani karibu ili kubadilisha data tofauti za lugha mbalimbali. Hatua yetu ni lugha huru na inaweza kutumika kutengeneza taarifa kwa ajili ya kutengeneza tofauti yoyote ya lugha ya asili kama vile wimbo au lugha tofauti inayohusiana na lugha ya asili. Tunatoa taarifa za matokeo ya majaribio kuhusu Levantine kwa kutafsiri Kiingereza kwa kutumia Tafsiri ya Mashine ya Kiurali. Tunaonyesha kuwa taarifa za pamoja zinaweza kutoa maendeleo makubwa zaidi ya mfumo wa kiwango kikubwa kwa zaidi ya pointi 2.8 Bleu na inaweza kutumika kwa ajili ya kutoa mfumo wa tafsiri inayoaminika kwa lugha inayozungumzwa ambayo haina takwimu zinazofanana.', 'tr': "Bu kagyzda, Neural Maşynyň terjime sistemalary üçin sintetik maglumaty bejermek üçin roman ýazşyny tanyşdyrýarys. Maksady çykyş dil wariantlaryny we dialektleri örän çykyş parallel okuw maglumaty bilen destekleýär. Bu ýer gollanýan ressoural-baý dilinden daý maglumatyny ulanmak üçin ýeterlik bar. Bu teklif etmek üçin ýerleşdirilen daýratyn suratlaryň ýerleşdirilip daýratyn suratlaryna dayalýar. Bu şekilde monolingüň içine golaý goňşgalaryň soraglaryny dil wariantlarynda üýtgetmek üçin paýlaşýar. Biziň ýaryşymyz dili boýunça we çeşme dilinde baglanmak ýaly slang ýa-da gepleşme dilinde bolan başga bir dil üçin ullanyp bilýär. Biz Levantine'yň netijelerini Neural Maşynyň terjimesini ulanyp Iňlis diline terjime edip görkeýäris. Biz sintetik maglumatyň 2.8-den gowy bir ölçekli sistemiň üstünde möhüm gelişmeleri bolup biler we bu sistemiň çykyş dialekte ýeterlik parallel maglumaty ýok bir terjime sistemini saýlamak üçin ullanylabilir.", 'fa': 'در این کاغذ، ما یک طریق روزنامه برای تولید داده\u200cهای سناتیک برای آموزش سیستم ترجمه\u200cهای ماشین عصبی معرفی می\u200cکنیم. این دستور پیشنهاد متغیرات زبانی و دیالکت\u200cها را با داده\u200cهای آموزش مشابه بسیار محدود می\u200cکند. این با استفاده از داده های دانه برای پروژه کلمات از زبان ثروتمند منبع نزدیک به یک متغیر زبان زیر منبع از طریق نمایش\u200cهای کلمات داخل می\u200cشود. این دستور پیشنهاد بر اساس پروژه\u200cهای محلی\u200cسازی\u200cشده\u200cای از نمایش\u200cهای پراکنده\u200cشده\u200cای است که استفاده می\u200cکند انجمن\u200cهای یک زبان و تقریباً نزدیکترین سوال\u200cهای همسایه\u200cها برای تغییر داده\u200cهای پارالی در طول متغییرات زبان دسترسی ما به زبان مستقل است و می تواند برای ایجاد داده\u200cها برای هر تغییر زبان منبع مثل slang یا dialect حرف زده یا حتی برای زبان متفاوتی که با زبان منبع ارتباط دارد استفاده شود. ما نتایج آزمایشی را در لووانتین به ترجمه انگلیسی با استفاده از ترجمه ماشین عصبی گزارش می دهیم. ما نشان می\u200cدهیم که داده\u200cهای سناتیک می\u200cتوانند بر یک سیستم مقیاس بسیار بزرگ با بیشتر از ۲.۸ نقطه\u200cهای بلئو بهترین\u200cها را پیشنهاد کنند و می\u200cتوانند برای تغییر سیستم قابل اطمینان برای یک دیالکت صحبت کننده استفاده کنند که داده\u200cهای متفاوتی کافی', 'af': "In hierdie papier, introduseer ons 'n roman toegang om sintetiese data te genereer vir die onderwerp van Neurale Masjien Vertaling stelsels. Die voorgestelde toegang ondersteun taal variante en dialekte met baie beperkte parallele onderwerking data. Hierdie word bereik deur 'n saad data te gebruik na projek woorde van 'n naby verwante hulpbron- ryk taal na' n onder- hulpbron taal variant deur woord inbêring voorstellings. Die voorgestelde toegang is gebaseer op lokaliseerde inbêding projeksie van verdeelde voorstellings wat monolinglike inbêding gebruik en omtrent nabiese nabyge vrae om parallele data te transformeer oor taal variante. Ons toegang is taal onveilig en kan gebruik word om data te genereer vir enige variante van die bron taal soos slang of gespreek dialekte of selfs vir 'n ander taal wat verwanter is met die bron taal. Ons rapporteer eksperimentale resultate op Levantine na Engels vertaling deur te gebruik Neural Masjien Vertaling. Ons wys dat die sintetiese data kan betekeurige verbeteringe verskaf oor 'n baie groot skaal stelsel deur meer as 2.8 Bleu punte en dit kan gebruik word om 'n betroubare vertalingsstelsel te verskaf vir' n praat dialekte wat nie genoeg parallele data het nie.", 'sq': 'Në këtë letër, ne paraqesim një metodë të re për të gjeneruar të dhëna sintetike për trajnimin e sistemeve të përkthimit të makinave nervore. Përqasja e propozuar mbështet variantet e gjuhës dhe dialektet me të dhëna shumë të kufizuara paralele trajnimi. Kjo arrihet duke përdorur një të dhënë fillimi për të projektuar fjalët nga një gjuhë e pasur me burime të afërt në një variant gjuhësh të pakërresursuar nëpërmjet përfaqësimeve të përfshirjes së fjalëve. Përqasja e propozuar bazohet në projektimin lokalizuar të përfaqësimeve të shpërndara që përdorin përfaqësime monogjuhësore dhe përshkruajnë pyetjet e fqinjëve më të afërt për të transformuar të dhënat paralele nëpër variantet gjuhësore. Përqasja jonë është e pavarur nga gjuha dhe mund të përdoret për të gjeneruar të dhëna për çdo variant të gjuhës burimore të tillë si slang apo dialekti i folur ose edhe për një gjuhë tjetër që është e lidhur me gjuhën burimore. Ne raportojmë rezultatet eksperimentale në Levantin në përkthimin anglez duke përdorur përkthimin e makinës nervore. Ne tregojmë se të dhënat sintetike mund të sigurojnë përmirësime të rëndësishme mbi një sistem në shkallë shumë të madhe me më shumë se 2.8 pikë Bleu dhe mund të përdoren për të siguruar një sistem të besueshëm përkthimi për një dialekt të folur që nuk ka të dhëna të mjaftueshme paralele.', 'hy': 'Այս թղթի մեջ մենք ներկայացնում ենք նոր մոտեցում, որպեսզի ստեղծենք սինթետիկ տվյալներ նյարդային մեքենաների թարգմանման համակարգերի ուսուցման համար: Առաջարկված մոտեցումը աջակցում է լեզվի տարբերակները և դիալեկտները, որոնք ունեն շատ սահմանափակ զուգահեռ ուսուցման տվյալներ: Սա հասնում է օգտագործելով սերմնային տվյալներ, որպեսզի նախագծենք բառերը սերտորեն կապված ռեսուրսների հարուստ լեզվից մինչև թերռեսուրսներ ունեցող լեզվի տարբերակ բառերի ներդրման միջոցով: Պատրաստված մոտեցումը հիմնված է տարածված ներդրման տեղական պրոեկցիայի վրա, որը օգտագործում է միալեզվով ներդրումներ և մոտավորապես մոտավոր հարցեր հարևաններին, որպեսզի փոխակերպեն զուգահեռ տվյալները լեզվի տարբերակների միջև: Մեր մոտեցումը անկախ է լեզվից և կարող է օգտագործվել տվյալներ ստեղծելու աղբյուր լեզվի ցանկացած տարբերակի համար, ինչպիսիք են սլենգը կամ խոսված դիալեկտը, կամ նույնիսկ աղբյուր լեզվին կապված այլ լեզվի համար: Մենք զեկուցում ենք Լեվանտինի փորձարկման արդյունքները անգլերեն թարգմանման օգտագործելով նյարդային մեքենայի թարգմանություն: Մենք ցույց ենք տալիս, որ սինթետիկ տվյալները կարող են նշանակալի բարելավումներ տալ շատ մեծ համակարգում ավելի քան 2.8 բլո կետերով և դրանք կարող են օգտագործվել վստահելի թարգմանման համակարգի տրամադրելու համար խոսում դիալեկտի համար, որը բավարար զուգահեռ տվյալներ չուն', 'az': 'Bu kağızda, Nöral Makina Çeviri Sistemləri təhsil etmək üçün sintetik məlumatları yaratmaq üçün yeni bir yol göstəririk. Təsdiq edilən tərzim dil variablarını və dialektləri çox sınırlı paralel təhsil məlumatlarını dəstəkləyir. Bu, yaxınlaşdırılmış ressurs-zengin dilindən çox qüvvətli dil dəyişikliyinə sözləri ilə daxil edilən dəyişiklik məlumatları ilə istifadə edilir. Bu təbliğ edilən tərzim, dil variabları arasında paralel məlumatları dəyişdirmək üçün yalnız dil içərisində istifadə edən dağıtılmış tərzlərin yerləşdirilməsi tərzinə dayanılır. Bizim tərəfimiz dil bağımsızdır və mənbə dilində olan hər cür dəyişiklik dilinin məlumatlarını yaratmaq üçün istifadə edilə bilər. Biz Levantin üçün təcrübə sonuçlarını Neural Machine Translation vasitəsilə İngilizce çevirisinə bildiririk. Biz sintetik məlumatların 2.8 Bleu noktalarından çox böyük ölçüdə sistemin üstündə möhkəm düzəltmələri sağlayacağını göstərərik və bu söylədikləri dialekt üçün güvenilir bir tercümə sistemi təmin edə bilər ki, bunun yetər paralel məlumatları yoxdur.', 'bs': 'U ovom papiru predstavljamo novi pristup stvaranju sintetičkih podataka za obuku neuronskih sustava prevoda mašina. Predloženi pristup podržava jezičke varijante i dijalekte sa vrlo ograničenim paralelnim podacima obuke. To se postigne korištenjem podataka o sjemenju za projektiranje riječi iz bliskog jezika s bogatim resursima na varijant jezika s manjim resursima putem predstavljanja riječima uključenih. Predloženi pristup se temelji na lokaliziranom ugrađenom projekciji distribuiranih predstavljanja koji koriste monojezičke ugrađenje i približavaju najbliže susjede pitanja za transformaciju paralelnih podataka u jezičkim varijantima. Naš pristup je jezik nezavisan i može se koristiti za proizvedenje podataka za bilo koji variant izvornog jezika kao što je slang ili govoren dijalekt ili čak i za drugi jezik koji se odnosi na izvorni jezik. Prijavljujemo eksperimentalne rezultate Levantina na engleski prevod koristeći Neuralnu prevod mašine. Pokazujemo da sintetički podaci mogu pružiti značajne poboljšanje u veoma velikoj skali sustava preko 2.8 Bleu to čki i koristiti se za pružanje pouzdanog sustava prevoda za govorni dijalekt koji nema dovoljno paralelnih podataka.', 'cs': 'V tomto článku představujeme nový přístup k generování syntetických dat pro trénink systémů neuronového strojového překladu. Navržený přístup podporuje jazykové varianty a dialekty s velmi omezenými paralelními výcvikovými daty. Toho je dosaženo pomocí semenních dat k promítání slov z úzce příbuzného jazyka bohatého na zdroje do nedostatečně zdrojové jazykové varianty prostřednictvím reprezentací vkládání slov. Navržený přístup je založen na lokalizované vkládací projekci distribuovaných reprezentací, která využívá monojazyčné vkládání a přibližné dotazy nejbližších sousedů k transformaci paralelních dat napříč jazykovými variantami. Náš přístup je jazykově nezávislý a může být použit k generování dat pro libovolnou variantu zdrojového jazyka, jako je slang nebo mluvený dialekt, nebo dokonce pro jiný jazyk, který souvisí s zdrojovým jazykem. Předkládáme experimentální výsledky levantinsko-anglického překladu pomocí neurálního strojového překladu. Ukazujeme, že syntetická data mohou poskytnout výrazné zlepšení oproti velmi velkému systému o více než 2.8 Bleu body a mohou být použita k poskytnutí spolehlivého překladového systému mluveného dialektu, který nemá dostatek paralelních dat.', 'ca': "En aquest article introduim un nou enfocament per a generar dades sintètiques per formar sistemes de traducció de màquines neuronals. L'enfocament proposat suporta variants de llenguatge i dialectes amb dades de formació paralleles molt limitades. Això es pot aconseguir fent servir dades de semilla per projectar paraules d'una llengua rica en recursos estretament relacionada a una variant de llengua amb menys recursos mitjançant representacions d'incorporació de paraules. L'enfocament proposat està basat en una projecció d'incorporació localitzada de representacions distribuïdes que utilitza incorporacions monolingües i aproximadament preguntes dels veïns més propers per transformar dades paralleles entre variants lingüístics. El nostre enfocament és independent del llenguatge i pot ser utilitzat per generar dades per a qualsevol variant del llenguatge d'origen com slang o dialecte parlat o fins i tot per a una llengua diferent relacionada amb el llenguatge d'origen. Informem resultats experimentals sobre Levantine a traducció anglesa fent servir la traducció neural de màquines. Mostrem que les dades sintètiques poden proporcionar millores significatives en un sistema de gran escala en més de 2,8 punts Bleu i poden ser utilitzades per proporcionar un sistema de traducció fiable per a un dialecte parlat que no té suficients datos parallels.", 'et': 'Käesolevas töös tutvustame uudset lähenemisviisi sünteetiliste andmete genereerimiseks neuroaalsete masintõlkesüsteemide koolitamiseks. Kavandatud lähenemisviis toetab keeleversioone ja murdeid väga piiratud paralleelse koolituse andmetega. See saavutatakse seemneandmete abil, et projekteerida sõnu tihedalt seotud ressursirikkast keelest alaressurssidega keelevarianti sõnade manustamise kaudu. Kavandatud lähenemisviis põhineb hajutatud esituste lokaliseeritud manustamisprojektsioonil, mis kasutab ühekeelseid manustamisi ja ligikaudseid naaberpäringuid paralleelsete andmete muundamiseks keelevariantide vahel. Meie lähenemisviis on keelesõltumatu ja seda saab kasutada andmete genereerimiseks mis tahes lähtekeele variandi kohta, nagu släng või räägitav murre või isegi muu keele kohta, mis on seotud lähtekeelega. Raporteerime eksperimentaalseid tulemusi Levantine inglise keelde neuroaalse masintõlke abil. Näitame, et sünteetilised andmed võivad pakkuda märkimisväärseid parandusi võrreldes väga suuremahulise süsteemiga rohkem kui 2,8 Bleu punkti võrra ning neid saab kasutada usaldusväärse tõlkesüsteemi pakkumiseks räägitud murdele, millel ei ole piisavalt paralleelseid andmeid.', 'fi': 'T채ss채 ty철ss채 esittelemme uudenlaisen l채hestymistavan synteettisen tiedon tuottamiseen neurokonek채채nn철sj채rjestelmien koulutukseen. Ehdotettu l채hestymistapa tukee kielimuunnelmia ja murteita, joiden rinnakkaiskoulutustiedot ovat hyvin rajalliset. T채m채 saavutetaan k채ytt채m채ll채 siemendataa, jolla voidaan projisoida sanoja l채heisesti liittyv채st채 resurssirikkaasta kielest채 vajaak채ytt철iseen kieliversioon sanan upotusesitysten avulla. Ehdotettu l채hestymistapa perustuu hajautettujen esitysten lokalisoituun upotusprojektioon, jossa hy철dynnet채채n monikielisi채 upotuksia ja likim채채r채isi채 l채himpien naapureiden kyselyj채 rinnakkaisten tietojen muuntamiseksi eri kieliversioiden v채lill채. L채hestymistapamme on kielest채 riippumaton ja sit채 voidaan k채ytt채채 tuottamaan tietoa mist채 tahansa l채hdekielen variantista, kuten slangista tai puhutusta murteesta tai jopa eri kielest채, joka liittyy l채hdekieleen. Raportoimme kokeellisia tuloksia Levantiinista englanninkieliseen k채채nn철kseen k채ytt채en hermokonek채채nn철st채. Osoitamme, ett채 synteettinen data voi tuottaa merkitt채vi채 parannuksia hyvin suuren mittakaavan j채rjestelm채채n yli 2,8 Bleu pisteell채 ja sen avulla voidaan tarjota luotettava k채채nn철sj채rjestelm채 puhuttuun murteeseen, jossa ei ole riitt채v채sti rinnakkaistietoa.', 'am': 'In this paper, we introduce a novel approach to generate synthetic data for training Neural Machine Translation systems.  በተዘጋጀው ሥርዓት የቋንቋ መለያየት እና ዳሌክቶችን በተለየ ተማሪ ዳታ ይረዳል፡፡ ይሄ ዘር ዳታ በመግለጫ ቃላት የተደረገ ነው፡፡ የተዘጋጀው ሥርዓት በቋንቋ መለያየት እና የቀረበ ጎረቤቶች የቅርብ ቅርብ መረጃዎችን ለመለወጥ በተለየ የአካባቢ ፕሮጀክት በመሠረት ላይ ነው፡፡ የቋንቋችን ነጻ ነው እናም የቋንቋ ቋንቋ ዳታዎችን ለመፍጠር ይችላል፡፡ We report experimental results on Levantine to English translation using Neural Machine Translation.  በ2.8 ብሊዮ ነጥቦች የሚበልጠውን የሲንቲካዊ ዳታዎች በክፍለ ትክክል ማድረግ እንዲችል እናሳየዋለን፡፡', 'bn': 'এই কাগজটিতে আমরা নিউরাল মেশিন অনুবাদ সিস্টেম প্রশিক্ষণের জন্য সিন্টেটিক ডাটা তৈরি করার জন্য একটি উপন্যাসের উপায় পরিচয় The proposed approach supports language variants and dialects with very limited parallel training data.  শব্দের প্রতিনিধিত্বের মাধ্যমে কাছাকাছি সম্পর্কিত রিসোর্স- সমৃদ্ধ ভাষার কাছ থেকে প্রকল্পের শব্দ ব্যবহার করে একটি বৃত্ত তথ্ প্রস্তাবিত পদ্ধতি স্থানীয় ভিত্তিক ভিত্তিক ভিত্তিক ভিত্তিতে বিতরণ করা প্রতিনিধিদের প্রকল্পের উপর ভিত্তিতে রয়েছে যা মোনোলিভাল ভাষায় প্রবে Our approach is language independent and can be used to generate data for any variant of the source language such as slang or spoken dialect or even for a different language that is related to the source language.  আমরা লেভান্তিনের পরীক্ষার ফলাফলের সংবাদ প্রদান করি নিউরাল মেশিন অনুবাদ ব্যবহার করে ইংরেজি অনুবাদের কাছে। আমরা দেখাচ্ছি যে সিন্টেটিক ডাটা ২. ৮ ব্লিউ বিন্দুর দ্বারা বিশাল স্ক্যালেক্সের ব্যাপারে গুরুত্বপূর্ণ উন্নতি প্রদান করতে পারে এবং কথা বলা ডায়ালেক', 'jv': 'Nang pepul iki, kita mulai dadi nganggo dolanan sing apik batir kanggo nggawe data senetik kanggo nggawe sistem penting Neral Mas Terjamahan. Laptop" and "Desktop Punika dipunangé perusahaan data winih kanggo kelompok word Awak dhéwé nggunakake dipunangé karo akeh basa Perusahaan langgambar neng akeh basa gambar sing dikarolan Njupuk dhéwé iku luwih-luwih akeh sing nesaturan lan iso nggawe data nggawe perusahaan kanggo ngubah winih sing koyra slang o nggawe dialecting, sampeyan kanggo langa sing diputaaken langa sing bakal terus bener. Awak dhéwé ngertuk perintal sing nyengkuyung Debian karo Tarjamahan Inggris nggambar Inggris Awak dhéwé éntuk sistem sing dikantenatik kanggo nyenggawe sistem sing gawe nggawe barang langgambar gawe 2.8 Punt sing katya karo 2.8', 'he': 'בעיתון הזה, אנחנו מציגים גישה חדשה לייצר נתונים סינטטיים לאימון מערכות תרגום מכונות נוירואליות. הגישה המוצעת תומכת בציוני שפה ודיאלקטים עם מידע אימון מקביל מוגבל מאוד. זה ניתן להשיג באמצעות נתוני זרע כדי לפרויקט מילים משפה עשירה במשאבים שקשורה קרוב לשפת מתחת למשאבים מתחת לשפת באמצעות ייצוגי מילים. הגישה המוצעת מבוססת על פרוייקציה מקומית של מייצגים מופרסמים שמשתמשים באמצעות מייצגים מונושפתיים וביקשות של שכנים הקרובים ביותר הגישה שלנו היא שפה עצמאית ואפשר להשתמש בה כדי ליצור נתונים לכל שונה של שפת המקור כמו סלנג או דיאלקט מדבר או אפילו לשפה שונה שקשורה לשפת המקור. אנחנו מדווחים על תוצאות ניסויים על לוואנטין לתרגום אנגלי באמצעות תרגום מכונת נוירואלית. אנו מראים שהנתונים הסינטטיים יכולים לספק שיפורים משמעותיים במערכת בקנה מידה גדולה ביותר מ-2.8 נקודות בלו והם יכולים להשתמש בכדי לספק מערכת תרגום אמינה לדיאלקט מדבר שאין לו מספיק נתונים מקבילים.', 'sk': 'V prispevku predstavljamo nov pristop k ustvarjanju sintetičnih podatkov za usposabljanje sistemov nevralnega strojnega prevajanja. Predlagani pristop podpira jezikovne različice in narečja z zelo omejenimi podatki o vzporednem usposabljanju. To je doseženo s pomočjo semenskih podatkov za projiciranje besed iz tesno povezanega jezika, bogatega z viri, v različico jezika s premalo virov prek predstavitev vključevanja besed. Predlagani pristop temelji na lokalizirani projekciji vdelave porazdeljenih predstavitev, ki uporablja enojezične vdelave in približne sosedske poizvedbe za pretvorbo vzporednih podatkov med jezikovnimi različicami. Naš pristop je jezikovno neodvisen in se lahko uporablja za ustvarjanje podatkov za katero koli različico izvornega jezika, kot je sleng ali govorjeno narečje, ali celo za drug jezik, ki je povezan z izvornim jezikom. S pomočjo nevralnega strojnega prevajanja poročamo o eksperimentalnih rezultatih levantinskega prevoda v angleščino. Pokazujemo, da sintetični podatki lahko pomembno izboljšajo nad zelo velikim sistemom za več kot 2,8 bleu točk in jih lahko uporabimo za zagotavljanje zanesljivega prevajalskega sistema za govorjeno narečje, ki nima zadostnih vzporednih podatkov.', 'ha': "Daga wannan takardan, Munã ƙara wani matsayi na nowaya dõmin ya ƙiƙiro data na synthetisk wa wa'urar tarjifani na Mashin Tarjifani na Neural. @ info: whatsthis This is achieved using a seed data to project words from a closely-related resource-rich language to an under-resourced language variant via word embedding representations.  @ item: inmenu Mataimakinmu yana da kure cikin harshen na'ura kuma ana iya amfani da in ƙiƙiro data wa wasu variant na harshen source kamar linjeri ko da aka yi magana, ko kuma kõ da zuwa wata harshe na daban da ke haɗi da harshen source. @ info: whatsthis Tuna nuna cewa data na haɗi za'a iya samar da shi mai girma kan tsari mai girma na ƙari na 2.8, kuma za'a iya amfani da shi dõmin a bãyar da wani tsari na fassarar fassarar da ya yi aminci ga wani dialakar da aka yi magana, wanda bai da daidai da data masu daidaita.", 'bo': 'ང་ཚོས་ཤོག་བུ་འདིའི་ནང་དུ་གསར་གཏོང་གི་ཐབས་ལམ་ཞིག་སྤྲོད་ནས་དབྱིན་རྩིས་འཁོར་གྱི་མ་ལག་ལུགས་སྤྲོད་ཀྱི་ཐབས་ལམ དམིགས་འཛུགས་ཀྱི་གཟུགས་སྐོར་འདིས་སྐད་ཡིག་གཟུགས་རིས་དང་dialects ལ་རྒྱབ་སྐྱོར་བྱེད་ཀྱི་ཡོད། འདི་ནི་ཆ་འཕྲིན་དང་འབྲེལ་བའི་རྒྱུ་དངོས་ཡིག་གཟུགས་པའི་སྐད་རིགས་དང་མཐུན་འབྲེལ་བའི་ཐོག་ལས་ཆ་འཕྲིན་ཡིག་ཆ་སྤྱད་ནས་སྤྱོད་ཐུབ་པ The proposed approach is based on localized embedding projection of distributed representations which uses monolingual embeddings and approximate nearest neighbors queries to transform parallel data across language variants. ང་ཚོའི་གཟུགས ང་ཚོས་དབྱིན་ཡིག་གི་སྐྱེས་ཚུལ་ལྟ་བུ་Levantine ལ་དབྱིན་ཡིག་གི་ཚིག་རྒྱུན་སྤྱོད་ནས་སྙན་ཞུ་བ་བྱེད We show that the synthetic data can provide significant improvements over a very large scale system by more than 2.8 Bleu points and it can be used to provide a reliable translation system for a spoken dialect which does not have enough parallel data.'}
{'en': 'Monolingual Embeddings for Low Resourced Neural Machine Translation', 'ar': 'تطريز أحادي اللغة للترجمة الآلية العصبية منخفضة الموارد', 'es': 'Incrustaciones monolingües para traducción automática neuronal de bajos recursos', 'fr': 'Embeddings monolingues pour la traduction automatique neuronale à faibles ressources', 'pt': 'Incorporações monolíngues para tradução automática neural com poucos recursos', 'ja': '低資源神経機械翻訳のためのモノリンガル埋め込み', 'zh': '以低资源神经机器翻译单语嵌之', 'hi': 'कम संसाधन तंत्रिका मशीन अनुवाद के लिए मोनोलिंगुअल एम्बेडिंग', 'ru': 'Одноязычные вставки для низкоресурсного нейронного машинного перевода', 'ga': "Leabaithe Aonteangacha d'Aistriúchán Meaisín Néar-Acmhainne Íseal", 'ka': 'Name', 'kk': 'Төменгі ресурстар невралдық машинаның аудармасының бір тілік ендірулері', 'el': 'Μονογλωσσικές ενσωματώσεις για Νευρική Μηχανική Μετάφραση Χαμηλών Πόρων', 'hu': 'Egynyelvű beágyazások az alacsony erőforrású neurális gépi fordításhoz', 'mk': 'Name', 'ms': 'Name', 'it': 'Embeddings monolingue per traduzione automatica neurale a bassa risorsa', 'ml': 'Monolingual Embeddings for Low Resourced Neural Machine Translation', 'mt': 'Embeddings Monolingual for Low Resource Neural Machine Translation', 'mn': 'Бага боловсруулагдсан мэдрэлийн машин хөрөнгө оруулахын тулд монолингийн нэвтрүүлэлт', 'lt': 'Mažai išteklių taurių mašinų vertimui skirtos monolingos įrangos', 'no': 'Name', 'ro': 'Încorporări monolingve pentru traducerea automată neurală cu resurse scăzute', 'pl': 'Jednojęzyczne osadzenia dla tłumaczenia maszynowego o niskich zasobach', 'sr': 'Monolingualne integracije za prevod neuralnih mašina s niskim resursima', 'so': 'Turjumidda qalabka afka', 'sv': 'Enspråkiga inbäddningar för låg resurs neural maskinöversättning', 'si': 'Name', 'ta': 'Name', 'ur': 'Name', 'uz': 'Name', 'vi': 'KCharselect unicode block name', 'bg': 'Едноезични вграждания за машинен превод с нисък ресурс', 'nl': 'Monolingual Embeddings voor Low Resourced Neural Machine Translation', 'da': 'Enkeltsprogede indlejringer til lav resourced neural maskinoversættelse', 'hr': 'Monolingualne integracije za prevod neuroloških strojeva s niskim resursima', 'de': 'Einsprachige Einbettungen für maschinelle Übersetzung mit geringer Resource', 'id': 'Embeddings Monolingual untuk Translation Low Resourced Neural Machine', 'ko': '저자원 신경기계를 위한 단어 삽입', 'fa': 'Name', 'sw': 'Mazingira ya lugha kwa Tafsiri ya Mashine ya Kiasili', 'tr': 'Täze Ressurat Nural Mazmunlar terjime üçin Monoli Diller', 'af': 'Name', 'sq': 'Embeddings Monolingual for Low Resourced Neural Machine Translation', 'am': 'Language', 'az': 'Aşağıdakı Nöral Makinelərin Tərcüməsi üçün Monoli Dili İfadələri', 'hy': 'Comment', 'bn': 'কম সম্পদ নিউরাল মেশিন অনুবাদের জন্য মনোলিভাল ইমেডিং', 'ca': 'Embeddings Monolingual for Low Resourced Neural Machine Translation', 'cs': 'Jednojazyčné vložení pro strojový překlad s nízkými zdroji', 'et': 'Ühekeelsed manustamised madala ressursiga neuroaalse masintõlke jaoks', 'bs': 'Monolingualne integracije za prevod neuroloških strojeva s niskim resursima', 'fi': 'Yksikieliset upotukset heikon resurssin hermojen konekääntämiseen', 'jv': 'Ngubah Monolngual kanggo Kejaratan Njaratan Jejaral', 'sk': 'Enojezične vdelave za strojno prevajanje nevronov z nizkimi viri', 'ha': 'KCharselect unicode block name', 'he': 'Monolingual Embeddings for Low Resourced Neural Machine Translation', 'bo': 'རྫུན་འབྱུང་བའི་དཔེ་གཞུང་གི་མ་ལག་འཁྱེར་ལ་བསྒྱུར་ནུས་ཀྱི་ནང་དུ་འཛམ་གླིང་སྟོན་པ'}
{'en': 'Neural machine translation (NMT) is the state of the art for  machine translation , and it shows the best performance when there is a considerable amount of data available. When only little data exist for a language pair, the  model  can not produce good representations for  words , particularly for rare words. One common solution consists in reducing data sparsity by segmenting words into sub-words, in order to allow rare words to have shared representations with other words. Taking a different approach, in this paper we present a method to feed an NMT network with word embeddings trained on monolingual data, which are combined with the task-specific embeddings learned at training time. This method can leverage an embedding matrix with a huge number of words, which can therefore extend the word-level vocabulary. Our experiments on two language pairs show good results for the typical low-resourced data scenario (IWSLT in-domain dataset). Our consistent improvements over the baselines represent a positive proof about the possibility to leverage  models  pre-trained on monolingual data in NMT.', 'ar': 'الترجمة الآلية العصبية (NMT) هي أحدث ما توصلت إليه الترجمة الآلية ، وهي تُظهر أفضل أداء عندما يكون هناك قدر كبير من البيانات المتاحة. عند وجود بيانات قليلة فقط لزوج لغوي ، لا يمكن للنموذج إنتاج تمثيلات جيدة للكلمات ، خاصةً للكلمات النادرة. يتمثل أحد الحلول الشائعة في تقليل تباين البيانات عن طريق تقسيم الكلمات إلى كلمات فرعية ، للسماح للكلمات النادرة بمشاركة التمثيلات مع كلمات أخرى. من خلال اتباع نهج مختلف ، نقدم في هذه الورقة طريقة لتغذية شبكة NMT بحفلات الزفاف المدربة على بيانات أحادية اللغة ، والتي يتم دمجها مع حفلات الزفاف الخاصة بالمهمة التي تم تعلمها في وقت التدريب. يمكن لهذه الطريقة الاستفادة من مصفوفة التضمين بعدد كبير من الكلمات ، والتي يمكن بالتالي توسيع نطاق مفردات مستوى الكلمة. تُظهر تجاربنا على أزواج لغتين نتائج جيدة لسيناريو البيانات النموذجية منخفضة الموارد (مجموعة بيانات IWSLT في المجال). تمثل التحسينات المتسقة التي أجريناها على خطوط الأساس دليلًا إيجابيًا على إمكانية الاستفادة من النماذج المدربة مسبقًا على البيانات أحادية اللغة في NMT.', 'es': 'La traducción automática neuronal (NMT) es el estado de la técnica de la traducción automática y muestra el mejor rendimiento cuando hay una cantidad considerable de datos disponibles. Cuando solo existen pocos datos para una combinación de idiomas, el modelo no puede producir buenas representaciones de palabras, especialmente de palabras poco comunes. Una solución común consiste en reducir la dispersión de datos mediante la segmentación de las palabras en subpalabras, a fin de permitir que las palabras raras compartan representaciones con otras palabras. Tomando un enfoque diferente, en este artículo presentamos un método para alimentar una red de NMT con incrustaciones de palabras entrenadas en datos monolingües, que se combinan con las incrustaciones específicas de la tarea aprendidas en el momento de la capacitación. Este método puede aprovechar una matriz de incrustación con una gran cantidad de palabras, lo que puede ampliar el vocabulario a nivel de palabras. Nuestros experimentos con dos pares de idiomas muestran buenos resultados para el escenario típico de datos de bajos recursos (conjunto de datos dentro del dominio IWSLT). Nuestras mejoras consistentes con respecto a las líneas de base representan una prueba positiva de la posibilidad de aprovechar los modelos previamente entrenados en datos monolingües en NMT.', 'pt': 'A tradução automática neural (NMT) é o estado da arte da tradução automática e apresenta o melhor desempenho quando há uma quantidade considerável de dados disponíveis. Quando existem poucos dados para um par de idiomas, o modelo não pode produzir boas representações para palavras, principalmente para palavras raras. Uma solução comum consiste em reduzir a dispersão de dados segmentando palavras em subpalavras, para permitir que palavras raras tenham representações compartilhadas com outras palavras. Tomando uma abordagem diferente, neste artigo apresentamos um método para alimentar uma rede NMT com embeddings de palavras treinados em dados monolíngues, que são combinados com os embeddings específicos da tarefa aprendidos no tempo de treinamento. Esse método pode alavancar uma matriz de incorporação com um grande número de palavras, o que pode, portanto, estender o vocabulário em nível de palavra. Nossos experimentos em dois pares de idiomas mostram bons resultados para o cenário típico de dados com poucos recursos (conjunto de dados no domínio IWSLT). Nossas melhorias consistentes sobre as linhas de base representam uma prova positiva sobre a possibilidade de alavancar modelos pré-treinados em dados monolíngues em NMT.', 'fr': "La traduction automatique neuronale (NMT) est à la pointe de la technologie en matière de traduction automatique, et elle affiche les meilleures performances lorsqu'une quantité considérable de données est disponible. Lorsqu'il n'existe que peu de données pour une paire de langues, le modèle ne peut pas produire de bonnes représentations pour les mots, en particulier pour les mots rares. Une solution courante consiste à réduire la dispersion des données en segmentant les mots en sous-mots, afin de permettre aux mots rares d'avoir une représentation partagée avec d'autres mots. En adoptant une approche différente, nous présentons dans cet article une méthode pour alimenter un réseau NMT avec des intégrations de mots entraînées sur des données monolingues, qui sont combinées avec les intégrations spécifiques aux tâches apprises au cours de la formation. Cette méthode peut tirer parti d'une matrice d'intégration avec un grand nombre de mots, ce qui permet d'étendre le vocabulaire au niveau des mots. Nos expériences sur deux paires de langues montrent de bons résultats pour le scénario de données à faibles ressources typique (jeu de données dans le domaine IWSLT). Nos améliorations constantes par rapport aux lignes de base constituent une preuve positive de la possibilité de tirer parti de modèles pré-entraînés sur des données monolingues en NMT.", 'ja': 'ニューラル機械翻訳（ NMT ）は、機械翻訳の最先端であり、利用可能なデータがかなりある場合に最高のパフォーマンスを示します。 言語ペアのデータがほとんど存在しない場合、モデルは単語、特に希少な単語の良い表現を生成することはできません。 一般的な解決策の1つは、希少な単語が他の単語と共有表現を持つことを可能にするために、単語をサブ単語にセグメント化することによってデータの希少性を減らすことです。 別のアプローチを用いて、この論文では、単一言語データでトレーニングされた単語埋め込みをNMTネットワークに供給する方法を提示します。これは、トレーニング時に学習されたタスク固有の埋め込みと組み合わせられます。 この方法は、膨大な数の単語を含む埋め込みマトリックスを活用することができ、したがって、単語レベルの語彙を拡張することができます。 2つの言語ペアの実験は、典型的な低リソースのデータシナリオ（ IWSLTドメイン内データセット）で良好な結果を示しています。 ベースラインに対する当社の一貫した改善は、NMTでモノリンガルデータに関する事前トレーニングを受けたモデルを活用する可能性についての肯定的な証拠を示しています。', 'zh': '神经机器翻译 (NMT) 者,机器翻译之最新术,当多可数,最为佳性。 言语对数甚少,模形不得为单词(尤希有单词)成文。 常见者解决方案因割单词为子单词以损数疏,以许稀有单词与他单词共之。 本文用异法,于本文中设单语数以练词嵌为NMT网络FMT网络法,其词嵌练时学特定务嵌合。 此可以大单词嵌矩阵,故可以广单词级词汇。 二言之实验,于典刑之低资源(IWSLT 域内数集),其效甚佳。 吾基线之易用于NMT单语数预先训练之极验也。', 'hi': 'तंत्रिका मशीन अनुवाद (एनएमटी) मशीन अनुवाद के लिए कला की स्थिति है, और यह सबसे अच्छा प्रदर्शन दिखाता है जब काफी मात्रा में डेटा उपलब्ध होता है। जब एक भाषा जोड़ी के लिए केवल थोड़ा डेटा मौजूद होता है, तो मॉडल शब्दों के लिए अच्छे प्रतिनिधित्व का उत्पादन नहीं कर सकता है, विशेष रूप से दुर्लभ शब्दों के लिए। एक सामान्य समाधान में शब्दों को उप-शब्दों में विभाजित करके डेटा स्पार्सिटी को कम करना शामिल है, ताकि दुर्लभ शब्दों को अन्य शब्दों के साथ साझा प्रतिनिधित्व करने की अनुमति मिल सके। एक अलग दृष्टिकोण लेते हुए, इस पेपर में हम मोनोलिंगुअल डेटा पर प्रशिक्षित शब्द एम्बेडिंग के साथ एनएमटी नेटवर्क को खिलाने के लिए एक विधि प्रस्तुत करते हैं, जो प्रशिक्षण के समय सीखे गए कार्य-विशिष्ट एम्बेडिंग के साथ संयुक्त होते हैं। यह विधि बड़ी संख्या में शब्दों के साथ एक एम्बेडिंग मैट्रिक्स का लाभ उठा सकती है, जो इसलिए शब्द-स्तर की शब्दावली का विस्तार कर सकती है। दो भाषा जोड़े पर हमारे प्रयोग विशिष्ट कम-संसाधन वाले डेटा परिदृश्य (IWSLT इन-डोमेन डेटासेट) के लिए अच्छे परिणाम दिखाते हैं। बेसलाइन पर हमारे लगातार सुधार एनएमटी में मोनोलिंगुअल डेटा पर पूर्व-प्रशिक्षित मॉडल का लाभ उठाने की संभावना के बारे में एक सकारात्मक सबूत का प्रतिनिधित्व करते हैं।', 'ru': 'Нейронный машинный перевод (НМП) - это самый современный способ машинного перевода, и он показывает наилучшую производительность при наличии значительного объема доступных данных. Когда для языковой пары существует лишь небольшое количество данных, модель не может дать хорошее представление о словах, особенно о редких словах. Одним из распространенных решений является сокращение разрозненности данных за счет сегментации слов на подслова, с тем чтобы редкие слова имели общие представления с другими словами. Используя другой подход, в этой статье мы представляем метод подачи сети НМТ с вложениями слов, обученными на одноязычных данных, которые объединяются с вложениями конкретных задач, изученными во время обучения. Этот метод может использовать матрицу вложений с огромным количеством слов, которые, следовательно, могут расширить словарный запас на уровне слов. Наши эксперименты на двух языковых парах показывают хорошие результаты для типичного сценария с низким объемом ресурсов (IWSLT in-domain dataset). Наши последовательные улучшения по сравнению с базовыми показателями являются положительным доказательством возможности использования моделей, предварительно подготовленных на основе одноязычных данных в НБП.', 'ga': 'Is é an t-aistriúchán meaisín néareolaíoch (NMT) an úrscothacht maidir le haistriúchán meaisín, agus taispeánann sé an fheidhmíocht is fearr nuair a bhíonn méid suntasach sonraí ar fáil. Nuair nach bhfuil ach beagán sonraí ar fáil do phéire teanga, ní féidir leis an tsamhail léiriú maith a thabhairt ar fhocail, go háirithe i gcás focail neamhchoitianta. Is éard atá i réiteach coiteann amháin ná tearcúlacht sonraí a laghdú trí fhocail a dheighilt ina bhfofhocail, ionas gur féidir focail neamhchoitianta a chomhroinnt le focail eile. Agus cur chuige difriúil á ghlacadh againn, cuirimid i láthair sa pháipéar seo modh chun líonra NMT a chothú le leabú focal oilte ar shonraí aonteangacha, a chomhcheanglaítear leis na leabaithe tasc-shonracha a foghlaimíodh ag am oiliúna. Is féidir leis an modh seo maitrís leabaithe a ghiaráil le líon mór focal, rud a fhéadfaidh, mar sin, stór focal leibhéal na bhfocal a leathnú. Léiríonn ár dturgnaimh ar dhá phéire teanga torthaí maithe don ghnáthchás sonraí ar acmhainní ísle (tacar sonraí in-fhearainn IWSLT). Léiríonn ár bhfeabhsuithe comhsheasmhacha thar na bonnlínte cruthúnas dearfach ar an bhféidearthacht samhlacha a réamh-oilte ar shonraí aonteangacha in NMT a ghiaráil.', 'hu': 'A neurális gépi fordítás (NMT) a gépi fordítás legkorszerűbb technológiája, és a legjobb teljesítményt mutatja, ha jelentős mennyiségű adat áll rendelkezésre. Ha csak kevés adat áll rendelkezésre egy nyelvpárra vonatkozóan, a modell nem tud jó szavakat ábrázolni, különösen ritka szavakat. Az egyik közös megoldás az, hogy csökkentse az adatok szűkösségét a szavak alszókra történő szegmentálásával annak érdekében, hogy a ritka szavak megoszthassák a reprezentációkat más szavakkal. Más megközelítést alkalmazva ebben a tanulmányban bemutatunk egy olyan módszert, amely egynyelvű adatokra képzett szóbeágyazásokkal táplálja az NMT hálózatot, amelyek kombinálódnak a képzés során tanult feladatspecifikus beágyazásokkal. Ez a módszer hatalmas számú szóval rendelkező beágyazási mátrixot használhat, így bővítheti a szószintű szókincset. Két nyelvpáron végzett kísérleteink jó eredményeket mutatnak a tipikus alacsony forrásokkal rendelkező adatforgalmi forgatókönyv (IWSLT in-domain adatkészlet) esetében. Az alapértékekhez képest folyamatos fejlesztésünk pozitív bizonyítékot jelent annak lehetőségére, hogy az NMT egynyelvű adatokra előre képzett modelleket használjunk.', 'el': 'Η νευρωνική μηχανική μετάφραση (NMT) είναι η τελευταία λέξη της τεχνολογίας για τη μηχανική μετάφραση, και εμφανίζει την καλύτερη απόδοση όταν υπάρχει σημαντική ποσότητα διαθέσιμων δεδομένων. Όταν υπάρχουν ελάχιστα δεδομένα για ένα γλωσσικό ζεύγος, το μοντέλο δεν μπορεί να παράγει καλές αναπαραστάσεις για λέξεις, ιδιαίτερα για σπάνιες λέξεις. Μια κοινή λύση συνίσταται στη μείωση της σπανιότητας δεδομένων με την κατάτμηση λέξεων σε υπο-λέξεις, προκειμένου να επιτραπεί στις σπάνιες λέξεις να έχουν κοινές αναπαραστάσεις με άλλες λέξεις. Λαμβάνοντας μια διαφορετική προσέγγιση, στην παρούσα εργασία παρουσιάζουμε μια μέθοδο τροφοδοσίας ενός δικτύου με ενσωμάτωση λέξεων εκπαιδευμένες σε μονογλωσσικά δεδομένα, τα οποία συνδυάζονται με τις ενσωματωμένες εργασίες που έχουν μάθει κατά τη διάρκεια της εκπαίδευσης. Αυτή η μέθοδος μπορεί να αξιοποιήσει έναν πίνακα ενσωμάτωσης με έναν τεράστιο αριθμό λέξεων, ο οποίος επομένως μπορεί να επεκτείνει το λεξιλόγιο σε επίπεδο λέξεων. Τα πειράματά μας σε δύο γλωσσικά ζεύγη δείχνουν καλά αποτελέσματα για το τυπικό σενάριο δεδομένων με χαμηλούς πόρους (σύνολο δεδομένων εντός τομέα). Οι συνεπείς βελτιώσεις μας σε σχέση με τις γραμμές βάσης αποτελούν μια θετική απόδειξη για τη δυνατότητα αξιοποίησης μοντέλων προεκπαίδευσης σε μονογλωσσικά δεδομένα σε NMT.', 'ka': 'ნეიროლური მანქანის გადაწყვეტილება (NMT) არის მანქანის გადაწყვეტილების მდგომარეობა, როდესაც მანქანის გადაწყვეტილებისთვის უკეთესი მსგავსი მნიშვნელობა ჩვენ როცა მხოლოდ ცოტა მონაცემები მხოლოდ ენაზის ზოგისთვის არსებობს, მოდელი არ შეიძლება წარმოადგენოთ სიტყვებისთვის, განსაკუთრებით ც ერთი საერთო პასუხი იქნება მონაცემების სიტყვებით სექმენტირებით, რომლებიც შესაძლებელია დამატებული სიტყვებით სხვა სიტყვებით გაყოფილი რედაქტირება. განსხვავებული პროგორმაციის გამოყენება, ამ დომენტში ჩვენ გამოყენებთ NMT ქსელის სიტყვებით, რომელიც მონოლენგური მონაცემებით განაკეთებული სიტყვებით, რომლებიც კომბიზებული მონაცემე ამ პროცემის შესაძლებელია, რომელიც სიტყვების დონე სიტყვებულების შესაძლებელია გადასრულება მარტიკის შესაძლებელია. ჩვენი ექსპერიმენტები ექსპერიმენტების ორივე ენის ზოგებისთვის კარგი შედეგები ჩვენებს ტიპიკური ცოტა რესპორციულ მონაცემების სენარიოს (IWSLT-  ჩვენი კონსტენსტური უფრო მეტი ბაზილური ხაზების შესაძლებლობად მოდელური მონაცემების მონაცემების შესაძლებლობა, რომელიც შესაძლებლობად მოდელური მონაცემების შესაძლებლო', 'it': "La traduzione automatica neurale (NMT) è lo stato dell'arte per la traduzione automatica e mostra le migliori prestazioni quando ci sono una notevole quantità di dati disponibili. Quando esistono solo pochi dati per una coppia linguistica, il modello non può produrre buone rappresentazioni per le parole, in particolare per le parole rare. Una soluzione comune consiste nel ridurre la scarsità dei dati segmentando le parole in sottoparole, in modo da consentire alle parole rare di avere rappresentazioni condivise con altre parole. Adottando un approccio diverso, in questo articolo presentiamo un metodo per alimentare una rete NMT con embedding di parole addestrati su dati monolingue, che sono combinati con embedding task-specific appresi al momento della formazione. Questo metodo può sfruttare una matrice di incorporazione con un numero enorme di parole, che può quindi estendere il vocabolario a livello di parola. I nostri esperimenti su due coppie linguistiche mostrano buoni risultati per il tipico scenario di dati a bassa risorsa (set di dati IWSLT in-domain). I nostri costanti miglioramenti rispetto alle linee di base rappresentano una prova positiva della possibilità di sfruttare modelli pre-formati su dati monolingue in NMT.", 'kk': 'Невралдық машинаның аударуы (NMT) компьютердің аударуының суретінің күйі, және ол қол жеткізген деректердің ең жақсы жылдамдығын көрсетеді. Егер тек тіл екі үшін кішкентай деректер бар болса, үлгі сөздер үшін жақсы түсініктер болмайды, өйткені кейбір сөздер үшін. Бір жалпы шешім деректердің кеңістігін сөздердің ішкі сөздеріне бөліп, басқа сөздермен бөліктеу үшін кеңістігін азайту үшін болады. Бұл қағазда бірнеше тәсілді қолдануға арналған, біз NMT желіне монолингі деректеріне үйренген сөздерді ендіру әдісін таңдаймыз. Бұл тапсырманың ерекше ендіру уақытында үйренген тапсырмаларды бірі Бұл әдіс сөз деңгейінің сөздерін кеңейтуге болады. Екі тіл екеуінің тәжірибелеріміз қалыпты төмен ресурс деректер сценариясының (доменде IWSLT деректер жинағы) жақсы нәтижелерін көрсетеді. Біздің негізгі жолдардың тұрақты жақсартуларымыз NMT монолингі деректерінің алдын- ала оқылған үлгілер үшін оқыту мүмкіндігін дұрыс көрсетеді.', 'lt': 'Neuralinis mašinų vertimas (NMT) yra mašinų vertimo pažangiausia technika ir rodo geriausius rezultatus, kai yra daug duomenų. Kai kalbos poros duomenys yra maži, modelis negali pateikti gerų žodžių, ypač retų žodžių, atvaizdų. One common solution consists in reducing data sparsity by segmenting words into sub-words, in order to allow rare words to have shared representations with other words.  Taikant kitokį požiūrį, šiame dokumente pristatome metodą, pagal kurį galima naudoti NMT tinklą su žodžių įdėjimais, mokomais vienkalbiniais duomenimis, kurie derinami su konkrečiomis užduotimis, mokomomis mokymo metu. Šis metodas gali padidinti įterpiamąją matricą daugeliu žodžių, todėl gali išplėsti žodžių lygį. Mūsų eksperimentai su dviem kalbų poromis rodo gerus tipinio mažų išteklių duomenų scenarijaus (IWSLT domeno duomenų rinkinys) rezultatus. Mūsų nuoseklūs patobulinimai bazinėse linijose yra teigiamas įrodymas apie galimybę panaudoti iš anksto parengtus modelius, naudojančius NMT monokalbinius duomenis.', 'mk': 'Неуралниот машински превод (НМТ) е најсовремен за машински превод, и го покажува најдобриот резултат кога постојат значителни количини на достапни податоци. Кога постојат само мали податоци за еден пар јазици, моделот не може да создаде добри претставувања за зборови, особено за ретки зборови. Едно заедничко решение е намалувањето на скратноста на податоците со сегментирање на зборовите во подзборови, со цел да се овозможи ретките зборови да имаат заеднички претставувања со други зборови. Со различен пристап, во овој документ претставуваме метод за храна на НМТ мрежа со зборови вградени на монојазични податоци, кои се комбинирани со задачи специфични вградувања научени во време на обука. Овој метод може да ја искористи вградената матрица со огромен број зборови, што може да го прошири речникот на зборови. Нашите експерименти на два пара јазици покажуваат добри резултати за типично сценарио со ниски ресурси на податоци (IWSLT in-domain dataset). Нашите постојани подобрувања во однос на базите претставуваат позитивен доказ за можноста да се искористат модели предобучени на монојазични податоци во НМТ.', 'ms': 'Terjemahan mesin saraf (NMT) adalah kemajuan seni untuk terjemahan mesin, dan ia menunjukkan prestasi terbaik apabila terdapat jumlah data yang cukup tersedia. Apabila hanya sedikit data wujud untuk pasangan bahasa, model tidak dapat menghasilkan perwakilan yang baik untuk perkataan, terutama untuk perkataan langka. Satu penyelesaian umum terdiri dalam mengurangi kecerapan data dengan segmen perkataan ke sub-perkataan, untuk membenarkan perkataan langka mempunyai perwakilan berkongsi dengan perkataan lain. Mengambil pendekatan yang berbeza, dalam kertas ini kami perkenalkan kaedah untuk memberi makan rangkaian NMT dengan penyembedding perkataan dilatih pada data monobahasa, yang digabungkan dengan penyembedding khusus tugas yang belajar pada masa latihan. Kaedah ini boleh menggunakan matriks penyembedding dengan sejumlah perkataan yang besar, yang sebab itu boleh memperluas vokbulari aras perkataan. Eksperimen kami pada dua pasangan bahasa menunjukkan keputusan yang baik untuk skenario data sumber rendah biasa (set data dalam domain IWSLT). Perbaikan konsisten kita di atas garis dasar mewakili bukti positif tentang kemungkinan untuk menggunakan model yang dilatih-dilatih pada data monobahasa dalam NMT.', 'ml': 'മെഷിന്\u200d പരിഭാഷക്കുള്ള കലാകാര്യത്തിന്റെ സ്ഥിതിയാണ് ന്യൂറല്\u200d മെഷീന്\u200d പരിഭാഷ (NMT), വലിയ വിവരങ്ങള്\u200d ലഭ്യമാകുമ്പോള്\u200d അതിന്റ ഭാഷ ജോഡികള്\u200dക്ക് മാത്രമേ ചെറിയ വിവരങ്ങള്\u200d ഉണ്ടാവുകയുള്ളൂ, മോഡല്\u200d വാക്കുകള്\u200dക്ക് നല്ല പ്രതിനിധികള്\u200d ഉണ്ടാക്കാന്\u200d  മറ്റു വാക്കുകളുമായി പങ്കുചേര്\u200dക്കാന്\u200d കുറഞ്ഞ വാക്കുകള്\u200d പങ്കുചേര്\u200dക്കാന്\u200d അനുവദിക്കുന്നതിനാല്\u200d വാക്കുകള്\u200d വേര്\u200dതിരിച ഈ പേപ്പറില്\u200d വ്യത്യസ്ത മാര്\u200dഗങ്ങള്\u200d എടുത്താല്\u200d പരിശീലന സമയത്ത് പഠിക്കുന്ന ജോലിയുടെ പ്രത്യേകിച്ചിരിക്കുന്ന ജോലിയുടെ പ്രത്യേകിച്ചുള്ള ഈ രീതിയില്\u200d ഒരു വലിയ വാക്കുകളോടൊപ്പം മാറ്റിക്സിന്റെ മാറ്റിക്സിന് ഉപയോഗിക്കാന്\u200d സാധിക്കും. അതുകൊണ്ട് വാക്ക്-  രണ്ടു ഭാഷ ജോടികളില്\u200d നമ്മുടെ പരീക്ഷണങ്ങള്\u200d സാധാരണ കുറഞ്ഞ വിഭവങ്ങളുടെ ഡേറ്റാ സിനേറ്റിയോയില്\u200d നല്ല ഫലങ്ങള്\u200d കാണിക്കുന്ന അടിസ്ഥാനങ്ങളില്\u200d നമ്മുടെ സ്ഥിരമായ മെച്ചപ്പെടുത്തിയിരിക്കുന്നു. നമ്മുടെ മെച്ചപ്പെട്ട മോഡലുകള്\u200dക്ക് മുമ്പ് പരിശീലനം നല', 'mn': 'Цэцгийн машин хөгжүүлэлт (NMT) нь машины хөгжүүлэлтийн урлагийн байдал юм. Энэ нь маш олон өгөгдлийн хэмжээ байх үед хамгийн сайн үйл ажиллагааг харуулдаг. Зөвхөн хэл хоёрын жижиг өгөгдлийн хувьд байхад загвар нь үгсийн хувьд сайн үзүүлэлт гаргаж чадахгүй, ялангуяа ховор үгсийн хувьд сайн үзүүлэлт гаргаж ча Нэг ерөнхий шийдэл нь өгөгдлийн хэмжээсүүдийг бусад үгийг хэлбэрээр хэлбэрээр багасгах боломжтой болно. Энэ цаасан дээр өөр арга хэрэглэхэд бид NMT сүлжээнд нэг хэлний өгөгдлийн сургалтыг ашиглах арга зааж өгдөг. Энэ нь сургалтын цаг хугацаанд сурсан ажлын тодорхойлолтой сүлжээтэй холбоотой. Энэ арга нь маш олон үгсийг нэмэгдүүлэх матриксийг ашиглаж чадна. Тэгэхээр үгсийн хэмжээг нэмэгдүүлж чадна. Хоёр хэл хоёр туршилтын туршилт нь энгийн бага хамааралтай өгөгдлийн хувилбарын сайн үр дүнг харуулдаг. Бидний суурь шугам дээрх тогтмол сайжруулалт нь NMT-ийн ганц хэлний өгөгдлийн талаар сургалтын өмнө сургалтын загваруудыг ашиглах боломжийг эерэг баталгаатай байдаг.', 'mt': 'Neural machine translation (NMT) is the state of the art for machine translation, and it shows the best performance when there is a considerable amount of data available.  When only little data exist for a language pair, the model cannot produce good representations for words, particularly for rare words.  Soluzzjoni komuni waħda tikkonsisti fit-tnaqqis tal-iskarsezza tad-dejta billi l-kliem jiġi segmentat f’subkliem, sabiex il-kliem rari jkun jista’ jkollhom rappreżentazzjonijiet kondiviżi ma’ kliem ieħor. B’approċċ differenti, f’dan id-dokument qed nippreżentaw metodu għall-alimentazzjoni ta’ netwerk NMT b’inkorporazzjonijiet ta’ kliem imħarrġa fuq dejta monolingwi, li huma kkombinati ma’ inkorporazzjonijiet speċifiċi għall-kompiti li jitgħallmu waqt it-taħriġ. Dan il-metodu jista’ jwassal għal matriċi inkorporattiva b’għadd kbir ta’ kliem, li għalhekk jista’ jestendi l-vokabulari tal-livell tal-kliem. L-esperimenti tagħna fuq żewġ pari lingwistiċi juru riżultati tajbin għax-xenarju tipiku ta’ dejta b’riżorsi baxxi (IWSLT in-domain dataset). It-titjib konsistenti tagħna fuq il-linji bażi jirrappreżenta prova pożittiva dwar il-possibbiltà li jintużaw mudelli mħarrġa minn qabel fuq dejta monolingwi fl-NMT.', 'no': '@ info Når berre lite data finst for eit språkopar, kan modellen ikkje laga gode representasjonar for ord, spesielt for rare ord. Ein vanleg løysing inneheld i å redusera datasparsitet ved å segmentera ord til underord, slik at det kan tillata rare ord å ha delte representasjonar med andre ord. Når vi ta eit anna tilnærming, i denne papiret presenterer vi ein metode for å køyre eit NMT- nettverk med ordinnbygging som treng på monospråk- data, som er kombinert med oppgåvespesifikke innbygging som er lært på treningstid. Denne metoden kan leverast ein innbygging matrise med eit stor tal ord, som derfor kan utvida ordnivåordordordordboka. Eksperimentane våre på to språkparar viser gode resultat for den typiske låg ressursert datascenarioen (IWSLT i domenedataset). Våre konsekvente forbedringar over baselinjene representerer eit positivt bevis om muligheten for å levera modeller som er forelært på monospråk-data i NMT.', 'pl': 'Neuronalne tłumaczenie maszynowe (NMT) jest najnowocześniejszym stanem techniki w zakresie tłumaczenia maszynowego i wykazuje najlepszą wydajność, gdy jest dostępna znaczna ilość danych. Kiedy dla pary językowej istnieje tylko niewiele danych, model ten nie może wytwarzać dobrych reprezentacji słów, zwłaszcza dla rzadkich słów. Jedno z wspólnych rozwiązań polega na zmniejszeniu ograniczenia ilości danych poprzez segmentowanie słów na podsłowa, tak aby rzadkie słowa mogły dzielić się reprezentacjami z innymi słowami. Przyjmując inne podejście, w niniejszym artykule przedstawiamy metodę zasilania sieci NMT osadzeniami słów przeszkolonymi na danych jednojęzycznych, które są połączone z osadzeniami specyficznymi dla zadań nauczonymi w czasie szkolenia. Metoda ta może wykorzystać matrycę osadzania z ogromną liczbą słów, co może zatem rozszerzyć słownictwo na poziomie słowa. Nasze eksperymenty na dwóch parach językowych pokazują dobre wyniki dla typowego scenariusza danych o niskich zasobach (IWSLT in-domain dataset). Nasze konsekwentne ulepszenia w stosunku do linii bazowych stanowią pozytywny dowód na możliwość wykorzystania modeli wstępnie przeszkolonych na danych jednojęzycznych w NMT.', 'ro': 'Traducerea automată neurală (NMT) este cea mai modernă tehnologie pentru traducerea automată și arată cele mai bune performanțe atunci când există o cantitate considerabilă de date disponibile. Atunci când există doar puține date pentru o pereche de limbi, modelul nu poate produce reprezentări bune pentru cuvinte, în special pentru cuvintele rare. O soluție comună constă în reducerea cantității de date prin segmentarea cuvintelor în subcuvinte, pentru a permite cuvintelor rare să aibă reprezentări comune cu alte cuvinte. Având o abordare diferită, în această lucrare prezentăm o metodă de alimentare a rețelei NMT cu încorporări de cuvinte instruite pe date monolingve, care sunt combinate cu încorporările specifice sarcinilor învățate în timpul instruirii. Această metodă poate utiliza o matrice de încorporare cu un număr imens de cuvinte, ceea ce poate, prin urmare, extinde vocabularul la nivel de cuvânt. Experimentele noastre pe două perechi de limbi arată rezultate bune pentru scenariul tipic de date cu resurse reduse (set de date IWSLT in-domain). Îmbunătățirile noastre constante față de liniile de referință reprezintă o dovadă pozitivă cu privire la posibilitatea de a utiliza modele pre-instruite pe date monolingve în NMT.', 'sr': 'Neuralni prevod mašine (NMT) je stanje umetnosti za prevod mašine, i pokazuje najbolji izvod kada postoji dostupna količina podataka. Kada postoje samo mali podaci za parove jezika, model ne može proizvesti dobre predstave za reči, posebno za rijetke reči. Jedno zajedničko rješenje se sastoji u smanjenju rezerviteta podataka segmentiranjem reèi u podreèi, kako bi rijetke reèi mogle da podijele predstave drugim reèima. Uzimajući drugačiji pristup, u ovom papiru predstavljamo metodu da nahranimo NMT mrežu sa riječima obučenim na monojezičkim podacima, koje su kombinirane sa određenim zadacima koje su naučene u treningu. Ova metoda može uticati na ugrađenu matricu sa ogromnim brojem reči, koja stoga može proširiti reč na nivou reči. Naši eksperimenti na dva jezička parova pokazuju dobre rezultate za tipični scenario niskog resursa podataka (IWSLT u domenu podataka). Naše konsekventne poboljšanje na osnovnim linijama predstavljaju pozitivan dokaz o mogućnosti primjene modela predobučenih na monojezičkim podacima u NMT-u.', 'si': 'න්\u200dයුරල් යන්ත්\u200dරය අවවාදය (NMT) තමයි යන්ත්\u200dරය අවවාදය සඳහා කලාවේ ස්ථිතිය, ඒක පෙන්වන්නේ හොඳම ක්\u200dරියාපනයක් තියෙන භාෂා ජෝඩයක් වෙනුවෙන් පොඩි දත්ත තියෙන්නේ විතරයි, මොඩේල් වචන් වෙනුවෙන් හොඳ ප්\u200dරතිරූපය නි එක සාමාන්\u200dය විස්තරයක් තියෙන්නේ දත්ත ස්පාර්සිටිය අඩු කරනවා වචන ස්පාර්සිට් වචන වලින්, අනිත් වචන වලින් සාමාන්\u200d වෙනස් ප්\u200dරවේශයක් අරගෙන, මේ පැත්තේ අපි NMT ජාලයෙක් කිරීමට ප්\u200dරවේශයක් තියෙනවා එක්ක භාෂාවික දත්ත සඳහා පුහුණු වචන පද්ධතියක් ස මේ විධානය පුළුවන් විශාල වචනයක් ලොකු සංඛ්යාවක් සඳහා ඇන්වීම් මැට්\u200dරික්ස් එක ප්\u200dරමාණය කරන්න, ඒකෙන් වච අපේ පරීක්ෂණය භාෂා දෙකක් වලින් හොඳ ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dර අපේ සාමාන්\u200dය විශ්වාසයක් නිර්මාණය කරන්න පුළුවන් සාක්ෂියක් තියෙනවා NMT වල මොඩේල් එක්ක භාෂාවක් දත්තේ ප්\u200dරධානය', 'so': 'Turjumista gaadiidka (NMT) waa xaalada farshaxanka ee turjumista machine, wuxuuna muujiyaa farshaxanka ugu wanaagsan marka ay jirto qiyaastii aad u badan. Markii macluumaad yar oo kaliya ay ku jiraan labo luqad, tusaale ahaan ma sameyn karo noocyo wanaagsan oo ku qoran hadal, khusuusan erayo aad u yarayd. Isku-qabsashada isku mid ah waxaa ka mid ah koob-hoos-dhigidda macluumaadka, si uu u fasaxo hadallada ugu yaraan inay ku qaybsadaan noocyada hadallada kale. Dhaqdhaqaaq kala duduwan ayaannu warqaddan ku qoraynaa qaab a an ku quudinno shabakadda NMT, taasoo lagu baran karo macluumaadka afka ah, kuwaas oo lagu xiriira goobta waxbarashada lagu baray. This method can leverage an embedding matrix with a huge number of words, which can therefore extend the word-level vocabulary.  Imtixaanadayada labada luqadood ee labo luqadood ah waxay tustaa resulto wanaagsan oo u eg sawirada hoos-resourceed (IWSLT in-domain dataset). Horumarinayada ku socota aasaaska waxaa ka mid ah caddeynta suurtagalka ah oo ku saabsan suurtagalka u baaraandegista modelalka afka ah ee NMT.', 'sv': 'Neural maskinöversättning (NMT) är den senaste tekniken för maskinöversättning och visar bästa prestanda när det finns en betydande mängd data tillgängliga. När det bara finns få data för ett språkpar kan modellen inte producera bra representationer för ord, särskilt för sällsynta ord. En gemensam lösning består i att minska datasparheten genom att segmentera ord i underord, så att sällsynta ord kan ha delade representationer med andra ord. Med ett annat tillvägagångssätt presenterar vi i denna uppsats en metod för att mata ett NMT-nätverk med ordinbäddningar utbildade på enspråkiga data, som kombineras med uppgiftsspecifika inbäddningar lärde sig vid utbildningstiden. Denna metod kan utnyttja en inbäddad matris med ett stort antal ord, vilket därför kan utöka ordförrådet på ordnivå. Våra experiment på två språkpar visar goda resultat för det typiska lågresursdatascenariot (IWSLT in-domain dataset). Våra konsekventa förbättringar jämfört med baslinjerna är ett positivt bevis på möjligheten att utnyttja modeller som är förberedda på enspråkiga data i NMT.', 'ta': 'கணினி மொழிபெயர்ப்பிற்கான நெயுரிய இயந்திர மொழிபெயர்ப்பு (NMT) என்பது இயந்திரம் மொழிபெயர்ப்பின் நிலையாகும், அது கிடைக் மொழி ஜோடிக்கு மட்டும் சிறிய தகவல் இருக்கும் போது, மாதிரி சொற்களுக்கு நன்றான பிரிவினைகளை உருவாக்க முடியாது,  ஒரு பொதுவான தீர்வு மற்ற வார்த்தைகளுடன் சிறிய வார்த்தைகளை துணை வார்த்தைகளால் பிரித்துக் கொள்ளும் தகவல் வெளியீட்டை கு ஒரு வேறு வழியை எடுத்து, இந்த காகிதத்தில் நாம் ஒரு NMT பிணையத்தை வழங்க ஒரு முறையைக் கொண்டு ஒரு முறையை வழங்குகிறோம் மொழிமொழிகளில் பயிற்சியான மிகப்பெரிய வார்த்தைகளுடன் இந்த முறைமையில் உள்ளிடப்பட்ட மதிப்பெண்களை சேர்க்க முடியும், அது வார்த்தை- மட்டத்தின் ச இரண்டு மொழி ஜோடிகளில் எங்கள் சோதனைகள் வழக்கமான குறைந்த தரவு காட்சிக்கு நல்ல முடிவு அடிப்படைகளில் எங்கள் முன்னேற்றங்கள் முன்னேற்றப்பட்டது என்எம்டியில் உள்ள மொன்மொழி தரவின் மூலம் பயிற்சிக்கப்பட்ட மாதிரிகளை பற', 'ur': 'نیورال ماشین ترجمہ (NMT) ماشین ترجمہ کے لئے آرت کا موقعیت ہے، اور یہ بہترین فعالیت دکھاتا ہے جبکہ ایک بہترین اندازہ ڈیٹا موجود ہوتا ہے۔ جب صرف ایک زبان جوڑے کے لئے چھوٹی ڈیٹا موجود ہوتی ہے، موڈل کلمات کے لئے بہترین نمونات پیدا نہیں کرتا، مخصوصاً بہترین کلمات کے لئے۔ ایک مشترک حل ہے کہ کلمات کو زیر کلمات میں تقسیم کرنے کے ذریعہ ڈیٹا سپرسیٹی کاٹ دینے میں، اس لئے کہ بہت کم کلمات کو دوسرے کلمات کے ساتھ تقسیم کرنے کی اجازت دیں. ہم اس کاغذ میں ایک مختلف طریقہ لیتے ہیں کہ ایک NMT نیٹ ورک کے ذریعے ایک ایسے طریقہ پیش کریں جو ایک زبان کے ڈاٹوں پر آموزش کی جاتی ہیں، جو ٹاکس-خاص ایمبڈینگ کے ساتھ آموزش وقت میں سکھائی جاتی ہیں۔ یہ طریقہ ایک مٹریکس کو ایک بڑی تعداد کلمات کے ساتھ اضافہ کر سکتا ہے، جس کی وجہ سے کلمات-سطح کا کلمات پھیلا سکتا ہے. دو زبان جوڑوں پر ہماری آزمائش بہترین نتیجے دکھاتی ہیں عام کم رسسورس ڈیٹ سیناریو کے لئے۔ ہمارے پائیس لینوں کے بارے میں ایک ثابت ثابت ہوتے ہیں کہ NMT میں ایک زبان سے پہلے آموزش کئے ہوئے موڈلوں کے لئے موڈلوں کو استعمال کرنے کی امکانات کے بارے میں ایک ثابت ثابت ہوتی ہے.', 'uz': "Name Faqat bir til ikki xil maʼlumot mavjud boʻlsa, model so'zlar uchun juda yaxshi taʼminlovchilar yaratib boʻlmaydi, ammo qisqa so'zlar uchun. Бир умумий ҳаёт маълумотнинг чегарасини камайтириш мақсадида, бутун сўзларга ўзгартириш сўзлари билан маълумотларни камайтириш учун фойдаланиш мумкин. Bu qogʻozda boshqa usulni olib tashlash uchun, biz o'sha vazifa vaqtda o'rganilgan vazifa haqida o'rganish vazifalar bilan birlashtirilgan so'zlar yordamida NMT tarmoqni saqlash usulini koʻrsatish. Name IWSLT maʼlumotlar soni (domen maʼlumotlar tarkibi IWSLT) ga yaxshi natijalarimizni ko'rsatadi. Bizning asosidagi davomida o'zgarishlarimiz NMT'ning monolingual maʼlumotdagi modellarni o'rganish imkoniyatini yaxshi hujjat qiladi.", 'vi': 'Dịch về máy thần kinh (NMB) là thời đại của kỹ thuật dịch chuyển máy, và nó thể hiện hiệu quả tốt nhất khi có một số lượng đáng kể dữ liệu có sẵn. Khi chỉ có ít dữ liệu tồn tại cho một cặp ngôn ngữ, mô hình không thể tạo ra những biểu tượng tốt cho từ, đặc biệt cho những từ hiếm. Một giải pháp chung là giảm tập tin của mình bằng cách chia sẻ các từ thành chữ ngầm, để dễ dàng chia sẻ các diễn văn với những từ khác. Với phương pháp khác, trong bài báo này chúng tôi có một phương pháp nuôi dưỡng mạng lưới NMT bằng những từ ngữ được đào tạo trên dữ liệu ngôn ngữ, kết hợp với những bài học đặc biệt trong thời gian huấn luyện. Phương pháp này có thể dùng một ma trận nhúng bằng một số từ rất lớn, mà có thể mở rộng ngôn ngữ cấp từ. Các thí nghiệm trên hai cặp ngôn ngữ cho thấy kết quả tốt cho kịch bản dữ liệu cung cấp thấp đặc trưng (IWSLT trong miền dữ liệu). Sự cải tiến liên tục của chúng ta trên đường hầm là một bằng chứng tích cực về khả năng sử dụng các mẫu được đào tạo trước về dữ liệu ngôn ngữ trong NMB.', 'bg': 'Невровият машинен превод (НМТ) е най-съвременното изкуство в машинния превод и показва най-доброто представяне, когато има значително количество данни. Когато има малко данни за езикова двойка, моделът не може да създаде добри представи за думи, особено за редки думи. Едно общо решение се състои в намаляване на оскъдността на данните чрез сегментиране на думите в поддуми, за да се позволи на редките думи да имат споделени изображения с други думи. По различен подход, в настоящата статия представяме метод за захранване на ММТ мрежа с вграждания на думи, обучени върху едноезични данни, които се комбинират със специфичните за задачата вграждания, научени по време на обучението. Този метод може да използва вградена матрица с огромен брой думи, което следователно може да разшири речника на ниво дума. Нашите експерименти с две езикови двойки показват добри резултати за типичния сценарий с ниски ресурси от данни (вътрешен набор от данни). Нашите последователни подобрения спрямо базовите линии представляват положително доказателство за възможността за използване на модели, предварително обучени върху едноезични данни в НМТ.', 'nl': 'Neural machine translation (NMT) is de state of the art voor machinevertaling en laat de beste prestaties zien wanneer er een aanzienlijke hoeveelheid gegevens beschikbaar is. Wanneer er maar weinig gegevens bestaan voor een taalpaar, kan het model geen goede representaties voor woorden produceren, vooral voor zeldzame woorden. Een gemeenschappelijke oplossing bestaat in het verminderen van dataschaarste door woorden in subwoorden te segmenteren, zodat zeldzame woorden representaties met andere woorden kunnen delen. Met een andere aanpak presenteren we in dit artikel een methode om een NMT netwerk te voeden met woord embeddings getraind op eentalige data, die worden gecombineerd met de taakspecifieke embeddings die tijdens de training zijn geleerd. Deze methode kan gebruik maken van een embedding matrix met een groot aantal woorden, waardoor de woordenschat op woordniveau kan worden uitgebreid. Onze experimenten met twee taalparen tonen goede resultaten voor het typische low-resourced data scenario (IWSLT in-domain dataset). Onze consequente verbeteringen ten opzichte van de basislijnen zijn een positief bewijs voor de mogelijkheid om modellen te gebruiken die vooraf getraind zijn op eentalige gegevens in NMT.', 'da': 'Neural maskinoversættelse (NMT) er den nyeste teknologi inden for maskinoversættelse, og den viser den bedste ydeevne, når der er en betydelig mængde data til rådighed. Når der kun findes få data for et sprogpar, kan modellen ikke producere gode repræsentationer for ord, især for sjældne ord. En fælles løsning består i at reducere dataspartheden ved at segmentere ord i underord, så sjældne ord kan have delt repræsentationer med andre ord. Med en anden tilgang præsenterer vi i denne artikel en metode til at fodre et NMT-netværk med ordindlejringer trænet på ensprogede data, som kombineres med de opgavespecifikke indlejringer, der blev lært på uddannelsestidspunktet. Denne metode kan udnytte en indlejringsmatrix med et stort antal ord, hvilket derfor kan udvide ordforrådet på ordniveau. Vores eksperimenter på to sprogpar viser gode resultater for det typiske datascenario med lav ressource (IWSLT in-domain datasæt). Vores konsekvente forbedringer i forhold til basislinjerne repræsenterer et positivt bevis på muligheden for at udnytte modeller, der er præ-trænet på ensprogede data i NMT.', 'de': 'Neuronale maschinelle Übersetzung (NMT) ist der Stand der Technik für die maschinelle Übersetzung und zeigt die beste Leistung, wenn eine beträchtliche Datenmenge verfügbar ist. Wenn nur wenige Daten für ein Sprachpaar vorliegen, kann das Modell keine guten Darstellungen für Wörter erzeugen, insbesondere für seltene Wörter. Eine gemeinsame Lösung besteht darin, die Datensparsamkeit zu reduzieren, indem Wörter in Unterwörter unterteilt werden, damit seltene Wörter Repräsentationen mit anderen Wörtern teilen können. Mit einem anderen Ansatz stellen wir in diesem Beitrag eine Methode vor, um ein NMT-Netzwerk mit Worteinbettungen zu versorgen, die auf einsprachigen Daten trainiert werden, die mit den aufgabenspezifischen Einbettungen kombiniert werden, die während der Schulungszeit gelernt wurden. Diese Methode kann eine Einbettungsmatrix mit einer großen Anzahl von Wörtern nutzen, die somit den Wortschatz auf Wortebene erweitern kann. Unsere Experimente an zwei Sprachpaaren zeigen gute Ergebnisse für das typische Low-Resourced Data Szenario (IWSLT In-Domain Dataset). Unsere konsequenten Verbesserungen gegenüber den Basislinien sind ein positiver Beweis für die Möglichkeit, Modelle zu nutzen, die auf einsprachigen Daten in NMT vortrainiert wurden.', 'hr': 'Neuralni prevod strojeva (NMT) je stanje umjetnosti za prevod strojeva i pokazuje najbolji učinkovit kada postoji dostupna količina podataka. Kada postoje samo mali podaci za parove jezika, model ne može proizvesti dobre predstave za riječi, posebno za rijetke riječi. Jedno zajedničko rješenje se sastoji u smanjenju rezervnosti podataka podriječjima riječima kako bi rijetke riječi mogle dijeliti predstave s drugim riječima. Uzimajući drugačiji pristup, u ovom papiru predstavljamo metodu za hranjenje NMT mreže riječima uključenim na monojezičke podatke, koje su kombinirane s određenim uključenim na vrijeme obuke. Ova metoda može utjecati na ugrađenu matricu sa ogromnim brojem riječi, što stoga može proširiti riječ na razini riječi. Naši eksperimenti na dva jezička parova pokazuju dobre rezultate za tipični scenario niskog resursa podataka (IWSLT u domenu podataka). Naše konsekventne poboljšanje na osnovnim linijama predstavljaju pozitivan dokaz o mogućnosti primjene modela predobučenih na monojezičkim podacima u NMT-u.', 'id': 'Terjemahan mesin saraf (NMT) adalah state of the art untuk terjemahan mesin, dan menunjukkan prestasi terbaik ketika ada jumlah data yang cukup tersedia. Ketika hanya sedikit data yang ada untuk pasangan bahasa, model tidak dapat menghasilkan representation yang baik untuk kata-kata, terutama untuk kata-kata langka. Satu solusi umum terdiri dalam mengurangi kecepatan data dengan segmen kata-kata ke sub-kata, untuk memungkinkan kata-kata langka untuk memiliki representation berbagi dengan kata-kata lain. Mengambil pendekatan yang berbeda, dalam kertas ini kami mempersembahkan metode untuk memberi makan jaringan NMT dengan pembangunan kata dilatih pada data monobahasa, yang digabungkan dengan pembangunan spesifik tugas yang belajar pada waktu latihan. Metode ini dapat mengaktifkan matriks penyembedding dengan jumlah besar kata, yang sehingga dapat memperluas vocabulari tingkat kata. Eksperimen kami pada dua pasangan bahasa menunjukkan hasil yang baik untuk skenario data tipis dengan sumber daya rendah (dataset dalam domain IWSLT). Our consistent improvements over the baselines represent a positive proof about the possibility to leverage models pre-trained on monolingual data in NMT.', 'fa': 'ترجمه ماشین عصبی (NMT) موقعیت هنری برای ترجمه ماشین است و بهترین فعالیت را نشان می\u200cدهد وقتی مقدار زیادی از داده\u200cهای موجود موجود باشد. وقتی فقط داده های کوچک برای جفت زبان وجود دارد، مدل نمی تواند نمایش های خوب برای کلمات تولید کند، مخصوصا برای کلمات نادر. یک راه حل مشترک در کاهش پاکیزه داده\u200cها با جدا کردن کلمات به زیر کلمات است تا اجازه دهد کلمات نادر که نمایش\u200cهای مشترک با کلمات دیگر باشند. با دستور متفاوتی در این کاغذ، یک روش برای تغذیه شبکه NMT را با کلمه\u200cهای آموزش بر داده\u200cهای یک زبان آموزش داده می\u200cکنیم، که در زمان آموزش آموزش آموزش داده می\u200cشوند، با وسیله\u200cهای ویژه\u200cای برای کار متفاوت می\u200cشوند. این روش می تواند یک ماتریس وارد کردن با تعداد بسیار بزرگ کلمات را تحت تاثیر قرار دهد که می تواند کلمات سطح کلمات را گسترش دهد. آزمایش\u200cهای ما در دو جفت زبان نتایج خوب برای سناریو داده\u200cهای معمولی پایین (IWSLT در مجموعه داده\u200cهای دامنی) نشان می\u200cدهند. توسعه\u200cهای همیشگی ما در خطوط پایین\u200cها ثابت مثبتی در مورد احتمال پیش\u200cآموزش مدل\u200cها در داده\u200cهای یک زبان در NMT نشان می\u200cدهند.', 'ko': '신경 기계 번역(Neural machine translation, NMT)은 기계 번역의 최신 기술로 대량의 사용 가능한 데이터가 있을 때 가장 좋은 성능을 나타낸다.한 언어가 아주 적은 데이터만 가지고 있을 때, 이 모델은 단어, 특히 보기 드문 단어를 잘 나타낼 수 없다.흔히 볼 수 있는 해결 방안은 단어를 하위 단어로 나누어 데이터의 희소성을 줄여 희귀 단어가 다른 단어와 표현을 공유하도록 하는 것이다.다른 방법으로 본고는 단어 데이터에 단어 삽입을 훈련하고 훈련할 때 학습하는 임무를 결합시켜 특정한 삽입을 하여 NMT 네트워크에 정보를 제공하는 방법을 제시했다.이런 방법은 대량의 단어를 포함하는 삽입 행렬을 이용할 수 있기 때문에 단어급 어휘표를 확장할 수 있다.두 언어에 대한 우리의 실험은 전형적인 저자원 데이터 장면(IWSLT in domain dataset)에 대한 우리의 실험 결과가 매우 좋다는 것을 보여준다.우리가 기선에 대한 지속적인 개선은 적극적인 증거로 NMT에서 단어 데이터를 이용하여 미리 훈련하는 모델의 가능성을 증명했다.', 'tr': "NMT, maşynyň terjimesi üçin sanat taýýarlamanyň durumyndyr we ol maşynyň terjimesi üçin iň gowy täsirimi görkezýär. Diňe kiçi maglumat bir dil çifti üçin bar bolsa, nusga sözler üçin gowy surat çykyp bilmeýär, ýöne nadir sözler üçin. Biri ortak çözüm, sözleri altı sözlere bölmek üzere verilen sparsiteleri azaltmak üçin nadir sözleri başka sözlere paylaşırlar. Bu kagyzda başga bir nusga almak üçin biz NMT şebekesini monodil maglumatlarda okuwçylan söz integrasy bilen üýtgetmek üçin bir täze görkeýäris. Bu käze wagtda öwrenmeli işiň takykly integrasy bilen birleştirilýär. Bu yöntem içeri giriş matrisini büyük bir sayı kelimelerle etkinleştirebilir ki bu yüzden kelime seviyesi sözlerini uzatabilir. Biziň iki dil çiftlikde synanyşymyz dürli kaynaklı maglumat senaryony üçin gowy netijeleri görkezýär. Bizim baz hatlarımızdaki sürekli gelişmelerimiz, NMT'de öňünden öňünden öňünden öňünden öňünden öňünden öňünden öňünden öňünden gelen modelleri etkinleştirme mümkinçiligi pozitif kanıtlar.", 'sw': 'Tafsiri ya mashine ya asili (NMT) ni hali ya sanaa kwa kutafsiri mashine, na inaonyesha ufanisi bora zaidi pale kuna kiasi cha data kinachopatikana. Wakati takwimu chache tu zinapo kwa ajili ya lugha mbili, mfano hauwezi kutengeneza maoni mazuri kwa maneno, hasa kwa maneno nadra. suluhisho moja la kawaida linajumuisha kupunguza kuongezeka kwa takwimu kwa kuchagua maneno katika maneno ya chini, ili kuruhusu maneno nadra ya kushirikisha uwakilishi kwa maneno mengine. Kuchukua mbinu tofauti, katika gazeti hili tunaweka mbinu ya kulisha mtandao wa NMT wenye ujumbe wa maneno yanayofundishwa na taarifa za lugha za kiutaifa, ambazo zinaunganishwa na ujumbe maalum wa kazi zilizojifunza wakati wa mafunzo. This method can leverage an embedding matrix with a huge number of words, which can therefore extend the word-level vocabulary.  Majaribio yetu katika viwili vya lugha viwili yanaonyesha matokeo mazuri kwa kituo cha taarifa cha kawaida cha chini cha rasilimali (IWSLT katika seti ya taarifa za ndani). Maendeleo yetu yanayoendelea zaidi ya misingi yanawakilisha ushahidi chanya kuhusu uwezekano wa kutumia miundo mbili zilizofundishwa kabla kwa takwimu za lugha za kimonolinguli nchini NMT.', 'af': "Neurale masjien vertaling (NMT) is die staat van die kuns vir masjien vertaling, en dit vertoon die beste prestasie wanneer daar 'n aansienlike hoeveelheid data beskikbaar is. Wanneer slegs klein data bestaan vir 'n taal paar, kan die model nie goeie voorstellings vir woorde produseer nie, veral vir selfde woorde. Een gemeenskaplike oplossing bestaan in die reduksie van data sparsiteit deur die segmentering van woorde in sub-woorde, om rare woorde te toelaat om gedeelde voorstellings met ander woorde te hê. As ons 'n ander toegang neem, in hierdie papier voorsien ons 'n metode om 'n NMT netwerk te voer met woord inbêdings wat op monolinglike data opgeleer is, wat word gekombineer met die opdragspesifieke inbêdings wat geleer word by onderwerp tyd. Hierdie metode kan 'n inbêring matriks met 'n groot aantal woorde verwyder, wat dan kan die woord-vlak woordeboek uitbrei. Ons eksperimente op twee taal pare vertoon goeie resultate vir die tipiese lae- hulpbron data scenario (IWSLT in- domein dataset). Ons konsistente verbeteringe oor die basis lyne verteenwoordig 'n positiewe bevestig oor die moontlik om modele vooraf-opgelei te maak op monolinglike data in NMT.", 'sq': 'Përkthimi i makinës nervore (NMT) është më i miri për përkthimin e makinës dhe tregon performancën më të mirë kur ka një sasi të konsiderueshme të dhënash në dispozicion. When only little data exist for a language pair, the model cannot produce good representations for words, particularly for rare words.  Një zgjidhje e përbashkët përbëhet në reduktimin e shpejtësisë së të dhënave duke segmentuar fjalët në nënfjalë, me qëllim që fjalët e rralla të kenë përfaqësime të përbashkëta me fjalë të tjera. Duke marrë një qasje të ndryshme, në këtë letër ne paraqesim një metodë për të ushqyer një rrjet NMT me përfshirje fjalësh të stërvitura në të dhënat monogjuhësore, të cilat kombinohen me përfshirjet specifike të detyrës mësuar në kohën e stërvitjes. Kjo metodë mund të përdorë një matricë përfshirëse me një numër të madh fjalësh, që kështu mund të zgjerojë fjalorin e nivelit të fjalëve. Eksperimentet tona në dy çifte gjuhësh tregojnë rezultate të mira për skenarin tipik të të dhënave me burime të ulta (IWSLT në domain dataset). Përmirësimet tona të vazhdueshme mbi linjat bazë përfaqësojnë një provë pozitive rreth mundësisë për të përdorur modele të paratrajnuar mbi të dhënat monogjuhësore në NMT.', 'am': 'የኔural machine translation (NMT) is the state of the art for machine translation, and it shows the best performance when there is a considerable amount of data available. ለቋንቋ ሁለቶች ብቻ ትንሽ ዳታዎች ቢኖሩ ምሳሌው ለቃላት፣ በተለየ ጥቂት ቃላት ጥሩ መልዕክቶችን ለማድረግ አይችልም፡፡ አንዱ የተለየ ትምህርት መፍትሄ በአካባቢ ቃላት እና በአካባቢ ቃላት እና በአካባቢ ቃላት እንዲያካፈሉ ነው፡፡ በተለየ ልዩ ሥርዓት አንቀሳቅስ፣ በዚህች ገጽ ውስጥ በሞሎ ቋንቋ ዳታዎች ላይ የተማሩት የንግግር ድረ ገጽ የመጠቀምን የNMT መረብ አቀናቅል፡፡ የቃላት-ደረጃ ቃላትን ለማስፋት ይችላል፡፡ በሁለቱ ቋንቋዎች ሁለት ዓይነቶች ላይ የተፈተናን የጥያቄ ዝቅተኛ የዳታ ሳንሱር (IWSLT in-domain ዳታተር ማሳየት) መልካሙን ፍሬ ያሳያል፡፡ በመደገፊያው አካባቢዎች ላይ የሚደረገውን ማስረጃ በNMT ውስጥ በሞሎንቋል ዳታዎችን ለመቀበል የሚችል ምናልባት አካባቢ ነው፡፡', 'bn': 'মেশিন অনুবাদের জন্য নিউরেল মেশিন অনুবাদ (এনএমটি) হল মেশিন অনুবাদের শিল্পের পরিস্থিতি, এবং এটি দেখাচ্ছে যখন তথ্যের বেশী পরিম যখন শুধুমাত্র ভাষার জোড়ার জন্য সামান্য তথ্য থাকে, তখন মডেল শব্দের জন্য ভাল প্রতিনিধিত্ব তৈরি করতে পারে না, বিশেষ কর একটি সাধারণ সমাধানের মধ্যে রয়েছে তথ্যের স্পারিসি কমিয়ে আনা শব্দ সাব-শব্দে বিভক্ত করার মাধ্যমে, যাতে অন্যান্য শব্দের সাথে প্রত এই কাগজটিতে আমরা একটি এনএমটি নেটওয়ার্ক খাওয়ার একটি পদ্ধতি উপস্থাপন করি যার মানুষের ভাষার তথ্যে প্রশিক্ষণ প্রদান করা হয়েছে, যা প্রশিক্ষণের সময় কাজ-বিশেষ প্রশ এই পদ্ধতি বিশাল কয়েকটি শব্দ দিয়ে বিভিন্ন সংখ্যার ম্যাট্রিক্স ব্যবহার করতে পারে, যার ফলে শব্দ-স্তরের শব্দভাণ্ডার বৃ Our experiments on two language pairs show good results for the typical low-resourced data scenario (IWSLT in-domain dataset).  আমাদের বেসেলাইনের বিভিন্ন উন্নয়নের প্রতিনিধিত্ব করছে এনএমটির মোনোলিভাল ভাষার তথ্যে প্রশিক্ষিত মডেলের সম্ভাবনা সম্পর্কে।', 'az': 'NMT maşın çevirilməsi məlumatının məlumatının məlumatıdır və bu maşın çevirilməsi üçün ən yaxşı performans göstərir. Bir dil çift üçün yalnız kiçik məlumat varsa, modeli sözlər üçün yaxşı ifadə edə bilməz, özlərinə də nadir sözlər üçün. Bir ortaq çətinlik məlumatları başqa sözlərlə paylaşdırmağa imkan vermək üçün, sözləri altı sözlərə bölüşdürmək üçün məlumatları küçültmək üçün məlumatlar olar. Bu kağızda başqa bir tərzim almaq üçün NMT a ğını təhsil etmək üçün monodil verilən sözlərlə təhsil edilmiş, təhsil vaxtında öyrənmiş işlər müəyyən edilmiş inbinglərlə birlikdə təhsil edirik. Bu metod, sözlərin səviyyəsini genişləyə bilən böyük bir sayı sözlərlə birlikdə içərilən matriksini təmin edə bilər. İki dil çiftlərimizin təcrübələrimiz düşük ressurslı veri senaryosu üçün yaxşı nəticələri göstərir. Bizim temel çətinlərin üstündə müəyyən edilən düzəltmələrimiz NMT-də monodil verilən modelləri təhsil etmək mümkünlüyü barəsində pozitif bir kanıt göstərir.', 'hy': 'Նյարդային մեքենայի թարգմանությունը (NMT) մեքենայի թարգմանման ամենաարդյունքն է, և այն ցույց է տալիս լավագույն արդյունքը, երբ հնարավոր է ունենալ բազմաթիվ տվյալներ: Երբ լեզվի զույգի համար միայն քիչ տվյալներ կան, մոդելը չի կարող լավ ներկայացնել բառերը, հատկապես հազվադեպ բառերը: Մի ընդհանուր լուծում կազմում է նվազեցնել տվյալների հազվադեպությունը բառերի բաժանելով ենթաբառերի, որպեսզի հազվադեպ բառերը ունենան ընդհանուր ներկայացումներ այլ բառերի հետ: Մեկ այլ մոտեցում օգտագործելով, այս թղթի մեթոդը ներկայացնում ենք NMT ցանցի կերակրելու մեթոդ միալեզվով տվյալների վրա վարժեցված բառերի ներդրումների հետ, որոնք համադրվում են աշխատանքի մասնավոր ներդրումների հետ, որոնք սովորվել են վարժեքի ժամանակ: Այս մեթոդը կարող է օգտագործել ներգրավող մատրիքսը բառերի մեծ քանակով, ինչը հետևաբար կարող է ընդլայնել բառի մակարդակը: Երկու լեզվի զույգերի վրա մեր փորձարկումները լավ արդյունքներ են ցույց տալիս ցածր ռեսուրսների տվյալների տիպիկ սցենարիայի համար (IwSlaT-ը տիպիկ տվյալների համակարգի համար): Our consistent improvements over the baselines represent a positive proof about the possibility to leverage models pre-trained on monolingual data in NMT.', 'ca': "La traducció neuromàquina (NMT) és l'última tecnologia de la traducció, i mostra el millor rendiment quan hi ha una quantitat considerable de dades disponibles. Quan només hi ha poques dades per un parell de llenguatges, el model no pot produir bones representacions per paraules, especialment per paraules rares. Una solució comú consisteix en reduir l'escassetat de dades segmentant les paraules en subparaules, per permetre que les paraules rares tinguin representacions compartides amb altres paraules. Amb un enfocament diferent, en aquest paper presentem un mètode per alimentar una xarxa NMT amb incorporacions de paraules entrenades en dades monolingües, que es combinan amb incorporacions específices per a les tasques aprengutes en temps d'entrenament. Aquest mètode pot aprofitar una matriu d'incorporació amb un gran nombre de paraules, que, per tant, pot estendre el vocabulari de nivell de paraules. Els nostres experiments en dos parells de llengües mostran bons resultats per a l'escenari típic de dades amb baixos recursos (IWSLT en domini). Les nostres millores constants sobre les línies de base representan una prova positiva de la possibilitat d'aprofitar models pré-entrenats en dades monolingües a la MTN.", 'bs': 'Neuralni prevod mašine (NMT) je stanje umjetnosti za prevod mašine, i pokazuje najbolji učinkovit kada postoji dostupna količina podataka. Kada postoje samo mali podaci za parove jezika, model ne može proizvesti dobre predstave za riječi, posebno za rijetke riječi. Jedno zajedničko rješenje se sastoji u smanjenju rezervnosti podataka segmentiranjem riječima u podriječima, kako bi rijetke riječi mogle dijeliti predstave s drugim riječima. Uzimajući drugačiji pristup, u ovom papiru predstavljamo metodu da nahranimo NMT mrežu sa riječima uključenim na monojezičke podatke, koje su kombinirane sa određenim uključenim na trening. Ova metoda može uticati na ugrađenu matricu sa ogromnim brojem riječi, što stoga može proširiti riječ na nivou riječi. Naši eksperimenti na dva jezička parova pokazuju dobre rezultate za tipični scenario niskog resursa podataka (IWSLT u domenu podataka). Naše konsekventne poboljšanje na osnovnim linijama predstavljaju pozitivan dokaz o mogućnosti utjecaja na predobučene modele na monojezičke podatke u NMT-u.', 'cs': 'Neurální strojový překlad (NMT) je nejmodernější pro strojový překlad a ukazuje nejlepší výkon, pokud je k dispozici značné množství dat. Pokud existuje jen málo dat pro jazykový pár, model nemůže produkovat dobré reprezentace slov, zejména pro vzácná slova. Jedno společné řešení spočívá ve snížení řídkosti dat segmentováním slov do dílčích slov, aby vzácná slova mohla sdílet reprezentace s jinými slovy. Na základě jiného přístupu představujeme v tomto článku metodu napájení NMT sítě slovními vloženími trénovanými na jednojjazyčných datech, které jsou kombinovány s úlohově specifickými vloženími naučenými v době tréninku. Tato metoda může využít vložení matice s velkým počtem slov, což může rozšířit slovní zásobu na úrovni slova. Naše experimenty na dvou jazykových párech ukazují dobré výsledky pro typický scénář dat s nízkými zdroji (IWSLT in-domain dataset). Naše konzistentní zlepšení nad základními liniemi představují pozitivní důkaz o možnosti využití modelů předškolených na monojazyčných datech v NMT.', 'fi': 'Neuraalinen konekäännös (NMT) on konekäännöksen uusinta tekniikkaa, ja se osoittaa parhaan suorituskyvyn, kun saatavilla on huomattava määrä tietoa. Kun kieliparista on vain vähän tietoa, malli ei pysty tuottamaan hyviä esityksiä sanoille, etenkään harvinaisille sanoille. Yhtenä yleisenä ratkaisuna on vähentää datan niukkuutta segmentoimalla sanat alasanoiksi, jotta harvinaiset sanat voisivat jakaa esitystä muiden sanojen kanssa. Tässä työssä esitellään menetelmä, jolla NMT-verkosto syötetään monikieliseen dataan koulutetuilla sanaupotuksilla, jotka yhdistetään harjoittelun aikana opittuihin tehtäväkohtaisiin upotuksiin. Tämä menetelmä voi hyödyntää upotusmatriisin valtavalla määrällä sanoja, mikä voi siten laajentaa sanastoa. Kahden kieliparin kokeilut osoittavat hyviä tuloksia tyypillisessä vähävaraisessa dataskenaariossa (IWSLT in-domain dataset). Jatkuvat parannuksemme lähtötasoon nähden ovat myönteinen osoitus mahdollisuudesta hyödyntää malleja, jotka on koulutettu monikieliseen dataan NMT:ssä.', 'et': 'Neuraalne masintõlge (NMT) on masintõlke tipptasemel ja see näitab parimat jõudlust, kui on olemas märkimisväärne hulk andmeid. Kui keelepaari kohta on ainult vähe andmeid, ei suuda mudel luua häid esitusi sõnadele, eriti haruldastele sõnadele. Üks levinud lahendus seisneb andmete hõreduse vähendamises, segmenteerides sõnad alamsõnadeks, et haruldastel sõnadel oleks teiste sõnadega jagatud esitusi. Teisest lähenemisviisist lähtudes tutvustame käesolevas töös meetodit NMT võrgustiku toitmiseks ühekeelsetel andmetel koolitatud sõnade manustamisega, mis on kombineeritud koolituse ajal omandatud ülesandepõhiste manustamistega. See meetod võib kasutada suure hulga sõnadega manustamismaatriksi, mis võib seega laiendada sõnatasemel sõnavara. Meie eksperimendid kahe keelepaariga näitavad häid tulemusi tüüpilise vähese ressursiga andmestsenaariumi (IWSLT-in-domain dataset) puhul. Meie järjepidevad parandused võrreldes lähtejoontega annavad positiivse tõestuse võimalusest kasutada mudeleid, mis on eelnevalt koolitatud ühekeelsete andmete põhjal NMT-s.', 'jv': "Nyong-sistem penting (NMT) iki dadi kelas nang art is kanggo tarjamahan kelas, lan iki iso nggawe barang bakal terus-penting ing ing teka-teka sing bisa bantuan sampeyan data sing berarti. Sampeyan bener datayang dipunangé kanggo kelas barang, model kuwi ora bisa dianggawe bener kanggo kelas telu, supaya kanggo kelas-kelas section NMT Network ngewehi sistem samihan yang alih, ning paper iki kita nggawe sistem kanggo nyelehon NMT network karo word embedding that is acaran on Monlanguage data, yang dipumungkan karo task-special embedding that is Learned at Learning time. Wurung iki iso nggawe nyimpen Where's the data Awakdhéwé éntuk barêng-barêng nggawe barang urip kuwi nggawe gerakan dadi sing apik dhéwé, banjur kuwi nggawe model sing gawe nguasai perusahaan anyar sampeyan ingkang NMT.", 'he': 'תורגם על ידי מכונת נוירולית (NMT) הוא מצב האומנות של תורגם מכונת, והוא מראה את ההופעה הטובה ביותר כשיש כמות גדולה של נתונים זמינים. כאשר רק מידע קטן קיים לזוג שפה, הדוגמא לא יכול לייצר מייצגים טובים למילים, במיוחד למילים נדירות. פתרון משותף אחד מכיל בהפחית חסרות נתונים על ידי מחלקת מילים לתת-מילים, כדי לאפשר מילים נדירות להיות מייצגים משותפים עם מילים אחרות. באמצעות גישה שונה, בעיתון הזה אנחנו מציגים שיטה להאכיל רשת NMT עם קישורים מילים מאומנים על נתונים monolingual, אשר משולבים עם קישורים ספציפיים למשימה ללמודים בזמן האימון. השיטה הזאת יכולה להשתמש במטריקס מעורבת עם מספר עצום של מילים, מה שאפשר להאריך את המילים ברמה המילים. Our experiments on two language pairs show good results for the typical low-resourced data scenario (IWSLT in-domain dataset).  שיפורים קבועים שלנו מעל קווי הבסיס מייצגים הוכחה חיובית על האפשרות להשתמש בדוגמנים מאומנים מראש על נתונים מונושפתיים ב-NMT.', 'ha': "translation Idan da data kaɗan za'a ƙunsa da wa wani nau'i biyu na harshe, shirin ayuka bã ya iya zartar da mai kyau wa maganar, kuma da ƙayyadadde masu sauri. Tsarin da ɗabi'a na ƙunsa da ya ƙara ƙanshi na data game da yin jinsigi-magana cikin sub-word, dõmin ya yarda da saurare da za'a sami shaidar da wasu kalmõmi. Ta riƙi wani hanyor daban, cikin wannan takarda, muna gabatar da wata hanyoyi wa su ciyar da wata shirin NMT na haɗi da word embedded on data na monoli, wanda aka haɗa da shirin cikin aikin da aka sanar a lokacin da aka yi wa lõkaci. Wannan metode yana iya cire wani matriki mai guda da wasu kalmõmi mai girma, da kuma yana iya cire maganar-daraja. Kayan jarrabõyinmu a kan nau'i biyu cikin harshen, suna da matsala mai kyau wa fasarin da aka typica da lower-resource data tsaro (IWSLT cikin-Domin). BabbayinMu na ƙari a kan asalin, yana gaya wata shaida tabbatacce a kan iya iya amfani da misãlai da za'a yi amfani da shi gaba-wa'a kan data na monoli-harshen cikin NMT.", 'sk': 'Nevralni strojni prevod (NMT) je najsodobnejši strojni prevod in kaže najboljšo zmogljivost, kadar je na voljo precejšnja količina podatkov. Če za jezikovni par obstaja le malo podatkov, model ne more ustvariti dobrih predstavitev besed, zlasti redkih besed. Ena od skupnih rešitev je zmanjšanje redkosti podatkov s segmentiranjem besed v podbesede, da bi redke besede lahko imele skupne predstavitve z drugimi besedami. Z drugačnim pristopom v prispevku predstavljamo metodo za napajanje mreže NMT z besednimi vdelavami, usposobljenimi na enojezičnih podatkih, ki so kombinirani z vdelavami za posamezne naloge, ki so se naučili v času usposabljanja. Ta metoda lahko izkoristi matriko vgradnje z ogromnim številom besed, kar lahko zato razširi besedišče na ravni besed. Naši eksperimenti na dveh jezikovnih parih kažejo dobre rezultate za tipični scenarij podatkov z nizkimi viri (IWSLT in-domain dataset). Naše dosledne izboljšave v primerjavi z izhodiščnimi vrednostmi predstavljajo pozitiven dokaz o možnosti uporabe modelov, ki so predhodno usposobljeni za enojezične podatke v NMT.', 'bo': 'ནུས་མེད་ལག་འཁྱེར་གྱི་སྐད་ཡིག་ཆ་ལྟ་བུའི་གནས་སྟངས་ཡིན། སྐད་ཡིག One common solution consists in reducing data sparsity by segmenting words in to sub-words, in order to allow rare words to have shared representations with other words. འོན་ཀྱང་། ཤོག་བྱང་འདིའི་ནང་དུ་བྱ་ཚིག་མ་འདྲ་ཞིག་འཇུག་རྒྱུ་དང་། འདི་ལྟ་བུའི་ལམ་ལུགས་འདིས་སྐྱེད་པའི་ཆ་ཤས་ཆེན་ཡིན་པའི་རྣམ་གྲངས་ཀ་ལས་ཕར་རིས་ཐུབ་པའི་ཡིག་རྟགས། ང་ཚོའི་སྐད་ཡིག་ཆ་གཉིས་ཀྱི་བརྟག་ཞིབ་ཀྱིས་རྒྱུན་ལྡན་པའི་ཆ་འཕྲིན་ཡིག་ཆའི་ནང་དུ་བཏུབ་པ་ཅིག་ཡིན། ང་ཚོའི་རྨང་གཞུང་གི་གནས་ཚུལ་མཚམས་ཆེན་སུ་གཏོང་བའི་མཐུན་རྐྱེན་གྱིས་ཕལ་ཆེན་ཞིག་ཡིན་པས།'}
{'en': 'Improving Zero-Shot Translation of Low-Resource Languages', 'ar': 'تحسين الترجمة بدون طلقة للغات منخفضة الموارد', 'fr': 'Améliorer la traduction Zero-Shot des langues à faibles ressources', 'es': 'Mejora de la traducción cero de idiomas de bajos recursos', 'pt': 'Melhorando a tradução zero-shot de idiomas de poucos recursos', 'ja': '低資源言語のゼロショット翻訳の改善', 'hi': 'कम संसाधन भाषाओं के शून्य-शॉट अनुवाद में सुधार', 'zh': '改入资源匮语零次性译', 'ru': 'Улучшение перевода с нулевым снимком малоресурсных языков', 'ga': 'Feabhas a chur ar Aistriúchán Neamh-Shóta ar Theangacha Acmhainne Íseal', 'ka': 'Name', 'el': 'Βελτίωση της μηδενικής μετάφρασης γλωσσών χαμηλών πόρων', 'hu': 'Az alacsony erőforrású nyelvek zéró fordításának javítása', 'it': 'Migliorare la traduzione a scatto zero delle lingue a basso contenuto di risorse', 'kk': 'Төменгі ресурс тілдерінің 0- тың аудармасын жақсарту', 'mk': 'Improving Zero-Shot Translation of Low-Resource Languages', 'lt': 'Mažų išteklių kalbų vertimo nuliniu būdu gerinimas', 'ms': 'Perbaikan Terjemahan Semula-Tembakan Bahasa Sumber Terrendah', 'mt': 'Titjib fit-Traduzzjoni Zero-Shot ta’ Lingwi b’Riżorsi Bażi', 'ml': 'കുറഞ്ഞ വിഭവങ്ങളുടെ ഭാഷകളുടെ പൂജ്യ- ഷോട്ട് പരിഭാഷകള്\u200d മെച്ചപ്പെടുത്തുന്നു', 'mn': 'Бага боловсролын хэлнүүдийн 0-Shot-ын хөгжлийг сайжруулах', 'no': 'Forbetra omsetjing av låg ressursspråk', 'pl': 'Poprawa tłumaczenia zerowego języków niskich zasobów', 'sr': 'Poboljšanje prevoda jezika niskih resursa', 'ro': 'Îmbunătățirea traducerii zero-shot a limbilor cu resurse reduse', 'si': 'Name', 'so': 'Improving Zero-Shot Translation of Low-Resource Languages', 'sv': 'Förbättra noll-skott översättning av lågresursspråk', 'ta': 'Name', 'ur': 'نیچے رسورسوس زبانوں کی صفر-شٹ ترجمہ بہتر کر رہی ہے', 'uz': 'Name', 'vi': 'Tăng cường việc dịch ngôn ngữ vùng thấp', 'nl': 'Verbetering van Zero-Shot vertaling van talen met weinig hulpbronnen', 'da': 'Forbedring af Zero Shot-oversættelse af lav ressource sprog', 'id': 'Menembangkan Translation Zero-Shot dari Bahasa Sumber Terrendah', 'ko': '저자원 언어의 제로 렌즈 번역을 개선하다', 'bg': 'Подобряване на превода с нулев изстрел на езици с ниски ресурси', 'fa': 'بهتر ترجمه زبانهای کم منبع صفر', 'hr': 'Poboljšanje prevoda jezika niskih resursa', 'de': 'Verbesserung der Zero-Shot-Übersetzung von ressourcenarmen Sprachen', 'sw': 'Kuboresha tafsiri za lugha za chini za rasilimali', 'af': 'Verbeter Nuwe- Shot Vertaling van Lae- Hulpbron Taal', 'sq': 'Përmirësimi i përkthimit zero-shot të gjuhëve me burime të ulëta', 'am': 'ቋንቋዎች', 'az': 'Aşağı Kaynaq Dillərin Sıfır-Shot tərcümünü daha yaxşılaşdırma', 'bs': 'Poboljšanje prevoda jezika niskih resursa', 'tr': 'Az-Ressurat Dilleriniň terjimesini bejermek', 'ca': 'millorar la traducció zero de llengües de baix recursos', 'cs': 'Zlepšení nulového překladu jazyků s nízkými zdroji', 'hy': 'Ցանկացած ռեսուրսների լեզուների զրո-կրակի թարգմանման բարելավումը', 'et': 'Vähese ressursiga keelte tõlkimise täiustamine', 'bn': 'নিম্নলিখিত সম্পদ ভাষার অনুবাদ', 'fi': 'Vähävaraisten kielten nollakuvan kääntämisen parantaminen', 'jv': 'Gjelongi Pasword', 'ha': 'KCharselect unicode block name', 'sk': 'Izboljšanje ničelnega prevoda jezikov z nizkimi viri', 'he': 'שיפור התרגום אפס של שפות משאבים נמוכות', 'bo': 'རྒྱུ་དངོས་ཉུང་བའི་སྐད་རིགས་ཀྱི་Zero-Shot ཡིན་ཚུལ་ཡར་རྒྱས་གཏོང'}
{'en': 'Recent work on multilingual neural machine translation reported competitive performance with respect to bilingual models and surprisingly good performance even on (zero-shot) translation directions not observed at training time. We investigate here a zero-shot translation in a particularly low-resource multilingual setting. We propose a simple iterative training procedure that leverages a duality of translations directly generated by the  system  for the zero-shot directions. The translations produced by the system (sub-optimal since they contain mixed language from the shared vocabulary), are then used together with the original parallel data to feed and iteratively re-train the multilingual network. Over time, this allows the  system  to learn from its own generated and increasingly better output. Our approach shows to be effective in improving the two zero-shot directions of our multilingual model. In particular, we observed gains of about 9 BLEU points over a baseline multilingual model and up to 2.08 BLEU over a pivoting mechanism using two bilingual models. Further analysis shows that there is also a slight improvement in the non-zero-shot language directions.', 'fr': "Des travaux récents sur la traduction automatique neuronale multilingue ont fait état de performances concurrentielles par rapport aux modèles bilingues et de performances étonnamment bonnes même dans des directions de traduction (zéro plan) non observées au moment de la formation. Nous étudions ici une traduction «\xa0zero-shot\xa0» dans un environnement multilingue particulièrement pauvre en ressources. Nous proposons une procédure de formation itérative simple qui exploite une dualité de traductions directement générées par le système pour les directions de tir zéro. Les traductions produites par le système (sous-optimales puisqu'elles contiennent des langues mixtes issues du vocabulaire partagé) sont ensuite utilisées avec les données parallèles d'origine pour alimenter et réentraîner de manière itérative le réseau multilingue. Au fil du temps, cela permet au système d'apprendre de ses propres résultats générés et de plus en plus performants. Notre approche s'avère efficace pour améliorer les deux directions de tir zéro de notre modèle multilingue. En particulier, nous avons observé des gains d'environ 9 points UEBL par rapport à un modèle multilingue de référence et jusqu'à 2,08 points UEBL par rapport à un mécanisme pivotant utilisant deux modèles bilingues. Une analyse plus approfondie montre qu'il y a également une légère amélioration dans les directions linguistiques non nulles.", 'pt': 'Trabalhos recentes sobre tradução automática neural multilíngue relataram desempenho competitivo em relação a modelos bilíngues e desempenho surpreendentemente bom mesmo em direções de tradução (zero-shot) não observadas no tempo de treinamento. Investigamos aqui uma tradução de tiro zero em um ambiente multilíngue particularmente com poucos recursos. Propomos um procedimento de treinamento iterativo simples que aproveita uma dualidade de traduções geradas diretamente pelo sistema para as direções de tiro zero. As traduções produzidas pelo sistema (sub-ótimas, pois contêm linguagem mista do vocabulário compartilhado) são então usadas junto com os dados paralelos originais para alimentar e retreinar iterativamente a rede multilíngue. Com o tempo, isso permite que o sistema aprenda com sua própria saída gerada e cada vez melhor. Nossa abordagem mostra-se eficaz em melhorar as duas direções de tiro zero do nosso modelo multilíngue. Em particular, observamos ganhos de cerca de 9 pontos BLEU em um modelo multilíngue de linha de base e até 2,08 BLEU em um mecanismo pivotante usando dois modelos bilíngues. Uma análise mais aprofundada mostra que também há uma ligeira melhoria nas direções da linguagem não-zero-shot.', 'ar': 'أفاد العمل الأخير في الترجمة الآلية العصبية متعددة اللغات عن أداء تنافسي فيما يتعلق بالنماذج ثنائية اللغة والأداء الجيد بشكل مدهش حتى في اتجاهات الترجمة (بدون طلقة) التي لم تتم ملاحظتها في وقت التدريب. نحن نحقق هنا في ترجمة بدون طائل في بيئة متعددة اللغات منخفضة الموارد بشكل خاص. نقترح إجراء تدريب تكراري بسيط يستفيد من ازدواجية الترجمات التي تم إنشاؤها مباشرة بواسطة النظام للاتجاهات الصفرية. يتم بعد ذلك استخدام الترجمات التي ينتجها النظام (دون المستوى الأمثل نظرًا لأنها تحتوي على لغة مختلطة من المفردات المشتركة) جنبًا إلى جنب مع البيانات الموازية الأصلية لتغذية الشبكة متعددة اللغات وإعادة تدريبها بشكل متكرر. بمرور الوقت ، يسمح هذا للنظام بالتعلم من مخرجاته التي تم إنشاؤها وتحسينها بشكل متزايد. يُظهر نهجنا فعاليته في تحسين الاتجاهين الصفريين لنموذجنا متعدد اللغات. على وجه الخصوص ، لاحظنا مكاسب بنحو 9 نقاط BLEU على نموذج أساسي متعدد اللغات وما يصل إلى 2.08 BLEU عبر آلية محورية باستخدام نموذجين ثنائي اللغة. يُظهر التحليل الإضافي أن هناك أيضًا تحسنًا طفيفًا في اتجاهات اللغة غير الصفرية.', 'es': 'Un trabajo reciente sobre la traducción automática neuronal multilingüe informó un rendimiento competitivo con respecto a los modelos bilingües y un rendimiento sorprendentemente bueno incluso en direcciones de traducción (cero) no observadas en el momento del entrenamiento. Aquí investigamos una traducción cero en un entorno multilingüe de recursos particularmente bajos. Proponemos un procedimiento de formación iterativo simple que aprovecha una dualidad de traducciones generadas directamente por el sistema para las direcciones cero. Las traducciones producidas por el sistema (subóptimas, ya que contienen un lenguaje mixto del vocabulario compartido), se utilizan junto con los datos paralelos originales para alimentar y volver a entrenar iterativamente la red multilingüe. Con el tiempo, esto permite que el sistema aprenda de su propia producción generada y cada vez mejor. Nuestro enfoque demuestra ser eficaz para mejorar las dos direcciones cero de nuestro modelo multilingüe. En particular, observamos ganancias de aproximadamente 9 puntos BLEU con respecto a un modelo multilingüe de referencia y hasta 2,08 BLEU con respecto a un mecanismo pivotante utilizando dos modelos bilingües. Un análisis más detallado muestra que también hay una ligera mejora en las direcciones del idioma que no son de tiro cero.', 'zh': '近世多言神经机器翻译事,告与双语模形竞见,虽未观(零次)译,亦令人惊地好。 研究资源特少语言境界中零镜头译。 立简迭代训练之际,当以系统直生零点向者偶性转易。 然后统成译(次优,以其包词汇表混言)与原始并行,以馈迭代重练多言网络。 随时推移,许从自己生成而愈好的输出中学习。 吾法明于多言之二零点有效也。 观基线多言模之益为9 BLEU点,用两双语之旋转机制者增高2.08 BLEU。 更加分析表明,非零镜头言亦有微改。', 'ja': '多言語ニューラル機械翻訳に関する最近の研究では、バイリンガルモデルに関して競争力のあるパフォーマンスと、トレーニング時に観察されなかった（ゼロショット）翻訳方向でも驚くほど良好なパフォーマンスが報告されました。 ここでは、特にリソースの少ない多言語環境でのゼロショット翻訳を調査します。 システムが直接生成する翻訳の二重性をゼロショット方向に活用したシンプルな反復トレーニング手順を提案します。 システムによって生成された翻訳（共有ボキャブラリからの混合言語が含まれているため、最適以下）は、オリジナルの並列データと一緒に使用され、多言語ネットワークにフィードし、繰り返し再トレーニングします。 時間の経過とともに、これにより、システムは独自に生成されたより優れた出力から学習することができます。 私たちのアプローチは、多言語モデルの2つのゼロショット方向を改善するのに効果的であることを示しています。 特に、2つのバイリンガルモデルを使用したピボット機構を通じて、ベースライン多言語モデルで約9 BLEUポイント、最大2.08 BLEUの利得を観察した。 さらに解析すると、ゼロショットでない言語の方向性にもわずかな改善が見られることが示されている。', 'hi': 'बहुभाषी तंत्रिका मशीन अनुवाद पर हाल के काम ने द्विभाषी मॉडल के संबंध में प्रतिस्पर्धी प्रदर्शन की सूचना दी और आश्चर्यजनक रूप से (शून्य-शॉट) अनुवाद दिशाओं पर भी अच्छे प्रदर्शन को प्रशिक्षण के समय नहीं देखा गया। हम यहां एक विशेष रूप से कम-संसाधन बहुभाषी सेटिंग में एक शून्य-शॉट अनुवाद की जांच करते हैं। हम एक सरल पुनरावर्ती प्रशिक्षण प्रक्रिया का प्रस्ताव करते हैं जो शून्य-शॉट दिशाओं के लिए सिस्टम द्वारा सीधे उत्पन्न अनुवादों के द्वंद्व का लाभ उठाता है। सिस्टम द्वारा उत्पादित अनुवाद (उप-इष्टतम क्योंकि उनमें साझा शब्दावली से मिश्रित भाषा होती है), फिर मूल समानांतर डेटा के साथ मिलकर बहुभाषी नेटवर्क को फीड करने और पुनरावर्ती रूप से फिर से प्रशिक्षित करने के लिए उपयोग किया जाता है। समय के साथ, यह सिस्टम को अपने स्वयं के उत्पन्न और तेजी से बेहतर आउटपुट से सीखने की अनुमति देता है। हमारा दृष्टिकोण हमारे बहुभाषी मॉडल के दो शून्य-शॉट दिशाओं में सुधार करने में प्रभावी होने के लिए दिखाता है। विशेष रूप से, हमने एक बेसलाइन बहुभाषी मॉडल पर लगभग 9 BLEU बिंदुओं के लाभ को देखा और दो द्विभाषी मॉडल का उपयोग करके एक धुरी तंत्र पर 2.08 BLEU तक। आगे के विश्लेषण से पता चलता है कि गैर-शून्य-शॉट भाषा दिशाओं में भी थोड़ा सुधार हुआ है।', 'ru': 'Недавние работы по многоязычному нейронному машинному переводу сообщили о конкурентной производительности по отношению к двуязычным моделям и удивительно хорошей производительности даже по (нулевым) направлениям перевода, которые не наблюдались во время обучения. Здесь мы исследуем перевод с нулевым выстрелом в особенно малоресурсной многоязычной среде. Мы предлагаем простую итеративную процедуру обучения, которая использует дуальность переводов, непосредственно генерируемых системой для направлений с нулевым выстрелом. Переводы, созданные системой (субоптимальные, поскольку они содержат смешанный язык из общего словаря), затем используются вместе с исходными параллельными данными для подачи и итерационного переобучения многоязычной сети. Со временем это позволяет системе извлекать уроки из собственной генерируемой и все более качественной продукции. Наш подход показывает свою эффективность в улучшении двух направлений с нулевым выстрелом нашей многоязычной модели. В частности, мы наблюдали увеличение примерно на 9 пунктов BLEU по сравнению с базовой многоязычной моделью и до 2,08 BLEU по сравнению с поворотным механизмом с использованием двух двуязычных моделей. Дальнейший анализ показывает, что наблюдается также некоторое улучшение в ненулевых языковых направлениях.', 'ga': 'Thuairiscigh obair a rinneadh le déanaí ar néaraistriúchán meaisín ilteangach feidhmíocht iomaíoch maidir le samhlacha dátheangacha agus feidhmíocht iontach maith fiú ar threoracha aistriúcháin (nialas) nár breathnaíodh ag am oiliúna. Déanaimid imscrúdú anseo ar aistriúchán náid i suíomh ilteangach a bhfuil acmhainní íseal go háirithe aige. Molaimid nós imeachta oiliúna atriallach simplí lena n-úsáidtear dúbailteacht aistriúcháin a ghineann an córas go díreach le haghaidh na dtreoracha náid. Úsáidtear na haistriúcháin a tháirgeann an córas (fo-optamach ós rud é go bhfuil teanga mheasctha ón bhfoclóir comhroinnte iontu), in éineacht leis na bunshonraí comhthreomhara ansin chun an líonra ilteangach a bheathú agus a athoiliúint go atriallach. Le himeacht ama, cuireann sé seo ar chumas an chórais foghlaim óna aschur ginte féin agus atá ag éirí níos fearr. Is léir go bhfuil ár gcur chuige éifeachtach maidir le feabhas a chur ar an dá threo náid ar ár múnla ilteangach. Go háirithe, thugamar faoi deara gnóthachain de thart ar 9 bpointe BLEU thar mhúnla bonnlíne ilteangach agus suas le 2.08 BLEU thar mheicníocht maolaithe ag baint úsáide as dhá mhúnla dhátheangach. Léiríonn anailís bhreise go bhfuil feabhas beag freisin ar na treoracha teanga neamh-nialais.', 'hu': 'A többnyelvű neurális gépi fordítással kapcsolatos közelmúltbeli munkák versenyképes teljesítményről számoltak be a kétnyelvű modellek tekintetében, és meglepően jó teljesítményről számoltak be még az edzések idején nem figyeltek meg (zero-shot) fordítási irányokon is. Itt egy nulla-shot fordítást vizsgálunk különösen alacsony erőforrású többnyelvű környezetben. Javasolunk egy egyszerű iteratív képzési eljárást, amely a rendszer által közvetlenül generált fordítások kettősségét használja a nulla lövési irányokhoz. A rendszer által készített fordítások (nem optimálisak, mivel vegyes nyelvet tartalmaznak a megosztott szókincsből), ezt követően az eredeti párhuzamos adatokkal együtt használják fel a többnyelvű hálózat továbbképzésére és ismétlődésére. Idővel ez lehetővé teszi a rendszer számára, hogy tanuljon saját generált és egyre jobb teljesítményéből. Megközelítésünk hatékonynak bizonyul a többnyelvű modellünk két zéró lövési irányának javításában. Különösen egy kiindulási többnyelvű modellhez képest körülbelül 9 BLEU-pont és 2,08 BLEU-pont növekedést figyeltünk meg egy forgó mechanizmussal szemben két kétnyelvű modellhez képest. További elemzések azt mutatják, hogy kismértékű javulás történik a nem nulla lövéses nyelvi irányokban is.', 'el': 'Πρόσφατες εργασίες για την πολύγλωσση νευρωνική μηχανική μετάφραση ανέφεραν ανταγωνιστικές επιδόσεις σε σχέση με τα δίγλωσσα μοντέλα και εκπληκτικά καλές επιδόσεις ακόμη και σε (μηδενικές) μεταφραστικές κατευθύνσεις που δεν παρατηρήθηκαν κατά τη διάρκεια της εκπαίδευσης. Ερευνούμε εδώ μια μετάφραση μηδενικού πυροβολισμού σε ένα ιδιαίτερα χαμηλού πόρου πολυγλωσσικό περιβάλλον. Προτείνουμε μια απλή επαναληπτική διαδικασία εκπαίδευσης που αξιοποιεί μια δυαδικότητα μεταφράσεων που παράγεται απευθείας από το σύστημα για τις κατευθύνσεις μηδενικού πυροβολισμού. Οι μεταφράσεις που παράγονται από το σύστημα (μη βέλτιστες καθώς περιέχουν μικτή γλώσσα από το κοινό λεξιλόγιο), χρησιμοποιούνται στη συνέχεια μαζί με τα αρχικά παράλληλα δεδομένα για να τροφοδοτήσουν και να επανεκπαιδεύσουν επανεκπαίδευση του πολύγλωσσου δικτύου. Με την πάροδο του χρόνου, αυτό επιτρέπει στο σύστημα να μάθει από τη δική του παραγόμενη και ολοένα και περισσότερο καλύτερη παραγωγή. Η προσέγγισή μας αποδεικνύεται αποτελεσματική στη βελτίωση των δύο κατευθύνσεων μηδενικής λήψης του πολύγλωσσου μοντέλου μας. Ειδικότερα, παρατηρήσαμε κέρδη περίπου 9 σημείων σε σχέση με ένα βασικό πολυγλωσσικό μοντέλο και μέχρι 2.08 σε έναν περιστρεφόμενο μηχανισμό χρησιμοποιώντας δύο δίγλωσσα μοντέλα. Περαιτέρω ανάλυση δείχνει ότι υπάρχει επίσης μια μικρή βελτίωση στις μη μηδενικές γλωσσικές κατευθύνσεις.', 'ka': 'შემდეგ მრავალური ნეიროლური მაქსინის გაგრძნობის მუშაობაში კონპექტიური მუშაობაზე გამოყენებული კონპექტიური მუშაობაზე, რომელიც ორივე ენის მოდელების შესახებ და საშ ჩვენ აქ აკეთებთ ნულ სტრიქტის გაგრძელება განსაკუთრებულად მულტი ენგუფიკაციაში. ჩვენ გვეძლევა ერთადერთი ინტერატიგური განაკლების პროცემი, რომელიც სისტემის მიერ გადაქმნილი სისტემის განაკლებების ეუბალეობის პროცემის გამოყენება. სისტემიდან გამოყენებული წარმატები (sub- optimal since they contain mixed language from the shared vocabulary), შემდეგ გამოყენებულია ორიგინალური პარალელური მონაცემებით ერთად გამოყენებული მრავალური ქსელის წარმატებისთვის და ისტორიურად ეს სისტემის შესაძლებელია თავიდან გავისწავლა და უფრო უფრო მეტი გამოყენება. ჩვენი პროგორმა ჩვენი მრავალენგური მოდელის ორი ნულ სტრიქტის მიზეზების უფრო ეფექტიურია. განსაკუთრებულია, ჩვენ დავხედავთ 9 BLEU წერტილების მიღება მრავალენგური მოდელზე და 2.08 BLEU წერტილის მიღება ორი მედელზე გამოყენებული. დამატებული ანალიზი ჩვენებს, რომ ასევე ცოტა უფრო მეტად უფრო მეტად უფრო მეტად უფრო მეტად იქნება სიტყვების მხარეს.', 'it': "I recenti lavori sulla traduzione automatica neurale multilingue hanno riportato prestazioni competitive rispetto ai modelli bilingui e prestazioni sorprendentemente buone anche su direzioni di traduzione (zero shot) non osservate durante l'allenamento. Investighiamo qui una traduzione zero-shot in un ambiente multilingue particolarmente a basso contenuto di risorse. Proponiamo una semplice procedura di formazione iterativa che sfrutta una dualità di traduzioni generate direttamente dal sistema per le direzioni zero-shot. Le traduzioni prodotte dal sistema (non ottimali in quanto contengono lingua mista dal vocabolario condiviso), vengono poi utilizzate insieme ai dati paralleli originali per alimentare e rieducare iterativamente la rete multilingue. Nel tempo, questo permette al sistema di imparare dal proprio output generato e sempre più migliore. Il nostro approccio dimostra di essere efficace nel migliorare le due direzioni zero-shot del nostro modello multilingue. In particolare, abbiamo osservato guadagni di circa 9 punti BLEU rispetto a un modello multilingue di base e fino a 2,08 punti BLEU rispetto a un meccanismo pivoting utilizzando due modelli bilingui. Un'ulteriore analisi mostra che c'è anche un leggero miglioramento nelle direzioni linguistiche non-zero-shot.", 'lt': 'Neseniai atliktas daugiakalbių nervinių mašinų vertimo darbas pranešė apie konkurencinius rezultatus dvikalbių modelių at žvilgiu ir stebimai gerus rezultatus net mokymo metu nenustatytose (nulinės nuotraukos) vertimo kryptimis. Mes čia tiriame nulinį vertimą ypač mažai išteklių turinčiomis daugiakalbėmis aplinkybėmis. Siūlome paprastą pakartotinę mokymo procedūrą, kuri suteiks svertą vertimų dvigubumui, kurį tiesiogiai sukuria nulinės nuotolinės krypties sistema. Sistemos parengti vertimai (nepakankamai optimal ūs, nes juose yra mišrių kalbų iš bendro žodyno) naudojami kartu su originaliais lygiagrečiais duomenimis daugiakalbiams tinklams perduoti ir pakartotinai atnaujinti. Laikui bėgant, tai leidžia sistemai pasimokyti iš savo sukauptų ir vis geresnių rezultatų. Mūsų požiūris rodo, kad yra veiksmingas gerinant dvi mūsų daugiakalbio modelio nulinės nuotraukos kryptis. In particular, we observed gains of about 9 BLEU points over a baseline multilingual model and up to 2.08 BLEU over a pivoting mechanism using two bilingual models.  Iš tolesnės analizės matyti, kad kalbos kryptis, kurios nėra nulinės nuotraukos, taip pat šiek tiek pagerėjo.', 'ms': 'Kerja baru-baru ini mengenai terjemahan mesin saraf berbilang bahasa melaporkan prestasi kompetitif terhadap model dua bahasa dan prestasi yang mengejutkan yang baik walaupun pada arah terjemahan (0-shot) tidak dilihat pada masa latihan. Kami menyelidiki di sini terjemahan 0-shot dalam seting berbilang bahasa yang sangat rendah sumber. Kami cadangkan prosedur latihan berulang yang mudah yang menggunakan dualitas terjemahan yang secara langsung dijana oleh sistem untuk arah tembakan sifar. Terjemahan yang dihasilkan oleh sistem (sub-optimal kerana ia mengandungi bahasa campuran dari vokbulari terkongsi), kemudian digunakan bersama dengan data paralel asal untuk memberi makan dan melatih semula secara berulang-ulang rangkaian berbilang bahasa. Selama masa, ini membolehkan sistem belajar dari hasilnya sendiri dan output yang semakin baik. pendekatan kita menunjukkan berkesan dalam memperbaiki dua arah tembakan sifar model berbilang bahasa kita. Secara khusus, kami memperhatikan keuntungan kira-kira 9 titik BLEU melalui model berbilang bahasa asas dan hingga 2.08 BLEU melalui mekanisme pemutaran menggunakan dua model dua bahasa. Analisis lanjut menunjukkan bahawa terdapat juga sedikit peningkatan dalam arah bahasa bukan-0-shot.', 'kk': 'Жуырдағы жұмыс көп тілді невралдық компьютердің аудармасында екі тіл үлгілеріне қатынау жұмыс жұмыс жұмыс істейді. Жұмыс уақытта қаралмаған (zero- shot) аудармасының бағыттарына қаралмаған Біз мұнда бірнеше көп тілік баптауларындағы нөл түрлендірімізді зерттейміз. Біз қарапайым қарапайым оқыту процедурын таңдаймыз. Бұл жүйе нөл түрлі бағыттарына тұратын аудармалардың екі қарапайым қарапайым түрлендіру процедурын жасайды. Жүйенің жасалған аудармалары (бөлек- оптимал тілдері ортақтастырылған сөздерден аралас тілдері) содан кейін көптілік желіні ақпараттау және қайталау үшін бастапқы параллель деректермен бірге қолданылады. Уақыт бойынша, бұл жүйе өзінің құрылған және жақсы шығысынан үйренуге мүмкіндік береді. Біздің тәсіліміз бірнеше тіл үлгісінің екі нөл түрлі бағыттарын жасау үшін эффективні болады. Әрине біз 9 BLEU нүктелерін негізгі көптілік үлгісінен және 2,08 BLEU көпшілік үлгісін қолдану механизміне көпшілік механизміне көп тіл үлгісін қолданып көрдік. Қосымша анализ нөл емес тіл бағыттарында бірнеше жақсарту бар дегенді көрсетеді.', 'mk': 'Неодамнешната работа на мултијазичкиот превод на невројални машини објави конкурентна резултат во однос на двојјазичните модели и изненадувачки добра резултат дури и на (нултијазични) преводи кои не се набљудуваат во време на тренинг Овде истражуваме превод со нула снимка во посебно ниско ресурсно мултијазичко место. Предложуваме едноставна процедура на итеративна обука која влијае на двојноста на преводи директно генерирани од системот за нуларни насоки. Преводите произведени од системот (суб-оптимални бидејќи тие содржат мешан јазик од споделениот речник), потоа се користат заедно со оригиналните паралелни податоци за храна и повторно обука на мултијазичката мрежа. Со текот на времето, ова му овозможува на системот да научи од сопствениот генериран и сé подобар излез. Нашиот пристап покажува дека е ефикасен во подобрувањето на двете нулански насоки на нашиот мултијазичен модел. In particular, we observed gains of about 9 BLEU points over a baseline multilingual model and up to 2.08 BLEU over a pivoting mechanism using two bilingual models.  Понатамошната анализа покажува дека постои и мало подобрување во насоките на јазикот без нула снимка.', 'ml': 'ട്രെയിനിക്ക് സമയത്ത് കണ്ടിട്ടില്ലാത്ത പല ഭാഷകങ്ങളുടെ ന്യൂറല്\u200d യന്ത്രങ്ങളുടെ പരിഭാഷണത്തിന്റെ അടുത്തുള്ള പ്രവര്\u200dത്തനങ്ങള്\u200d പരിശോധിപ്പി നമ്മള്\u200d ഇവിടെ ഒരു പൂര്\u200dണ്ണമായ വെടിവെക്കപ്പെട്ട പരിഭാഷത്തെ അന്വേഷിക്കുന്നു. പ്രത്യേകിച്ച് കുറഞ്ഞ വിഭ സിസ്റ്റത്തില്\u200d നേരിട്ട് നേരിട്ട് നിര്\u200dമ്മിക്കുന്ന വിഭാഗങ്ങളുടെ രണ്ടു വിഭാഗങ്ങള്\u200d ഉല്\u200dപാദിക്കുന്ന ഒരു എളുപ്പമാ The translations produced by the system (sub-optimal since they contain mixed language from the shared vocabulary), are then used together with the original parallel data to feed and iteratively re-train the multilingual network.  സമയം കഴിഞ്ഞാല്\u200d, സ്വന്തം സൃഷ്ടിച്ചുണ്ടാക്കിയ സിസ്റ്റമില്\u200d നിന്നും കൂടുതല്\u200d മെച്ചപ്പെട്ട ഫലം പഠ നമ്മുടെ സാഹചര്യം നമ്മുടെ പല ഭാഷ മോഡലിന്റെ രണ്ട് പൂജ്യത്തിന്റെ വഴികള്\u200d മുന്\u200dകൂട്ടുന്നതില്\u200d പ്രാപ്തികമായ പ്രത്യേകിച്ച്, നമ്മള്\u200d ബെസ്ലിയുവിന്റെ കൂട്ടത്തില്\u200d 9 ബെല്ലു പോയിന്\u200dറുകള്\u200d കണ്ടിട്ടുണ്ട്. ഒരു ബെസ്റ്റ്ലൈന്\u200d മൊഡലിന്\u200dറെ മേലും 2.08 ബെല കൂടുതല്\u200d അന്വേഷണം കാണിക്കുന്നത് പൂജ്യത്തില്ലാത്ത ഭാഷ മാര്\u200dഗങ്ങളില്\u200d കുറച്ച് മെച്ചപ്പെടുത്തുന്നത', 'mt': "Recent work on multilingual neural machine translation reported competitive performance with respect to bilingual models and surprisingly good performance even on (zero-shot) translation directions not observed at training time.  Hawnhekk ninvestigaw traduzzjoni mingħajr skopijiet f’ambjent multilingwi b’riżorsi partikolarment baxxi. Aħna qed nipproponu proċedura sempliċi ta' taħriġ iterattiv li tagħti spinta lid-doppju ta' traduzzjonijiet iġġenerati direttament mis-sistema għad-direzzjonijiet mingħajr skop. It-traduzzjonijiet prodotti mis-sistema (subottimali peress li fihom lingwa mħallta mill-vokabulari kondiviż), imbagħad jintużaw flimkien mad-dejta parallela oriġinali biex jiġi alimentat u jitħarreġ mill-ġdid b’mod iterattiv in-netwerk multilingwi. Maż-żmien, dan jippermetti lis-sistema titgħallem mill-produzzjoni ġġenerata u dejjem aktar aħjar tagħha. L-approċċ tagħna juri li huwa effettiv fit-titjib taż-żewġ direzzjonijiet żero-shot tal-mudell multilingwi tagħna. B’mod partikolari, osservajna qligħ ta’ madwar 9 punti BLEU fuq mudell multilingwi ta’ linja bażi u sa 2.08 BLEU fuq mekkaniżmu ta’ pivoting bl-użu ta’ żewġ mudelli bilingwi. Further analysis shows that there is also a slight improvement in the non-zero-shot language directions.", 'no': 'Nyleg arbeid på fleirspråk neuralmaskinsomsetjing rapporterte konkurentivt utvikling med respekt til bilinguelmodeller og overraska godt utvikling sjølv på (null- shot) omsetjingsrektingar som ikkje er observert på treningstid. Vi undersøker ei omsetjing med null-bilde i eit spesielt lite ressurs-multispråk innstilling. Vi foreslår ein enkel iterativ opplæringsprosess som leverer ein dualitet av omsetjingar direkte oppretta av systemet for retningane med nullstilt. Omsetjingane produsert av systemet (underoptimalt sidan dei inneheld blandet språk frå delt ordbok), vert derfor bruka saman med den opprinnelige parallelle data for å køyra og gjentakast fleire språk på nytt. I løpet av tid kan systemet lære frå eigne genererte og større bedre utdata. Tilnærminga vårt viser å vera effektivt i å forbedra dei to retningane med nullsatt i vårt fleirspråk modell. I særskilt observerte vi oppnådd av omtrent 9 BLEU-punkt over ein grunnspråk multispråk modell og til 2,08 BLEU over ein pivotmekanisme med to bilinguelt modeller. Fleire analyser viser at det finst også ein liten forbedring i språkkretningane som ikkje er null.', 'pl': 'Ostatnie prace nad wielojęzycznym tłumaczeniem maszynowym neuronowym odnotowały konkurencyjność w odniesieniu do modeli dwujęzycznych i zaskakująco dobrą wydajność nawet w kierunkach tłumaczenia (zero-shot) nieobserwowanych w czasie treningu. Badamy tutaj tłumaczenie zero-shot w szczególnie niskiej ilości zasobów wielojęzycznych. Proponujemy prostą iteracyjną procedurę szkoleniową, która wykorzystuje dualność tłumaczeń generowanych bezpośrednio przez system dla kierunków zerowych strzałów. Tłumaczenia stworzone przez system (nieoptymalne, ponieważ zawierają język mieszany ze wspólnego słownictwa) są następnie wykorzystywane wraz z oryginalnymi danymi równoległymi do zasilania i iteracyjnego przeszkolenia sieci wielojęzycznej. Z biegiem czasu pozwala to systemowi uczyć się na własnych generowanych i coraz lepszych wynikach. Nasze podejście okazuje się skuteczne w ulepszaniu dwóch kierunków zerowych naszego wielojęzycznego modelu. W szczególności obserwowano zyski około 9-punktów BLEU w porównaniu z modelem wielojęzycznym bazowym oraz do 2.08 BLEU w porównaniu z mechanizmem obrotowym wykorzystującym dwa modele dwujęzyczne. Dalsza analiza pokazuje, że nastąpiła również nieznaczna poprawa kierunków językowych niezerowych.', 'mn': 'Хоёр хэл загварын тухай өрсөлдөөний үйл ажиллагааг олон хэл мэдрэлийн механикийн хөгжлийн дамжуулалтын тухай ажиллагаа хоёр хэл загварын тухай харуулсан. Гэтэл гайхалтай үйл ажиллагаа нь сургалтын цаг ху Бид энд хэлний олон хэлний хэлбэрээс бага боломжтой бага боломжтой түвшинд 0-н зурагт орнуудыг судалж байна. Бид энгийн давтамжлалтын давтамжлалтын процедурыг санал болгож байна. Энэ нь системээс шууд зураг шууд үүсгэсэн орнуудын хоёр давтамжлалтыг ашигладаг. Олон хэлний сүлжээгээс холбогдсон хэл байдаг учраас системээс гаргасан хөгжлийн хөгжлийн хөгжлийн хөгжлийн хөгжлийн хөгжлийн хөгжлийн хөгжлийн үндсэн параллель өгөгдлийн хамт хэрэглэгддэг. Цаг хугацааны дараа системийг өөрийн бий болгон, илүү сайн үр дүнээс суралцах боломжтой болгодог. Бидний арга зам нь олон хэлний загварын хоёр зураг загварыг сайжруулахад үр дүнтэй болж байна. Ялангуяа бид олон хэл загварын суурь шугам дээр 9 БЛЕС цэгүүдийг харсан ба 2 хэл загварыг ашиглан 2.08 БЛЕС хүртэл 2.08 БЛЕС хүртэл хоёр хэл загварыг ашигладаг. Дараа нь шинжилгээ нь тэр хэл биш хэлний замаар бага зэрэг сайжруулагддаг гэдгийг харуулж байна.', 'ro': 'Lucrările recente privind traducerea automată neurală multilingvă au raportat performanțe competitive în ceea ce privește modelele bilingve și performanțe surprinzător de bune chiar și pe direcțiile de traducere (zero-shot) care nu au fost observate în timpul antrenamentului. Investigăm aici o traducere zero-shot într-un cadru multilingv deosebit de scăzut de resurse. Propunem o procedură simplă de instruire iterativă care valorifică o dualitate a traducerilor generate direct de sistem pentru direcțiile zero-shot. Traducerile produse de sistem (sub-optime deoarece conțin limbaj mixt din vocabularul comun), sunt apoi folosite împreună cu datele paralele originale pentru a alimenta și re-instrui iterativ rețeaua multilingvă. De-a lungul timpului, acest lucru permite sistemului să învețe din propria sa producție generată și din ce în ce mai bună. Abordarea noastră arată că este eficientă în îmbunătățirea celor două direcții zero-shot ale modelului nostru multilingv. În special, am observat câștiguri de aproximativ 9 puncte BLEU față de un model multilingv de bază și de până la 2,08 BLEU față de un mecanism pivotant utilizând două modele bilingve. Analizele suplimentare arată că există, de asemenea, o ușoară îmbunătățire a direcțiilor lingvistice non-zero-shot.', 'si': 'ගොඩක් භාෂාවක් න්\u200dයූරාල් මැෂින් වාර්තාවයේ අනුවෙන් වැඩ කරනවා දෙවල් භාෂාවක් මෝඩේල් වලට සම්බන්ධයෙන් ප්\u200dරශ්නය කර අපි මෙතන පරීක්ෂණය කරන්නේ ශූන්ය ශූන්ය වාර්තාවක් විශේෂයෙන් අඩුම භාෂාවක් සැකසුම් වල අපි සාමාන්\u200dය ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් ප්\u200dරශ්නය කරනවා, ඒකෙන් සුන්ධ විදිහට පද්ධතියෙන් ප්\u200dරශ් පද්ධතියෙන් නිර්මාණය කරපු භාෂාව (සුබ- හොඳ භාෂාව සම්බන්ධ කරපු භාෂාව සම්බන්ධ කරපු භාෂාව), පස්සේ පද්ධතිය සමාන්\u200dය ද වෙලාවෙන්, මේක පද්ධතියට පුළුවන් වෙනවා එයාගේ සිද්ධියෙන් ඉගෙන ගන්න හා වැඩිය හොඳ ප්\u200dරතිප අපේ පරීක්ෂණය පෙන්වන්නේ අපේ බොහොම භාෂාවක් මොඩේල් එකේ ශූන්ය විස්තර දෙකක් වැඩ කරන්න ප්\u200dරයෝජනය විශේෂයෙන්, අපි බලාගෙන ඉන්නේ බ්ලූස් 9 පින්තුරු වලින් අධාර්ථ භාෂාවක් වලින් අධාර්ථ භාෂාවක් වලින් අධාර්ථා තවත් විශ්ලේෂණය පෙන්වන්නේ නෑ ශූන් විසින් භාෂාවට ප්\u200dරශ්නයක් තියෙනවා කියලා.', 'sr': 'Nedavno rad na prevodu multijezičkih neuralnih strojeva prijavio je konkurentnu funkciju u pogledu dvojezičkih modela i iznenađujuće dobre izvedbe čak i na (nulotvorne) uputstva prevoda koje nisu primijećene u treningu. Istražujemo prevod od nule snimke u posebno manjoj multijezičkoj nastavi. Predlažemo jednostavnu iterativnu proceduru obuke koja utiče na dvostruku prevode koje je izravno stvorio sistem za upute nule pucnjave. Prevodi koji su proizvedeni sustavom (podoptimalni pošto sadrže miješani jezik iz zajedničkog re čnika), zatim se koriste zajedno sa originalnim paralelnim podacima za hranjenje i ponovno obučavanje multijezičke mreže. Tokom vremena, to omogućava sistemu da nauči od svojih proizvedenih i većih boljih izlaza. Naš pristup pokazuje da je efikasan u poboljšanju dva uputa našeg multijezičkog modela. Posebno, posmatrali smo dobitke od oko 9 BLEU bodova preko početnog multijezičkog model a i do 2,08 BLEU zbog mehanizma glasanja koji koristi dva dvojezička modela. Još jedna analiza pokazuje da postoji i malo poboljšanja u uputstvu jezika bez nule uputstva.', 'so': 'Recent work on multilingual neural machine translation reported competitive performance with respect to bilingual models and surprisingly good performance even on (zero-shot) translation directions not observed at training time.  Halkan waxaynu baaraynaa turjumaan aan aan zero lagu dhuftay, si gaar ah noocyo badan oo rasmi ah. Waxaynu soo jeedaynaa koorsooyin fudud oo waxbarasho si fudud ah, kaasoo soo saara tarjumaadyo kala duduwan oo si toos ah u soo dhashay nidaamka hagitaanka nuurka. Turjumaadda nidaamka (sub-optimal marka ay ku jiraan luqada isku xiran oo ay ka yimaadaan hadalka qayb-gaarka ah), waxaa lagu isticmaalaa isla markaasna la isticmaalaa macluumaadka asalka ah si ay u quudiyaan oo u isticmaalaan shabakadda luuqadaha kala duduwan. Waqti dheer, taasi waxay habboon in nidaamka laga barto midhihiisa dhalay oo aad u sii wanaagsan. Dhaqdhaheenna waxay muuqataa inay tayo ku yeelan karto horumarinta labada hagitaan oo noocyada luuqadaha kala duduwan. Si gaar ah, waxaynu aragnay qiyaastii 9 BLEU oo ku qoran qoraal luuqad kala duduwan iyo ilaa 2.08 BLEU oo ku qoran meymisyo codeynta oo isticmaalaya laba noocyo oo luuqad ah. Fiiritaanka dheeraad ah waxay muuqataa in sidoo kale hagaajinta qof yar oo ku jirta hagitaanka luqada aan zero lahayn.', 'sv': 'Nyligen utförda arbeten med flerspråkig neural maskinöversättning rapporterade konkurrenskraftig prestanda i förhållande till tvåspråkiga modeller och överraskande bra prestanda även på (noll-skott) översättningsriktningar som inte observerades vid träningstiden. Vi undersöker här en noll-shot översättning i en särskilt låg resurs flerspråkig miljö. Vi föreslår ett enkelt iterativt utbildningsförfarande som utnyttjar en dualitet av översättningar som genereras direkt av systemet för nollskotts riktningar. De översättningar som produceras av systemet (suboptimala eftersom de innehåller blandat språk från det delade ordförrådet) används sedan tillsammans med originaldata för att mata och iterativt vidareutbilda det flerspråkiga nätverket. Med tiden gör det möjligt för systemet att lära av sin egen genererade och allt bättre produktion. Vårt tillvägagångssätt visar sig vara effektivt när det gäller att förbättra de två riktningarna i vår flerspråkiga modell. Framför allt observerade vi vinster på cirka 9 BLEU-poäng jämfört med en flerspråkig basmodell och upp till 2,08 BLEU jämfört med en vridmekanism med två tvåspråkiga modeller. Ytterligare analyser visar att det också finns en liten förbättring i språkriktningarna utan nollskott.', 'ur': 'بہت سی زبان نورول ماشین کی ترجمہ پر اچھا کام دو زبان کی مدلکوں کے معاملہ میں مسابقات کی پرورش کی گزاری ہے اور (صفر-شٹ) ترجمہ کی طریقوں پر بھی (صفر-شٹ) اچھا کام بھی اچھا کام ہے جو تدریس وقت میں نہیں دیکھ ہم یہاں ایک صفر-شٹ کی ترجمہ کی تحقیق کررہے ہیں مخصوصاً کم منبع ملتی زبان تنظیم میں. ہم ایک ساده دوبارہ تکلیف کی روش کی پیشنهاد کرتے ہیں جو صفر-شٹ کی طرف سے سیسٹم نے سیسٹم کے ذریعے پیدا کیا ہے۔ سیسٹم کے ذریعہ پیدا کئے گئے ترجمے (اس کے بعد کہ ان میں شریک لکھنے کی زبان سے ملی ہوئی زبان ہے) اس کے بعد متحدہ لکھنے کے لئے اور دوبارہ متحدہ لکھنے کے لئے متحدہ لکھنے کے لئے متحدہ لکھنے کے لئے استعمال کئے جاتے ہیں۔ یہ سیسٹم کو اپنے پیدا ہونے سے اور زیادہ اچھی آوٹ سے سیکھنے کی اجازت دیتا ہے۔ ہمارا طریقہ دکھاتا ہے کہ ہمارے بہت سی زبان موڈل کی دو صفر شٹ کی دکھانے میں اثر ہے۔ مخصوصاً ہم نے نو بلیوس پوینٹوں کا فائدہ دیکھا تھا ایک بنسٹ لین متعدد زبان موڈل پر اور 2.08 بلیوس پر دو زبان موڈل کے مطابق ایک پیوٹ مکانیسی پر۔ اور اضافہ تحلیل دکھاتا ہے کہ بے صفر-شٹ زبان کی طریقوں میں بھی تھوڑا بہتر ہے.', 'ta': 'அண்மையில் பல மொழி புதிய இயந்திரத்தின் மொழிமாற்றி செயல்பாடு இரு மொழிகள் மாதிரிகள் பற்றிய முயற்சி மற்றும் ஆச்சரியமாக நல்ல செயல்பாடுகள் கூ நாம் இங்கே ஒரு சிறிது குறைந்த மூலத்தின் மொழிமாற்றம் இங்கே ஒரு பூஜ்ஜியம் சுட்ட மொழிமாற்றம் தேடுகிறோம நாம் ஒரு எளிய உருவாக்க பயிற்சி செயல்பாட்டை நிர்ணயிக்கும் மொழிபெயர்ப்புகள் நேரடியாக உருவாக்கப்பட்டுள்ளது என்று ம @ info மேலும் நேரத்திற்கு மேலும், இது கணினியை தன் சொந்த உருவாக்கிய மற்றும் அதிகமாக சிறந்த வெளியீடு எங்கள் செயல்பாடு நமது பல மொழி மாதிரியின் இரண்டு பூஜ்ஜியத்தை மேம்படுத்துவதற்கு தெரியும். குறிப்பிட்டு, நாங்கள் அடிப்படைக்கோட்டில் பல மொழி மாதிரியில் 9 பிலியு புள்ளிகளின் செல்வத்தை பார்த்தோம் மற்றும் 2.08 பிலியு மேல்  மேலும் ஆராய்ச்சி காட்டுகிறது பூஜ்ஜியமில்லாத மொழி திசைகளில் ஒரு சிறிய முன்னேற்றம் உள்ளது.', 'uz': "Yaqinda bir necha tilda neyron tarjima qilish vazifasi ikkita tillar modellari haqida rivojlanish muvaffaqiyatli berdi va juda ajoyib yaxshi bajarish natijasi haqida o'zgarishni taʼminlovchi vaqtda koʻrsatilmagan tizimga qaramadi. Biz bu yerda bir necha tillar tarjimasini o'rganamiz. We propose a simple iterative training procedure that leverages a duality of translations directly generated by the system for the zero-shot directions.  Name Vaqtdan ortiq, bu tizimni o'z yaratilgan va yaxshi natijadan o'rganishga ruxsat beradi. Bizning tilimizning ikki nuqta modelning ikki nuqta yo'zlarimizni o'zgartirish uchun ishlaydi. Ko'pchilik, biz bir necha tillar modelida 9 BLEU nuqta narsalarni ko'rib turdik va ikki tili modeldan foydalanadigan pivoting mechanismni 2.08 BLEU'ga ko'rdik. Koʻproq taʼminlovchi esa bu tilning hech narsa yo'q tizimda bir necha yaxshi o'zgarishni ko'rsatadi.", 'vi': 'Công việc gần đây về dịch chuyển máy thần kinh đa dạng báo cáo năng lượng cạnh tranh đối với các mô hình hai chiều và khả năng tốt ngạc nhiên thậm chí trên hướng dịch (quay không) chưa được quan sát vào thời điểm huấn luyện. Chúng tôi điều tra ở đây một bản dịch không phát vào một thiết lập ngôn ngữ nghèo nàn. Chúng tôi đề xuất một thủ tục đơn giản để thực hiện một bản dịch hai chiều trực tiếp do hệ thống tạo ra cho hướng bắn không. Các bản dịch sản xuất bởi hệ thống (tối ưu bởi chúng chứa ngôn ngữ trộn từ các từ phổ biến) được sử dụng cùng với các dữ liệu song song song để cung cấp thức ăn và theo thời gian tái tạo mạng đa dạng. Qua thời gian, điều này cho phép hệ thống học hỏi từ nguồn sản xuất của nó và ngày càng tốt hơn. Cách tiếp cận của chúng tôi cho thấy hiệu quả trong việc cải thiện hai hướng quay không của mô hình đa dạng. Chúng tôi quan sát được lợi nhuận của khoảng chín tiếng đứng trên một mô hình con đại học cơ bản và lên tới 2.08 cả tiếng bíp bằng cách dùng hai mô hình hai thứ hai. Phân tích thêm cho thấy cũng có một sự cải tiến nhỏ trong hướng ngôn ngữ không bắn bằng không.', 'da': 'Det seneste arbejde med flersproget neural maskinoversættelse rapporterede om konkurrencedygtige resultater i forhold til tosprogede modeller og overraskende gode resultater selv på (zero-shot) oversættelsesvejledninger, der ikke blev observeret på træningstidspunktet. Vi undersøger her en zero-shot oversættelse i en særligt lav ressource flersproget indstilling. Vi foreslår en simpel iterativ træningsprocedure, der udnytter en dualitet af oversættelser, der direkte genereres af systemet for nul-skud retninger. Oversættelserne produceret af systemet (suboptimale, da de indeholder blandet sprog fra det fælles ordforråd), bruges derefter sammen med de originale parallelle data til at fodre og gentræne det flersprogede netværk. Over tid giver dette systemet mulighed for at lære af sit eget genererede og stadig bedre output. Vores tilgang viser sig at være effektiv til at forbedre de to nulskudsretninger i vores flersprogede model. Vi observerede især gevinster på ca. 9 BLEU point i forhold til en flersproget basismodel og op til 2,08 BLEU i forhold til en drejemekanisme ved hjælp af to tosprogede modeller. Yderligere analyse viser, at der også er en lille forbedring i sprogretningerne uden nulskud.', 'bg': 'Последната работа по многоезичен невронен машинен превод отчита конкурентна производителност по отношение на двуезичните модели и изненадващо добра производителност дори при (нулеви) насоки на превод, които не са наблюдавани по време на тренировка. Ние разследваме тук един нулев превод в особено нискоресурсна многоезична обстановка. Предлагаме проста итеративна процедура за обучение, която използва дуалност на преводите, пряко генерирани от системата за направленията с нулев изстрел. Преводите, произведени от системата (не са оптимални, тъй като съдържат смесен език от споделения речник), след това се използват заедно с оригиналните паралелни данни за подаване и повторно обучение на многоезичната мрежа. С течение на времето това позволява на системата да се учи от собствената си генерирана и все по-добра продукция. Нашият подход показва, че е ефективен в подобряването на двете направления на нашия многоезичен модел. По-специално, наблюдавахме печалби от около 9 точки спрямо базов многоезичен модел и до 2,08 спрямо въртящ механизъм, използващ два двуезични модела. Допълнителният анализ показва, че има и леко подобрение в направленията на езика, които не са нулеви.', 'nl': 'Recent onderzoek naar meertalige neurale machinevertaling meldde concurrerende prestaties met betrekking tot tweetalige modellen en verrassend goede prestaties zelfs op (zero-shot) vertaalrichtingen die niet werden waargenomen tijdens de training. We onderzoeken hier een zero-shot vertaling in een bijzonder low-resource meertalige omgeving. We stellen een eenvoudige iteratieve trainingsprocedure voor die gebruik maakt van een dualiteit van vertalingen die rechtstreeks door het systeem worden gegenereerd voor de zero-shot richtingen. De door het systeem geproduceerde vertalingen (suboptimaal omdat ze gemengde taal uit de gedeelde woordenschat bevatten) worden vervolgens samen met de originele parallelle gegevens gebruikt om het meertalige netwerk te voeden en iteratief opnieuw te trainen. Na verloop van tijd kan het systeem leren van zijn eigen gegenereerde en steeds betere output. Onze aanpak blijkt effectief te zijn in het verbeteren van de twee zero-shot richtingen van ons meertalige model. In het bijzonder zagen we winsten van ongeveer 9 BLEU punten ten opzichte van een baseline meertalig model en tot 2.08 BLEU over een draaimechanisme met behulp van twee tweetalige modellen. Verdere analyse toont aan dat er ook een lichte verbetering is in de non-zero-shot taalrichtingen.', 'id': 'Pekerjaan baru-baru ini pada terjemahan mesin saraf multibahasa melaporkan prestasi kompetitif terhadap model dua bahasa dan prestasi yang mengejutkan yang baik bahkan pada arah terjemahan (0-shot) yang tidak diperhatikan pada waktu latihan. Kami menyelidiki di sini terjemahan nol dalam pengaturan multibahasa yang sangat rendah sumber daya. Kami mengusulkan prosedur latihan iteratif sederhana yang mempengaruhi dualitas terjemahan langsung dihasilkan oleh sistem untuk arah zero-shot. Terjemahan yang diproduksi oleh sistem (sub-optimal karena mereka mengandung bahasa campuran dari vokabular berbagi), kemudian digunakan bersama dengan data paralel asli untuk memberi makan dan terus menerus melatih ulang jaringan berbilang bahasa. Selama waktu, ini memungkinkan sistem untuk belajar dari hasilnya sendiri dan semakin baik output. pendekatan kita menunjukkan bahwa efektif dalam memperbaiki dua arah zero-shot model multibahasa kita. Terutama, kami memperhatikan keuntungan sekitar 9 poin BLEU atas model multibahasa dasar dan hingga 2,08 BLEU atas mekanisme pemutaran menggunakan dua model dua bahasa. Analisi lebih lanjut menunjukkan bahwa ada juga sedikit peningkatan dalam arah bahasa bukan-0-shot.', 'de': 'Neuere Arbeiten zur mehrsprachigen neuronalen maschinellen Übersetzung berichteten von wettbewerbsfähigen Leistungen in Bezug auf zweisprachige Modelle und überraschend guten Leistungen auch in (Zero-Shot) Übersetzungsrichtungen, die zum Trainingszeitpunkt nicht beobachtet wurden. Wir untersuchen hier eine Zero-Shot-Übersetzung in einem besonders ressourcenarmen mehrsprachigen Umfeld. Wir schlagen ein einfaches iteratives Trainingsverfahren vor, das eine Dualität von Übersetzungen nutzt, die direkt vom System für die Nullschussrichtungen generiert werden. Die vom System erstellten Übersetzungen (suboptimal, da sie gemischte Sprache aus dem gemeinsamen Vokabular enthalten) werden dann zusammen mit den ursprünglichen parallelen Daten verwendet, um das mehrsprachige Netzwerk zu speisen und iterativ neu zu trainieren. So lernt das System im Laufe der Zeit aus dem eigenen generierten und immer besseren Output. Unser Ansatz erweist sich als effektiv bei der Verbesserung der beiden Null-Schuss-Richtungen unseres mehrsprachigen Modells. Insbesondere konnten wir Gewinne von etwa 9 BLEU Punkten gegenüber einem mehrsprachigen Basismodell und bis zu 2.08 BLEU über einen Pivot Mechanismus mit zwei zweisprachigen Modellen beobachten. Weitere Analysen zeigen, dass es auch eine leichte Verbesserung der Nicht-Null-Schuss-Sprachrichtungen gibt.', 'ko': '최근 다국어 신경기계 번역에 관한 연구 보고서에 따르면 이중 언어 모델은 경쟁적인 성능을 가지고 있으며, 심지어 훈련 시 관찰하지 못한 (영포) 번역 방향에서도 놀라운 좋은 성능을 가지고 있다고 한다.우리가 여기서 연구한 것은 자원이 매우 적은 다언어 환경에서의 제로 렌즈 번역이다.우리는 간단한 교체 훈련 프로그램을 제시했는데 이 프로그램은 시스템이 직접 생성한 영포 방향의 평이의 대구성을 이용한다.그리고 시스템이 생성한 번역(차선, 어휘표를 공유하는 혼합언어를 포함하기 때문)은 원시 병행 데이터와 함께 사용되고 다중 언어 네트워크에 정보를 제공하며 반복적으로 재훈련된다.시간의 추이에 따라 시스템은 자신이 만들어낸 점점 더 좋은 출력에서 배울 수 있다.우리의 방법은 우리의 다국어 모델을 개선하는 두 개의 영포 방향에 효과적이다.특히 베이스라인 다국어 모델에 비해 BLEU 포인트가 약 9개, 이중 언어 모델을 사용하는 회전 기구에 비해 BLEU 포인트가 2.08개 증가한 것으로 관찰됐다.진일보한 분석에 의하면 비영렌즈 언어의 방향도 경미한 개선이 있었다.', 'sw': 'Kazi ya hivi karibuni kuhusu tafsiri ya mashine ya asili ya lugha mbalimbali iliripoti utendaji wa ushindani kwa ajili ya mifano miwili ya lugha na utendaji mzuri hata kwenye maelekezo ya tafsiri ambayo hayakutazamwa wakati wa mafunzo. Tunachunguza hapa tafsiri yenye risasi sifuri katika mazingira ya rasilimali yenye chini ya lugha mbalimbali. Tunazipendekeza mbinu rahisi za mafunzo yenye utaratibu wa tafsiri zinazotengenezwa moja kwa moja na mfumo kwa njia zisizo na risasi. Tafsiri zilizotengenezwa na mfumo (yenye optimal kwa sababu zina lugha tofauti kutoka kwenye lugha inayoshirikiana), zinatumiwa pamoja na taarifa za asili zinazofanana ili kulisha na kufundisha mtandao wa lugha mbalimbali. Kwa muda, hii inaruhusu mfumo kujifunza kutoka kwa uzalishaji wake mwenyewe na kuongezeka kwa matunda bora zaidi. Hatua yetu inaonyesha kuwa na ufanisi katika kuboresha mitazamo miwili yenye picha sifuri ya mifano yetu ya lugha mbalimbali. Kwa hakika, tulishuhudia mafanikio ya takriban pointi 9 BLEU juu ya mifano ya lugha mbalimbali ya msingi na hadi 2.08 BLEU juu ya mfumo wa kupiga kura kwa kutumia mifano miwili ya lugha. Uchambuzi zaidi unaonyesha kuwa kuna maendeleo kidogo katika maelekezo ya lugha isiyo na sifuri.', 'fa': 'کارهای اخیرا روی ترجمه ماشین عصبی\u200cهای زیادی زبان گزارش داده شده است که عملکرد رقابتی با ارتباط به مدل دو زبان و عملکرد فوق العاده\u200cای خوب حتی در راهنمایی ترجمه\u200cهای صفر در زمان آموزش مشاهده نشده است. ما در اینجا یک ترجمه صفر در یک تنظیم بسیار کم منابع زیادی زبان تحقیق می کنیم. ما پیشنهاد می\u200cکنیم یک روش آموزش ساده\u200cای که دوگاهی از ترجمه\u200cها را مستقیما توسط سیستم برای مسیرهای صفر شلیک می\u200cکند. ترجمه\u200cهایی که توسط سیستم تولید می\u200cشود (از زمانی که آنها زبان\u200cهای مختلف از واژه\u200cهای مشترک را دارند) سپس با داده\u200cهای متناسب اصلی برای تغذیه و دوباره آموزش شبکه چندین زبان استفاده می\u200cشوند. در طول زمان، این به سیستم اجازه می دهد که از طریق تولید شده و بیشتر نتیجه بهتر یاد بگیرد. دستور ما نشان می دهد که در improving the two zero-shot directions of our multilingual model موثر است. در خصوص، ما در مقابل ۹ نقطه بلوپ بر روی یک مدل متعدد زبان پایین و تا ۲.۸۸ بلوپ بر روی یک مکانیسم بازی با استفاده از دو مدل دو زبان مشاهده کردیم. تحلیل بیشتری نشان می دهد که در مسیرهای زبان غیر صفر نیز کمی بهتر شده.', 'tr': 'Ýakynda köp dilli näyral maşynyň terjimesinde ýüz dil nusgasyna sereden täsirli eserleşmeler we hatda (zero-shot) terjime edilýän terjime edilýän işlemlerde hem gowy eserleşmeler berildi. Biz bu ýerde 0-atly terjime edip ýakyn bir çoxly dil düzümlerinde. Biz ýeňil bir tekrarat taýýarlama prosedüsini teklip edýäris ki sistemiň täzeden-de bir terjime döredigini 0-at yönünde döretýär. Sistem tarapyndan üretilen terjimeler Bu sistemiň öz üretildiklerinden öwrenmesini we daha gowy çykarmagyny mümkin edýär. Biziň ýaryşymyz, biziň multi dil nusgasymyzyň iki görnöşi ýüze çykmakda täsirli bolup görünýär. Özellikde biz 9 BLEU noktalaryň üstüni bilim dili modeliniň arasynda we 2.08 BLEU-a çenli iki dil modeliniň ullanýan terjime mekanizmasynda gözledik. Diňe analýusy 0-dan ýok dillerinde biraz gowurak bardygyny görkezýär.', 'hr': 'Nedavno rad o prevodu multijezičkih neuralnih strojeva prijavio je konkurentne učinke u pogledu dvojezičkih modela i iznenađujuće dobar učinkovit čak i na uputstvima za prevod (nulo-snimanje) koje nisu primjećene na vrijeme obuke. Ovdje istražujemo prevod nula pucnjave u posebno višejezičkom postavku s niskim resursima. Predlažemo jednostavnu postupku za iterativnu obuku koja utječe na dvostruku prevodu izravno proizvedenu sustavom uputstva na nulu uputstva. Prevodi proizvođeni sustavom (podoptimalni pošto sadrže pomiješan jezik iz zajedničkog riječnika), zatim se koriste zajedno s originalnim paralelnim podacima za hranjenje i ponovno obučavanje multijezičke mreže. Tijekom vremena, to omogućava sistemu naučiti od svojih proizvedenih i većih boljih izlaza. Naš pristup pokazuje da je učinkovit u poboljšanju dva uputa našeg multijezičkog modela. Posebno, posmatrali smo dobitke od oko 9 BLEU bodova na početnom multijezičkom modelu i do 2,08 BLEU zbog mehanizma pivotiranja koristeći dva dvojezička model a. Daljnja analiza pokazuje da postoji i malo poboljšanja u smjerovima jezika koji nisu upućeni na nulu.', 'af': "Onlangse werk op multitaalske neural masjien vertaling het aangewys gemeenskaplike prestasie met respek na twee tale modele en verwonderbaar goeie prestasie selfs op (zero- shot) vertalingsdireksies wat nie op onderwerp tyd aangewys word nie. Ons ondersoek hier 'n nul-skoot vertaling in 'n bepaalde lae-hulpbron multilinguele instelling. Ons voorstel 'n eenvoudige iteratiewe onderwerking prosedure wat 'n duweliteit van vertalings direk deur die stelsel genereer word vir die nulskoot rigtings. Die vertalings wat deur die stelsel geproduseer word (sub- optimal omdat hulle gemengde taal bevat van die gedeelde woordeboek), word dan saam met die oorspronklike parallele data gebruik word om die multitaal netwerk te voer en iteratief hertrein. Hierdie laat die stelsel toe om van sy eie genereerde en vermeerder beter uitvoer te leer. Ons toegang wys dat effektief is in die verbetering van die twee nul-skoot rigtings van ons multitaal model. Ons het besonderlik aangesien van omtrent 9 BLES punte oor 'n basisline multitaal model en tot by 2.08 BLES oor 'n pivoting mekanisme deur twee bilinguele modele te gebruik. Verdere analisie wys dat daar ook 'n klein verbetering is in die non-zero-shot taal rigtings.", 'sq': 'Puna e fundit mbi përkthimin e makinave nervore shumëgjuhëse raportoi performancë konkurruese lidhur me modelet dygjuhëse dhe performancë surprizuese të mirë edhe në drejtimet e përkthimit (zero-shot) që nuk u vëzhguan në kohën e stërvitjes. Ne hetojmë këtu një përkthim zero-shot në një ambient veçanërisht të ulët me burime shumëgjuhësore. Ne propozojmë një procedurë të thjeshtë të trajnimit të përsëritur që nxjerr një dualitet përkthimesh të gjeneruar direkt nga sistemi për drejtimet zero-shot. Përkthimet e prodhuara nga sistemi (sub-optimal pasi ato përmbajnë gjuhë të përziera nga fjalorin e përbashkët), përdoren së bashku me të dhënat origjinale paralele për të ushqyer dhe përsëritur rindërtimin e rrjetit shumëgjuhës. Me kalimin e kohës, kjo lejon sistemin të mësojë nga prodhimi i vet i gjeneruar dhe gjithnjë e më i mirë. Përqafimi ynë tregon se është efektiv në përmirësimin e dy drejtimeve zero-shot të modelit tonë shumëgjuhës. Veçanërisht, kemi vëzhguar fitime prej rreth 9 pikësh BLEU lidhur me një model bazë shumëgjuhës dhe deri në 2.08 BLEU lidhur me një mekanizëm rrotullues duke përdorur dy modele dygjuhësh. Analiza e mëtejshme tregon se ka gjithashtu një përmirësim të vogël në drejtimet e gjuhës jo zero-shot.', 'hy': 'Բազլեզու նյարդային մեքենայի թարգմանման վերջին աշխատանքները հայտարարեցին երկլեզու մոդելների հետ կապված մրցակցության արդյունք և զարմանալիորեն լավ արդյունք նույնիսկ (զրոյի) թարգմանման ուղղությունների վրա, որոնք կրթության ժամանակ չե Այստեղ մենք ուսումնասիրում ենք զրոյական թարգմանություն հատկապես ցածր ռեսուրսների բազլեզու միջավայրում: Մենք առաջարկում ենք մի պարզ կրկնվող ուսումնասիրության գործընթաց, որը օգտագործում է թարգմանությունների երկուսականությունը, որոնք անմիջապես ստեղծվում են համակարգով զրոյի ուղղությունների համար: Համակարգման կողմից արտադրված թարգմանությունները (ենթաօպտիմալ, քանի որ դրանք պարունակում են տարբեր լեզուներ ընդհանուր բառարանից), հետո օգտագործվում են միասին սկզբնական զուգահեռ տվյալների հետ, որպեսզի կերակրեն և կրկնօրինակ վերադասա Ժամանակի ընթացքում սա թույլ է տալիս համակարգին սովորել իր սեփական ստեղծված և ավելի լավ արդյունքից: Մեր մոտեցումը ցույց է տալիս, որ արդյունավետ է բարելավում մեր բազլեզու մոդելի երկու զրոյի ուղղությունները: In particular, we observed gains of about 9 BLEU points over a baseline multilingual model and up to 2.08 BLEU over a pivoting mechanism using two bilingual models.  Ավելի վերլուծությունը ցույց է տալիս, որ ոչ զրոյական լեզվի ուղղություններում նույնպես փոքր բարելավում է:', 'am': 'በአሁኑ የቋንቋ ቋንቋዎች የናውሬል መተርጓም ላይ የሚደረገውን ሥራ በሁለት ቋንቋዎች እና በተደናቂው መልካም የድምፅ ስርዓት (zero-shot) በተማሪዎች ጊዜ ያልታወቀ ትርጓሜዎች ላይ ተቃውሞ የተቃወመ ትርጓሜ ገልጸዋል፡፡ ወደዚህ በክፍለ ዋና በቋንቋ ቋንቋ ውስጥ የzero-shot ትርጓሜን እናምርመራለን፡፡ በጽሑፉ አካባቢ የዘረጉትን ትርጓሜዎች በቁጥር የተፈጠረውን እናስገልጣለን፡፡ በተለያዩ ቋንቋ ውስጥ የተለየ ቋንቋ ካደረጉት ጀምሮ የተጠቃሚ ትርጉም በተለየ እና በተለየ መረብ ለመጠቀም እና ለመጠቀም እንደተጨማሪም መረብ ለመጠቀም በተቀማሚ ተርጓሚዎች ተጠቃሚ ነው፡፡ Over time, this allows the system to learn from its own generated and increasingly better output.  የሁለቱ የቋንቋ ቋንቋዎች ዓይነታችንን አካባቢ ማሻሻል የሚያሳየው ነው፡፡ በተለይም የ9 ቢልዩን ነጥቦች በብዛት ቋንቋ እና በ2.08 ቢሊዩን በሁለት የቋንቋ ምሳሌዎችን በመጠቀም የፎቶ አካሄድ አካሄድን አየን፡፡ በተጨማሪም ማስታወቂያው በቋንቋ-ቋንቋ ቋንቋ መግለጫ ጥቂት ማድረግ እንዳለ ያሳያል፡፡', 'bn': 'সাম্প্রতিক ভাষায় নিউরাল মেশিন অনুবাদের কাজের ব্যাপারে প্রতিযোগিতা প্রদান করেছে দুই ভাষার মডেল এবং বিস্ময়কর ভালো কার্যক্রমের প্রতি এবং  আমরা এখানে একটি শূন্য-গুলি অনুবাদ অনুসন্ধান করি বিশেষ করে একটি কম-সম্পদ মাল্টিভাষায়। আমরা একটি সাধারণ প্রশিক্ষণ প্রস্তাব করি যা সিস্টেমের দ্বিতীয় অনুবাদের দ্বিতীয়তা বানিয়ে দিয়েছে যা শুধুমাত্র গুলি নির্দ সিস্টেমের দ্বারা তৈরি করা অনুবাদ (সাব-optimal যেহেতু তারা শেয়ার করা শব্দভাণ্ডার থেকে মিশ্র ভাষা থাকে), তারপর মূল পার্লেল ডাটার সাথে ব্যবহার করা হয়, যা সময়ের মধ্যে, এটি সিস্টেম তার নিজের উৎপাদন থেকে শিখতে পারে এবং আরও ভালো আউটপুট থেকে শিখতে পারে। আমাদের প্রতিক্রিয়া দেখা যাচ্ছে আমাদের বহুভাষার মডেলের দুটি শুটের দিকে উন্নতি করার জন্য কার্যকর। বিশেষ করে, আমরা প্রায় ৯ বিলিউ পয়েন্টের অর্থ দেখেছি একটি বেস্টালাইন মাল্টিভাষার মডেলের উপর এবং দুই ভাষার মডেল ব্যবহার করে পিভোটিং মেক্সিমেন আরো বিশ্লেষণ দেখাচ্ছে যে শুধুমাত্র শব্দের নির্দেশে একটু উন্নতি রয়েছে।', 'az': 'Müxtəlif dil nöral maşına çevirilməsi haqqında yeni işlər iki dil modellərə qarşı müəllif performansı bildirdi və təhsil vaxtında görmədiyimiz (0-shot) çevirilməsi tərzlərində də şaşırtıcı təhsil etdi. Biz burada çoxlu dil qurğularında sıfır-fəsad tercümünü incidirik. Biz sadəcə bir iterativ təhsil prosedüsini təklif edirik ki, sistemin sıfır-vuruş yönəltməsi üçün sistemin təhsil edilmiş təhsil edilməsini dəyişdirər. Sistemdən ürəklənən tərzimlər (paylaşılan sözlərdən karışıq dillər barəsində) sonra, çoxlu dil ağını yedirmək və tekrar tərzim etmək üçün orijinal paralel məlumatlarla birlikdə istifadə edilir. Bu sistemin özünün yaratdığı və daha xeyirli çıxartdığını öyrənməsinə izin verir. Bizim tərzimiz çoxlu dil modelimizin iki sıfır tərəflərini yaxşılaşdırmaq üçün faydalanır. Özellikle, biz çoxlu dil modeli üzərində 9 BLEU nöqtələrinin üstünü və 2.08 BLEU-ə qədər iki dil modeli istifadə edən pivot mehānismi ilə görürdük. Daha çox analizi göstərir ki, sıfır olmayan dil tərəflərində də də az bir improvement var.', 'cs': 'Nedávné práce na vícejazyčném neuronovém strojovém překladu zaznamenaly konkurenční výkon s ohledem na dvojjazyčné modely a překvapivě dobrý výkon i na směrech překladu (nulový výstřel), který nebyl pozorován v době tréninku. Zde zkoumáme nulový překlad v obzvláště nízkém mnohojazyčném prostředí. Navrhujeme jednoduchý iterativní tréninkový postup, který využívá duality překladů generovaných přímo systémem pro směry nulového výstřelu. Překlady vytvořené systémem (suboptimální, protože obsahují smíšený jazyk ze sdílené slovní zásoby), jsou pak použity společně s původními paralelními daty pro napájení a iterativní přeškolení vícejazyčné sítě. Postupem času to umožňuje systému učit se z vlastního generovaného a stále lepšího výstupu. Náš přístup ukazuje, že je efektivní při zlepšování dvou směrů nulového záběru našeho vícejazyčného modelu. Zejména jsme pozorovali zisky asi devíti BLEU bodů oproti základnímu vícejazyčnému modelu a až 2.08 BLEU nad otočným mechanismem pomocí dvou dvojjazyčných modelů. Další analýza ukazuje, že dochází i k mírnému zlepšení jazykových směrů non-nulového záběru.', 'fi': 'Viimeaikaiset monikieliset neurokonekäännöstyöt raportoivat kilpailukyvystä kaksikielisten mallien suhteen ja yllättävän hyvästä suorituskyvystä jopa (nolla-shot) käännössuunnissa, joita ei ole havaittu harjoitushetkellä. Tutkimme tässä nollakäännöstä erityisen vähävaraisessa monikielisessä ympäristössä. Ehdotamme yksinkertaista iteratiivista harjoitusmenettelyä, joka hyödyntää järjestelmän suoraan tuottamien käännösten dualiteettia nollalaukauksen suuntiin. Järjestelmän tuottamat käännökset (jotka eivät ole optimaalisia, koska ne sisältävät sekakieltä jaetusta sanastosta), käytetään sitten yhdessä alkuperäisten rinnakkaistietojen kanssa monikielisen verkon syöttämiseen ja iteratiiviseen uudelleenkouluttamiseen. Ajan myötä järjestelmä voi oppia omasta ja yhä paremmasta tuotoksestaan. Lähestymistapamme on osoittautunut tehokkaaksi monikielisen mallimme kahden nollakuvan suunnan parantamisessa. Erityisesti havaittiin noin 9 BLEU-pistettä verrattuna monikieliseen perusmalliin ja enintään 2,08 BLEU-pistettä verrattuna kahta kaksikielistä mallia hyödyntävään pivoting-mekanismiin. Jatkoanalyysi osoittaa, että myös muiden kuin nollalaukausten kielten suunnissa on tapahtunut pientä parannusta.', 'ca': "Treballs recents sobre traducció de màquines neurals multillengües van reportar desempenys competitius en relació amb models bilingües i sorprenentment bons desempenys fins i tot en direccions de traducció (zero-shot) no observades en el temps d'entrenament. Investiguem aquí una traducció de zero en un entorn multillenguatge de baix recursos. Proposem un senzill procediment iteratiu d'entrenament que aprofiti una dualitat de traduccions generades directament pel sistema per a les direccions zero-shot. Les traduccions produïdes pel sistema (sub-optimals ja que contenen llenguatges mixtes del vocabulari compartit), es fan servir juntament amb les dades paralleles originals per alimentar i repetir la xarxa multillengua. Al llarg del temps, això permet al sistema aprendre de la seva pròpia producció generada i cada vegada millor. El nostre enfocament demostra que és eficaç per millorar les dues direccions de dispars zero del nostre model multilingüe. En particular, vam observar guanys d'uns 9 punts BLEU sobre un model multilingüe de base i fins a 2,08 BLEU sobre un mecanisme de votació utilitzant dos models bilingües. Further analysis shows that there is also a slight improvement in the non-zero-shot language directions.", 'bs': 'Nedavno rad o prevodu multijezičkih neuralnih strojeva prijavio je konkurentne učinke u pogledu dvojezičkih modela i iznenađujuće dobrih učinka čak i na (nul-snimanje) uputstva prevoda koje nisu primjećene na vrijeme obuke. Istražujemo prevod od nule snimke u posebno niskom resursu multijezičkom nastavu. Predlažemo jednostavnu iterativnu proceduru obuke koja utiče na dvostruku prevodu izravno stvorenu od sustava za upute nule pucnjave. Prevodi proizvođeni sustavom (podoptimalni jer sadrže miješani jezik iz zajedničkog re čnika), zatim se koriste zajedno sa originalnim paralelnim podacima za hranjenje i ponovno obučavanje multijezičke mreže. Preko vremena, to omogućava sistemu da nauči od svog stvorenog i većeg boljeg proizvoda. Naš pristup pokazuje da je učinkovit u poboljšanju dva uputa na nulu našeg multijezičkog modela. Posebno, posmatrali smo dobitke od oko 9 BLEU bodova na početnom multijezičkom modelu i do 2,08 BLEU-a zbog mehanizma glasanja koji koristi dva dvojezička modela. Još jedna analiza pokazuje da postoji i manje poboljšanje u uputstvu jezika bez nule uputstva.', 'et': 'Hiljutised tööd mitmekeelse neuromasintõlke valdkonnas näitasid konkurentsivõimelist võrreldes kakskeelsete mudelitega ja üllatavalt head jõudlust isegi (null-shot) tõlkesuunades, mida treeningu ajal ei täheldatud. Me uurime siin null-shot tõlget eriti vähese ressursiga mitmekeelses keskkonnas. Pakume välja lihtsa iteratiivse koolituse protseduuri, mis võimendab süsteemi poolt otseselt loodud tõlkete dualiteetsust null-shot suundades. Süsteemi poolt loodud tõlkeid (mis ei ole optimaalsed, sest need sisaldavad ühisest sõnavarast pärit segakeelt) kasutatakse seejärel koos algsete paralleelsete andmetega mitmekeelse võrgu toitmiseks ja iteratiivseks ümberõpetamiseks. Aja jooksul võimaldab see süsteemil õppida oma loodud ja üha paremast väljundist. Meie lähenemisviis näitab olevat tõhus meie mitmekeelse mudeli kahe null-shot suuna parandamisel. Eelkõige täheldati ligikaudu 9 BLEU-punkti võrra suuremat kasu võrreldes baaskeelse mitmekeelse mudeliga ja kuni 2,08 BLEU-punkti võrreldes pöördemohhanismiga, kus kasutati kahte kahekeelset mudelit. Täiendav analüüs näitab, et ka mitte-null-shot keele suunas on veidi paranenud.', 'sk': 'Nedavno delo na področju večjezičnega nevronskega strojnega prevajanja je poročalo o konkurenčni učinkovitosti glede na dvojezične modele in presenetljivo dobro učinkovitost tudi v smereh prevajanja brez strela, ki niso bile opažene v času treninga. Tukaj raziskujemo ničelni prevod v večjezičnem okolju s posebno nizkimi viri. Predlagamo preprost iterativni postopek usposabljanja, ki izkorišča dvojnost prevodov, ki jih neposredno ustvari sistem za smeri ničelnega strela. Prevodi, ki jih izdeluje sistem (neoptimalni, saj vsebujejo mešani jezik iz skupnega besedišča), se nato uporabijo skupaj z izvirnimi vzporednimi podatki za podajanje in ponovno usposabljanje večjezičnega omrežja. Sčasoma to omogoča sistemu, da se uči iz lastnih ustvarjenih in vedno boljših izhodov. Naš pristop kaže, da je učinkovit pri izboljšanju dveh smeri ničelnega strela našega večjezičnega modela. Zlasti smo opazili približno 9 točk BLEU v primerjavi z osnovnim večjezičnim modelom in do 2,08 BLEU v primerjavi z vrtilnim mehanizmom z uporabo dveh dvojezičnih modelov. Nadaljnja analiza kaže, da je prišlo do rahlega izboljšanja tudi v smereh jezika, ki niso ničelne.', 'ha': "Taurar aiki na fassarar masu mulki-littafin neural na mutane, na gaya fassarar aikin da suka yi fassara game da masu motsi biyu-bilin da mai baƙanci mai kyau, ko kuma da shiryoyin fassarori (sifo-shot) ba a gani a lokacin mafarako. Tuna zarraba wani fassarar-shirin sifiri a cikin wani misalin-mutane na'ura. Tuna goyya da wani aikin muhalli na masu sauƙi da ke samar da dubu na fassarori wanda aka samar da shi na'urar-na'ura na sifiri. Tsarin fassarar da aka samar da shi na'urar (sub-Optical don su ƙunsa da harshen mai gauraya daga maganar da aka raba shi), za'a yi amfani da shi tare da data na farko da za'a tsare shi kuma ya tsare jerin multilala. Ga bayan lokaci, wannan yana yarda in karanta na'ura daga wanda aka gina shi, kuma yana ƙara mafi kyau ga fitarwa. Tayiyinmu na nũna yana da mafiya amfani ga improve hanyõyin biyu na sifiri da misalin mulki. Haƙĩƙa, mun gane mafaniki na taki 9 BLEU points a kan misalin mulki-lingui da ke ƙaranci zuwa 2.08 BLEU a kanan mutane da ke amfani da misãlai biyu biyu biyu. Ana ƙari ya nuna cewa, there are also some ƙari mai improve cikin shiryoyin harshen waɗanda bã ya yin sifo.", 'he': 'העבודה האחרונה על התרגום של מכונות עצביות רבות שפויות דיווחה על ביצועים תחרותיים בנוגע לדוגמאות שתיים שפויות ובביצועים טובים באופן מפתיע אפילו בכיוונים התרגום (אפס-ירי) שלא נצפה בזמן האימון. אנחנו חוקרים כאן תרגום אפס-יריות במיוחד מסיבה מיוחדת של משאבים נמוכים. אנו מציעים תהליך אימון חד-פעמי שגורם לתרגומות כפולות שנוצרו ישירות על ידי המערכת לכיווני האפס. התרגשות שנוצרות על ידי המערכת (sub-optimal מכיוון שהן מכילות שפה מעורבת מהמלון המילים המשותף), משתמשות לאחר מכן יחד עם הנתונים המקומיים המקוריים כדי להאכיל ולאימון מחדש את הרשת הרבועה. במהלך הזמן, זה מאפשר למערכת ללמוד מהיציאה המיוצרת והטובה יותר ויותר. הגישה שלנו מראה להיות יעילה בשיפור של שני הכיוונים של אפס יריות של המודל הרב-שפתי שלנו. במיוחד, צפינו ברווחים של כ-9 נקודות BLEU על דוגמא רבולוגית בסיסית ועד 2.08 BLEU על מנגנון פיבוט בשימוש בשני דוגמאות שתיים. ניתוח נוסף מראה שיש גם שיפור קל בכיוונים של שפה ללא אפס.', 'jv': 'biasane karo multi-lenguang teljamahan, ajeng nggo ngerasahan kapan pawaran karo hal-ingkang model sing apik tur angel gak bukane saiki, dadi iso nggawe tarjamahan (nul-sho) tarjamahan sing ora bisa nggawe ngupakan lanjut. Awak dhéwé éntuk karo nggambar nul-atan kanggo nguasai multilanggar neng hasil Awak dhéwé ngerasai perusahaan anyar nggawe sistem sing arep nggawe gerakan diolah, dadi sing nyimpen kuwi tarjamahan kanggo nguasai nggawe barang 0. Terjamahan sing nggawe sistem Nanging ora, iki dipoleh sistem kanggo kelas nang sampeyan kelangan lan saka njaluk luwih apik. Awakdhéwé éntuk paneluké nggawe barang nggawe gerakan sing luwih-luwih basa gambar nggawe modèl sing luwih basa. Awak ngomong, kita ngulinggo perusahaan ning sampeyan 9 CLUE kuwi diangkat liyane multilenguangka sing dumadhi tanggal 2.2008 CLUE seneng pisan manungsa pirotyani iki model sing bisa lenguangka. Balita luwih akeh pisan mengko, mengko iso nggawe akeh njaluk luwih dumateng banget gak 0.', 'bo': 'འཕྲལ ང་ཚོས་འདིར་རྒྱ་ནག་གི་སྒྲིག་སྟངས་མང་ཆེ་བའི་སྒྲིག་འཛུགས་ཀྱི་ནང་དུ་བློ་གཏད་རྙེད་མི་་ཅིག་ལ་ཞིབ་དཔྱད ང་ཚོས་མ་ལག་གི་སྐོར་ཐག་གཤམ་ལ་བསྡད་པའི་ལྟ་བུའི་རིམ་པ་ཞིག་གིས་ The translations produced by the system (sub-optimal since they contain mixed language from the shared vocabulary), are then used together with the original parallel data to feed and iteratively re-train the multilingual network. Over time, this allows the system to learn from its own generated and increasing output. ང་ཚོའི་ཐབས་ལམ་དེ་ནུས་ཡོད་པས་ང་ཚོའི་སྐད་རིགས་དབྱིབས་ཀྱི་གཟུགས་རིས་གཉིས་ཀྱི་བཟོ་བཅོས་བྱེད་ཀྱི་ཡོད། ང་ཚོས་རང་ཉིད་ཀྱིས་སྐྱེས་ཆེན་གྱི་ཐབས་ལམ་ཞིག་ལས་མཐོང་བ་ཐག་ཆེ་བའི་གཟུགས་བརྙན་ཚུལ་ལྡན་༩། དབྱེ་ཞིབ་གཞན་ལས་ཀྱང་ནི་སྐད་རིགས་ཀྱི་གནས་སྟངས་ལ་ཉུང་བའི་འགྱུར་བ་ཞིག་ཡིན་པ་མིན་པར།'}
{'en': 'Evolution Strategy Based Automatic Tuning of Neural Machine Translation Systems', 'ar': 'إستراتيجية التطور المعتمدة على الضبط التلقائي لأنظمة الترجمة الآلية العصبية', 'fr': "Réglage automatique basé sur la stratégie d'évolution des systèmes de traduction automatique neuronaux", 'es': 'Ajuste automático basado en la estrategia de evolución de los sistemas de traducción automática neuronal', 'pt': 'Ajuste Automático Baseado em Estratégia de Evolução de Sistemas de Tradução Automática Neural', 'ja': '神経機械翻訳システムの進化戦略ベースの自動チューニング', 'zh': '盖进化策之神经机器翻译,统自调校也', 'hi': 'विकास रणनीति आधारित तंत्रिका मशीन अनुवाद प्रणालियों की स्वचालित ट्यूनिंग', 'ru': 'Стратегия Evolution основанная на автоматической настройке нейронных систем машинного перевода', 'ga': 'Tiúnadh Uathoibríoch ar Chórais Néar-Aistriúcháin Meaisín Bunaithe ar Straitéis Evolution', 'ka': 'EvolutionName', 'hu': 'Evolution Stratégia Alapú Neurális Gépi Fordító Rendszerek Automatikus Hangolása', 'el': 'Στρατηγική εξέλιξης με βάση τον αυτόματο συντονισμό των νευρωνικών συστημάτων μηχανικής μετάφρασης', 'it': 'Sintonizzazione automatica basata sulla strategia di evoluzione dei sistemi di traduzione automatica neurale', 'lt': 'Evolution Strategy Based Automatic Tuning of Neural Machine Translation Systems', 'mk': 'Evolution Strategy Based Automatic Tuning of Neural Machine Translation Systems', 'ms': 'Pengaturan Automatik Sistem Terjemahan Mesin Neural berdasarkan Strategi Evolution', 'kk': 'Evolution стратегиясы негіздеген нейрондық машинаны аудару жүйелерін автоматты түрде баптау', 'ml': 'Evolution Strategy Based Automatically Tuning of Neural Machine Translation Systems', 'mt': 'L-Aġġustament Awtomatiku tas-Sistemi tat-Traduzzjoni tal-Magni Newrali bbażat fuq l-Istrateġija tal-Evoluzzjoni', 'mn': 'Evolution Strategy Based Automatic Tuning of Neural Machine Translation Systems', 'no': 'Evolution-strategi basert automatisk tilpassing av neiralmaskinsomsetjingssystemer', 'pl': 'Oparte na strategii ewolucji Automatyczne dostrojenie neuronowych systemów tłumaczenia maszynowego', 'ro': 'Reglarea automată bazată pe strategie de evoluție a sistemelor de traducere automată neurală', 'sr': 'Evolucijska strategija osnovana na automatskom prilagodbi sistema neuroloških prevoda mašina', 'si': 'විකෘතිය පද්ධතිය ස්වයංක්\u200dරියාත්මක පද්ධතියේ ස්වයංක්\u200dරියාත්මක පද්ධතිය', 'so': 'Evolution Strategy Based Automatic Tuning of Neural machine Translation Systems', 'sv': 'Evolutionsstrategibaserad automatisk justering av neurala maskin√∂vers√§ttningssystem', 'ta': 'மொழிபெயர்ப்பு மொழிபெயர்ப்பு அமைப்பு', 'ur': 'ایوولیوشن استراتٹریسی بنیاد نائورل ماشین ترجمہ سیسٹم کی آٹوٹی ٹونگ', 'uz': 'Comment', 'vi': 'Phương pháp tiến hóa Kết hợp Hệ thống dịch máy móc thần kinh', 'bg': 'Автоматично настройване на неврални системи за машинен превод, базирано на стратегията на еволюцията', 'nl': 'Evolutiestrategie gebaseerde automatische afstemming van neurale machinevertaalsystemen', 'da': 'Evolution Strategibaseret Automatisk Tuning af Neurale Maskinoversættelsessystemer', 'hr': 'Evolution strategija temeljena na automatskom prilagodbi sustava neurološkog prevoda', 'ko': '진화 전략 기반의 신경 기계 번역 시스템 자동 조정', 'de': 'Evolutionsstrategiebasiertes automatisches Tuning von neuronalen maschinellen Übersetzungssystemen', 'tr': 'Evolutioniň Strateýgisi Nural Maşynyň Terjime Sistemleri', 'sw': 'Mikakati ya Evolution Kutokana na Uandishi wa Kifaransa wa Mifumo ya Tafsiri ya Mashine ya Kiasili', 'id': 'Strategi Evolution Berdasarkan Pengaturan Otomatis Sistem Translasi Mesin Neural', 'af': 'Evolution Strategie gebaseerde automatiese toepassing van neurale masjien vertaling stelsels', 'sq': 'Rregullimi automatik i sistemeve të përkthimit të makinave nervore bazuar në strategjinë e Evolution', 'fa': 'استراتژی Evolution Based Automatic Tuning of Neural Machine Translation Systems', 'am': 'አድራሻ', 'hy': 'Էվոլյուցիայի ռազմավարությունը հիմնված է նյարդային մեքենաների թարգմանման համակարգերի ավտոմատիկ հարմարեցմանը', 'az': 'Evolution Strateji Nöral Makina Çeviri Sistemlərinin Avtomatik Tuning', 'bn': 'Evolution স্বয়ংক্রিয়ভাবে ভিত্তিক কৌশল নিউরাল মেশিন অনুবাদ সিস্টেম', 'bs': 'Evolution strategija osnovana na automatskom prilagodbi sustava neuroloških prevoda', 'ca': "L'ajustament automàtic dels sistemes de traducció de màquines neuronals basat en l'estratègia d'evolució", 'cs': 'Automatické ladění neuronových strojových překladů založené na strategii evoluce', 'et': 'Evolutsiooni strateegiapõhine automaatne häälestamine neuroaalsetes masintõlkesüsteemides', 'fi': 'Evolution Strategy Based Automatic Tuning of Neural Machine Translation Systems', 'jv': 'evoluiton Stasi Batasai Manual Tuning Nyural Majin Terjamasi Sistem', 'he': 'Evolution Strategy Based Automatic Tuning of Neural Machine Translation Systems', 'sk': 'Avtomatsko nastavitev nevralnih strojnih prevajalskih sistemov na podlagi strategije evolucije', 'ha': 'KCharselect unicode block name', 'bo': 'ཨི་བོ་ལུ་ཤཱན་དཔལ་འབྱོར་གཞི་རྟེན་ནས་རང་འགུལ་གྱིས་སྒྲིག་འཛུགས་ཀྱི་འདྲ་མ་ལག་ལུགས་'}
{'en': 'Neural machine translation (NMT) systems have demonstrated promising results in recent years. However, non-trivial amounts of manual effort are required for tuning  network architectures ,  training configurations , and  pre-processing settings  such as byte pair encoding (BPE). In this study, we propose an evolution strategy based automatic tuning method for  NMT . In particular, we apply the covariance matrix adaptation-evolution strategy (CMA-ES), and investigate a Pareto-based multi-objective CMA-ES to optimize the translation performance and computational time jointly. Experimental results show that the proposed  method  automatically finds NMT systems that outperform the initial manual setting.', 'es': 'Los sistemas de traducción automática neuronal (NMT) han demostrado resultados prometedores en los últimos años. Sin embargo, se requieren cantidades no triviales de esfuerzo manual para ajustar las arquitecturas de red, las configuraciones de capacitación y los ajustes de preprocesamiento, como la codificación de pares de bytes (BPE). En este estudio, proponemos un método de ajuste automático basado en la estrategia de evolución para NMT. En particular, aplicamos la estrategia de adaptación-evolución de la matriz de covarianza (CMA-ES) e investigamos una CMA-ES multiobjetivo basada en Pareto para optimizar el rendimiento de la traducción y el tiempo computacional de forma conjunta. Los resultados experimentales muestran que el método propuesto encuentra automáticamente sistemas NMT que superan la configuración manual inicial.', 'ar': 'أظهرت أنظمة الترجمة الآلية العصبية (NMT) نتائج واعدة في السنوات الأخيرة. ومع ذلك ، هناك حاجة إلى كميات غير تافهة من الجهد اليدوي لضبط معماريات الشبكة ، وتكوينات التدريب ، وإعدادات المعالجة المسبقة مثل تشفير زوج البايت (BPE). في هذه الدراسة ، نقترح طريقة الضبط التلقائي القائمة على إستراتيجية التطور لـ NMT. على وجه الخصوص ، نحن نطبق استراتيجية تطور تكيف مصفوفة التغاير (CMA-ES) ، ونبحث في CMA-ES متعددة الأغراض على أساس باريتو لتحسين أداء الترجمة والوقت الحسابي بشكل مشترك. تظهر النتائج التجريبية أن الطريقة المقترحة تعثر تلقائيًا على أنظمة NMT التي تتفوق على الإعداد اليدوي الأولي.', 'fr': "Les systèmes de traduction automatique neuronale (NMT) ont donné des résultats prometteurs ces dernières années. Cependant, des efforts manuels non négligeables sont nécessaires pour régler les architectures réseau, les configurations d'apprentissage et les paramètres de prétraitement tels que le codage de paires d'octets (BPE). Dans cette étude, nous proposons une méthode de réglage automatique basée sur une stratégie d'évolution pour la NMT. En particulier, nous appliquons la stratégie d'adaptation-évolution de la matrice de covariance (CMA-ES) et étudions une CMA-ES multi-objectifs basée sur Pareto afin d'optimiser conjointement les performances de traduction et le temps de calcul. Les résultats expérimentaux montrent que la méthode proposée détecte automatiquement les systèmes NMT qui surpassent le réglage manuel initial.", 'pt': 'Os sistemas de tradução automática neural (NMT) têm demonstrado resultados promissores nos últimos anos. No entanto, quantidades não triviais de esforço manual são necessárias para ajustar arquiteturas de rede, configurações de treinamento e configurações de pré-processamento, como codificação de par de bytes (BPE). Neste estudo, propomos um método de ajuste automático baseado em estratégia de evolução para NMT. Em particular, aplicamos a estratégia de adaptação-evolução da matriz de covariância (CMA-ES) e investigamos um CMA-ES multi-objetivo baseado em Pareto para otimizar o desempenho da tradução e o tempo computacional em conjunto. Resultados experimentais mostram que o método proposto encontra automaticamente sistemas NMT que superam a configuração manual inicial.', 'zh': '近年以来,神经机器翻译(NMT)统已见有望矣。 然调网络体系结构训练预处理置(如字节编码 (BPE))大手动。 本论进化策者NMT自调谐之术。 宜协方差矩阵应进化策(CMA-ES),究其帕累托之多CMA-ES,以共优化译性与计日月。 实验结果表明,所提法得自优于初手动所置NMT统。', 'ja': '神経機械翻訳（ NMT ）システムは、近年有望な結果を示しています。しかしながら、ネットワークアーキテクチャのチューニング、トレーニング構成、およびバイトペアエンコーディング（ BPE ）などの前処理設定には、わずかな手作業量が必要である。本研究では， NMTの自動チューニング手法に基づく進化戦略を提案する．特に、共分散行列適応進化戦略（ CMA - ES ）を適用し、パレートベースの多目的CMA - ESを調査して、翻訳パフォーマンスと計算時間を共同で最適化します。実験結果は、提案された方法が、初期の手動設定を上回る性能のNMTシステムを自動的に見つけることを示しています。', 'hi': 'तंत्रिका मशीन अनुवाद (एनएमटी) प्रणालियों ने हाल के वर्षों में आशाजनक परिणामों का प्रदर्शन किया है। हालांकि, नेटवर्क आर्किटेक्चर, प्रशिक्षण कॉन्फ़िगरेशन और बाइट जोड़ी एन्कोडिंग (बीपीई) जैसी पूर्व-प्रसंस्करण सेटिंग्स ट्यूनिंग के लिए मैन्युअल प्रयास की गैर-तुच्छ मात्रा की आवश्यकता होती है। इस अध्ययन में, हम एनएमटी के लिए एक विकास रणनीति आधारित स्वचालित ट्यूनिंग विधि का प्रस्ताव करते हैं। विशेष रूप से, हम सहप्रसरण मैट्रिक्स अनुकूलन-विकास रणनीति (सीएमए-ईएस) को लागू करते हैं, और संयुक्त रूप से अनुवाद प्रदर्शन और कम्प्यूटेशनल समय को अनुकूलित करने के लिए एक परेटो-आधारित बहु-उद्देश्य सीएमए-ईएस की जांच करते हैं। प्रयोगात्मक परिणाम बताते हैं कि प्रस्तावित विधि स्वचालित रूप से NMT सिस्टम ढूँढती है जो प्रारंभिक मैन्युअल सेटिंग को मात देती है।', 'ru': 'Системы нейронного машинного перевода (НМП) в последние годы продемонстрировали многообещающие результаты. Однако для настройки сетевых архитектур, обучающих конфигураций и настроек предварительной обработки, таких как кодирование пары байтов (BPE), требуются нетривиальные объемы ручного усилия. В этом исследовании мы предлагаем метод автоматической настройки на основе стратегии эволюции для НБП. В частности, мы применяем стратегию адаптации-матрицы ковариации (CMA-ES) и исследуем многоцелевой CMA-ES на основе Парето для оптимизации производительности перевода и вычислительного времени совместно. Экспериментальные результаты показывают, что предлагаемый метод автоматически находит системы НБ, которые превосходят первоначальную ручную настройку.', 'ga': "Tá torthaí dóchais léirithe ag córais néaraistriúcháin meaisín (NMT) le blianta beaga anuas. Mar sin féin, tá gá le méideanna neamhfhánacha d'iarracht láimhe chun ailtireachtaí líonra a oiriúnú, cumraíochtaí oiliúna, agus socruithe réamhphróiseála amhail ionchódú beart beachta (BPE). Sa staidéar seo, molaimid modh tiúnta uathoibríoch bunaithe ar straitéis éabhlóide do NMT. Go háirithe, cuirimid an straitéis oiriúnaithe-éabhlóide maitrís chomhathraitheas (CMA-ES) i bhfeidhm, agus déanaimid imscrúdú ar CMA-ES ilchuspóireach atá bunaithe ar Pareto chun an fheidhmíocht aistriúcháin agus an t-am ríomhaireachtúil a bharrfheabhsú i gcomhpháirt. Léiríonn torthaí turgnamhacha go n-aimsíonn an modh atá beartaithe go huathoibríoch córais NMT a sháraíonn an socrú láimhe tosaigh.", 'el': 'Τα συστήματα νευρωνικής μηχανικής μετάφρασης (NMT) έχουν δείξει πολλά υποσχόμενα αποτελέσματα τα τελευταία χρόνια. Ωστόσο, απαιτούνται ελάχιστες ποσότητες χειρωνακτικής προσπάθειας για τον συντονισμό των αρχιτεκτονικών δικτύων, των διαμορφώσεων κατάρτισης και των ρυθμίσεων προεπεξεργασίας, όπως η κωδικοποίηση ζεύγους Byte (BPE). Στην παρούσα μελέτη, προτείνουμε μια μέθοδο αυτόματης προσαρμογής που βασίζεται σε στρατηγική εξέλιξης για την NMT. Ειδικότερα, εφαρμόζουμε τη στρατηγική προσαρμογής-εξέλιξης της μήτρας συμπαραγωγής (και ερευνούμε ένα βασισμένο στο Πάρετο πολυ-αντικειμενικό για τη βελτιστοποίηση της απόδοσης μετάφρασης και του υπολογιστικού χρόνου από κοινού. Τα πειραματικά αποτελέσματα δείχνουν ότι η προτεινόμενη μέθοδος βρίσκει αυτόματα συστήματα που ξεπερνούν την αρχική χειροκίνητη ρύθμιση.', 'hu': 'A neurális gépi fordító (NMT) rendszerek ígéretes eredményeket mutattak az elmúlt években. A hálózati architektúrák, a képzési konfigurációk és az előfeldolgozási beállítások, mint például a bájtpár kódolás (BPE) hangolásához azonban nem triviális mennyiségű manuális erőfeszítésre van szükség. Ebben a tanulmányban egy evolúciós stratégia alapú automatikus hangolási módszert javasolunk NMT esetében. Különösen a kovariancia mátrix adaptációs-evolúciós stratégiát (CMA-ES) alkalmazzuk, és egy Pareto alapú multi-objektív CMA-ES vizsgálatát vizsgáljuk a fordítási teljesítmény és a számítási idő együttes optimalizálása érdekében. A kísérleti eredmények azt mutatják, hogy a javasolt módszer automatikusan megtalálja a kezdeti manuális beállítást felülmúló NMT rendszereket.', 'ka': 'ნეიროლური მაქინის გადაწყვეტილება (NMT) სისტემები ახალ წლის შემდეგ მომხსენებელი წარმოდგენა. მაგრამ საქაღალური აქტიქტურების შესახებ, კონფიგურაციების შესახებ და წინაპროცესის პარამეტრები, როგორც ბაიტების კონფიგურაციას (BPE) შესაძლებელია. ამ კვლევაში, ჩვენ განვითარებით განვითარების სტრატიგია, რომელსაც NMT-ს ავტომატური კონფიგურაციის მეტი. განსაკუთრებულია, ჩვენ კოგარიანსის მარტიკის აეტაცია-ევოლუციის სტრატიგია (CMA-ES) დავყენებთ და პერეტში მრავალექტიური მრავალექტიური CMA-ES-ის განსაკუთრება დავუყენებთ, რომ გადაწყ ექსპერიმენტიური წარმოდგენები ჩვენებს, რომ პირველი პროგენტის შესაძლებლობა ავტომატურად NMT სისტემები მოიძებნა, რომელიც გავაკეთებს პირვე', 'it': "I sistemi di traduzione automatica neurale (NMT) hanno dimostrato risultati promettenti negli ultimi anni. Tuttavia, sono necessarie quantità non banali di sforzo manuale per sintonizzare architetture di rete, configurazioni di formazione e impostazioni di pre-elaborazione come la codifica della coppia di byte (BPE). In questo studio, proponiamo un metodo di tuning automatico basato sulla strategia di evoluzione per NMT. In particolare, applichiamo la strategia di adattamento-evoluzione della matrice di covarianza (CMA-ES), ed esaminiamo una CMA-ES multi-obiettivo basata su Pareto per ottimizzare congiuntamente le prestazioni di traduzione e il tempo computazionale. I risultati sperimentali mostrano che il metodo proposto trova automaticamente i sistemi NMT che superano l'impostazione manuale iniziale.", 'lt': 'Pastaraisiais metais neurologinių mašin ų vertimo (NMT) sistemos parodė perspektyvius rezultatus. Vis dėlto tinklo architektūroms, mokymo konfigūracijoms ir išankstiniam apdorojimui, pvz., byte pair kodavimui (BPE), pritaikyti reikalingos nedidelės rankinės pastangos. Šiame tyrime siūlome NMT automatinio koregavimo metodą, pagrįstą evoliucijos strategija. Visų pirma mes taikome kovarinės matricos prisitaikymo prie evoliucijos strategiją (CMA-ES) ir tiriame Pareto pagrindu grindžiamą daugiametę CMA-ES, kad kartu būtų optimizuotas vertimo rezultatas ir skaičiavimo laikas. Eksperimentiniai rezultatai rodo, kad siūlomu metodu automatiškai nustatomos NMT sistemos, viršijančios pradinį rankinį nustatymą.', 'kk': 'Нейрондық компьютердің аударуы (NMT) жүйелері соңғы жылдар бойынша үлкен нәтижелерді көрсетті. Бірақ желі архитектураларды, баптауларды баптау және алдын- процессердің параметрлері (BPE) секілді байт кодтамасын баптау үшін үлкен қолмен жұмыс істеу керек. Бұл зерттеулерде NMT үшін автоматты түрде баптау әдісін негіздеген эволюциялық стратегиясын ұсынамыз. Мысалы, біз конвариант матрицасының адаптациялау-эволюциялау стратегиясын (CMA-ES) қолданатын және парето негіздеген көп мақсатты CMA-ES аудармаларды және компьютерлік уақытын біріктіру үшін зерттеуді. Эксперименталдық нәтижелері келтірілген тәсілі автоматты түрде NMT жүйелерін бастапқы қолмен орнату жүйелерін таба алады.', 'ms': 'Neural machine translation (NMT) systems have demonstrated promising results in recent years.  Namun, jumlah usaha manual yang tidak kecil diperlukan untuk menyesuaikan arkitektur rangkaian, konfigurasi latihan, dan tetapan pra-proses seperti pengekodan pasangan bait (BPE). Dalam kajian ini, kami cadangkan strategi evolusi berdasarkan kaedah penyesuaian automatik untuk NMT. Secara khususnya, kami melaksanakan strategi penyesuaian-evolusi matriks kovarians (CMA-ES), dan menyelidiki CMA-ES berbilang-objektif berdasarkan Pareto untuk optimumkan prestasi terjemahan dan masa pengiraan bersama-sama. Hasil percubaan menunjukkan bahawa kaedah yang diusulkan secara automatik mencari sistem NMT yang melebihi tetapan manual awal.', 'ml': 'അടുത്ത വര്\u200dഷങ്ങളില്\u200d ന്യൂറല്\u200d മെഷീന്\u200d പരിഭാഷ (NMT) സിസ്റ്റമുകള്\u200d വാഗ്ദാനം ചെയ്യുന്ന ഫലങ്ങള്\u200d പ്രത്യക്ഷിച എങ്കിലും ബൈറ്റ് ജോടി കോണ്\u200dകോഡിങ് പോലുള്ള ക്രമീകരണങ്ങള്\u200d, പരിശീലന ക്രമീകരണങ്ങള്\u200d, പ്രവര്\u200dത്തിപ്പിക്കുന്ന സജ്ജീകരണങ്ങള്\u200dക്ക് കൈയ്യൂട്ടുകാ ഈ പഠനത്തില്\u200d നമ്മള്\u200d നിങ്ങള്\u200dക്ക് നിര്\u200dദ്ദേശിക്കുന്ന ഒരു വികസ്ഥാനത്തിന്റെ സ്വയമായി ട്യൂണിങ്ങ് രീതി പ്രത്യേകിച്ച്, നമ്മള്\u200d വേറെന്\u200dസ് മാറ്റ്രിക്സ് അഡാപ്റ്റേഷന്\u200d വികസിപ്പിനുള്ള strategy (CMA-ES) പ്രയോഗിക്കുകയും, പാര്\u200dട്ടോവിന്\u200dറെ അടിസ്ഥാനത്തുള്ള പല-ലക്ഷ് പരീക്ഷിക്കുന്ന ഫലങ്ങള്\u200d കാണിക്കുന്നത് പ്രൊദ്ദേശിച്ച രീതിയില്\u200d സ്വയം NMT സിസ്റ്റം കണ്ടെത്തുന്നു', 'mt': 'Is-sistemi tat-traduzzjoni tal-magni newrali (NMT) urew riżultati promettenti f’dawn l-aħħar snin. Madankollu, huma meħtieġa ammonti mhux trivjali ta’ sforz manwali għall-aġġustament tal-arkitetturi tan-netwerk, il-konfigurazzjonijiet tat-taħriġ, u l-konfigurazzjonijiet ta’ qabel l-ipproċessar bħall-kodifikazzjoni tal-par ta’ bytes (BPE). F’dan l-istudju, qed nipproponu metodu ta’ aġġustament awtomatiku bbażat fuq strateġija ta’ evoluzzjoni għall-NMT. B’mod partikolari, napplikaw l-istrateġija ta’ adattament-evoluzzjoni tal-matriċi ta’ kovarjanza (CMA-ES), u ninvestigaw CMA-ES b’objettivi multipli bbażati fuq Pareto sabiex l-a ħjar prestazzjoni tat-traduzzjoni u ż-żmien komputattiv jiġu ottimizzati flimkien. Ir-riżultati sperimentali juru li l-metodu propost isib awtomatikament sistemi NMT li jaqbżu s-sett manwali inizjali.', 'mn': 'Сүүлийн жилүүдэд мэдрэлийн машин орчуулалт (NMT) системүүд амлалтай үр дүнг харуулсан. Гэвч сүлжээний архитектурууд, сургалтын загварууд болон байт хоёр кодлох (BPE) шиг аль хэмжээний үйлдвэрлэлийн хэмжээний шаардлагатай. Энэ судалгаанд бид хөгжлийн стратегийг NMT-ийн автоматжуулах аргыг суурилуулдаг. Ялангуяа бид ковариантын матриц адилтгал хөгжлийн стратегийг (CMA-ES) ашиглаж, Парето суурилсан олон зорилготой CMA-ES-г хамтдаа орчуулахын тулд хөгжлийн үйл ажиллагааг, тооцоолох цаг хамтдаа сайжруулахын Эмчилгээний үр дүнд санал өгсөн арга нь автоматаар NMT системийг автоматаар олж чадна.', 'no': '@ info Men ikkje-trivial mengdar manuelle innstillingar er nødvendig for å setja opp nettverksarkitekturar, opplæringsoppsett og forhandteringsinnstillingar som byte-par-koding (BPE). I denne studien foreslår vi eit utviklingsstrategi basert på automatisk oppsettmetode for NMT. I særskilt bruker vi kovariansmatrisen for adaptasjon-evolueringsstrategien (CMA-ES) og undersøker ein multimålrett CMA-ES på Pareto-basert for å optimisera utviklinga av omsetjinga og datamaskina kopla. Eksperimentale resultat viser at den foreslåde metoden finn automatisk NMT- systemet som utfører oppstartsinnstillingane for manuelt.', 'pl': 'Systemy neuronowego tłumaczenia maszynowego (NMT) wykazały w ostatnich latach obiecujące rezultaty. Jednak do dostrojenia architektur sieciowych, konfiguracji szkoleniowych i ustawień wstępnego przetwarzania, takich jak kodowanie pary bajtów (BPE) wymagane są nietrywialne wysiłki ręcznego. W niniejszym opracowaniu proponujemy metodę automatycznego dostrajania NMT opartą na strategii ewolucji. W szczególności stosujemy strategię adaptacji-ewolucji macierzy kowariancyjnej (CMA-ES) i badamy opartą na Pareto wielobiektywną CMA-ES w celu wspólnej optymalizacji wydajności tłumaczenia i czasu obliczeniowego. Wyniki eksperymentalne pokazują, że proponowana metoda automatycznie znajduje systemy NMT, które przewyższają początkowe ustawienie ręczne.', 'ro': 'Sistemele de traducere automată neurală (NMT) au demonstrat rezultate promiţătoare în ultimii ani. Cu toate acestea, sunt necesare cantități non-triviale de efort manual pentru reglarea arhitecturilor de rețea, a configurațiilor de instruire și a setărilor de pre-procesare, cum ar fi codificarea perechilor de octeți (BPE). În acest studiu, propunem o metodă de reglare automată bazată pe strategie de evoluție pentru NMT. În special, aplicăm strategia de adaptare-evoluție a matricei covariante (CMA-ES) și investigăm un CMA-ES multi-obiectiv bazat pe Pareto pentru a optimiza performanța traducerii și timpul de calcul împreună. Rezultatele experimentale arată că metoda propusă găsește automat sistemele NMT care depășesc setarea manuală inițială.', 'mk': 'Системите за превод на неврални машини (НМТ) покажаа ветувачки резултати во последните години. Сепак, не се потребни тривијални количини рачни напори за прилагодување на мрежните архитектури, конфигурации за обука и поставувања за преобработување како што е кодирање на пар бајти (BPE). Во оваа студија, предложуваме еволуциска стратегија базиран на метод на автоматско налаштување за НМТ. Особено, ја применуваме стратегијата за адаптација-еволуција на коваријанската матрица (CMA-ES), и истражуваме мултиобјективна CMA-ES базирана на Pareto за оптимизација на преводот и компјутационалното време заедно. Експерименталните резултати покажуваат дека предложениот метод автоматски ги пронајде НМТ системите кои го надминуваат првичното рачно поставување.', 'so': 'Isticmaalka neural machine turjumista (NMT) waxay muujiyaan resulto ballan ah ee ugu dambeeyey. Si kastaba ha ahaatee waxaa loo baahan yahay in aad u sameysato dhismaha shabakadda, dhismaha waxbarashada, iyo hagitaanka ka hor-baaraandegista, tusaale ahaan bandhig labaad (BPE). Waxbarashadan waxaynu horumarinaynaa qoraal bedelka ah oo ku saleysan qaab iskumar ah oo u gaaraya NMT. Si gaar ah, waxaynu u codsanaynaa qoraalka bedelashada matrix-bedelka (CMA-ES), waxaana baaritaannaa qoraalka ku qoran baaritaanka faro badan ee CMA-ES oo Pareto ku qoran, si aan ugu horumarinno muuqashada turjumaadda iyo waqtiga xisaabta. Imtixaanka waxaa muuqda in qaabka lagu soo jeeday uu si automatic ah u helo nidaamka NMT oo sameynaya qoraalka qofka hore.', 'sr': 'Neuralni sistemi prevoda mašine (NMT) pokazali su obećavajuće rezultate u poslednjih godina. Međutim, ne-trivialne količine rukovnih napora su potrebne za tuniranje mrežnih arhitektura, konfiguracija obuke i nastave pre obrade poput kodiranja bajtova par (BPE). U ovom istraživanju predlažemo metodu automatskog prilagodbe za NMT-a na temelju evolucijske strategije. Posebno, mi primjenjujemo strategiju za adaptaciju i evoluciju kovarijancije matrice (CMA-ES) i istražujemo multiobjektivnu CMA-ES na paritu kako bi zajedno optimizirali provedbu prevođenja i računalno vrijeme. Eksperimentalni rezultati pokazuju da predloženi metod automatski pronađe NMT sisteme koji iznose početnu manualnu postavku.', 'si': 'අන්තිම අවුරුද්දේ ප්\u200dරතිචාර පද්ධතියක් පෙන්වන්න තියෙනවා. නමුත්, බායිට් ජෝවිත සංවිධානය, ප්\u200dරීක්ෂණා සැකසුම් සහ ප්\u200dරීක්ෂණා සැකසුම් වගේ බායිට් ජෝවිත සංවිධානය (BPE) ටියු මේ පරීක්ෂණයේදී, අපි NMT වෙනුවෙන් ස්වයංක්\u200dරිය සංවිධානයක් අධාරණය කරනවා. විශේෂයෙන්, අපි කෝවාරියාන්ස් මැට්\u200dරික්ස් සැකසුම්-විකෘතිය සංවිධානය (CMA-ES යි), සහ පරීටෝ අධ්\u200dයාත්මක විකෘතිය CMA-ESය පරීක් පරීක්ෂණාත්මක ප්\u200dරතිචාර ප්\u200dරතිචාර පද්ධතිය ස්වයංක්\u200dරමයෙන් NMT පද්ධතිය හොයාගන්න පුළුවන', 'ur': 'نیورال ماشین ترجمہ (NMT) سیستموں نے اگلے سالوں میں وعدہ کا نتیجہ دکھایا ہے۔ However, non-trivial quantities of manual effort are required for tuning network architectures, training configurations, and pre-processing settings such as byte pair encoding (BPE). اس تحقیقات میں، ہم نے NMT کے لئے اٹولیوشن استراتژی کی بنیاد رکھی ہے۔ مخصوصا، ہم کاوارینس ماٹریکس ادامه-تکامل استراتژی (CMA-ES) کو لازم کرتے ہیں، اور ایک پارتو-بنیاد متعدی موجود CMA-ES کی تحقیق کرتے ہیں تاکہ ترجمہ کرنا اور کمپیوٹریشن زمان کے ساتھ اچھی طرح کرے۔ Experimental results show that the proposed method automatically finds NMT systems that perform the initial manual setting outperformed.', 'ta': 'அண்மைய ஆண்டுகளில் நியூரால் இயந்திர மொழிபெயர்ப்பு (NMT) அமைப்புகள் வாக்களிக்கும் முடிவுகளை காட்டிய ஆனால், பைட் ஜோட் குறியீட்டு போன்ற பைட் அமைப்புகள், பயிற்சி அமைப்புகள் மற்றும் முன் செயல்படுத்தல் அமைப்பு இந்த ஆராய்ச்சியில், நாம் ஒரு முன்னேற்றம் திட்டத்திற்கான தானியங்கி முன்னேற்றம் முறைமையை நினைவூட் குறிப்பிட்டு, மொழிபெயர்ப்பு செயல்பாடு மற்றும் கணக்கீட்டு நேரத்தை ஒன்றாக மாற்றுவதற்கான மாறுபாட்டு பார்டோ அடிப்படையிலுள்ள பல- பொருள் CMA-ES செயல்பாட் முயற்சி முடிவுகள் தெரியும் முறைமையை தானாகவே NMT அமைப்புகளை கண்டுபிடிக்கும் என்பதை காட்டுகிறது.', 'sv': 'Neural machine translation (NMT) system har visat lovande resultat under de senaste åren. Icke-triviala mängder manuell ansträngning krävs dock för att justera nätverksarkitekturer, utbildningskonfigurationer och förbehandlingsinställningar såsom byte pair encoding (BPE). I denna studie föreslår vi en utvecklingsstrategibaserad automatisk justeringsmetod för NMT. I synnerhet tillämpar vi kovariansmatris adaptation-evolution strategy (CMA-ES) och undersöker en Pareto-baserad multi-objektiv CMA-ES för att optimera översättningsprestandan och beräkningstiden gemensamt. Experimentella resultat visar att den föreslagna metoden automatiskt hittar NMT-system som överträffar den ursprungliga manuella inställningen.', 'uz': "Name Lekin, tarmoq maktablarini, taʼminlovchi moslamalarni moslash uchun qoʻlbola qilmagan qismi kerak, balki byte kodlash (BPE) kabi oldingi boshqarish moslamalarini moslash uchun. Bu o'rganida, biz NMT uchun avtomatik o'zgartirish usuli asosida evolyutsiya strategini tahlil qilamiz. Shunday qilib, biz murakkab matrix adaptation-evolution strategiga (CMA-ES) qoʻllayapmiz, va Pareto asosida ko'plab-obʼekt CMA-ES bilan bir necha obʼektlarni qidirish uchun tarjima drayverini va kompyuterning vaqtini birlashtirish uchun ko'ra olamiz. Name", 'vi': 'Hệ thống dịch chuyển máy thần kinh (NMB) đã có kết quả đầy hứa hẹn trong những năm gần đây. Tuy nhiên, chỉ cần một lượng công sức bằng tay không tầm thường để chỉnh cấu trúc mạng lưới, cấu hình huấn luyện và thiết lập xử lý trước như gồm gồm gồm gồm gồm gồm gồm gồm gồm gồm hai byte pair. Trong nghiên cứu này, chúng tôi đề nghị phương pháp nghiên cứu kế hoạch tiến hóa dựa trên NMT. Đặc biệt, chúng tôi áp dụng chiến lược thích ứng ma trận covariance (CMA-Es) và điều tra một CMA-Es đa mục tiêu của Mẹ để tối ưu hóa thành quả dịch và thời gian tính trên cùng. Kết quả thí nghiệm cho thấy rằng phương pháp được đề nghị tự động tìm thấy hệ thống NMT chạy ngoài thiết lập tay ban đầu.', 'bg': 'Системите за невронен машинен превод (НМТ) демонстрират обещаващи резултати през последните години. Въпреки това, не-тривиални количества ръчни усилия са необходими за настройване на мрежови архитектури, конфигурации за обучение и настройки за предварителна обработка, като байт двойка кодиране (БПЕ). В настоящото проучване предлагаме метод за автоматично настройване на НМТ базиран на еволюционна стратегия. По-специално, ние прилагаме стратегията за адаптация-еволюция на ковариационната матрица (CMA-ES) и изследваме базирана на Парето многообективна CMA-ES, за да оптимизираме съвместно ефективността на превода и изчислителното време. Експерименталните резултати показват, че предложеният метод автоматично открива системи, които превъзхождат първоначалната ръчна настройка.', 'nl': 'Neuronale machine translation (NMT) systemen hebben de afgelopen jaren veelbelovende resultaten laten zien. Er is echter niet-triviale hoeveelheid handmatige inspanning vereist voor het afstemmen van netwerkarchitecturen, trainingsconfiguraties en pre-processing-instellingen, zoals byte pair encoding (BPE). In deze studie stellen we een evolutiestrategiegebaseerde automatische tuning methode voor NMT voor. In het bijzonder passen we de covariantiematrix adaptatie-evolutie strategie (CMA-ES) toe en onderzoeken we een Pareto-gebaseerde multi-objectieve CMA-ES om samen de vertaalprestaties en rekentijd te optimaliseren. Experimentele resultaten tonen aan dat de voorgestelde methode automatisch NMT-systemen vindt die de initiële handmatige instelling overtreffen.', 'hr': 'U posljednjih godina su sustavi neurološkog prevoda (NMT) pokazali obećavajuće rezultate. Međutim, ne-trivialne količine ručnih napora su potrebne za prilagodbu arhitektura mreže, konfiguracije obuke i nastave predobrazovanja poput kodiranja bajtskih parova (BPE). U ovom ispitivanju predlažemo metodu automatske prilagodbe za NMT-u temeljnu strategiju evolucije. Posebno, primjenjujemo strategiju za adaptaciju i evoluciju kovarijancije matrice (CMA-ES) i istražujemo multiobjektivnu CMA-ES na paritu kako bi zajedno optimizirali učinkovitost prevođenja i računalno vrijeme. Eksperimentalni rezultati pokazuju da predložena metoda automatski pronalazi NMT sustave koji iznose početnu rukovnu postavku.', 'da': 'Neurale maskinoversættelsessystemer (NMT) har i de seneste år vist lovende resultater. Ikke-trivielle mængder manuel indsats er imidlertid påkrævet for at justere netværksarkitekturer, træningskonfigurationer og forhåndsbehandlingsindstillinger såsom byte pair kodning (BPE). I denne undersøgelse foreslår vi en evolutionsstrategi baseret automatisk tuning metode for NMT. Især anvender vi covarians matrix adaptation-evolution strategy (CMA-ES), og undersøger en Pareto-baseret multi-objektiv CMA-ES for at optimere oversættelses ydeevne og beregningstid i fællesskab. Eksperimentelle resultater viser, at den foreslåede metode automatisk finder NMT-systemer, der overgår den oprindelige manuelle indstilling.', 'id': 'Sistem terjemahan mesin saraf (NMT) telah menunjukkan hasil yang berjanji dalam tahun-tahun terakhir. Namun, jumlah usaha manual yang tidak trivial diperlukan untuk menyesuaikan arsitektur jaringan, konfigurasi pelatihan, dan pengaturan pra-proses seperti pengekodan pasang byte (BPE). Dalam studi ini, kami mengusulkan strategi evolusi berdasarkan metode pengaturan otomatis untuk NMT. Terutama, kami menerapkan strategi adaptasi-evolusi matriks kovarians (CMA-ES), dan menyelidiki CMA-ES berbasis Pareto untuk optimisasi prestasi terjemahan dan waktu komputasi bersama-sama. Hasil eksperimen menunjukkan bahwa metode yang diusulkan secara otomatis menemukan sistem NMT yang melebihi pengaturan manual awal.', 'ko': '최근 몇 년 동안 신경기계번역(NMT) 시스템은 이미 고무적인 결과를 나타냈다.그러나 네트워크 구조, 교육 설정과 사전 처리 설정(예를 들어 바이트 인코딩(BPE)을 조정하는 데는 많은 수동 작업이 필요하다.본 연구에서 진화 전략을 바탕으로 하는 NMT 자동 조정 방법을 제시했다.특히 우리는 협방차 매트릭스 자체 적응 진화 전략(CMA-ES)을 응용하여 파르토리오 기반의 다목적 CMA-ES를 연구하여 번역 성능과 계산 시간을 공동으로 최적화시켰다.실험 결과 이 방법은 처음 수동으로 설정한 NMT 시스템보다 성능이 우수한 것을 자동으로 찾을 수 있음을 나타냈다.', 'fa': 'سیستم\u200cهای ترجمه ماشین عصبی (NMT) در سال اخیر نتایج قول\u200cدهنده را نشان داده\u200cاند. با این حال، مقدار تلاش دستی غیر قابل توجه برای تنظیم معماری شبکه، پیکربندی آموزش و تنظیمات پیش\u200cپردازی مثل رمزبندی جفت بایت (BPE) نیاز دارد. در این مطالعه، ما یک استراتژی تکامل بر اساس روش تنظیم اتوماتیک برای NMT پیشنهاد می کنیم. مخصوصا، ما استراتژی تغییرات و تکامل تغییرات ماتریکس کاواریانس (CMA-ES) را استفاده می\u200cکنیم و تحقیق می\u200cکنیم یک CMA-ES متعدد هدف بسیار پارتو تا عملکرد ترجمه و زمان کامپیوتری را با هم بهترین کند. نتیجه\u200cهای تجربه نشان می\u200cدهد که روش پیشنهاد به طور خودکار سیستم\u200cهای NMT را پیدا می\u200cکند که از تنظیمات دستی اولیه بیشتر از آن انجام می\u200cدهند.', 'de': 'Neuronale Machine Translation (NMT)-Systeme haben in den letzten Jahren vielversprechende Ergebnisse gezeigt. Für die Abstimmung von Netzwerkarchitekturen, Trainingskonfigurationen und Vorverarbeitungseinstellungen wie Byte Pair Encoding (BPE) ist jedoch ein nicht trivialer manueller Aufwand erforderlich. In dieser Studie schlagen wir eine evolutionsstrategiebasierte automatische Tuning-Methode für NMT vor. Insbesondere wenden wir die Kovarianzmatrix Adaptation-Evolution Strategie (CMA-ES) an und untersuchen ein Pareto-basiertes Multiobjektiv CMA-ES, um die Übersetzungsleistung und Rechenzeit gemeinsam zu optimieren. Experimentelle Ergebnisse zeigen, dass die vorgeschlagene Methode automatisch NMT-Systeme findet, die die anfängliche manuelle Einstellung übertreffen.', 'sw': 'Mifumo ya kutafsiri mashine ya kijamii (NMT) imeonyesha matokeo yanayoahidi katika miaka ya hivi karibuni. Hata hivyo, jitihada zisizo za vibaya zinahitajika kwa ajili ya kuunganisha majengo ya mitandao, mifumo ya mafunzo, na mazingira ya kabla ya upasuaji kama vile encoding mbili (BPE). Katika utafiti huu, tunapendekeza mkakati wa mabadiliko unaotengeneza mbinu za kujifunza kwa ajili ya NMT. Kwa hakika, tunatumia mkakati wa kubadilisha matrix wa mabadiliko (CMA-ES), na uchunguzi wa CMA-ES yenye malengo mengi ya Pareto ili kuboresha ufanisi wa tafsiri na muda wa hesabu pamoja. Matokeo ya majaribio yanaonyesha kuwa mbinu zilizopendekezwa inaweza kupata mifumo ya NMT yenye mifumo ya mwanzo ya mikono.', 'af': "Nurale masjien vertaling (NMT) stelsels het beloftende resultate in onlangse jaar bevestig. Maar, nie- triviale hoeveelheid handversoek is benodig vir die tuning van netwerk arkitektuure, onderwerp konfigurasies en voorafverwerking instellings soos byte paar kodering (BPE). In hierdie studie voorstel ons 'n evolusie strategie gebaseerde automatiese tuning metode vir NMT. In spesifieke is ons die koarians matriks adaptasie-evolusie-strategie (CMA-ES) aanwend en 'n Pareto-gebaseerde multi-objekte CMA-ES ondersoek om die vertaling-prestasie en rekenasietyd saamgevind te optimaliseer. Eksperimentale resultate wys dat die voorgestelde metode automatysk NMT stelsels vind wat die aanvanklike hand instelling uitvoer.", 'tr': 'NMT sistemalary soňky ýyllarda söz berýän netijeleri görkezildi. Ýöne, syýal arhitektarlaryny täzelemek üçin kiçi kynçylyk ýok çabalary gerek. Bu okuwda, biz NMT üçin awtomatik taýýarlama strategiýasynda teklip edip otyrýarys. Özellikle, biz covarians matriks adaptasyon-evolusiyon stratejisini (CMA-ES) uygulapdyk we Pareto tabanly bir multi-maksady CMA-ES terjime etkinleşigini we hesaplamak wagtyny bir aralygy bejermek üçin bardyk. Aramanyň netijeleri görkezilýän täze hili özüne NMT sistemalary başlangyç eliň düzümlerini üstün tutan ýagdaýyny otomatik tapýar.', 'sq': 'Sistemet e përkthimit neuronal të makinave (NMT) kanë demonstruar rezultate premtuese në vitet e fundit. Megjithatë, sasitë jo-trivial të përpjekjeve manuale janë të nevojshme për rregullimin e arkitekturave të rrjetit, konfigurimet e trajnimit dhe rregullimet e paraprocesimit të tilla si kodimi i çiftit byte (BPE). Në këtë studim, propozojmë një metodë të rregullimit automatik të bazuar në strategjinë e evolucionit për NMT. Ne veçanërisht, ne aplikojmë strategjinë e adaptimit-evolucionit të matricës kovariance (CMA-ES) dhe hetojmë një CMA-ES me shumë objektiva bazuar në Pareto për të optimizuar performancën e përkthimit dhe kohën llogaritare së bashku. Rezultatet eksperimentale tregojnë se metoda e propozuar gjen automatikisht sisteme NMT që tejkalojnë rregullimin fillestar manual.', 'am': 'የናውራዊ መሻሻን ትርጉም (NMT) ስርዓቶች በቅርብ ዓመታት የተስፋ ውጤቶች አሰልተዋል፡፡ ምንም እንኳን፣ የባይት ሁለት የኮድ ክፍተት (BPE) ለሚመስሉ የመረብ መሠረት፣ አስተማሪ ምርጫዎች እና የፊተኛውን ክፍተት ማቀናጃ ያስፈልጋል፡፡ በዚህ ትምህርት፣ ለNMT የተመሳሳይ የውጤት strategieን ለራሱ ማቀናቀል እናስጀራለን፡፡ በተለይም፣ የተለየ ማትሪክስ አካባቢ-የውጤት strategy (CMA-ES) እና የፓሮቲ-based ብዙአዊ የCMA-ES ተርጓሚውን እና ቁጥጥር ሰዓት በመጠቀም እናደርጋለን፡፡ ፈተና ውጤቶች የጀመሪያው እጁን ማዘጋጀት የሚችሉትን NMT ሲስተም በራስነት እንዲያገኝ ያሳያል፡፡', 'hy': 'Neural machine translation (NMT) systems have demonstrated promising results in recent years.  Այնուամենայնիվ, ոչ փոքրիկ ձեռքի ջանքեր են պահանջվում ցանցի ճարտարապետությունների, ուսումնասիրության կոնֆորմացիաների և նախավերամշակման կարգավորումների, ինչպիսիք են բայտի զույգ կոդավորումը (Bpe). Այս ուսումնասիրության ընթացքում մենք առաջարկում ենք էվոլյուցիայի ռազմավարության հիմնված ավտոմատիկ հարմարեցման մեթոդ NMT-ի համար: Մենք հատկապես կիրառում ենք կովարիանսային մատրիքսի ադապտացիայի-էվոլյուցիայի ռազմավարությունը (ԿՄԱ-ԷՍ) և ուսումնասիրում ենք Pareto-ի հիմնված բազմաօբյեկտիվ ԿՄԱ-ԷՍ-ը, որպեսզի միասին օպտիմացվի Փորձարկման արդյունքները ցույց են տալիս, որ առաջարկված մեթոդը ինքնաբերաբար գտնում է NMT համակարգեր, որոնք գերազանցում են սկզբնական ձեռքի սահմանումը:', 'az': "NMT sistemləri son illərdə vəd verən sonuçları göstərdilər. Lakin, ağ arhitektarını, təhsil yapılandırmaları və bayt çift kodlaması (BPE) kimi əvvəlcə işləmə qurğuları təyin etmək üçün çox kiçik olmayan qulaqlar lazımdır. Bu təcrübədə, NMT üçün avtomatik təcrübə metodlarını təklif edirik. Özellikle, biz covarians matris adaptasyon-evolution stratejisini (CMA-ES) istifadə edirik və Pareto-tabanlı çox-objektif CMA-ES'i birlikdə tercümə performansını və hesablama vaxtını optimizləmək üçün bir araşdırarıq. Müvəffəqiyyət sonuçları təbliğ edilmiş metodların başlanğıc əlli qurğulu təyin etməsindən üstün olan NMT sistemlərini avtomatik olaraq tapır.", 'bn': 'সাম্প্রতিক বছরে নিউরেল মেশিন অনুবাদ (এনএমটি) সিস্টেম প্রতিশ্রুতি প্রদর্শন করেছে। তবে নেটওয়ার্ক কাঠামো সংযোগ, প্রশিক্ষণ কনফিগারেশন এবং পূর্ব প্রক্রিয়ার বৈশিষ্ট্য যেমন বাইট জোড়া এনকোডিং (বিপেই) সংযোগের জন্য ব্যবস্থা প্রয় এই গবেষণায় আমরা এনএমটির জন্য একটি বিবর্তন কৌশলের প্রস্তাব করি। বিশেষ করে আমরা প্যারেটো ভিত্তিক বহুবস্তু সিএমএস-এর কৌশলটি প্রয়োগ করি অনুবাদের প্রদর্শন এবং গণনালীন সময়ের সম্মানিত করার জন্য। পরীক্ষার ফলাফল দেখাচ্ছে যে প্রস্তাবিত পদ্ধতি স্বয়ংক্রিয়ভাবে NMT সিস্টেম পাওয়া যায় যা প্রাথমিক হাত ব্যবস্থা', 'bs': 'Sistemi neurološkog prevoda (NMT) pokazali su obećavajuće rezultate u posljednjih godina. Međutim, ne-trivialne količine ručnih napora su potrebne za tuniranje arhitektura mreže, konfiguracije obuke i nastave pre obrade poput kodiranja bajtskih parova (BPE). U ovoj studiji predlažemo metodu automatskog prilagodbe za NMT-a na temelju evolucijske strategije. Posebno, primjenjujemo strategiju adaptacije matrice kovarijancije (CMA-ES) i istražujemo multiobjektivnu CMA-ES na Pareto-baziranu za optimizaciju učinkovitosti prevođenja i računalnog vremena zajedno. Eksperimentalni rezultati pokazuju da predložena metoda automatski pronalazi NMT sustave koji iznose početnu manualnu postavku.', 'ca': "Els sistemes neuromàquines de traducció (NMT) han demostrat resultats prometedors en els últims anys. No obstant això, s'exigeixen quantitats no trivials d'esforç manual per ajustar arquitectures de xarxa, configuracions de capacitació i configuracions de pré-processament com la codificació de parell de bytes (BPE). En aquest estudi, proposem un mètode d'ajustament automàtic basat en una estratègia d'evolució per a la MTN. En particular, aplicam l'estratègia d'adaptació-evolució de la matriu de covariança (CMA-ES) i investigam una CMA-ES multiobjectiva basada en Pareto per optimitzar el rendiment de traducció i el temps computacional conjuntament. Els resultats experimentals mostren que el mètode proposat troba automàticament sistemes NMT que superen la configuració manual inicial.", 'cs': 'Systémy neuronového strojového překladu (NMT) v posledních letech prokázaly slibné výsledky. Nicméně, netriviální množství manuálního úsilí je nutné pro ladění síťových architektur, konfigurací školení a nastavení předzpracování, jako je kódování bajtových párů (BPE). V této studii navrhujeme evoluční strategii založenou na automatickém ladění NMT metody. Konkrétně aplikujeme strategii adaptace-evoluce kovarianční matice (CMA-ES) a zkoumáme multiobjektivní CMA-ES založený na Paretovi, abychom společně optimalizovali výkon překladu a výpočetní čas. Experimentální výsledky ukazují, že navrhovaná metoda automaticky najde NMT systémy, které překonávají počáteční manuální nastavení.', 'et': 'Neuraalne masintõlke (NMT) süsteem on viimastel aastatel näidanud paljulubavaid tulemusi. Võrguarhitektuuride, koolituskonfiguratsioonide ja eeltöötlusseadete (nt baidipaari kodeerimise) häälestamiseks on siiski vaja mittetiviaalset käsitsi pingutust. Selles uuringus pakume välja arengustrateegial põhineva automaatse häälestamise meetodi NMT jaoks. Eelkõige rakendame kovariatsiooni maatriksi kohanemise-evolutsiooni strateegiat (CMA-ES) ja uurime Pareto-põhist mitmeobjektiivset CMA-ES, et optimeerida tõlkimisjõudlust ja arvutusaega ühiselt. Eksperimentaalsed tulemused näitavad, et kavandatud meetod leiab automaatselt NMT süsteemid, mis ületavad esialgset käsitsi seadistust.', 'fi': 'Neuroiden konekäännösjärjestelmät (NMT) ovat osoittaneet lupaavia tuloksia viime vuosina. Verkkoarkkitehtuurien, harjoituskokoonpanojen ja BPE:n kaltaisten esikäsittelyasetusten, virittämiseen tarvitaan kuitenkin ei-triviaalisia määriä manuaalista työtä. Tässä tutkimuksessa ehdotamme NMT:n evoluutiostrategiaan perustuvaa automaattista viritysmenetelmää. Erityisesti sovellamme kovarianssimatriisin adaptaatio-evoluutio-strategiaa (CMA-ES) ja tutkimme Pareto-pohjaista monitavoitetta CMA-ES optimoidaksemme käännöksen suorituskyvyn ja laskennallisen ajan yhdessä. Kokeelliset tulokset osoittavat, että ehdotettu menetelmä löytää automaattisesti NMT-järjestelmät, jotka suoriutuvat manuaalista alkuasetusta paremmin.', 'jv': 'Sistem Neral Perintah (NMT)kang dipuapakan uwong dadi kapan nang banjur sing dumadhi. politenessoffpolite"), and when there is a change ("assertivepoliteness NMT Juara sing, kéné isaké nggambar aturan karo cobalén matères adalah-evolusi nggawe gerakan, karo sak yatêmat urikno sing basa gambar luwih-buku multi-object Ndoleh sing paling-paling mbutuhan nganggo cara nggawe sistem NMT kang sampeyan Manual', 'sk': 'Sistemi nevralnega strojnega prevajanja (NMT) so v zadnjih letih pokazali obetavne rezultate. Vendar pa je za nastavitev omrežnih arhitektur, konfiguracij usposabljanja in nastavitev predobdelave, kot je kodiranje bajtnih parov (BPE), potrebna netrivialna količina ročnega napora. V tej študiji predlagamo evolucijsko strategijo temelječo metodo avtomatskega uglaševanja za NMT. Zlasti uporabljamo strategijo adaptacije-evolucije kovariance matrice (CMA-ES) in raziskujemo večnamenski CMA-ES, ki temelji na Paretu, da bi skupaj optimizirali učinkovitost prevajanja in računalniški čas. Poskusni rezultati kažejo, da predlagana metoda samodejno najde sisteme NMT, ki presegajo začetno ročno nastavitev.', 'he': 'Neural machine translation (NMT) systems have demonstrated promising results in recent years.  בכל אופן, כמויות לא טריוויאליות של מאמץ ידני נדרשות לתאים ארכיטקטורות רשת, תצורות אימון, ותיקים לפני העבודה, כמו הקוד של זוג בייטים (BPE). במחקר הזה, אנו מציעים אסטרטגיה לאבולוציה מבוססת על שיטת התאמה אוטומטית עבור NMT. In particular, we apply the covariance matrix adaptation-evolution strategy (CMA-ES), and investigate a Pareto-based multi-objective CMA-ES to optimize the translation performance and computational time jointly.  תוצאות ניסויים מראות שהשיטה המוצעת מוצאת באופן אוטומטי מערכות NMT שמעבירות את ההגדרה הידית הראשונה.', 'ha': "@ info: whatsthis Amma, ana ƙayyade nau'in aiki na hannayen da ba'a yi sauna dõmin tunkuɗe tsarin masu tsari na shawarar, da tsarin tsari na zaman aikin aiki, kamar kodi-biyu (BPA). A cikin wannan littafin, Munã buɗa wani takwai na buɗe ko da ɗabi'a zuwa NMT. Kayyan, Munã amfani da kimar adaptation-ciwal matriki (CMA-ES), kuma munãƙidãya a saman-Parato-bayan wasu masu jiyyan CMA-ES, dõmin ya yi amfani da fassarar fassarar da lokaci na lissafi. Faramar jarrabawa ya nuna cewa metoden da aka buƙata ana farat ɗaya na gane na'urar NMT da ke gabatar da tsarin manual na farko.", 'bo': 'ནུས་མེད་ལག་གི་སྒེར་གྱི་རྩིས་འཁྲུལ་ནས་དབྱིབས་འགྱུར་བ(NMT)མ་ལག་གིས་འཆར་བར་ཆད་པ་ལྟར་སྐྱེས་ However, non-trivial amounts of manual effort are required for tuning network architectures, training configurations, and pre-processing settings such as byte pair encoding (BPE). དབྱེ་ཞིབ་འདིའི་ནང་དུ་འུ་ཅག་གིས་འཕེལ་རིམ་གྱི་ཐབས་ལམ་ལུགས་རང་འགུལ་གྱི་སྒྲིག་འཛུགས་ཀྱི་ཐབས་ལམ་ཞིག དམིགས་བསལ་ན། ང་ཚོས་covariance matrix adaptation-evolution strategy(CMA-ES)དང་སྤྲོད་ཀྱི་འདྲི་ཞིབ་བྱས་པ་ལས་ པེ་རོཊ་ལ་གཞི་བརྟེན་པའི་སྣ་གཟུགས Experimental results show that the proposed method automatically finds NMT systems that perform the initial manual setting.'}
{'en': 'Continuous Space Reordering Models for Phrase-based MT MT', 'es': 'Modelos de reordenación continua del espacio para MT basada en frases', 'fr': "Modèles de réorganisation continue de l'espace pour la TA basée sur des phrases", 'pt': 'Modelos de Reordenamento de Espaço Contínuo para MT Baseado em Frases', 'ar': 'نماذج إعادة ترتيب المساحة المستمرة في الترجمة الآلية القائمة على العبارات', 'hi': 'वाक्यांश-आधारित एमटी के लिए निरंतर अंतरिक्ष पुनर्क्रमण मॉडल', 'zh': '盖短语之 MT ,连空复序', 'ja': 'フレーズベースのMTのための連続的なスペースリオーダーモデル', 'ru': 'Модели непрерывного переупорядочения пространства для МП на основе фраз', 'ga': 'Múnlaí Athordaithe Spás Leanúnacha le haghaidh MT Frás-bhunaithe', 'ka': 'Comment', 'el': 'Μοντέλα συνεχούς αναδιατάστασης διαστήματος για ΜΤ με βάση φράσεις', 'hu': 'Folyamatos űrberendezési modellek a kifejezésalapú MT-hez', 'it': 'Modelli di riordino continuo dello spazio per MT basato su frasi', 'kk': 'Фраз негіздеген MT үшін келесі бос орын қайта реттеу үлгілері', 'lt': 'Nuolatiniai frazėmis pagrįsto MT kosmoso reguliavimo modeliai', 'mk': 'Континуиран вселенски модел за реорганизација за MT базиран на фрази', 'ms': 'Model Pengurusan Semula Ruang Terus untuk MT berdasarkan Frasa', 'ml': 'വാക്കുകള്\u200d അടിസ്ഥാനമാക്കുന്ന MT- നുള്ള വാക്കുകള്\u200dക്കുള്ള സ്പെയിസ് വീണ്ടും ഓര്\u200dഡര്\u200d മോഡലു', 'mt': 'Mudelli kontinwi ta’ Ordnar mill-ġdid tal-Ispazju għal MT ibbażat fuq il-Frażi', 'mn': 'Ураагийн төлөвлөгөөтэй MT-ийн загварыг үргэлжлүүлэх орон зай', 'no': 'Continuous Space Reorder Models for Phrase-based MT', 'pl': 'Modele ciągłej zmiany porządku przestrzeni dla MT opartych na frazach', 'ro': 'Modele de reordonare continuă a spațiului pentru MT bazate pe fraze', 'sr': 'Neprestano raspoređenje svemirskih modela za prekrasni MT', 'si': 'ප්\u200dරේස් අධාරිත MT වෙනුවෙන් නිතරම් අවධාන අවධානය ආපහු අවධානය කරන්න', 'so': 'Isku socda Space Reordering Models for Phrase-based MT', 'sv': 'Modeller för kontinuerlig omredning av rymden för frasbaserad MT', 'ta': 'சொற்றொடர் அடிப்படையிலான MT க்கான தொடர் இடைவெளி மறுவரிசைப்படுத்தல் மாதிரிகள்', 'ur': 'فارس بنیادی MT کے لئے قائم رہنے والی جگہ', 'uz': 'Name', 'vi': 'Chế độ lắp đặt vũ trụ liên tục MTV', 'bg': 'Модели за непрекъснато пренареждане на пространството за фразата базирана MT', 'hr': 'Continuous Space Reordering Models for Phrase-based MT', 'da': 'Modeller til kontinuerlig rumomlægning for sætningsbaserede MT', 'nl': 'Continue ruimteherschikkingsmodellen voor op zinnen gebaseerde MT', 'id': 'Model Pengurusan Ruang Terus untuk MT berdasarkan Frasa', 'fa': 'مدل\u200cهای دوباره دستور دادن فضا برای MT بر اساس عبارت', 'tr': 'Fraz tabanly MT üçin daýygna Seleň Girdir', 'de': 'Continuous Space Reordering Modelle für Phrase-basierte MT', 'ko': '짧은 말 기반의 기계 번역 연속 공간 정렬 모델', 'sq': 'Modelet e rregullimit të vazhdueshëm të hapësirës për MT bazuar në fryzë', 'sw': 'Simulizi za kudhibiti Nyakati kwa MT inayosimamia maneno', 'hy': 'Continuous Space Reordering Models for Phrase-based MT', 'af': 'Continuous Space Reorder Models for Phrase-based MT', 'az': 'Fraz tabanl캼 MT 칲칞칲n s칲r톛kli uzay yeni s캼ralama modell톛ri', 'bn': 'বাক্য-ভিত্তিক MT- এর জন্য অব্যাহত স্পেস পুনরায় নির্ধারণ মডেল', 'bs': 'Continuous Space Reordering Models for Phrase-based MT', 'ca': 'Models continus de reestructuració espacialper MT basat en frases', 'am': 'የፊደል ቅርጽ ምርጫዎች', 'cs': 'Modely kontinuálního uspořádání prostoru pro MT založené na frázích', 'fi': 'Jatkuvan tilan uudelleenjärjestelyn mallit lausepohjaiselle MT:lle', 'et': 'Pidev ruumi ümberkorraldamine mudelid fraasipõhise MT jaoks', 'jv': 'Reorder', 'sk': 'Modeli stalnega prerazporejanja prostora za MT, ki temelji na frazah', 'ha': '@ action', 'he': 'מודלים ממשיכים להזמין מחדש חלל עבור MT מבוסס על פרזה', 'bo': 'ཡིག་ཆ་གཞི་བརྟེན་པའི་MT ལ་རྣམ་པའི་མིག་དཔེ་དབྱིབས་བསྐྱར་བཀོད་བྱེད་སྐབས'}
{'en': 'Bilingual sequence models improve phrase-based translation and reordering by overcoming phrasal independence assumption and handling long range reordering. However, due to data sparsity, these  models  often fall back to very small context sizes. This problem has been previously addressed by learning sequences over generalized representations such as  POS tags  or word clusters. In this paper, we explore an alternative based on  neural network models . More concretely we train neuralized versions of lexicalized reordering [ 1 ] and the operation sequence models [ 2 ] using feed-forward neural network. Our results show improvements of up to 0.6 and 0.5 BLEU points on top of the baseline German!English and English!German systems. We also observed improvements compared to the  systems  that used POS tags and word clusters to train these  models . Because we modify the bilingual corpus to integrate reordering operations, this allows us to also train a sequence-to-sequence neural MT model having explicit reordering triggers. Our motivation was to directly enable reordering information in the encoder-decoder framework, which otherwise relies solely on the attention model to handle long range reordering. We tried both coarser and fine-grained reordering operations. However, these experiments did not yield any improvements over the baseline Neural MT systems.', 'es': 'Los modelos de secuencia bilingües mejoran la traducción y el reordenamiento basados en frases al superar la suposición de independencia de la frase y manejar el reordenamiento de largo alcance. Sin embargo, debido a la escasez de datos, estos modelos suelen recurrir a tamaños de contexto muy pequeños. Este problema se ha abordado previamente aprendiendo secuencias sobre representaciones generalizadas como etiquetas POS o grupos de palabras. En este artículo, exploramos una alternativa basada en modelos de redes neuronales. Más concretamente, entrenamos versiones neuralizadas de reordenamiento lexicalizado [1] y los modelos de secuencia de operación [2] utilizando una red neuronal feed-forward. ¡Nuestros resultados muestran mejoras de hasta 0.6 y 0.5 puntos BLEU por encima del alemán de referencia! ¡Inglés e inglés! Sistemas alemanes. También observamos mejoras en comparación con los sistemas que utilizaban etiquetas POS y grupos de palabras para entrenar estos modelos. Debido a que modificamos el corpus bilingüe para integrar operaciones de reordenamiento, esto nos permite también entrenar un modelo de MT neuronal secuencia a secuencia que tiene desencadenantes de reordenamiento explícitos. Nuestra motivación fue permitir directamente el reordenamiento de la información en el marco del codificador-decodificador, que de lo contrario se basa únicamente en el modelo de atención para gestionar el reordenamiento de largo alcance. Probamos operaciones de reordenamiento tanto gruesas como finas. Sin embargo, estos experimentos no produjeron ninguna mejora con respecto a los sistemas de MT Neural de referencia.', 'ar': 'تعمل نماذج التسلسل ثنائي اللغة على تحسين الترجمة القائمة على العبارات وإعادة الترتيب من خلال التغلب على افتراض استقلالية الجمل الفعلية والتعامل مع إعادة الترتيب طويلة المدى. ومع ذلك ، نظرًا لتباين البيانات ، غالبًا ما تعود هذه النماذج إلى أحجام سياق صغيرة جدًا. تمت معالجة هذه المشكلة مسبقًا من خلال تسلسل التعلم عبر التمثيلات المعممة مثل علامات نقاط البيع أو مجموعات الكلمات. في هذه الورقة ، نستكشف بديلًا يعتمد على نماذج الشبكة العصبية. بشكل أكثر تحديدًا ، نقوم بتدريب الإصدارات العصبية من إعادة الترتيب المعجمية [1] ونماذج تسلسل العملية [2] باستخدام شبكة التغذية الأمامية العصبية. تُظهر نتائجنا تحسينات تصل إلى 0.6 و 0.5 نقطة من BLEU أعلى أنظمة الألمانية! الإنجليزية والإنجليزية! الألمانية الأساسية. لاحظنا أيضًا تحسينات مقارنة بالأنظمة التي تستخدم علامات نقاط البيع ومجموعات الكلمات لتدريب هذه النماذج. نظرًا لأننا نقوم بتعديل المجموعة ثنائية اللغة لدمج عمليات إعادة الترتيب ، فإن هذا يسمح لنا أيضًا بتدريب نموذج MT العصبي المتسلسل إلى التسلسل الذي يحتوي على مشغلات إعادة ترتيب واضحة. كان دافعنا هو تمكين إعادة ترتيب المعلومات بشكل مباشر في إطار عمل وحدة فك التشفير ، والذي يعتمد بخلاف ذلك فقط على نموذج الانتباه للتعامل مع إعادة الترتيب بعيد المدى. لقد جربنا كلاً من عمليات إعادة الترتيب الخشنة والدقيقة. ومع ذلك ، لم تسفر هذه التجارب عن أي تحسينات على أنظمة الترجمة الآلية العصبية الأساسية.', 'pt': 'Os modelos de sequências bilíngues melhoram a tradução e a reordenação baseadas em frases, superando a suposição de independência frasal e lidando com a reordenação de longo alcance. No entanto, devido à escassez de dados, esses modelos geralmente voltam a tamanhos de contexto muito pequenos. Este problema foi abordado anteriormente por sequências de aprendizado sobre representações generalizadas, como tags POS ou agrupamentos de palavras. Neste artigo, exploramos uma alternativa baseada em modelos de redes neurais. Mais concretamente, treinamos versões neuralizadas de reordenação lexicalizada [1] e os modelos de sequência de operação [2] usando rede neural feed-forward. Nossos resultados mostram melhorias de até 0,6 e 0,5 pontos BLEU sobre os sistemas de base alemão!inglês e inglês!alemão. Também observamos melhorias em relação aos sistemas que usavam tags POS e clusters de palavras para treinar esses modelos. Como modificamos o corpus bilíngue para integrar as operações de reordenação, isso nos permite também treinar um modelo de MT neural sequência a sequência com gatilhos de reordenação explícitos. Nossa motivação era permitir diretamente a reordenação de informações na estrutura codificador-decodificador, que de outra forma depende apenas do modelo de atenção para lidar com a reordenação de longo alcance. Tentamos operações de reordenamento mais grosseiras e de granulação fina. No entanto, esses experimentos não produziram nenhuma melhoria em relação aos sistemas de MT neural de linha de base.', 'fr': "Les modèles de séquence bilingues améliorent la traduction et la réorganisation basées sur les phrases en surmontant l'hypothèse d'indépendance phrasale et en gérant la réorganisation à longue portée. Cependant, en raison de la rareté des données, ces modèles se rabattent souvent à de très petites tailles de contexte. Ce problème a déjà été résolu par des séquences d'apprentissage sur des représentations généralisées telles que des balises POS ou des groupes de mots. Dans cet article, nous explorons une alternative basée sur des modèles de réseaux neuronaux. Plus concrètement, nous formons des versions neuralisées de réordonnancement lexicalisé [1] et des modèles de séquences d'opérations [2] à l'aide d'un réseau de neurones par anticipation. Nos résultats montrent des améliorations allant jusqu'à 0,6 et 0,5 point UEBL en plus de l'allemand de référence\xa0! Anglais et anglais\xa0! Systèmes allemands. Nous avons également observé des améliorations par rapport aux systèmes qui utilisaient des balises POS et des groupes de mots pour entraîner ces modèles. Comme nous modifions le corpus bilingue pour intégrer des opérations de réorganisation, cela nous permet également d'entraîner un modèle de TA neuronale séquence à séquence ayant des déclencheurs de réorganisation explicites. Notre motivation était d'activer directement les informations de réorganisation dans le cadre codeur-décodeur, qui autrement repose uniquement sur le modèle d'attention pour gérer la réorganisation à longue distance. Nous avons essayé des opérations de réorganisation plus grossières et plus fines. Cependant, ces expériences n'ont apporté aucune amélioration par rapport aux systèmes de MT neurale de référence.", 'ja': 'バイリンガルシーケンスモデルは、フレーズの独立性の仮定を克服し、長距離の並べ替えを処理することによって、フレーズベースの翻訳と並べ替えを改善する。 しかし、データの希少性のため、これらのモデルはしばしば非常に小さなコンテキストサイズに戻ります。 この問題は、POSタグやワードクラスタなどの一般化された表現を介した学習シーケンスによって以前に対処されてきた。 本稿では、ニューラルネットワークモデルに基づく代替案を探求する。 より具体的には、フィードフォワードニューラルネットワークを使用して、辞書化された再順序付けのニューラライズされたバージョン[1]と操作シーケンスモデル[2]をトレーニングします。 私たちの結果は、ベースラインのドイツ語の上に最大0.6と0.5のBLEUポイントの改善を示しています！英語とドイツ語だまた、POSタグとワードクラスタを使用してこれらのモデルをトレーニングしたシステムと比較して、改善が見られました。 バイリンガルコーパスを修正して再順序付け動作を統合するため、これにより、明示的な再順序付けトリガを有するシーケンスツーシーケンスニューラルMTモデルも訓練することができる。 私たちの動機は、エンコーダデコーダフレームワーク内の情報を直接並べ替えることを可能にすることでした。そうでなければ、長距離並べ替えを処理するために注意モデルのみに依存しています。 私たちは、より粗雑で細かい並べ替え作業の両方を試みました。 しかしながら、これらの実験は、ベースラインのニューラルMTシステムよりも改善をもたらさなかった。', 'zh': '双语序因克短语独立性假设处分长距离更进短语译、重序。 然数疏性,常回退至小上下文大小。 前此已学序而非广义(如 POS 标单词簇)以决之。 本文中,探一基于神经网络模代方案。 更具体地说,用前馈神经网络训练词法化重排 [1] 及操作序 [2] 神经化版本。 吾之的结果显示,基线德语之基,以崇0.60.5 BLEU! 英语和英语! 德国系统。 比之 POS 标单词聚类,观其所改。 因改双语语料库以集成重序,仍许训练显式重排触发器序于神经 MT 式。 吾之机,直于编码器-解码器框架中得息而复之,不然框架但依意模形以处远程。 试更粗细粒度重排序。 然与基线神经MT系统相比,实验无所更进。', 'ru': 'Двуязычные модели последовательности улучшают фразеологический перевод и переупорядочение, преодолевая предположение о независимости фраз и обрабатывая переупорядочение на большие расстояния. Однако из-за ограниченности данных эти модели часто возвращаются к очень небольшим размерам контекста. Ранее эта проблема была решена путем обучения последовательностей над обобщенными представлениями, такими как POS-теги или кластеры слов. В этой статье мы исследуем альтернативу, основанную на моделях нейронных сетей. Более конкретно, мы обучаем нейрализованные версии лексикализованного переупорядочения [1] и модели последовательности операций [2] с использованием прямой нейронной сети. Наши результаты показывают улучшения до 0,6 и 0,5 баллов BLEU на вершине базовой немецкой!Английский и английский!Немецкие системы. Мы также наблюдали улучшения по сравнению с системами, которые использовали POS-теги и кластеры слов для обучения этих моделей. Поскольку мы модифицируем двуязычный корпус для интеграции операций переупорядочения, это позволяет нам также обучать последовательную нейронную модель MT, имеющую явные триггеры переупорядочения. Наша мотивация состояла в том, чтобы напрямую включить переупорядочение информации в фреймворке кодировщик-декодер, который в противном случае полагается исключительно на модель внимания для обработки переупорядочения на большие расстояния. Мы попробовали как более грубые, так и мелкозернистые операции по переупорядочению. Тем не менее, эти эксперименты не дали никаких улучшений по сравнению с базовыми системами нейронной МТ.', 'hi': 'द्विभाषी अनुक्रम मॉडल वाक्यांश-आधारित अनुवाद में सुधार करते हैं और phrasal स्वतंत्रता धारणा पर काबू पाने और लंबी दूरी की reordering हैंडलिंग द्वारा reordering। हालांकि, डेटा स्पार्सिटी के कारण, ये मॉडल अक्सर बहुत छोटे संदर्भ आकारों में वापस आ जाते हैं। इस समस्या को पहले सामान्यीकृत अभ्यावेदन जैसे POS टैग या शब्द क्लस्टर पर अनुक्रमों को सीखकर संबोधित किया गया है। इस पेपर में, हम तंत्रिका नेटवर्क मॉडल के आधार पर एक विकल्प का पता लगाते हैं। अधिक ठोस रूप से हम लेक्सिकलाइज्ड रीऑर्डरिंग [1] और ऑपरेशन अनुक्रम मॉडल [2] के न्यूरलाइज्ड संस्करणों को फीड-फॉरवर्ड न्यूरल नेटवर्क का उपयोग करके प्रशिक्षित करते हैं। हमारे परिणाम बेसलाइन जर्मन के शीर्ष पर 0.6 और 0.5 BLEU अंक तक के सुधार दिखाते हैं! अंग्रेजी और अंग्रेजी! जर्मन प्रणालियों। हमने उन प्रणालियों की तुलना में सुधार भी देखा जो इन मॉडलों को प्रशिक्षित करने के लिए पीओएस टैग और शब्द समूहों का उपयोग करते थे। क्योंकि हम पुनर्क्रमण संचालन को एकीकृत करने के लिए द्विभाषी कॉर्पस को संशोधित करते हैं, यह हमें स्पष्ट पुन: क्रमबद्ध ट्रिगर्स वाले अनुक्रम-से-अनुक्रम तंत्रिका एमटी मॉडल को भी प्रशिक्षित करने की अनुमति देता है। हमारी प्रेरणा सीधे एनकोडर-डिकोडर फ्रेमवर्क में रीऑर्डरिंग जानकारी को सक्षम करना था, जो अन्यथा लंबी दूरी की रीऑर्डरिंग को संभालने के लिए पूरी तरह से ध्यान मॉडल पर निर्भर करता है। हम दोनों मोटे और ठीक दानेदार reordering आपरेशनों की कोशिश की. हालांकि, इन प्रयोगों ने बेसलाइन न्यूरल एमटी सिस्टम पर कोई सुधार नहीं किया।', 'ga': "Feabhsaíonn samhlacha seichimh dátheangacha aistriúchán agus athordú bunaithe ar fhrásaí trí thoimhde neamhspleáchais frásaí a shárú agus athordú fadraoin a láimhseáil. Mar gheall ar ghanntanas sonraí, áfach, is minic a thiteann na samhlacha seo siar go méideanna comhthéacs an-bheag. Tugadh aghaidh ar an bhfadhb seo roimhe seo trí sheichimh foghlama thar léirithe ginearálaithe ar nós clibeanna POS nó cnuasaigh focal. Sa pháipéar seo, déanaimid iniúchadh ar rogha eile atá bunaithe ar shamhlacha líonraí néaracha. Ar bhealach níos nithiúla déanaimid oiliúint ar leaganacha néaraithe den athordú lexicalized [1] agus na samhlacha seicheamh oibríochta [2] ag baint úsáide as líonra néaraíoch beathaithe ar aghaidh. Léiríonn ár dtorthaí feabhsuithe suas le 0.6 agus 0.5 pointe BLEU ar bharr na gcóras bunlíne Gearmáinise!Béarla agus Béarla! Thugamar faoi deara freisin feabhsuithe i gcomparáid leis na córais a d'úsáid clibeanna POS agus cnuasaigh focal chun na samhlacha seo a oiliúint. Toisc go ndéanaimid an corpas dátheangach a mhodhnú chun oibríochtaí athordaithe a chomhtháthú, ligeann sé seo dúinn freisin múnla néar MT seicheamh-go-seicheamh a oiliúint a bhfuil truicear athordaithe soiléir aige. Ba é an spreagadh a bhí againn ná faisnéis athordaithe a chumasú go díreach sa chreat ionchódóra-díchódóra, a bhraitheann ar shlí eile go hiomlán ar an tsamhail aird chun athordú fadraoin a láimhseáil. Rinneamar oibríochtaí athordaithe níos garbh agus míne araon. Mar sin féin, níor tháinig aon fheabhsuithe as na turgnaimh seo ar na córais bhunlíne Neural MT.", 'el': 'Τα δίγλωσσα μοντέλα αλληλουχίας βελτιώνουν τη μετάφραση και την αναδιατάσταση με βάση τις φράσεις ξεπερνώντας την υπόθεση της ανεξαρτησίας φράσεων και διαχειρίζονται την αναδιατάσταση μεγάλης εμβέλειας. Ωστόσο, λόγω της έλλειψης δεδομένων, αυτά τα μοντέλα συχνά επιστρέφουν σε πολύ μικρά μεγέθη περιβάλλοντος. Αυτό το πρόβλημα έχει αντιμετωπιστεί προηγουμένως με μαθησιακές ακολουθίες πάνω από γενικευμένες αναπαραστάσεις όπως ετικέτες ή ομάδες λέξεων. Σε αυτή την εργασία, διερευνούμε μια εναλλακτική λύση βασισμένη σε μοντέλα νευρωνικών δικτύων. Πιο συγκεκριμένα εκπαιδεύουμε νευραλισμένες εκδόσεις της λεξιλογικής αναδιοργάνωσης [1] και τα μοντέλα ακολουθίας λειτουργίας [2] χρησιμοποιώντας το τροφοδοτικό νευρικό δίκτυο. Τα αποτελέσματά μας δείχνουν βελτιώσεις μέχρι 0.6 και 0.5 πόντους στην κορυφή των βασικών γερμανικών! Αγγλικά και Αγγλικά! Γερμανικά συστήματα. Παρατηρήσαμε επίσης βελτιώσεις σε σύγκριση με τα συστήματα που χρησιμοποίησαν ετικέτες και ομάδες λέξεων για να εκπαιδεύσουν αυτά τα μοντέλα. Επειδή τροποποιούμε το δίγλωσσο σώμα για να ενσωματώσουμε τις λειτουργίες αναδιοργάνωσης, αυτό μας επιτρέπει επίσης να εκπαιδεύσουμε ένα νευρωνικό μοντέλο ακολουθίας-ακολουθίας με ρητές ενεργοποιητές αναδιοργάνωσης. Το κίνητρό μας ήταν να επιτρέψουμε άμεσα την αναδιατάσταση πληροφοριών στο πλαίσιο κωδικοποιητή-αποκωδικοποιητή, το οποίο διαφορετικά βασίζεται αποκλειστικά στο μοντέλο προσοχής για να χειριστεί την αναδιατάσταση μεγάλης εμβέλειας. Δοκιμάσαμε και πιο χοντρές και λεπτόκοκκες εργασίες αναδιατάστασης. Ωστόσο, αυτά τα πειράματα δεν επέφεραν καμία βελτίωση σε σχέση με τα βασικά Νευρικά ΜΤ συστήματα.', 'hu': 'A kétnyelvű szekvencia modellek javítják a kifejezésalapú fordítást és az átrendezést a kifejezések függetlenségének feltételezésével és a hosszú távú átrendezés kezelésével. Azonban az adatok ritkasága miatt ezek a modellek gyakran nagyon kis kontextusméretekre esnek vissza. Ezt a problémát korábban az általánosított reprezentációk, például POS címkék vagy szóklaszterek feletti tanulással kezelték. Ebben a tanulmányban egy neurális hálózati modelleken alapuló alternatívát vizsgálunk. Konkrétabban tanítjuk a lexikalizált átrendezés neuralizált verzióit [1] és az operációs szekvencia modelleket [2] feed-forward neurális hálózat segítségével. Eredményeink 0,6 és 0,5 BLEU pontos javulást mutatnak a német alap felett! Angol és angol! Német rendszerek. Továbbá javulást figyeltünk meg azokhoz a rendszerekhez képest, amelyek POS címkéket és szóklasztereket használtak ezeknek a modelleknek a képzésére. Mivel a kétnyelvű korpuszt úgy módosítjuk, hogy integráljuk az átrendezési műveleteket, ez lehetővé teszi számunkra, hogy egy szekvencia-szekvencia neurális MT modellt is képezzünk, amely explicit átrendezési triggereket tartalmaz. motivációnk az volt, hogy közvetlenül lehetővé tegyük az információk átrendezését a kódoló-dekóder keretrendszerben, amely egyébként kizárólag a figyelemmodellre támaszkodik a hosszú távú átrendezés kezeléséhez. Megpróbáltuk durvább és finomszemcsésebb átrendezési műveleteket is. Ezek a kísérletek azonban nem eredményeztek javulást a kiindulási Neural MT rendszerekhez képest.', 'ka': 'მეორე წერტილის მოდელები ფრაზების გარეშექმნა და რეორდირება გარეშექმნა ფრაზალების განსაზღვრებით და განსაზღვრებით განსაზღვრებით გარეშექმნა. მაგრამ ეს მოდელები ძალიან პატარა კონტექსტური ზომის შემდეგ დაბრუნდება. ეს პრობლემა წინასწარმოდგენა სწავლის წინასწორებებით, როგორც POS- ნიშანები ან სიტყვების კლასტერები. ჩვენ ამ დოკუნეში ალტენტრუმენტი ნეიროლური ქსელის მოდელების ბაზედან ვხედავთ. უფრო კონკრეტულად ჩვენ ლექსიკალიზებული რეორდენციის ნეირალიზებული ვერსიების გაგრძნობა [1] და პერაციის რეორდენციის მოდელების [2] გამოყენება feed-forward ნეირალური ქს ნაქთრვ პვჱსლრართ ოჲკაჱგარ ოჲ-ეჲბპჲჟრთრვ ნა 0,6 თ 0,5 BLEU რჲფკთ ნა ოჲჟლვენთრვ დვპმანუთ! ანდლთჟკთ თ ანდლთჟკთ! გერმანული სისტემები. ჩვენ ასევე დავინახვეთ უფრო მეტადება, რომლებიც POS-ის ჭდეები და სიტყვების კლასტერების გამოყენება ამ მოდელების გასწავლად. რადგან ჩვენ ორიენგური კორპუსს შეცვალოთ, რომ რეორდირება პერაციების ინტერგურაციას, ეს მოდის ჩვენ შეგვიძლია შეგვიძლია შეგვიძლია შეგვიძლია შეგვიძლია შეგვიძლია შეგვიძლია შეგვი ჩვენი მოტივაცია იყო, რომ პირად შეუძლებელია ინფორმაციის რეორდირება ინფორმაციას რეკოდერების ფრამეტში, რომელიც სხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვა მხოლოდ ჩვენ შევცდილოთ ორივე საკუთარი და საკუთარი საკუთარი საკუთარი რეორდირება. მაგრამ, ეს ექსპერიმენტები არ მივიღეთ ნეიროლური MT სისტემის შესაძლებლობა.', 'it': "I modelli di sequenza bilingue migliorano la traduzione e il riordino basati sulle frasi superando l'assunzione di indipendenza frasale e gestendo il riordino a lungo raggio. Tuttavia, a causa della scarsità dei dati, questi modelli spesso ricadono a dimensioni di contesto molto piccole. Questo problema è stato precedentemente affrontato imparando sequenze su rappresentazioni generalizzate come tag POS o cluster di parole. In questo articolo, esploriamo un'alternativa basata su modelli di rete neurale. Più concretamente addestriamo versioni neuralizzate del riordino lessicalizzato [1] e dei modelli di sequenza operativa [2] utilizzando la rete neurale feed-forward. I nostri risultati mostrano miglioramenti fino a 0,6 e 0,5 punti BLEU in cima alla base tedesca! Inglese e inglese! Sistemi tedeschi. Abbiamo anche osservato miglioramenti rispetto ai sistemi che hanno utilizzato tag POS e cluster di parole per addestrare questi modelli. Poiché modifichiamo il corpo bilingue per integrare le operazioni di riordino, questo ci permette anche di addestrare un modello MT neurale sequenza-sequenza con trigger di riordino esplicito. La nostra motivazione era quella di consentire direttamente il riordino delle informazioni nel framework encoder-decoder, che altrimenti si basa esclusivamente sul modello di attenzione per gestire il riordino a lungo raggio. Abbiamo provato sia operazioni di riordino grossolano che a grana fine. Tuttavia, questi esperimenti non hanno prodotto alcun miglioramento rispetto ai sistemi di MT neurale basale.", 'lt': 'Bilingual sequence models improve phrase-based translation and reordering by overcoming phrasal independence assumption and handling long range reordering.  Tačiau dėl duomenų trūkumo šie modeliai dažnai būna labai maži. This problem has been previously addressed by learning sequences over generalized representations such as POS tags or word clusters.  Šiame dokumente mes tiriame alternatyvą, pagrįstą nervinių tinklų modeliais. Konkrečiau mes mokome neuralizuotas lexikalizuoto reorganizavimo versijas [1] ir eksploatavimo sekos modelius [2] naudojant priekinį nervinį tinklą. Mūsų rezultatai rodo pagerėjimą iki 0,6 ir 0,5 BLEU taško virš pradinės vokiečių! Anglų ir anglų! Vokietijos sistemos. Taip pat stebėjome patobulinimus, palyginti su sistemomis, kurios naudojo POS ženklus ir žodžių klasterius šiems modeliams treniruoti. Kadangi mes modifikuojame dvikalbį korpusą, kad integruotume pertvarkymo operacijas, tai taip pat leidžia mums treniruoti iš eilės į eilę neuralinį MT model į, turintį aiškius pertvarkymo įjungiklius. Mūsų motyvacija buvo tiesiogiai suteikti galimybę pertvarkyti informaciją kodavimo kodavimo sistemoje, kuri kitaip būtų grindžiama tik dėmesio modeliu, kuriuo būtų galima tvarkyti ilgalaikį pertvarkymą. Mes bandėme ir griežtesnių, ir smulkių grūdų pertvarkymo operacijų. Tačiau šie eksperimentai nepagerino pradinių neurologinių MT sistemų.', 'mk': 'Дивјазичните модели на секвенца го подобруваат преводот на фрази и реорганизацијата преку надминуваое на претпоставуваоето на независност на фразите и справуваоето со реорганизацијата на долг домет. Сепак, поради скратноста на податоците, овие модели често се враќаат на многу мали контекстни големини. Овој проблем претходно беше решен со учење секвенции во врска со генерализираните претставувања како што се POS ознаки или зборни групи. Во овој весник истражуваме алтернатива базирана на модели на нервната мрежа. Поконкретно, ги обучуваме неурализираните верзии на лексикализираното реорганизирање [1] и моделите на оперативната секвенца [2] со употреба на feed-forward неурална мрежа. Нашите резултати покажуваат подобрувања од 0,6 и 0,5 БЛЕУ поени врз основната германска точка! Англиски и англиски! German systems.  Ние, исто така, забележавме подобрувања во споредба со системите кои користеа POS ознаки и зборни групи за обука на овие модели. Бидејќи го модификуваме двојјазичниот корпус за да ги интегрираме операциите за реорганизација, ова ни овозможува и да тренираме секвенца-на-секвенца нервен модел MT со експлицитни реорганизациски активатори. Нашата мотивација беше директно да овозможиме реорганизација на информациите во рамката за кодер-декодирање, која инаку се потпира само на моделот на внимание за справување со реорганизацијата на долг домет. Се обидовме да ги реорганизираме операциите. Сепак, овие експерименти не донесоа никакви подобрувања во однос на основниот неурален МТ систем.', 'kk': 'Екі реттеу үлгілері сөздерді негіздеген аудару және қайта реттеу арқылы сөздердің тәуелсіздігін қайта реттеу және ұзын аумақтарды қайта реттеу арқылы. Бірақ, деректердің кеңістігінің себебі, бұл үлгілер көбінесе өте кішкентай контекстің өлшеміне қайтарылады. Бұл мәселе алдында POS тегтері не сөз кластері секілді жалпы мәліметтерден өзгертілген кезектерді үйрену арқылы шешілді. Бұл қағазда, невралдық желі моделдеріне негізделген альтернативті зерттейміз. Біз лексикализациялық қайта реттеу [1] және операциялық реттеу үлгілерін [2] көмегімен невралдық желінің неврализациялық нұсқаларын үйренеміз. Біздің нәтижелеріміз 0,6 мен 0,5 BLEU нәтижелерін негізгі неміс тілдеріне жақсарту көрсетеді! Ағылшын және ағылшын тілінде! Неміс жүйелері. Сонымен қатар, POS тегтерін және сөз кластерін қолданатын жүйелердің жақсартуларын көрдік. Екі тілді корпусты қайта реттеу операцияларын біріктіру үшін өзгертеміз, сондай-ақ біз реттеу ретінде невралдық MT үлгісін түсіндіруге мүмкіндік береміз. Біздің мотивациямыз, кодер декодерлердің қоршауындағы мәліметті қайта реттеу үшін, әйтпесе ол толық ұзын аумақтарды қайта реттеу үшін қайта реттеу үшін тәуелді. Біз жұлдыз және жақсы зерттеу операцияларын тексердік. Бірақ бұл тәжірибелер негізгі невралдық MT жүйелерінде бір жақсарту жоқ.', 'ms': 'Model urutan berbincang memperbaiki terjemahan berdasarkan frasa dan penyesuaian semula dengan mengatasi asumsi kemerdekaan frasa dan mengendalikan penyesuaian julat panjang. Namun, disebabkan kecepatan data, model ini sering jatuh ke saiz konteks yang sangat kecil. Masalah ini telah diselesaikan sebelumnya oleh urutan belajar atas perwakilan umum seperti tag POS atau kumpulan perkataan. Dalam kertas ini, kita mengeksplorasi alternatif berdasarkan model rangkaian saraf. More concretely we train neuralized versions of lexicalized reordering [1] and the operation sequence models [2] using feed-forward neural network.  Our results show improvements of up to 0.6 and 0.5 BLEU points on top of the baseline German! Inggeris dan Inggeris! Sistem Jerman. Kami juga memperhatikan peningkatan dibandingkan dengan sistem yang menggunakan tag POS dan kumpulan perkataan untuk melatih model ini. Kerana kita mengubahsuai korpus bilingual untuk mengintegrasikan operasi penyesuaian semula, ini juga membolehkan kita melatih model MT saraf berturut-turut mempunyai pemicu penyesuaian semula secara eksplicit. Motif kami adalah untuk memungkinkan secara langsung penyesuaian maklumat dalam kerangka penyesuaian-penyesuaian, yang sebaliknya hanya bergantung pada model perhatian untuk mengendalikan penyesuaian jangkauan panjang. Kami cuba kedua-dua operasi penyesuaian yang lebih kasar dan baik-baik. Namun, eksperimen ini tidak memberikan sebarang peningkatan atas sistem MT Neural asas.', 'ml': 'ബൈലിങ്കുള്ള സെക്കന്\u200dസ് മോഡലുകള്\u200d വാക്കിന്\u200dറെ അടിസ്ഥാനമായ വിഭാഷത്തിലുള്ള പരിഭാഷകള്\u200d മെച്ചപ്പെടുത്തുകയും, വാക്കുകളുടെ സ് എന്നാലും, ഡേറ്റാ സ്പെയിസിറ്റിയില്\u200d നിന്നും കാരണം ഈ മാതൃകങ്ങള്\u200d എപ്പോഴും വളരെ ചെറിയ കെസ്റ്റെക്സ്റ പോസ് ടാഗ്സോ വാക്ക് ക്ലാസ്റ്റര്\u200d എന്ന പ്രതിനിധികളില്\u200d പഠിക്കുന്ന സെക്കന്\u200dസുകള്\u200d പഠിക്കുന്നതിനാല്\u200d മുമ്പ് ഈ ഈ പത്രത്തില്\u200d, നമ്മള്\u200d ന്യൂറല്\u200d നെറ്റൂറല്\u200d നെറ്റോവര്\u200dക്ക് മോഡലുകളില്\u200d അടിസ്ഥാനമായി മറ്റൊരു മാറ്റം  ലെക്സിക്സിക്കല്\u200d വീണ്ടും ഉത്തരവ് ചെയ്യുന്നതിന്റെ ന്യൂറലിഷ് പതിപ്പുകള്\u200d നാം കൂടുതല്\u200d പരിശീലിപ്പിക്കുന്നു. [1] പിന്നെ പ്രവര്\u200dത്തന നമ്മുടെ ഫലങ്ങള്\u200d 0. 6 വരെയും 0. 5 ബിലിയു പോയിന്\u200dറുകള്\u200d മുകളില്\u200d ജര്\u200dമ്മന്\u200dകാരുടെ മുന്\u200dഗണനം കാണിക്കുന്നു! ഇംഗ്ലീഷും ഇംഗ്ലീഷും! ജര്\u200dമ്മന്\u200d സിസ്റ്റമുകള്\u200d. പോസ് ടാഗും വാക്ക് ക്ലാസ്റ്റരും ഉപയോഗിക്കുന്ന സിസ്റ്റമുള്ള മെച്ചപ്പെട്ട മുന്\u200dഗണങ്ങള്\u200d ഞങ്ങള്\u200d കണ്ടുപിട കാരണം നമ്മള്\u200d രണ്ടു ഭാഷ കോര്\u200dപ്പാസിനെ മാറ്റിവെക്കുന്നു. വീണ്ടും നിര്\u200dദ്ദേശിക്കുന്ന പ്രവര്\u200dത്തനങ്ങള്\u200d ഒന്നിച്ച് ചേര്\u200dക്കാന്\u200d ഇത് നമുക്ക് അനുവദിക നമ്മുടെ ഉദ്ദേശം നേരിട്ട് കോഡോര്\u200d ഫ്രെയിമ്പില്\u200d വിവരങ്ങള്\u200d വീണ്ടും ഉത്തരവാദിപ്പിക്കുന്നത് പ്രാവര്\u200dത്തികമാക്കുക എന്നതായിരുന് ഞങ്ങള്\u200d കോഴ്സെരും നല്ല കൈയ്യില്\u200d പിന്നീട് ഓപ്പറേഷനും ശ്രമിച്ചു. എന്നാലും ഈ പരീക്ഷണങ്ങള്\u200d നെയുറല്\u200d എംടി സിസ്റ്റത്തില്\u200d മെച്ചപ്പെടുത്തിയിട്ടില്ല.', 'pl': 'Dwujęzyczne modele sekwencji usprawniają tłumaczenie oparte na frazach i reorganizację poprzez przezwyciężenie założenia niezależności fraz i obsługę reorganizacji dalekiego zasięgu. Jednak ze względu na niewielką ilość danych modele te często powracają do bardzo małych rozmiarów kontekstu. Problem ten został wcześniej rozwiązany przez sekwencje uczenia się nad uogólnionymi reprezentacjami, takimi jak tagi POS lub klastry słów. W niniejszym artykule badamy alternatywę opartą na modelach sieci neuronowych. Bardziej konkretnie trenujemy neuralizowane wersje leksykalizowanego reorganizacji [1] oraz modele sekwencji operacyjnych [2] przy użyciu feed-forward sieci neuronowej. Nasze wyniki pokazują poprawę punktów BLEU do 0,6 i 0,5 nad początkowym niemieckim! Angielski i angielski! Niemieckie systemy. Zaobserwowaliśmy również ulepszenia w porównaniu z systemami, które wykorzystywały tagi POS i klastry słów do treningu tych modeli. Ponieważ modyfikujemy dwujęzyczny korpus, aby zintegrować operacje zmiany porządku, pozwala nam to również trenować neuronowy model MT posiadający wyraźne wyzwalacze zmiany porządku. Naszą motywacją było bezpośrednie umożliwienie zmiany kolejności informacji w ramach kodera-dekodera, który w przeciwnym razie opiera się wyłącznie na modelu uwagi do obsługi zmian zasięgu. Próbowaliśmy zarówno grubszych, jak i drobnoziarnistych operacji zmiany zamówienia. Jednak eksperymenty te nie przyniosły żadnej poprawy w stosunku do wyjściowych układów neuronowych MT.', 'mn': 'Хоёр дахь дарааллын загварууд хэлэлцээний үндсэн орчуулалт болон дахин эргүүлэхээр хэлэлцээний тусгаарлалт болон урт хэмжээний эргүүлэлтийг удирдах боломжтой болгодог. Гэхдээ эдгээр загварууд маш жижиг байдлын хэмжээнд буцаж ирдэг. Энэ асуудал өмнө нь POS тэмдэг эсвэл үгний кластер зэрэг ерөнхийлөгч үзүүлэлтийн дарааллаар суралцаж байсан. Энэ цаасан дээр бид мэдрэлийн сүлжээний загварын үндсэн өөрчлөлтийг судалж байна. Бид илүү тодорхой хэлбэртэй хэлбэрээр сэтгэл хөдлөл, сэтгэл хөдлөлийн мэдрэлийн сүлжээг ашиглан сэтгэл хөдлөл болон үйл ажиллагааны дарааллын загваруудыг сургадаг. Манай үр дүнд 0.6 болон 0.5 BLEU цэгүүдийг Германы суурь шугам дээр гаргаж байна! Англи, Англи хэл! Германы систем. Бид үүнийг загваруудыг суралцахын тулд POS тэмдэгтийг, үгний кластеруудыг ашигладаг системүүдтэй харьцуулсан сайжруулалтыг ажигласан. Яагаад гэвэл бид хоёр хэлний корпус дахин дахин эргүүлэх үйл ажиллагааг нэгтгэхэд өөрчлөх боломжтой болно. Энэ нь бидэнд тодорхой дахин эргүүлэх тархины MT загварын дарааллаар дахин дарааллаар сургаж өгдө Бидний урам зориулалт нь коддогч хэлбэрээр мэдээллийг шууд дахин зохицуулах боломжтой байлаа. Эсвэл энэ нь зөвхөн урт хэлбэрээр дахин зохицуулах анхаарлын загвараас хамаарна. Бид хоёулаа дахин дахин дахин эргүүлэх үйл ажиллагааг хичээсэн. Гэвч эдгээр туршилтууд үндсэн мэдрэлийн MT системийн хувьд ямар ч сайжруулалт гаргадаггүй.', 'no': 'Bilinguelle sekvensmodeller forbetra oversettelse og reorderering ved å overvåka frasaluavhengighetsassumpsjon og handtera lang område. Men på grunn av datasparsitet fallar desse modelane ofte tilbake til svært lite kontekststorleik. Dette problemet er førre adressert ved læring av sekvensar over generelle representasjonar som POS- taggar eller ordklassar. I denne papiret utforskar vi eit alternativ basert på neuralnettverksmodeller. Meir konkrett treng vi neuraliserte versjonar av rekorderinga [1] og operasjonsrekkjefølgjande modeller [2] ved hjelp av kjeldeframført neuralnettverk. Resultatet våre viser forbedringar til 0,6 og 0,5 BLEU-punkt på toppen av baseline tysk! Engelsk og engelsk! Tysk systemer. Vi har også observert forbedringar samanlikna med systemet som bruka POS-taggar og ordklassar for å trena desse modelane. Fordi vi endrar den bilinguelte korpusen for å integrere rekorderingsoperasjonar, kan dette tillèt oss også trenga ein sekvens-til-sekvens neurale MT-modell som har eksplisitt rekorderingsutløysar. motivasjonen vårt var å slå på rekorderinga av informasjon i koderingsrammeverket, som ellers er bare på oppmerksmodellen for å handtera lang rekorderingsområde. Vi prøvde både område og fjerne rekorderingsoperasjonar. Desse eksperimentene gjeve imidlertid ikkje nokon forbedringar over grunnlinjesystemet for neurale MT.', 'ro': 'Modelele de secvență bilingvă îmbunătățesc traducerea și reordonarea bazată pe fraze, depășind presupunerea independenței frazale și gestionând reordonarea pe termen lung. Cu toate acestea, datorită lipsei datelor, aceste modele se întorc adesea la dimensiuni foarte mici ale contextului. Această problemă a fost abordată anterior prin învățarea secvențelor peste reprezentări generalizate, cum ar fi etichetele POS sau clusterele de cuvinte. În această lucrare, explorăm o alternativă bazată pe modele de rețea neurală. Mai concret, instruim versiuni neuralizate de reordonare lexicalizată [1] și modele de secvență operațională [2] folosind rețeaua neurală feed-forward. Rezultatele noastre arată îmbunătățiri de până la 0,6 și 0,5 puncte BLEU peste nivelul de referință german! Engleză şi engleză! Sisteme germane. De asemenea, am observat îmbunătățiri în comparație cu sistemele care au folosit etichete POS și clustere de cuvinte pentru a instrui aceste modele. Deoarece modificăm corpul bilingv pentru a integra operațiunile de reordonare, acest lucru ne permite, de asemenea, să antrenăm un model MT neural secvență-la-secvență având declanșatori expliciți de reordonare. Motivația noastră a fost de a permite în mod direct reordonarea informațiilor în cadrul encoder-decoder, care altfel se bazează exclusiv pe modelul atenției pentru a gestiona reordonarea pe rază lungă. Am încercat atât operațiuni de reordonare grosieră, cât și cu granule fine. Cu toate acestea, aceste experimente nu au produs îmbunătăţiri faţă de sistemele MT neurale iniţiale.', 'so': 'Tusaalada xiliga luqada bilowga ah ayaa bedela tarjumaadka afka lagu qoray, wuxuuna ku bedelaa mid ku xadgudba malaynta xorriyadda ah iyo xuquuqda dib u dhigidda. Si kastaba ha ahaatee, tusaalahan waxey inta badan ku noqonayaan qiyaastii aad u yar. Dhibaatadan horay waxaa loola macaamilooday barashada xiliga barbaarinta, tusaale ahaan calaamadaha POS ama xarumaha hadalka. Qoraalkan waxaynu ka baaraannaa mid kale oo ku saleysan qaababka shabakadda neurada. Wixii kaloo xilli ah ayaannu ku tababarinnaa habab neural ah oo lexicalized reordering[1] iyo modellada qaabka shaqada [2] isticmaalaya shabakadda midhaha-forward neurada. Abaalkayaga waxaa ka muuqda horumarinta ilaa 0.6 iyo 0.5 barta BLEU oo ku qoran Jarmalka hoose! Ingiriis iyo Ingiriis! nidaamka Jarmalka Waxaan kaloo aragnay horumarinta oo la baro nidaamka isticmaalaya alaabta POS iyo qoraalka si aan u tababarinno modelladan. Maxaa yeelay waxaynu beddeli karnaa qalabka labada luqadood si aan u qabno hawlaha hagitaanka, taasuna waxay inagu fasaxaa inaannu tababarinno muusikada neurada ee MT-ka xilliga ah oo ay leedahay hagitaanka si cad. Dhaqdhaqaalkayagu waa in si toos ah u dalban karo macluumaad cusboonaysiinta shirkadda codsiga, taas oo si kale mooyaane waxay ku xiran tahay modelka கவriska si ay u xambaaraan hagitaanka waqtiga dheer. Waxaannu isku daynay koorsooyin iyo howlaha hagitaanka oo wanaagsan. Si kastaba ha ahaatee imtixaankaasi ma sababin hagaajinta nidaamka MT ee asalka ah.', 'sr': 'Modeli dvostruke sekvence poboljšavaju prevod i reordinaciju na frazu, prevladavajući pretpostavku nezavisnosti frazala i reagiranje dugog raspona. Međutim, zbog rezerviteta podataka, ovi modeli često se vraćaju na veoma male veličine konteksta. Ovaj problem je ranije rešen učenjskim sekvencijama iznad generaliziranih predstavljanja kao što su POS etikete ili skupine riječi. U ovom papiru istražujemo alternativu baziranu na modelima neuralne mreže. Još konkretnije treniramo neuralizovane verzije leksikaliziranog reorderinga [1] i modele sekvence operacije [2] koristeći neuralnu mrežu napredne hrane. Naši rezultati pokazuju poboljšanje do 0,6 i 0,5 BLEU bodova na vrhu početne nemačke linije! Engleski i engleski! Njemaèki sistemi. Takoðe smo posmatrali poboljšanja u usporedbi sa sistemima koji su koristili oznake POS-a i grupe reèi za obuku ovih modela. Zato što izmijenimo dvojezički korpus da integrišemo reordering operacije, to nam omogućava da treniramo i neuralni MT model koji ima eksplicitne reordering okidače. Naša motivacija je bila da se direktno omogućimo reordinaciju informacija u okviru kodera-dekodera, što inače oslanja samo na pažnju modela da se rešimo reordinaciji dugog dometa. Pokušali smo i kapetane i fino zrno reorderiranje operacija. Međutim, ovi eksperimenti nisu pružili nikakve poboljšanje u osnovnim nervnim MT sistemima.', 'si': 'Bilingual sequence Models improvise Phrase-based translation and re-rdering by overaching Phrase Freedom assumption and handling long ranges re-rdering. නමුත්, දත්ත සැකසුම් නිසා, මේ මොඩේල් සාමාන්\u200dය පොඩි සංවේදනයේ ප්\u200dරමාණයකට ආපහු වෙනවා. මේ ප්\u200dරශ්නය කලින් ප්\u200dරශ්නයක් ප්\u200dරශ්නය කරලා තියෙන්නේ ප්\u200dරශ්නයක් සඳහා සාමාන්\u200dය ප්\u200dරශ්නයක් වගේ POS ටැ මේ පත්තරේ අපි වෙනස් විදිහක් පරීක්ෂා කරනවා න්\u200dයූරල් ජාලයේ මොඩේල් එක්ක. අපි ප්\u200dරශ්නයක් වෙනුවෙන් ප්\u200dරශ්නය කරනවා ලෙක්සිකල් කරපු ප්\u200dරශ්නය [1] සහ ප්\u200dරශ්නය ක්\u200dරමාණ පරීක්ෂණ පරීක්ෂණ පර අපේ ප්\u200dරතිචාර පරීක්ෂණය පෙන්වන්නේ 0.6 සහ 0.5 BLUE ප්\u200dරතිචාර පරීක්ෂණය ජර්මන් වල ඉහළට! ඉංග්\u200dරීසි සහ ඉංග්\u200dරීසි! ජර්මන් පද්ධතිය. අපි පද්ධතියේ පොස් ටැග් සහ වචන ක්ලොස්ටර් වලට ප්\u200dරයෝජනය කරන්න ප්\u200dරවෘත්තිය සඳහා ප්\u200dරවෘත්තිය ප්\u200dරවේශන මොකද අපි දෙවල් භාෂාවක් කොර්පස් වෙනස් කරනවා ආපහු ක්\u200dරියාලය සම්බන්ධ කරගන්න, මේක අපිට පුළුවන් වෙන්න පුළුවන් වෙන්න පුළ අපේ ප්\u200dරතික්\u200dරියාව තමයි කෝඩාර්-ඩිකෝඩර් වර්ගයේ තොරතුරු ප්\u200dරතික්\u200dරමාණය සක්රිය කරන්න, ඒක නැත්තම් ප්\u200dරතික්\u200dරියාවක්  අපි කෝර්සර් සහ හොඳ ග්\u200dරේන්ඩ් නිර්මාණය කරන්න උත්සාහ කළා. නමුත්, මේ පරීක්ෂණාවල් මූලික MT පද්ධතියේ කිසිම ප්\u200dරවේශනයක් නැති වුනා.', 'sv': 'Tvåspråkiga sekvensmodeller förbättrar frasbaserad översättning och omordning genom att övervinna frasoberoende antagande och hantera långdistansomordning. Men på grund av datagleshet faller dessa modeller ofta tillbaka till mycket små sammanhangsstorlekar. Detta problem har tidigare åtgärdats genom att lära sekvenser över generaliserade representationer som POS-taggar eller ordkluster. I denna uppsats undersöker vi ett alternativ baserat på neurala nätverksmodeller. Mer konkret tränar vi neuraliserade versioner av lexikaliserad omordning [1] och operationssekvensmodeller [2] med hjälp av feed-forward neuralt nätverk. Våra resultat visar förbättringar på upp till 0,6 och 0,5 BLEU poäng utöver baslinjen tyska! Engelska och engelska! Tyska system. Vi observerade också förbättringar jämfört med de system som använde POS-taggar och ordkluster för att träna dessa modeller. Eftersom vi modifierar tvåspråkig korpus för att integrera omordning operationer, tillåter detta oss också att träna en sekvens-till-sekvens neural MT modell med explicita omordning triggers. Vår motivation var att direkt möjliggöra omordning av information i encoder-dekoder ramverket, som annars enbart förlitar sig på uppmärksamhetsmodellen för att hantera omordning med lång räckvidd. Vi provade både grovare och finkorniga omorganisationer. Dessa experiment gav dock inga förbättringar jämfört med baslinjen Neural MT system.', 'mt': 'Il-mudelli tas-sekwenza bilinguali jtejbu t-traduzzjoni u r-riorganizzazzjoni bbażati fuq il-frażi billi jegħlbu s-suppożizzjoni tal-indipendenza tal-frażi u jimmaniġġjaw ir-riorganizzazzjoni fuq medda twila. Madankollu, minħabba l-iskarsezza tad-dejta, dawn il-mudelli spiss jaqgħu lura għal daqsijiet ta’ kuntest żgħar ħafna. Din il-problema kienet indirizzata qabel permezz ta’ sekwenzi ta’ tagħlim fuq rappreżentazzjonijiet ġeneralizzati bħal tikketti POS jew gruppi ta’ kliem. F’dan id-dokument, nistudjaw alternattiva bbażata fuq mudelli tan-netwerk newrali. B’mod aktar konkret aħna nħarrġu verżjonijiet newralizzati tar-riorganizzazzjoni lexikalizzata [1] u l-mudelli tas-sekwenza tal-operat [2] bl-użu tan-netwerk newrali feed-forward. Ir-riżultati tagħna juru titjib sa 0.6 u 0.5 punti BLEU fuq il-linja bażi Ġermaniża! Ingliż u Ingliż! Sistemi Ġermaniżi. Osservajna wkoll titjib meta mqabbel mas-sistemi li użaw it-tikketti POS u l-gruppi tal-kliem biex jitħarrġu dawn il-mudelli. Minħabba li mmodifikaw il-korpus bilingwi biex jintegraw l-operazzjonijiet ta’ riorganizzazzjoni, dan jippermettilna wkoll li nħarrġu mudell ta’ MT newrali minn sekwenza għal sekwenza b’attivaturi ta’ riorganizzazzjoni espliċiti. Il-motivazzjoni tagħna kienet li tippermetti direttament ir-riorganizzazzjoni tal-informazzjoni fil-qafas tal-kodifikatur-dekoder, li inkella tiddependi biss fuq il-mudell ta’ attenzjoni biex tittratta r-riorganizzazzjoni fuq medda twila. Aħna ppruvajna kemm operazzjonijiet ta’ riorganizzazzjoni aktar grossi kif ukoll dawk fini. Madankollu, dawn l-esperimenti ma taw l-ebda titjib fuq is-sistemi ta’ MT newrali fil-linja bażi.', 'ta': 'Bilingual sequence models improve phrase- based translation and re order by overcoming phrase independence assumption and handling long range reordering. ஆனால், இந்த மாதிரிகள் மிகவும் சிறிய சூழல் அளவுகளுக்கு திரும்பி வருகிறது. இந்த பிரச்சினை முன்பு போஸ் குறிகள் அல்லது வார்த்தை குறிப்புகள் போன்ற பொதுவாக்கிய குறிப்புகளின் மேல் கற்றல்  இந்த காகிதத்தில், நாம் புதிய வலைப்பின்னல் மாதிரிகளை அடிப்படையில் ஒரு மாற்று தேடுகிறோம். மேலும் குறிப்பாக நாம் லெக்சிக்சியமைக்கப்பட்ட புதிய பதிப்புகளை பயிற்சி செய்கிறோம் [1] மற்றும் செயல்பாடு தொடர்ந்து மாதிரிகள எங்கள் முடிவுகள் 0. 6 மற்றும் 0. 5 பிலியு புள்ளிகள் மேல் ஜெர்மன் மேல் முன்னேற்றத்தை காட்டுகிறது! ஆங்கிலம் மற்றும் ஆங்கிலம்! ஜெர்மன் அமைப்புகள். இந்த மாதிரிகளை பயிற்சி செய்யும் போஸ் குறிகள் மற்றும் வார்த்தை குறிப்புகளை பயன்படுத்தி முன்னேற்றங்களை ஒ ஏனென்றால் நாம் இரண்டு மொழி குறுக்குகளை மாற்றி மறுவரிசையில் சேர்க்கும் செயல்களை ஒன்றாக்க அனுமதிக்கிறது, இது நமக்கும் தொடர்ச்சில் இருந்து வரும் பி எங்கள் ஊக்கத்தை நேரடியாக மறுவரிசையாக்கும் சட்டத்தில் தகவலை மீண்டும் செயல்படுத்துவதற்காக செயல்படுத்தும், இல்லை நாங்கள் தொடர் மற்றும் நல்ல பிடிக்கப்பட்ட மீண்டும் கட்டளை செயல்களை முயற்சித்தோம். However, these experiments did not yield any improvements over the baseline Neural MT systems.', 'ur': 'بیلینگ سیٹرینگ موڈل فریزوں کی بنیادی ترجمہ اور دوبارہ ترجمہ کرنے کے ذریعہ مزید ترجمہ کریں اور دوبارہ ترجمہ کریں اگرچہ، ڈاٹا سٹیٹی کے باعث، یہ موڈل اکثر بہت چھوٹے کنٹنسیٹ کی سازوں میں واپس آتے ہیں۔ یہ مسئلہ پہلے کے ذریعہ پوس ٹاگ یا کلسٹر کے ذریعہ سیکھنے کے ذریعہ مشکل کی گئی ہے۔ اس کاغذ میں ہم ایک الٹ بیٹر نیورل نیٹ ورک موڈل پر بنیاد رکھتے ہیں۔ ہم اس سے زیادہ مضبوط طریقہ سے لکسیکلیز ریئورڈینگ کی نیورلیز نسخہ کی تربیت کرتے ہیں [1] اور عملکرد ریئورڈ نمڈل [2] کو فیڈ-فورڈ نیورل نیورل نیٹ ورک کے استعمال سے استعمال کرتے ہیں. ہمارے نتیجے 0.6 اور 0.5 BLEU پوینٹ کے اوپر سی لین جرمن کے اوپر بہترین ترکیب دکھاتے ہیں! انگلیسی اور انگلیسی! جرمن سیسٹم. ہم نے بھی ان سیستموں کے مقابلہ میں بہترین تربیت دیکھی جو POS ٹاگ اور کلسٹر کو ان مدلوں کی تعلیم کے لئے استعمال کرتے تھے۔ کیونکہ ہم دونوں زبان کی کورپوس کو تغییر دیتے ہیں کہ دوبارہ اورڈرینگ اپراتوریوں کو تغییر کریں، یہ ہمیں بھی ایک سطح سے اورڈرینگ مٹی موڈل کی تعلیم دیتا ہے جس میں کھول کھول کر دوبارہ اورڈرینگ ٹریگر ہے۔ ہماری رغبت یہ تھی کہ کوڈر ڈیکوڈر فرمیک میں معلومات کو مستقیماً دوبارہ ترکیب کرنے کی امکانات کریں، جس کے علاوہ صرف طویل مدینہ دوبارہ ترکیب کرنے کے لئے توجه کی مدینل پر اعتماد کرتا ہے. اور ہم نے آزمائش کی تھی کہ ان دونوں کو آزمائش کریں However, these experiments did not produce any improvement over the baseline Neural MT systems.', 'uz': "Name Lekin, bu modellar odatda juda kichkina tarkib sizlarga qaytadi. Name Bu qogʻozda, biz neyrol tarmoq modellari asosida boshqa boshqa narsa topamiz. @ info Bizning natijalarimiz 0.6 va 0.5 BLEU asosiy Olmonchaning yuqorida o'zgarishni ko'rsatadi! Inglizcha va inglizcha! Olmon tizimlari. Biz bu modellarni o'rganish uchun POS taglari va so'zlar qo'llangan tizimlar bilan o'zgarishni ko'rsatdik. Chunki biz ikkita tillar corpusini qayta tartib qilish amallarini o'zgartirish imkoniyatini o'zgartirish imkoniyatini beradi. Bu bizga qayta tartib qilish muvaffaqiyatlarini bajarishimiz mumkin. Bizning tashkilotlarimiz kodlash chegarasini aniqlash uchun oddiy tartib berish mumkin edi. Aks holda, ularning faqat uzoq soʻzni qaytadan boshlash modeliga ishlatadi. Biz ko'pchilik va yaxshi ko'pchilik amallarini reorder qildik. However, these experiments did not yield any improvements over the baseline Neural MT systems.", 'vi': 'Cách chế độ lưỡng điệu cải thiện dịch từ điển thành ngữ và sắp xếp lại bằng cách vượt qua khả năng độc lập thành từ và sắp xếp lại phạm vi dài. Tuy nhiên, nhờ những dữ liệu sơ sài, các mô hình này thường quay về với kích cỡ hoàn cảnh rất nhỏ. Trước đây, vấn đề này đã được giải quyết bằng các chuỗi học tập về các biểu tượng phổ biến như thẻ bài vị hoặc cụm từ. Trong tờ giấy này, chúng tôi khám phá một phương pháp khác dựa trên các mô hình mạng thần kinh. Thực tế hơn, chúng tôi đào tạo phiên bản rối loạn hóa đơn (1) và các mô hình phân tử thao tác (2) sử dụng mạng thần kinh truyền. Kết quả của chúng tôi cho thấy có cải tiến trên đỉnh 0.6 và 0.5... (Tiếng Nga) trên đỉnh của tổng tuyến Đức! Anh và Anh! Hệ thống Đức. Chúng tôi cũng nhận thấy sự cải tiến so với hệ thống sử dụng thẻ POS và cụm từ để huấn luyện các mô hình này. Bởi vì chúng tôi sửa đổi hệ thống sóng hai dạng để bổ sung các thao tác sắp xếp lại, điều này cho phép chúng tôi cũng huấn luyện một mô hình sóng thần-- Mục đích của chúng tôi là thiết lập lại đơn giản thông tin trong bộ mã hóa, thứ mà hoàn to àn dựa vào mô hình chú ý để xử lý việc sắp xếp lại phạm vi dài. Chúng tôi đã thử những hoạt động hỗn loạn. Tuy nhiên, các thí nghiệm này không có hiệu quả cải thiện hệ thống thần kinh MTV cơ bản.', 'bg': 'Двуезичните модели на последователност подобряват превода и пренареждането на фразата чрез преодоляване на предположението за независимост на фразата и обработка на пренареждането на дълги разстояния. Въпреки това, поради оскъдността на данните, тези модели често се връщат към много малки контекстни размери. Този проблем е бил решен преди това чрез изучаване на последователности върху обобщени представи като тагове или словни клъстери. В тази статия изследваме алтернатива, базирана на модели на невронна мрежа. По-конкретно ние тренираме неврализирани версии на лексикализираното пренареждане [1] и моделите на оперативната последователност [2], използвайки невронна мрежа за подаване напред. Нашите резултати показват подобрения от до 0.6 и 0.5 точки над базовата немска! Английски и английски! Немски системи. Също така наблюдавахме подобрения в сравнение със системите, които използваха ПОС тагове и словни клъстери за обучение на тези модели. Тъй като модифицираме двуезичния корпус, за да интегрираме операциите по пренареждане, това ни позволява да тренираме и неврален модел от последователност към последователност, който има изрични спусъци за пренареждане. Мотивацията ни беше директно да позволим пренареждане на информация в рамката на кодер-декодер, която в противен случай разчита единствено на модела на внимание за справяне с пренареждането на дълги разстояния. Опитахме както груби, така и фини операции по пренареждане. Тези експерименти обаче не доведоха до подобрения спрямо изходните неврални МТ системи.', 'hr': 'Modeli dvostrukog sekvencije poboljšavaju prevod i reorderiranje na frazi prekršavajući pretpostavku nezavisnosti frazala i reagiranje dugog raspona. Međutim, zbog rezerviteta podataka, ovi modeli često se vraćaju na veoma male veličine konteksta. Ovaj problem je ranije riješen učenjem sekvencijama nad generaliziranim predstavljanjima poput oznake POS-a ili skupina riječi. U ovom papiru istražujemo alternativu baziranu na modelima neuralne mreže. Još konkretnije treniramo neuralizirane verzije leksikaliziranog reorderinga [1] i modele sekvence operacije [2] koristeći neuralnu mrežu naprijed hrane. Naši rezultati pokazuju poboljšanje do 0,6 i 0,5 BLEU bodova na vrhu početnog njemačkog! Engleski i engleski! Njemački sustavi. Također smo primijetili poboljšanje u usporedbi s sustavima koji su koristili oznake i skupine riječi za obuku ovih modela. Zato što izmijenimo dvojezički korpus da bi integrirali reordering operacije, to nam omogućava da obučimo i sekvencijski neuronski model MT-a koji ima jasno reordering okidač. Naša motivacija je bila da se direktno omogućimo reorderiranje informacija u okviru kodera-dekodera, što inače oslanja samo na model pažnje da se riješi reorderiranje dugog dometa. Pokušali smo i preuređivanje i preuređivanje. Međutim, ovi eksperimenti nisu pružili nikakve poboljšanje u osnovnim nervnim MT-ovim sustavima.', 'da': 'To-sprogede sekvensmodeller forbedrer sætningsbaseret oversættelse og omorrangering ved at overvinde frasal uafhængighed antagelse og håndtere lang rækkefølge omorrangering. Men på grund af datasparthed falder disse modeller ofte tilbage til meget små kontekststørrelser. Dette problem er tidligere blevet løst ved at lære sekvenser over generaliserede repræsentationer såsom POS tags eller ordklynger. I denne artikel undersøger vi et alternativ baseret på neurale netværksmodeller. Mere konkret træner vi neuraliserede versioner af leksikaliseret omorrangering [1] og operationssekvensmodellerne [2] ved hjælp af feed-forward neurale netværk. Vores resultater viser forbedringer på op til 0,6 og 0,5 BLEU point oven på baseline tysk! Engelsk og engelsk! Tyske systemer. Vi observerede også forbedringer i forhold til de systemer, der brugte POS tags og ordklynger til at træne disse modeller. Fordi vi ændrer det tosprogede korpus til at integrere omorrangering operationer, giver dette os også mulighed for at træne en sekvens-til-sekvens neural MT model med eksplicitte omorrangering triggers. Vores motivation var direkte at muliggøre omlægning af oplysninger i encoder-dekoder framework, som ellers udelukkende er afhængig af opmærksomhedsmodellen til at håndtere omlægning af lang rækkevidde. Vi prøvede både grovere og finkornede omlægningsoperationer. Disse forsøg gav imidlertid ingen forbedringer i forhold til baseline Neural MT systemer.', 'nl': 'Tweetalige sequentiemodellen verbeteren frase-based vertaling en herschikking door het overwinnen van frase onafhankelijkheid veronderstelling en afhandelen van herschikking op lange afstand. Vanwege de schaarse data vallen deze modellen echter vaak terug op zeer kleine contextgroottes. Dit probleem is eerder aangepakt door leersequenties over algemene representaties zoals POS-tags of woordclusters. In dit artikel onderzoeken we een alternatief gebaseerd op neurale netwerkmodellen. Concreet trainen we neuraliseerde versies van lexicaliseerde herschikking [1] en de operatiesequentiemodellen [2] met behulp van feed-forward neural network. Onze resultaten tonen verbeteringen aan van tot 0,6 en 0,5 BLEU punten boven de basisDuitse! Engels en Engels! Duitse systemen. We zagen ook verbeteringen in vergelijking met de systemen die POS tags en woordclusters gebruikten om deze modellen te trainen. Omdat we het tweetalige corpus aanpassen om herschikkingsoperaties te integreren, kunnen we ook een sequence-to-sequence neural MT model trainen met expliciete herschikkingtriggers. Onze motivatie was om het opnieuw ordenen van informatie direct mogelijk te maken in het encoder-decoder framework, dat anders alleen afhankelijk is van het aandachtsmodel om het opnieuw ordenen van lange afstanden te verwerken. We probeerden zowel grovere als fijnkorrelige nabestellingen. Deze experimenten leverden echter geen verbeteringen op ten opzichte van de basisneuronale MT-systemen.', 'de': 'Zweisprachige Sequenzmodelle verbessern die phrasenbasierte Übersetzung und Neuordnung, indem sie die Annahme der phrasalen Unabhängigkeit überwinden und lange Entfernungen neu anordnen. Aufgrund der knappen Datenlage greifen diese Modelle jedoch häufig auf sehr kleine Kontextgrößen zurück. Dieses Problem wurde zuvor durch Lernsequenzen über generalisierte Darstellungen wie POS-Tags oder Wortcluster gelöst. In diesem Beitrag untersuchen wir eine Alternative basierend auf neuronalen Netzwerkmodellen. Konkret trainieren wir neuralisierte Versionen der lexikalisierten Neuordnung [1] und der Operationssequenzmodelle [2] mit Hilfe von feed-forward neuronalen Netzen. Unsere Ergebnisse zeigen Verbesserungen von bis zu 0,6 und 0,5 BLEU Punkten über dem Basiswert Deutsch! Englisch und Englisch! Deutsche Systeme. Wir haben auch Verbesserungen im Vergleich zu den Systemen beobachtet, die POS-Tags und Word-Cluster verwendet haben, um diese Modelle zu trainieren. Da wir den zweisprachigen Korpus modifizieren, um Reorder-Operationen zu integrieren, können wir damit auch ein sequenz-to-sequenz neuronales MT-Modell trainieren, das explizite Reorder-Trigger aufweist. Unsere Motivation war es, die Nachbestellung von Informationen direkt im Encoder-Decoder-Framework zu ermöglichen, das sonst ausschließlich auf das Aufmerksamkeitsmodell angewiesen ist, um die Nachbestellung über große Entfernungen zu handhaben. Wir haben sowohl grobere als auch feinkörnige Nachbestellungen ausprobiert. Diese Experimente ergaben jedoch keine Verbesserungen gegenüber den Basisneuronalen MT-Systemen.', 'id': 'Model urutan dua bahasa meningkatkan terjemahan berdasarkan frasa dan reorganisasi dengan mengatasi asumsi kemerdekaan frasa dan menangani reorganisasi jangkauan panjang. Namun, karena kecepatan data, model-model ini sering jatuh kembali ke ukuran konteks yang sangat kecil. Masalah ini telah diselesaikan sebelumnya oleh urutan belajar di atas representation generalisasi seperti tag POS atau kumpulan kata. In this paper, we explore an alternative based on neural network models.  Lebih konkret kita melatih versi neuralisasi dari reorganisasi lexikalisasi [1] dan model urutan operasi [2] menggunakan jaringan neural feed-forward. Hasil kami menunjukkan peningkatan sampai 0,6 dan 0,5 poin BLEU di atas dasar Jerman! Inggris dan Inggris! Sistem Jerman. Kami juga memperhatikan peningkatan dibandingkan dengan sistem yang menggunakan tag POS dan kumpulan kata untuk melatih model ini. Karena kita mengubah tubuh dua bahasa untuk mengintegrasi operasi penyesuaian ulang, ini memungkinkan kita juga melatih model MT saraf secara urutan yang memiliki pemicu penyesuaian ulang secara eksplicit. Our motivation was to directly enable reordering information in the encoder-decoder framework, which otherwise relies solely on the attention model to handle long range reordering.  Kami mencoba operasi penyesuaian yang lebih besar dan lebih baik. Namun, eksperimen ini tidak memberikan peningkatan apapun atas sistem MT Neural dasar.', 'fa': 'مدل\u200cهای ترکیب دوگانی بر اساس ترجمه\u200cهای عبارت و تغییر اندازه\u200cگیری با فرضیه\u200cهای استفاده از جمله\u200cهای دوگانه به سوی تغییر دادن فرضیه\u200cهای استفاده و کنترل تغییر اندازه طولانی بهتر می\u200cشود با این حال، به دلیل کمی داده ها، این مدلها اغلب به اندازه\u200cهای محیط کوچک برمی\u200cگردند. این مشکل قبلا توسط تعلیم\u200cهای یادگیری بر روی نمایش\u200cهای عمومی مانند نقاشی POS یا کلاس\u200cهای کلمه\u200cهای کلمه\u200cها دریافت شده است. در این کاغذ، ما یک جایگزینی بر اساس مدل شبکه عصبی تحقیق می کنیم. دقیقاً ما نسخه\u200cهای عصبی\u200cسازی از تغییر\u200cسازی لکس\u200cکالیز آموزش می\u200cکنیم [1] و مدل\u200cهای مدل\u200cهای عملیات [2] با استفاده از شبکه عصبی\u200cسازی\u200cسازی\u200cسازی\u200cسازی\u200cسازی\u200c نتیجه\u200cهامون بهترین نقطه\u200cهای 0.6 و 0.5 BLEU بر بالای خط اصلی آلمان نشان می\u200cدهند! انگليسي و انگليسي سیستم آلمانی. ما همچنین بهترین\u200cها را در مقایسه با سیستم\u200cهایی که از نقاشی\u200cهای POS و کلاس\u200cهای کلمه\u200cهای کلمه برای آموزش این مدلها استفاده می\u200cکردند دیدیم. چون ما جسد دو زبانی را تغییر می\u200cدهیم تا عملیات باز\u200cاندازی را تغییر دهیم، این به ما اجازه می\u200cدهد که یک مدل مطمئنی MT عصبی را تغییر دهیم که ماشه\u200cهای باز\u200cاندازی مشخص دارد. انگیزه ما این بود که مستقیماً برای تغییر دستور دادن اطلاعات در چهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچ ما هر دوی عملیات تغییر دهنده\u200cها و دانه\u200cها را امتحان کردیم. با این حال، این آزمایشات هیچ تغییری بر روی سیستم\u200cهای سیستم\u200cهای عصبی به وجود نداشت.', 'sw': 'Mfano wa mfululizo wa lugha huboresha tafsiri yenye msingi wa maneno na kuamuru tena kwa kushinda dhana ya uhuru wa lugha na kudhibiti amri ya upya katika ngazi ndefu. Hata hivyo, kwa sababu ya kuongezeka kwa takwimu, mifano hii mara nyingi hurudi kwenye ukubwa mdogo sana. Tatizo hili lilikuwa likizungumzwa na mfululizo wa kujifunza zaidi ya uwakilishi wa jumla kama vile alama za POS au viungo vya maneno. Katika karatasi hii, tunatafuta mabadiliko yanayotokana na mitandao ya kijamii. Kwa ujumla zaidi tunafundisha toleo la neuralised versions of reorder of lexicalized lexicologies[1] and models of upasuaji [2] using network-forward neural network. Our results show improvements of up to 0.6 and 0.5 BLEU points on top of the baseline German! Kiingereza na Kiingereza! Mfumo wa Ujerumani. Pia tuliona maendeleo yanayofanana na mfumo uliotumia alama za POS na viungo vya maneno ili kufundisha mifano haya. Kwa sababu tunabadilisha makampuni ya lugha mbili ili kuunganisha upya operesheni, hii inaruhusu pia kutufundisha modeli ya MT kwa mfululizo wa mfululizo wa mfululizo wa mfululizo wa mfululizo na kuweza kuwaamrisha wageni wa wazi. Hatua yetu ilikuwa ni kuwawezesha moja kwa moja kuamuru taarifa za upya katika mfumo wa kodi-decodi, ambapo vinginevyo hutegemea tu kwa mfano wa ufuatiliaji ili kukabiliana na amri mpya. Tulijaribu mbili za kodi na operesheni nzuri ya kuamuru tena. Hata hivyo, majaribio haya hayakuleta maendeleo yoyote katika mfumo wa MT wa msingi wa Mfumo wa Neural.', 'ko': '이중 언어 서열 모델은 단어의 독립성 가설을 극복하고 장거리 재배열 서열을 처리함으로써 단어를 바탕으로 하는 번역과 재배열을 개선한다.그러나 데이터의 희소성 때문에 이런 모델은 보통 매우 작은 상하문 크기로 퇴화된다.이 문제는 이전에 광의적 표시(예를 들어 어성 라벨이나 어족)의 서열을 배워서 해결되었다.본고에서 우리는 신경 네트워크 모델을 바탕으로 하는 대체 방안을 모색했다.더욱 구체적으로 말하면 우리는 피드백 신경 네트워크를 이용하여 어휘화 재배열 모델[1]과 조작 서열 모델[2]의 신경화 버전을 훈련한다.우리의 결과 베이스라인 독일어에 더해 BLEU 점수가 0.6점과 0.5점 높아졌다!영어랑 영어!독일 시스템.우리는 또한 이러한 모델을 훈련하기 위해 어성 라벨과 어구를 사용하는 시스템에 비해 이런 시스템이 개선되었다는 것을 관찰했다.우리는 이중 언어 자료 라이브러리를 수정하여 재배열 조작을 통합시키기 때문에, 또한 서열에서 서열까지의 신경기계 번역 모델을 훈련할 수 있다. 이 모델은 명확한 재배열 트리거를 가지고 있다.우리의 동기는 인코더 - 디코더 프레임워크에서 직접 정렬 정보를 사용합니다. 그렇지 않으면 주의 모델에 의존해서 원격 정렬을 처리합니다.우리는 굵은 입도와 가는 입도의 재배열 조작을 시도했다.그러나 기선신경기계번역시스템에 비해 이 실험들은 아무런 개선이 없었다.', 'af': "Bilinguele volgorde modele verbeter frase-gebaseerde vertaling en herordening deur oorwelding van frase-onveiligheid aanvaardig en die handling van lang omvang herordening. Maar vanweë data sparsiteit, val hierdie modele dikwels terug na baie klein konteksgrootte. Hierdie probleem is voorheen gespreek deur leer sekwensies oor generaliseerde voorstellings soos POS etikette of woord klassters. In hierdie papier, ons ondersoek 'n alternatief gebaseer op neurale netwerk modele. Meir natuurlik tref ons neuraliseerde weergawes van leksikaliseerde herordening [1] en die operasie sekwensiemodele [2] met gebruik van feed- forward neuralnetwerk. Ons resultate wys verbeteringe van tot 0,6 en 0,5 BLEU punte bo-op die basilyn Duits! Engels en Engels! Duitse stelsels. Ons het ook verbeteringe aangesien vergelyk met die stelsels wat POS etikette en woordklassters gebruik het om hierdie modele te oefen. Omdat ons die tweedelingse korpus verander om reerdering operasies te integreer, laat dit ons toe om ook 'n sekvensie-na-sekvensie neurale MT model te tref met eksplisiese reerdering triggers. Ons motivasie was om direk die herordening van inligting in die enkoder-dekoder raamwerk te aktiveer, wat anders verlig sole op die aandag model om lange omvang herordening te hanteer. Ons probeer beide koerser en fyn-graad reerdering operasies. Maar hierdie eksperimente het nie enige verbeteringe oor die basisline Neurale MT stelsels gegee nie.", 'sq': 'Modelet e sekuencës dygjuhësore përmirësojnë përkthimin dhe riorganizimin bazuar në fraza duke kapërcyer supozimin e pavarësisë së frazës dhe trajtimin e riorganizimit të distancës së gjatë. Megjithatë, për shkak të pakësisë së të dhënave, këto modele shpesh kthehen në madhësi shumë të vogla konteksti. Ky problem është trajtuar më parë nga mësimi i sekuencave mbi përfaqësimet e gjeneralizuara të tilla si etiketat POS apo grupet e fjalëve. Në këtë letër, ne eksplorojmë një alternativë bazuar në modelet e rrjetit nervor. Më konkretisht ne trajnojmë versione të neuralizuara të riorganizimit lexikal [1] dhe modelet e sekuencës së operacionit [2] duke përdorur rrjetin nervor feed-forward. Rezultatet tona tregojnë përmirësime deri në 0.6 dhe 0.5 pikë BLEU mbi bazën gjermane! Anglisht dhe anglisht! Sisteme gjermane. We also observed improvements compared to the systems that used POS tags and word clusters to train these models.  Sepse ne modifikojmë korpusin dy-gjuhës për të integruar operacionet e riorganizimit, kjo na lejon të trajnojmë gjithashtu një model MT-në-sekuencë neural duke pasur shkaktues të riorganizimit eksplicit. Motivi ynë ishte të mundësonim drejtpërdrejt riorganizimin e informacionit në kuadrin e koduesit-dekoderit, që përndryshe mbështetet vetëm në model in e vëmendjes për të trajtuar riorganizimin e distancës së gjatë. U përpoqëm të rregullonim operacionet më të mëdha dhe më të holla. Megjithatë, këto eksperimente nuk dhanë asnjë përmirësim lidhur me sistemet bazë të MT neuronale.', 'am': 'የቋንቋዊው ስርዓት ዓይነቶች የቃላትን መተርጓሜ በሚያሳድጉ እና በንግግር ነፃነትን በማሸንፍ እና የረጅም ርዝመት መስረጃን በመስጠት ማሻሻል፡፡ ነገር ግን ከዳታ ብዛት የተነሣ እነዚህ ምሳሌዎች ብዙ ጊዜ በጣም ትንሽ እጥፍ ወደፊት ይመለሳሉ፡፡ ይህ ጉዳዩ አስቀድሞ በመማር ግንኙነቶችን እንደ POS ምልክቶች ወይም የንግግር ክፍተቶችን በመማር የተጠቃሚ ተጨማሪ ሆኖአል፡፡ በዚህ ገጾች ውስጥ የናውሬል መረብ ምሳሌዎችን በመሠረት ላይ የሚለይ ምርጫ እንፈልጋለን፡፡ በተጨማሪም የሊክሲካዊ የደረጃ መልዕክት እና የስብሰባ ምርጫዎችን በመጠቀም የፍሬት-forward ኔural መረብ እናስተምራለን፡፡ Our results show improvements of up to 0.6 and 0.5 BLEU points on top of the baseline German! እንግሊዘኛ እና እንግሊዘኛ! ጀርመን እንደዚህ ምሳሌዎችን ለማስተምር ከPOS tags እና ቃላት ጉዳዮች ጋር የተጠቀሙትን ስርዓቶች እና አካባቢዎችን አግኝተናል፡፡ ስለዚህም የሁለት ቋንቋ ካርፓስ ክፍተቶችን ለመቀናቀል እናደርጋለን፤ ይህም ደግሞ ግልፅ የሚያስፈልጉትን የግልፅ ተቃዋሚዎችን እናስተምር ዘንድ ይፈቅዳል፡፡ ጉዳዩ የፊደል አካባቢ ክፍል ውስጥ መረጃዎችን ማቅረብ ማድረግ ነው፤ እንደዚህም ባይሆን የረጅም ርዝመት ዳርቻን ለመቀበል በጥያቄ ሞዴል ብቻ ነው፡፡ የኮርፖርት እና ጥሩ ተሳካቾች የሥርዓት ተሟጋቾችን ሞከርን:: ምንም እንኳን እነዚህ ፈተናዎች የኔural MT ስርዓቶች በጥያቄ ላይ ምንም ማስታወቂያ አላደረጉም፡፡', 'hy': "Երկլեզու հաջորդականության մոդելները բարելավում են արտահայտություններով հիմնված թարգմանությունը և վերադասավորումը' հաղթահարելով արտահայտությունների անկախության ենթադրությունը և վերադասավորելով երկար տարածքներ: Այնուամենայնիվ, տվյալների հազվադեպության պատճառով, այս մոդելները հաճախ վերադառնում են շատ փոքր կոնտեքստի չափերին: Այս խնդիրը նախկինում լուծվել է սովորելու հաջորդականություններով ընդհանուր ներկայացումների, ինչպիսիք են POS թեգերը կամ բառերի խմբերը: Այս թղթի մեջ մենք ուսումնասիրում ենք մի այլընտրանք, որը հիմնված է նյարդային ցանցի մոդելների վրա: Ավելի կոնկրետ, մենք վարժեցնում ենք լեքսիկալիզացված վերադասավորման նյարդային տարբերակները [1] և գործողության հաջորդականության մոդելները [2] օգտագործելով հոսանքային նայարդային ցանց: Մեր արդյունքները ցույց են տալիս բարելավումներ մինչև 0.6 և 0.5 ԲԼԵՎ միավորի բարելավումներ հիմնական գերմանացիների վրա: Անգլերեն և անգլերեն: German systems.  Մենք նաև հետազոտեցինք զարգացումներ, համեմատած այն համակարգերի հետ, որոնք օգտագործում էին POS նշաններ և բառերի խմբեր այս մոդելների ուսուցման համար: Քանի որ մենք փոփոխում ենք երկլեզու կորպոսը, որպեսզի ինտեգրենք վերադասավորման գործողությունները, սա թույլ է տալիս մեզ նաև վարժեցնել նյարդային ՄԹ մոդելը հաջորդականությամբ հաջորդականությամբ, որը ունի բացահայտ վերադա Մեր մոտիվացիան անմիջապես հնարավորություն էր տալիս վերադասավորել տեղեկատվությունը կոդերի-կոդերի շրջանակում, որը հակառակ դեպքում հիմնված է միայն ուշադրության մոդելի վրա, որպեսզի կառավարվի երկար տարածության վերադասավորումը: Մենք փորձեցինք ավելի խիստ և նուրբ վերադասավորման գործողություններ: Այնուամենայնիվ, այս փորձերը ոչ մի բարելավում չունեին հիմնական նյարդային մաթեմատիկական համակարգերի վերաբերյալ:", 'tr': 'Ikinji sany senaryň modleri frazler tabanly terjime edip, frazler boýunlukynyň assumplaryny üstine alyp ýöredýär we uzay aralygy gaýtalandyrylyp ýöredýär. Bu nusgalar köplenç örän kiçi bir kontekst ululykyna uçurýarlar. Bu mesele, POS tägleri ýa-da söz clusterleri ýaly döredilen suratlardan öwrenmek üçin öňünden çözüldi. Bu kagyzda, neural şebeke nusgalaryna daýan ýan alternatiwny keşif edip bilýäris. Köp dogry bir şekilde biz, leksikalized düzenlemek üçin neiralyzed sürümlerini [1] we operasiýa düzenlemek modellerini [2] feer-forward neural şebekesini ulanýarys. Biziň netijelerimiz 0,6 we 0,5 BLEU köprügine seredeniň üstünde Almança gelişmelerini görkeýär! Iňlisçe we Iňlisçe! Almança sistemalar. Biz hem POS etiketleri we söz klasterleri ulanan sistemalara görä gelişmeleri gözledik. Çünkü biz ikinji dilli korpusy täzeden düzenlemek üçin üýtgedik, bu bize a şyk düzenlemek üçin bir terjime-täzeden neural MT modelini de öwretmek üçin mümkin edýäris. Biziň motivasymyz ködleme-dekoderlemek çerçevesinde maglumaty düzenlemek üçin düzetmelidi. Bu diňe uzak düzenlemek üçin diňe üns modinde güýçlidir. Biz ikimiz garşylary we süýji düzenlemek işlerini synanypdyk. Ýöne bu deneyler, Neural MT sistemalarynda hiç hili gowurak ýüze çykmady.', 'az': 'İkinci seçmə modelləri frazlərin dəyişdirilməsi və dəyişdirilməsini çox uzun dəyişdirilməsi ilə dəyişdirirlər. Lakin, məlumatlar küçük olmasına görə bu modellər çox kiçik məlumatlara qayıdacaqlar. Bu problemin əvvəlcə POS etiketləri və söz klasterləri kimi genel göstəricilər üzərində öyrənmək sıraları ilə çəkilmişdir. Bu kağıtda, nöral ağ modellərinə dayanan alternatifi keşif edirik. Daha da müfəssəl olaraq, biz leksikalizirləndirilmiş reordering verzijlərini təhsil edirik [1] və operasyon sıralama modellərini [2] istifadə edirik. Sonuçlarımız 0,6 və 0,5 BLEU nöqtələrinin üstündə düzəltdiklərini Almanca göstərir! İngilizce və İngilizce! Almanca sistemləri. Biz də bu modelləri təhsil etmək üçün POS etiketlərini və söz klasterlərini istifadə edən sistemlərlə qarşılaşdığımız yaxşılıqları görmüşdük. Çünki biz ikili dil korpusu yeni ordenama işləri birləşdirmək üçün dəyişdiririk, bu da bizə müfəssəl reordering tetikləri olan nöral MT modelini təhsil etmək üçün imkan verir. Bizim motivasimiz kodlayıcı frameworkdə məlumatları yenidən ordenaması müəyyən edilmək idi. Bu isə yalnız uzun səviyyənin yenidən ordenaması üçün gözləmə modelində təvəkkül edir. Biz hər ikisini fərqli və gözəl dənəli düzəltməyi imtahana çəkdik. Ancaq bu təcrübələr əsas səviyyədə Nöral MT sistemlərinin üstündə heç bir yaxşılıqlarını göstərmədi.', 'bs': 'Modeli dvostruke sekvence poboljšavaju prevod i reordinaciju na frazi, prevladavajući pretpostavku nezavisnosti frazala i reagiranje dugog raspona. Međutim, zbog rezerviteta podataka, ovi modeli često se vraćaju na veoma male veličine konteksta. Ovaj problem je ranije riješen učenjskim sekvencijama iznad generaliziranih predstavljanja kao što su POS etikete ili skupine riječi. U ovom papiru istražujemo alternativu baziranu na modelima neuralne mreže. Još konkretnije treniramo neuralizovane verzije leksikaliziranog reorderinga [1] i modele sekvence operacije [2] koristeći neuralnu mrežu napredne hrane. Naši rezultati pokazuju poboljšanje do 0,6 i 0,5 BLEU bodova na vrhu početne njemačke linije! Engleski i engleski! Njemački sistemi. Također smo primijetili poboljšanja u usporedbi sa sistemima koji su koristili oznake i skupine riječi za obuku ovih modela. Zato što izmijenimo dvojezički korpus da integrišemo reordering operacije, to nam omogućava da treniramo i sekvencijski neuralni MT model koji ima eksplicitne reordering okidače. Naša motivacija je bila da se direktno omogućavamo reorderiranje informacija u okviru kodera-dekodera, što se inače oslanja samo na model pažnje da se riješi reorderiranje dugog dometa. Pokušali smo i preuređivati operacije. Međutim, ovi eksperimenti nisu pružili nikakve poboljšanje u osnovnim nervnim MT sistemima.', 'bn': 'বিলিঙ্গুয়েল সেকেন্সের মডেল বাক্য-ভিত্তিক অনুবাদ এবং বাক্যের স্বাধীনতা বিজয়ী এবং দীর্ঘ সীমান্ত পুনরায় আদেশ প্রদান করে। তবে ডাটা স্প্রাইজের কারণে এই মডেলগুলো প্রায়শই খুব সামান্য প্রতিক্রিয়ায় ফিরে যায়। পোস ট্যাগ অথবা শব্দ ক্ল্যাস্টারের মতো প্রতিনিধিদের বিভিন্ন সাধারণ প্রতিনিধিত্বের ক্ষেত্রে শিক্ষা প্রাপ্ত In this paper, we explore an alternative based on neural network models.  আরো নিশ্চিতভাবে আমরা লেক্সিক্সিকালের পুনরায় আদেশের নিউরুলিজ সংস্করণ প্রশিক্ষণ প্রশিক্ষণ করি [1] এবং অপারেশন সেকেন্সেকেন্স মডেল [2 আমাদের ফলাফল প্রদর্শন করা হয়েছে বেসেলাইন জার্মানের উপরে প্রায় ০. ইংরেজি এবং ইংরেজি! জার্মান সিস্টেম। এছাড়াও আমরা পোস ট্যাগ এবং শব্দ ক্লাস্টার ব্যবহার করা এই মডেল প্রশিক্ষণের তুলনায় উন্নতি দেখেছি। কারণ আমরা দুই ভাষার কোর্পাস পরিবর্তন করি পুনরায় নির্দেশ প্রকাশ করার জন্য, এটি আমাদের পুনরায় নির্দেশ দেয়ার জন্য পুনরায় নির্দেশ প্রশিক্ষণ প্রদান করতে পারে। আমাদের উদ্দেশ্য ছিল সরাসরি এনকোডার-ডেকোডার ফ্রেমে তথ্য পুনরায় আদেশ করার জন্য, যা অন্যথায় কেবল দীর্ঘ সীমান্ত পুনরায় নির্দেশ করার জন্ আমরা পুনরায় নির্দেশের কাজের চেষ্টা করেছিলাম। তবে এই পরীক্ষাগুলো নেউরাল এমটি সিস্টেমের উপর কোন উন্নতি পায়নি।', 'ca': "Els models de seqüència bilingüe milloren la traducció basada en frases i la reordenació superant l'assumpció d'independència de frases i manejant la reordenació a llarg alcance. Però, a causa de l'escassetat de dades, aquests models sovint retornen a grans dimensions contextuals. Aquest problema s'ha abordat anteriorment aprenent seqüències sobre representacions generalitzades com etiquetes POS o agrupaments de paraules. In this paper, we explore an alternative based on neural network models.  Més concretament entrenem versions neuralitzades de reorganització lexicalitzada [1] i els models de seqüència operativa [2] utilitzant una xarxa neural feed-forward. Els nostres resultats mostran millores de fins a 0,6 i 0,5 punts BLEU a dalt de la línia alemana! anglès i anglès! sistemes alemanys. També vam observar millors comparats amb els sistemes que utilitzaven etiquetes POS i grups de paraules per formar aquests models. Perquè modifiquem el cos bilingüe per integrar les operacions de reorganització, això ens permet treinar un model de MT neural secuencia a secuencia amb desencadenants explícits de reorganització. La nostra motivació era permetre directament la reorganització de la informació en el marc del codificador-decoder, que d'altra manera es basava exclusivament en el model d'atenció per gestionar la reorganització a llarg alcance. Vam intentar operacions de reorganització més groses i fins. No obstant això, aquests experiments no van produir cap millora en relació amb els sistemes de MT neuronal basals.", 'cs': 'Dvoujazyčné sekvenční modely zlepšují překlad a reřazování frází tím, že překonávají předpoklad frazální nezávislosti a zvládají reřazování dlouhého rozsahu. Nicméně vzhledem k řídkosti dat se tyto modely často vracejí k velmi malým kontextům. Tento problém byl dříve řešen učebními sekvencemi nad zobecněnými reprezentacemi, jako jsou POS tagy nebo slovní clustery. V tomto článku zkoumáme alternativu založenou na modelech neuronových sítí. Konkrétněji trénujeme neuralizované verze lexikalizovaného reřazení [1] a modely operačních sekvencí [2] pomocí feed-forward neuronové sítě. Naše výsledky ukazují zlepšení až 0,6 a 0,5 BLEU bodů nad základní němčinou! Angličtina a angličtina! Německé systémy. Také jsme pozorovali zlepšení ve srovnání se systémy, které používaly POS tagy a slovní clustery k tréninku těchto modelů. Protože modifikujeme dvojjazyčný korpus tak, abychom integrovali reřazovací operace, umožňuje nám to také trénovat sekvenční neuronový MT model s explicitními reřazovacími triggery. Naší motivací bylo přímo umožnit změnu uspořádání informací v rámci kodéru-dekodéru, který jinak spoléhá pouze na model pozornosti pro zpracování přeměny na dlouhý dosah. Zkoušeli jsme hrubší i jemnozrnnější operace přeobjednávání. Tyto experimenty však nepřinesly žádné zlepšení oproti základním neuronovým MT systémům.', 'et': 'Kahekeelsed jadamudelid parandavad fraasipõhist tõlkimist ja ümberkorraldamist, ületades fraasisõltumatuse eelduse ja käsitledes pikaajalist ümberkorraldamist. Andmete vähesuse tõttu langevad need mudelid sageli tagasi väga väikeste kontekstide suurusteni. Seda probleemi on varem lahendatud üldiste esituste nagu POS siltide või sõnaklastrite õppimisega. Käesolevas töös uurime alternatiivi, mis põhineb neurovõrgu mudelitel. Konkreetsemalt treenime leksikaliseeritud ümberkorraldamise neuraliseeritud versioone [1] ja operatsioonijada mudeleid [2] kasutades edasise neurovõrgu abil. Meie tulemused näitavad paranemist kuni 0,6 ja 0,5 BLEU punkti võrreldes algtasemega Saksa! Inglise ja inglise keeles! Saksa süsteemid. Samuti täheldasime edusamme võrreldes süsteemidega, mis kasutasid POS silte ja sõnaklastreid nende mudelite koolitamiseks. Kuna me modifitseerime kakskeelset korpust ümberkorraldamise operatsioonide integreerimiseks, võimaldab see meil koolitada ka järjestuse-järjestuseni neuraalset MT mudelit, millel on selged ümberkorraldamise käivitajad. Meie motivatsioon oli otseselt võimaldada andmete ümberkorraldamist kodeerija-dekooderi raamistikus, mis muidu tugineb ainult tähelepanu mudelile pikaajalise ümberkorraldamise korraldamisel. Proovisime nii jämedamat kui peeneteralist ümberkorraldamist. Need katsed ei andnud siiski mingeid paranemisi võrreldes esialgse neuraalse MT süsteemiga.', 'fi': 'Kaksikieliset sekvenssimallit parantavat fraasipohjaista kääntämistä ja uudelleenjärjestelyä voittamalla fraasiriippumattomuuden olettamuksen ja käsittelemällä pitkän aikavälin uudelleenjärjestelyä. Tietojen niukkuuden vuoksi nämä mallit kuitenkin usein palaavat hyvin pieniin kontekstikokokoihin. Tätä ongelmaa on aiemmin käsitelty oppimalla sekvenssejä yleisten esitysten, kuten POS-tunnisteiden tai sanaklusterien, avulla. Tässä työssä tutkimme neuroverkkomalleihin perustuvaa vaihtoehtoa. Konkreettisemmin koulutamme neuralisoituja versioita leksikalisoidusta uudelleenjärjestelystä [1] ja toimintasekvenssimalleja [2] käyttäen syöttöhermoverkkoa. Tuloksemme osoittavat parannuksia jopa 0,6 ja 0,5 BLEU pistettä verrattuna perussaksalaiseen! Englantia ja englantia! Saksalaiset järjestelmät. Havaitsimme myös parannuksia verrattuna järjestelmiin, joissa käytettiin POS-tunnisteita ja sanaklustereita näiden mallien kouluttamiseen. Koska muokkaamme kaksikielistä korpusta integroidaksemme uudelleenjärjestelytoiminnot, voimme myös kouluttaa sekvenssi-sekvenssiin neuraalista MT-mallia, jossa on eksplisiittiset uudelleenjärjestelyn triggerit. Motivaatiomme oli mahdollistaa tiedon uudelleenjärjestely suoraan kooderi-dekooderikehyksessä, joka muuten nojaa pelkästään huomiomalliin pitkän kantaman uudelleenjärjestelyssä. Kokeilimme sekä karkeampia että hienorakeisia uudelleenjärjestelyjä. Näissä kokeissa ei kuitenkaan saatu aikaan parannuksia lähtötilanteeseen verrattuna Neural MT -järjestelmään.', 'jv': 'Bilingual model sing beranduh nyimpen kelas-usual terjamah lan urip banter Nanging, kaya data sithik, model iki dadi mesthi gabut gak bakal ditambah sampeyan anyar. before Nang mapun iki, kéné kelas maning Alternate sing basa supoyo model tambah alat. Simulate Rejalaké awakdhéwé menyang kakéh sabanjuré 0.6 lan 0.5 B luwih barang kanggo barang alam kuwi mau ! Inggris karo Inggris! Sistem alam. Awak dhéwé éntuk sistem sing bisa nggawe gerarané karo sistem sing bisa nggawe tags-tags lan word clusters kuwi nggawe model iki. Soalé awak dhéwé éntuk sistem iki tanggal kanggo digunggal urip nggawe nguasai operasi nggawe, iki supoyo awak dhéwé kuwi wis beranduh sekonduran-tanggal MT model sing beranduh operasi sing beranduh terandukan. Awakdhéwé aksi kanggo digawe mulai nggawe informasi ning acara dadi koder-decoder, supoyo nyimpen kuwi modèl kuwi ngregani martingan tur angel. @item: checkbox Nanging, nik uwong sing paling-uwong iki ora ono nggawe luwih nggawe sistem Neral MT sing uwis.', 'sk': 'Dvojezični modeli zaporedja izboljšujejo prevajanje na osnovi fraz in preurejanje vrstnega reda s premagovanjem predpostavke neodvisnosti fraz in obdelavo preurejanja dolgega razpona. Vendar pa zaradi redkosti podatkov ti modeli pogosto padejo nazaj v zelo majhne velikosti konteksta. Ta težava je bila prej rešena z učenjem sekvenc prek posplošenih predstavitev, kot so POS oznake ali besedne skupine. V prispevku raziskujemo alternativo, ki temelji na modelih nevronskih omrežij. Natančneje usposabljamo nevralizirane različice leksikaliziranega preurejanja [1] in modele zaporedja operacij [2] z uporabo feed-forward nevronskega omrežja. Naši rezultati kažejo izboljšave za do 0,6 in 0,5 točke BLEU nad izhodiščno nemščino! Angleščina in angleščina! Nemški sistemi. Opazili smo tudi izboljšave v primerjavi s sistemi, ki so uporabljali oznake POS in besedne grozde za usposabljanje teh modelov. Ker spreminjamo dvojezični korpus za integracijo operacij reordenacije, nam to omogoča tudi usposabljanje nevronskega MT modela zaporedja v zaporedje, ki ima eksplicitne sprožilce reordenacije. Naša motivacija je bila, da bi neposredno omogočili preurejanje informacij v okviru kodirnika-dekoderja, ki se sicer zanaša izključno na model pozornosti za obravnavanje preurejanja dolgega dosega. Poskusili smo tako grobo kot drobnozrnato preureditev. Vendar pa ti poskusi niso prinesli nobenih izboljšav v primerjavi z izhodiščnimi nevronskimi MT sistemi.', 'ha': "@ action: button A lokacin da aka sami tsarin data, misalin waɗannan ana ƙara zuwa girmar mazaɓa masu ƙaranci. @ info: whatsthis Daga wannan takardan, Munã jarraba wata matsayi a kan misalin zanen neura. Ko da tabbatacce, za'a ƙidãya koyan version na reordainin leksisiya[1] da misãlai masu sequence wa aikin[2] sunã amfani da zanen feed-forward neural. Our results show improvements of up to 0.6 and 0.5 BLEU points on top of the baseline German! @ item Spelling dictionary @ item license (short name) Mun sami mafarinta sami da tsari waɗanda suka yi amfani da tagogi na haske da kalma na tsari masu motsi. Saboda haka, muna gyarar makampuni biyu dõmin a haɗa aikin da za'a yi umurni da shi, wannan yana yarda mu kofi wata misalin MT-na'urar-dubi'in-sequence da za'a sami na fara ta fara ta fara ɗaya. Jiyyanmu ya kasance ana iya fara idan an sake fara tsarin bayani na koda-koda, wanda ba haka ba, yana dõgara kawai kan misalin muhalli da za'a yi amfani da tsarin da za'a yi dubi'a tsawo. Kuma ba mu jarraba dukansu na takarda da surori mai kyau. Haƙĩƙa, wannan jarrabai ba ta ƙara wani improvements ba kan system-na'ura MT na basuɓa ba.", 'he': 'Bilingual sequence models improve phrase-based translation and reordering by overcoming phrasal independence assumption and handling long range reordering.  עם זאת, בגלל נדירות נתונים, דוגמנים אלה לעתים קרובות נופלים בחזרה לגודלים הקשר קטנים מאוד. הבעיה הזאת כבר התייחסת בעבר על ידי רצפי לימוד מעל מייצגות כלליות כמו תוויות POS או קבוצות מילים. בעיתון הזה, אנחנו חוקרים אלטרנטיבה מבוססת על דוגמני רשת עצבית. במיוחד אנו מאמן גרסאות עצביות של שינוי הלקסיקלי [1] ומדוגמני רצף הפעולה [2] באמצעות רשת עצבית feed-forward. התוצאות שלנו מראות שיפורים עד 0.6 ו-0.5 נקודות BLEU על גבוהה הגרמנית! אנגלית ואנגלית! מערכות גרמניות. צפינו גם בשיפורים בהשוואה למערכות שהשתמשו בתגים POS וקבוצות מילים לאימון דוגמנים אלה. בגלל שאנחנו משנים את הקורפוס השולשי כדי להשתלב במבצעי שינוי, זה מאפשר לנו גם לאמן מודל MT עצבי רצף לרצף המוטיבציה שלנו היתה לאפשר ישירות שינוי מידע במסגרת הקודר-קידור, אשר אחרת תלוי רק על מודל תשומת לב לטפל בשינוי מרחק ארוך. ניסינו גם מבצעי שינוי חמורים וגמונים יותר. עם זאת, הניסויים האלה לא הביאו שום שיפור מעל מערכות MT נוירואליות בסיסית.', 'bo': 'Bilingual sequence models improve phrase-based translation and reordering by overcoming phrasal independence assumption and handling long range reordering. ཡིན་ནའང་། ཆེ་ཆས་བཞག་པའི་བྱ་སྤྱོད་བྱས་ན་སྐྱེས་པའི་མིག་ཆས་འདི་དག་རྒྱུན་ལྡན་སྐྱེས་པའི་ཆེ་ཆུང དཀའ་ངལ་འདི་སྔོན་གྱིས་གྲངས་སུ་དབྱེ་རིམ་གྱི་རྣམ་པ་ལ་བསྟུན་ནས་དབྱེ་བ་བཟོ་ཚར་བ། ཤོག་བྱང་འདིའི་ནང་དུ་འུ་ཚོས་ཀྱི་དཔེ་དབུས་མཐུད་ལམ་ལུགས་གཞན་ཞིག་འཚོལ་ཞིབ་བྱེད་ཀྱི་ཡོད། More concretely we train neuralized versions of lexicalized reordering [1] and the operation sequence models [2] using feed-forward neural network. ང་ཚོའི་འབྲུག དབྱིན་ཡིག་དང་དབྱིན་ཡིག་གཟུགས། སྐད་ཡིན་ལག་ལུགས། ང་ཚོས་མཐོང་སྣང་གིས་མ་དབྱིབས་འདི་དག་གི་རྩོམ་པ་དང་ཐིག་གྲངས་ཡིག་གི་འགྱུར་བ་དང་མཐོང་བའི་རིམ་ལ་མཐོང་སྣ གང་ལགས་ཞེ་ན། ང་ཚོས་ཀྱིས་གཉིས་ཡིག་གི་སྒེར་གྱི་དབུས་མཐུན་དེ་སྡུད་གོ་སྒྲིག་བཀོད་པ་ལ་བཟོ་བཅོས་བྱེད་ཀྱི་ཡོད། Our motivation was to directly enable reordering information in the encoder-decoder framework, which otherwise relies solely on the attention model to handle long range reordering. ང་ཚོས་འཛིན་པ་དང་ཆེ་བའི་ལྕགས་རིམ་གྱི་སྔ་ནས་བཟོ་བར་བྱེད་ཀྱི་ཡོད། ཡིན་ནའང་། བརྟག་ཞིབ་འདི་དག་གིས་རྨང་གཞིའི་སྒེར་གྱི་རྒྱུ་ལམ་ལ་ཡར་རྒྱས་གཏོང་ཐབས་མེད།'}
{'en': 'Data Selection with Cluster-Based Language Difference Models and Cynical Selection', 'es': 'Selección de datos con modelos de diferencias lingüísticas basados en clústeres y selección cínica', 'pt': 'Seleção de dados com modelos de diferença de idioma baseados em cluster e seleção cínica', 'zh': '盖聚类言语差异,与愤世嫉俗之数据选择', 'fr': 'Sélection de données avec des modèles de différence de langage basés sur des clusters et une sélection cynétique', 'ja': 'クラスタベースの言語の違いモデルとシニカルな選択によるデータ選択', 'hi': 'क्लस्टर-आधारित भाषा अंतर मॉडल और निंदक चयन के साथ डेटा चयन', 'ru': 'Выбор данных с кластерными моделями различий языков и циничный выбор', 'ar': 'اختيار البيانات باستخدام نماذج الفروق اللغوية المستندة إلى الكتلة والاختيار المتهكم', 'ga': 'Roghnú Sonraí le Múnlaí Difríochta Teanga Cnuasbhunaithe agus Roghnú Ciniciúil', 'ka': 'მონაცემების არჩევა კლასტერის ბაზის ენის განსხვავების მოდელებით და უნიკალური არჩევა', 'el': 'Επιλογή δεδομένων με μοντέλα γλωσσικής διαφοράς βασισμένα σε σύμπλεγμα και κυνική επιλογή', 'hu': 'Adatválasztás klaszter alapú nyelvi különbség modellekkel és cinikus kiválasztással', 'it': 'Selezione dei dati con modelli di differenza linguistica basati su cluster e selezione cinica', 'lt': 'Duomenų atranka su klasteriniais kalbų skirtumų modeliais ir ciniu atranka', 'mk': 'Data Selection with Cluster-Based Language Difference Models and Cynical Selection', 'kk': 'Cluster- негіздеген тіл айырмашылық үлгілері мен циникалық таңдау', 'ms': 'Pemilihan Data dengan Model Perbezaan Bahasa Berasas Kelompok dan Pemilihan Sinik', 'ml': 'ക്ലൌസ്റ്റര്\u200d അടിസ്ഥാനമായ ഭാഷ വ്യത്യസ്ത മോഡലുകളും സൈനിക്കല്\u200d തെരഞ്ഞെടുക്കുക', 'mt': 'Għażla tad-dejta b’Mudelli ta’ Differenza Lingwistika bbażati fuq Raggruppamenti u Għażla Ċinika', 'mn': 'Клустер-сан хэл өөрчлөлт загвар болон Циникийн сонголттай өгөгдлийн сонголт', 'no': 'Databaseutval med klasterbaserte språk-forskjellingsmodeller og synisk utval', 'pl': 'Wybór danych za pomocą klastrowych modeli różnic językowych i selekcji cynicznej', 'ro': 'Selectarea datelor cu modele de diferență lingvistică bazate pe cluster și selecție cinică', 'sr': 'Izaber podataka sa modelima razlike jezika na bazi klustera i ciničkim izborima', 'si': 'Cluster- ආධාරිත භාෂා වෙනස් මොඩල් සහ සිනිකාල තෝරණය සමග දත්ත තෝරණය', 'so': 'Doorashada macluumaadka ee ku qoran luqada kala duwan ee Cluster-Based', 'sv': 'Dataval med klusterbaserade språkskillnader modeller och cyniskt urval', 'ta': 'Data Selection with Cluster-Based Language Difference Models and Cynical Selection', 'ur': 'کلسٹر-بنیادی زبان مختلف موڈل اور سینیک انتخاب کے ساتھ ڈاٹ انتخاب', 'uz': 'Name', 'vi': 'Chọn độ phân biệt ngôn ngữ với các mẫu liên kết và chọn người máy', 'bg': 'Избор на данни с модели на езикови разлики въз основа на клъстери и цинична селекция', 'hr': 'Izabranje podataka sa modelima razlike jezika na bazi klustera i ciničkim izborima', 'da': 'Datavalg med klyngebaserede sprogforskellmodeller og kynisk valg', 'de': 'Datenauswahl mit clusterbasierten Sprachdifferenzmodellen und zynischer Selektion', 'id': 'Pemilihan Data dengan Model Perbedaan Bahasa Berdasarkan Kelompok dan Pemilihan Sinik', 'nl': 'Dataselectie met clustergebaseerde taalverschillen en cynische selectie', 'ko': '분류된 언어 차이 모델과 분세 질속 선택을 바탕으로 한 데이터 선택', 'sw': 'Uchaguzi wa data kwa lugha tofauti na Uchaguzi wa Kireno', 'tr': 'Cluster-Baserden Diller Çeşmeleri we Kinik Saýlaw', 'af': 'Data Keuse met Cluster- Based Taal Verskil Modelle en Siniese Keuse', 'am': 'የዳታ ምርጫ', 'az': 'Cluster-Based Dil Farklƒ±lƒ±q Modell…ôri v…ô Sinikal Se√ßimi', 'fa': 'انتخاب داده با مدل متفاوت زبان و انتخاب سینیکی پایه\u200cهای کلاستر', 'sq': 'Zgjidhja e të dhënave me modele ndryshimi gjuhësh bazuar në grupe dhe zgjedhje cinike', 'ca': 'Selecció de dades amb models de diferència lingüística basats en grups i selecció cínica', 'bn': 'Name', 'bs': 'Izaber podataka sa modelima razlike jezika na bazi klustera i ciničkim selekcijom', 'et': 'Andmete valimine klastripõhiste keeleerinevuste mudelite ja küünilise valikuga', 'hy': 'Տվյալների ընտրությունը խմբի հիմնված լեզվի տարբերության մոդելներով և սինիկ ընտրությամբ', 'fi': 'Tietojen valinta klusteripohjaisilla kielieromalleilla ja kyynisellä valinnalla', 'cs': 'Výběr dat s klastrovými modely jazykových rozdílů a cynickým výběrem', 'jv': 'undo-type', 'ha': '@ action', 'he': 'בחירת נתונים עם דוגמני שינויים מבוססים על קבוצות ובחירה סינית', 'sk': 'Izbira podatkov z modeli jezikovnih razlik na podlagi grozdov in cinično izbiro', 'bo': 'Cluster-Based Language Difference Models and Cynical Selection with Data Selection'}
{'en': 'We present and apply two methods for addressing the problem of selecting relevant training data out of a general pool for use in tasks such as  machine translation . Building on existing work on class-based language difference models [ 1 ], we first introduce a cluster-based method that uses Brown clusters to condense the vocabulary of the corpora. Secondly, we implement the cynical data selection method [ 2 ], which incrementally constructs a training corpus to efficiently model the task corpus. Both the cluster-based and the cynical data selection approaches are used for the first time within a machine translation system, and we perform a head-to-head comparison. Our intrinsic evaluations show that both new methods outperform the standard Moore-Lewis approach (cross-entropy difference), in terms of better perplexity and OOV rates on in-domain data. The cynical approach converges much quicker, covering nearly all of the in-domain vocabulary with 84 % less data than the other methods. Furthermore, the new approaches can be used to select machine translation training data for training better  systems . Our results confirm that class-based selection using Brown clusters is a viable alternative to POS-based class-based methods, and removes the reliance on a  part-of-speech tagger . Additionally, we are able to validate the recently proposed cynical data selection method, showing that its performance in SMT models surpasses that of traditional cross-entropy difference methods and more closely matches the sentence length of the task corpus.', 'ar': 'نقدم ونطبق طريقتين لمعالجة مشكلة اختيار بيانات التدريب ذات الصلة من مجموعة عامة لاستخدامها في مهام مثل الترجمة الآلية. بناءً على العمل الحالي على نماذج الفروق اللغوية المستندة إلى الفصل [1] ، نقدم أولاً طريقة قائمة على الكتلة تستخدم مجموعات براون لتكثيف مفردات المجموعة. ثانيًا ، نقوم بتنفيذ طريقة اختيار البيانات المتهكمة [2] ، والتي تبني بشكل تدريجي مجموعة تدريب لنمذجة مجموعة المهام بكفاءة. يتم استخدام كل من نهج اختيار البيانات القائم على الكتلة والنهج الساخر لأول مرة في نظام الترجمة الآلية ، ونقوم بإجراء مقارنة وجهاً لوجه. تُظهر تقييماتنا الجوهرية أن كلتا الطريقتين الجديدتين تتفوقان في الأداء على نهج Moore-Lewis القياسي (اختلاف الانتروبيا المتقاطعة) ، من حيث الارتباك الأفضل ومعدلات OOV على البيانات داخل المجال. يتقارب النهج الساخر بشكل أسرع ، ويغطي تقريبًا كل المفردات في المجال ببيانات أقل بنسبة 84٪ من الطرق الأخرى. علاوة على ذلك ، يمكن استخدام الأساليب الجديدة لتحديد بيانات تدريب الترجمة الآلية لتدريب أنظمة أفضل. تؤكد نتائجنا أن الاختيار المستند إلى الفصل باستخدام مجموعات Brown هو بديل قابل للتطبيق للطرق المستندة إلى فئة POS ، ويزيل الاعتماد على أداة تمييز جزء من الكلام. بالإضافة إلى ذلك ، نحن قادرون على التحقق من صحة طريقة اختيار البيانات المتشائمة المقترحة مؤخرًا ، والتي توضح أن أدائها في نماذج SMT يفوق أداء طرق الاختلاف التقليدية عبر الانتروبيا والمزيد\nيتطابق بشكل وثيق مع طول جملة مجموعة المهام.', 'pt': 'Apresentamos e aplicamos dois métodos para resolver o problema de selecionar dados de treinamento relevantes de um conjunto geral para uso em tarefas como tradução automática. Com base no trabalho existente sobre modelos de diferenças de linguagem baseados em classes [1], primeiro introduzimos um método baseado em clusters que usa clusters Brown para condensar o vocabulário dos corpora. Em segundo lugar, implementamos o método de seleção de dados cínico [2], que constrói incrementalmente um corpus de treinamento para modelar eficientemente o corpus de tarefas. Ambas as abordagens de seleção de dados baseada em cluster e cínica são usadas pela primeira vez em um sistema de tradução automática, e realizamos uma comparação direta. Nossas avaliações intrínsecas mostram que ambos os novos métodos superam a abordagem padrão de Moore-Lewis (diferença de entropia cruzada), em termos de melhores taxas de perplexidade e OOV em dados no domínio. A abordagem cínica converge muito mais rápido, cobrindo quase todo o vocabulário do domínio com 84% menos dados do que os outros métodos. Além disso, as novas abordagens podem ser usadas para selecionar dados de treinamento de tradução automática para treinar sistemas melhores. Nossos resultados confirmam que a seleção baseada em classe usando clusters Brown é uma alternativa viável aos métodos baseados em classe baseados em POS e remove a dependência de um tagger de parte da fala. Além disso, podemos validar o método de seleção de dados cínicos recentemente proposto, mostrando que seu desempenho em modelos SMT supera o dos métodos tradicionais de diferença de entropia cruzada e mais\ncorresponde ao comprimento da frase do corpus da tarefa.', 'fr': "Nous présentons et appliquons deux méthodes pour résoudre le problème de la sélection de données de formation pertinentes à partir d'un pool général à utiliser dans des tâches telles que la traduction automatique. En nous appuyant sur les travaux existants sur les modèles de différence linguistique basés sur les classes [1], nous introduisons d'abord une méthode basée sur les clusters qui utilise des clusters de Brown pour condenser le vocabulaire des corpus. Deuxièmement, nous mettons en œuvre la méthode cyniques de sélection des données [2], qui construit progressivement un corpus de formation pour modéliser efficacement le corpus de tâches. Les approches basées sur les clusters et les approches cyniques de sélection de données sont utilisées pour la première fois dans un système de traduction automatique, et nous effectuons une comparaison directe. Nos évaluations intrinsèques montrent que les deux nouvelles méthodes surpassent l'approche standard de Moore-Lewis (différence d'entropie croisée), en termes de meilleure perplexité et de taux OOV sur les données internes au domaine. L'approche cynique converge beaucoup plus rapidement, couvrant presque tout le vocabulaire du domaine avec 84\xa0% de données en moins que les autres méthodes. En outre, les nouvelles approches peuvent être utilisées pour sélectionner des données de formation en traduction automatique afin de former de meilleurs systèmes. Nos résultats confirment que la sélection basée sur les classes à l'aide de clusters Brown est une alternative viable aux méthodes basées sur les classes basées sur les POS, et supprime la dépendance à un marqueur de partie du discours. De plus, nous sommes en mesure de valider la méthode de sélection de données cyniques récemment proposée, montrant que ses performances dans les modèles SMT dépassent celles des méthodes traditionnelles de différence d'entropie croisée et plus encore\ncorrespond étroitement à la longueur de phrase du corpus de tâches.", 'es': 'Presentamos y aplicamos dos métodos para abordar el problema de seleccionar datos de capacitación relevantes de un grupo general para su uso en tareas como la traducción automática. Sobre la base del trabajo existente sobre modelos de diferencias lingüísticas basados en clases [1], primero introducimos un método basado en clústeres que utiliza grupos Brown para condensar el vocabulario de los corpus. En segundo lugar, implementamos el método cínico de selección de datos [2], que construye gradualmente un corpus de entrenamiento para modelar eficientemente el corpus de tareas. Tanto el enfoque basado en clústeres como el de selección cínica de datos se utilizan por primera vez en un sistema de traducción automática, y realizamos una comparación directa. Nuestras evaluaciones intrínsecas muestran que ambos métodos nuevos superan al enfoque estándar de Moore-Lewis (diferencia de entropía cruzada), en términos de mejor perplejidad y tasas de OOV en los datos dentro del dominio. El enfoque cínico converge mucho más rápido, cubriendo casi todo el vocabulario del dominio con un 84% menos de datos que los otros métodos. Además, los nuevos enfoques se pueden utilizar para seleccionar datos de capacitación en traducción automática para entrenar mejores sistemas. Nuestros resultados confirman que la selección basada en clases con clústeres Brown es una alternativa viable a los métodos basados en clases basados en POS, y elimina la dependencia de un etiquetador de parte del discurso. Además, podemos validar el método cínico de selección de datos recientemente propuesto, lo que demuestra que su rendimiento en los modelos SMT supera al de los métodos tradicionales de diferencia de entropía cruzada y más\ncoincide estrechamente con la longitud de la oración del cuerpo de tareas.', 'ja': '機械翻訳などのタスクで使用するための一般的なプールから関連するトレーニングデータを選択する問題に対処するための2つの方法を提示し、適用します。 クラスベースの言語差異モデル[1]に関する既存の研究に基づいて、まずブラウン・クラスターを使用してコーラの語彙を凝縮するクラスターベースの方法を導入する。 第二に、タスクコーパスを効率的にモデル化するためにトレーニングコーパスを段階的に構築するシニカルなデータ選択方法[2]を実装する。 クラスターベースのデータ選択アプローチとシニカルなデータ選択アプローチの両方が、機械翻訳システム内で初めて使用され、対面比較を行います。 我々の本質的な評価は、両方の新しい方法が、ドメイン内データのより良い困惑性及びＯＯＶ率の点で、標準的なムーア＝ルイスアプローチ（クロスエントロピー差）を上回ることを示している。 皮肉なアプローチの収束ははるかに早く、ドメイン内のほぼすべての語彙を他の方法よりも84 ％少ないデータでカバーします。 さらに、新しいアプローチを使用して、より良いシステムをトレーニングするための機械翻訳トレーニングデータを選択することができます。 私たちの結果は、Brownクラスターを使用したクラスベースの選択が、POSベースのクラスベースの方法に代わる実行可能な代替手段であることを確認し、発話部分タグへの依存を排除しました。 さらに、最近提案された皮肉なデータ選択方法を検証することができ、SMTモデルでのパフォーマンスが従来のクロスエントロピー差分法などを上回ることを示しています。\nタスクコーパスの文の長さとよく一致します。', 'zh': '臣等请用二法以决之,择相关训练,以施机器翻译等事。 盖言语异[1],先引一聚类之法,用Brown聚类压缩语料库之词汇量。 其次,遂愤世嫉俗之数据选择法[2],增量构一练语料库,以效语料库建模。 盖集群愤世嫉俗之数据选择,首用机器翻译统,比之头也。 吾等内评,此二法域内数有佳困惑度与OOV率优于格摩尔-刘易斯法(交熵差)。 愤世嫉俗之法,收敛愈速,几涵盖一切域内词汇,数少于他84%。 此外,新法可于择机器翻译练数,以练善统。 臣等证之,用Brown集群之基于类者,POS之基于类者,可以代方,而消词性器之所恃也。 又验近世愤世嫉俗之数据选择,明SMT形之性过于旧熵差也。\n与任语料库句相匹。', 'hi': 'हम मशीन अनुवाद जैसे कार्यों में उपयोग के लिए एक सामान्य पूल से प्रासंगिक प्रशिक्षण डेटा का चयन करने की समस्या को संबोधित करने के लिए दो विधियों को प्रस्तुत करते हैं और लागू करते हैं। वर्ग-आधारित भाषा अंतर मॉडल [1] पर मौजूदा काम पर निर्माण, हम पहले एक क्लस्टर-आधारित विधि पेश करते हैं जो कॉर्पोरेट की शब्दावली को संघनित करने के लिए ब्राउन क्लस्टर का उपयोग करता है। दूसरे, हम सनकी डेटा चयन विधि [2] को लागू करते हैं, जो कार्य कॉर्पस को कुशलतापूर्वक मॉडल करने के लिए एक प्रशिक्षण कॉर्पस का निर्माण करता है। क्लस्टर-आधारित और निंदक डेटा चयन दृष्टिकोण दोनों का उपयोग पहली बार मशीन अनुवाद प्रणाली के भीतर किया जाता है, और हम एक सिर-से-सिर तुलना करते हैं। हमारे आंतरिक मूल्यांकन से पता चलता है कि दोनों नए तरीके मानक मूर-लुईस दृष्टिकोण (क्रॉस-एन्ट्रॉपी अंतर) से आगे निकलते हैं, इन-डोमेन डेटा पर बेहतर उलझन और ओओवी दरों के संदर्भ में। निंदक दृष्टिकोण बहुत तेज़ी से अभिसरण करता है, अन्य तरीकों की तुलना में 84% कम डेटा के साथ लगभग सभी इन-डोमेन शब्दावली को कवर करता है। इसके अलावा, नए दृष्टिकोणों का उपयोग बेहतर प्रणालियों के प्रशिक्षण के लिए मशीन अनुवाद प्रशिक्षण डेटा का चयन करने के लिए किया जा सकता है। हमारे परिणाम पुष्टि करते हैं कि ब्राउन क्लस्टर का उपयोग करके वर्ग-आधारित चयन पीओएस-आधारित वर्ग-आधारित विधियों के लिए एक व्यवहार्य विकल्प है, और एक भाग-के-भाषण टैगर पर निर्भरता को हटा देता है। इसके अतिरिक्त, हम हाल ही में प्रस्तावित सनकी डेटा चयन विधि को मान्य करने में सक्षम हैं, यह दिखाते हुए कि एसएमटी मॉडल में इसका प्रदर्शन पारंपरिक क्रॉस-एन्ट्रापी अंतर विधियों और अधिक से अधिक है।\nबारीकी से कार्य कॉर्पस की वाक्य लंबाई से मेल खाता है।', 'ru': 'Мы представляем и применяем два метода решения проблемы выбора релевантных обучающих данных из общего пула для использования в таких задачах, как машинный перевод. Опираясь на существующую работу над моделями различий языков на основе классов [1], мы сначала вводим метод на основе кластеров, который использует Брауновские кластеры для уплотнения словарного запаса корпусов. Во-вторых, мы реализуем циничный метод отбора данных [2], который постепенно строит обучающий корпус для эффективного моделирования корпуса задач. Как кластерный, так и циничный подходы к выбору данных используются впервые в системе машинного перевода, и мы выполняем прямое сравнение. Наши внутренние оценки показывают, что оба новых метода превосходят стандартный подход Мура-Льюиса (перекрестная разница энтропии) с точки зрения лучшей недоумения и коэффициентов OOV по внутридоменным данным. Циничный подход сходится гораздо быстрее, охватывая почти весь внутридоменный словарь с на 84% меньше данных, чем другие методы. Кроме того, новые подходы могут быть использованы для выбора данных обучения машинному переводу для обучения лучших систем. Наши результаты подтверждают, что классовый выбор с использованием кластеров Брауна является жизнеспособной альтернативой методам на основе классов на основе POS, и устраняет зависимость от тегера части речи. Кроме того, мы можем подтвердить недавно предложенный циничный метод выбора данных, показывая, что его производительность в моделях SMT превосходит производительность традиционных методов перекрестной энтропии и более\nблизко соответствует длине предложения корпуса задачи.', 'ga': 'Cuirimid i láthair agus cuirimid i bhfeidhm dhá mhodh chun dul i ngleic leis an bhfadhb a bhaineann le sonraí oiliúna ábhartha a roghnú as comhthiomsú ginearálta lena n-úsáid i dtascanna ar nós aistriúchán meaisín. Ag tógáil ar an obair atá ar siúl cheana féin ar mhúnlaí rang-bhunaithe difríochta teanga [1], tugaimid isteach ar dtús modh atá bunaithe ar bhraislí a úsáideann braislí Brown chun foclóir an chorpora a chomhdhlúthú. Ar an dara dul síos, cuirimid i bhfeidhm an modh roghnúcháin sonraí ciniciúil [2], a dhéanann corpas oiliúna de réir a chéile chun an tascchorp a shamhaltú go héifeachtach. Baintear úsáid as na cineálacha cur chuige bunaithe ar bhraisle agus as na cineálacha cur chuige ciniciúil um roghnú sonraí don chéad uair laistigh de chóras aistriúcháin meaisín, agus déanaimid comparáid duine le duine. Léiríonn ár meastóireachtaí intreacha go sáraíonn an dá mhodh nua cur chuige caighdeánach Moore-Lewis (difríocht tras-eantrópachta), i dtéarmaí seachnachta níos fearr agus rátaí OOV ar shonraí in-fearainn. Tagann an cur chuige ciniciúil le chéile i bhfad níos tapúla, ag clúdach beagnach gach ceann de na stór focal in-fearainn le 84% níos lú sonraí ná na modhanna eile. Ina theannta sin, is féidir na cineálacha cur chuige nua a úsáid chun sonraí oiliúna meaisín-aistriúcháin a roghnú chun córais níos fearr a oiliúint. Deimhníonn ár dtorthaí gur rogha inmharthana é roghnú rangbhunaithe ag baint úsáide as braislí Brown ar mhodhanna rangbhunaithe POS, agus go mbaintear an spleáchas ar chlibeálaí cuid de chaint. Ina theannta sin, táimid in ann an modh roghnúcháin sonraí ciniciúil a moladh le déanaí a bhailíochtú, rud a thaispeánann go sáraíonn a fheidhmíocht i samhlacha SMT ná modhanna traidisiúnta difríochta tras-eantrópachta agus go leor eile.\nmeaitseálann sé go dlúth le fad na habairte sa chorpas taisc.', 'ka': 'ჩვენ ჩვენ აყენებთ და გამოყენებთ ორი მეტი პრობლემას, რომელიც მაქსინური გაგრძელებაში გამოყენებელი მონაცემების გამოყენების პრობლემა. კლასის განსხვავების მოდელზე მუშაობაში მუშაობაში მუშაობაში მუშაობაში [1], ჩვენ პირველად კლასტერის განსხვავებული მეტი დავიყენებთ, რომელიც Brown კლასტერის გამოყენება, რომ კოპორ მეორე, ჩვენ ვამყენებთ უნიკალური მონაცემების მონიშნული მეტი [2], რომელიც კენტიმენტურად კოპუსს შექმნა, რომელიც უფრო ეფექტიურად მოდელურად მონაცემებ კლასტერის და სინიკალური მონაცემების გამოყენება პირველად გამოყენებულია მაქინის გაგრძელების სისტემაში, და ჩვენ გავაკეთებთ თავიდან თავიდან თავიდან თავიდან. ჩვენი ინტერნექტური განსაზღვრებები აჩვენებენ, რომ ორივე ახალი მეტოვები სტანდარტური მოსურ-ლუისის გადასვლა (კრესენტროპური განსხვავება), უკეთესი პროპლექტი და OOV სიმარ უნიკალური პროგორმა უფრო სიჩვენებულია, რომელიც პირდაპირად ყველა დომინური სიტყვებულაზე 84% უფრო ნაკლები მონაცემებით, ვიდრე სხვა მეტირებით. დამატებით, ახალი პროგრამები შეიძლება გამოიყენება მაქსინური განაცვლის მონაცემებისთვის უკეთესი სისტემებისთვის. ჩვენი წარმოდგენები დარწმუნდება, რომ ბრას კლასტრების გამოყენებით კლასტრების გამოყენება POS-დაბათული კლასტური მეტოვებისთვის ცხოვრებელი ალტენტიფიკატია და გამოყენება სიტყვის ტე დამატებით, ჩვენ შეგვიძლია გავაკეთოთ ახლა მხოლოდ უნიკალური მონაცემების მონიშვნის მეტი, რომელიც ჩვენ გავაკეთებთ, რომ მისი მონაცემებში SMT მოდელში გავაკეთება ტრადიციონალ\nმხოლოდ მოთავსდება საქმე კორპუსის სიგრძე.', 'hu': 'Két módszert mutatunk be és alkalmazunk arra, hogy kezeljük a releváns képzési adatokat egy általános készletből kiválasztjuk az olyan feladatokhoz, mint a gépi fordítás. Az osztály alapú nyelvi különbségmodelleken végzett már meglévő munkákra építve [1] először egy klaszter alapú módszert vezetünk be, amely a barna klasztereket használja a testek szókincsének tömörítésére. Másodszor a cinikus adatválasztási módszert [2] alkalmazzuk, amely fokozatosan felépít egy tréningkorpuszt, hogy hatékonyan modellezze a feladatkorpuszt. Mind a klaszter alapú, mind a cinikus adatválasztási megközelítést első alkalommal használjuk egy gépi fordító rendszerben, és fej-fej összehasonlítást végzünk. Belső értékeléseink azt mutatják, hogy mindkét új módszer felülmúlja a Moore-Lewis szabványos megközelítést (cross-entrópia különbség), ami jobb zavaróságot és OOV arányt illeti a domain belüli adatokon. A cinikus megközelítés sokkal gyorsabban konvergál, és a többi módszernél 84%-kal kevesebb adattal fedi le majdnem az egész domain szókincset. Ezenkívül az új megközelítések alkalmazhatók a gépi fordítási képzési adatok kiválasztására a jobb rendszerek képzéséhez. Eredményeink megerősítik, hogy a Brown klasztereket használó osztályalapú kiválasztás életképes alternatívája a POS-alapú osztályalapú módszereknek, és eltávolítja a beszédrészes címkézőre való támaszkodást. Ezenkívül a nemrégiben javasolt cinikus adatválasztási módszer validálására is képesek vagyunk, bizonyítva, hogy az SMT modellek teljesítménye meghaladja a hagyományos keresztentrópiás különbségek módszereit és egyéb módszereket.\nszorosan illeszkedik a feladatkorpusz mondathosszához.', 'el': 'Παρουσιάζουμε και εφαρμόζουμε δύο μεθόδους αντιμετώπισης του προβλήματος επιλογής σχετικών δεδομένων κατάρτισης από μια γενική ομάδα για χρήση σε εργασίες όπως η μηχανική μετάφραση. Βασιζόμενοι στην υπάρχουσα εργασία σε μοντέλα γλωσσικής διαφοράς βάσει τάξεων [1], εισάγουμε πρώτα μια μέθοδο βασισμένη σε ομάδες που χρησιμοποιεί Brown clusters για να συμπυκνώσει το λεξιλόγιο των σωμάτων. Δεύτερον, εφαρμόζουμε τη μέθοδο κυνικής επιλογής δεδομένων [2], η οποία κατασκευάζει σταδιακά ένα εκπαιδευτικό σώμα για να μοντελοποιήσει αποτελεσματικά το σώμα εργασιών. Τόσο η προσέγγιση που βασίζεται σε ομάδες όσο και η κυνική επιλογή δεδομένων χρησιμοποιούνται για πρώτη φορά μέσα σε ένα σύστημα μηχανικής μετάφρασης και διεξάγουμε μια άμεση σύγκριση. Οι εγγενείς αξιολογήσεις μας δείχνουν ότι και οι δύο νέες μέθοδοι ξεπερνούν την τυπική προσέγγιση Moore-Lewis (διαφορά διασταυρούμενης εντροπίας), όσον αφορά την καλύτερη σύγχυση και τους ρυθμούς OOV σε δεδομένα εντός του τομέα. Η κυνική προσέγγιση συγκλίνει πολύ πιο γρήγορα, καλύπτοντας σχεδόν όλο το λεξιλόγιο του τομέα με 84% λιγότερα δεδομένα από τις άλλες μεθόδους. Επιπλέον, οι νέες προσεγγίσεις μπορούν να χρησιμοποιηθούν για την επιλογή δεδομένων κατάρτισης μηχανικής μετάφρασης για την εκπαίδευση καλύτερων συστημάτων. Τα αποτελέσματά μας επιβεβαιώνουν ότι η επιλογή με βάση τις τάξεις με τη χρήση Brown clusters είναι μια βιώσιμη εναλλακτική λύση στις μεθόδους που βασίζονται στις τάξεις και καταργεί την εξάρτηση από μια ετικέτα μερικού λόγου. Επιπλέον, είμαστε σε θέση να επικυρώσουμε την πρόσφατα προτεινόμενη μέθοδο επιλογής κυνικών δεδομένων, δείχνοντας ότι η απόδοσή της στα μοντέλα ξεπερνά αυτή των παραδοσιακών μεθόδων διαφοράς διασταυρούμενης εντροπίας και άλλα\nταιριάζει στενά με το μήκος της πρότασης του σώματος εργασιών.', 'kk': 'Біз компьютердің аудармасы секілді тапсырмаларда қолдану үшін жалпы бағдарламаларды таңдау мәселесін шешу үшін екі әдістерді көрсету және қолданамыз. Клас негіздеген тіл айырмашылық үлгілерінде барлық жұмыс істеу [1] үшін біріншіден, корпораның сөздігін бағыттау үшін Браун кластерін қолданатын кластердің негіздеген әдісін таңдап берем Екіншіден, біз циникалық деректерді таңдау әдісін [2] іске асырып, тапсырманың корпус үшін оқыту корпус құрылады. Кластер негізінде және циникалық деректерді таңдау кезінде бірінші рет компьютерді аудару жүйесінде қолданылады. Біз басқа-басқа салыстырып тұрмыз. Біздің ішкі оқиғаларымыздың екі жаңа әдістері Муре-Луис стандартты тәртібінен (көпентропиялық айырмашылығы), домен деректерінде жұмыс істеу және OOV жиіліктерінің тең. Киникалық жағдай, домендегі сөздердің барлығын басқа әдістерден 84% дегеннен аз деректерді таңдайды. Қосымша, жүйелерді жақсы оқыту үшін машинаны аудару мәліметін таңдау үшін жаңа арқылы қолданылады. Біздің нәтижелеріміз Браун кластерлерді қолданып класс негізінде таңдау - POS класс негізінде негізінде тұратын әдістерінің альтернативі, және сөйлеу тегтерінің бір бөлігіне сенімдігін өшіруді Сонымен қатар, біз соңғы келтірілген циникалық деректерді таңдау әдісін тексере аламыз. SMT үлгілерінде оның істеу әдістері әдімгі көптеген ентропиялық әдістерін және көптеген\nТапсырма корпусының ұзындығына сәйкес келеді.', 'ml': 'ഒരു പൊതുവായ പൂളില്\u200d നിന്നും പരിശീലനത്തിന്റെ പ്രശ്നം തെരഞ്ഞെടുക്കുന്നതിനുള്ള പ്രശ്നത്തിനുള്ള രണ്ടു രീതികള്\u200d ഞങ്ങള്\u200d  ക്ലാസ്സ് അടിസ്ഥാനത്തുള്ള ഭാഷയുടെ വ്യത്യാസമാതൃകങ്ങളില്\u200d നിലവിലുള്ള ജോലി നിര്\u200dമ്മിക്കുന്നതിന്\u200dറെ [1], ആദ്യം ഞങ്ങള്\u200d ഒരു ക്ലാസ്റ്റര്\u200d  രണ്ടാമതായി, നാം സൈനിക്കല്\u200d ഡേറ്റാ തെരഞ്ഞെടുക്കുന്ന രീതി ക്ലാസ്റ്റര്\u200d അടിസ്ഥാനമാക്കുന്നതും സൈനിക്കല്\u200d ഡേറ്റാ തെരഞ്ഞെടുക്കുന്നതും ആദ്യമായി മെഷീന്\u200d പരിഭാഷണ സിസ്റ്റത്തില്\u200d ഉപയോഗിക് നമ്മുടെ അകത്തുള്ള വിലാസങ്ങള്\u200d കാണിക്കുന്നത് നമ്മുടെ പുതിയ മാര്\u200dഗ്ഗങ്ങള്\u200d സാധാരണ മൂര്\u200d-ലീവിസിന്റെ മാര്\u200dഗ്ഗങ്ങള്\u200d പ്രവര്\u200dത്തിപ്പിക്കുന്നത സൈനിക്കല്\u200d നടപടി വളരെ വേഗത്തില്\u200d മാറുന്നു. കുറച്ച് ഡോമെന്\u200d വാക്കുകള്\u200d മൂലം മാറ്റുന്നു. മറ്റു രീതികളെക്കാള്\u200d 84% കുറഞ്ഞ വ അതിനുശേഷം, മെഷീന്\u200d പരിശീലനത്തിനുള്ള പരിശീലന വിവരങ്ങള്\u200d തെരഞ്ഞെടുക്കാന്\u200d പുതിയ മാറ്റങ്ങള്\u200d ഉപയോഗിക് Our results confirm that class-based selection using Brown clusters is a viable alternative to POS-based class-based methods, and removes the reliance on a part-of-speech tagger.  കൂടുതല്\u200d സൈനിക്കല്\u200d ഡേറ്റാ തെരഞ്ഞെടുക്കുന്ന രീതിയില്\u200d നമുക്ക് പ്രാപ്തികമായി പരിശോധിക്കാന്\u200d കഴിയുന്നു. അതിന്റെ പ്രകടനം SMT മോഡലിലുള്ള\nജോലി കോര്\u200dപ്പുസിന്റെ വാക്കിന്റെ നീളമായി അടുത്ത് പൊരുതുന്നു.', 'lt': 'Mes pristatome ir taikome du metodus, kuriais sprendžiama problem a, susijusi su atitinkamų mokymo duomenų atranka iš bendros grupės, skirtos tokioms užduotims kaip vertimas mašinomis. Remdamiesi esamais klasės kalbų skirtumų modelių kūrimu [1], pirmiausia įvedame klasteriniu metodu, kuriuo naudojami rudi klasteriai kondensuojant korpros žodyną. Antra, įgyvendiname cininių duomenų atrankos metodą [2], kuris palaipsniui sukuria mokymo korpusą, kad būtų veiksmingai modeliuojamas darbo korpusas. Tiek klasteriniu, tiek cininiu duomenų atrankos metodai pirmą kartą naudojami mašinų vertimo sistemoje, ir mes atliekame palyginimą galva į galvą. Mūsų vidutiniai vertinimai rodo, kad abu nauji metodai viršija standartinį Moore-Lewis metodą (tarpentropinis skirtumas), kiek tai susiję su geresniu perplexumu ir OOV rodikliais srities duomenų atžvilgiu. Cininis požiūris konvergencija gerokai greitesnė, apimanti beveik visą domeno žodyną su 84 % mažiau duomenų nei kiti metodai. Be to, norint rengti geresnes sistemas, gali būti naudojami nauji metodai, kaip pasirinkti mašinų vertimo mokymo duomenis. Mūsų rezultatai patvirtina, kad klasės pasirinkimas naudojant ruduosius klasterius yra gyvybinga alternatyva į POS klasės metodus ir pašalina priklausomybę nuo kalbos dalies žymeklio. Be to, galime patvirtinti neseniai pasiūlytą cininių duomenų atrankos metodą, rodantį, kad jo veiksmingumas SMT modeliuose viršija tradicinių tarpentropinių skirtumų metodų ir daugiau\natidžiai atitinka užduoties korpuso sakinio trukmę.', 'it': "Presentiamo e applichiamo due metodi per affrontare il problema della selezione dei dati formativi pertinenti da un pool generale da utilizzare in attività come la traduzione automatica. Basandoci sul lavoro esistente sui modelli di differenza linguistica basati su classi [1], introduciamo per prima cosa un metodo basato su cluster che utilizza i cluster Brown per condensare il vocabolario dei corpora. In secondo luogo, implementiamo il metodo cinico di selezione dei dati [2], che costruisce progressivamente un corpus di formazione per modellare efficacemente il corpus di attività. Sia l'approccio cluster-based che quello cinico della selezione dei dati vengono utilizzati per la prima volta all'interno di un sistema di traduzione automatica, ed eseguiamo un confronto testa a testa. Le nostre valutazioni intrinseche mostrano che entrambi i nuovi metodi superano l'approccio standard Moore-Lewis (differenza tra entropia incrociata), in termini di migliore perplessità e tassi OOV sui dati in-domain. L'approccio cinico converge molto più velocemente, coprendo quasi tutto il vocabolario del dominio con l'84% in meno di dati rispetto agli altri metodi. Inoltre, i nuovi approcci possono essere utilizzati per selezionare i dati di formazione sulla traduzione automatica per formare sistemi migliori. I nostri risultati confermano che la selezione basata su classi utilizzando cluster Brown è una valida alternativa ai metodi basati su classi POS e rimuove la dipendenza da un tag part-of-speech. Inoltre, siamo in grado di convalidare il metodo di selezione cinica dei dati recentemente proposto, dimostrando che le sue prestazioni nei modelli SMT superano quelle dei metodi tradizionali di differenza di entropia incrociata e altro ancora\ncorrisponde strettamente alla lunghezza della frase del corpus di attività.", 'ms': 'Kami mempersembahkan dan melaksanakan dua kaedah untuk mengatasi masalah pemilihan data latihan yang berkaitan dari kolam umum untuk digunakan dalam tugas seperti terjemahan mesin. Membangun pada kerja yang wujud pada model perbezaan bahasa berdasarkan kelas [1], kita pertama-tama memperkenalkan kaedah berdasarkan kelompok yang menggunakan kelompok Brown untuk mengkondensasi vokbulari korpra. Kedua, kita melaksanakan kaedah pemilihan data cinik [2], yang secara bertambah membina sebuah korpus latihan untuk memmodelkan korpus tugas secara efektif. Kedua-dua pendekatan pemilihan data berdasarkan kelompok dan cinik digunakan untuk pertama kalinya dalam sistem terjemahan mesin, dan kita melakukan perbandingan head-to-head. Evaluasi intrinsik kami menunjukkan bahawa kedua-dua kaedah baru melampaui pendekatan Moore-Lewis piawai (perbezaan salib-entropi), dalam terma kekacauan yang lebih baik dan kadar OOV pada data dalam domain. pendekatan cinik berkumpul lebih cepat, meliputi hampir semua vokbulari dalam domain dengan 84% data kurang daripada kaedah lain. Lagipun, pendekatan baru boleh digunakan untuk memilih data latihan terjemahan mesin untuk latihan sistem yang lebih baik. Hasil kami mengesahkan bahawa pemilihan berdasarkan kelas menggunakan kumpulan Brown adalah alternatif yang mudah untuk kaedah berdasarkan kelas berdasarkan POS, dan menghapuskan kepercayaan pada sebahagian-dari-ucapan tag. Selain itu, kita boleh sahkan kaedah pemilihan data cinik yang baru-baru ini diusulkan, menunjukkan bahawa prestasinya dalam model SMT melebihi kaedah perbezaan salib entropi tradisional dan lebih\nsepadan dengan panjang kalimat bagi korpus tugas.', 'mk': 'Презентираме и аплицираме два методи за решавање на проблемот со изборот на релевантни податоци за обука од општиот базен за употреба во задачи како што е машинскиот превод. Користејќи ја постојаната работа на моделите на разлика на јазикот на класа [1], прво воведуваме метод базиран на групи кој користи Браун групи за кондензирање на речникот на капората. Второ, го спроведуваме методот на селекција на цинички податоци [2], кој постојано изградува обукен корпус за ефикасно да го моделира задачниот корпус. И пристапите за селекција на податоци базирани на групи, и циничките се користат за прв пат во системот на машински превод, и ние вршиме споредба глава во глава. Нашите внатрешни проценки покажуваат дека двата нови методи го надминуваат стандардниот пристап Мур-Луис (разлика во крстоентропијата), во поглед на подобра перфектност и стапки ООВ на податоците во доменот. Циничкиот пристап се приближува многу побрзо, покривајќи речиси целиот речник во доменот со 84 отсто помалку податоци од другите методи. Покрај тоа, новите пристапи може да се користат за избор на податоци за обука за машински превод за обука на подобри системи. Нашите резултати потврдуваат дека селекцијата базирана на класата користејќи го Браун кластерите е жива алтернатива на методите базирани на класата на POS, и ја отстранува зависноста од дел од говорот. Покрај тоа, можеме да го потврдиме неодамна предложениот метод за селекција на цинички податоци, покажувајќи дека неговата резултат во СМТ моделите го надминува она на традиционалните методи на разлика во крстоентропијата и повеќе\nclosely matches the sentence length of the task corpus.', 'mn': 'Бид машины орчуулалт шиг ажилд хэрэглэх боломжтой нийтийн сургалтын өгөгдлийг сонгох асуудлыг олох хоёр арга зам ашиглаж байна. Бид ангид суурилсан хэл ялгаатай загвар дээр суурилсан ажил дээр байгуулсан [1], эхлээд бид Браун кластеруудыг корпораны үгийг зориулахын тулд хэрэглэдэг кластеруудын суурилсан арга загварыг танилцуулдаг. Хоёрдугаарт, бид Циникийн өгөгдлийн сонголтын аргыг [2] хэрэгжүүлдэг. Энэ нь ажлын корпус-г үр ашигтай загварчлахын тулд сургалтын корпус үүсгэдэг. Кластер суурилсан болон циник өгөгдлийн сонголтын тулд машины орчуулах системийн дотор анхны удаа хэрэглэгддэг. Бид толгойд толгойд нь толгойд харьцуулж байна. Бидний дотоод шинэ арга барилга нь Мур-Луисийн стандарт арга барилга (эсрэг энтропийн ялгаа), илүү төвөгтэй байдал болон ООВ-ын тоо барилгын хувьд илүү төвөгтэй гэдгийг харуулдаг. Циникийн арга баримтууд бусад аргаас 84% бага өгөгдлийг илүү хурдан холбогддог. Дараа нь шинэ арга зам нь илүү сайн сургалтын төлөө машины хөгжлийн дасгал өгөгдлийг сонгоход ашиглаж болно. Бидний үр дүнд Браун кластеруудыг ашиглан хичээл дээр суурилсан сонголт нь POS-д суурилсан хичээл дээр суурилсан арга замыг ашиглаж, илтгэлийн нэг хэсэг дээр итгэлтэй байдлыг устгаж байна. Мөн бид саяхан шинэ санал дэвшүүлсэн циник өгөгдлийн сонголтын аргыг шалгаж чадна. Энэ нь SMT загварын үйл ажиллагаа нь уламжлалтай эсрэг энтропийн ялгааны арга болон илүү олон төрлийн арга загваруудыг\nажлын корпусын үгийг ойролцоогоор тэнцүү.', 'pl': 'Przedstawiamy i stosujemy dwie metody rozwiązania problemu wyboru odpowiednich danych szkoleniowych z ogólnej puli do wykorzystania w zadaniach takich jak tłumaczenie maszynowe. Opierając się na istniejących pracach nad klasowymi modelami różnic językowych [1], wprowadzamy najpierw metodę klastrową, która wykorzystuje klastry brązowe do skondensowania słownictwa korpusów. Po drugie, wdrażamy metodę cynicznej selekcji danych [2], która stopniowo buduje korpus szkoleniowy w celu efektywnego modelowania korpusu zadań. Zarówno klastrowe, jak i cyniczne metody selekcji danych są wykorzystywane po raz pierwszy w systemie tłumaczenia maszynowego i przeprowadzamy porównanie head-to-head. Nasze wewnętrzne oceny pokazują, że obie nowe metody przewyższają standardowe podejście Moore-Lewis (różnica w entropii krzyżowej), pod względem lepszego zdezorientowania i współczynnika OOV na danych wewnątrz domeny. Cyniczne podejście zbiega się znacznie szybciej, obejmując niemal całe słownictwo w domenie z 84% mniejszą ilością danych niż inne metody. Ponadto nowe podejścia mogą być wykorzystane do wyboru danych szkoleniowych z tłumaczenia maszynowego w celu szkolenia lepszych systemów. Nasze wyniki potwierdzają, że selekcja oparta na klasach za pomocą klastrów Brown jest realną alternatywą dla metod opartych na klasach POS i eliminuje zależność od tagera części mowy. Dodatkowo jesteśmy w stanie zweryfikować niedawno zaproponowaną metodę cynicznego selekcji danych, pokazując, że jej wydajność w modelach SMT przewyższa tradycyjne metody różnicy krzyżowej i więcej\nściśle pasuje do długości zdania korpusu zadania.', 'no': 'Vi presenterer og bruker to metodar for å handsama problemet for å velja relevant øvingsdata ut av eit generell pool for bruk i oppgåver som maskineoversettelse. Bygger vi først på eksisterande arbeid på klassebaserte språk-forskjellingsmodeller [1], introduserer vi ein klassebasert metode som brukar Brown-clusters for å kondensera ordboka i korporen. I andre, implementerer vi den cynical datautvalmetoden [2], som inkrementalt konstruerer eit øvingskorpus for å gjere effektivt modell oppgåvekorpusen. Både klosterabaserte og det cynicalske datautvalet blir brukt for første gong i eit maskinsomsetjingssystem, og vi gjer ei sammenligning med hovud-til-hode. Våre interne evalueringar viser at begge nye metodar utfører standardinnstillingane Moore-Lewis (cross-entropy difference), i uttrykket av betre kompleksiteten og OOV-rate på inndomenedata. Cynical approach converges much faster, covering nearly all the in-domain vocabulary with 84% less data than the other methods. I tillegg kan dei nye tilnærmingane brukast for å velja datafor for opplæring av maskineomsetjingar for betre systemer. Resultatet våre stadfestar at klassebasert utval ved bruk av Brown-kluster er ein viktig alternativ for POS-baserte klassebaserte metoder, og fjernar tilbakekallinga på ein del av talemerker. I tillegg kan vi validera den nyleg foreslåde cynical data-utval metoden, som viser at utviklinga i SMT-modeller overpassar den tradisjonelle kryssentropiske forskjellingsmetodane og meir\npassar nærare med setninga på oppgåvekorpusen.', 'sr': 'Predstavljamo i primjenjujemo dve metode za rješavanje problem a izabranja relevantnih podataka obuke iz općeg bazena za upotrebu u zadataka poput prevoda strojeva. Na temelju postojećeg rada na modelima razlike jezika na klasi [1], prvo predstavljamo metodu na bazi grupa koja koristi Browne skupove kako bi kondenzirali rečnik korpore. Drugo, implementiramo metodu selekcije ciničkih podataka [2], koja povećavajući konstruira trening korpus kako bi efikasno modelio zadatak korpus. Oboje pristupe za izbor ciničkih podataka se koriste za prvi put u sistemu prevoda mašine, a mi obavljamo usporedbu glave na glavu. Naše unutrašnje procjene pokazuju da obe nove metode iznose standardni pristup Moore-Lewisa (krsnoentropijska razlika), u smislu boljih kompleksnosti i stopa OOV-a na podacima u domenu. Cinički pristup se zbližava mnogo brže, pokrivajući skoro sve rečnike u domenu sa 84% manjim podacima od drugih metoda. Osim toga, novi pristupi se mogu iskoristiti za izabranje podataka za obuku automatskih prevoda za bolji sustav obuke. Naši rezultati potvrđuju da je selekcija bazirana na klasi koristeći Browne skupine održiva alternativa za metode bazirane na klasi na POS-u i uklanja pouzdanost na deo govornog značka. Osim toga, mi smo u mogućnosti da potvrdimo nedavno predloženu metodu izbora ciničkih podataka, pokazujući da njegova učinka u modelima SMT-a prelazi tradicionalne metode razlike preko entropije i više\nblizu odgovara dužini rečenice poslovnog korpusa.', 'mt': 'Aħna nippreżentaw u napplikaw żewġ metodi biex nindirizzaw il-problem a tal-għa żla tad-dejta rilevanti tat-taħriġ minn ġabra ġenerali għall-użu f’kompiti bħat-traduzzjoni bil-magna. Abbażi tax-xogħol eżistenti fuq mudelli tad-differenzi lingwistiċi bbażati fuq il-klassi [1], l-ewwel a ħna nintroduċu metodu bbażat fuq raggruppament li juża raggruppamenti kannella biex jikkondensaw il-vokabulari tal-korpora. It-tieni nett, nimplimentaw il-metodu ċiniku tal-għa żla tad-dejta [2], li inkrementalment jibni korpus ta’ taħriġ biex jimmudella b’mod effiċjenti l-korpus ta’ ħidma. Kemm l-approċċi tal-għa żla tad-dejta bbażati fuq ir-raggruppament kif ukoll dawk ċiniċi jintużaw għall-ewwel darba fi ħdan sistema ta’ traduzzjoni tal-magna, u nagħmlu paragun minn ras għal oħra. L-evalwazzjonijiet intrinsiċi tagħna juru li ż-żewġ metodi ġodda jaqbżu l-approċċ standard Moore-Lewis (differenza bejn l-entropija), f’termini ta’ perplessità aħjar u rati OOV fuq id-dejta fid-dominju. L-approċċ ċiniku jikkonverġi ħafna aktar malajr, li jkopri kważi l-vokabulari kollha fid-dominju b’84 % inqas dejta mill-metodi l-oħra. Barra minn hekk, l-approċċi l-ġodda jistgħu jintużaw biex tintgħażel id-dejta tat-taħriġ tat-traduzzjoni bil-magna għat-taħriġ ta’ sistemi aħjar. Ir-riżultati tagħna jikkonfermaw li l-għa żla bbażata fuq il-klassi bl-użu ta’ raggruppamenti Brown hija alternattiva vijabbli għal metodi bbażati fuq il-klassi bbażati fuq POS, u tneħħi d-dipendenza fuq tagger ta’ parti mid-diskors. Barra minn hekk, nistgħu nivvalidaw il-metodu ta’ għażla tad-dejta ċinika propost dan l-aħħar, li juri li l-prestazzjoni tiegħu fil-mudelli SMT taqbeż dik tal-metodi tradizzjonali ta’ differenza bejn l-entropija u aktar\njaqbel mill-qrib mat-tul tas-sentenza tal-task corpus.', 'ro': 'Prezentăm și aplicăm două metode pentru abordarea problemei selectării datelor relevante de formare dintr-o bază generală pentru utilizarea în sarcini precum traducerea automată. Bazându-ne pe lucrările existente pe modelele diferențelor de limbă bazate pe clasă [1], introducem mai întâi o metodă bazată pe cluster care utilizează clusterele Brown pentru a condensa vocabularul corporelor. În al doilea rând, implementăm metoda cinică de selecție a datelor [2], care construiește treptat un corpus de instruire pentru a modela eficient corpul de sarcini. Atât abordarea bazată pe cluster, cât și cea cinică a selecției datelor sunt utilizate pentru prima dată în cadrul unui sistem de traducere automată și efectuăm o comparație directă. Evaluările noastre intrinsece arată că ambele metode noi depășesc abordarea standard Moore-Lewis (diferența intropiei încrucișate), în ceea ce privește o mai bună perplexitate și rate OOV pe datele din domeniu. Abordarea cinică converge mult mai rapid, acoperind aproape tot vocabularul din domeniu cu 84% mai puține date decât celelalte metode. În plus, noile abordări pot fi utilizate pentru a selecta datele de instruire în traducerea automată pentru a instrui sisteme mai bune. Rezultatele noastre confirmă faptul că selecția bazată pe clase utilizând clustere Brown este o alternativă viabilă la metodele bazate pe clase POS și elimină dependența de un etichetor part-of-speech. În plus, suntem capabili să validăm metoda recent propusă de selecție cinică a datelor, arătând că performanța sa în modelele SMT depășește cea a metodelor tradiționale de diferență de entropie încrucișată și mai mult\ncorespunde îndeaproape cu lungimea propoziției corpului de sarcini.', 'so': 'Waxaannu soo wadannaa oo u codsanaynaa laba qaab, si aan u xambaarano dhibaatada doorashada macluumaadka la xiriira laga doorto balliga guud ee lagu isticmaalo shaqooyin tusaale ahaan turjumaadda machine. Bulshada shaqada oo ku qoran noocyada kala duwan luuqada fasalka [1], marka ugu horeysa waxaynu soo bandhignaa qaab ku saleysan luqada oo lagu isticmaalo kooxo Brown si uu u sameeyo warqada shirkadda. Second, waxaynu sameynaa qaababka doorashada macluumaadka habaarka ah[2], taasoo dhisaya koronta waxbarashada si ay u sameyn karto kooxda shaqada si fiican. Inta ugu horraysa waxaa loo isticmaalaa habka turjumaadda machineedka, waxaana sameynaa isbarbardhigga madaxa. Qiimeynta gudaha ah waxay muuqataa in labada qaab cusub ay ka samaystaan qaababka caadiga ah ee Moore-Lewis (kala duduwanka koowaad), si ka fiican muranka iyo qiimaha OOV ee macluumaadka gudaha ku jira. Dhaqdhaqaaqa xiliga ah waxay u bedeshaa si dhaqso u dhaqso, kaas oo ku daboolaya macluumaadka afka gudaha oo dhan, wuxuuna ku qoraa 84% ka yar macluumaadka kale. Intaas waxaa dheer oo loo isticmaali karaa hababka cusub in lagu doorto macluumaadka waxbarashada turjumista machine si loo barayo nidaamka aad u wanaagsan. Fasalkayaga waxay xaqiijiyaan in doorashada fasalka lagu isticmaalo kooxaha Brown waa mid ka bedel kara qaababka fasalka ku saleysan POS, wuxuuna ka ridaa ku tiirsashada qeyb ka mid ah warqadaha hadalka. Sidoo kale waxaynu awoodi karnaa inaannu xaqiijinno qaababka doorashada macluumaadka ee ugu dhowaad ee la soo jeeday, waxaynu tusnaynaa in dhaqdhaqaalaha sameynta SMT ay ka kooban tahay qaababka kala duwan ee caadiga ah ee suuqsinimada\nsi aad u dhow ayuu u eg yahay dhererka shaqada.', 'si': 'අපි ප්\u200dරශ්නයක් තියෙන්නේ සමාන්\u200dය ප්\u200dරශ්නයක් තෝරාගන්න ප්\u200dරශ්නයක් සඳහා ප්\u200dරශ්නයක් තෝරාගන්න සඳහා ප්\u200dරශ ප්\u200dරදේශ භාෂාව වෙනස් මොඩේල් වලින් තියෙන්නේ වැඩේ ඉන්නේ [1], අපි මුලින්ම ප්\u200dරධාන විදේශයක් ප්\u200dරවේශ කරනවා බ්\u200dරාන් ක්ලාස්ට දෙවෙනි විදියට, අපි සිනිකාල දත්ත තෝරණය විදියට ප්\u200dරවේශනය කරනවා [2], ඒක විශේෂයෙන් ප්\u200dරවේශනය කර්පස් එකක් හදන්නේ ව මැෂින් වාර්ථාව පද්ධතියක් ඇතුලට පළමුවෙනි වතාවක් පාවිච්චි කරනවා, අපි ඔළුවෙන් ඔළුවෙන් ඔළුවෙන් වාර්ථාව කර අපේ ඇතුළත් අවශ්\u200dය විශ්ලේෂණය පෙන්වන්නේ අළුත් විදිහා දෙන්නම මූර්-ලූවිස් ප්\u200dරමාණය (ක්\u200dරිස්ටෙන්ට්\u200dරොපි වෙනස්), හොඳ සං සායිනිකාලික ප්\u200dරවේශය ගොඩක් වේගයෙන් සම්බන්ධ වෙනවා, අනිත් ප්\u200dරවේශයෙන් 84% ටිකක් තොරතුරු තොරතුරු තියෙනවා. ඊට පස්සේ, අලුත් විදිහට ප්\u200dරයෝජනය කරන්න පුළුවන් පද්ධතිය හොඳ පද්ධතියෙන් පරීක්ෂණය කරන්න පද්ධත අපේ ප්\u200dරතිචාර ප්\u200dරතිචාරයක් ස්ථානයින් විශ්වාස කරන්න ප්\u200dරතිචාරයක් ප්\u200dරයෝජනය කරනවා කියලා ප්\u200dරතිචාරයක් ප්\u200dරයෝජනය කරනවා  තවත්, අපිට පුළුවන් අවස්ථානයේ සිනිකාල් දත්ත තෝරාගැනීමේ විදියට පරීක්ෂා කරන්න පුළුවන් විදියට, පෙන්වන්නේ SMT මොඩේල\nවාර්ථාව කොර්පුස් වලගේ වාක්ය ප්\u200dරමාණය සම්බන්ධයි.', 'sv': 'Vi presenterar och tillämpar två metoder för att hantera problemet med att välja relevanta utbildningsdata ur en allmän pool för användning i uppgifter som maskinöversättning. Baserat på befintligt arbete med klassbaserade språkdifferensmodeller [1], introducerar vi först en klusterbaserad metod som använder bruna kluster för att kondensera ordförrådet i korporarna. För det andra implementerar vi den cyniska dataurvalsmetoden [2], som stegvis konstruerar en träningskorpus för att effektivt modellera arbetskorpusen. Både klusterbaserade och cyniska dataval används för första gången inom ett maskinöversättningssystem, och vi gör en head-to-head jämförelse. Våra inneboende utvärderingar visar att de båda nya metoderna överträffar Moore-Lewis standard-metoden (cross-entropi difference), i termer av bättre perplexitet och OOV frekvenser på in-domain data. Det cyniska tillvägagångssättet konvergerar mycket snabbare och täcker nästan hela domänordförrådet med 84% mindre data än de andra metoderna. Dessutom kan de nya metoderna användas för att välja ut utbildningsdata för maskinöversättning för att utbilda bättre system. Våra resultat bekräftar att klassbaserat urval med bruna kluster är ett livskraftigt alternativ till POS-baserade klassbaserade metoder, och tar bort beroende av en del-av-tal taggare. Dessutom kan vi validera den nyligen föreslagna cyniska dataurvalsmetoden, vilket visar att dess prestanda i SMT-modeller överträffar traditionella cross-entropi difference metoder och mer\nmatchar meningslängden för uppgiftskorpusen.', 'ta': 'கணினி மொழிபெயர்ப்பு போன்ற பணிகளில் பயன்படுத்துவதற்கான பொதுவான துரிய பயிற்சி தகவலை தேர்ந்தெடுக்கும் பிரச்சனையை நாம வகுப்பு அடிப்படையில் உள்ள மொழி வேறுபாடு மாதிரிகளில் உள்ள வேலை உருவாக்குதல் முதலில் நாம் ஒரு க்ளூஸ்டர் அடிப்படையான முறையை குறிக்கும் ப இரண்டாவது, நாம் சைனிக் தரவு தேர்வு முறைமையை செயல்படுத்துகிறோம் [2], அது வேலை குறியீட்டை வெளிப்படையாக மாதிரியும் பயிற்சி க கணினி மொழிபெயர்ப்பு அமைப்பிற்கு முதல் முறையாக பயன்படுத்தப்படுகிறது, மற்றும் நாம் தலைப்பு தலைப்பு ஒப்பீடு செய்கிறோம். நம்முடைய உள்ளிருக்கும் மதிப்புகள் காண்பிக்கப்பட்டுள்ளது என்றால் இரண்டு புதிய முறைகளும் நிலையான நூர்- லீவிஸ் நெறிமுறையை விட செயல்படுத் சைனிக் செயல்பாடு மிகவும் வேகமாக மாறுகிறது, கிட்டத்தட்ட அனைத்து டோமைன் சொல்வளத்தை மூடுகிறது, மற்ற முறைமைகளை விட 84% கு அதற்கும், புதிய முறைமைகளை பயிற்சி சிறந்த அமைப்புகளுக்கு இயந்திர மொழிபெயர்ப்பு தகவலை தேர்ந்தெடு எங்கள் முடிவுகள் பிரான் கிளாஸ் கிளாஸ்டரை பயன்படுத்தி வகுப்பு தேர்வு உறுதிப்படுத்துகிறது போஸ் அடிப்படையிலான வகுப்பு முறைகளுக்கு ஒரு வி மேலும், நாம் சமீபத்தில் பரிந்துரைக்கப்பட்ட சைனிக்கல் தரவு தேர்வு முறைமையை சரிபார்க்க முடியும், அது SMT மாதிரிகளில் செயல்படுத்தும் பாரம்பர\nவேலைக்குறியீட்டின் வாக்கு நீளம் பொருந்தும்.', 'ur': 'ہم نے دو طریقے پیش کیے ہیں اور دو طریقے استعمال کیے ہیں کہ مسئلہ کی تعلیم دادہ کو ایک عمومی پول سے استعمال کریں جیسے ماشین ترجمن کے مطابق استعمال کیے جاتے ہیں۔ کلاس کی بوسیدہ زبان کے متفاوت موڈل پر موجود کام بنانے کے لئے [1] ہم پہلے ایک کلسٹر بنیادی طریقہ کو معلوم کرتے ہیں جو براون کلسٹر کا استعمال کرتا ہے کہ کورپورا کے لکھنے کے لئے کاندنس کرے. دوسرا، ہم نے سینیک ڈیٹا انتخاب طریقہ کو عملہ کر دیا ہے [2], جس نے اضافہ طور پر ایک ٹرینگ کورپوس بنایا ہے تابع کورپوس کی مدل کے لئے. کلسٹر پر بنیاد ہے اور سینیک ڈیٹا انتخاب کے طریقے پہلی بار ایک ماشین ترجمہ سیسٹم کے اندر استعمال کیے جاتے ہیں، اور ہم سر-سر-سر مقایسہ کرتے ہیں. ہماری داخلی تحقیقات دکھاتی ہے کہ دونوں نئی طریقے مور-لوئیس کی طریقہ سے زیادہ استاندارد ہیں (کرس-انٹروپی تفاوت) اچھی پرپرلکتی اور OOV رخصت دامنی پر۔ سینیک طریقہ بہت تیز ترکیب کرتا ہے، قریب ہے کہ دومین میں تمام آواز شناسی کے ساتھ 84% کم ڈیٹا دوسرے طریقے سے چھپاتے ہیں۔ اور اس کے علاوہ، نئی طریقے استعمال کر سکتے ہیں کہ مشین ترینس ترینس ڈیٹا انتخاب کرنے کے لئے بہتر سیستموں کے لئے استعمال کر سکتے ہیں. ہمارے نتیجے مطمئن ہیں کہ براون کلسٹر کے استعمال سے کلاس بنیاد گزینے کا ایک قابل اختیار ہے POS بنیاد کلاس بنیاد رکھے ہوئے طریقے، اور بات ٹیجر کے ایک حصہ پر اعتماد کو ہٹا دیتا ہے. اور اضافہ، ہم نے اچھے سے پیشنهاد کی سینیکی ڈیٹ انتخاب طریقے کی تصدیق کر سکتے ہیں، دکھاتے ہیں کہ اس کی عملکرد SMT موڈلوں میں بہت زیادہ گزر جاتی ہے\nتابع کورپوس کے مجلس کی مدت کے مطابق مطابق ہے۔', 'uz': "Biz hozir qilamiz va mashina tarjima qiladigan vazifalardan foydalanish uchun muhim ta'lim sohasini tanlash uchun ikkita usuldan qoʻllamiz. Faylning birinchi darajadagi tillar o'zgarishga ishni yaratishimiz, birinchi marta, bu kompaniya soʻzni boshqarish uchun Brown clusterlardan foydalanishi mumkin. Ikkinchi so'zda, biz o'sha vazifa kompyuteriga tayyor model qilish uchun foydalanish usulini bajaramiz. Name Bizning ichki qiymatlarimizni ko'rsatishimiz mumkin, yangi usullar bu ikkita oddiy Moore-Lewis usulida (cross-entropy ўзгартиarini) bajaradi va domen maʼlumotlarida yaxshi murakkablik va OOV ratesi. Name Koʻrsatilgan, yangi qoidalar yaxshi tizimni tahrirlash uchun mashina tarjima maʼlumotni tanlash uchun foydalanadi. Bizning natijalarimiz Brown clusterlarni ishlatish uchun sinfning asosida tanlash mumkin, POS asosidagi sinfning asosida o'zgarishga ishonchini olib tashlaydi va gapirish yordamchisining qismini olib tashlash mumkin. Ko'pchilik, biz yaqinda ilova qilingan siniq maʼlumot tanlash usulini bajarishimiz mumkin, bu SMT modelidagi amalni o'xshash o'zgarishga o'zgartiradi va ko'proq o'zgarishni o'zgartiradi.\nvazifa qo'shishlarining o'zgarishga murojaat qiladi.", 'vi': 'Chúng tôi giới thiệu và áp dụng hai phương pháp để giải quyết vấn đề lựa chọn dữ liệu giáo dục liên quan ra khỏi một hồ sơ chung để sử dụng trong các công việc như dịch thuật máy. Dựa trên công trình dựa trên các mô hình ngôn ngữ khác nhau tại lớp[1] Đầu tiên chúng tôi giới thiệu một phương pháp dựa vào cụm thiên văn dùng cụm Trường Nâu để kết hợp các cụm từ của cơ thể. Thứ hai, chúng ta thực hiện phương pháp bí ẩn lựa chọn dữ liệu (2), mà xây dựng dần một tập thể huấn để mô hình hiệu quả tập đoàn các nhiệm vụ. Cả các phương pháp lựa chọn dữ liệu dựa trên cụm và hoài nghi được sử dụng lần đầu tiên trong một hệ thống dịch chuyển máy, và chúng tôi tiến hành so sánh từ đầu tới đầu. Bản đánh giá của chúng tôi cho thấy cả hai phương pháp mới vượt qua tiêu chuẩn Moore-Lewis (khác nhau về phương trình tồn tại khác nhau, về mặt phức tạp hơn và giá OOOV trên dữ liệu nội bộ. Sự hoài nghi của chúng ta hội tụ nhanh hơn nhiều, bao gồm gần hết các từ điển trong miền với 84=.* ít dữ liệu hơn các phương pháp khác. Hơn nữa, các phương pháp mới có thể được dùng để chọn dữ liệu đào tạo dịch chuyển máy để đào tạo hệ thống tốt hơn. Những kết quả của chúng tôi xác nhận việc chọn dựa vào lớp Brown sẽ là một sự thay đổi khả thi cho các phương pháp dựa trên lớp POS, và xóa bỏ sự dựa dẫm vào một phần của câu khẩu phần. Thêm vào đó, chúng tôi có thể xác nhận phương pháp bí ẩn chọn dữ liệu đã được đề nghị gần đây, cho thấy hiệu quả của nó trong mô hình SMT vượt trội so với các phương pháp khác nhau truyền thống và nhiều hơn\nKhớp với độ dài của tập đoàn này.', 'de': 'Wir präsentieren und wenden zwei Methoden an, um das Problem der Auswahl relevanter Trainingsdaten aus einem allgemeinen Pool für Aufgaben wie maschinelle Übersetzung anzugehen. Aufbauend auf bestehenden Arbeiten an klassenbasierten Sprachdifferenzmodellen [1] stellen wir zunächst eine clusterbasierte Methode vor, die Brown Cluster verwendet, um das Vokabular der Korpora zu verdichten. Zweitens implementieren wir die zynische Datenauswahl [2], die inkrementell einen Trainingskorpus konstruiert, um den Aufgabenkorpus effizient zu modellieren. Sowohl der clusterbasierte als auch der zynische Datenauswahlansatz werden erstmals innerhalb eines maschinellen Übersetzungssystems eingesetzt und wir führen einen direkten Vergleich durch. Unsere intrinsischen Auswertungen zeigen, dass beide neuen Methoden den Standard-Moore-Lewis-Ansatz (Cross-Entropie Difference) hinsichtlich besserer Verwirrung und OOV-Raten auf In-Domain-Daten übertreffen. Der zynische Ansatz konvergiert viel schneller und deckt fast das gesamte In-Domain Vokabular mit 84% weniger Daten ab als die anderen Methoden. Darüber hinaus können die neuen Ansätze genutzt werden, um Trainingsdaten für maschinelle Übersetzungen auszuwählen, um bessere Systeme zu trainieren. Unsere Ergebnisse bestätigen, dass klassenbasierte Selektion mit Brown Clustern eine praktikable Alternative zu POS-basierten klassenbasierten Methoden ist und die Abhängigkeit von einem Teil-der-Sprache-Tagger beseitigt. Darüber hinaus können wir die kürzlich vorgeschlagene zynische Datenauswahl validieren und zeigen, dass ihre Leistung in SMT-Modellen die von traditionellen Cross-Entropie-Differenzmethoden und mehr übertrifft.\nentspricht der Satzlänge des Aufgabenkorpus.', 'nl': 'We presenteren en passen twee methoden toe om het probleem van het selecteren van relevante trainingsgegevens uit een algemene pool aan te pakken voor taken zoals machinevertaling. Voortbouwend op bestaand werk aan class-based language difference modellen [1], introduceren we eerst een cluster-based methode die Brown clusters gebruikt om de woordenschat van de corpora te verdichten. Ten tweede implementeren we de cynische gegevensselectiemethode [2], die incrementel een trainingscorpus construeert om het taakcorpus efficiënt te modelleren. Zowel de cluster-gebaseerde als de cynische dataselectie benaderingen worden voor het eerst gebruikt binnen een machinevertaalsysteem en we voeren een head-to-head vergelijking uit. Onze intrinsieke evaluaties tonen aan dat beide nieuwe methoden beter presteren dan de standaard Moore-Lewis-benadering (cross-entropieverschil), wat betreft betere verwarring en OOV-percentages op in-domein data. De cynische benadering convergeert veel sneller en bestrijkt bijna alle in-domein woordenschat met 84% minder gegevens dan de andere methoden. Bovendien kunnen de nieuwe benaderingen worden gebruikt om trainingsgegevens voor machinevertaling te selecteren voor het trainen van betere systemen. Onze resultaten bevestigen dat class-based selectie met Brown clusters een haalbaar alternatief is voor POS-gebaseerde class-based methodes, en de afhankelijkheid van een part-of-speech tagger wegneemt. Daarnaast zijn we in staat om de recent voorgestelde cynische gegevensselectiemethode te valideren, waaruit blijkt dat de prestaties in SMT-modellen overtreffen die van traditionele cross-entropieverschilmethoden en meer\nkomt overeen met de lengte van de zin van het taakcorpus.', 'hr': 'Predstavljamo i primjenjujemo dvije metode za rješavanje problem a izbora relevantnih podataka obuke iz općeg bazena za upotrebu u zadataka poput prevoda strojeva. Na temelju postojećeg rada na modelima razlike jezika osnovanih na klasi [1], prvo predstavljamo metodu baziranu na skupini koji koristi Brown skupine kako bi se kondenzirali riječ tijela. Drugo, implementiramo metodu izbora ciničkih podataka [2], koja povećavajući konstruira trening korpus kako bi učinkovito modelirao zadatak korpus. Oba pristupa za izbor ciničkih podataka se primjenjuju prvi put u sustavu prevoda strojeva i uspoređujemo glavu na glavu. Naše unutrašnje procjene pokazuju da obje nove metode nadmađuju standardni pristup Moore-Lewisa (razlika kroz entropiju), u smislu boljih kompleksnosti i stopa OOV-a na podacima u domenu. Cinički pristup se zbližava mnogo brže, pokrivajući gotovo sve riječnike u domenu s 84% manjim podacima od drugih metoda. Osim toga, novi pristupi se mogu koristiti za odabrati podatke o obuci strojeva za bolji sustav obuke. Naši rezultati potvrđuju da je izbor baziran na klasi koristeći Browne skupine održiva alternativa metodi baziranih na klasi POS-u i uklanja pouzdanost na dijelom govornog značka. Osim toga, mi smo u mogućnosti potvrditi nedavno predloženu metodu izbora ciničkih podataka, pokazujući da njegova učinka u modelima SMT-a nadmaže tradicionalne metode razlike preko entropije i više\nblizu odgovara dužini kazne zadatka korpusa.', 'da': 'Vi præsenterer og anvender to metoder til at løse problemet med at udvælge relevante uddannelsesdata ud af en generel pulje til brug i opgaver som maskinoversættelse. Med udgangspunkt i det eksisterende arbejde med klassebaserede sprogforskellmodeller [1] introducerer vi først en klyngebaseret metode, der bruger brune klynger til at kondensere korporaernes ordforråd. For det andet implementerer vi den kyniske dataudvælgelsesmetode [2], som gradvist konstruerer et træningskorpus til effektivt at modellere opgavekorpset. Både klyngebaserede og kyniske dataudvælgelsesmetoder anvendes for første gang i et maskinoversættelsessystem, og vi foretager en head-to-head sammenligning. Vores iboende evalueringer viser, at begge nye metoder overgår standard Moore-Lewis metoden (cross-entropi difference), hvad angår bedre forvirring og OOV rater på in-domæne data. Den kyniske tilgang konvergerer meget hurtigere og dækker næsten hele domænenes ordforråd med 84% færre data end de andre metoder. Desuden kan de nye fremgangsmåder anvendes til at udvælge maskinoversættelsesdata til uddannelse af bedre systemer. Vores resultater bekræfter, at klassebaseret udvælgelse ved hjælp af brune klynger er et levedygtigt alternativ til POS-baserede klassebaserede metoder, og fjerner afhængigheden af en del-of-tale tagger. Derudover er vi i stand til at validere den nyligt foreslåede kyniske dataudvælgelsesmetode, der viser, at dens ydeevne i SMT modeller overgår traditionelle cross-entropi difference metoder og mere\nsvarer nøje til sætningslængden af opgavekorpset.', 'id': 'Kami mempersembahkan dan menerapkan dua metode untuk mengatasi masalah memilih data latihan relevan dari kolam umum untuk digunakan dalam tugas seperti terjemahan mesin. Berdasarkan pekerjaan yang ada pada model perbedaan bahasa berdasarkan kelas [1], kami pertama-tama memperkenalkan metode berdasarkan kelompok yang menggunakan kelompok Brown untuk mengkondensasi vokbulari dari korpora. Kedua, kami mengimplementasikan metode pemilihan data cinis [2], yang secara bertambah-turut membangun sebuah korpus latihan untuk memadel korpus tugas secara efisien. Kedua pendekatan pemilihan data berdasarkan klaster dan cinis digunakan untuk pertama kalinya dalam sistem terjemahan mesin, dan kita melakukan perbandingan kepala ke kepala. Evaluasi intrinsik kami menunjukkan bahwa kedua metode baru melebihi pendekatan standar Moore-Lewis (perbedaan interentropi), dalam terma kekacauan yang lebih baik dan kadar OOV pada data dalam domain. pendekatan cinis konvergesi jauh lebih cepat, menutupi hampir semua vokabular dalam domain dengan 84% data kurang dari metode lain. Selain itu, pendekatan baru dapat digunakan untuk memilih data pelatihan terjemahan mesin untuk melatih sistem yang lebih baik. Hasil kami mengkonfirmasi bahwa seleksi berdasarkan kelas menggunakan kelas Brown adalah alternatif yang dapat dihidupkan untuk metode berdasarkan kelas POS, dan menghapus ketergantuan pada bagian dari pembicaraan tagger. Selain itu, kita dapat mengkvalifikasi metode pemilihan data cinis yang baru-baru ini diusulkan, menunjukkan bahwa prestasinya dalam model SMT melebihi metode perbedaan transentropi tradisional dan lebih\nsangat cocok dengan panjang kalimat dari task corpus.', 'bg': 'Представяме и прилагаме два метода за решаване на проблема с подбора на подходящи данни за обучение от общ пул за използване в задачи като машинен превод. Въз основа на съществуващата работа по модели на езикова разлика, базирани на класове [1], ние първо въвеждаме метод, базиран на клъстери, който използва кафяви клъстери за кондензиране на речника на корпорите. Второ, прилагаме метода на циничен подбор на данни [2], който постепенно изгражда обучителен корпус за ефективно моделиране на корпуса на задачите. За първи път в системата за машинен превод се използват както клъстерно-базираните, така и циничните подходи за подбор на данни и ние извършваме сравнително сравнение. Нашите вътрешни оценки показват, че и двата нови метода превъзхождат стандартния подход на Мур-Луис (разлика между кръстосаната ентропия) по отношение на по-добра перфлексичност и проценти на OOV при вътрешните данни. Циничният подход се сближава много по-бързо, обхващайки почти целия речник в областта с 84% по-малко данни от другите методи. Освен това новите подходи могат да бъдат използвани за подбор на данни за обучение на машинен превод за обучение на по-добри системи. Резултатите ни потвърждават, че подборът на базата на класове с помощта на Браун клъстери е жизнеспособна алтернатива на базираните класове методи и премахва зависимостта от маркер за част от речта. Освен това успяхме да валидираме наскоро предложения метод за циничен подбор на данни, показвайки, че ефективността му в моделите надминава тази на традиционните методи за кръстосана ентропия и др.\nСъвпада с дължината на изречението на корпуса на задачите.', 'sw': 'Tunakutana na kutumia mbinu mbili za kutatua tatizo la kuchagua data muhimu za mafunzo kutoka kwenye viwanja jumla kwa ajili ya matumizi katika kazi kama vile tafsiri ya mashine. Kujenga kazi inayopo kwenye mifano ya tofauti ya lugha yenye darasani [1], tunaanzisha kwa mara ya kwanza njia yenye msingi inayotumia viungo vya Brown ili kuendesha maneno ya kampuni hiyo. Pili, tunatekeleza mbinu ya uchaguzi wa taarifa za kawaida[2], ambayo inajenga chombo cha mafunzo kwa ufanisi wa kutengeneza chombo cha kazi. Vyote vinavyoishi na mbinu za uchaguzi wa taarifa za kawaida zimetumika kwa mara ya kwanza ndani ya mfumo wa kutafsiri mashine, na tunafanya ulinganisho mkuu wa juu. Tathmini zetu za ndani zinaonyesha kuwa mbinu mpya zote zinaonyesha mbinu mpya za mwelekeo wa Moore-Lewis (tofauti za kutangazwa kwa watu), kwa sababu ya utata bora na kiwango cha OOV kinachohusu takwimu za ndani. Matokeo ya kawaida yanabadilisha kwa haraka sana, yanayoandika takribani lugha zote za ndani na asilimia 84 chini ya data kuliko njia nyingine. Furthermore, the new approaches can be used to select machine translation training data for training better systems.  Matokeo yetu yanathibitisha kuwa uchaguzi wa darasa kwa kutumia viungo vya Brown ni mbadala muhimu wa njia zilizoko kwenye darasa la POS, na kuondoa kutegemea imani kwa sehemu ya alama za hotuba. Kwa nyongeza, tunaweza kuthibitisha njia ya uchaguzi wa takwimu za kiraia za hivi karibuni zinazopendekezwa, zinaonyesha kwamba utendaji wake katika mifano ya SMT unapitisha njia za tofauti za kitamaduni za upatikanaji na zaidi\nkwa karibu inafanana na hukumu ya umri wa kazi.', 'ko': '우리는 유니버설 탱크에서 관련 훈련 데이터를 선택하여 기계 번역 등 임무에 사용하는 문제를 해결하기 위해 두 가지 방법을 제시하고 응용한다.기존의 클래스 기반의 언어 차이 모델[1]을 토대로 우리는 먼저 클래스 기반의 방법을 소개했다. 이 방법은 갈색 클래스를 사용하여 어료 라이브러리의 어휘를 압축한다.그 다음에 우리는 세상 물정에 어두운 데이터 선택 방법을 실현했다[2]. 이 방법은 훈련 자료 라이브러리를 증량하여 효과적으로 모델링 임무 자료 라이브러리를 구축한다.기계 번역 시스템에서 처음으로 분류와 질투를 바탕으로 하는 데이터 선택 방법을 사용하고 머리를 맞댔다.우리의 내재적인 평가에 의하면 이 두 가지 새로운 방법은 역내 데이터의 곤혹도와 OOV율 방면에서 모두 표준적인 몰 루이스 방법(교차엔트로피차)보다 우수하다는 것을 알 수 있다.세속에 분개하고 질투하는 방법은 더욱 빨리 수렴되고 거의 모든 분야의 어휘표를 덮으며 데이터량이 다른 방법보다 84% 적다.이 밖에 새로운 방법은 기계 번역 훈련 데이터를 선택하여 시스템을 더욱 잘 훈련할 수 있도록 할 수 있다.우리의 결과에 의하면 브라운 클래스를 사용하는 클래스 기반의 선택은 단어 기반의 클래스 기반의 방법의 실행 가능한 대체 방법이며 단어 표기에 대한 의존을 없앴다.또한 최근에 제기된 불공평한 데이터 선택 방법을 검증하여 SMT 모델에서의 성능이 전통적인 교차 엔트로피 차법 등을 초과했음을 나타낼 수 있다\n작업 자료 라이브러리의 문장 길이와 매우 일치합니다.', 'tr': 'Maşynyň terjime edilýän işlerde ulanmak üçin uly pool daşarynda nähili eğitim maglumatyny çözmek üçin iki yönden çykýarys we üýtget. Ders tabanly dil üýtgeşmeleri üçin bar işlerine guruldyk [1], başlangyçda kluster tabanly yöntemi korporanyň sözlerini taýýarlamak üçin browny klusterslerden ullanýar. Ikinjisi, kinik maglumaty saýlamak yöntemini ýerine ýetirdik [2], işgärlik korpusyny etkinleýän şekilde üýtgetmek üçin bir bilim korpusyny gurýar. cluster-dan daşarylan we cinik maglumat saýlamasynyň hem ilkinji gezek maşynyň terjime sistemasynda ulanylýar. Biz kellä kellä kellä karşılaştyrylýarys. Bizim iç düşmeklerimiz hem täze yöntemlerimiz standart Moore-Lewis (cross-entropy farklygy), domeniň üstündeki karmaşıklygy we OOV hasaplamalarynyň üstünde çözmesini gösterir. Sinik yaklaşım daha hızlı bir şekilde, domain sözlerinin hepsini 84% daha az veri ile diğer yönlerden kaplıyor. Munuň üçin, täze golaýlar maşynyň terjime etmek üçin gowy sistemlerde okuw etmek üçin ulanyp biler. Biziň netijelerimiz brow sanlaryny ulanan klas tabanly saýlawy POS-dan tabanly klas taýýarlanan yöntemleriň üçin ýeterli bir üýtgedir we güýjüni çykar. Hemmäpli, biz ýakyn teklip eden kinik maglumat saýlawyň yöntemini takyklaşdyryp bileris, SMT nusgalarynda däpli çarpyş-entropi üýtgeşmeleriniň we köp üýtgeşmeleriniň üstünden geçýändigini görkezip bileris.\nişiň korpusynyň sözleriň uzunlygyny ýakynlaşýar.', 'am': 'እናሳየዋለን እና ሁለትን ሥርዓት ለመቆጣጠር የግንኙነትን ማህበረሰብ ዳታዎችን ከጠቅላላ ጉዳይ በመምረጥ እናደርጋለን፡፡ በክፍለ ቋንቋ ልዩነት ዓይነቶች ላይ የሚኖረውን ሥራ በመሥራት ላይ (1) በመጀመሪያ የኮርፖርት ቃላትን ለማሳመር የብሮንስ ኮንተር የሚጠቅመውን የኮርፖርት ቃላት ለመጠበቅ የሚጠቅመውን የቅድመ ደረጃ እናስጠጋለን፡፡ በሁለተኛው፣ የስራ ኮርፓስ ትክክል ለማሳመር የጥያቄ ዳታ ምርጫ method (2) እናደርጋለን፡፡ የኮንተር እና የጥያቄ ዳታ ምርጫዎች በመጀመሪያ ጊዜ በመስመር ውስጥ ለመጀመሪያ ይጠቀማሉ፤ እናም የራስ-ራስ ትክክል እናደርጋለን፡፡ የውስጥ ውጤታችን አዲስ ደረጃዎች የሞራ-ሊዊ ቀዳሚ (የድምፅ ግንኙነት የክፍለ ግንኙነት) እና የኦኦቪ ውጤት በዶሜን ዳታዎች ላይ በሚያሳየው ጥያቄ እንዲያሳዩ ያሳያል፡፡ የጥያቄ ሥርዓት ከሌሎቹ ዘዴዎች ይልቅ 84 በመቶ የሚያንስ ዳታ ይለውጣል፡፡ ከዚህም በላይ አዲስ ደረጃዎች ለመምረጥ የመኪን ትርጉም ዳታዎችን ለመምረጥ ይጠቀማሉ፡፡ Our results confirm that class-based selection using Brown clusters is a viable alternative to POS-based class-based methods, and removes the reliance on a part-of-speech tagger.  በተጨማሪም፣ የቀኑ የጥያቄ ዳታ ምርጫ method ማረጋገጥ እናስችላለን፣ የSMT ዓይነቶች ፍለጋውን የባሕላዊ cross-entropy ልዩነት ማድረግ እና አብዛኛውን\nየስራ ኮርፓስ ግንኙነትን አቅራቢያ ይተካክላል፡፡', 'hy': 'Մենք ներկայացնում ենք և կիրառում ենք երկու մեթոդ լուծելու խնդիրը, որը նշանակում է ընտրել նշանակալի ուսումնասիրության տվյալներ ընդհանուր համակարգում օգտագործելու համար, ինչպիսիք են մեքենայի թարգմանությունը: Հիմնվելով գոյություն ունեցող աշխատանքի վրա դասարանի լեզվի տարբերության մոդելների վրա [1] մենք առաջին անգամ ներկայացնում ենք խմբերի հիմնված մեթոդ, որը օգտագործում է Բրաուն խմբերը կոնդենսավորելու համար մարմնի բառարանը: Երկրորդ, մենք կիրառում ենք սինիկ տվյալների ընտրության մեթոդը [2] որը միաժամանակ կառուցում է ուսուցման կորպոս, որպեսզի արդյունավետ մոդելավորի աշխատանքի կորպոսը: Առաջին անգամ մեքենայի թարգմանման համակարգում օգտագործվում են խմբերի և ցինիկ տվյալների ընտրության մոտեցումները, և մենք կատարում ենք գլխավոր առ գլխավոր համեմատություն: Մեր ներքին գնահատումները ցույց են տալիս, որ երկու նոր մեթոդները գերազանցում են ստանդարտ Մուր-Լյուիս մոտեցումը (խաչը-էնտրոպիայի տարբերությունը), ավելի լավ խառնաշփոթի և OOO արագությունների առումով բնագավառի տվյալների վրա: Սինիկ մոտեցումը շատ ավելի արագ է համընկնում, ներառելով գրեթե բոլոր բնագավառի բառարանները 84 տոկոսով ավելի քիչ տվյալներով, քան մյուս մեթոդները: Ավելին, նոր մոտեցումները կարող են օգտագործվել մեքենային թարգմանման ուսուցման տվյալներ ընտրելու համար ավելի լավ համակարգերի ուսուցման համար: Մեր արդյունքները հաստատում են, որ դասի հիմնված ընտրությունը, օգտագործելով Բրաուն կլաստերն, POS-ի հիմնված դասի հիմնված մեթոդների հնարավոր այլընտրանք է և հեռացնում է խոսքի մի մասի վրա կախվածությունը: Ավելին, մենք կարողանում ենք ստուգել վերջերս առաջարկված սինիկ տվյալների ընտրության մեթոդը, ցույց տալով, որ դրա արտադրողությունը SMT մոդելներում գերազանցում է ավանդական փոխէնտրոպիայի տարբերության մեթոդների և ավելի շատ\nհամապատասխանում է գործի մարմնի նախադասության երկարությանը:', 'az': 'Biz maşın çevirilməsi kimi işlərdə istifadə etmək üçün çoxlu təhsil məlumatlarını seçmək üçün iki yol göstəririk və istifadə edirik. Sınıf tabanlı dillərin fərqli modellerinin üstündə olan işlərə inşa edirik [1], biz ilk dəfə korporanın sözlərini təsdiqləmək üçün Brown clusters istifadə edən bir cluster tabanlı metodlarını tanıyırıq. İkincisi, biz cinik məlumat seçmə metodunu [2] istifadə edirik ki, işin korpusu təhsil etmək üçün təhsil korpusu təhsil edir. Hər ikisi də pul tabanlı və cinik məlumat seçməsi ilk dəfə maşın çevirim sistemində istifadə edilir. Biz baş ilə baş ilə baş ilə qarşılaşdırırıq. İçindəki değerlendirmələrimiz hər ikisi yeni metodların standart Moore-Lewis yaxınlığını (çox entropi fərqli-fərqli-fərqli-fərqli-fərqli-fərqli-fərqli) ilə daha yaxşı karışıqlıq və OOV dərəcələrini domain verilənlərində daha yax Cinical approach daha hızlı birləşdirir, domain sözlərinin neredeyse bütün məlumatları digər metodlardan 84% az verilən bütün məlumatları örtür. Daha sonra, daha yaxşı sistemləri təhsil etmək üçün maşın təhsil təhsil məlumatlarını seçmək üçün yeni təhsil işlədilir. Bizim sonuçlarımız Sınıf tabanlı seçimlərimiz, Brown clusters vasitəsilə, POS tabanlı sınıf tabanlı metodlarına uyğun bir alternatif olduğunu təsdiqləyir və söz etiketçisinin bir parçasını silər. Daha da, biz yeni təklif edilmiş cinik məlumatlar seçmə metodunu təsdiqləyə bilərik, SMT modellərində onun performansının nəticəli cür entropi fərqli metodlarının və daha çoxluğundan üstün olduğunu göstərək,\nişin korpusunun uzunluğuna yaxınlaşır.', 'bn': 'আমরা উপস্থাপন করি এবং দুই পদ্ধতি প্রয়োগ করি মেশিন অনুবাদের মতো কাজে ব্যবহার করার জন্য একটি সাধারণ পুল থেকে প্রশিক্ষণের তথ্য নির্বাচনে ক্লাস ভিত্তিক ভাষার পার্থক্য মডেলের উপর বিদ্যমান কাজ নির্মাণ করা হচ্ছে [১], আমরা প্রথমে একটি ক্লাস্টার ভিত্তিক পদ্ধতি চিহ্নিত করি যা ব্র দ্বিতীয়, আমরা সাইনিকাল ডাটা নির্বাচন পদ্ধতি ব্যবহার করি [২] যা ক্রমাগত কর্পোসের মডেল করার জন্য একটি প্রশিক্ষণ কোর্পাস তৈরি করে। ক্লাস্টার ভিত্তিক এবং সাইকিনাল ডাটা নির্বাচনের ক্ষেত্রে প্রথমবার ব্যবহার করা হয়েছে মেশিন অনুবাদ সিস্টেমের মধ্যে, এবং আমরা মাথা আমাদের অভ্যন্তরীণ পরিস্থিতি দেখা যাচ্ছে যে দুটি নতুন পদ্ধতি মুর-লেউসের স্বাভাবিক প্রতিক্রিয়া (ক্রাস-এন্ট্রোপির পার্থক্য) আর ডোমেইনের ত সাইনিকেল প্রযুক্তি অন্যান্য পদ্ধতির চেয়ে ৮৪% কম তথ্য দ্রুত পরিবর্তন করে। এছাড়াও, মেশিন অনুবাদ প্রশিক্ষণের জন্য নতুন পদ্ধতি ব্যবহার করা যাবে। আমাদের ফলাফল নিশ্চিত করে যে ব্রাউন ক্লাস্টার ব্যবহার করে ব্রাউন ক্লাস্টার ব্যবহার করে ক্লাসের ভিত্তিক নির্বাচনের বিকল্প হলো পোস ভিত্তিক ক ক তাছাড়াও, আমরা সম্প্রতি প্রস্তাবিত সাইনিকাল ডাটা নির্বাচন পদ্ধতি বৈধ করতে পারি, যেখানে দেখাচ্ছি যে এসএমটি মডেলে এর প্রভাব পার্থক্যের পার্থক\nclosely matches the sentence length of the task corpus.', 'ca': "Presentam i aplicam dos mètodes per abordar el problem a de seleccionar les dades de capacitació pertinents d'un grup general per utilitzar en tasques com la traducció màquina. Construïnt-nos en els models de diferències lingüístices basats en classe [1], vam introduir primer un mètode basat en grups que utilitza grups marrons per condensar el vocabulari de la corpora. En segon lloc, implementam el mètode de selecció de dades cíniques [2], que incrementalment construeix un cos d'entrenament per modelar eficientment el cos de tasques. Tant els enfocaments de selecció de dades basats en grups com cínics s'utilitzen per primera vegada dins un sistema de traducció màquina, i fem una comparació cap a cap. Les nostres evaluacions intrínsecs mostren que ambdós nous mètodes superen l'enfocament Moore-Lewis estándar (diferència entre entropies), en termes de millor perplexitat i índex d'OOV en dades en domini. L'enfocament cínic convergeix molt més ràpid, cobrint gairebé tot el vocabulari en domini amb 84% menys dades que els altres mètodes. A més, es poden utilitzar els nous enfocaments per seleccionar dades d'entrenament de traducció màquina per formar millors sistemes. Els nostres resultats confirmen que la selecció basada en classes utilitzant clusters Brown és una alternativa viable als mètodes basats en classes POS, i elimina la dependència d'un etiquetador de part de la xerrada. A més, podem validar el mètode de selecció de dades cíniques proposat recentment, mostrant que el seu rendiment en models SMT supera el dels mètodes tradicionals de diferència entre entropies i més\ncoincideix estretament amb la llargada frase del corps de tasca.", 'fa': 'ما دو روش برای حل مشکل تعلیم مربوط به انتخاب داده های آموزشی مربوط به یک استخراج عمومی برای استفاده از کار مثل ترجمه ماشین را پیشنهاد می کنیم و کاربرد می کنیم. بر روی کار موجود در مورد مدل تفاوت های زبان بر پایه کلاس [1] اولین بار یک روش بر پایه کلاستر معرفی می کنیم که از کلاسترهای براون استفاده می کند تا کلاس های کوپرا را کاندینس کند. دوم، ما روش انتخاب داده های سینیکی [2] را اجرای می کنیم که به طور اضافه یک کورپوس آموزش را برای موثرت مدل کورپوس کار می سازد. هر دو از طریق انتخاب داده های سینیک بر اساس کلاستر برای اولین بار در سیستم ترجمه ماشین استفاده می\u200cشوند، و ما یک مقایسه سر به سر انجام می\u200cدهیم. ارزیابی داخلی ما نشان می دهند که هر دو روش جدید از طریق استاندارد مور-لوئیس (تفاوت متوسط انتروپی) در مورد نرخ\u200cهای پرچسبی بهتر و نرخ\u200cهای OOV در داده\u200cهای داخل داخلی انجام می\u200cدهد. این دستور شینیکی بسیار سریع تر است، که تقریباً تمام کلمه\u200cهای دامنه\u200cی دامنه\u200cای را با ۸۴ درصد اطلاعات کمتر از روش\u200cهای دیگر پوشانده می\u200cشود. علاوه بر این، روش\u200cهای جدید برای انتخاب داده\u200cهای ترجمه ماشین برای آموزش سیستم\u200cهای بهتر استفاده می\u200cشود. نتیجه\u200cهای ما تایید می\u200cکند که انتخاب بر پایه کلاس با استفاده از کلاس\u200cهای براون جایگزینی قابل قابل قابل قابل قابل استفاده برای روش\u200cهای بر پایه کلاس POS است و اعتماد به یک قسمتی از نقاشی سخنرانی را حذف می\u200cکند. به اضافه، ما می توانیم روش انتخاب داده های سینیکی را تایید کنیم، که نشان می دهیم که عملکرد آن در مدل های SMT از روش تفاوت های متفاوت متفاوت سنتی و بیشتر\nنزدیک به طول مجازات کورپوس کار مشابه دارد.', 'cs': 'Představujeme a aplikujeme dvě metody řešení problému výběru relevantních vzdělávacích dat z obecného fondu pro použití v úkolech, jako je strojový překlad. Na základě existující práce na modelech jazykových rozdílů založených na třídách [1] nejprve představujeme klastrovou metodu, která používá Brown clustery ke zkompenzování slovní zásoby korpusů. Za druhé implementujeme metodu cynického výběru dat [2], která postupně konstruuje tréninkový korpus pro efektivní modelování korpusu úloh. V rámci strojového překladu se poprvé používá klastrový i cynický přístup k výběru dat a provádíme srovnání hlavou k hlavě. Naše vnitřní hodnocení ukazují, že obě nové metody překonávají standardní Moore-Lewisovu přístup (cross-entropie diference), pokud jde o lepší zmatenost a OOV rychlost v doméně dat. Cynický přístup se sbírá mnohem rychleji a pokrývá téměř veškerou slovní zásobu v doméně s 84% méně dat než ostatní metody. Kromě toho lze nové přístupy použít k výběru dat tréninku strojového překladu pro školení lepších systémů. Naše výsledky potvrzují, že výběr založený na třídě pomocí Brown clusterů je životaschopnou alternativou k metodám založeným na POS a odstraňuje spolehlivost na tagger části řeči. Kromě toho jsme schopni ověřit nedávno navrženou metodu cynického výběru dat, která ukazuje, že její výkon v SMT modelech překonává tradiční metody cross-entropie diference a další\núzce odpovídá délce věty korpusu úkolu.', 'et': 'Esitleme ja rakendame kahte meetodit, et lahendada probleemi, mis puudutab asjakohaste koolitusandmete valimist üldisest kogumist selliste ülesannete täitmiseks nagu masintõlge. Tuginedes olemasolevale tööle klassipõhiste keeleliste erinevuste mudelitel [1], tutvustame esmalt klastripõhist meetodit, mis kasutab korpuste sõnavara tihendamiseks pruunide klastrite abil. Teiseks rakendame küünilise andmevaliku meetodit, [2] mis konstrueerib järk-järgult koolituskorpuse ülesandekorpuse efektiivseks modelleerimiseks. Masintõlkesüsteemis kasutatakse esmakordselt nii klastripõhist kui ka küünilist andmete valimise lähenemisviisi ning teostame vastastikuse võrdluse. Meie sisemised hinnangud näitavad, et mõlemad uued meetodid ületavad standardset Moore-Lewise lähenemisviisi (ristentroopia erinevus) parema segaduse ja OOV määra osas domeenisiseste andmete puhul. Küüniline lähenemine läheneb palju kiiremini, hõlmates peaaegu kogu valdkonnasisest sõnavara 84% vähem andmeid kui muud meetodid. Lisaks saab uusi lähenemisviise kasutada masintõlke koolituse andmete valimiseks paremate süsteemide koolitamiseks. Meie tulemused kinnitavad, et klassipõhine valik Browni klastrite abil on elujõuline alternatiiv POS-põhistele klassipõhistele meetoditele ja eemaldab sõltuvuse kõneosa sildistajast. Lisaks on meil võimalik valideerida hiljuti välja pakutud küünilise andmete valimise meetodit, näidates, et selle jõudlus SMT mudelites ületab traditsiooniliste ristentroopia erinevuste meetodite jõudlust.\nvastab täpselt ülesandekorpuse karistuse pikkusele.', 'af': "Ons voorsien en toewend twee metodes vir die probleem van die kies van relevante onderwerp data uit 'n algemene pool vir gebruik in opdragte soos masjien vertaling. By gebou op bestaande werk op klas-gebaseerde taal verskil modele [1], introduseer ons eerste 'n cluster-gebaseerde metode wat gebruik Brown clusters om die woordeboek van die korpora te kondenseer. Tweede, ons implementeer die siniese data keuse metode [2], wat inkrementeer 'n oefening korpus konstrukteer om effektief die taak korpus te model. Beide die cluster-gebaseerde en die ciniese data-keuse toegang word gebruik vir die eerste keer binne 'n masjien vertalingsstelsel, en ons doen 'n kop-na-kop vergelyking. Ons binneste evaluasies vertoon dat beide nuwe metodes die standaard Moore-Lewis toegang (kruisentropie verskil), in terms van beter perpleksie en OOV rate op in-domein data uitvoer. Die siniese toegang versamel baie vinniger, oordek byna al die in-domein woordeboek met 84% minder data as die ander metodes. Verder kan die nuwe toegang gebruik word om masjien vertaling data te kies vir beter stelsels te oefen. Ons resultate bevestig dat klas-gebaseerde keuse gebruik Brown-clusters 'n bepaalde alternatief is vir POS-gebaseerde klas-gebaseerde metodes, en verwyder die vertrouing op 'n deel-van-woorde-etiket. In addition, we are able to validate the recently proposed cynical data selection method, showing that its performance in SMT models overpasses that of traditional cross-entropy difference methods and more\ntoe ooreenstem die setlengte van die taak korpus.", 'sq': 'Ne paraqesim dhe aplikojmë dy metoda për trajtimin e problemit të zgjedhjes së të dhënave të treinimit të rëndësishme nga një grup i përgjithshëm për përdorim në detyra të tilla si përkthimi i makinave. Duke u mbështetur në punën ekzistuese në modelet e diferencës gjuhësore bazuar në klasë [1], ne së pari futim një metodë bazuar në grupe që përdor grupe kafe për të kondensuar fjalorin e korprës. Secondly, we implement the cynical data selection method [2], which incrementally constructs a training corpus to efficiently model the task corpus.  Si metodat e zgjedhjes së të dhënave të bazuara në grupe, ashtu dhe të dhënave cinike përdoren për herë të parë brenda një sistemi përkthimi automatik dhe ne bëjmë një krahasim kokë-kokë. Vlerësimet tona të brendshme tregojnë se të dy metodat e reja tejkalojnë metodën standard Moore-Lewis (ndryshimin ndër-entropi), në lidhje me perplexitet më të mirë dhe normat OOV në të dhënat në domeni. Përqasja cinike konvergon shumë më shpejt, duke mbuluar pothuajse të gjithë fjalorin në domeni me 84% më pak të dhëna se metodat e tjera. Përveç kësaj, qasjet e reja mund të përdoren për të zgjedhur të dhënat e trajnimit të përkthimit të makinave për trajnimin e sistemeve më të mira. Rezultatet tona konfirmojnë se zgjedhja me bazë në klasë duke përdorur klasat Brown është një alternativë e jetueshme ndaj metodave me bazë në klasë POS dhe heq mbështetjen në një pjesë të fjalimit tagger. Përveç kësaj, ne jemi në gjendje të vlerësojmë metodën e zgjedhjes së dhënave cinike të propozuara kohët e fundit, duke treguar se performanca e saj në modelet SMT kalon atë të metodave tradicionale të ndryshimit ndër-entropi dhe më shumë\ni përshtatet afër gjatësisë së fjalës së korpusit të detyrës.', 'fi': 'Esittelemme ja sovellamme kahta menetelmää, joilla ratkaistaan ongelma, joka liittyy koulutustietojen valintaan yleisestä poolista esimerkiksi konekääntämiseen. Luokkapohjaisten kielieromallien [1] pohjalta esittelemme ensin klusteripohjaisen menetelmän, joka tiivistää korpusten sanastoa Brown klustereilla. Toiseksi toteutamme kyynisen tiedonvalintamenetelmän [2], joka rakentaa asteittain koulutuskorpusen tehokkaaksi mallintamiseksi. Konekäännösjärjestelmässä käytetään ensimmäistä kertaa sekä klusteripohjaista että kyynistä tiedonvalintaa, ja teemme vertaisvertailun. Sisäänrakennetut arviointimme osoittavat, että molemmat uudet menetelmät ovat parempia kuin Moore-Lewisin standardi (cross-entropia difference). Kyyninen lähestymistapa lähentyy paljon nopeammin ja kattaa lähes kaiken verkkotunnuksen sanaston 84% vähemmän tietoja kuin muut menetelmät. Lisäksi uusia lähestymistapoja voidaan käyttää konekäännöskoulutuksen tietojen valinnassa parempien järjestelmien kouluttamiseksi. Tuloksemme vahvistavat, että Brown-klustereiden luokkapohjainen valinta on käyttökelpoinen vaihtoehto POS-pohjaisille luokkapohjaisille menetelmille ja poistaa riippuvuuden puheen osatunnisteeseen. Lisäksi pystymme validoimaan hiljattain ehdotetun kyynisen datan valintamenetelmän, joka osoittaa, että sen suorituskyky SMT-malleissa ylittää perinteiset ristientropia-erotusmenetelmät ja enemmän\nVastaa tarkasti tehtäväkorpusen lauseen pituutta.', 'bs': 'Predstavljamo i primjenjujemo dvije metode za rješavanje problem a izbora relevantnih podataka obuke iz općeg bazena za upotrebu u zadatkima poput prevoda stroja. Na temelju postojećeg rada na modelima razlike jezika baziranih na klasi [1], prvo predstavljamo metodu baziranu na skupini koji koristi Browne skupine kako bi se kondenzirali rečnik korpore. Drugo, implementiramo metodu selekcije ciničkih podataka [2], koja povećavajući konstruira trening korpus kako bi učinkovito modelirao zadatak korpus. Oba pristupa za izbor ciničkih podataka se koriste prvi put u sustavu prevoda mašine, a mi obavljamo usporedbu glave na glavu. Naše unutrašnje procjene pokazuju da obje nove metode iznose standardni pristup Moore-Lewisa (razlika kroz entropiju), u smislu boljih kompleksnosti i stopa OOV-a na podacima u domenu. Cinički pristup se zbližava mnogo brže, pokrivajući skoro sve riječnike u domenu sa 84% manjim podacima od drugih metoda. Osim toga, nove pristupe se mogu koristiti za odabrati podatke o obuci strojeva za bolji sustav obuke. Naši rezultati potvrđuju da je selekcija bazirana na klasi koristeći Browne skupine održiva alternativa metodi baziranih na klasi na POS-u i uklanja pouzdanost na dijelom govornog značka. Osim toga, mi smo u mogućnosti potvrditi nedavno predloženu metodu izbora ciničkih podataka, pokazujući da njen učinkovit u modelima SMT-a prelazi od tradicionalnih metoda razlike preko entropije i više\nblizu odgovara dužini kazne zadatka korpusa.', 'jv': 'Awak dhéwé nggawe lan aplikasi durung maneh kanggo nambah perbudhakan kanggo nggawe perbudhakan kanggo nggawe data nggawe barang nggawe sistem sing nyimpen kanggo nggawe tarjamahan [1] Sekonder, we install the cisNIC data select method [2] that Includes the design Body to Effectly model the task Body. Sampeyan nganggo sistem itwasi gambar lan nganggo perusahaan dadi dong sing dibutuhe nggo sistem itwasan karo perusahaan, lan kita gewis ngerasakno perusahaan Head-to-Head. We Intinsec assertions show that the new Methods out do the Standard Mure-Lewis method (inter-Entropy contrast), in terms of older perplixty and OOOOOC rate on in-domain data. Dino pernik dadi nggambar luwih-luwih basa, sinau kanggo sabên onh-sabên kelas karo sekondirno karo sekondirno karo pernik-sabên. politenessoffpolite, politenessoffpolite"), and when there is a change ("assertivepoliteness Rejalekan dhéwé nggunakaé punika-punika dipileksi basa kelas nang nggunakaé basa basa basa dunyane mulasar kelas bron Mungkin, kita iso nggambar nggambar kelas sistem sing nyeanye perusahaan data donde, iso nggambar nggawe barang sistem SMT kuwi wis mulasar tentang karo sistem sing perusahaan karo sistem dadi-Entropy sing berarti tambah karo akeh dumadhi;\nechoH e l l o space w o r l d periodHelloworldHello world', 'he': 'אנחנו מציגים ושימושים שתי שיטות להתמודד עם הבעיה של לבחור נתונים אימונים רלוונטיים מבריכה כללית לשימוש במשימות כמו תרגום מכונות. בניין על עבודה קיימת על דוגמני ההבדל בשפה מבוססים בכיתה [1], אנו קודם מכירים שיטה מבוססת על קבוצה שמשתמשת בקבוצות בראון שנית, אנו מפעילים את שיטת הבחירה של נתונים סיניקים [2], שמבנה באופן שלילי קורפוס אימון כדי לדוגמא באופן יעיל את קורפוס המשימה. שני גישות הבחירה של נתונים המבוססים על הקלאסטר והציניות משתמשות בפעם הראשונה בתוך מערכת תרגום מכונת, ואנחנו מבצעים שיוואי ראש לראש. הערכות הפנימיות שלנו מראות ששתי השיטות החדשות מעליפות את הגישה הסטנדרטית של מור-לואיס (הבדל בין אנטרופיה הצלבית), במונחים של בלכות טובה יותר וקצבי OOV על נתונים בתחום. הגישה הצינית מתקרבת הרבה יותר מהר, מכסה כמעט את כל המילים בתחום עם 84% פחות נתונים מהשיטות האחרות. בנוסף, אפשר להשתמש באמצעות הגישות החדשות כדי לבחור נתוני אימון תרגום מכונות לאימון מערכות טובות יותר. התוצאות שלנו מאשרות שבחירה מבוססת בכיתה באמצעות קבוצות בראון היא אלטרנטיבה חיונית לשיטות מבוססת בכיתה POS, ומסירה את תלויה בחלק של טגיר דיבור. Additionally, we are able to validate the recently proposed cynical data selection method, showing that its performance in SMT models surpasses that of traditional cross-entropy difference methods and more\nמתאים מקרוב לאורך המשפט של הקורפוס המשימה.', 'sk': 'Predstavljamo in uporabljamo dve metodi za reševanje problema izbire ustreznih podatkov o usposabljanju iz splošnega nabora za uporabo v nalogah, kot je strojno prevajanje. Na podlagi obstoječega dela na modelih jezikovnih razlik, ki temeljijo na razredu [1], smo najprej uvedli metodo, ki temelji na grozdih, ki uporablja rjave grozde za zgoščanje besedišča korpusov. Drugič, izvajamo cinično metodo izbire podatkov [2], ki postopoma konstruira korpus usposabljanja za učinkovito modeliranje korpusa nalog. V sistemu strojnega prevajanja prvič uporabljamo tako grozdni kot cinični pristop za izbiro podatkov, pri čemer izvajamo primerjavo. Naše notranje ocene kažejo, da obe novi metodi presegata standardni Moore-Lewis pristop (razlika med navzkrižno entropijo) v smislu boljše zmedenosti in stopnje OOV na podatkih v domeni. Cinični pristop se veliko hitreje konvergira, saj zajema skoraj celoten domenski besednjak z 84% manj podatkov kot pri drugih metodah. Poleg tega se lahko novi pristopi uporabljajo za izbiro podatkov o usposabljanju strojnega prevajanja za usposabljanje boljših sistemov. Naši rezultati potrjujejo, da je izbira na osnovi razreda z uporabo Brown grozdov izvedljiva alternativa metodam na osnovi razreda na osnovi POS in odpravlja zanašanje na označevalnik dela govora. Poleg tega smo lahko potrdili nedavno predlagano cinično metodo izbire podatkov, ki kaže, da njena učinkovitost pri modelih SMT presega tradicionalne metode navzkrižne entropije in več\ntesno se ujema z dolžino kazni v korpusu opravil.', 'ha': "Tuna halatar da su, kuma tuna amfani da shiryoyin biyu dõmin su yi addu'a wa masu jarraba data masu amfani da shi daga wata pool mai amfani da su cikin aikin kama da fassarar mashine. Kana samar da aikin wanda ke samar a kan misãlai masu sãɓa wa harshen fasa[1], za'a gabatar da wata hanyor ta wajen kwamfyuta wanda ke amfani da Brown clubs to run maganar firma. Kijan da, za mu yi amfani da hanyon zaɓanta na kynical data[2], wanda ke samun wata umarni mai amfani da kwamfyuta dõmin ya sami kwamfyutan aikin. @ info: whatsthis Bayanmu da ke ƙari yana nũna cẽwa, hanyoyinMu biyu suna ƙaranci kowanin hanyoyin wata na'urar Mũse-Leusi (tsohon-shiga), a cikin masu tsarin mazaɓa da sauri na kamfani na OOV da sauri kan data masu cikin-Domin. Tsarin kyani na musanya masu kaso da gaggawa, yana rufe taki duk abu na cikin-Domen, da kuma da 84% ƙaranci da data daga wasu shiryoyin. Furan wannan, za'a iya amfani da hanyoyin sãbuwa don a zãɓi data na tsarin translation na mashine dõmin ya yi amfani da tsarin mafiya kyau na'urar system. MatamayinMu na gaskata cewa zaɓen mai fasa da ke amfani da Brown clubs, yana da wata shida mai inganci wa hanyõyin-danne-bane na PSS, kuma yana tafiyar da dõgara a kan wani abu na-faɗi. Ina iya ƙaranci, za'a iya gaskata metoden zaɓallin danne na ƙarani da aka buƙata, kuma za mu nuna cewa aikin kwamfyutan SMT na tsohon hanyoyin tarawa na daban-entropy da waɗancan\nna sami sosai da cire tsakiyar aiki.", 'bo': 'ང་ཚོས་ལག་ལེན་འཐབ་ཐབས་ལམ་གཉིས་ཀྱི་གནད་དོན་ལ་གནད་དོན་ཡོད་པའི་དཀའ་ངལ་སྤེལ་བའི་གནས ང་ཚོའི་ནང་དུ་ཡོད་པའི་ལས་འགན་སྡེར་བརྟེན་གྱི་སྐད་ཀྱི་ཁྱད་པར་དབྱིབས་ཀྱི་ཐབས་ལམ་ལ་བཟོ་བྱས་པ་དང་པོ་ནས་གསར་བརྗོད་བྱས་པའི་ཐབས་ལམ་ཞིག གཉིས་པ། ང་ཚོས་ཀྱིས་སྤྱི་ཚོགས་གདམ་ཀ་ཐབས་ལམ་དེ་ལས་མཐུན་བཟོ་བྱེད་མཁན་གྱི་དབུགས་རྩིས་ཡོད་པ་ལས་སྦྱོར་གཙོ་བོ་ཞིག་གི་དབ གླིང་མོ་དང་ལྟ་བུའི་གནད་དོན་གཉིས་ཀྱིས་མནར་སྤྲོད་པའི་གནད་དོན་གཉིས་ལས་རང་ཉིད་ཀྱི་གནད་དོན་གཉིས་ཀྱིས་ལག་ལེན་འཐབ་རྒྱུ་ཡི Our intrinsic evaluations show that both new methods outperform the standard Moore-Lewis approach (cross-entropy difference), in terms of better perplexity and OOV rates on in-domain data. སྤྱི་ཚོགས་ཀྱི་ཐབས་ལམ་དེ་འདྲ་མཉམ་དུ་བཏོན་གཏོང་ཐབས་ལམ་གཞན་ལས་བརྩོན་འགྱུར་བ་ཡིན་པའི་ནང་གི་བརྡ་སྤྲོད་ཀྱི་ནང་གི་ཐོག ད་དུང་། ཐབས་ལམ་གསར་བ་ནི་ལག་ལེན་འཐབ་བཏང་ཡོད་པ་ལས་མ་ལག་གི་སྐད་ཡིག ང་ཚོའི་འབྲུག འོན་ཀྱང་། ང་ཚོས་ཉེ་ཆར་གྱིས་སྤྲོད་པའི་ཐབས་ལམ་ལུགས་བདམས་ཟིན་པའི་ཚོར་བ་སྐྱེན་གྱི་ཐབས་ལམ་སྟོན་བྱེད་པར་བྱེད་སྲིད།\nདེ་ནི་དུས་ཡུལ་གྱི་ཚིག་རྩིས་འབྲེལ་བ་དང་མཐུན་པོ་ཡོད།'}
