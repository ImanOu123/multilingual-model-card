{'en': 'Operationalizing a National Digital Library : The Case for a Norwegian Transformer Model', 'pt': 'Operacionalizando uma biblioteca digital nacional: o caso de um modelo de transformador norueguês', 'ar': 'تشغيل مكتبة رقمية وطنية: حالة نموذج محولات نرويجية', 'fr': "Mise en œuvre d'une bibliothèque numérique nationale\xa0: le cas d'un modèle de transformateur norvégien", 'es': 'Puesta en marcha de una biblioteca digital nacional: el caso de un modelo de transformador noruego', 'ja': '国立デジタル図書館の運用：ノルウェーの変圧器モデルのケース', 'ru': 'Внедрение национальной цифровой библиотеки: пример норвежской модели трансформатора', 'zh': '国数字图书馆运:挪威变压器模例', 'hi': 'एक राष्ट्रीय डिजिटल लाइब्रेरी का संचालन: नॉर्वेजियन ट्रांसफॉर्मर मॉडल के लिए मामला', 'ga': 'Leabharlann Dhigiteach Náisiúnta a Oibriú: An Cás ar son Múnla Trasfhoirmeora na hIorua', 'ka': 'Name', 'hu': 'Egy nemzeti digitális könyvtár működtetése: a norvég transzformátor modellnek az esete', 'el': 'Λειτουργοποίηση μιας Εθνικής Ψηφιακής Βιβλιοθήκης: Η υπόθεση για ένα Νορβηγικό μοντέλο μετασχηματιστή', 'it': 'Funzionalizzazione di una biblioteca digitale nazionale: il caso di un modello di trasformatore norvegese', 'mk': 'Операциonaliзација на националната дигитална библиотека: Случајот за норвешки трансформиран модел', 'kk': 'Ұлттық цифрлық жиындарды операциялау: The Case for a Norwegian Transformer Model', 'lt': 'Nacionalinės skaitmeninės bibliotekos veikimas: Norvegijos transformatoriaus modelio atvejis', 'ml': 'നാഷണല്\u200d ഡിജിറ്റല്\u200d ലൈബ്രറിയില്\u200d പ്രവര്\u200dത്തിപ്പിക്കുന്നു: നോര്\u200dവേജിയന്\u200d ട്രാന്\u200dസ്ഫോര്\u200dമാറ്റര്\u200d മോഡി', 'mn': 'National Digital Library: The Case for a Norwegian Transformer Model', 'mt': 'Operationalizing a National Digital Library: The Case for a Norwegian Transformer Model', 'no': 'Operaliserer ein nasjonal digital bibliotek: The Case for a Norwegian Transformer Model', 'ro': 'Operaționalizarea unei biblioteci digitale naționale: cazul unui model de transformator norvegian', 'pl': 'Operacjonalizacja narodowej biblioteki cyfrowej: przykład norweskiego modelu transformatora', 'sr': 'Operalizacija nacionalne digitalne biblioteke: Case for a Norwegian Transformer Model', 'si': 'The case for a Norvegian Transfer Model', 'ms': 'Operationalizing a National Digital Library: The Case for a Norwegian Transformer Model', 'so': 'Operating a National Digital Library: The Case for a Norwegian Transfer Model', 'ta': 'The Case for a Norwegian Transformer Model', 'ur': 'The Case for a Norwegian Transformer Model', 'sv': 'Att driva ett nationellt digitalt bibliotek: Fallet för en norsk transformatormodell', 'vi': 'Quản lý một thư viện kỹ thuật số quốc gia: Trường hợp cho mô hình biến hình người Na Uy', 'uz': 'Name', 'da': 'Operationalisering af et nationalt digitalt bibliotek: Sagen for en norsk transformatormodel', 'nl': 'Operationaliseren van een Nationale Digitale Bibliotheek: De zaak voor een Noors Transformatormodel', 'hr': 'Operalizacija nacionalne digitalne biblioteke: Slučaj za norveški model transformera', 'bg': 'Оперативност на национална цифрова библиотека: примерът за норвежки трансформаторен модел', 'id': 'Memoperasikan Pustaka Digital Nasional: Kasus Model Transformer Norwegian', 'fa': 'عملیات کتابخانه\u200cی دیجیتال ملی: The Case for a Norwegian Transformer Model', 'ko': '국가 디지털 도서관 운영: 노르웨이 변압기 모델 사례', 'tr': 'The Case for a Norwegian Transformer Model', 'sw': 'Kufanya kazi Maktaba ya Taifa ya Kidijitali: Kesi kwa ajili ya Modeli ya Tafsiri ya Norway', 'af': 'Name', 'de': 'Operationalisierung einer nationalen digitalen Bibliothek: Der Fall für ein norwegisches Transformatormodell', 'sq': 'Operacionimi i një biblioteke kombëtare dixhitale: rasti për një model transformues norvegjez', 'hy': 'Operationalizing a National Digital Library: The Case for a Norwegian Transformer Model', 'az': 'Ulusal Digital KitabńĪnńĪn iŇül…ôm…ôsi: The Case for a Norwegian Transformer Model', 'am': 'በመጫን ላይ የብሔራዊ ዲጂታል መብረቢያ: The Case for a Norwegian Transfer Model', 'ca': 'Operationalizing a National Digital Library: The Case for a Norwegian Transformer Model', 'bs': 'Operalizacija nacionalne digitalne biblioteke: Slučaj za Norveški model transformera', 'bn': 'একটি জাতীয় ডিজিটাল লাইব্রেরীর কাজ: নরওয়েজিয়ান ট্রান্সফার্নার মডেলের কেস', 'et': 'Rahvusliku digitaalraamatukogu toimimine: Norra transformaatori mudeli juhtum', 'fi': 'Kansallisen digitaalisen kirjaston operationalisointi: norjalaisen muuntajamallin tapaus', 'cs': 'Provoz Národní digitální knihovny: Případ norského transformátoru', 'ha': 'KCharselect unicode block name', 'sk': 'Upravljanje nacionalne digitalne knjižnice: primer norveškega modela transformatorjev', 'bo': 'Operationalizing a National Digital Library: The Case for a Norwegian Transformer Model', 'he': 'Operationalizing a National Digital Library: The Case for a Norwegian Transformer Model', 'jv': 'The Case for a Norwegian Transformer model'}
{'en': 'In this work, we show the process of building a large-scale training set from digital and digitized collections at a national library. The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models in several token and sequence classification tasks for both Norwegian Bokml and Norwegian Nynorsk. Our model also improves the mBERT performance for other languages present in the corpus such as English, Swedish, and Danish. For languages not included in the corpus, the weights degrade moderately while keeping strong multilingual properties. Therefore, we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.', 'ar': 'في هذا العمل ، نعرض عملية بناء مجموعة تدريب واسعة النطاق من مجموعات رقمية ورقمية في مكتبة وطنية. تتفوق تمثيلات التشفير ثنائية الاتجاه الناتجة من نموذج اللغة القائم على المحولات (BERT) للنرويجية على نماذج BERT متعددة اللغات (mBERT) في العديد من مهام تصنيف الرموز والتسلسل لكل من Bokmål النرويجي و Nynorsk النرويجي. يعمل نموذجنا أيضًا على تحسين أداء mBERT للغات الأخرى الموجودة في المجموعة مثل الإنجليزية والسويدية والدنماركية. بالنسبة للغات غير المدرجة في المجموعة ، تنخفض الأوزان بشكل معتدل مع الاحتفاظ بخصائص قوية متعددة اللغات. لذلك ، نظهر أن بناء نماذج عالية الجودة داخل مؤسسة ذاكرة باستخدام محتوى التعرف الضوئي على الأحرف (OCR) المزعج إلى حد ما أمر ممكن ، ونأمل أن نمهد الطريق لمؤسسات الذاكرة الأخرى لتتبعها.', 'pt': 'Neste trabalho, mostramos o processo de construção de um conjunto de treinamento em larga escala a partir de coleções digitais e digitalizadas em uma biblioteca nacional. O modelo de linguagem baseado em Representações do Codificador Bidirecional de Transformadores (BERT) resultante para o norueguês supera os modelos multilíngues do BERT (mBERT) em várias tarefas de classificação de token e sequência para o norueguês Bokmål e o norueguês Nynorsk. Nosso modelo também melhora o desempenho do mBERT para outros idiomas presentes no corpus, como inglês, sueco e dinamarquês. Para idiomas não incluídos no corpus, os pesos degradam moderadamente, mantendo fortes propriedades multilíngues. Portanto, mostramos que a construção de modelos de alta qualidade dentro de uma instituição de memória usando conteúdo de reconhecimento óptico de caracteres (OCR) um pouco barulhento é viável e esperamos abrir caminho para outras instituições de memória seguirem.', 'fr': "Dans ce travail, nous montrons le processus de création d'un ensemble de formation à grande échelle à partir de collections numériques et numérisées dans une bibliothèque nationale. Le modèle de langage basé sur les représentations de codeurs bidirectionnels à partir de transformateurs (BERT) pour le norvégien surpasse les modèles BERT multilingues (mBERT) dans plusieurs tâches de classification de jetons et de séquences pour le bokmål norvégien et le nynorsk norvégien. Notre modèle améliore également les performances mBERt pour d'autres langues présentes dans le corpus, telles que l'anglais, le suédois et le danois. Pour les langues non incluses dans le corpus, les pondérations se dégradent modérément tout en conservant de fortes propriétés multilingues. Par conséquent, nous montrons qu'il est possible de créer des modèles de haute qualité au sein d'une institution de mémoire en utilisant un contenu de reconnaissance optique de caractères (OCR) quelque peu bruyant, et nous espérons ouvrir la voie à d'autres institutions de mémoire.", 'es': 'En este trabajo, mostramos el proceso de construcción de un conjunto de capacitación a gran escala a partir de colecciones digitales y digitalizadas en una biblioteca nacional. El modelo lingüístico basado en Bidirectional Encoder Representations from Transformers (BERT) para noruego supera a los modelos BERT multilingües (mBERT) en varias tareas de clasificación de señales y secuencias tanto para el noruego Bokmål como para el noruego Nynorsk. Nuestro modelo también mejora el rendimiento de mBert para otros idiomas presentes en el corpus, como el inglés, el sueco y el danés. Para los idiomas no incluidos en el corpus, las ponderaciones se degradan moderadamente mientras mantienen sólidas propiedades multilingües. Por lo tanto, demostramos que es factible crear modelos de alta calidad dentro de una institución de memoria utilizando contenido de reconocimiento óptico de caracteres (OCR) algo ruidoso, y esperamos allanar el camino para que otras instituciones de memoria lo sigan.', 'ja': '本作では、国立図書館のデジタルコレクションとデジタルコレクションから大規模なトレーニングセットを構築する過程を示します。結果として得られるノルウェー語のためのトランスフォーマー（ BERT ）ベースの言語モデルからの双方向エンコーダ表現は、ノルウェー語のBokmålとノルウェー語のNynorskの両方のいくつかのトークンおよびシーケンス分類タスクで多言語のBERT （ mBERT ）モデルを上回ります。当社のモデルはまた、英語、スウェーデン語、デンマーク語などのコーパスに存在する他の言語のmBERTパフォーマンスを向上させます。コーパスに含まれていない言語の場合、ウェイトは強力な多言語特性を維持しながら適度に劣化します。そこで、ある程度ノイズの多い光学式文字認識（ OCR ）コンテンツを用いてメモリ機関内で高品質なモデルを構築することが実現可能であることを示し、他のメモリ機関が従う道を開いていきたいと考えている。', 'zh': '于是展国家图书馆数数字化馆藏之大培训集。 Transformers (BERT) 之挪威语言双向编码器者,挪威博克马尔语、挪威尼诺斯克语之多表序也,优于多言 BERT (mBERT) 。 更入语料库中语言(如英语、瑞典语、丹麦语)mBERT 性。 语料库中未言语,权重则降,强者多言。 故臣等明用嘈杂光学字符识(OCR)于存储之内高质量其形可也,愿为他存储机构铺平道路。', 'hi': 'इस काम में, हम एक राष्ट्रीय पुस्तकालय में डिजिटल और डिजीटल संग्रह से बड़े पैमाने पर प्रशिक्षण सेट बनाने की प्रक्रिया दिखाते हैं। परिणामस्वरूप ट्रांसफॉर्मर (BERT) से द्विदिश एन्कोडर प्रतिनिधित्व- नॉर्वेजियन के लिए आधारित भाषा मॉडल नॉर्वेजियन बोकमाल और नॉर्वेजियन Nynorsk दोनों के लिए कई टोकन और अनुक्रम वर्गीकरण कार्यों में बहुभाषी BERT (mBERT) मॉडल को मात देता है। हमारा मॉडल कॉर्पस में मौजूद अन्य भाषाओं जैसे अंग्रेजी, स्वीडिश और डेनिश के लिए mBERT प्रदर्शन में भी सुधार करता है। कॉर्पस में शामिल नहीं की गई भाषाओं के लिए, मजबूत बहुभाषी गुणों को रखते हुए वजन मामूली रूप से कम हो जाता है। इसलिए, हम दिखाते हैं कि कुछ हद तक शोर ऑप्टिकल चरित्र मान्यता (ओसीआर) सामग्री का उपयोग करके एक मेमोरी संस्थान के भीतर उच्च गुणवत्ता वाले मॉडल का निर्माण संभव है, और हम अन्य स्मृति संस्थानों का पालन करने के लिए मार्ग प्रशस्त करने की उम्मीद करते हैं।', 'ru': 'В этой работе мы показываем процесс создания крупномасштабного обучающего комплекта из цифровых и оцифрованных коллекций в национальной библиотеке. Полученные двунаправленные представления кодировщика от трансформеров (BERT) -ОСНОВАННАЯ языковая модель для норвежского языка превосходит многоязычные модели BERT (MBERT) в нескольких задачах классификации токенов и последовательностей как для норвежского языка Bokmål, так и для норвежского языка Nynorsk. Наша модель также улучшает производительность mBERT для других языков, присутствующих в корпусе, таких как английский, шведский и датский. Для языков, не включенных в корпус, весовые коэффициенты ухудшаются умеренно, сохраняя при этом сильные многоязычные свойства. Поэтому мы показываем, что построение высококачественных моделей в запоминающем устройстве с использованием несколько шумного оптического контента для распознавания символов (OCR) осуществимо, и мы надеемся проложить путь для других запоминающих учреждений.', 'ga': "Sa saothar seo, léirímid an próiseas a bhaineann le sraith oiliúna ar mhórscála a thógáil ó bhailiúcháin dhigiteacha agus dhigiteacha ag leabharlann náisiúnta. Tá an tsamhail teanga bunaithe ar Chlaochladáin (BERT) don Iorua níos fearr ná na samhlacha ilteangacha BERT (mBERT) i roinnt tascanna aicmithe comhartha agus seicheamh do Bokmål na hIorua agus don Ioruais Nynorsk araon. Cuireann ár múnla feabhas freisin ar fheidhmíocht mBERT do theangacha eile atá sa chorpas mar Bhéarla, Sualainnis agus Danmhairgis. I gcás teangacha nach bhfuil san áireamh sa chorpas, díghrádaíonn na meáchain go measartha agus coinníonn siad airíonna láidre ilteangacha. Mar sin, léirímid go bhfuil sé indéanta samhlacha ardcháilíochta a thógáil laistigh d'institiúid chuimhne ag baint úsáide as ábhar aitheantais optúla carachtar (OCR) atá beagán torannach, agus tá súil againn an bealach a réiteach d'institiúidí cuimhne eile.", 'ka': 'ამ სამუშაოში ჩვენ ჩვენ გაჩვენებთ დიზიტალური და დიზიტალური კოლექციებიდან დიზიტალური განათლების პროცესი. შემდეგ ორდირექციონალური კოდირების გამოსახულებები ტრანფორმეტრების (BERT) ენერგული მოდელის მრავალენგური BERT (mBERT) მოდელის მრავალენგური კოდირების მოდელში და კოდირების კლასიფიკაციის მოდულებები ნორგური ბოკმალში და ნორგურ ჩვენი მოდელი ასევე უფრო მეტია mBERT-ის გამოსახულება სხვა ენებისთვის, როგორც კორპუსში, როგორც ინგლისური, შვედიური და დენიური. ენებისთვის, რომლებიც კორპუსში არ ჩანაწყებულია, სიმაღლეები მოდიდრებულია, რომლებიც ძალიან მრავალენგური მნიშვნელობების შესახებ. ამიტომ ჩვენ ჩვენ აჩვენებთ, რომ მეხსიერების ინსტუტიციაში მაღალური მოდელების შექმნა, რომელიც გამოიყენება ძალიან ძალიან ძალიან ძალიან სიტყვირული ოპტიკალური სიტყვირს განაცნობა (OCR) შესა', 'el': 'Σε αυτή την εργασία, παρουσιάζουμε τη διαδικασία δημιουργίας ενός μεγάλης κλίμακας εκπαιδευτικού συνόλου από ψηφιακές και ψηφιοποιημένες συλλογές σε μια εθνική βιβλιοθήκη. Το προκύπτουν αμφίδρομες αναπαραστάσεις κωδικοποιητή από γλωσσικό μοντέλο βασισμένο σε μετασχηματιστές (BERT) για τα νορβηγικά ξεπερνά τα πολύγλωσσα μοντέλα BERT (mBERT) σε αρκετές εργασίες ταξινόμησης σημάτων και ακολουθίας τόσο για τη νορβηγική Bokmal όσο και για τη νορβηγική Nynorsk. Το μοντέλο μας βελτιώνει επίσης την απόδοση για άλλες γλώσσες που υπάρχουν στο σώμα, όπως τα αγγλικά, τα σουηδικά και τα δανικά. Για τις γλώσσες που δεν περιλαμβάνονται στο σώμα, τα βάρη υποβαθμίζονται μετρίως διατηρώντας ισχυρές πολυγλωσσικές ιδιότητες. Ως εκ τούτου, αποδεικνύουμε ότι η κατασκευή υψηλής ποιότητας μοντέλων μέσα σε ένα ίδρυμα μνήμης χρησιμοποιώντας κάπως θορυβώδες περιεχόμενο οπτικής αναγνώρισης χαρακτήρων (και ελπίζουμε να ανοίξει το δρόμο για άλλα ιδρύματα μνήμης να ακολουθήσουν.', 'it': "In questo lavoro, mostriamo il processo di costruzione di un set di formazione su larga scala da collezioni digitali e digitalizzate presso una biblioteca nazionale. Il modello linguistico basato su BERT (Bidirectional Encoder Representations from Transformers) risultante per il norvegese supera i modelli multilingue BERT (mBERT) in diverse attività di classificazione dei token e delle sequenze sia per il norvegese Bokmal che per il norvegese Nynorsk. Il nostro modello migliora anche le prestazioni di mBERT per altre lingue presenti nel corpus come inglese, svedese e danese. Per le lingue non incluse nel corpus, i pesi si degradano moderatamente mantenendo forti proprietà multilingue. Pertanto, dimostriamo che costruire modelli di alta qualità all'interno di un istituto di memoria utilizzando contenuti di riconoscimento ottico dei caratteri (OCR) alquanto rumorosi è fattibile e speriamo di aprire la strada ad altri istituti di memoria.", 'hu': 'Ebben a munkában bemutatjuk egy nemzeti könyvtár digitális és digitalizált gyűjteményeiből származó nagyszabású képzési készlet kiépítésének folyamatát. A kétirányú kódolók ábrázolása transzformátorokból (BERT) alapú norvég nyelvi modell többnyelvű BERT (mBERT) modelleket is felülmúlja a norvég Bokmal és a norvég nynorsk számára egyaránt többnyelvű osztályozási feladatokban. Modellünk javítja az mBERT teljesítményét a korpuszban jelen lévő más nyelvek esetében is, mint például angol, svéd és dán. A korpuszban nem szereplő nyelvek esetében a súlyok mérsékelten romlanak, miközben erős többnyelvű tulajdonságokat tartanak fenn. Ezért megmutatjuk, hogy kiváló minőségű modellek építése egy memóriaintézményen belül kissé zajos optikai karakterfelismerő (OCR) tartalmat használva valósítható meg, és reméljük, hogy megnyitjuk az utat más memóriaintézmények számára is.', 'kk': 'Бұл жұмыс ішінде, ұлттық жиындағы цифрлық және цифрлық жиындардан үлкен мақсатты оқыту процесін көрсетеді. Норвегия тілінің бірнеше тілді BERT (mBERT) үлгілеріне негізделген екі бағытталған кодтардың түрлендірушілері (BERT) үлгілері норвегиялық бокмал мен норвегиялық ньнорск үлгілерінде бірнеше берт (мBERT) үлгілерінде бірнеше таңба Біздің моделіміз мBERT әрекетін қорпустағы басқа тілдер үшін ағылшын, Швед және Дания тілдерінде жақсартады. Корпус тілдеріне ендірілмеген үшін, көптілік қасиеттерді сақтау үшін теңіздің көпшілігін көп тілдерінің қасиеттері деңгейінде деңгейінде деңге Сондықтан біз жады институциясының ішінде жоғары сапатты моделдерді құру мүмкіндігін көрсетедік. Басқа жады институттарының келесі жолын қолдану мүмкін.', 'lt': 'Šiame darbe parodomi didelio masto mokymo iš skaitmeninių ir skaitmeninių kolekcijų nacionalinėje bibliotekoje kūrimo procesą. Atsižvelgiant į tai, kad Norvegijos kalbos modelis yra dvikrypčiai koduotojų atstovai iš transformatorių (BERT), jis atlieka daugiakalbius BERT (mBERT) modelius keliose Norvegijos Bokmal ir Norvegijos Nynorsk ženklų ir sekos klasifikavimo užduotyse. Our model also improves the mBERT performance for other languages present in the corpus such as English, Swedish, and Danish.  Kalbų, neįtrauktų į korpusą, svoris vidutiniškai mažėja, išlaikant stiprias daugiakalbes savybes. Todėl mes parodome, kad aukštos kokybės modelių kūrimas atminties įstaigoje, naudojant šiek tiek triukšmingą optinio požymio pripažinimo turinį, yra įmanomas, ir tikimės atverti kelią kitoms atminties įstaigoms sekti.', 'mk': 'Во оваа работа, го покажуваме процесот на изградба на голема обука од дигитални и дигитализирани колекции во национална библиотека. Резултатот на дводносните претставувања на кодерот од јазичкиот модел базиран на Трансформери (БЕРТ) за норвешкиот јазик ги надминува мултијазичните модели на БЕРТ (МБЕРТ) во неколку задачи за класификација на знаци и секвенца за норвешкиот Бокмал и норвешкиот Ни Нашиот модел, исто така, ја подобрува изведбата на mBERT за други јазици присутни во корпусот како што се англиски, шведски и дански. За јазиците кои не се вклучени во корпусот, теговите се деградираат умерено, при што се одржуваат силни мултијазични сопствености. Затоа, покажуваме дека изградбата на висококвалитетни модели во мемориска институција користејќи малку шумно оптичко признавање на карактеристиката (OCR) е остварлива и се надеваме дека ќе го отвориме патот за следење на другите мемориски институции.', 'ms': 'In this work, we show the process of building a large-scale training set from digital and digitized collections at a national library.  The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models in several token and sequence classification tasks for both Norwegian Bokmal and Norwegian Nynorsk.  Model kami juga meningkatkan prestasi mBERT untuk bahasa lain yang ada dalam korpus seperti Inggeris, Swedish, dan Danish. Untuk bahasa yang tidak termasuk dalam corpus, berat badan merendah sementara menyimpan ciri-ciri berbilang bahasa yang kuat. Therefore, we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.', 'ml': 'ഈ പ്രവര്\u200dത്തനത്തില്\u200d, നാഷണല്\u200d ലൈബ്രറിയില്\u200d നിന്നും ഡിജിറ്റല്\u200d സംഘടിപ്പിക്കപ്പെട്ട ഒരു വലിയ പരിശീലനം പണിയുന്നതിന നോര്\u200dവേജിയന്\u200d പുറത്തുള്ള ഭാഷ മോഡലില്\u200d നിന്നുള്ള ബിര്\u200dട്ടി (mBERT) മോഡലുകളില്\u200d നിന്നുള്ള ബൈഡഡിയല്\u200d എന്\u200dകോഡിയര്\u200d പ്രതിനിധികള്\u200d നോര്\u200dവെയിജിയന്\u200d ബോക്ക്മലും നോര്\u200dവേജിയന്\u200d നിന്നോര്\u200dസ് Our model also improves the mBERT performance for other languages present in the corpus such as English, Swedish, and Danish.  കോര്\u200dപ്പുസില്\u200d ചേര്\u200dന്നിട്ടില്ലാത്ത ഭാഷകള്\u200d അതുകൊണ്ട് ഞങ്ങള്\u200d ഒരു മെമ്മറിയുടെ സ്ഥാനത്തില്\u200d ഉയര്\u200dത്തുന്ന മോഡലുകള്\u200d ഉണ്ടാക്കുന്നത് കാണിക്കുന്നു. ഒരു ശബ്ദം ഒപ്പിക്കല്\u200d കാര്യം തിരിച്ചറിയു', 'mt': 'F’dan ix-xogħol, naraw il-proċess tal-bini ta’ sett ta’ taħriġ fuq skala kbira minn kollezzjonijiet diġitali u diġitalizzati f’librerija nazzjonali. Ir-Rappreżentazzjonijiet Bidirezzjonali tal-Kodifikaturi minn mudell lingwistiku bbażat fuq it-Transformaturi (BERT) li jirriżultaw għal mudelli tal-BERT multilingwi (mBERT) f’diversi kompiti ta’ klassifikazzjoni ta’ tokens u sekwenza kemm għal Bokmal Norveġiż kif ukoll għal Nynorsk Norveġiż. Il-mudell tagħna jtejjeb ukoll il-prestazzjoni tal-mBERT għal lingwi oħra preżenti fil-korpus bħall-Ingliż, l-Isvediż u d-Daniż. For languages not included in the corpus, the weights degrade moderately while keeping strong multilingual properties.  Therefore, we show that building high-quality models within a memory institution using somewhat noisy optical character recognition (OCR) content is feasible, and we hope to pave the way for other memory institutions to follow.', 'mn': 'Энэ ажил дээр бид үндэсний номын сан дээр маш их хэмжээний суралцах үйл ажиллагааг бий болгодог. Норвегийн давхар хэлний BERT (mBERT) загварын хоёр давхар кодчуудын төлөөлөлт нь олон хэлний BERT (mBERT) загваруудыг Норвегийн Бокмал болон Норвегийн Ньнорск хоёр давхар давхар болон дарааллаар хуваалцах даалгаваруудын хоёр хэлбэрийн загваруу Бидний загвар мөн Англи, Швед, Данийг зэрэг корпус дээр байгаа бусад хэл дээр mBERT үйл ажиллагааг сайжруулдаг. Корпус-д оролцохгүй хэлний хувьд хүчтэй олон хэлний чадварыг хадгалах үед жин нь орчин үед багасгадаг. Тиймээс бид дурсамж байгууллагын дотор өндөр чанартай загвар бүтээхийг харуулж байна. ОКР-ын тодорхойлолтыг ашиглаж бусад дурсамж байгууллагуудын дагах арга замыг нэмэгдүүлэх гэж найдаж байна.', 'no': 'I denne arbeiden viser vi prosessen for å bygge eit stor opplæring sett frå digitale og digitaliserte samlingar i eit nasjonal bibliotek. Det følgjande divretningskkoderingsrepresentasjonane frå transformeringsmodellen (BERT)-basert språk for norsk utfører fleire språk BERT (mBERT) modellen i fleire teikn og sekvensklassifikasjonar for både norsk bokmal og norsk Nynorsk. Vårt modell forbedrar også mBERT-utviklinga for andre språk som finst i korpusen som engelsk, svensk og dansk. For språk som ikkje er inkludert i korpusen, vil vekten degradera modert mens dei held sterke fleirspråk eigenskapar. Derfor viser vi at bygging av høg kvalitet-modeller i ein minningssystem med noko støy optisk teiknkonkjenning (OCR) er tilgjengeleg, og vi håper å lage måten for andre minningssystemer å følgja.', 'pl': 'W niniejszej pracy przedstawiamy proces budowy dużej skali zestawu szkoleń z cyfrowych i zdigitalizowanych zbiorów w bibliotece narodowej. Powstałe reprezentacje kodera dwukierunkowego z modelu językowego opartego na transformatorach (BERT) dla norweskiego przewyższają wielojęzyczne modele BERT (mBERT) w kilku zadaniach klasyfikacji tokenów i sekwencji zarówno dla norweskiego Bokmala, jak i norweskiego Nynorska. Nasz model poprawia również wydajność mBERT dla innych języków obecnych w korpusie, takich jak angielski, szwedzki i duński. W przypadku języków nieobjętych korpusem wagi ulegają umiarkowanemu degradacji, zachowując silne właściwości wielojęzyczne. Dlatego też pokazujemy, że budowanie wysokiej jakości modeli w instytucji pamięci z wykorzystaniem nieco hałaśliwej zawartości optycznego rozpoznawania znaków (OCR) jest możliwe i mamy nadzieję utorować drogę dla innych instytucji pamięci.', 'ro': 'În această lucrare, prezentăm procesul de construire a unui set de instruire la scară largă din colecții digitale și digitalizate la o bibliotecă națională. Modelul lingvistic bazat pe reprezentări bidirecționale de la transformatori (BERT) rezultat pentru norvegiană depășește modelele multilingve BERT (mBERT) în mai multe sarcini de clasificare a token-urilor și secvențelor atât pentru Bokmal norvegian, cât și pentru Nynorsk norvegian. Modelul nostru îmbunătățește, de asemenea, performanța mBERT pentru alte limbi prezente în corp, cum ar fi engleză, suedeză și daneză. Pentru limbile care nu sunt incluse în corp, greutățile se degradează moderat, păstrând în același timp proprietăți multilingve puternice. Prin urmare, arătăm că construirea de modele de înaltă calitate în cadrul unei instituții de memorie folosind conținut oarecum zgomotos de recunoaștere optică a caracterului (OCR) este fezabilă și sperăm să deschidem calea pentru alte instituții de memorie.', 'sr': 'U ovom poslu pokazujemo proces izgradnje velike obuke koje su postavili iz digitalnih i digitalnih kolekcija u nacionalnoj biblioteci. Rezultati predstavljanja dvodirektivnog kodera iz transformatora (BERT)-baziranog jezičkog modela za norveški iznosi multijezičke modele BERT (mBERT) u nekoliko znakova i sekvencijskih klasifikacijskih zadataka za Norveški bokmal i norveški Nynorsk. Naš model takođe poboljšava performancu mBERT za drugi jezici koji su prisutni u korpusu kao što su engleski, švedski i danski. Za jezike koji nisu uključeni u korpus, težina se moderno smanjuje dok drže jake multijezičke vlasništvo. Stoga, pokazujemo da je moguće izgradnju visokokvalitetnih modela unutar institucije za pamćenje koristeći neku buku optičku prepoznavanju karaktera (OCR) sadržaj, i nadamo se da ćemo otpratiti put drugim institucijama za pamćenje.', 'si': 'මේ වැඩේ අපි පෙන්වන්නේ ජාතික පුළුවන් වලින් ලොකු ස්ථානයක් නිර්මාණය කරන්න ප්\u200dරශ්නයක්. ප්\u200dරතිස්ථාපනය බියාරිකේෂණික කෝඩාර් ප්\u200dරතිස්ථාපනය (BERT)-අධිරූපය භාෂාව ප්\u200dරතිස්ථාපනය නෝර්වේජියාන් වෙනුවෙන් බොක්මල් සහ නෝර්වේජියා අපේ මෝඩේල් එක්ක ම්බෙර්ට් ප්\u200dරමාණය විස්තර කරනවා අනිත් භාෂාවට ඉංග්\u200dරීස්, ස්වීඩිෂ්, ඩැනිෂ් වගේ  භාෂාවට කෝර්පස් වලට සම්බන්ධ වෙන්නේ නැති විදියට, බලය සම්බන්ධ භාෂාත්මක විශේෂ විශේෂ ව ඉතින්, අපි පෙන්වන්නම් ස්මාර්ථක ස්ථානයක් ඇතුලට උඩ ප්\u200dරතිශේෂ ප්\u200dරමාණයක් නිර්මාණය කරන්න පුළුවන් කියලා, සහ අපි හිතනවා අනිත් මතකය', 'so': 'Markaas waxan ka muuqanaynaa baaraandegista dhismaha waxbarasho aad u weyn oo ka soo qoran ururada digital iyo digitiyada ee maktabadda qaranka ah. Midowga Bidirectional Encoder Representations from Transformers (BERT)-based model of Norwegian outperforms models of BERT (mBERT) in several signs and sequence classification tasks for both Norwegian Bokmal and Norwegian Nynorsk. Tusaale ahaan ayaa horumarinaya muuqashada muuqashada MBERT oo ku qoran luqadaha kale ee ku qoran afka Ingiriis, Iswidish iyo Daanish. Luqadaha aan ku jirin korpuska, miisaanka ayaa si hagaagsan u hoosaysiiya marka uu haysto hantida luuqadaha kala duwan. Sidaa darteed waxaynu tusnaynaa in dhismaha qaababka sare ee xusuusta gudahooda lagu isticmaalo aqoonsashada xarumaha dhaqdhaqaaqa ah (OCR) waa suurtogal, waxaana rajaynaynaa in aan hagaajinno jidadka xubnaha kale oo ay soo raacaan.', 'sv': 'I detta arbete visar vi processen att bygga en storskalig utbildningsuppsättning från digitala och digitaliserade samlingar på ett riksbibliotek. Den resulterande tvåriktade kodare Representations from Transformers (BERT)-baserade språkmodellen för norska överträffar flerspråkiga BERT-modeller (mBERT) i flera token- och sekvensklassificeringsuppgifter för både norska Bokmal och norska Nynorsk. Vår modell förbättrar också mBERT-prestandan för andra språk som finns i korpusen, såsom engelska, svenska och danska. För språk som inte ingår i korpusen försämras vikterna måttligt samtidigt som starka flerspråkiga egenskaper bibehålls. Därför visar vi att det är möjligt att bygga högkvalitativa modeller inom en minnesinstitution med något bullrigt optiskt teckenkänningsinnehåll (OCR), och vi hoppas kunna bana väg för andra minnesinstitutioner att följa.', 'ta': 'இந்த வேலையில், நாம் ஒரு நாட்டின் நூலகத்தில் இருந்து ஒரு பெரிய அளவு பயிற்சியை உருவாக்கும் செயலை காட்டுகிறோம். Name எங்கள் மாதிரி முறைமையில் எம்பெர்ட் செயல்பாட்டை மேம்படுத்துகிறது கார்ப்ஸ் போன்ற மொழிகளில் இருக்கும் மொழ கோர்ப்ஸில் சேர்க்கப்படவில்லையான மொழிகளுக்கு, பலம் மொழிகளின் பண்புகளை வைத்திருக்கும் போது தூக்கங்கள் சர அதனால், நாம் ஒரு நினைவகத்தில் உயர்தரமான மாதிரிகளை காட்டுகிறோம் என்று, ஒரு நினைவகம் நிறுவனத்தில் சில சப்தமான ஆசிரிக்கல் எழுத்து அடையாளம் (OCR) உள்', 'ur': 'ہم نے اس کام میں ایک بڑی سطح کی تدریس بنانے کی پروسس دکھائی ہے جیجیٹل اور ڈیجیٹل کی جماعتوں سے ایک ملی لائیبری میں۔ نوروژی کے لئے بہت سی زبان BERT (mBERT) نمڈلوں میں بہت سی ٹوکنوں اور کلاسپیٹ کے کاموں میں بہت سی ٹوکنوں اور کلاسپیٹ کے کاموں میں موجود ہوتے ہیں۔ ہماری مدل نے بھی mBERT کی پرورشش کو اور زبانوں کے لئے بہتر کر دیا ہے جیسے انگلیسی، سوئڈی اور دانش کی۔ زبانوں کے لئے جسم میں شامل نہ ہوا، وزن میدان سے کمی ہوتی ہے اور بہت سی زبانوں کے اموال رکھتے ہیں۔ لہٰذا ہم دکھاتے ہیں کہ ایک یاد آوری ایستونٹ کے اندر اچھی کیفیت موڈل بنانے کا مطابق بہت زیادہ غروب آپٹیکل شخصیٹ کی شناسایی (OCR) موجود ہے، اور ہم امید رکھتے ہیں کہ دوسرے یاد آوری ایستونٹ کے لئے راہ پیدا کریں', 'uz': "Bu ishda, biz natijadagi kutubxonasidagi raqamli va raqamli toʻplamlarni yaratish jarayonini ko'rsamiz. Name Bizning modelimiz bir xil tildagi ingliz, iswidhish, va Danish kabi boshqa tillar uchun mBERT ishni bajaradi. Korpus tilida qo'llanmagan tillar uchun, ko'plab tilning xususiyatlarini saqlab qolayotganda o'smirlarning o'smirlari oddiy darajada qo'yiladi. Shunday qilib, biz xotira tashkilotdagi sariq modellar yaratishni xotiraga ko'rsamiz va bir necha qiyin optik qoidalar (OCR) tarkibini ishlatish mumkin, va biz boshqa xotira tashkilotlariga qo'llanmiz.", 'vi': 'Trong công việc này, chúng tôi cho thấy quá trình xây dựng một bộ huấn luyện trên diện rộng từ các bộ sưu tập kỹ thuật số và số hóa tại một thư viện quốc gia. Kết quả là Đài Truyền Thông qua qua hai hướng từ mô hình ngôn ngữ biến hình (BERT) của người Na Uy sản xuất ra các mô hình lớn thiếu niên thiếu sót (mBERT) trong nhiều nhiệm vụ phân loại hình tượng trưng và dãy số cho cả người Nau Boarcal và Na Uy Nynorsk. Hệ thống của chúng tôi cũng cải thiện kết quả mBERT cho những ngôn ngữ khác trong tập thể như tiếng Anh, Thụy Điển và Đan Mạch. Đối với những ngôn ngữ không nằm trong tập thể, các tạ lượng giảm giá một cách vừa phải trong khi giữ các tính chất đa dạng mạnh. Cho nên, chúng tôi cho thấy việc xây dựng các mô hình chất cao trong một cơ sở bộ nhớ sử dụng các nội dung nhận dạng quang học hơi ồn ào (OCR) có khả thi, và chúng tôi hy vọng là mở đường cho các bộ nhớ khác theo.', 'bg': 'В тази работа показваме процеса на изграждане на мащабен обучителен набор от дигитални и дигитализирани колекции в национална библиотека. Полученият двупосочен кодер за представяне от трансформатори (BERT) базиран езиков модел за норвежки превъзхожда многоезичните BERT (mBERT) модели в няколко задачи за класификация на символи и последователности както за норвежкия Бокмал, така и за норвежкия Нинорск. Нашият модел също така подобрява ефективността на други езици, присъстващи в корпуса, като английски, шведски и датски. За езици, които не са включени в корпуса, тежестите се влошават умерено, като същевременно запазват силни многоезични свойства. Ето защо ние показваме, че изграждането на висококачествени модели в рамките на институция памет, използваща донякъде шумно съдържание за оптично разпознаване на знаци (ОКР), е осъществимо и се надяваме да проправим пътя за други институции памет да следват.', 'hr': 'U ovom poslu pokazujemo proces izgradnje velike obuke iz digitalnih i digitalnih kolekcija u nacionalnoj biblioteci. Rezultati predstavljanja dvosmjernog kodera iz modela jezika koji se temelji na transformatorima (BERT) za Norveški iznosi multijezičke modele BERT (mBERT) u nekoliko znakova i sekvencijskih klasifikacijskih zadataka za Norveški bokmil i norveški Nynorsk. Naš model također poboljšava performancu mBERT-a za drugi jezici koji su prisutni u korpusu kao što su engleski, švedski i danski. Za jezike koje nisu uključene u korpus, težina se umjereno smanjuje dok drže jake multijezičke vlasništvo. Stoga, pokazujemo da je moguće izgradnju visokokvalitetnih modela unutar institucije pamćenja koristeći neku buku optičku prepoznavanju karaktera (OCR) sadržaj, i nadamo se da ćemo otpratiti put drugim institucijama pamćenja.', 'da': 'I dette arbejde viser vi processen med at opbygge et stort træningssæt ud fra digitale og digitaliserede samlinger på et nationalt bibliotek. Den resulterende bidirectional Encoder Representations from Transformers (BERT)-baserede sprogmodel for norsk overgår flersprogede BERT-modeller (mBERT) i flere token- og sekvensklassifikationsopgaver for både norsk Bokmal og norsk Nynorsk. Vores model forbedrer også mBERT-præstationen for andre sprog, der findes i korpuset, såsom engelsk, svensk og dansk. For sprog, der ikke er inkluderet i korpuset, nedbrydes vægtene moderat, samtidig med at de bevarer stærke flersprogede egenskaber. Derfor viser vi, at det er muligt at bygge modeller af høj kvalitet i en hukommelsesinstitution ved hjælp af noget støjende optisk karakterkendelse (OCR) indhold, og vi håber at bane vejen for andre hukommelsesinstitutioner at følge.', 'nl': 'In dit werk tonen we het proces van het bouwen van een grootschalige trainingsset uit digitale en gedigitaliseerde collecties in een nationale bibliotheek. Het resulterende Bidirectionele Encoder Representaties van Transformers (BERT)-gebaseerd taalmodel voor Noors overtreft meertalige BERT (mBERT) modellen in verschillende token- en sequentieclassificatietaken voor zowel Noors Bokmal als Noors Nynorsk. Ons model verbetert ook de mBERT prestaties voor andere talen die in het corpus aanwezig zijn, zoals Engels, Zweeds en Deens. Voor talen die niet in het corpus zijn opgenomen, nemen de gewichten matig af, terwijl sterke meertalige eigenschappen behouden blijven. Daarom laten we zien dat het bouwen van hoogwaardige modellen binnen een geheugeninstelling met behulp van ietwat noise optische karakterherkenning (OCR) inhoud haalbaar is, en we hopen de weg vrij te maken voor andere geheugeninstellingen om te volgen.', 'ko': '이 작업에서 우리는 국가 도서관의 디지털과 디지털화 소장에서 대규모 교육집을 구축하는 과정을 보여 주었다.노르웨이 Bokmal과 노르웨이 Nynorsk의 몇 개의 영패와 서열 분류 작업에서 Transformers(BERT)의 노르웨이 언어 모델을 바탕으로 만들어진 양방향 인코더는 다중 언어 BERT(mBERT) 모델보다 우수하다는 것을 나타낸다.우리의 모델은 어료 라이브러리의 다른 언어(예를 들어 영어, 스웨덴어, 덴마크어)의 mBERT 성능도 향상시켰다.자료 라이브러리에 포함되지 않은 언어에 대한 권한은 적당히 낮아지고 강력한 다중 언어 속성을 유지한다.따라서 우리는 한 기억 기구 내에서 일정한 소음이 있는 광학 문자인식(OCR) 내용을 사용하여 고품질 모델을 구축하는 것이 가능하다는 것을 증명했고 다른 기억 기구의 모방을 위해 길을 닦기를 희망한다.', 'id': 'Dalam pekerjaan ini, kami menunjukkan proses membangun set latihan skala besar dari koleksi digital dan digitalisasi di perpustakaan nasional. Perwakilan Bidirectional Encoder yang berasal dari model bahasa berdasarkan Transformers (BERT) untuk bahasa Norwegian melebihi model BERT (mBERT) berbilang bahasa dalam beberapa tugas token dan urutan klasifikasi untuk Bokmal Norwegian dan Nynorsk Norwegian. Model kami juga meningkatkan prestasi mBERT untuk bahasa lain yang ada di tubuh seperti Inggris, Swedish, dan Danish. For languages not included in the corpus, the weights degrade moderately while keeping strong multilingual properties.  Oleh karena itu, kami menunjukkan bahwa pembangunan model kualitas tinggi di dalam sebuah institusi memori menggunakan konten sedikit suara pengenalan karakter optik (OCR) adalah feasible, dan kami berharap untuk membuka jalan untuk institusi memori lain untuk mengikuti.', 'fa': 'در این کار، ما فرآیند ساختن یک آموزش مسیر بزرگ را از جمع\u200cآوری\u200cهای دیجیتال و دیجیتال در کتابخانه ملی نشان می\u200cدهیم. نتیجه\u200cای از نمونه\u200cهای زبان\u200cهای ابتدایی\u200cکننده\u200cها (BERT) برای نمونه\u200cهای BERT (mBERT) چندین زبان\u200cهای نوروژی در چندین نشانه\u200cها و دسته\u200cهای جدایی\u200cگرایی\u200cکننده\u200cها برای بوکمل نوروژی و نینورسک نوروژی می\u200cکند. مدل ما هم عملکرد mBERT را برای زبانهای دیگری که در کورپوس موجود هستند، مثل انگلیسی، سوئدی و دانش بهتر می کند. برای زبان\u200cها که در کورپوس شامل نشده\u200cاند، وزن\u200cها در حالی که دارای ویژه\u200cهای زیادی زبان\u200cها قوی می\u200cباشند به مدرسه نابود می\u200cشوند. بنابراین، ما نشان می دهیم که ساختن مدل\u200cهای کیفیت بالا در یک موسسه حافظه با استفاده از محتوای شناسایی شخصیت\u200cهای مصنوعی صوتی (OCR) ممکن است، و امیدواریم راه را برای سازمان\u200cهای حافظه\u200cهای دیگر پیروی کنیم.', 'de': 'In dieser Arbeit zeigen wir den Prozess des Aufbaus eines großen Schulungssatzes aus digitalen und digitalisierten Sammlungen in einer Nationalbibliothek. Das resultierende bidirektionale Encoder Repräsentationen aus Transformers (BERT)-basiertem Sprachmodell für Norwegisch übertrifft mehrsprachige BERT (mBERT)-Modelle in mehreren Token- und Sequenzklassifizierungsaufgaben für Norwegisch Bokmal und Norwegisch Nynorsk. Unser Modell verbessert auch die mBERT-Leistung für andere im Korpus vorhandene Sprachen wie Englisch, Schwedisch und Dänisch. Bei Sprachen, die nicht im Korpus enthalten sind, werden die Gewichte moderat abgebaut, während starke mehrsprachige Eigenschaften beibehalten werden. Daher zeigen wir, dass es möglich ist, qualitativ hochwertige Modelle innerhalb einer Speicherinstitution mit etwas rauschenden optischen Zeichenerkennungsinhalten (OCR) zu erstellen, und wir hoffen, den Weg für andere Speicherinstitutionen zu ebnen.', 'sw': 'Katika kazi hii, tunaonyesha mchakato wa kujenga mafunzo makubwa yanayotengenezwa kutoka kwenye mikusanyiko ya kidijitali na kidijitali katika maktaba ya kitaifa. Matokeo yake yaliyosababisha Repositories from Transformers (BERT) anayeishi kwa lugha ya Norway hufanya mifano mingi ya lugha mbalimbali ya BERT (mBERT) katika kazi kadhaa za usambazaji kwa ajili ya Bokmal wa Norway na Nynorsk wa Norway. Mfano wetu pia unaboresha utendaji wa mBERT kwa lugha nyingine zinazopo kwenye makampuni kama vile Kiingereza, Kiswadishi na Kidenishi. Kwa lugha ambazo hazijumuishwa kwenye viungo, mizigo inapungua kwa kiasi kikubwa wakati wa kuweka vifaa vya lugha mbalimbali. Kwa hiyo, tunaonyesha kwamba kujenga mifano yenye ubora mkubwa katika taasisi ya kumbukumbu kwa kutumia maudhui ya tamasha yenye kelele (OCR) inawezekana, na tunatumaini kuendelea njia ya taasisi nyingine za kumbukumbu zifuataze.', 'am': 'በዚህ ሥራ፣ ከዲጂታሌ እና ዲጂትሪካዊ ጉዳዮች ከሀገሪቱ መብረቢያ የተደረገውን ትልቅ ትልቅ ትምህርት መሥራትን እናሳየዋለን፡፡ የኖርዌጂያን ቋንቋ ምሳሌ (mBERT) ለሚያሳየው የBidirectional Encoder Representatives from Transformers (BERT)-based ቋንቋ model ለNorwegian BERT (mBERT) models በብዙ tokens and sequence classification tasks for Norwegian Bokmal እና Norwegian Nynorsk. ሞዴሌያችን ደግሞ በኮርፓስ ውስጥ ያሉትን ሌሎችን ቋንቋዎች ማድረግ ያበጃል፡፡ ቋንቋዎች በቆሮፓስ ውስጥ ያልተካሄዱ፥ ሚዛኖቹ በብዙ ቋንቋዎች ስርዓቶች በመጠበቅ በመካከለኛ ያዋርዳሉ። ስለዚህም እናሳየዋለን የብልሃት ምሳሌዎችን በአስታወስ አካባቢ ውስጥ እናሳየዋለን፡፡', 'tr': 'Bu işde, milli kitaphanada digital we digital toplumlardan uly ölçekli eğitim gurmanyň prosesini görkezýäris. Nörveççe üçin Bidirectional Ködler ködlemeleri (BERT)-dan tabanly dil nusgalary näçe dil BERT (mBERT) nusgalarynda birnäçe token we sıralama täzelikleri Norwegiýa Bokmal we Norwegiýa Nynorsk üçin çykýar. Biziň modelimiz hem korpusda bolan başga diller üçin mBERT ukypyny bejerýär. Iňlisçe, Şwetçe we Dança ýaly. Korpusda dahil edilmedik diller üçin, ağırlyklary ortadan azalýar we güýçli multi diller hasaplanyşyny goramaýarlar. Şol sebäpli, hatyra institucijasynda ýokary kaliwatli nusgalary gurmak mümkin däldir we beýleki hatyra institucijalaryň yzarlamagyna umyt edýäris.', 'sq': 'Në këtë punë, ne tregojmë procesin e ndërtimit të një stërvitjeje në shkallë të madhe nga koleksionet dixhitale dhe dixhitale në një bibliotekë kombëtare. Përfaqësuesit dy-drejtorë të koduesve nga modeli gjuhësor me bazë në Transformers (BERT) për gjuhën norvegjeze shfaqin modele shumëgjuhëse BERT (mBERT) në disa detyra të klasifikimit të token dhe sekuencës për si Bokmal Norvegjez ashtu dhe Nynorsk norvegjez. Modeli ynë përmirëson gjithashtu shfaqjen e mBERT për gjuhë të tjera të pranishme në korpus si anglisht, suedez dhe danisht. Për gjuhët që nuk janë përfshirë në korpus, peshat degradojnë moderatamente ndërsa mbajnë prona të forta shumëgjuhësore. Prandaj, ne tregojmë se ndërtimi i modeleve të cilësisë së lartë brenda një institucioni kujtese duke përdorur përmbajtjen disi zhurmuese të njohjes optike të karakterit (OCR) është i realizueshëm dhe shpresojmë të hapim rrugën për institucionet e tjera të kujtesës që të ndjekin.', 'af': "In hierdie werk vertoon ons die proses van 'n groot-skaal onderwerking te bou van digitale en digitiseerde versamelings in 'n nasionale biblioteek. Die resulteerde tweederigtinglike enkoder voorstellings van Transformers (BERT)-gebaseerde taal model vir Noorweë uitvoer veelvuldige tale BERT (mBERT) modele in veelvuldige teken en volgorde klasifikasie opdragte vir beide Noorweë Bokmal en Noorweë Nynorsk. Ons model verbeter ook die mBERT-prestasie vir ander tale wat in die korpus is, soos Engels, Sweedse en Daniese. Vir tale wat nie in die korpus ingesluit is nie, is die gewigte modereer terwyl die sterk multitaalske eienskappe hou. Daarom, ons wys dat die bou van hoë-kwaliteit-modele binne 'n geheue institusie gebruik met iets geluidige optiese karaktererkenning (OCR) inhoud is moontlik, en ons hoop om die pad te laat optrek vir ander geheue institusies om te volg.", 'az': 'Bu işlərdə, ulusal kütüphanədə böyük ölçülük təhsil inşaması təhsil edilməsini göstəririk. Nörveçə dillərinin çoxlu dil BERT (mBERT) modeli ilə birlikdə nörveçə Bokmal və Nörveçə Nörveçə üçün bir neçə token və seçmə klasifikasiya işləri ilə müxtəlif iki yönəlli kodlayıcı (BERT) modeli göstərilir. Bizim modelimiz də İngilizce, İsveçə və Danca kimi korpusda olan başqa dillər üçün mBERT performansını yaxşılaşdırır. Körpüs dillərində daxil olmayan dillər üçün ağırlıq çoxlu dil özelliklərini saxlayarkən ortadan azaldır. Beləliklə, çox yüksək kaliteli modellər inşa edirik, yada salma institucijasının içində, gürültülü optik karakterlərin tanımasını (OCR) ilə istifadə edərək, və digər yada salma institucijlərinin ardınca gəlməsini umuruq.', 'bs': 'U ovom poslu pokazujemo proces izgradnje velike obuke koje su postavili iz digitalnih i digitalnih kolekcija u nacionalnoj biblioteci. Rezultati predstavljanja dvosmjernog kodera iz modela jezika koji se temelji na transformatorima (BERT) za norveški iznosi multijezičke modele BERT (mBERT) u nekoliko znakova i sekvencijskih klasifikacijskih zadataka za Norveški bokmal i norveški Nynorsk. Naš model također poboljšava performancu mBERT za drugi jezici koji su prisutni u korpusu kao što su engleski, švedski i danski. Za jezike koji nisu uključeni u korpus, težina se umjerno smanjuje dok drže jake multijezičke vlasništvo. Stoga, pokazujemo da je moguće izgradnju visokokvalitetnih modela unutar institucije za pamćenje koristeći neku buku optičku prepoznavanju karaktera (OCR) sadržaj, i nadamo se da ćemo otpratiti put drugim institucijama za pamćenje.', 'hy': 'Այս աշխատանքի ընթացքում մենք ցույց ենք տալիս մեծ մասշտաբով ուսուցման գործընթացը թվային և թվային հավաքածուներից ազգային գրադարանում: The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models in several token and sequence classification tasks for both Norwegian Bokmal and Norwegian Nynorsk.  Our model also improves the mBERT performance for other languages present in the corpus such as English, Swedish, and Danish.  For languages not included in the corpus, the weights degrade moderately while keeping strong multilingual properties.  Այսպիսով, մենք ցույց ենք տալիս, որ հիշողության հաստատության մեջ բարձր որակի մոդելներ կառուցելը, որոնք օգտագործում են մի քիչ աղմկոտ օպտիկական կերպարների ճանաչման (OCR) պարունակությունը, իրականացնելի է, և հույս ունենք, որ ճանապարհ կպատրա', 'bn': 'এই কাজে আমরা জাতীয় লাইব্রেরীতে ডিজিটাল এবং ডিজিটিজিটাল সংগ্রহের একটি বিশাল প্রশিক্ষণ নির্মাণের প্রক্রিয়া দেখা নরওয়েজিয়ান ভিত্তিক ভাষার মডেলের জন্য বাইডেডিয়াল এনকোডার প্রতিনিধিত্ব (বেরেটি)-এর ফলে নরওয়েজিয়ান বোকমাল এবং নরওয়েজিয়ান নাইননর্স্কের জন্য বেশ কয়েকটি চিহ্ন এবং সেকেন্ড বিভাগ Our model also improves the mBERT performance for other languages present in the corpus such as English, Swedish, and Danish.  কোর্পাসে ভাষাগুলোর মধ্যে নেই, মাল্টিভাষার বৈশিষ্ট্যাবলীর বৈশিষ্ট্য রাখার সময় ভাষাগুলো মানুষের ভাষ তাই আমরা দেখাচ্ছি যে একটি মেমরি প্রতিষ্ঠানের মধ্যে উচ্চ মানের মডেল তৈরি করা হচ্ছে কিছু শব্দ অক্ষরের অক্ষর স্বীকৃতি ব্যবহার করে আর আমরা আশা করছি অন্যান্য স', 'cs': 'V této práci ukazujeme proces budování rozsáhlého vzdělávacího souboru z digitálních a digitalizovaných sbírek v národní knihovně. Výsledné obousměrné reprezentace kódování z jazykového modelu založeného na transformátorech (BERT) pro norštinu překonává vícejazyčné modely BERT (mBERT) v několika úlohách klasifikace tokenů a sekvencí pro norský Bokmal i norský Nynorsk. Náš model také zlepšuje výkon mBERT pro další jazyky přítomné v korpusu, jako je angličtina, švédština a dánština. U jazyků, které nejsou součástí korpusu, se váhy mírně snižují a zachovávají silné vícejazyčné vlastnosti. Proto ukazujeme, že budování vysoce kvalitních modelů v paměťové instituci pomocí poněkud hlučného obsahu optického rozpoznávání znaků (OCR) je možné a doufáme, že připravíme cestu pro další paměťové instituce.', 'et': 'Käesolevas töös näitame rahvusraamatukogus digitaalsetest ja digiteeritud kogudest laiaulatusliku koolituskomplekti ehitamise protsessi. Sellest tulenev kahesuunaline kodeerija esindused transformaatoritest (BERT) põhinev norra keelemudel ületab mitmekeelseid BERT (mBERT) mudeleid mitmes märgi- ja järjestusklassifitseerimisülesandes nii norra bokmal kui norra nynorsk. Meie mudel parandab mBERT-i jõudlust ka teistes korpuses esinevates keeltes, nagu inglise, rootsi ja taani keel. Korpusesse mittekuuluvate keelte puhul langeb kaal mõõdukalt, säilitades samas tugevad mitmekeelsed omadused. Seetõttu näitame, et kvaliteetsete mudelite ehitamine mäluasutuses, kasutades mürakat optilist märgituvastust (OCR) sisu, on teostatav, ja loodame sillutada teed teistele mäluasutustele järgimiseks.', 'fi': 'Tässä työssä esitellään valtakunnallisen kirjaston laajamittaisen koulutuskokonaisuuden rakentamisprosessia digitaalisista ja digitoiduista kokoelmista. Tuloksena saatu kaksisuuntainen kooderi-representations from Transformers (BERT) -pohjainen kielimalli norjalaisille suoriutuu monikielisistä BERT-malleista (mBERT) -malleista useissa token- ja sekvenssiluokitustehtävissä sekä norjalaisille bokmalille että norjalaisille Nynorskille. Mallimme parantaa myös mBERT-suorituskykyä muilla korpusessa esiintyvillä kielillä, kuten englanniksi, ruotsiksi ja tanskaksi. Korpuseen kuulumattomien kielten painot heikkenevät maltillisesti ja säilyttävät vahvat monikieliset ominaisuudet. Tämän vuoksi osoitamme, että korkealaatuisten mallien rakentaminen muistilaitokseen, jossa käytetään jonkin verran meluisaa optista merkkitunnistusta (OCR), on mahdollista, ja toivomme tasoittavan tietä muille muistilaitoksille.', 'ca': "En aquesta feina, demostram el procés de construir un conjunt d'entrenaments a gran escala a partir de col·leccions digitals i digitals d'una biblioteca nacional. The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models in several token and sequence classification tasks for both Norwegian Bokmal and Norwegian Nynorsk.  El nostre model també millora el rendiment mBERT per altres llengües presents al corpus com l'anglès, el suec i el danès. Per a les llengües no incloses al corpus, els pesos es degraden moderadament mantenint fortes propietats multilingües. Per tant, demostram que construir models d'alta qualitat dins una institució de memòria utilitzant continguts de reconeixement òptic de caràcter ruidosos (OCR) és viable, i esperem preparar el camí per a que altres institucions de memòria siguin.", 'jv': 'Nang trabah iki, kita ngubah perusahaan nggawe sistem sing berarti cara-cara sing paling Digital lan nganggo dolanan digitisar neng akeh pancen dumadhi. Bidirections model dhéwé kuwi nglanggar nglanggar nggawe mBERT kanggo langgar bantuan liyane karo koyo cara-cara sing itlanjut Inggris, Suwit, lan Danis. Pasang langga ora luwih dumadhi bangsane, njuk-ngomong kuwi kapan mulasar tentang karo hal-hal sing dirasane podho sing apik bangsane. Kaya, awak dhéwé menehi kudu nggawe model sing akeh-kalitas nang institusi ingkang pisan ingkang, iso nggambar aturan sing perbudhakan bakal nguasakno (OCR) sing apik dhéwé, lan awak dhéwé iso nggawe netemu kanggo kalawartasané institusi ingkang sambar uwong.', 'ha': "Daga wannan aikin, Muke nũna aikin ka sami tsarin mai girma daga matsayin digitali da digitally a cikin littafin mataifa. @ item Spelling dictionary MisalinMu yana ƙaranci da aikin mBERT wa wasu harshe waɗanda ke cikin karbonsa kamar Ingiriya, Iswidish, da Danish. Ga harshen waɗanda ba su haɗa shi ba cikin makampyuta, masu nauyi yana shagala a mutane a lokacin da za'a tsare tayari masu ƙarfi. Saboda haka, muna nũna wa ka samun misãlai masu tsari da nau'i a cikin shirin hukuru da ke amfani da sunan sigar sauri na'ura (OCR), kuma tuna kwaɗayin mu sami hanya zuwa ga masu tsarin hukuru da ke ƙara.", 'sk': 'V delu predstavljamo proces gradnje obsežnega nabora usposabljanj iz digitalnih in digitaliziranih zbirk v nacionalni knjižnici. Izhajajoči jezikovni model za norveščino, ki temelji na dvosmernih kodirnih predstavitvah iz transformatorjev (BERT), presega večjezične modele BERT (mBERT) v več opravilih klasifikacije žetonov in zaporedja za norveški bokmal in norveški Nynorsk. Naš model izboljšuje tudi učinkovitost mBERT za druge jezike, prisotne v korpusu, kot so angleščina, švedščina in danščina. Pri jezikih, ki niso vključeni v korpus, teže zmerno razpadajo, hkrati pa ohranjajo močne večjezične lastnosti. Zato pokažemo, da je izgradnja visokokakovostnih modelov znotraj pomnilniške institucije, ki uporabljajo nekoliko hrupno optično prepoznavanje znakov (OCR), izvedljiva in upamo, da bomo utrli pot drugim pomnilniškim institucijam.', 'bo': 'ང་ཚོས་རྒྱལ་ཁབ་ཅིག་ནང་དུ་གྲངས་སྒྲིག་དང་གྲངས The resulting Bidirectional Encoder Representations from Transformers (BERT)-based language model for Norwegian outperforms multilingual BERT (mBERT) models in several token and sequence classification tasks for both Norwegian Bokmal and Norwegian Nynorsk. ང་ཚོའི་མ་དབྱིབས་དབྱིབས་གཅིག་སྐད་ཡིག་ནང་གི་མིང་རྩལ་གྱི་ནང་དུ་མི་སྒྲ་ཚིགས་ཀྱི་ལས་འགན་ལ་ཡར་རྒྱས་གཏོང་། སྐད་རིགས་ཀྱི་དབུགས་ཀྱི་ནང་དུ་མ་ཡོད་པ་ལས་གནས་ཚུལ་བསྐྱེད་པའི་ཚད་ཆེ་བ་མང་ཆེ་ཤོས་ཀྱི་རྒྱུ་དངོས བྱས་ཙང་། ང་ཚོས་བྱ་ཚིག་ལས་བཟོ་བརྩོན་བྱས་བའི་རྣམ་པ་མཐོང་ཚད་ལྡན་པའི་ནང་དོན་ཡིན་པ་ཞིག་ལས་ཕན་ཚུན་རྐྱེན་བཟོ་བྱེད་འདོད་ཡོད།', 'he': 'בעבודה הזו, אנו מראים את התהליך של בניית אסוף אימונים בקנה מידה גדולה מאוספים דיגיטליים ודיגיטליים בספרייה לאומית. מודל שפת המבוסס על BERT (Transformers - BERT) למופע נורבגי מודלים BERT (mBERT) רבות שפתיים במספר משימות סימנים ורצף שיעורי שיעורי שיעור עבור גם Bokmal נורבגי וגם Nynorsk נורבגי. המודל שלנו משפר גם את ההופעה של mBERT לשפות אחרות הנוכחות בקורפוס כמו אנגלית, שוודית ודנית. For languages not included in the corpus, the weights degrade moderately while keeping strong multilingual properties.  לכן, אנו מראים שבניית מודלים איכות גבוהה בתוך מוסד זיכרון בשימוש בתוכן זיהוי אופטי רעש (OCR) הוא אפשרי, ואנחנו מקווים לפתוח את הדרך למוסדות זיכרון אחרים לעקוב אחריו.'}
{'en': 'Large-Scale Contextualised Language Modelling for Norwegian', 'pt': 'Modelagem de linguagem contextualizada em larga escala para norueguês', 'es': 'Modelado lingüístico contextualizado a gran escala para noruego', 'ar': 'نمذجة اللغة السياقية على نطاق واسع للنرويجية', 'fr': 'Modélisation contextualisée à grande échelle du norvégien', 'ja': 'ノルウェー語のための大規模な文脈化された言語モデリング', 'ru': 'Крупномасштабное контекстуализированное языковое моделирование для норвежского языка', 'zh': '向挪威语大语境化语建模', 'hi': 'नॉर्वेजियन के लिए बड़े पैमाने पर प्रासंगिक भाषा मॉडलिंग', 'ga': 'Samhaltú Teanga Comhthéacsaithe ar Mhórscála don Ioruais', 'ka': 'დიდი- სკენტექსტუალური ენის მოდელირება ნორგვეული', 'hu': 'Nagyléptékű kontextualizált nyelvmodellezés norvég számára', 'el': 'Μοντελοποίηση γλώσσας μεγάλης κλίμακας για τα νορβηγικά', 'lt': 'Norvegų kalbos modeliavimas dideliu mastu', 'mk': 'Моделирање на големи контекстни јазици за норвешки', 'it': 'Modellazione linguistica contestualizzata su larga scala per il norvegese', 'ms': 'Modelan Bahasa Berkonteks Skala Besar untuk Norwegian', 'mt': 'Mudellar tal-Lingwa Kontekstwali fuq Skala kbira għan-Norveġiż', 'mn': 'Норвегийн үеийн том-хэмжээний контекст хэл загвар', 'kk': 'Норвегиялық үлкен масштабтағы контекстуализияланған тіл үлгісі', 'ml': 'നോര്\u200dവേജിയനുള്ള ഭാഷ മോഡിലേക്കു് വലിയ- വലിപ്പം വികസിപ്പിക്കുക', 'no': 'Stor skalering av kontekstualisert språk for norsk', 'sr': 'Velika skala kontekstualizacija jezika za norveški', 'si': 'නෝර්වේජියාන් වෙනුවෙන් ලොකු ප්\u200dරමාණය සම්බන්ධ භාෂාව මඩේලින්', 'so': 'Isku wareegista luqada ee Norwegian', 'ro': 'Modelare lingvistică contextualizată la scară largă pentru norvegiană', 'pl': 'Kontekstualizowane modelowanie języka na dużą skalę dla norweskiego', 'ta': 'நார்வீஜியனுக்கான பெரிய அளவு மாற்று', 'sv': 'Storskalig kontextualiserad språkmodellering för norska', 'ur': 'نوروژی کے لئے بڑا- اسکیل کنٹکسٹولیز زبان موڈلینگ', 'uz': 'Name', 'vi': 'Chế độ ngôn ngữ phổ biến cho người Na Uy', 'da': 'Storskalig kontekstualiseret sprogmodellering for norsk', 'nl': 'Grootschalige contextualiseerde taal modellering voor Noors', 'id': 'Modeling Bahasa Berkontekstualisasi Skala Besar untuk Norwegian', 'ko': '노르웨이어 대규모 어경화 언어 모델링', 'bg': 'Широко контекстуализирано езиково моделиране за норвежки език', 'hr': 'Velika skala kontekstualizacija jezika za norveški', 'de': 'Umfangreiche kontextualisierte Sprachmodellierung für Norwegisch', 'tr': 'Norwegiýan üçin Ullakan Kalamlar Konteksualized Dil Modeli', 'af': 'Groot- Skaleer Konteksualiseerde Taal Modelling vir NoorweegsName', 'fa': 'ЩҶЩ…ЩҲЩҶЩҮвҖҢШіШ§ШІЫҢ ШІШЁШ§ЩҶЫҢ ШЁШІШұЪҜ ШЁШұШ§ЫҢ ЩҶЩҲШұЩҲЪҳЫҢ', 'hy': 'Նորվեգերենի լեզվի մեծ մասշտաբ կոնտեքստալիզացված մոդելը', 'az': 'N칬rve칞톛 칲칞칲n b칬y칲k-칬l칞칲l칲 m칲xt톛lif dil modell톛ri', 'sw': 'Kiwango kikubwa kinachohusiana na lugha ya KiNorway', 'bn': 'নরওয়েজিয়ার জন্য বড়- আকার পরিবর্তন করা ভাষা মডেলিংল', 'sq': 'Modelimi i gjuhës me shkallë të madhe në kontekst për norvegjisht', 'bs': 'Velika skala kontekstualizacija jezika za norveški', 'am': 'undo-type', 'ca': 'Modell de llenguatge contextualitzat a gran escala per a norueguès', 'cs': 'Rozsáhlé kontextualizované modelování jazyka pro norštinu', 'et': 'Norra keele suuremahuline kontekstualiseeritud keele modelleerimine', 'fi': 'Laajamittainen kontekstualisoitu kielimallinnus norjalle', 'jv': 'Language', 'he': 'דוגמנית שפת מקורקסטולוגית במערכת גדולה לנורבגית', 'ha': '@ item Spelling dictionary', 'sk': 'Obsežno kontekstualizirano jezikovno modeliranje za norveščino', 'bo': 'ནོར་ཝེ་ཤེས་ལ་སྤྱོད་པའི་སྐད་རིགས་ཚད་ཆེ་བའི་ཁོར་ཡིག་རྟགས་བཀོད་པ'}
{'en': 'We present the ongoing NorLM initiative to support the creation and use of very large contextualised language models for Norwegian (and in principle other Nordic languages), including a ready-to-use software environment, as well as an experience report for data preparation and training. This paper introduces the first large-scale monolingual language models for Norwegian, based on both the ELMo and BERT frameworks. In addition to detailing the training process, we present contrastive benchmark results on a suite of NLP tasks for Norwegian. For additional background and access to the data, models, and software, please see : http://norlm.nlpl.eu', 'fr': "Nous présentons l'initiative NorLM en cours visant à soutenir la création et l'utilisation de très grands modèles linguistiques contextualisés pour le norvégien (et en principe d'autres langues nordiques), y compris un environnement logiciel prêt à l'emploi, ainsi qu'un rapport d'expérience pour la préparation des données et la formation. Cet article présente les premiers modèles linguistiques monolingues à grande échelle pour le norvégien, basés à la fois sur les frameworks ElMo et BERT. En plus de détailler le processus de formation, nous présentons des résultats de référence contrastés sur une série de tâches de PNL pour le norvégien. Pour plus d'informations et pour accéder aux données, aux modèles et aux logiciels, veuillez consulter\xa0: http://norlm.nlpl.eu", 'pt': 'Apresentamos a iniciativa NorLM em andamento para apoiar a criação e uso de modelos de linguagem contextualizados muito grandes para norueguês (e, em princípio, outras línguas nórdicas), incluindo um ambiente de software pronto para uso, bem como um relato de experiência para preparação de dados e treinamento . Este artigo apresenta os primeiros modelos de linguagem monolíngue em grande escala para norueguês, baseados nas estruturas ELMo e BERT. Além de detalhar o processo de treinamento, apresentamos resultados comparativos contrastantes em um conjunto de tarefas de PNL para norueguês. Para informações adicionais e acesso aos dados, modelos e software, consulte: http://norlm.nlpl.eu', 'ar': 'نقدم مبادرة NorLM الجارية لدعم إنشاء واستخدام نماذج لغة كبيرة جدًا حسب السياق للنرويجية (ولغات الشمال الأخرى من حيث المبدأ) ، بما في ذلك بيئة برمجية جاهزة للاستخدام ، بالإضافة إلى تقرير تجربة لإعداد البيانات والتدريب . تقدم هذه الورقة النماذج الأولى واسعة النطاق للغة أحادية اللغة للنرويجية ، بناءً على كل من إطار عمل ELMo و BERT. بالإضافة إلى تفصيل عملية التدريب ، نقدم نتائج معيارية متباينة على مجموعة من مهام البرمجة اللغوية العصبية للنرويجية. للحصول على خلفية إضافية والوصول إلى البيانات والنماذج والبرامج ، يرجى الاطلاع على: http://norlm.nlpl.eu', 'es': 'Presentamos la iniciativa NorLM en curso para apoyar la creación y el uso de modelos lingüísticos contextualizados muy grandes para noruego (y, en principio, otros idiomas nórdicos), incluido un entorno de software listo para usar, así como un informe de experiencia para la preparación de datos y la capacitación. Este artículo presenta los primeros modelos lingüísticos monolingües a gran escala para noruego, basados en los marcos eLMO y BERT. Además de detallar el proceso de capacitación, presentamos resultados de referencia contrastantes en un conjunto de tareas de PNL para Norwegian. Para obtener información adicional y acceso a los datos, los modelos y el software, visite: http://norlm.nlpl.eu', 'zh': '臣等建NorLM计,以扶持为挪威语(,原是他北欧语言)创用非常大的上下文化语言模样,即用软件境内,以及备培训经验以闻。 本文首基ELMoBERT框架之挪威语大单语语言模样。 自详练之外,为挪威语给NLP者试于基准。 其他背景及访问数软件,请参阅:http://norlm.nlpl.eu', 'ja': '私たちは、すぐに使用できるソフトウェア環境を含む、ノルウェー語（および原則として他の北欧言語）のための非常に大きな文脈化された言語モデルの作成と使用をサポートするための継続的なNorLMイニシアチブと、データの準備とトレーニングのための経験レポートを提示します。ELMoとBERTの両方のフレームワークに基づいて、ノルウェー語のための最初の大規模な単一言語言語モデルを紹介します。トレーニングプロセスの詳細化に加えて、ノルウェー人のための一連のNLPタスクの対照的なベンチマーク結果を提示します。データ、モデル、ソフトウェアへの追加の背景とアクセスについては、http://norlm.nlpl.euを参照してください。', 'hi': 'हम नॉर्वेजियन (और सिद्धांत रूप में अन्य नॉर्डिक भाषाओं) के लिए बहुत बड़े प्रासंगिक भाषा मॉडल के निर्माण और उपयोग का समर्थन करने के लिए चल रही NorLM पहल प्रस्तुत करते हैं, जिसमें एक रेडी-टू-यूज सॉफ़्टवेयर वातावरण, साथ ही साथ डेटा तैयारी और प्रशिक्षण के लिए एक अनुभव रिपोर्ट भी शामिल है। यह पेपर नार्वेजियन के लिए पहले बड़े पैमाने पर मोनोलिंगुअल भाषा मॉडल का परिचय देता है, जो ELMo और BERT फ्रेमवर्क दोनों पर आधारित है। प्रशिक्षण प्रक्रिया का विवरण देने के अलावा, हम नॉर्वेजियन के लिए एनएलपी कार्यों के एक सूट पर कंट्रास्टिव बेंचमार्क परिणाम प्रस्तुत करते हैं। अतिरिक्त पृष्ठभूमि और डेटा, मॉडल और सॉफ़्टवेयर तक पहुंच के लिए, कृपया देखें: http://norlm.nlpl.eu', 'ru': 'Мы представляем текущую инициативу NorLM по поддержке создания и использования очень больших контекстуализированных языковых моделей для норвежского (и в принципе других скандинавских языков), включая готовую к использованию программную среду, а также отчет об опыте подготовки данных и обучения. В настоящем документе представлены первые крупномасштабные одноязычные языковые модели для норвежского языка, основанные на концепциях ELMo и BERT. В дополнение к детализации процесса обучения, мы представляем контрастные контрольные результаты по набору задач NLP для норвежского языка. Для получения дополнительной информации и доступа к данным, моделям и программному обеспечению, пожалуйста, посетите: http://norlm.nlpl.eu', 'ga': 'Cuirimid an tionscnamh leanúnach NorLM i láthair chun tacú le cruthú agus le húsáid samhlacha teanga comhthéacsúla an-mhór don Ioruais (agus i bprionsabal do theangacha Nordacha eile), lena n-áirítear timpeallacht bogearraí atá réidh le húsáid, chomh maith le tuarascáil taithí maidir le hullmhú sonraí agus oiliúint. . Tugann an páipéar seo na chéad mhúnlaí teanga aonteangacha ar mhórscála don Ioruais, bunaithe ar chreataí ELMo agus BERT araon. Chomh maith leis an bpróiseas oiliúna a mhionsonrú, cuirimid torthaí tagarmharcála codarsnacha i láthair ar shraith tascanna NLP don Ioruais. Le haghaidh cúlra breise agus rochtain ar na sonraí, samhlacha, agus bogearraí, féach le do thoil: http://norlm.nlpl.eu', 'el': 'Παρουσιάζουμε τη συνεχιζόμενη πρωτοβουλία για την υποστήριξη της δημιουργίας και χρήσης πολύ μεγάλων γλωσσικών μοντέλων για τα νορβηγικά (και καταρχήν άλλες σκανδιναβικές γλώσσες), συμπεριλαμβανομένου ενός έτοιμου προς χρήση περιβάλλοντος λογισμικού, καθώς και μια έκθεση εμπειρίας για την προετοιμασία δεδομένων και την κατάρτιση. Η παρούσα εργασία εισάγει τα πρώτα μεγάλης κλίμακας μονογλωσσικά μοντέλα γλώσσας για τα νορβηγικά, βασισμένα τόσο στο πλαίσιο ELMo όσο και στο πλαίσιο BERT. Εκτός από τη λεπτομερή περιγραφή της διαδικασίας κατάρτισης, παρουσιάζουμε αντιφατικά αποτελέσματα αναφοράς σε μια σειρά εργασιών για τη Νορβηγική. Για επιπλέον υπόβαθρο και πρόσβαση στα δεδομένα, μοντέλα και λογισμικό, δείτε: http://norlm.nlpl.eu', 'it': "Presentiamo l'iniziativa NorLM in corso per sostenere la creazione e l'uso di modelli linguistici contestualizzati molto ampi per il norvegese (e in linea di principio altre lingue nordiche), incluso un ambiente software pronto all'uso, nonché un report di esperienza per la preparazione e la formazione dei dati. Questo articolo introduce i primi modelli linguistici monolingue su larga scala per il norvegese, basati sia sul framework ELMo che BERT. Oltre a descrivere dettagliatamente il processo di formazione, presentiamo risultati di benchmark contrastanti su una serie di attività NLP per Norwegian. Per ulteriori informazioni sullo sfondo e l'accesso a dati, modelli e software, vedere: http://norlm.nlpl.eu", 'hu': 'Bemutatjuk a NorLM folyamatban lévő kezdeményezését, amely támogatja a norvég (és elvileg más skandináv nyelvek) nagyon nagy kontextuális nyelvi modelljeinek létrehozását és használatát, beleértve a használatra kész szoftverkörnyezetet, valamint egy tapasztalatjelentést az adatok előkészítéséhez és képzéséhez. Ez a tanulmány bemutatja az első nagyszabású, egynyelvű nyelvi modelleket norvég számára, amelyek mind az ELMo, mind a BERT keretrendszereken alapulnak. A képzési folyamat részletes bemutatása mellett kontrasztos referenciaeredményeket mutatunk be a norvég NLP feladatok számára. Az adatokhoz, modellekhez és szoftverekhez való további háttér és hozzáférés a következő oldalon található: http://norlm.nlpl.eu', 'ka': 'ჩვენ ჩვენ ჩვენ განვითარებით ნორგვეული (და პრინციპში სხვა ნორდიული ენებისთვის) პროგრამეტური პროგრამეტური გამოყენებელი პროგრამეტური გარეშე და გამოყენებას, რომელიც გამოყენებელი პროგრამეტური გარეშე და გამოყენებელ ამ დოკუნტის პირველი დიდი მენოლენგური მოდელის მოდელები ნორვეგიური სახელისთვის, რომელიც ELMo და BERT ფრამების დაბაზეულია. პროცესის დამატებით, ჩვენ ნორგვეული NLP სამუშაოდ კონტრასტური ბენქმერის შედეგების შედეგება ჩვენ ჩვენ ჩვენ გავაკეთებთ. დამატებული ფონი და მონაცემები, მოდელები და პროგრამის შესაძლებლობად მოდით: http://norlm.nlpl.eu', 'mk': 'Ние ја претставуваме тековната иницијатива НорЛМ за поддршка на создавањето и употребата на многу големи контекстуализирани јазички модели за норвешки (и во принцип други нордски јазици), вклучувајќи софтверска средина подготвена за употреба, како и извештај за искуство за Овој весник ги претставува првите модели на монојазички јазик на голема skalа за норвешки, базирани на рамките на ЕЛМО и БЕРТ. Покрај детализирањето на процесот на обука, ги претставуваме контрастивните резултати на резултатите на резултатите на групата на НЛП задачи за норвешки. For additional background and access to the data, models, and software, please see:  http://norlm.nlpl.eu', 'kk': 'Біз Norwegian (және басқа Норвегия тілдерінің басқа тілдері) үшін үлкен контекстуалды тіл моделдерін құру және қолдану үшін қолданатын NorLM инициативасын таңдаймыз. Бағдарламаларды қолдануға дайын және деректерді дайындау және оқыту Бұл қағаз Норвегияның бірінші үлкен тіл үлгілерін келтіреді, ELMo және BERT фреймдерінің негізінде. Оқыту процесінің егжей- тегжейін біз норвегиялық NLP тапсырмаларының көмегімен контрастық белгісін көрсетедік. Қосымша аясы мен деректерге, үлгілеріне және бағдарламаға қатынау үшін қараңыз: http://norlm.nlpl.eu', 'ml': 'നോര്\u200dവേജിയന്റെ സൃഷ്ടിപ്പിനും ഉപയോഗിക്കാനും വളരെ വലിയ ഭാഷ മോഡലുകള്\u200dക്കും പിന്തുണയ്ക്കാനും നോര്\u200dട്ടിക്ക് ഭാഷകള്\u200dക്കും പ്രധാനപ്പെടുത്തുന്ന നോര്\u200dഎല്\u200dഎമിന്റ ഈ പത്രത്തില്\u200d നോര്\u200dവേജിയന്റെ ആദ്യത്തെ വലിയ ഭാഷ മോഡലുകളെ പരിചയപ്പെടുത്തുന്നു. എല്\u200dമോയും ബെര്\u200dട്ടി ഫ്രെയിമോര്\u200dക്കും അടിസ്ഥ പരിശീലനത്തിന്റെ പ്രക്രിയയെ വിശദീകരിക്കുന്നതിനു ശേഷം, നോര്\u200dവേജിയന്റെ ഒരു സ്യൂട്ട് NLP ജോലികള്\u200dക്ക് വേണ്ടി നിര്\u200d കൂടുതല്\u200d പശ്ചാത്തലം, ഡേറ്റ, മോഡല്\u200d, സോഫ്റ്റ്\u200cവെയര്\u200d എന്നിവയ്ക്കുള്ള അനുവാദം, ദയവായി നോക്കുക: http://norlm.nlpl.eu', 'mt': 'Aħna nippreżentaw l-inizjattiva NorLM li għaddejja bħalissa biex tappoġġja l-ħolqien u l-użu ta’ mudelli lingwistiċi kuntestwalizzati kbar ħafna għan-Norveġiż (u fil-prinċipju lingwi Nordiċi oħra), inkluż ambjent ta’ softwer lest għall-użu, kif ukoll rapport ta’ esperjenza għall-preparazzjoni u t-taħriġ tad-dejta. Dan id-dokument jintroduċi l-ewwel mudelli monolingwi fuq skala kbira għan-Norveġiż, ibbażati kemm fuq l-oqfsa ELMo kif ukoll BERT. Minbarra d-dettalji tal-proċess ta’ taħriġ, a ħna nippreżentaw riżultati ta’ parametri ta’ referenza kuntrastivi dwar sett ta’ kompiti NLP għan-Norveġiż. Għal sfond addizzjonali u aċċess għad-dejta, mudelli u softwer, jekk jogħġbok ara: http://norlm.nlpl.eu', 'ms': 'We present the ongoing NorLM initiative to support the creation and use of very large contextualised language models for Norwegian (and in principle other Nordic languages), including a ready-to-use software environment, as well as an experience report for data preparation and training.  Kertas ini memperkenalkan model bahasa monobahasa skala besar pertama untuk bahasa Norwegian, berdasarkan bingkai ELMo dan BERT. Selain menjelaskan proses latihan, kami mempersembahkan hasil benchmark bertentangan pada suite tugas NLP untuk Norwegian. Untuk latar belakang tambahan dan akses ke data, model, dan perisian, sila lihat: http://norlm.nlpl.eu', 'no': 'Vi presenterer den gjeldande NorLM-initiativet for å støtta opprettinga og bruk av veldig stor kontekstualiserte språk-modeller for norsk (og i prinsippet andre nordske språk), inkludert eit program-miljø som er klar for bruk, og ein opplevingsrapport for data forberedning og opplæring. Denne papiret introduserer den første stor språkkmodellen for norsk, basert på begge ELMo- og BERT- rammeverka. I tillegg til detaljering av opplæringsprosessen, presenterer vi kontrastbenchmarkresultatet på ein del av NLP-oppgåver for norsk. For ekstra bakgrunn og tilgang til data, modeller og programvare, sjå: http://norlm.nlpl.eu', 'lt': 'Mes pristatome dabartinę NorLM iniciatyvą remti labai didelių kontekstinių kalbų modelių kūrimą ir naudojimą norvegų kalbomis (ir iš esmės kitomis Šiaurės kalbomis), įskaitant parengtos naudoti programinės įrangos aplinką, taip pat duomenų rengimo ir mokymo patirties ataskaitą. Šiame dokumente pateikiami pirmieji didelio masto monokalbiniai norvegų kalbos modeliai, grindžiami tiek ELMo, tiek BERT sistemomis. Be išsamaus mokymo proceso, mes pateikiame prieštaringus lyginamuosius rezultatus dėl daugelio NLP užduočių norvegams. Daugiau informacijos ir prieigos prie duomenų, modelių ir programinės įrangos galima rasti: http://norlm.nlpl.eu', 'ro': 'Prezentăm inițiativa NorLM în curs de desfășurare pentru a sprijini crearea și utilizarea unor modele lingvistice contextualizate foarte mari pentru norvegiană (și, în principiu, alte limbi nordice), inclusiv un mediu software gata de utilizare, precum și un raport de experiență pentru pregătirea și formarea datelor. Această lucrare prezintă primele modele lingvistice monolingve la scară largă pentru norvegiană, bazate atât pe cadrele ELMo, cât și BERT. Pe lângă detaliile procesului de formare, prezentăm rezultate contrastante de referință pentru o suită de sarcini PNL pentru Norwegian. Pentru informații suplimentare și acces la date, modele și software, consultați: http://norlm.nlpl.eu', 'mn': 'Бид Норвегийн (бас зарим Норвегийн хэлний хэлний үндсэн хэл), програм хангамж хэрэглэх бэлэн орчин болон өгөгдлийн бэлтгэл болон сургалтын туршлагатай туршлагатай мэдээллийг бүтээх болон хэрэглэхийн тулд оршиж байгаа NorLM инициативийг тайлбарлаж байна. Энэ цаас Норвегийн хэлний анхны том хэлний загварыг илтгэдэг. ELMo болон BERT хэлбэрүүдийн хоёуланг үндсэн. Боловсролын үйл явцыг нарийвчлахад бид Норвегийн NLP үйл ажиллагааны тухай эсрэг тооны үзүүлэлтийг харуулж байна. Өгөгдөл, загвар, програм хангамжинд нэмэлт хэсгүүд болон хүртэл хангамжийн тулд хараарай: http://norlm.nlpl.eu', 'pl': 'Przedstawiamy trwającą inicjatywę NorLM wspierającą tworzenie i stosowanie bardzo dużych kontekstowych modeli językowych dla języków norweskich (a w zasadzie innych języków nordyckich), w tym gotowego do użycia środowiska oprogramowania, a także raport doświadczeń dotyczący przygotowywania danych i szkoleń. Niniejszy artykuł przedstawia pierwsze wielkoskalowe jednojęzyczne modele językowe dla norweskiego, oparte zarówno na ramach ELMo, jak i BERT. Oprócz szczegółowych opisów procesu szkolenia, prezentujemy kontrastywne wyniki referencyjne dla zestawu zadań NLP dla Norwegian. Dodatkowe informacje i dostęp do danych, modeli i oprogramowania znajdują się na stronie: http://norlm.nlpl.eu', 'so': 'Waxaynu soo bandhignaynaa howlaha NorLM ee soo socda si uu u caawiyo abuuridda iyo isticmaalidda noocyada afka Norweynta oo aad u weyn (iyo ugu horeynta luuqadaha kale ee Waqooyiga Yurub), kuwaas oo ah qoraal u diyaar ah in loo isticmaalo qoraalka software, iyo warqad arimaha u diyaarinta iyo waxbarashada. Warqaddan waxaa lagu soo bandhigaa noocyada afka ugu horeysa ee afka Norwey ee ugu horeeyay oo ku saleysan qoraalka ELMo iyo BERT. Ka sokoow koorasyada waxbarashada, waxaynu soo bandhignaynaa matooyin kala duwan oo ka soo jeeda kooxda shaqaalaha NLP ee Norwegian. Ogsoonow background dheeraad ah iyo isticmaalka macluumaadka, tusaalaha iyo software: http://norlm.nlpl.eu', 'sv': 'Vi presenterar det pågående NorLM-initiativet för att stödja skapandet och användningen av mycket stora kontextuella språkmodeller för norska (och i princip andra nordiska språk), inklusive en användarfärdig mjukvaromiljö, samt en erfarenhetsrapport för dataförberedelse och utbildning. Denna uppsats introducerar de första storskaliga enspråksmodellerna för norska, baserade på både ELMo och BERT ramverk. Förutom att detaljera träningsprocessen presenterar vi kontrasterande jämförelseresultat på en uppsättning NLP-uppgifter för Norwegian. För ytterligare bakgrund och tillgång till data, modeller och programvara, se: http://norlm.nlpl.eu', 'ta': 'நாம் நோர்வீஜியனுக்கு மிகப்பெரிய பாதிக்கப்பட்ட மொழி மாதிரிகளை உருவாக்க மற்றும் பயன்படுத்துவதற்காக நடப்பு நோர்ல்எம் திட்டமைப்பை காண்பிக்கிறோம் (மற்றும் ம இந்த தாள் முதல் பெரிய அளவு மொழி மொழி மாதிரி மாதிரிகளை குறிப்பிடுகிறது, ELMo மற்றும் BERT சட்டங்களையும் அடிப்படையில். பயிற்சி செயல்பாட்டை விவரமாக்குவதற்குக் கூட, நாம் நார்வீஜியனுக்கான NLP பணிகளின் தொகுதியில் வித்தியாசமான பெங கூடுதல் பின்னணி மற்றும் தரவு, மாதிரி, மற்றும் மென்பொருள் அணுகலுக்கு, தயவு செய்து பார்க்கவும்: http://norlm.nlpl.eu', 'sr': 'Predstavljamo trenutnu NorLM inicijativu kako bi podržali stvaranje i upotrebu veoma velikih kontekstualiziranih jezičkih modela za Norveške (i u principu drugih Nordičkih jezika), uključujući i spremno za upotrebu softvera, kao i izveštaj o iskustvu za pripremu i obuku podataka. Ovaj papir predstavlja prve velike monojezičke modele za norveške, zasnovane na ELMo i BERT okvirama. Pored detalja procesa obuke, predstavljamo rezultate kontrastivne kritike za apartman zadataka NLP za Norveške. Za dodatni pozadini i pristup podacima, modelima i softvera, molim vas da vidite: http://norlm.nlpl.eu', 'si': 'අපි නෝර්වේජියාන් වලට ගොඩක් ලොකු භාෂාව ප්\u200dරවේශය සහ භාෂාව ප්\u200dරවේශනය සඳහා ප්\u200dරවේශනය සඳහා සොෆ්ටවේජ් වර්තනය සඳහා සූදානම් වෙන්න ප්\u200dරවේශනය මේ පත්තේ නෝර්වේජියානු වෙනුවෙන් පළමු ලොකු විශාල භාෂාවක් මෝඩේල් එක්ක ප්\u200dරමාණය කරනවා, ELMo සහ BERT ප්\u200dරමාණ ප්\u200dරධාන පරීක්ෂණය විස්තර කරන්න, අපි නෝර්වේජියාන්ට NLP වැඩක් සූදානයක් වෙනුවෙන් ප්\u200dරතිදේශ බෙන්ච්මා තවත් පසුබිම් සහ දත්ත, මොඩේල් සහ සොෆ්ටවර් එකට ප්\u200dරවේශනය සඳහා, කරුණාකරලා බලන්න: http://norlm.nlpl.eu', 'ur': 'ہم نے نوروژی کے لئے بہت بڑے کنٹکسٹیٹ کی زبان مدلکوں کی پیدائش اور استعمال کی مدد کرنے کے لئے (اور دوسری نوروژی زبانوں کی اصل میں) ایک سوفٹ ویروژی کا استعمال کرنے کے لئے تیار ہے اور ایک تجربہ گزارش کے لئے۔ This paper introduces the first large-scale monolingual language models for Norwegian, based on both ELMo and BERT frameworks. تدریس پرسس کی تفصیل کے علاوہ، ہم نوروژی کے لئے NLP کاموں کے ایک سوئٹ پر کنٹرسیٹ بنچم مارک کا نتیجہ دکھاتے ہیں. ڈیٹا، موڈل اور سوفٹوفر کے لئے اضافہ پچھلی اور دسترسی کے لئے، لطفاً دیکھو: http://norlm.nlpl.eu', 'uz': "Biz Norvegiya uchun juda katta taʼminlov qilingan tillar modellarini qoʻllash va ishlatish uchun hozir Norvegiya (va boshqa tillar asosida, dastur muhitida ishlatish tayyorligi va maʼlumot tayyorlash va tahrirlash uchun tajriba haqida xabar beruvchiga ega. Bu qogʻoz ELMo va BERT freymlari asosida, Norvegiyaga birinchi katta tildagi monolingual modellarini koʻrsatadi. Taʼminlovchi jarayoni taʼrif qilishdan koʻproq, Norvegcha uchun NLP vazifalar tizimi haqida ko'rsatuvchimiz. Qoʻshimcha orqa fon uchun maʼlumot, modellar va dasturga murojaat qilish uchun iltimos koʻring: http://norlm.nlpl.eu", 'vi': 'Chúng tôi xin giới thiệu một khởi đầu kiểu NorLM để hỗ trợ tạo và sử dụng các mô hình ngôn ngữ ngữ ngữ ngữ ngữ được áp chế rất lớn cho người Na Uy (và trên nguyên tắc là những ngôn ngữ Bắc Âu khác), bao gồm một môi trường mềm sẵn sàng sử dụng, và một báo cáo kinh nghiệm để chuẩn bị dữ liệu và đào tạo. Tờ giấy này giới thiệu những mô hình ngôn ngữ ngữ ngôn ngữ lớn đầu tiên của người Na Uy, dựa trên cấu trúc ElMo và BERT. Ngoài việc mô tả chi tiết về quá trình đào tạo, chúng tôi cung cấp kết quả tiêu chuẩn khác nhau về một loạt các công việc của NMB cho người Na Uy. Để tìm hiểu thêm về dữ liệu, mẫu và phần mềm, xem: http://norlm.nlpl.eu', 'da': 'Vi præsenterer det igangværende NorLM initiativ til at støtte skabelsen og brugen af meget store kontekstualiserede sprogmodeller for norsk (og i princippet andre nordiske sprog), herunder et brugsklart softwaremiljø, samt en erfaringsrapport til dataforberedelse og træning. Denne artikel introducerer de første storstilede ensprogede sprogmodeller for norsk, baseret på både ELMo og BERT rammer. Ud over at detaljere træningsprocessen præsenterer vi kontrastive benchmark resultater på en række NLP-opgaver for Norwegian. For yderligere baggrund og adgang til data, modeller og software, se venligst: http://norlm.nlpl.eu', 'bg': 'Представяме текущата инициатива за подкрепа на създаването и използването на много големи контекстуализирани езикови модели за норвежки (и по принцип други скандинавски езици), включително готова за използване софтуерна среда, както и доклад за опита за подготовка и обучение на данни. Настоящата статия представя първите широкомащабни едноезични езикови модели за норвежки език, базирани както на рамките на ЕЛМО, така и на BERT. Освен подробно описание на процеса на обучение, ние представяме контрастни референтни резултати по набор от задачи за НЛО за норвежки. За допълнителна информация и достъп до данните, моделите и софтуера, моля вижте: http://norlm.nlpl.eu', 'nl': 'We presenteren het lopende NorLM-initiatief ter ondersteuning van de creatie en het gebruik van zeer grote contextualiseerde taalmodellen voor het Noors (en in principe andere Scandinavische talen), inclusief een kant-en-klare softwareomgeving, evenals een ervaringsrapport voor gegevensvoorbereiding en training. Dit document introduceert de eerste grootschalige eentalige taalmodellen voor het Noors, gebaseerd op zowel de ELMo als BERT frameworks. Naast het detailleren van het trainingsproces presenteren we contrasterende benchmarkresultaten op een reeks NLP-taken voor Noors. Zie voor aanvullende achtergrond en toegang tot de gegevens, modellen en software: http://norlm.nlpl.eu', 'de': 'Wir stellen die laufende NorLM-Initiative vor, um die Erstellung und Verwendung sehr großer kontextualisierter Sprachmodelle für Norwegisch (und im Prinzip andere nordische Sprachen) zu unterstützen, einschließlich einer gebrauchsfertigen Softwareumgebung sowie eines Erfahrungsberichts zur Datenaufbereitung und Schulung. Dieser Beitrag stellt die ersten großangelegten monolingualen Sprachmodelle für Norwegisch vor, die sowohl auf dem ELMo- als auch auf dem BERT-Framework basieren. Neben der detaillierten Darstellung des Trainingsprozesses präsentieren wir kontrastive Benchmark-Ergebnisse für eine Reihe von NLP-Aufgaben für Norwegian. Weitere Hintergründe und Zugriff auf Daten, Modelle und Software finden Sie unter: http://norlm.nlpl.eu', 'hr': 'Predstavljamo nastavku NorLM inicijativu kako bi podržali stvaranje i upotrebu veoma velikih kontekstualiziranih jezičkih modela za Norveške (i u principu drugih nordskih jezika), uključujući pripremnu softver za upotrebu, kao i izvještaj o iskustvu za pripremu i obuku podataka. Ovaj papir predstavlja prve velike monojezičke modele za norveške jezike na temelju i ELMo i BERT okvira. Osim detalja procesa obuke, predstavljamo rezultate kontrastne kritike o apartmanu zadataka NLP-a za Norveške. Za dodatni pozadini i pristup podacima, modelima i softvera, molim vas vidjeti: http://norlm.nlpl.eu', 'ko': '노르웨이어(원칙적으로 다른 북유럽어도 있음)를 지원하는 대형 어경화 언어 모델의 창설과 사용, 수시로 사용할 수 있는 소프트웨어 환경과 데이터 준비 및 교육에 대한 경험 보고서를 포함하여 현재 진행 중인 NorLM 계획을 소개했다.본고는 첫 번째로 ELMo와 BERT 구조를 바탕으로 한 대규모 노르웨이어 단어 언어 모델을 소개한다.교육 과정을 자세히 설명하는 것 외에 노르웨이인들의 일련의 NLP 임무에 대한 비교 기준 결과도 제공했다.데이터, 모델 및 소프트웨어에 대한 추가 배경 및 액세스 권한은 다음을 참조하십시오.http://norlm.nlpl.eu', 'fa': 'ما ابتدایی NorLM در حال حاضر را برای پشتیبانی ساختن و استفاده از مدل\u200cهای زبان بسیار بزرگ برای نوروژی (و در اصل دیگر زبان\u200cهای نوروژی) نشان می\u200cدهیم، شامل یک محیط نرم\u200cافزار آماده برای استفاده، و گزارش تجربه برای آماده و آموزش داده\u200cها. این کاغذ اولین مدل زبان بزرگ برای نوروژی معرفی می\u200cکند که بر اساس هر دو فرم ELMo و BERT است. در addition to detailing the training process, we present contrastive benchmark results on a suite of NLP tasks for Norwegian. برای پشتیبانی اضافه و دسترسی به داده ها، مدل ها و نرم افزار لطفاً ببینید: http://norlm.nlpl.eu', 'sw': 'Tunawasilisha mradi unaoendelea NorLM kuunga mkono ubunifu na kutumia mifano makubwa ya lugha kwa ajili ya Norway (na kanuni nyingine ya lugha ya Norway), ikiwa ni pamoja na mazingira ya matumizi ya programu za programu, pamoja na ripoti ya uzoefu wa kuandaa na mafunzo ya data. Gazeti hili linaonyesha mifano ya kwanza ya lugha za kiutamaduni kwa ajili ya Norway, kwa kutumia miundombinu ya ELMo na BERT. Zaidi ya kuelezea mchakato wa mafunzo, tunaonyesha matokeo yanayopingana na bendera kuhusu suti la kazi za NLP kwa raia wa Norway. Kwa orodha ya ziada na upatikanaji wa taarifa, mifano na programu, tafadhali tazama: http://norlm.nlpl.eu', 'sq': 'Ne paraqesim nismën në vazhdim të NorLM për të mbështetur krijimin dhe përdorimin e modeleve të mëdha të gjuhës kontekstuale për gjuhën norvegjeze (dhe në parim gjuhë të tjera norvegjeze), duke përfshirë një mjedis programi të gatshëm për përdorim si dhe një raport përvoje për përgatitjen e të dhënave dhe trainimin. Ky dokument paraqet modelet e para monogjuhësore në shkallë të madhe për gjuhën norvegjeze, bazuar si në kuadrin ELMo ashtu edhe BERT. Përveç detajeve të procesit të trainimit, ne paraqesim rezultate kontraste të referimit në një sërë detyrash NLP për norvegjisht. Për sfond shtesë dhe akses ndaj të dhënave, modeleve dhe softuereve, ju lutem shihni: http://norlm.nlpl.eu', 'am': 'የአሁኑን ኖርLM ፕሮጀክት ለኖርዌጂያን መፍጠር እና ለመጠቀም የተለየ ትልቅ የቋንቋ ምሳሌዎችን ለመጠቀም እና ለመጠቀም እና ለዳታ ማዘጋጀት እና ለመጠቀም የተዘጋጀ ፕሮፍትዌር አካባቢ እና ለመጠቀም የተዘጋጀ ፕሮግራም እና ለመዘጋጀት የዝግጅት ዝርዝር እና ለመጠ ይህ ፕሮግራም የኖርዌጂያን የ.አ.አ.አ.አ.አ.አ.አ.አ.አ.አ.አ.አ.አ. ትምህርት ፕሮጀክቱን ለመግለጽ በተጨማሪም፣ ለኖርዌጂያን የNLP ስራ ጉዳይ ፍሬዎችን እናቀርባታለን፡፡ ለመጨመር መደብ እና ለዳታ፣ ሞዴል እና ሶፍትዌር ለመግኘት እባክዎ ተመልከቱ: http://norlm.nlpl.eu', 'hy': 'Մենք ներկայացնում ենք NORLM-ի ընթացքում գոյություն ունեցող նախաձեռնությունը, որպեսզի աջակցենք նորվեգերենի (և սկզբունքով այլ նորվեգերենի լեզուների) շատ մեծ լեզվային մոդելների ստեղծման և օգտագործման, ներառյալ օգտագործելու պատրաստ ծրագրային միջավայր, ինչպես Այս հոդվածը ներկայացնում է նորվեգերենի համար առաջին մեծ լեզվով մեկլեզվով մոդելները, որոնք հիմնված են ELMo-ի և BER-ի շրջանակների վրա: Ապրոցեսի մանրամասնությունը բացի, մենք ներկայացնում ենք հակադրական համեմատական արդյունքներ նորվեգերենի համար ՆՈՊ-ի խնդիրների մի շարք համեմատական վերաբերյալ: Ավելին, տվյալների, մոդելների և ծրագրերի հասանելիության համար, խնդրում եմ տեսնել. http://norlm.nlpl.eu', 'az': 'Nörveçlər üçün çox böyük müxtəlif dil modellərinin yaradılmasını və istifadə etməsini təsdiqləmək üçün sürəkli NorLM iniciativünü təsdiqləyirik (və əslində başqa Nordik dillərində), proqramlı vasitəsilə istifadə etmək üçün hazırlanmış və istifadə etmək üçün hazırlanmış bir proqramın ortamı da, məlumatlar tədar Bu kağıt Norveçə üçün ilk böyük dil modellərini təşkil edir, ELMo və BERT frameworklarına dayanan. Eğitim prosesini detaylaşdırmaq üçün, Norveçə üçün NLP işlərinin müxtəlif benchmark sonuçlarını göstəririk. Əlavə arxa plan və məlumatlara, modellərə və proqramlara istifadə etmək üçün, lütfen görün: http://norlm.nlpl.eu', 'id': 'Kami mempersembahkan inisiatif NorLM yang sedang berlangsung untuk mendukung penciptaan dan penggunaan model bahasa kontekstualisasi yang sangat besar untuk bahasa Norwegia (dan pada prinsip bahasa Nordik lainnya), termasuk lingkungan perangkat lunak yang siap untuk digunakan, serta laporan pengalaman untuk persiapan dan pelatihan data. Kertas ini memperkenalkan model bahasa monobahasa skala pertama untuk bahasa Norwegia, berdasarkan bingkai ELMo dan BERT. Selain mendetail proses pelatihan, kami mempersembahkan hasil benchmark kontras pada suite tugas NLP untuk Norwegian. Untuk latar belakang tambahan dan akses ke data, model, dan perangkat lunak, silakan lihat: http://norlm.nlpl.eu', 'bn': 'আমরা নরওয়েজিয়ার জন্য বিশাল প্রতিযোগিতা ভাষার মডেল ব্যবহারের সমর্থন এবং ব্যবহার করার জন্য চলমান নরএলএম উদ্যোগ উপস্থাপন করেছি, যার মধ্যে একটি সফটওয়্যার পরিবেশ ব্যবহারের প্রস এই পত্রিকাটি নরওয়েজিয়ার প্রথম বড় মানুষ ভাষার মডেলটিকে উপস্থাপন করেছে, যা ইলমো এবং বার্টি ফ্রেমের ভিত্তিতে ভিত্তিক। এই প্রশিক্ষণ প্রক্রিয়া বিস্তারিত বার্তা ছাড়াও আমরা নরওয়েজিয়াদের জন্য এনএলপি কাজের একটি স্যুটের উপর বিপরীত বেনম্যার্ক অতিরিক্ত পটভূমি এবং তথ্য, মডেল এবং সফটওয়্যারের প্রবেশের জন্য অনুগ্রহ করে দেখুন: http://norlm.nlpl.eu', 'bs': 'Predstavljamo trenutnu NorLM inicijativu kako bi podržali stvaranje i upotrebu veoma velikih kontekstualiziranih jezičkih modela za Norveške (i u principu drugih nordskih jezika), uključujući i spremno za upotrebu softvera, kao i izvješće o iskustvu za pripremu i obuku podataka. Ovaj papir predstavlja prve velike monojezičke modele za norveške, na temelju i ELMo i BERT okvira. Osim detalja procesa obuke, predstavljamo rezultate kontrastne kritike za apartman zadataka NLP za Norveške. Za dodatni pozadini i pristup podacima, modelima i softvera, molim vas vidjeti: http://norlm.nlpl.eu', 'ca': "Presentam la iniciativa NorLM en curs per sostenir la creació i l'ús de models de llenguatge contextualitzats molt grans per a la noruega (i, en principi, altres llenguatges nórdiques), incloent un entorn de software pràctic d'utilitzar, i un informe d'experiència per la preparació i formació de dades. Aquest article introdueix els primers models de llenguatge monolingüe a gran escala en norueg, basats tant en els marcs ELMo com BERT. A més de detallar el procés de formació, presentem resultats contrastants en una sèrie de tasques de NLP per a noruegues. Per a obtenir més fons i accés a les dades, models i software, veureu: http://norlm.nlpl.eu", 'af': "Ons stel die huidige NorLM-iniciatief voor te ondersteun die skep en gebruik van baie groot kontekstualiseerde taal modele vir Noorweërs (en in prinsipe ander Noordse tale), insluitend 'n sagteware omgewing wat gereed is om te gebruik, en ook 'n erfaring raporteer vir data voorbereiding en onderwerp. Hierdie papier introduseer die eerste groot-skaal monolinglike taal modele vir Noorweë, gebaseer op beide die ELMo en BERT raamwerke. In addition to detailing the training process, we present contrastive benchmark results on a suite of NLP tasks for Norwegian. Vir addisionele agtergrond en toegang tot die data, modele en sagteware, sien asseblief: http://norlm.nlpl.eu", 'cs': 'Představujeme probíhající iniciativu NorLM na podporu tvorby a používání velmi rozsáhlých kontextualizovaných jazykových modelů pro norštinu (a v zásadě další severské jazyky), včetně softwarového prostředí připraveného k použití, stejně jako zprávu o zkušenostech s přípravou dat a školením. Tento článek představuje první rozsáhlé jednojjazyčné modely pro norštinu, založené na ELMo a BERT frameworku. Kromě detailního popisu tréninkového procesu prezentujeme kontrastní výsledky referenčních hodnot na sadě NLP úkolů pro Norwegian. Další pozadí a přístup k datům, modelům a softwaru naleznete na: http://norlm.nlpl.eu', 'fi': 'Esittelemme parhaillaan käynnissä olevan NorLM-aloitteen, jolla tuetaan norjan (ja periaatteessa myös muiden pohjoismaisten kielten) erittäin laajojen kontekstualisoitujen kielimallien luomista ja käyttöä, mukaan lukien käyttövalmis ohjelmistoympäristö, sekä kokemusraportin tietojen valmistelusta ja koulutuksesta. Tässä artikkelissa esitellään ensimmäiset monikieliset norjan kielimallit, jotka perustuvat sekä ELMo- että BERT-viitekehyksiin. Koulutuksen yksityiskohtaisen kuvauksen lisäksi esittelemme vertailutuloksia Norjan NLP-tehtävistä. Lisätietoa tiedoista, malleista ja ohjelmistoista saat osoitteesta: http://norlm.nlpl.eu', 'et': 'Esitleme käimasolevat NorLMi algatust, mille eesmärk on toetada norra (ja põhimõtteliselt ka teiste Põhjamaade keelte) väga suurte kontekstipõhiste keelemudelite loomist ja kasutamist, sealhulgas kasutusvalmis tarkvarakeskkonna ning andmete ettevalmistamise ja koolituse kogemuste aruannet. Käesolevas dokumendis tutvustatakse esimesi laiaulatuslikke ühekeelseid norra keele mudeleid, mis põhinevad nii ELMo kui BERT raamistikul. Lisaks koolitusprotsessi üksikasjalikule kirjeldamisele esitame Norra NLP ülesannete komplekti kontrastsed võrdlustulemused. Täiendava tausta ja juurdepääsu andmetele, mudelitele ja tarkvarale leiate aadressilt: http://norlm.nlpl.eu', 'tr': 'Biz öz içinde Norwegiýanyň (we prinsipde beýleki Nordik diller) üçin örän uly contekstualizalýan dil nusgalaryny bejermek we ullanmak üçin gollaýan Norwegiýa başlangyçyny görkeýäris. Softwaryň ortamyny hem taýýarlyk etmäge taýýarlyk we maglumatlar üçin tejribe berilýäris Bu kagyz Norwegça üçin ilkinji uly ölçekli monodil nusgalaryny tanyşdyrýar, ELMo we BERT çarpaklarynda daýanýar. Eğitim prosesini detaylaşdyrmak üçin, Norwegiýanyň NLP işi üçin düýbürli benchmark netijesini görkeýäris. Diňe arkaplan we maglumatlar, modeller we programlere erişim üçin, haýyşt gör: http://norlm.nlpl.eu', 'jv': 'Awak dhéwé éntukno sistem NorLM sing nggawe nggawe akeh iso nggawe lan ngubah sistem sing luwih bantuan kanggo ingkang Norwegian (lan saben usul kanggo langa Nordic liyane), iso nguasai sakjane sampeyan software sing wis arep nggawe, tambah bantuan ingkang data nggawe tarjamahan lan susahe. Perintah iki dadi nggawe model sing sampeyan wigat akeh lan sampeyan ingkang Norwegian, sing basa saben nggawe ETMo karo BERT sampeyan. Genjer-Genjer kelengatan langgar nggambar carane, kita wis nggawe bench-bench sing wis ana karo nggawe NLP adalah kanggo Norwegian. Tulung penyanggang liwat karo akses nang data, model lan software, supoyo ngono: http://norlm.nlpl.eu', 'bo': 'We present the ongoing NorLM initiative to support the creation and use of very large contextualized language models for Norwegian (and in principle of other Nordic languages), including a ready-to-use software environment, as well as an experience report for data preparation and training. འོག་གི་ཤོག་བྱང་འདིས་སྔོན་པོ་ཞིག་གི་ཆེ་ཆུང་བའི་སྐད་རིགས་ཀྱི་མིང་དཔེ་དབྱིབས་དཔེ་གཏོང་བྱས། ELMo་དང་ BERT སྒྲོམ་རིམ་ In addition to detailing the training process, we present contrastive benchmark results on a suite of NLP tasks for Norwegian. རྒྱབ་ལྗོངས་ཁ་སྐོང་རྒྱབ་ལྗོངས་སུ་ཡིག་ཆ། མིག་དཔེ་རིགས། མཉེན་ཆས་ཡིག་ཆར་འདྲི་ཞིབ་བྱེད་རོ http://norlm.nlpl.eu', 'sk': 'Predstavljamo tekočo pobudo NorLM za podporo ustvarjanju in uporabi zelo velikih kontekstualiziranih jezikovnih modelov za norveščino (in načeloma druge nordijske jezike), vključno s programskim okoljem, ki je pripravljeno za uporabo, ter poročilo o izkušnjah pri pripravi in usposabljanju podatkov. V prispevku so predstavljeni prvi obsežni enojezični jezikovni modeli za norveščino, ki temeljijo na okviru ELMo in BERT. Poleg podrobnosti o procesu usposabljanja predstavljamo kontrastne referenčne rezultate za nabor nalog NLP za Norwegian. Za dodatno ozadje in dostop do podatkov, modelov in programske opreme glejte: http://norlm.nlpl.eu', 'ha': "Tuna halatar da shirin NaLM wanda ke tafiya yana ƙara bayan halittar da ake amfani da misãlai masu girma wa harshen farko wa Narayiya (kuma maincin da wasu harshen Nakuriya), kamar an yi amfani da mazaɓa wa amfani da kwamfyutan ayuka masu ƙaranci, da wani jarrabi mai amfani da wajen gayarwa ga tsari da mafarin data. This paper introduces the first large-scale monolingual language models for Norwegian, based on both the ELMo and BERT frameworks.  Babu daki-daki ga jarayon mafarin aiki, ko kuma muna halatar da fassarar bangon-bangon na fassarar a matsayin NLP na Narayiya. Don wani bango da za'a iya haɗi zuwa masana, misogi, da shiryoyin ayuka, don ka dũba: http://norlm.nlpl.eu", 'he': 'אנו מציגים את ההזדמנות הנוכחית של NorLM כדי לתמוך ביצירת ושימוש של דוגמנים שפות גדולים מאוד מקונטוקטואליזציונים לנורווגית (ובעקרון שפות נורדיות אחרות), כולל סביבת תוכנה מוכנה להשתמש, כמו גם דו"ח ניסיון להכינת ומאימון נתונים. העבודה הזו מציגה את דוגמני השפה המונושפתית הראשונים במסגרת גדולה לנורווגית, מבוססים על המסגרות ELMo וגם BERT. בנוסף לפרט את תהליך האימונים, אנחנו מציגים תוצאות רמז בניגוד לסוויטה של משימות NLP לנורווגית. עבור רקע נוסף וגישה למידע, דוגמנים ותוכנה, בבקשה ראה: http://norlm.nlpl.eu'}
{'en': 'A Baseline Document Planning Method for Automated Journalism', 'fr': 'Une méthode de planification documentaire de base pour le journalisme automatisé', 'es': 'Un método de planificación documental de referencia para el periodismo automatizado', 'pt': 'Um método de planejamento de documentos de linha de base para jornalismo automatizado', 'ar': 'طريقة تخطيط الوثيقة الأساسية للصحافة الآلية', 'ja': '自動ジャーナリズムのためのベースラインドキュメントプランニング方法', 'zh': '一向自动化新闻基线文档规法', 'ru': 'Метод планирования базового документа для автоматизированной журналистики', 'hi': 'स्वचालित पत्रकारिता के लिए एक आधार रेखा दस्तावेज़ योजना विधि', 'ga': 'Modh Pleanála Doiciméad Bonnlíne don Iriseoireacht Uathoibrithe', 'ka': 'Name', 'el': 'Μια μέθοδος σχεδιασμού εγγράφων βάσης για την αυτοματοποιημένη δημοσιογραφία', 'hu': 'Alapvető dokumentumtervezési módszer az automatizált újságíráshoz', 'it': 'Un metodo di pianificazione dei documenti di base per il giornalismo automatizzato', 'lt': 'Automatizuoto žurnalisto pradinis dokumentų planavimo metodas', 'kk': 'Автоматты журналистің негізгі құжат жоспарлау әдісі', 'mk': 'Метод за планирање на документите за автоматизиран новинарство', 'ms': 'Name', 'ml': 'സ്വയമേറ്റിട്ടുള്ള ജാര്\u200dജിലാസിനുള്ള ഒരു ബേസെലൈന്\u200d രേഖയുടെ പദ്ധതി', 'mt': 'Metodu ta’ Pjanar tad-Dokumenti fil-Linja Bażi għal Ġurnalizmu Awtomatiku', 'mn': 'Автоматтын журналистын үндсэн баримт төлөвлөх арга', 'no': 'Name', 'pl': 'Podstawowa metoda planowania dokumentów dla zautomatyzowanego dziennikarstwa', 'ro': 'O metodă de planificare a documentelor de referință pentru jurnalismul automatizat', 'sr': 'Основни метод планирања документа за автоматичен журналист', 'si': 'Name', 'so': 'A Baseline Document Planning Method for Automatic Journalism', 'sv': 'En referensmetod för dokumentplanering för automatiserad journalistik', 'ta': 'தானியங்கி செய்தியியலுக்கான ஒரு அடிப்படையான ஆவணம் திட்டமைப்பு முறைமை', 'ur': 'آٹوماٹیڈ جرنیلیسم کے لئے بنیس لین ڈوکیم پلانینگ طریقہ', 'uz': 'Name', 'vi': 'Phương pháp lập trình tài liệu nền tảng cho báo chí tự động', 'hr': 'Fondalni metod planiranja dokumenta za automatski novinarstvo', 'da': 'En grundlæggende dokumentplanlægningsmetode for automatiseret journalistik', 'de': 'Eine Basismethode zur Dokumentenplanung für automatisierten Journalismus', 'nl': 'Een basismethode voor documentplanning voor geautomatiseerde journalistiek', 'id': 'A Baseline Document Planning Method for Automated Journalism', 'ko': '자동화 뉴스의 기본 문서 기획 방법', 'sw': 'Utawala wa Uandishi wa Habari', 'tr': 'Otomatik žurnalistiň üçin esasy sened Planlamak Metini', 'af': 'Name', 'am': 'ሰነድ', 'bg': 'Базов метод за планиране на документи за автоматизирана журналистика', 'sq': 'Një metodë bazë planifikimi i dokumenteve për gazetarinë e automatizuar', 'bn': 'স্বয়ংক্রিয়ভাবে সাংবাদিকতার জন্য একটি মেসেলিন নথি পরিকল্পনার উপায়', 'bs': 'Fondalni metod planiranja dokumenta za automatski novinarstvo', 'ca': 'A Baseline Document Planning Method for Automated Journalism', 'cs': 'Základní metoda plánování dokumentů pro automatizovanou žurnalistiku', 'hy': 'Comment', 'az': '䅶瑯浡瑩欠킶톃톀킽킰킻楺洠쎼쎧쎼渠욏獡猠叉饴楲⁄쎶毃뱭즙⁐污湬慭愠䵥瑨潤椊', 'et': 'Automatiseeritud ajakirjanduse alusdokumentide planeerimise meetod', 'fi': 'Automatisoitua journalismia koskeva asiakirjasuunnittelumenetelm├Ą', 'fa': 'Name', 'jv': 'A basic document PLning Method for Automated Perialization', 'sk': 'Osnovna metoda načrtovanja dokumentov za avtomatizirano novinarstvo', 'ha': 'KCharselect unicode block name', 'bo': 'རང་བཞིན་གྱིས་སྒེར་གྱི་དྲ་རྒྱ་ལ་ཉེར་སྤྱོད་པའི་ཡིག་ཆ་གླེང་སྒྲུབ་ཀྱི་ཐབས་ལམ།', 'he': 'שיטת תכנון מסמכים בסיסית עבור עיתונאות אוטומטית'}
{'en': 'In this work, we present a method for content selection and document planning for automated news and report generation from structured statistical data such as that offered by the European Union’s statistical agency, EuroStat. The method is driven by the data and is highly topic-independent within the statistical dataset domain. As our approach is not based on machine learning, it is suitable for introducing news automation to the wide variety of domains where no training data is available. As such, it is suitable as a low-cost (in terms of implementation effort) baseline for document structuring prior to introduction of domain-specific knowledge.', 'es': 'En este trabajo, presentamos un método para la selección de contenido y la planificación de documentos para la generación automatizada de noticias e informes a partir de datos estadísticos estructurados, como el que ofrece la agencia de estadística de la Unión Europea, EuroStat. El método se basa en los datos y es muy independiente del tema dentro del dominio del conjunto de datos estadísticos. Como nuestro enfoque no se basa en el aprendizaje automático, es adecuado para introducir la automatización de noticias en una amplia variedad de dominios en los que no hay datos de capacitación disponibles. Como tal, es adecuado como base de bajo costo (en términos de esfuerzo de implementación) para la estructuración de documentos antes de la introducción del conocimiento específico del dominio.', 'ar': 'في هذا العمل ، نقدم طريقة لاختيار المحتوى وتخطيط الوثائق للأخبار الآلية وإنشاء التقارير من البيانات الإحصائية المنظمة مثل تلك التي تقدمها وكالة الإحصاء التابعة للاتحاد الأوروبي ، EuroStat. الطريقة مدفوعة بالبيانات وهي مستقلة إلى حد كبير عن الموضوع داخل مجال مجموعة البيانات الإحصائية. نظرًا لأن نهجنا لا يعتمد على التعلم الآلي ، فهو مناسب لإدخال أتمتة الأخبار إلى مجموعة واسعة من المجالات التي لا تتوفر فيها بيانات تدريبية. على هذا النحو ، فهو مناسب كخط أساس منخفض التكلفة (من حيث جهود التنفيذ) لهيكلة المستندات قبل إدخال المعرفة الخاصة بالمجال.', 'pt': 'Neste trabalho, apresentamos um método de seleção de conteúdo e planejamento de documentos para geração automatizada de notícias e relatórios a partir de dados estatísticos estruturados como o oferecido pela agência estatística da União Européia, EuroStat. O método é orientado pelos dados e é altamente independente de tópicos dentro do domínio do conjunto de dados estatísticos. Como nossa abordagem não é baseada em aprendizado de máquina, ela é adequada para introduzir a automação de notícias em uma ampla variedade de domínios onde não há dados de treinamento disponíveis. Como tal, é adequado como uma linha de base de baixo custo (em termos de esforço de implementação) para a estruturação de documentos antes da introdução do conhecimento específico do domínio.', 'fr': "Dans ce travail, nous présentons une méthode de sélection de contenu et de planification de documents pour la génération automatisée de nouvelles et de rapports à partir de données statistiques structurées telles que celles proposées par l'agence statistique de l'Union européenne, Eurostat. La méthode est pilotée par les données et est hautement indépendante du sujet dans le domaine de l'ensemble de données statistiques. Comme notre approche n'est pas basée sur l'apprentissage automatique, elle est adaptée pour introduire l'automatisation des actualités dans la grande variété de domaines où aucune donnée de formation n'est disponible. En tant que tel, il convient comme base peu coûteuse (en termes d'effort de mise en œuvre) pour la structuration de documents avant l'introduction de connaissances spécifiques au domaine.", 'ja': '本研究では、欧州連合の統計機関EuroStatが提供するような構造化された統計データからの自動ニュースおよびレポート生成のためのコンテンツ選択および文書計画のための方法を提示します。この方法は、データによって推進され、統計データセットドメイン内でトピックに依存しません。当社のアプローチは機械学習に基づくものではないため、トレーニングデータが利用できない幅広い領域にニュース自動化を導入するのに適しています。したがって、ドメイン固有の知識を導入する前の文書構造化のための低コスト（実装努力の点で）ベースラインとして適しています。', 'hi': 'इस काम में, हम स्वचालित समाचार के लिए सामग्री चयन और दस्तावेज़ योजना के लिए एक विधि प्रस्तुत करते हैं और संरचित सांख्यिकीय डेटा से पीढ़ी की रिपोर्ट करते हैं जैसे कि यूरोपीय संघ की सांख्यिकीय एजेंसी, यूरोस्टैट द्वारा पेश किया जाता है। विधि डेटा द्वारा संचालित है और सांख्यिकीय डेटासेट डोमेन के भीतर अत्यधिक विषय-स्वतंत्र है। जैसा कि हमारा दृष्टिकोण मशीन लर्निंग पर आधारित नहीं है, यह विभिन्न प्रकार के डोमेन में समाचार स्वचालन शुरू करने के लिए उपयुक्त है जहां कोई प्रशिक्षण डेटा उपलब्ध नहीं है। इस प्रकार, यह डोमेन-विशिष्ट ज्ञान की शुरुआत से पहले दस्तावेज़ संरचना के लिए कम लागत (कार्यान्वयन प्रयास के संदर्भ में) बेसलाइन के रूप में उपयुक्त है।', 'zh': '凡此诸事,择文档规画,以从结构化统计数据(欧盟统计局之数)自成新闻报。 其法数驱之,而独立于统计数据集域中。 吾道非机器学也,故引入新闻自动化于未练之域。 故宜先特定而文档结构化者低成本(行而言)基线。', 'ru': 'В этой работе мы представляем метод отбора содержания и планирования документов для автоматизированной подготовки новостей и отчетов на основе структурированных статистических данных, как, например, метод, предложенный статистическим агентством Европейского союза "Евростат". Метод основывается на данных и в значительной степени не зависит от тематики в области статистического набора данных. Поскольку наш подход не основан на машинном обучении, он подходит для внедрения автоматизации новостей в широкий спектр областей, где нет данных для обучения. Таким образом, он подходит в качестве недорогостоящего (с точки зрения усилий по внедрению) базового уровня для структурирования документов до внедрения знаний по конкретным областям.', 'ga': 'San obair seo, cuirimid i láthair modh chun ábhar a roghnú agus chun doiciméid a phleanáil do ghiniúint nuachta agus tuairiscí uathoibrithe ó shonraí struchtúracha staidrimh amhail na cinn a thairgeann gníomhaireacht staidrimh an Aontais Eorpaigh, EuroStat. Tá an modh á thiomáint ag na sonraí agus tá sé an-neamhspleách ar ábhair laistigh den réimse tacair sonraí staidrimh. Toisc nach bhfuil ár gcur chuige bunaithe ar mheaisínfhoghlaim, tá sé oiriúnach chun uathoibriú nuachta a thabhairt isteach don raon leathan réimsí nach bhfuil aon sonraí oiliúna ar fáil iontu. Mar sin, tá sé oiriúnach mar bhonnlíne ar chostas íseal (i dtéarmaí na hiarrachta cur chun feidhme) le haghaidh struchtúrú doiciméad sula dtabharfar isteach eolas a bhaineann go sonrach leis an bhfearann.', 'el': 'Στην παρούσα εργασία, παρουσιάζουμε μια μέθοδο επιλογής περιεχομένου και προγραμματισμού εγγράφων για την αυτοματοποιημένη παραγωγή ειδήσεων και εκθέσεων από δομημένα στατιστικά δεδομένα όπως αυτά που προσφέρει ο στατιστικός οργανισμός της Ευρωπαϊκής Ένωσης, EuroStat. Η μέθοδος καθοδηγείται από τα δεδομένα και είναι εξαιρετικά ανεξάρτητη από θέματα εντός του τομέα του στατιστικού συνόλου δεδομένων. Δεδομένου ότι η προσέγγισή μας δεν βασίζεται στη μηχανική μάθηση, είναι κατάλληλη για την εισαγωγή αυτοματοποίησης ειδήσεων σε μια μεγάλη ποικιλία τομέων όπου δεν υπάρχουν διαθέσιμα δεδομένα κατάρτισης. Ως εκ τούτου, είναι κατάλληλη ως βάση χαμηλού κόστους (όσον αφορά την προσπάθεια υλοποίησης) για τη διάρθρωση εγγράφων πριν από την εισαγωγή ειδικών γνώσεων στον τομέα.', 'hu': 'Ebben a munkában bemutatjuk a tartalom kiválasztásának és dokumentumtervezésének módszerét az automatizált hírek és jelentések készítéséhez strukturált statisztikai adatokból, mint például az Európai Unió statisztikai ügynöksége, az EuroStat által kínált adatok. A módszert az adatok vezérlik, és a statisztikai adatkészlet tartományán belül nagymértékben témafüggetlen. Mivel a megközelítésünk nem a gépi tanuláson alapul, alkalmas a hírek automatizálásának bevezetésére olyan területeken, ahol nincs elérhető képzési adat. Ennek megfelelően alacsony költségű (végrehajtási erőfeszítések szempontjából) alapként alkalmas dokumentumok strukturálására a domain-specifikus ismeretek bevezetése előtt.', 'ka': 'ამ სამუშაოში ჩვენ აჩვენებთ სტრუქტურაციული სტატისტიკური მონაცემების შესაძლებლობა და დოკუმენტის პლანზაციის მეთოდი, როგორც იგი, როგორც იყოს EUROPEAN სტატისტიკური მეტი მონაცემების გამოყენება და სტატისტიკური მონაცემების დემომინში ძალიან თავისუფლებელია. როგორც ჩვენი პროგრამა მაქსინური სწავლების დაბაზიან არ არის, ეს საჭიროა ატვირთო ატვირთო მომხმარებისთვის ყველა განსხვავებული დიომენების განსხვავებას, სადა როგორც ასე, ეს საჭიროა, როგორც ცოტა კომპლემენტის სტრუქტურაციისთვის, როგორც ცოტა კომპლემენტის კომპლემენტის კომპლემენტის შესახებ,', 'kk': 'Бұл жұмыс ішінде, Еуропалық Союзның статистикалық агенциясы, Eurostat, автоматты жаңалық және хабарламаларды құру үшін мазмұнын таңдау және құжаттарды планировау әдісін таңдаймыз. Әдісі деректер қолданылады және статистикалық деректер қорлығының доменінде көп нақышты тәуелді. Біздің қасиетіміз машина оқытуға негізделмеген сияқты, бұл жаңалық автоматты түрде бір түрлі доменге ауыстыруға мүмкіндік береді, бұл жерде оқыту деректері жоқ. Бұл үшін доменге ерекше білім беру алдында құжат құрылымының негізгі жолы (жұмыс істеу жұмысының қасиетінде) төменгі бағасы ретінде қолданылады.', 'it': "In questo lavoro, presentiamo un metodo per la selezione dei contenuti e la pianificazione dei documenti per la generazione automatica di notizie e report da dati statistici strutturati come quello offerto dall'agenzia statistica dell'Unione Europea, Eurostat. Il metodo è guidato dai dati ed è altamente indipendente dall'argomento all'interno del dominio del set di dati statistici. Poiché il nostro approccio non si basa sul machine learning, è adatto per introdurre l'automazione delle notizie nell'ampia varietà di domini in cui non sono disponibili dati di formazione. In quanto tale, è adatto come base di riferimento a basso costo (in termini di sforzo di attuazione) per la strutturazione dei documenti prima dell'introduzione di conoscenze specifiche per il settore.", 'ml': 'ഈ പ്രവര്\u200dത്തനത്തില്\u200d, നമ്മുടെ ഉള്ളടക്കം തെരഞ്ഞെടുക്കുന്നതിനും രേഖകളുടെ പരിപാടിയും നമ്മള്\u200d ഒരു രീതിയില്\u200d കൊണ്ടുവരുന്നു. സ്വതന്ത്രികമായ വാര്\u200dത ഡേറ്റായുള്ള രീതിയില്\u200d പ്രവര്\u200dത്തിപ്പിക്കുന്നു. പുരോഗസ്ഥാനത്തിലെ ഡാറ്റാസെറ്റ് ഡൊമെയിനില്\u200d അത്യു മെഷിന്\u200d പഠിപ്പിക്കുന്നതിന്\u200dറെ അടിസ്ഥാനത്ത് നമ്മുടെ പ്രായോഗ്യം നല്\u200dകുന്നത് കൊണ്ടാണ്, പരിശീലന വിവരങ്ങള്\u200d ലഭ്യമായിട്ടി അതുപോലെ, ഡൊമൈന്\u200d - പ്രത്യേക അറിവ് കാണിക്കുന്നതിനു മുമ്പ് രേഖയുടെ അടിസ്ഥാനത്തിനുള്ള (പ്രവര്\u200dത്തിപ്പിക്കുന്ന ശ്രമത്തിന്', 'mk': 'Во оваа работа, претставуваме метод за селекција на содржина и планирање на документи за автоматизирана генерација на вести и извештаи од структурирани статистички податоци како што е она што ја понуди Статистичката агенција на Европската унија, Еурустат. Методот се води од податоците и е високо независен од темата во доменот на статистичките податоци. Бидејќи нашиот пристап не е базиран на машинско учење, тој е соодветен за воведување автоматизација на вестите во широката различност на домени каде што не се достапни податоци за обука. Како таква, истиот е соодветен како база за ниски цени (во поглед на напорите за спроведување) за структурирање на документите пред воведувањето на знаење специфично за доменот.', 'ms': 'Dalam kerja ini, kami memperkenalkan kaedah untuk pemilihan kandungan dan rancangan dokumen untuk pembuatan berita automatik dan laporan dari data statistik struktur seperti yang ditawarkan oleh agensi statistik Uni Eropah, EuroStat. Kaedah dipandu oleh data dan sangat bebas-topik dalam domain set data statistik. Sebab pendekatan kita tidak berdasarkan pembelajaran mesin, ia sesuai untuk memperkenalkan automatasi berita kepada pelbagai domain luas di mana tiada data latihan tersedia. Sebagai demikian, ia sesuai sebagai dasar kosong rendah (dalam terma usaha pelaksanaan) untuk strukturasi dokumen sebelum perkenalan pengetahuan spesifik domain.', 'mt': "F'din il-ħidma, qed nippreżentaw metodu għall-għa żla tal-kontenut u l-ippjanar tad-dokumenti għall-aħbarijiet awtomatizzati u l-ġenerazzjoni tar-rapporti minn dejta statistika strutturata bħal dik offruta mill-aġenzija statistika tal-Unjoni Ewropea, l-EuroStat. Il-metodu huwa mmexxi mid-dejta u huwa indipendenti ħafna mis-suġġetti fi ħdan id-dominju tas-sett tad-dejta statistika. Peress li l-approċċ tagħna mhuwiex ibbażat fuq it-tagħlim tal-magni, huwa adatt għall-introduzzjoni tal-awtomatizzazzjoni tal-aħbarijiet għall-varjetà wiesgħa ta’ oqsma fejn m’hemmx dejta dwar it-taħriġ disponibbli. Bħala tali, huwa xieraq bħala linja bażi ta’ spiża baxxa (f’termini ta’ sforz ta’ implimentazzjoni) għall-istrutturar tad-dokumenti qabel l-introduzzjoni ta’ għarfien speċifiku għad-dominju.", 'no': 'I denne arbeiden presenterer vi ein metode for utval av innhaldet og dokumentplanlegging for automatisk nyhetar og rapportering frå strukturerte statistiske data, slik som tilbygd av Eurostat-statistiske agencien i Den europeiske union. Metoden vert drivt av data og er svært emne-uavhengig i den statistiske datasettdomenet. Etter at tilnærming vårt ikkje er basert på maskinelæring, er det passande for å introdusera nyhetar automatisk til dei store forskjellige domene der ingen treningsdata er tilgjengeleg. Som slik er det passande som ein låg kostnad (i tillegg til implementasjonsverktøy) baselinje for dokumentstrukturering før innføring av domenespesifikke kunnskap.', 'pl': 'W niniejszej pracy przedstawiamy metodę doboru treści i planowania dokumentów dla automatycznego generowania wiadomości i raportów ze strukturyzowanych danych statystycznych, takich jak te oferowane przez agencję statystyczną Unii Europejskiej EuroStat. Metoda ta opiera się na danych i jest wysoce niezależna od tematu w dziedzinie zbiorów danych statystycznych. Ponieważ nasze podejście nie opiera się na uczeniu maszynowym, nadaje się do wprowadzenia automatyzacji wiadomości w szerokiej gamie dziedzin, w których nie są dostępne dane szkoleniowe. W związku z tym nadaje się jako tani (pod względem nakładu wdrożeniowego) podstawa strukturyzacji dokumentów przed wprowadzeniem wiedzy specyficznej dla danej dziedziny.', 'mn': 'Энэ ажлын тулд бид Европын Холбооны статистикийн агентлаг EuroStat-ын автоматжуулагдсан мэдээллийн төлөвлөгөөс автоматжуулагдсан мэдээллийн сонголт болон баримтын төлөвлөгөөний аргыг тайлбарлаж байна. Энэ арга нь өгөгдлийн хөдөлгөөн, статистикийн өгөгдлийн сангийн холбооны дотор маш их сэдвээс хамааралтай. Машин ойлголт машин суралцах сургалтын үндсэн биш учраас, мэдээллийн автоматжуулалт автоматжуулах нь маш олон төрлийн салбарт хэрэгтэй. Үүний тулд энэ нь бага үнэтэй (хэрэгжүүлэх чадвар) баримтын бүтээлүүдийн суурь шугам юм. Домены тодорхой мэдлэг нээхээс өмнө нь баримтын бүтээлүүдийн суурь шугам юм.', 'ro': 'În această lucrare, prezentăm o metodă de selecție a conținutului și planificare a documentelor pentru generarea automată de știri și rapoarte din date statistice structurate precum cea oferită de agenția de statistică a Uniunii Europene, Eurostat. Metoda este condusă de date și este foarte independentă de subiecte în domeniul setului de date statistice. Deoarece abordarea noastră nu se bazează pe machine learning, este potrivită pentru introducerea automatizării știrilor în marea varietate de domenii în care nu sunt disponibile date de formare. Ca atare, este adecvată ca bază de referință la costuri reduse (în ceea ce privește efortul de implementare) pentru structurarea documentelor înainte de introducerea cunoștințelor specifice domeniului.', 'lt': 'Šiame darbe pristatome turinio atrankos metodą ir dokumentų planavimą automatizuotoms naujienoms ir ataskaitų rengimui iš struktūrizuotų statistinių duomenų, kaip antai Europos Sąjungos statistikos agentūros EuroStat, siūlomą metodą. Šis metodas grindžiamas duomenimis ir yra labai nepriklausomas nuo temos statistinių duomenų rinkinio srityje. Kadangi mūsų požiūris nėra grindžiamas mašininio mokymosi principu, jis tinka įvesti naujienų automatizavimą įvairiose srityse, kuriose nėra mokymo duomenų. Todėl jis yra tinkamas kaip mažų sąnaudų (įgyvendinimo pastangų) pagrindas dokumentų struktūrai prieš įdiegiant konkrečiai sričiai skirtas žinias.', 'sr': 'U ovom poslu predstavljamo metodu selekcije sadržaja i planiranja dokumenta za automatske vijesti i izveštaj iz strukturiranih statističkih podataka kao što je ponudila Statistička agencija Evropske unije Eurostat. Metod se vodi podacima i vrlo je nezavisan od teme u domenu statističkog seta podataka. Zato što naš pristup nije baziran na učenju mašine, odgovara je za uvedenje novosti automatski u široku raznovrsnu domenu gde nema podataka o obuci. Kao takav, odgovara je kao osnovna linija niskoštanja (u smislu provedbenih napora) za strukturu dokumenta prije uvođenja znanja specifičnih domena.', 'so': "Markaas waxan shaqada, waxaynu soo bandhignaa qaab u qorsheyno doorashada waxyaabaha ku jira iyo qorsheynta qorsheynta warbixinta iyo wargelinta u qorsheynta macluumaadka la dhisay, tusaale ahaan waxa bixiyay hay'adda takhasuska ee Yurub, Yurub Stat. Midabka waxaa lagu maamulaa macluumaadka, waxaana ku jira macluumaad aad u xor ah oo ku jirta hoyga sawirada. Sida aan horumarintu ku saleyn karin waxbarashada mashiinadaha, waxaa haboon in lagu soo bandhigaa macluumaadka oo ah meelo kala duduwan oo aan laga helin macluumaad waxbarasho. Sida darteed waxaa haboon inaad u sameyso qoraalka qoraalka ka hor inta aan la soo saarin aqoonta gaar ah ee gudaha.", 'si': 'මේ වැඩේ අපි පිළිගන්නවා ස්වයංක්\u200dරියාත්මක වාර්තාවක් තෝරගන්න සහ ලිපින්ත සැලසුම් කරන්න සහ සංවිධානය කරපු සංවිධානය සංව විධානය තොරතුරු වලින් ප්\u200dරයෝජනය කරනවා ඒ වගේම විධානය ස්ථාපනය තොරතුරු සැකසුම් ඩොමේන් වල අපේ පරීක්ෂණය මැෂින් ඉගෙන ගන්නේ නැති විදියට, ඒක ස්වයංක්\u200dරියාවක් ප්\u200dරවේශනය කරන්න පුළුවන් විශාල විදියට ප්\u200dරව ඒ වගේම, ඒක අඩුම විශේෂ දැනගන්න කලින් විශේෂ විශේෂ දැනගන්න කලින් ලොකු සංස්ථාපනය සඳහා අඩුම විශේෂ වි', 'sv': 'I detta arbete presenterar vi en metod för innehållsval och dokumentplanering för automatiserad nyhets- och rapportgenerering utifrån strukturerade statistiska data såsom den som erbjuds av Europeiska unionens statistikbyrå Eurostat. Metoden drivs av data och är mycket ämnesoberoende inom statistikdataområdet. Eftersom vårt tillvägagångssätt inte bygger på maskininlärning är det lämpligt att introducera nyhetsautomation till de många olika områden där inga utbildningsdata finns tillgängliga. Som sådan är det lämpligt som en låg kostnadsbas (när det gäller genomförandeinsatser) för dokumentstrukturering före introduktion av domänspecifik kunskap.', 'ta': 'இந்த வேலையில், நாம் உள்ளடக்க தேர்வு மற்றும் ஆவண திட்டமிட்டத்திற்கு ஒரு முறையைக் கொண்டுள்ளோம் அமைப்பு புள்ளிவிவரங்களை உருவாக்கும் போது,  இந்த முறைமை தகவல் மூலம் இயக்கப்படுகிறது மற்றும் புள்ளிவிவரமான தகவல் அமைப்பு களத்தில் மிகவும் சுதந்திரமாக உள்ளத எங்கள் செயல்பாடு இயந்திரம் கற்றல் அடிப்படையில் இல்லையென்றால், பயிற்சி தகவல் இல்லையென்று செய்தி தானாக்கியமைப்பது பொருத்தம உதாரணமாக, இது ஒரு குறைந்த செலவு (செயல்பாடு முயற்சியின் மூலம்) ஆவண அமைப்புக்கான அடிப்படைக்கோடு பொருத்தமானது domain- குறிப்பிட்ட அறி', 'ur': 'ہم اس کام میں ایک طریقہ پیش کریں گے کہ منصفات انتخاب کرنے اور سند انتخاب کرنے کے لئے اٹوکیٹ نویس کے لئے منصفات کریں اور ساختاری ایسٹیسٹ ڈیٹے سے منصفات کریں جیسے اروپا یونیون کی ایسٹیسٹ ایجنسی، یوروسٹ کے ذری یہ طریقہ ڈیٹا کے ذریعہ چلایا جاتا ہے اور ایسٹیٹسٹ ڈیٹسٹ ڈومین کے اندر بہت زیادہ موضوع-مستقل ہے. جیسے ہمارا تقریبا ماشین تعلیم پر بنیاد نہیں ہے، یہ اخبار اتوماتیک کرنے کے لئے بہت سی مختلف ڈومین کے لئے مناسب ہے جہاں کوئی تعلیم دادہ موجود نہیں ہے. اس طرح، یہ کم قیمت کے طور پر (عملکرد کی کوشش کے طور پر) سند کی ساختاری کے لئے نیز لین کے طور پر مناسب ہے ڈومین کے معلوم علم سے پہلے۔', 'uz': "Bu ishda, biz Avtomatik ravishda yangilash va hujjatni boshqarish usulini hosil qilamiz. Ulaya Union statistical tashkilotlari, yurub Statistika tashkilotlari orqali yaratilgan statistika maʼlumotdan foydalanishimiz mumkin. Name Bizning qismimiz mashinalar o'rganish asosida emas, bu xabarlarni avtomatik o'rganish uchun juda bog'liq bo'lmagan maqsadlar tarkibida o'zgarishga ega. Hullas, domen- specific maʼlumotni koʻrsatishdan oldin hujjatning tuzuvlari uchun qisqa qismi (ishga tushirish jarayonligi bilan ishlatish) asosiy.", 'vi': 'Trong công việc này, chúng tôi có một phương pháp chọn nội dung và lên kế hoạch tài liệu cho các tin tức tự động và phân tích từ các dữ liệu thống kê được cấu trúc như của cơ quan thống kê của Liên bang, Euro. Phương pháp này được mã hóa bởi dữ liệu và rất độc lập về chủ đề trong lĩnh vực bộ dữ liệu thống kê. Do phương pháp của chúng ta không dựa trên việc học máy, nó phù hợp để đưa tin tự động vào những khu vực rộng lớn không có dữ liệu về huấn luyện. Nó phù hợp với mức giá thấp (về nỗ lực ứng dụng) để cấu trúc tài liệu trước khi giới thiệu kiến thức đặc biệt miền.', 'hr': 'U ovom poslu predstavljamo metodu selekcije sadržaja i planiranja dokumenta za automatske vijesti i izvještaj iz strukturiranih statističkih podataka kao što je ponudila EuroStat statistička agencija EuroUnion. Metod se vodi podacima i vrlo je nezavisna od teme u domenu statističkog seta podataka. Jer naš pristup nije temeljen na učenju strojeva, odgovara je za uvođenje novosti automatizacije širokoj raznolikosti domena gdje nema podataka o obuci. Kao takva je odgovarajuća kao osnovna linija niskih troškova (u smislu provedbenih napora) za strukturu dokumenta prije uvođenja znanja specifičnih domena.', 'de': 'In dieser Arbeit stellen wir eine Methode zur Inhaltsauswahl und Dokumentenplanung zur automatisierten Nachrichtengenerierung und Berichterstellung aus strukturierten statistischen Daten vor, wie sie von der statistischen Agentur der Europäischen Union EuroStat angeboten wird. Die Methode wird von den Daten angetrieben und ist innerhalb der statistischen Datensatzdomäne stark themenunabhängig. Da unser Ansatz nicht auf maschinellem Lernen basiert, eignet er sich für die Einführung der Nachrichtenautomatisierung in die unterschiedlichsten Bereiche, in denen keine Trainingsdaten verfügbar sind. Als solches eignet es sich als kostengünstige (im Hinblick auf den Implementierungsaufwand) Basis für die Dokumentenstrukturierung vor der Einführung von domänenspezifischem Wissen.', 'bg': 'В настоящата работа е представен метод за избор на съдържание и документално планиране за автоматизирано генериране на новини и отчети от структурирани статистически данни като тази, предлагана от статистическата агенция на Европейския съюз ЕвроСтат. Методът се ръководи от данните и е силно независим от темата в областта на статистическия набор от данни. Тъй като нашият подход не се основава на машинно обучение, той е подходящ за въвеждане на автоматизация на новините в голямото разнообразие от области, в които няма налични данни за обучение. Като такава, тя е подходяща като нискотарифна (по отношение на усилията за изпълнение) база за структуриране на документи преди въвеждането на специфични за областта знания.', 'nl': 'In dit werk presenteren we een methode voor inhoudselectie en documentplanning voor geautomatiseerde nieuws- en rapportgenerering op basis van gestructureerde statistische gegevens zoals die van het statistische bureau van de Europese Unie, EuroStat. De methode wordt gedreven door de data en is zeer onderwerp-onafhankelijk binnen het statistische dataset domein. Omdat onze aanpak niet gebaseerd is op machine learning, is het geschikt om nieuwsaautomatisering te introduceren in de grote verscheidenheid aan domeinen waar geen trainingsgegevens beschikbaar zijn. Als zodanig is het geschikt als een low-cost (in termen van implementatieinspanning) basislijn voor documentstructurering voorafgaand aan de introductie van domeinspecifieke kennis.', 'da': "I dette arbejde præsenterer vi en metode til indholdsvalg og dokumentplanlægning til automatiseret nyheds- og rapportgenerering ud fra strukturerede statistiske data som den, der tilbydes af EU's statistiske bureau, Eurostat. Metoden drives af dataene og er meget emneouafhængig inden for det statistiske datasæt domæne. Da vores tilgang ikke er baseret på maskinlæring, er den velegnet til at introducere nyhedsautomatisering til de mange forskellige områder, hvor der ikke findes træningsdata. Som sådan er det velegnet som en billig (med hensyn til implementeringsindsats) basislinje for dokumentstrukturering før introduktion af domænespecifik viden.", 'ko': '이 작업에서 우리는 구조화된 통계 데이터(예를 들어 유럽연합 통계국이 제공한 데이터)에서 뉴스와 보고서를 자동으로 생성하는 내용 선택과 문서 기획 방법을 제시했다.이 방법은 데이터로 구동되어 통계 데이터 집합 분야에서 주제에 고도로 독립한다.우리의 방법은 기계 학습을 바탕으로 하는 것이 아니기 때문에 뉴스 자동화를 훈련 데이터가 없는 각종 분야에 도입하기에 적합하다.따라서 특정 분야의 지식을 도입하기 전에 저비용(실시 작업의 경우)의 문서 구조 기선으로 적합하다.', 'sw': 'Katika kazi hii, tunaweka mbinu ya uchaguzi wa maudhui na nyaraka zinazopanga kwa ajili ya habari na taarifa za kutengeneza taarifa za takwimu zilizotengenezwa kama vile ilivyotolewa na shirika la takwimu la Umoja wa Ulaya, EuroStat. Mradi huu unaendeshwa na taarifa na ni mada yenye uhuru sana ndani ya maeneo ya takwimu. Kwa sababu mbinu yetu haina msingi wa kujifunza mashine, ni sahihi kwa ajili ya kuanzisha uhuru wa habari kwa maeneo mbalimbali ambapo hakuna taarifa za mafunzo. Kwa mfano, ni sawa kama gharama ndogo (kwa kiasi cha juhudi za kutekeleza) kwa ajili ya muundo wa nyaraka kabla ya kutangaza maarifa maalum ya ndani.', 'af': "In hierdie werk voorgestel ons 'n metode vir inhoudskiesing en dokumentplanning vir outomatiese nuus en raporteer generasie van struktureerde statistiese data soos wat deur die Europeese Unie se statistiese agencie EuroStat aanbied is. Die metode word deur die data gedryf en is baie onderwerp-onderwerp binne die statistiese datastel domein. Omdat ons toegang nie gebaseer is op masjien leer nie, is dit geskik om nuusautomaties te introduseer aan die wyde verskeie domeine waar geen onderwerp data beskikbaar is nie. Soos so, dit is geskik as 'n lae koste (in terms of implementering effort) basisline vir dokumentstrukturering voor introduksie van domein-spesifieke kennis.", 'fa': 'در این کار، ما یک روش برای انتخاب محتویات و برنامه\u200cریزی سند برای اخبار اتوماتیک و گزارش دادیم از داده\u200cهای آمار ساخته\u200cشده مانند آن که توسط سازمان آمار اروپایی اروپایی اروپایی اروپایی اروپایی پیشنهاد می\u200cدهد. این روش توسط داده ها رانندگی می\u200cشود و در دامنه\u200cهای مجموعه داده\u200cهای آمار بسیار مستقل از موضوع است. در حالی که دستور ما بر روی یادگیری ماشین نیست، برای توضیح دادن اخبار به مختلف دامنه های فراوانی مناسب است که هیچ داده آموزشی در دسترس ندارد. به عنوان این، به عنوان یک خط بنیادی برای ساختاری سند پیش از معرفی دانش خاص دامنه مناسب است.', 'sq': 'Në këtë punë, ne paraqesim një metodë për zgjedhjen e përmbajtjeve dhe planifikimin e dokumenteve për krijimin e lajmeve të automatizuara dhe raporteve nga të dhënat statistikore të strukturuara të tilla si ajo që ofrohet nga agjensia statistike e Bashkimit Evropian, Eurostat. The method is driven by the data and is highly topic-independent within the statistical dataset domain.  Duke qenë se qasja jonë nuk bazohet në mësimin e makinave, është e përshtatshme për futjen e automatizimit të lajmeve në shumicën e gjerë të fushave ku nuk ka të dhëna trajnimi në dispozicion. Si e tillë, ajo është e përshtatshme si një bazë me kosto të ulët (në lidhje me përpjekjen e zbatimit) për strukturimin e dokumenteve përpara futjes së njohurive specifike për fushën.', 'id': 'Dalam pekerjaan ini, kami mempersembahkan metode untuk pemilihan konten dan rencana dokumen untuk pembuatan berita dan laporan otomatis dari data statistik struktur seperti yang ditawarkan oleh agensi statistik Uni Eropa, EuroStat. The method is driven by the data and is highly topic-independent within the statistical dataset domain.  As our approach is not based on machine learning, it is suitable for introducing news automation to the wide variety of domains where no training data is available.  Sebagai seperti itu, itu cocok sebagai dasar biaya rendah (dalam terma usaha implementasi) untuk strukturasi dokumen sebelum perkenalan pengetahuan spesifik domain.', 'am': 'በዚህ ሥራ፣ የአውሮፓውያን የአውሮፓውያን ስታታሪካ አካባቢ፣ ዩሮፓውያን ስታተር አካባቢ፣ የተሰኘውን የዜና እና የዝርዝር ትውልድ አካባቢ እና የሰርቨርስቲካዊ አካባቢ እና አካባቢ እናደርጋለን፡፡ የሥርዓት አካባቢ አካባቢ አካባቢ ነው፡፡ አካሄዳችን በመሣሪያዎች ማምረጃ ላይ እንደሌለ፣ የዜና አውቶማቲን ለመግለጥ የተገባ ነው፡፡ እንደዚህ፣ የዶሜን-specific እውቀት ከመግለጥ በፊት የሰነድ አካባቢ መሠረት ዋጋ ያህል (በአውሮፓዊ ጉዳይ ክስተት) በመስጠት ይችላል፡፡', 'az': 'Bu işdə, Avropa Birliğinin Statistik Agensiyası Eurostat təbliğ etdiyi kimi, qurulmuş statistik məlumatlarından, avtomatik xəbərlər üçün məlumat seçilməsi və məlumat planlaması üçün bir metodu göstəririk. Bu metod məlumatlar tarafından sürüklənir və statistik veri qutusu domeinlərin içində yüksək məlumat bağımsızdır. Bizim yaxınlığımız maşın öyrənməsinə dayanılmadığı üçün, müxtəlif məlumatların faydalanması üçün xəbərləri avtomatik etmək üçün münasibdir. Bütün bunlardan əvvəl, domena müəyyən elmi tanıtmadan əvvəl belə düşük maliyyətlər (implementation effort üzərində) səhifələrin qurulması üçün əsas səhifəsi kimi uyğundur.', 'hy': 'Այս աշխատանքի ընթացքում մենք ներկայացնում ենք պարունակության ընտրության և փաստաթղթերի պլանավորման մեթոդ ավտոմատիկ նորությունների և զեկույցների ստեղծման համար կառուցված վիճակագրական տվյալներից, ինչպիսիք են օրինակ Եվրոպայի վիճակագրական գործակ Մեթոդը հիմնված է տվյալների վրա և շատ անկախ է թեմաներից վիճակագրական տվյալների ոլորտում: As our approach is not based on machine learning, it is suitable for introducing news automation to the wide variety of domains where no training data is available.  Այսպիսով, այն համապատասխանում է որպես ցածր արժեքի (իրականացման ջանքի առումով) հիմք փաստաթղթերի կառուցվածքի համար, նախքան բնագավառի մասնավոր գիտելիքների ներդրումը:', 'bn': "In this work, we present a method for content selection and document planning for automated news and report generation from structured statistical data such as that offered by the European Union's statistical agency, EuroStat.  এই পদ্ধতি তথ্য দ্বারা চালানো হয় এবং পরিসংখ্যান ডাটাসেট ডোমেইনের মধ্যে খুব স্বাধীন বিষয়। যেহেতু আমাদের পদ্ধতি মেশিন শিক্ষার উপর ভিত্তিক নয়, সেখানে সংবাদ স্বয়ংক্রিয়ভাবে সংবাদ স্বয়ংক্রিয়ভাবে প্রযুক্তি প যেমন, ডোমেইন-নির্দিষ্ট জ্ঞান প্রকাশের আগে ডকুমেন্ট কাঠামোর জন্য এটি কম খরচ হিসেবে যথেষ্ট।", 'bs': 'U ovom poslu predstavljamo metodu selekcije sadržaja i planiranja dokumenta za automatske vijesti i izvještaj iz strukturiranih statističkih podataka kao što je ponudila Statistička agencija Evropske unije EuroStat. Metod se vodi podacima i vrlo nezavisna od teme u domenu statističkog seta podataka. Jer naš pristup nije baziran na učenju strojeva, odgovara je za uvođenje novosti automatizacije širom raznolikosti domena gdje nema podataka o obuci. Kao takav, odgovara je kao osnovna linija niskih troškova (u smislu provedbenih napora) za strukturu dokumenta prije uvođenja znanja specifičnih domena.', 'cs': 'V této práci představujeme metodu výběru obsahu a plánování dokumentů pro automatizované generování zpráv a zpráv ze strukturovaných statistických dat, jakou nabízí statistická agentura Evropské unie EuroStat. Metoda je řízena daty a je vysoce tématicky nezávislá v oblasti statistických datových souborů. Vzhledem k tomu, že náš přístup není založen na strojovém učení, je vhodný pro zavedení automatizace zpravodajství do široké škály oblastí, kde nejsou k dispozici žádná data o školení. Jako takový je vhodný jako nízkonákladový (z hlediska implementačního úsilí) základní základ pro strukturování dokumentů před zavedením doménově specifických znalostí.', 'fi': 'Tässä työssä esittelemme menetelmän sisällön valintaan ja dokumenttien suunnitteluun automatisoituun uutisten ja raporttien tuottamiseen strukturoidusta tilastotiedosta, kuten Euroopan unionin tilastovirasto EuroStat. Menetelmä perustuu aineistoon ja on hyvin aihekohtainen tilastoaineiston alueella. Koska lähestymistapamme ei perustu koneoppimiseen, se soveltuu uutisautomaation käyttöönottoon monilla eri aloilla, joilla koulutustietoa ei ole saatavilla. Näin ollen se soveltuu edulliseksi (täytäntöönpanoponnistuksen kannalta) lähtökohdaksi asiakirjojen jäsentämiselle ennen alakohtaisen tietämyksen käyttöönottoa.', 'ca': "En aquesta feina, presentem un mètode de selecció de continguts i planificació de document s per a la generació automàtica de notícies i informes a partir de dades estadístices estructuradas com la oferta per l'agència estadística de la Unió Europea, EuroStat. El mètode està guiat per les dades i està molt independent del tema dins el domini del conjunt de dades estadístices. As our approach is not based on machine learning, it is suitable for introducing news automation to the wide variety of domains where no training data is available.  As such, it is suitable as a low-cost (in terms of implementation effort) baseline for document structuring prior to introduction of domain-specific knowledge.", 'tr': 'Bu işde biz Avrupa Birleşik Uniýasynyň Statistik agenciýasy EuroStat tarapyndan üýtgedilen maksady s a ýlamak we sened planlamasynyň bir yöntemi görkeýäris. Metin berüvler tarapyndan sürüklenilýär we statistik dataset çi domaýynda gaty tema-bağımsızdyr. Biziň ýaryşymyz maşyny öwrenmäge daýanmaýandygy üçin, täzelikler awtomatik taýýarlamak üçin näçe görnüş sahypalaryň içine hiç hili maglumaty ýok. Bu şekilde, domaýy bilen tanyşmadan öň sened düzəldirmesi üçin azalyk kän bahalar ýaly ýeterli.', 'et': 'Käesolevas töös tutvustame sisuvaliku ja dokumentide planeerimise meetodit automatiseeritud uudiste ja aruannete genereerimiseks struktureeritud statistilistest andmetest nagu Euroopa Liidu statistikaamet EuroStat. Meetod põhineb andmetel ja on statistiliste andmete valdkonnas väga teemasõltumatu. Kuna meie lähenemine ei põhine masinõppel, sobib see uudisteautomaatika tutvustamiseks paljudes valdkondades, kus koolitusandmed puuduvad. Seetõttu sobib see madala kuluga (rakendusjõupingutuste mõttes) aluseks dokumentide struktureerimiseks enne valdkonnapõhiste teadmiste kasutuselevõttu.', 'ha': "Daga wannan aikin, Munã gabatar da wata hanyoyi wa zaɓen takardar masu husika da takardar aiki na shirin da aka yi wa zane wa ɗabi'a wa news da kuma da mai bãyar da shirin da aka rubuta daga data na fassarar, kamar abin da aka ba da na Shirika na Statistical Union, EURStat. An tafiyar da shirin bayani na data, kuma yana da jiɓinci mai girma guda cikin tsarin danne-taki. Kama da hanyarmu ba ta zama a kan karatun mafarin kwamfyutan, sai yana da amfani da wajen ka farata farat-farat lãbãri zuwa duk jama'a-jama'a, inda bãbu wani data na amfani da shi. Kamar wannan, ya kamata da amfani da ko wani ma'auni mai ƙara (cikin jarrabar aikin da za'a zartar da shi) wa tsari na dokuman gaba da ya shiga wani ilmi na guda.", 'sk': 'V tem delu predstavljamo metodo izbire vsebin in načrtovanja dokumentov za avtomatizirano ustvarjanje novic in poročil iz strukturiranih statističnih podatkov, kot je statistična agencija Evropske unije EuroStat. Metoda temelji na podatkih in je zelo neodvisna od teme znotraj področja statističnega nabora podatkov. Ker naš pristop ne temelji na strojnem učenju, je primeren za uvajanje avtomatizacije novic v široko paleto področij, kjer ni na voljo podatkov o usposabljanju. Kot tak je primeren kot nizko stroškovno (z vidika prizadevanj za izvajanje) osnovo za strukturiranje dokumentov pred uvedbo znanja za posamezno področje.', 'jv': 'Nang barêng-barêng iki, kéné gunakake sistem kanggo ngubah gambar perusahaan karo nggawe dokumen sing beraksi kanggo nggawe barang sistem wartané perusahaan gambar dadi istatistik sing koyo nggawe ngubah akeh nggawe barang nggawe ngubah winih The method is drived by the data and is extreme tema-free in the istatical dataset domain. Tereje awak dhéwé ora bisa mbatalé karo sistem sing sampeyan, iso ngempek nggawe barang sistem sing isine perkaran winih akeh sabên neng sampek dadi sing ora bisa nêmên. Where am I', 'bo': 'འོན་ཀྱང་། ང་ཚོས་ནང་དུ་ཡིག The method is driven by the data and is highly topic-independent in the statistical data set domain. ང་ཚོའི་གཟུགས As such, it is suitable as a low-cost (in terms of implementation effort) baseline for document structuring prior to introduction of domain-specific knowledge.', 'he': 'בעבודה הזו, אנחנו מציגים שיטה לבחירת תוכן ותכנית מסמכים עבור חדשות אוטומטיות ויוצר דווחים מידע סטטיסטי מבוסס כזה שהצעה על ידי סוכנות הסטטיסטיקה של האיחוד האירופאי, אירוסטט. השיטה מונעת על ידי המידע והיא עצמאית מאוד על הנושא בתוך תחום המידע הסטטיסטי. כיוון שהגישה שלנו לא מבוססת על לימוד מכונות, היא מתאימה להציג אוטומטיקה חדשות למגוון רחב של תחומות שבו אין נתונים אימונים זמינים. As such, it is suitable as a low-cost (in terms of implementation effort) baseline for document structuring prior to introduction of domain-specific knowledge.'}
{'en': 'Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification', 'fr': 'Multilingue et Zero-Shot se rapproche de la classification monolingue des registres Web', 'pt': 'Multilíngue e Zero-Shot estão se aproximando da classificação de registro monolíngue da Web', 'ar': 'متعدد اللغات و Zero-Shot يقترب من تصنيف سجل الويب أحادي اللغة', 'es': 'Multilingüe y Zero-Shot se acerca a la clasificación de registros web monolingües', 'ru': 'Многоязычный и нулевой снимки приближаются к одноязычной классификации веб-регистров', 'hi': 'बहुभाषी और शून्य-शॉट मोनोलिंगुअल वेब रजिस्टर वर्गीकरण पर बंद हो रहा है', 'zh': '多言与零镜头方近单语网络寄存器类', 'ja': '多言語およびゼロショットは、単語ウェブレジスタ分類でクローズインしています', 'ga': 'Tá Ilteangach agus Urchar Nialasach ag druidim le hAicmiú Clár Gréasáin Aonteangach', 'hu': 'Többnyelvű és Zero-Shot bezáródik az egynyelvű webes regiszterek osztályozására', 'ka': 'მრავალენგური და ნულ- სურათი კლასიფიკაციაში დახურება', 'it': 'Multilingue e Zero-Shot si stanno avvicinando alla classificazione del registro web monolingue', 'kk': 'Көп тілді және нөл- жол бір тілді Веб регистр классификациясында жабылады', 'lt': 'Daugiakalbis ir nulinis tirpalas užbaigiamas Monolingual Web Registry klasifikacijoje', 'mk': 'Мултијазичен и нула- пукање се затвора на класификацијата на монолингвален веб- регистар', 'ml': 'മണോളില്\u200d വെബ് റിജിസ്റ്റര്\u200d ക്ലാസിഷനില്\u200d അടച്ചുകൊണ്ടിരിക്കുന്നു', 'ms': 'Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification', 'el': 'Η πολύγλωσση και μηδενική βολή πλησιάζει στο μονογλωσσικό μητρώο ιστού Ταξινόμηση', 'mt': 'Il-Klassifikazzjoni tar-Reġistru Monolingwali tal-Internet Multilingual u Zero-Shot qed tingħalaq', 'no': 'Fleirspråk og null-skytt lukkar i ved å klassifisera mellomspråk nettregistrering', 'ro': 'Multilingv și Zero-Shot se apropie de clasificarea registrului web monolingv', 'sr': 'Većina jezika i nula pucnjava se zatvara u klasifikaciji jednojezičkog web registra', 'si': 'Multilanguage and Zero-Shot are close in Monolingual Web Recorder Classified', 'so': 'Luuqado badan iyo suurtogal-shoo waxay ku xidhan yihiin fasaxa diiwaangelinta internetka ee Monolingual', 'sv': 'Flerspråkiga och Zero-Shot närmar sig en enspråkig webbregisterklassificering', 'mn': 'Олон хэл болон Zero-Shot нь Монолингийн Веб Регистр Классификацийн хувьд', 'ta': 'பல மொழி மற்றும் பூஜ்ஜி- ஷாட் மோனோலிங்கல் இணைய பதிவு வகைப்படுத்தலில் மூடுகிறது', 'pl': 'Wielojęzyczny i Zero-Shot zbliża się do jednojęzycznego rejestru internetowego', 'ur': 'Multilingual and Zero-Shot are closing in Monolingual Web Register Classification', 'uz': 'Name', 'vi': 'Nhiều ngôn ngữ và Zero-Shot đang đóng cửa vào đơn vị Đánh dấu mạng.', 'hr': 'Većina jezika i nula pucnjava se zatvara na jednojezičkom registraciji web registracije', 'bg': 'Многоезичният и нулевият изстрел наближава класификацията на едноезичния уеб регистър', 'nl': 'Meertalig en Zero-Shot nadert op Monolingual Web Register Classification', 'ko': '다중 언어와 제로 렌즈가 단어 네트워크 어역 분류에 접근하고 있습니다', 'da': 'Flersproget og Zero-Shot er ved at lukke ind på ensproget webregisterklassificering', 'id': 'Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification', 'sw': 'Mpigo wa lugha nyingi na risasi zisizo na maandishi yanafungwa kwenye tovuti ya uandikishaji wa Kimonolinguli', 'tr': 'Çoklu diller we Zero-Shot Monoli Dilli Web Register Seçgisinde ýapylýar', 'fa': 'تعداد زبان\u200cهای زیادی و صفر-شلیک در کلاس ثبت\u200cنامه\u200cی وب یک زبان بسته می\u200cشود', 'af': 'Multilingual en Zero-Shot sluit op Monolingueel Web Register Klassifikasie', 'sq': 'Multilingual and Zero-Shot is Closing in on Monolingual Web Register Classification', 'hy': 'Բազլեզու և զրո-կրակը փակվում է MonoLingue վեբ ռեժիստրի դասակարգում', 'az': 'Çoxlu dil və Sıfır-Shot Monoling Web Register Klasifikasyonunda Qapılır', 'de': 'Multilingual und Zero-Shot nähern sich der Monolingual Web Register Klassifizierung', 'ca': "Multilingüe i zero-Shot s'encerra a la classificació del registre web monolingüe", 'cs': 'Vícejazyčný a Zero-Shot se blíží na jednojjazyčném webovém rejstříku Klasifikace', 'et': 'Mitmekeelne ja Zero-Shot läheneb ühekeelsele veebiregistri klassifikatsioonile', 'am': 'Multilingual and Zero-Shot are closing in Monolingual Web Register Classification', 'bs': 'Većina jezika i nula pucnjava se zatvara u klasifikaciji jednojezičkog web registra', 'bn': 'মোনোলোলিভাল ওয়েব রেজিস্টার ক্লাসিকেশনে অনেক ভাষা এবং শুট বন্ধ হচ্ছে', 'fi': 'Monikielinen ja Zero-Shot lähestyvät monikielistä verkkorekisteriä', 'jv': 'Multilenguang lan nulo-shot iku diputalo ning Monolngual web regiter', 'sk': 'Večjezični in Zero-Shot se približujeta enojezični spletni register Klasifikacija', 'he': 'רישום רב-שפתי ואפס סוגרים את שיעור רשום האינטרנט Monolingual', 'ha': 'KCharselect unicode block name', 'bo': 'སྐད་རིགས་དབྱེ་སྤྱོད་དང་Zero-Shot Monolingual Web Register སྒྲིག་འཛིན་གྱི་ནང་དུ་སྒོ་རྒྱག་ཡོད་པ'}
{'en': 'This article studies register classification of documents from the unrestricted web, such as news articles or opinion blogs, in a multilingual setting, exploring both the benefit of training on multiple languages and the capabilities for zero-shot cross-lingual transfer. While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved. In this study, we show that training on multiple languages 1) benefits languages with limited amounts of register-annotated data, 2) on average achieves performance on par with monolingual models, and 3) greatly improves upon previous zero-shot results in Finnish, French and Swedish. The best results are achieved with the multilingual XLM-R model. As data, we use the CORE corpus series featuring register annotated data from the unrestricted web.', 'ar': 'تسجل دراسات هذه المقالة تصنيف المستندات من الويب غير المقيد ، مثل المقالات الإخبارية أو مدونات الرأي ، في بيئة متعددة اللغات ، واستكشاف كل من فوائد التدريب على لغات متعددة وإمكانيات النقل عبر اللغات بدون طلقة. في حين أن النطاق الواسع للتنوع اللغوي الموجود على الويب يشكل تحديات لتصنيف التسجيل ، فقد أظهرت الدراسات الحديثة أنه يمكن تحقيق مستويات جيدة من النقل عبر اللغات من مجموعة CORE الإنجليزية الشاملة إلى لغات أخرى. في هذه الدراسة ، نوضح أن التدريب على لغات متعددة 1) يفيد اللغات بكميات محدودة من البيانات المشروحة بالتسجيل ، 2) يحقق متوسط الأداء على قدم المساواة مع النماذج أحادية اللغة ، و 3) يتحسن بشكل كبير على النتائج الصفرية السابقة باللغة الفنلندية ، الفرنسية والسويدية. يتم تحقيق أفضل النتائج مع نموذج XLM-R متعدد اللغات. كبيانات ، نستخدم سلسلة CORE corpus التي تعرض تسجيل البيانات المشروحة من الويب غير المقيد.', 'pt': 'Este artigo estuda a classificação de registros de documentos da web irrestrita, como artigos de notícias ou blogs de opinião, em um ambiente multilíngue, explorando tanto o benefício do treinamento em vários idiomas quanto os recursos para transferência de vários idiomas. Embora a ampla variedade de variações linguísticas encontradas na web represente desafios para a classificação de registros, estudos recentes mostraram que bons níveis de transferência entre idiomas do extenso corpus CORE inglês para outros idiomas podem ser alcançados. Neste estudo, mostramos que o treinamento em vários idiomas 1) beneficia idiomas com quantidades limitadas de dados anotados por registro, 2) em média alcança desempenho equivalente a modelos monolíngues e 3) melhora muito os resultados anteriores de tiro zero em finlandês, francês e sueco. Os melhores resultados são alcançados com o modelo multilíngue XLM-R. Como dados, utilizamos a série CORE corpus com dados cadastrais anotados da web irrestrita.', 'fr': "Cet article étudie la classification des registres de documents provenant du Web non restreint, tels que des articles de presse ou des blogs d'opinion, dans un environnement multilingue, en explorant à la fois les avantages de la formation dans plusieurs langues et les possibilités de transfert multilingue zero-shot. Alors que le large éventail de variations linguistiques trouvées sur le Web pose des défis pour la classification des registres, des études récentes ont montré qu'il est possible d'atteindre de bons niveaux de transfert interlinguistique du vaste corpus anglais CORE vers d'autres langues. Dans cette étude, nous montrons que la formation sur plusieurs langues 1) profite aux langues avec des quantités limitées de données annotées dans les registres, 2) atteint en moyenne des performances comparables à celles des modèles monolingues et 3) améliore considérablement les résultats zero-shot précédents en finnois, en français et en suédois. Les meilleurs résultats sont obtenus avec le modèle XLM-R multilingue. En tant que données, nous utilisons la série de corpus CORE contenant des données annotées de registre provenant du Web non restreint.", 'es': 'Este artículo estudia la clasificación de registros de documentos de la web sin restricciones, como artículos de noticias o blogs de opinión, en un entorno multilingüe, explorando tanto el beneficio de la capacitación en múltiples idiomas como las capacidades de transferencia interlingüística sin posibilidad de transferencia interlingüística. Si bien la amplia gama de variaciones lingüísticas que se encuentran en la web plantea desafíos para la clasificación de registros, estudios recientes han demostrado que se pueden lograr buenos niveles de transferencia multilingüe del extenso corpus CORE en inglés a otros idiomas. En este estudio, mostramos que la capacitación en varios idiomas 1) beneficia a los idiomas con cantidades limitadas de datos anotados por el registro, 2) en promedio logra un rendimiento a la par de los modelos monolingües y 3) mejora en gran medida los resultados previos de tiro cero en finés, francés y sueco. Los mejores resultados se obtienen con el modelo XLM-R multilingüe. Como datos, utilizamos la serie de corpus CORE que incluye datos anotados de registro de la web sin restricciones.', 'zh': '本文究于多言之境网络(如新闻文论博客)注册分类于文档,讨多种语言培训之益,及零次跨语移之功。 虽于网络上见广言语异于注册分类,而近者研明,从广英语CORE语料库到他语之跨言移水平可致也。 于此论之,多种语言之教1)使注册注有限之言受益,2)均至与单语相似,3)大改前芬兰语,法语与瑞典语之零拍摄也。 用多言 XLM-R 模可获得最佳效。 为数者,吾用CORE语料库系列,其包自不拘者网络注注数也。', 'ja': 'この記事では、多言語環境でのニュース記事やオピニオンブログなどの制限のないウェブからの文書の分類を研究し、複数言語でのトレーニングの利点とゼロショットのクロスリンガル転送機能の両方を探求します。ウェブ上で発見された幅広い言語変動は、レジスタ分類に課題をもたらすが、最近の研究では、広範な英語のコアコーパスから他の言語への良好なレベルのクロスリンガル転送を達成できることが示されている。この研究では、複数の言語に関するトレーニングが、1 ）登録されたデータの量が限られている言語に利益をもたらすこと、2 ）平均的に単一言語モデルと同等のパフォーマンスを達成すること、3 ）以前のフィンランド語、フランス語、スウェーデン語のゼロショット結果よりも大幅に改善することを示しています。最良の結果は、多言語XLM - Rモデルで達成されます。データとしては、制限のないウェブからの注釈付きデータを登録することを特徴とするコアコーパスシリーズを使用しています。', 'ru': 'В этой статье изучается классификация документов из неограниченного Интернета, таких как новостные статьи или блоги мнений, в многоязычной среде, исследуя как преимущества обучения на нескольких языках, так и возможности для межязычной передачи с нулевым выстрелом. Хотя в связи с широким диапазоном лингвистических различий, наблюдаемых в Интернете, возникают проблемы с классификацией регистров, недавние исследования показали, что можно добиться хорошего уровня межязыкового перехода от обширного ОСНОВНОГО НАБОРА английских языков к другим языкам. В этом исследовании мы показываем, что обучение на нескольких языках 1) приносит пользу языкам с ограниченным объемом регистровых аннотированных данных, 2) в среднем достигает производительности наравне с одноязычными моделями и 3) значительно улучшается по сравнению с предыдущими результатами нулевого снимка на финском, французском и шведском языках. Наилучшие результаты достигаются с помощью многоязычной модели XLM-R. В качестве данных мы используем БАЗОВУЮ серию корпусов, содержащую регистровые аннотированные данные из неограниченной сети.', 'hi': 'यह लेख अप्रतिबंधित वेब से दस्तावेजों के वर्गीकरण को पंजीकृत करता है, जैसे कि समाचार लेख या राय ब्लॉग, एक बहुभाषी सेटिंग में, कई भाषाओं पर प्रशिक्षण के लाभ और शून्य-शॉट क्रॉस-लिंगुअल ट्रांसफर की क्षमताओं दोनों की खोज करते हैं। जबकि वेब पर पाए जाने वाले भाषाई भिन्नता की विस्तृत श्रृंखला रजिस्टर वर्गीकरण के लिए चुनौतियां पैदा करती है, हाल के अध्ययनों से पता चला है कि व्यापक अंग्रेजी कोर कॉर्पस से अन्य भाषाओं में क्रॉस-लिंगुअल हस्तांतरण के अच्छे स्तर प्राप्त किए जा सकते हैं। इस अध्ययन में, हम दिखाते हैं कि कई भाषाओं पर प्रशिक्षण 1) सीमित मात्रा में रजिस्टर-एनोटेटेड डेटा के साथ भाषाओं को लाभ पहुंचाता है, 2) औसतन मोनोलिंगुअल मॉडल के बराबर प्रदर्शन प्राप्त करता है, और 3) फिनिश, फ्रेंच और स्वीडिश में पिछले शून्य-शॉट परिणामों पर बहुत सुधार करता है। बहुभाषी XLM-R मॉडल के साथ सर्वोत्तम परिणाम प्राप्त किए जाते हैं। डेटा के रूप में, हम अप्रतिबंधित वेब से रजिस्टर एनोटेट डेटा की विशेषता वाली कोर कॉर्पस श्रृंखला का उपयोग करते हैं।', 'ga': 'Déanann an t-alt seo staidéar ar aicmiú doiciméad ón ngréasán neamhshrianta, ar nós ailt nuachta nó blaganna tuairime, i suíomh ilteangach, ag fiosrú an tairbhe a bhaineann le hoiliúint ar iltheangacha agus na hacmhainní le haghaidh aistrithe trasteangacha gan urchar. Cé go gcruthaíonn an raon leathan éagsúlachta teanga atá le fáil ar an ngréasán dúshláin maidir le haicmiú cláir, léirigh staidéir a rinneadh le déanaí gur féidir leibhéil mhaithe aistrithe tras-teanga a bhaint amach ó chorpas fairsing Béarla CORE go teangacha eile. Sa staidéar seo, léirímid go dtéann oiliúint ar iltheangacha 1) chun sochair do theangacha le méideanna teoranta sonraí cláraithe, 2) go mbaintear amach feidhmíocht ar chomhchéim le samhlacha aonteangacha, agus 3) go bhfeabhsaítear go mór é ar thorthaí nialasacha san Fhionlainnis roimhe seo, Fraincis agus Sualainnis. Baintear na torthaí is fearr amach leis an tsamhail ilteangach XLM-R. Mar shonraí, úsáidimid an tsraith CORE corpus ina bhfuil sonraí anótáilte cláir ón ngréasán neamhshrianta.', 'hu': 'Ez a cikk a korlátozás nélküli internetről származó dokumentumok, például hírek vagy vélemény blogok osztályozását tanulmányozza többnyelvű környezetben, feltárva mind a többnyelvű képzés előnyeit, mind pedig a nulla-shot keresztnyelvű transzfer lehetőségeit. Míg az interneten található nyelvi variációk széles skálája kihívásokat jelent a nyilvántartások besorolása szempontjából, a közelmúltbeli tanulmányok azt mutatták, hogy a kiterjedt angol CORE korpusztól más nyelvekre való átvitel jó szintje érhető el. Ebben a tanulmányban azt mutatjuk, hogy a több nyelven folytatott képzés 1) korlátozott mennyiségű regisztrációs adattal rendelkező nyelveket használ, 2) átlagosan az egynyelvű modellekkel egyenlő teljesítményt ér el, és 3) jelentősen javul a korábbi nulla-shot eredményekhez képest finn, francia és svéd nyelven. A legjobb eredményeket a többnyelvű XLM-R modell érheti el. Adatként a korlátozás nélküli internetről származó jegyzetelt adatokat tartalmazó CORE corpus sorozatot használjuk.', 'el': 'Αυτό το άρθρο μελετά την ταξινόμηση εγγράφων από τον απεριόριστο ιστό, όπως άρθρα ειδήσεων ή ιστολόγια γνώμης, σε ένα πολύγλωσσο περιβάλλον, διερευνώντας τόσο το όφελος της κατάρτισης σε πολλές γλώσσες όσο και τις δυνατότητες για μηδενική διασυνοριακή μεταφορά. Ενώ το ευρύ φάσμα γλωσσικών παραλλαγών που υπάρχουν στο διαδίκτυο θέτει προκλήσεις για την ταξινόμηση μητρώων, πρόσφατες μελέτες έχουν δείξει ότι μπορούν να επιτευχθούν καλά επίπεδα διασυνοριακής μεταφοράς από το εκτεταμένο αγγλικό σώμα CORE σε άλλες γλώσσες. Στην παρούσα μελέτη, καταδεικνύουμε ότι η εκπαίδευση σε πολλαπλές γλώσσες 1) ωφελεί τις γλώσσες με περιορισμένες ποσότητες σχολιασμένων δεδομένων, 2) κατά μέσο όρο επιτυγχάνει απόδοση ίση με τα μονογλωσσικά μοντέλα, και 3) βελτιώνει σημαντικά σε σχέση με τα προηγούμενα αποτελέσματα μηδενικής λήψης στα φινλανδικά, γαλλικά και σουηδικά. Τα καλύτερα αποτελέσματα επιτυγχάνονται με το πολύγλωσσο μοντέλο. Ως δεδομένα, χρησιμοποιούμε τη σειρά Corpus CORE που περιλαμβάνει δεδομένα με σχόλια μητρώου από τον απεριόριστο ιστό.', 'ka': 'ამ წესტის შესწავლობა უფრო მრავალური წესების კლასიფიკაცია, როგორც ახალგაზრულები ან მინდომის ბლოგები, მრავალური წესების კონფიკაციაში, მრავალური წესების გამოსახულების გამოსახულების და უფ თუმცა ინგლისტიკური განსხვავებები, რომელიც ინგლისტიკური კორიფიკაციის განსხვავებულია, განსხვავებული კორიფიკაციის განსხვავებებისთვის განსხვავებულია, შემდეგ განსხვავებული კორიფიკაციის შესაძლებელია, რომ ამ კვლევაში ჩვენ ჩვენ აჩვენებთ, რომ მრავალ ენაზე განაკეთება 1) გამოიყენება ენაზე, რომლებიც რეგისტრის ანოტირებული მონაცემების ზომა, 2) სინამდვილეში მონაცემებით მონაცემებით მონაცემებით მონაცემებით, და 3) წ ყველაზე საუკეთესო შედეგი მოდელი იქნება მრავალენგური XLM-R მოდელზე. როგორც მონაცემები, ჩვენ გამოყენებთ CORE corpus სერია, რომელსაც რეგისტრისტის მონაცემები არსებული ინტერფეტიდან გამოყენება.', 'it': "Questo articolo studia la classificazione dei documenti provenienti dal web senza restrizioni, come articoli di notizie o blog di opinione, in un ambiente multilingue, esplorando sia i vantaggi della formazione su più lingue che le capacità di trasferimento cross-lingual zero shot. Mentre l'ampia gamma di variazioni linguistiche rilevate sul web pone sfide per la classificazione dei registri, studi recenti hanno dimostrato che è possibile raggiungere buoni livelli di trasferimento translinguale dall'ampio corpus inglese CORE ad altre lingue. In questo studio, mostriamo che la formazione su più lingue 1) favorisce le lingue con quantità limitate di dati annotati nel registro, 2) raggiunge in media prestazioni alla pari dei modelli monolingue e 3) migliora notevolmente rispetto ai precedenti risultati zero-shot in finlandese, francese e svedese. I migliori risultati si ottengono con il modello XLM-R multilingue. Come dati, utilizziamo la serie CORE corpus con dati annotati dal web senza restrizioni.", 'lt': 'Šiame straipsnyje tiriamas neribotų interneto dokumentų, pavyzdžiui, naujienų straipsnių ar nuomonės blogų, klasifikavimas daugiakalbėje aplinkoje, išnagrinėjant mokymo įvairiomis kalbomis naudą ir galimybes neperdirbti tarpkalbinį perdavimą. Nors internete nustatytas didelis kalbų skirtumas kelia iššūkių registrų klasifikavimui, neseniai atlikti tyrimai parodė, kad galima pasiekti gerą tarpkalbinio perkėlimo lygį iš plataus anglų CORE corpus į kitas kalbas. Šiame tyrime parodome, kad mokymas įvairiomis kalbomis 1) naudingas kalboms, kuriose registruose yra ribotas duomenų kiekis, 2) vidutiniškai pasiekia rezultatus lygiaverčius vienkalbiniams modeliams ir 3) gerokai pagerėja ankstesnių nulinių rezultatų suomių, prancūzų ir švedų kalbomis. Geriausi rezultatai pasiekti naudojant daugiakalbį XLM-R model į. Kaip duomenys naudojame CORE corpus seriją, kurioje registruojami neribotos interneto anotacijos duomenys.', 'kk': 'Бұл мақала, жаңалық мақалалар немесе ойлау блогтар секілді, көптеген тілдерде, бірнеше тілдердің оқыту мүмкіндіктерін және нөл тілдердің көптеген аудару мүмкіндіктерін зерттеу үшін құжаттарды сақтау жүйесін зерт Интернетте табылған лингвистикалық айырмашылығының көпшілігі категориялау үшін өзгерістерді көрсетеді, соңғы зерттеулер тілдерді көпшілікті CORE корпусынан басқа тілдерге жеткізуге болады. Бұл зерттеулерде бірнеше тілдерді оқыту (1) тілдерді көмектесу үшін көмектесетін деректердің шектелген мөлшерлері бар, 2) орташа бірнеше тілдердің монолингі үлгілері бар, және 3) алдыңғы нөл шарттарының Финляндия, Француз Ең жақсы нәтижелер көп тілді XLM- R үлгісімен жетілді. Деректер үшін CORE корпус сериясын қолданамыз. Керек емес веб- тегінен жазылған деректерді көрсетеді.', 'mk': 'Оваа статија ја проучува класификацијата на документите од неограничениот веб, како што се новинските статии или блоговите за мислење, во мултијазични услови, истражувајќи ја и користта од обуката на повеќе јазици како и способностите за нула-снимка крстојазичен трансфер. И покрај тоа што широкиот опсег на јазични варијации пронајдени на веб-страницата претставува предизвици за класификација на регистарите, неодамнешните студии покажаа дека може да се постигнат добри нивоа на прекујазичен трансфер од екстремниот англиски корпус CORE на други јази Во оваа студија покажуваме дека обуката на повеќекратни јазици 1) има корист од јазиците со ограничени количини на регистарски анотирани податоци, 2) во просек постигнува резултати во споредба со монојазичните модели и 3) значително се подобрува со претходните резултати со нула снимка на фински, француски Најдобрите резултати се постигнати со мултијазичкиот модел XLM-R. Како податоци, ја користиме серијата CORE corpus со регистрирање на анотирани податоци од неограничен интернет.', 'ms': 'Artikel ini mempelajari klasifikasi dokumen dari web yang tidak terhalang, seperti artikel berita atau blog pendapat, dalam tetapan berbilang bahasa, mengeksplorasi kedua-dua keuntungan latihan dalam bahasa berbilang dan kemampuan untuk pemindahan saling bahasa tanpa tembakan sifar. Sementara julat luas variasi bahasa yang ditemui di web mengakibatkan cabaran untuk klasifikasi daftar, kajian baru-baru ini menunjukkan bahawa tahap yang baik pemindahan saling bahasa dari CORE corpus Inggeris yang luas ke bahasa lain boleh dicapai. Dalam kajian ini, kami menunjukkan bahawa latihan dalam bahasa berbilang 1) bahasa keuntungan dengan jumlah terbatas data yang dicatat-daftar, 2) rata-rata mencapai prestasi sama dengan model monobahasa, dan 3) meningkat jauh pada keputusan 0-shot terdahulu dalam bahasa Finlandia, Perancis dan Swedia. Hasil terbaik dicapai dengan model XLM-R berbilang bahasa. Sebagai data, kami menggunakan siri CORE corpus yang mengandungi daftar data yang dicatat dari web yang tidak terhalang.', 'ml': 'വെബില്\u200d നിന്നും അസ്ഥിരപ്പെടാത്ത രേഖകളുടെ വിവരങ്ങളുടെ ക്ലാസ്ഫികേഷന്\u200d രേജിസ്റ്റ് ചെയ്യുന്നു. വാര്\u200dത്തകള്\u200d അല്ലെങ്കില്\u200d തിരിച്ചറിയുന്ന വ്യാഖ്യാപങ് വെബ് പോസിന്റെ വ്യത്യാസങ്ങളില്\u200d കണ്ടെത്തിയിരിക്കുന്ന വ്യത്യാസങ്ങള്\u200dക്ക് വേണ്ടി രേജിസ്റ്റര്\u200d ക്ലാസ്ഫിക്ഷനുള്ള വിലാസങ്ങള്\u200dക്ക് വേണ്ടി വെച്ച്  ഈ പഠനത്തില്\u200d നമ്മള്\u200d കാണിക്കുന്നു, പല ഭാഷകളില്\u200d പരിശീലിക്കുന്നത് നാം കാണിച്ചിരിക്കുന്നു. നിര്\u200dണ്ണയിക്കപ്പെട്ട വിവരങ്ങളുടെ കൂട്ടത്തില്\u200d നിര്\u200dണ്ണയിക്കപ്പെട്ട വ ഏറ്റവും മികച്ച ഫലങ്ങള്\u200d ഏറ്റവും മികച്ച ഭാഷ എക്സ്\u200cഎല്\u200dഎംആര്\u200d മോഡലില്\u200d എത്തിയിരിക്കുന്നു. വിവരങ്ങളായി നമ്മള്\u200d കോര്\u200dപ്പുസ് സീരിസില്\u200d ഉപയോഗിക്കുന്നു. റെജിസ്റ്റര്\u200d വെബില്\u200d നിന്നും വിവരിച്ചുകൊടുക്കുന', 'mn': 'Энэ баримтын судалгаагаар хэлний хэл дээр дасгал хөдөлгөөн болон тэгш хэлний шилжүүлэх боломжуудыг олон хэлний хэл дээр суралцах боломжтой боломжуудыг судалж байна. Веб дээр олон олон хэлний өөрчлөлт бичиж буй хэлний хэлбэрээс өөрчлөлт бичиж буй хэлбэрээс илүү сайн хэлний шилжүүлэлтийг харуулж байна. Энэ судалгаанд бид олон хэл дээр суралцах нь 1) хэл дээр хязгаарлагдсан өгөгдлийн хэмжээтэй ашигтай, 2) дунджаар нэг хэл загвартай үйл ажиллагааг гаргадаг, 3) Финляндын, Француз, Шведийн өмнөх 0 шат үр дүнд их сайжруулдаг. Хамгийн сайн үр дүн нь олон хэл XLM-R загвартай гарч ирсэн. Өгөгдлийн хувьд бид CORE корпус цувралыг ашиглаж байна. Хязгааргүй веб-ээс зарцуулсан мэдээллийг харуулж байна.', 'no': 'Denne artikkelen studierer registrering av klassifikasjon av dokument frå ikkje-strekte nettet, slik som nye artikler eller synleg bloggar, i ein fleirspråk innstilling, og utforskar både nyttigheten på trening på fleire språk og kapasiteten for null-shot krysspråk overføring. Mens det brede området av lingviske variasjonar funne på nettet poserer utfordringar for registreringsklassifikasjon, har nyleg studiar vist at gode nivåar av krysspråk-overføring frå den ekstra engelske korpusen CORE til andre språk kan oppnå. I denne studien viser vi at opplæring på fleire språk 1) nyttar språk med begrensede mengdar av registrerte data, 2) gjennomsnittlig gjennomsnittlig gjennomsnittlig gjennomsnittlig gjennomsnittlig utvikling på par med monospråk modeller, og 3) er stort forbetra ved førre nullsatt resultat i finsk, fransk og svensk Den beste resultatene er oppnådd med den fleire språk XLM-R-modellen. Som data, bruker vi CORE-korpusserien som inneheld registrerte data frå nettet utan streking.', 'pl': 'Niniejszy artykuł bada klasyfikację dokumentów pochodzących z nieograniczonej sieci internetowej, takich jak artykuły informacyjne czy blogi opinii, w wielojęzycznym otoczeniu, badając zarówno korzyści płynące ze szkolenia z wielu języków, jak i możliwości zerowego transferu między językami. Podczas gdy szeroki zakres zróżnicowań językowych znajdujących się w internecie stanowi wyzwanie dla klasyfikacji rejestru, ostatnie badania wykazały, że można osiągnąć dobry poziom transferu między językami z obszernego angielskiego korpusu CORE na inne języki. W niniejszym badaniu pokazujemy, że szkolenie z wieloma językami 1) korzysta z języków z ograniczoną ilością danych z adnotacjami rejestru, 2) średnio osiąga wydajność na równi z modelami jednojęzycznymi, a 3) znacznie poprawia wcześniej wyniki zero-shot w języku fińskim, francuskim i szwedzkim. Najlepsze rezultaty osiąga się z wielojęzycznym modelem XLM-R. Jako dane wykorzystujemy serię korpusów CORE zawierającą dane z adnotacji rejestru z nieograniczonej sieci internetowej.', 'ro': 'Acest articol studiază clasificarea documentelor de pe web fără restricții, cum ar fi articole de știri sau bloguri de opinie, într-un cadru multilingv, explorând atât beneficiile instruirii în mai multe limbi, cât și capacitățile de transfer încrucișat de zero-shot. În timp ce gama largă de variații lingvistice găsite pe internet reprezintă provocări în ceea ce privește clasificarea registrelor, studiile recente au arătat că pot fi atinse niveluri bune de transfer translingvistic de la corpul extins CORE în limba engleză la alte limbi. În acest studiu, demonstrăm că instruirea pe mai multe limbi 1) beneficiază limbile cu cantități limitate de date adnotate în registru, 2) atinge în medie performanțe egale cu modelele monolingve și 3) îmbunătățește considerabil rezultatele anterioare zero-shot în finlandeză, franceză și suedeză. Cele mai bune rezultate sunt obținute cu modelul XLM-R multilingv. Ca date, folosim seria CORE corpus care conține date adnotate de registru de pe web nelimitat.', 'sr': 'Ovaj članak proučava registriranje klasifikacije dokumenta iz neograničene mreže, kao što su novinski članovi ili blogovi mišljenja, u multijezičkom stanju, istražujući i korist obuke na višestrukim jezicima i mogućnosti za prebacivanje jezika nulog snimka. Iako širok raspon lingvističkih varijacija nalaženih na internetu predstavlja izazove za klasifikaciju registracija, nedavno ispitivanje pokazalo je da se mogu postići dobar nivo prevođenja preko jezika iz širokog engleskog korpusa CORE na druge jezike. U ovom ispitivanju pokazujemo da obuka na višestrukim jezicima 1) koristi jezike sa ograničenim količinama podataka o registraciji, 2) na prosjeku postiže učinkovitost na par sa monojezičkim modelima, a 3) veoma poboljšava na prethodnim rezultatima nule snimanja na finskom, francuskom i švedskom. Najbolji rezultati su postignuti sa multijezičkim XLM-R modelom. Kao podaci, koristimo seriju korpusa CORE-a koja uključuje registraciju annotiranih podataka iz neograničene mreže.', 'si': 'මේ ලේඛනය අධ්\u200dයාස කරන්නේ නැති වෙබ් වලින් ලේඛනයේ ලේඛනය ලේඛනය ලේඛනය ලේඛනය ලේඛනය හා විශ්වාස බ්ලෝග් වලින්, බොහොම භාෂාවක සැකසුම් වලින් ප වෙබ් එකේ හොයාගත්ත භාෂාවික වෙනස් විසින් විසින් විසින් විසින් විසින් අවශ්\u200dය වෙනුවෙන් විසින් විසින් විසින් විසින් විසින් ප්\u200dර මේ පරීක්ෂණයේදී, අපි පෙන්වන්නේ වැඩි භාෂාවල් 1) භාෂාවට ප්\u200dරයෝජනය කරන්න පුළුවන් භාෂාවට ප්\u200dරයෝජනය කරන්න, 2) පරීක්ෂණයෙන් ප්\u200dරයෝජනයෙන් ප්\u200dරයෝජන හොඳම ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිය XLM-R ම දත්ත විදියට, අපි CORE කෝර්පුස් සීමාව පාවිච්චි කරනවා, ප්\u200dරතිස්ථාපනය කරන්නේ නැති වෙබ් වලින් ප්\u200dරති', 'so': "Warqaddaas wuxuu ka baranayaa qoraalka qoraalka dukumentiyada ee internetka aan rasmi lahayn, tusaale ahaan warqadaha warqadaha ama bogagga aragtida, kaas oo ku baaraandegaya faa'iidada waxbarashada luuqadaha badan iyo awoodda wareejinta luuqadaha nooca ah. Inta lagu jiro iskuulka diiwaangelinta waxaa laga helaa dhibaatooyin kala duduwan oo luuqadaha kala duduwan, waxbarashada ugu dambeysayna waxay caddaynayeen in heerarka wanaagsan ee laga soo wareejiyo qoraalka afka ingiriisiga ee CORE ilaa luuqadaha kale. Waxbarashadan waxaynu tusnaynaa in waxbarasho ku qoran luuqado kala duduwan 1) lagu faa'iido luuqado ah oo ku qoran macluumaad diiwaangelinta, 2) ugu badnaan waxyaabaha lagu sameyn karo tusaalaha afka noocyada ah, iyo 3) waxey aad u bedeshaa arimaha hore oo lagu qoray Finnish, Faraansiis iyo Iswidishka. Midhaha ugu wanaagsan waxaa lagu helaa modelka XLM-R ee luuqadaha kala duduwan. Wixii macluumaad ah, waxaynu isticmaalnaa safarka CORE oo ku qoran macluumaadka diiwaangelinta oo la caddeeyey bogagga aan la isticmaalin.", 'mt': 'Dan l-artikolu jistudja l-klassifikazzjoni tad-dokumenti mill-internet mhux ristrett, bħal artikoli tal-a ħbarijiet jew blogs tal-opinjonijiet, f’ambjent multilingwi, li jesplora kemm il-benefiċċju tat-taħriġ dwar lingwi multipli kif ukoll il-kapaċitajiet għal trasferiment translingwi b’ritratt żero. Filwaqt li l-firxa wiesgħa ta’ varjazzjoni lingwistika misjuba fuq l-internet to ħloq sfidi għall-klassifikazzjoni tar-reġistri, studji riċenti wrew li jistgħu jinkisbu livelli tajbin ta’ trasferiment translingwistiku mill-CORE corpus estensiv Ingliż għal lingwi oħra. F’dan l-istudju, nagħmlu evidenza li t-taħriġ dwar diversi lingwi 1) jibbenefika lingwi b’ammonti limitati ta’ dejta annotata fir-reġistru, 2) bħala medja jikseb prestazzjoni daqs mudelli monolingwi, u 3) itejjeb ħafna fuq riżultati preċedenti b’ritratt żero fil-Finlandiż, Franċiż u Svediż. L-aħjar riżultati jinkisbu bil-mudell XLM-R multilingwi. Bħala dejta, aħna nużaw is-serje CORE corpus li fiha reġistru tad-dejta annotata mill-internet mhux ristrett.', 'ta': 'இந்த கட்டுரையில் அறியாத இணையத்திலிருந்து ஆவணங்கள் வகைப்படுத்தலை படிக்கும், செய்தி கட்டுரைகள் அல்லது கருத்துரைகள் போன்ற, பல மொழிகள் அமைப்பில், பல மொழிகளில் பயிற்ச While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved.  இந்த ஆராய்ச்சியில், நாம் பல மொழிகளில் பயிற்சியை காட்டுகிறோம் என்று காண்பிக்கிறோம் அது எல்லாம் பதிவேட்டில் குறிப்பிட்ட தகவல்களுடன் பயன்படுத்தப்படும் மொழிகளில் ஒர பல மொழி XLM-R மாதிரியால் சிறந்த முடிவுகள் அடையப்பட்டது. தகவலாக, நாம் CORE கார்ப்ஸ் தொடர்களை பயன்படுத்துகிறோம் வெளிப்படுத்தப்படாத இணையத்திலிருந்து பதிவு அறிவிக்கப்பட்ட தக', 'ur': 'یہ لکھا بغیر محدودیت ویب سے لکھی ہوئی دلیلیں کا کلیسٹ کریسٹر کی تعلیم کرتا ہے، جیسے اخبار لکھائی یا منظور بلاگ، ایک بہت سی زبان تنظیم میں، دونوں کی تعلیم کے فائدہ کا تحقیق کرتا ہے بہت سی زبانوں پر اور صفر-شٹ کریسٹ While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that cross-lingual transfer levels from the extensive English CORE corpus to other languages can be achieved. اس تحقیق میں ہم دکھاتے ہیں کہ تعلیم کئی زبانوں پر 1) لکھی زبانوں کا فائدہ پہنچاتا ہے جن کے ذریعہ سے محدودہ دکھائے گئے ہیں، 2) متوسط سے ایک زبان مدل کے ساتھ فائدہ پہنچاتا ہے، اور 3) فنلاندی, فرانسوی اور سوئدی کے پہلے صفر شوٹ نتیجے پر بہت اضافہ ہوتا بہترین نتیجے ملتی زبان کے XLM-R موڈل سے پہنچ جاتے ہیں. ہم نے CORE کورپوس سریریس کے طور پر استعمال کیا ہے جس میں رسیسٹر کے ذریعہ غیر محدودہ ویب سے اظہار کیا گیا ہے.', 'sv': 'Denna artikel studerar klassificering av dokument från den obegränsade webben, såsom nyhetsartiklar eller opinionsbloggar, i en flerspråkig miljö, och undersöker både fördelarna med utbildning i flera språk och möjligheterna för noll-skott tvärspråklig överföring. Även om det breda utbudet av språkliga variationer som finns på webben innebär utmaningar för registerklassificering, har nyligen genomförda studier visat att goda nivåer av tvärspråklig överföring från den omfattande engelska CORE-korpusen till andra språk kan uppnås. I denna studie visar vi att utbildning på flera språk 1) gynnar språk med begränsade mängder registernoterade data, 2) i genomsnitt uppnår prestanda i nivå med enspråkiga modeller och 3) förbättrar avsevärt jämfört med tidigare nollskottsresultat på finska, franska och svenska. De bästa resultaten uppnås med den flerspråkiga XLM-R modellen. Som data använder vi CORE corpus-serien med registerkommenterade data från den obegränsade webben.', 'uz': "Ushbu maqola mavjud boʻlmagan veb tarkibidagi hujjatlarni tahrirlash imkoniyatini o'rganadi, masalan news maqolalari yoki maoni bog'lanuvchilari, ko'plab tillarda, ko'plab tillar uchun trening foydalanishini va ikkita tillar orqali o'zgartirish imkoniyatini o'rganadi. Veb- saytda ko'p tillar o'zgarishning kengaytmalari qo'shish uchun muammolar bo'ladi. Yaqinda o'rganishlar qo'shilgan ingliz tilidan bogʻliq tillardan CORE corpusdan boshqa tillardan uzoq darajalari imkoniyatini ko'rsatadi. Bu taʼminotda biz bir necha tillar bilan bir xil tilda taʼminlov qilishni ko'rsatdik, boshqa tillar qo'llangan maʼlumotlar bilan chegara bo'lgan tillar bilan foydalanadi, 2) o'rtasida o'rtacha monolingual modellar bilan bajarishni bajaradi va 3) oldingi nuqta ishlatilgan natijalari Finnish, Fransuzcha va Sh Ko'pchilik tili XLM-R modeli bilan eng yaxshi natijalar bajarildi. Maʼlumotlar sifatida, biz CORE corpus seriидан foydalanamiz, oʻrnatilmagan veb- sahifa haqida yangilangan maʼlumotni tanlash mumkin.", 'vi': 'This article studies register classification of documents from the hạn chế web, such as news articles hay ý kiến blog, in a multiple settle, exploret both the condition of đào tạo on multiple languages and the capacity for zero-shot cross-ngôn ngữ. Trong khi những nghiên cứu gần đây đầy đủ các biến đổi ngôn ngữ trên trang web gây ra nhiều thử thách cho việc phân loại đăng ký, nhưng cũng có những nghiên cứu cho thấy có khả năng truyền qua ngôn ngữ rộng rãi từ tập đoàn Cortland tiếng Anh sang các ngôn ngữ khác. Trong nghiên cứu này, chúng tôi cho thấy giáo dục về nhiều ngôn ngữ Những kết quả tốt nhất được đạt được với mô hình XLM-R đa dạng. Là dữ liệu, chúng tôi sử dụng bộ R.E. với dữ liệu ghi chú từ mạng nội bộ không hạn chế.', 'nl': 'Dit artikel bestudeert registerclassificatie van documenten van het onbeperkte web, zoals nieuwsberichten of opinieblogs, in een meertalige omgeving, waarbij zowel het voordeel van training in meerdere talen als de mogelijkheden voor zero-shot cross-lingual transfer wordt onderzocht. Hoewel de grote verscheidenheid aan taalkundige variaties op het web uitdagingen oplevert voor de classificatie van registers, hebben recente studies aangetoond dat goede niveaus van trans-linguale overdracht van het uitgebreide Engelse CORE-corpus naar andere talen kunnen worden bereikt. In deze studie tonen we aan dat training in meerdere talen ten goede komt aan talen met beperkte hoeveelheden gegevens met aantekeningen in het register, 2) gemiddeld prestaties behaalt die vergelijkbaar zijn met eentalige modellen, en 3) aanzienlijk verbetert ten opzichte van eerdere zero-shot resultaten in het Fins, Frans en Zweeds. De beste resultaten worden bereikt met het meertalige XLM-R model. Als data gebruiken we de CORE corpusserie met registergeannoteerde gegevens uit het onbeperkte web.', 'bg': 'Тази статия изучава класификацията на документите от неограничената интернет мрежа, като например статии с новини или блогове с мнения, в многоезична обстановка, изследвайки както ползите от обучението на няколко езика, така и възможностите за нулев междуезичен трансфер. Въпреки че широката гама от езикови вариации, открити в интернет, поставя предизвикателства за класификацията на регистрите, последните проучвания показват, че могат да бъдат постигнати добри нива на междуезичен трансфер от обширния английски корпус CORE към други езици. В това проучване ние показваме, че обучението на няколко езика 1) облагодетелства езици с ограничено количество анотирани данни от регистъра, 2) средно постига представяне на ниво едноезични модели и 3) значително се подобрява спрямо предишните нулеви резултати на финландски, френски и шведски език. Най-добрите резултати се постигат с многоезичния модел. Като данни използваме корпусната серия с анотирани регистри данни от неограничената интернет страница.', 'id': 'This article studies register classification of documents from the unrestricted web, such as news articles or opinion blogs, in a multilingual setting, exploring both the benefit of training on multiple languages and the capabilities for zero-shot cross-lingual transfer.  Sementara jangkauan luas variasi bahasa yang ditemukan di web menunjukkan tantangan untuk klasifikasi register, penelitian baru-baru ini menunjukkan bahwa tingkat yang baik dari transfer saling bahasa dari CORE corpus Inggris ekstensif ke bahasa lain dapat dicapai. Dalam penelitian ini, kami menunjukkan bahwa pelatihan dalam berbagai bahasa 1) bahasa keuntungan dengan jumlah terbatas data-annotasi register, 2) rata-rata mencapai prestasi yang sama dengan model monobahasa, dan 3) meningkat jauh pada hasil zero-shot sebelumnya dalam Finlandia, Perancis dan Swedia. Hasil terbaik dicapai dengan model XLM-R berbilang bahasa. Sebagai data, kami menggunakan seri CORE corpus yang menampilkan register data annotasi dari web yang tidak terbatas.', 'hr': 'Ovaj članak proučava registriranje klasifikacije dokumenta iz neograničene mreže, kao što su novinski članovi ili blogovi mišljenja, u multijezičkom postavljanju, istražujući i korist obuke na višestrukim jezicima i mogućnosti prijenosa s nulom snimkom preko jezika. Iako širok raspon lingvističkih promjena nalaženih na internetu predstavlja izazove za klasifikaciju registracija, nedavna ispitivanja pokazala su da se mogu postići dobra razina preko jezika prebacivanja iz širokog engleskog korpusa CORE na druge jezike. U ovom ispitivanju pokazujemo da obuka na višestrukim jezicima 1) koristi jezike s ograničenim količinama podataka o registraciji, 2) na prosjeku postiže učinkovitost na par s monojezičkim modelima, a 3) značajno poboljšava na prethodnim rezultatima nule snimanja na finskom, francuskom i švedskom. Najbolji rezultati su postignuti s multijezičkim XLM-R modelom. Kao podaci, koristimo seriju korpusa CORE-a koja uključuje registraciju annotiranih podataka iz neograničene mreže.', 'ko': '본고는 다중 언어 환경에서 비제한 네트워크에서 온 문서(예를 들어 뉴스 기사나 관점 블로그)를 등록 분류하고 다중 언어 교육의 장점과 제로 크로스 언어 이동 능력을 연구했다.비록 인터넷에서 발견된 광범위한 언어 변이가 어역 분류에 도전을 가져왔지만 최근의 연구에 의하면 대량의 영어 핵심 어료 라이브러리에서 다른 언어로의 양호한 크로스 언어 이동을 실현할 수 있다고 한다.이 연구에서 우리는 다양한 언어의 훈련 1)이 어역 주석 데이터량이 제한된 언어에 유리하고, 2) 평균적으로 단어 모델과 비슷한 성능에 이르는 것을 발견했다. 3) 핀란드어, 프랑스어, 스웨덴어에서의 제로 포 결과를 크게 개선했다.다국어 XLM-R 모델을 사용하면 최상의 효과를 얻을 수 있습니다.데이터로서 우리는 핵심 어료 라이브러리 시리즈를 사용하는데 그 중에서 무제한 네트워크에서 온 등록 주석 데이터를 포함한다.', 'fa': 'این مقاله تحقیقات گروه\u200cبندی سند\u200cها را از وب غیرمحدود ثبت می\u200cکند، مانند مقاله\u200cهای خبری یا بلاگ نظر، در یک تنظیم بسیاری زبان، در جستجو هر دو سود آموزش بر زبان\u200cهای متعدد و توانایی برای انتقال زبان\u200cهای متعدد صفر، تحقیق می\u200cکند. در حالی که مدت گسترده تغییرات زبان\u200cشناسی که در وب یافته\u200cاند، چالش\u200cهایی برای گروه\u200cشناسی ثبت می\u200cکند، تحقیقات اخیرا نشان داده\u200cاند که سطح خوبی از انتقال متفاوت زبان\u200cهای زیادی از کورپوس انگلیسی CORE به زبان\u200cهای در این مطالعه، ما نشان می دهیم که آموزش روی زبانهای متعدد 1) به زبانها سود می دهد که با تعداد محدودیت داده\u200cهای ثبت شده، ۲) در میانگین عملکرد را با مدل\u200cهای متعدد زبان می\u200cرساند، و ۳) در نتیجه\u200cهای صفر پیشین در فنلاندی، فرانسوی و سوئدی بهتر می\u200cشود. بهترین نتایج با مدل XLM-R چند زبان رسیده می شوند. به عنوان اطلاعات، ما از مجموعه CORE corpus استفاده می کنیم که از وب غیرمحدودیت اطلاعات آشنا شده را مشخص می کنیم.', 'sw': 'Makala hii inasoma kutangaza usambazaji wa nyaraka kutoka tovuti isiyo sahihi, kama vile makala za habari au blogu za maoni, katika mazingira ya lugha mbalimbali, kwa kutambua faida ya mafunzo katika lugha mbalimbali na uwezo wa usafirishaji wa lugha zisizo na sifa. Wakati mabadiliko mengi ya lugha yanayopatikana kwenye mtandao unaposhindwa changamoto za kutangazwa kwa ajili ya uandishi wa kujiandikisha, tafiti za hivi karibuni zimeonyesha kuwa kiwango vizuri cha uhamiaji wa lugha mbalimbali kutoka makampuni ya Kiingereza ya CORE hadi lugha nyingine zinaweza kufanikiwa. Katika utafiti huu, tunaonyesha kuwa mafunzo katika lugha mbalimbali ya 1) yanafaidia lugha zenye idadi kubwa ya taarifa zinazoandikishwa, 2) kwa wastani hufanikiwa utendaji wa mifano ya lugha za kimonolinguli, na 3) kwa kiwango kikubwa kinaongezeka kwa matokeo yasiyo na sifa iliyopita katika Kifinishi, Kifaransa na Kiswadishi. Matokeo bora yamefanishwa na modeli ya XLM-R ya lugha mbalimbali. Kama taarifa, tunatumia mfululizo wa makampuni ya CORE na kuonyesha taarifa zilizotajwa kutoka kwenye tovuti isiyo sahihi.', 'da': 'Denne artikel undersøger klassificeringen af dokumenter fra det ubegrænsede web, såsom nyhedsartikler eller meningsblogge, i en flersproget miljø, og undersøger både fordelene ved træning i flere sprog og mulighederne for nulskud tværsproget overførsel. Mens den brede vifte af sproglige variationer, der findes på nettet, udgør udfordringer for registerklassificering, har nylige undersøgelser vist, at der kan opnås gode niveauer af tværsproget overførsel fra det omfattende engelske CORE korpus til andre sprog. I denne undersøgelse viser vi, at træning på flere sprog 1) gavner sprog med begrænsede mængder registernoterede data, 2) i gennemsnit opnår resultater på samme niveau som ensprogede modeller, og 3) forbedrer betydeligt i forhold til tidligere nulskudsresultater på finsk, fransk og svensk. De bedste resultater opnås med den flersprogede XLM-R model. Som data bruger vi CORE corpus serien med registernoterede data fra det ubegrænsede web.', 'tr': 'Bu makala çykyş edilmedik web sahypalaryndan, täzelikler ýa-da düşünýän bloglary ýaly, bir näçe dil düzümlerinde, birnäçe diller üçin okuw gurmanyň faydasyny we zerw atly dillerden geçirmek üçin mümkinçilikleri bardyr. Web içinde bulunan lingwistiki üýtgeşmeler klasifikasy üçin kynçylyklar döredip görkezilýär, soňky araştyrmalar iňlisçe CORE korpusdan başga dillere ýetip bilýärler. Bu aramda, biz birnäçe diller üçin bilim taýýarlanmasy 1) sany diýmek isleýän dillerden, 2) ortalamada monolingw modelleri bilen taýýarlanmasy üçin gowurar, we 3) öňki 0-atak netijelerini Finlandiýa, fransuzça we Şwediýada gowurar. Iň gowy netijeler birnäçe dilli XLM-R modeli bilen berilýär. Maglumat hökmünde biz CORE korpus serisini ulanýarys, daýalanmaýan web tarapyndan berilen ýazylan maglumaty barýarlar.', 'am': 'ይህ ጽሑፍ በብዛት ቋንቋዎች ላይ የመጠቀም ጥቅም እና በክፍለ ቋንቋ መቀናቀል የክፍል ቋንቋ መቃወሚያ የሚችሉትን የሰነዶች ክፍልፍሎች ማነሳትን እና የክፍል ቋንቋ መቀናቀል የሚችሉትን አካባቢነትን በመጠቀም ያስተምራል፡፡ While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved.  በዚህ ትምህርት፣ በብዛት ቋንቋዎች ላይ የተጠቃሚ ቋንቋ 1) የተጠቃሚ ቁጥጥር የመረጃ ዳታዎችን የሚጠቅመውን ቋንቋዎች (2) በመተካከለኛውም ብዛት በሞሎንቋል ዓይነቶች ላይ ድምፅን ያገኛል፡፡ የበለጠ ፍሬዎች በብዙ ቋንቋ በXLM-R ሞዴል የተደረገ ነው፡፡ እንደ ዳራዎች፣ CORE ኮርፓስ ተርሚዛን እናስቀምጣለን፡፡', 'de': 'Dieser Artikel untersucht die Registerklassifizierung von Dokumenten aus dem uneingeschränkten Web, wie Nachrichtenartikel oder Meinungsblogs, in einem mehrsprachigen Umfeld und untersucht sowohl den Nutzen von Schulungen in mehreren Sprachen als auch die Möglichkeiten für den Zero-Shot-crosslingualen Transfer. Während das breite Spektrum an sprachlichen Variationen im Web Herausforderungen für die Registerklassifizierung darstellt, haben aktuelle Studien gezeigt, dass ein guter translingualer Transfer vom umfangreichen englischen CORE-Korpus in andere Sprachen erreicht werden kann. In dieser Studie zeigen wir, dass das Training in mehreren Sprachen 1) Sprachen mit begrenzter Menge an Registerannotierten Daten zugute kommt, 2) im Durchschnitt Leistung auf Augenhöhe mit einsprachigen Modellen erzielt und 3) die bisherigen Null-Shot-Ergebnisse in Finnisch, Französisch und Schwedisch deutlich verbessert. Die besten Ergebnisse werden mit dem mehrsprachigen XLM-R Modell erzielt. Als Daten verwenden wir die CORE-Korpusserie mit Registerkommentierungen aus dem uneingeschränkten Web.', 'sq': 'This article studies register classification of documents from the unrestricted web, such as news articles or opinion blogs, in a multilingual setting, exploring both the benefit of training on multiple languages and the capabilities for zero-shot cross-lingual transfer.  Ndërsa gama e gjerë e variacioneve gjuhësore e gjetur në internet paraqet sfida për klasifikimin e regjistrimit, studimet e fundit kanë treguar se nivele të mira të transferimit ndërgjuhësor nga korpusi i gjerë anglez CORE në gjuhë të tjera mund të arrihen. In this study, we show that training on multiple languages 1) benefits languages with limited amounts of register-annotated data, 2) on average achieves performance on par with monolingual models, and 3) greatly improves upon previous zero-shot results in Finnish, French and Swedish.  Rezultatet më të mira arrihen me modelin XLM-R shumëgjuhës. Si të dhëna, ne përdorim serinë CORE corpus që paraqet të dhëna të regjistruara nga rrjeti i pa kufizuar.', 'af': "Hierdie artikel studiereer klasifikasie van dokumente registreer van die onverstrekte web, soos nuusartikels of besonderhede blogs, in 'n veelvuldige instelling, ondersoek beide die voordeel van onderwerp op veelvuldige tale en die kapasiteite vir nul-skoot kruistale oordrag. Alhoewel die wyde omvang van lingwisiese veranderinge gevind op die web poseer uitdagings vir registrasie klasifikasie, het onlangse studie vertoon dat goeie vlakke van kruistale oordrag van die uitbreidige Engelske korpus na ander tale kan bereik word. In hierdie studie, wys ons dat onderwerp op veelvuldige tale 1) voordeel tale met beperkte hoeveelheid registreerde data, 2) op gemiddelde bereik uitvoeging op par met monolinge modele, en 3) baie verbeter op vorige nul-skoot resultate in Finnish, Frans en Sweedse. Die beste resultate word bereik met die multilinglike XLM-R model. As data, gebruik ons die CORE corpus reeks wat die registrasie aangetelde data van die ongestrekte web.", 'hy': 'Այս հոդվածը ուսումնասիրում է անսահմանափակ ցանցի փաստաթղթերի դասակարգումը, ինչպիսիք են նորությունների հոդվածները կամ կարծիքի բլոգերը, բազլեզվով միջավայրում, ուսումնասիրելով բազմալեզվով լեզուների ուսումնասիրության առավելությունը, ինչպես նաև զրոյի Մինչդեռ ցանցում գտնվող լեզվաբանական տարբերությունների լայն տարբերակը դժվարություններ է առաջացնում ռեստորանների դասակարգման համար, վերջին ուսումնասիրությունները ցույց են տալիս, որ կարելի է հասնել լեզվաբանական փոխանցման լավ մակարդակներին անգլերենի CORECorpus-ից այլ լեզուների Այս ուսումնասիրության ընթացքում մենք ցույց ենք տալիս, որ բազմալեզուների ուսումնասիրությունը 1) օգտակար է լեզուներին, որոնք ունեն սահմանափակ քանակությամբ գրված տվյալներ, 2) միջինում հասնում են մեկլեզու մոդելների և 3) մեծ բարելավում են ֆինլարենի, ֆրանսիացի և շվեդիացի դեպքում առաջին զրոյ Լավագույն արդյունքները հասնում են XLM-R բազլեզու մոդելի միջոցով: Որպես տվյալներ, մենք օգտագործում ենք CORECorpus-ի շարքը, որը ներկայացնում է անսահմանափակ ցանցի գրված տվյալներ:', 'bn': 'এই প্রবন্ধটি অন্যান্য ভাষায় সংবাদ প্রবন্ধ বা মতামত ব্লগের নথিগুলোর বিভাগ নিবন্ধকতা গবেষণা করছে, যেমন সংবাদ প্রবন্ধ বা ব্লগ, অনেক ভাষায় বিভিন্ন ভাষায় প্রশিক্ষণের সু While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved.  এই গবেষণায় আমরা দেখাচ্ছি যে বেশ কয়েকটি ভাষায় প্রশিক্ষণের প্রশিক্ষণ সীমিত ভাষায় সুবিধা প্রদান করা হয়েছে যার সাথে সীমিত নিবন্ধন-বিজ্ঞাত তথ্য, ২) সাধারণ ভাষায় সাধারণ ভা বহুভাষায় XLM-R মডেল দিয়ে সবচেয়ে ভাল ফলাফল অর্জন করা হয়েছে। তথ্য হিসেবে আমরা কোর্পাস সিরিজ ব্যবহার করি যেখানে রেজিস্টারের বিস্তারিত তথ্য প্রকাশ করা হয়েছে অথচ ওয়েব থেকে।', 'az': 'Bu m…ôktub √ßoxlu dil qurńüularńĪnda, √ßoxlu dil qurńüularńĪnda t…ôhsil etm…ô faydalarńĪnńĪ v…ô sńĪfńĪr-shot √ßoxlu dil transferisinin f…ôaliyy…ôtini t…ôhsil edir. ńįnternet i√ßind…ô bulunan dil d…ôyiŇüiklikl…ôrinin geniŇüliyi s…ôb…ôbi register klasifikasyonu √ľ√ß√ľn √ß…ôtinlikl…ôr…ô m…ôcbur ed…ôrk…ôn, son d…ôyiŇüiklikl…ôrin √ßox geniŇüliyi ńįngilis CORE korpusundan baŇüqa dill…ôr…ô istifad…ô edil…ô bil…ôc…ôyini g√∂st…ôrdil…ôr. Bu t…ôcr√ľb…ôd…ô, √ßoxlu dill…ôrd…ô t…ôcr√ľb…ô 1 il…ô m√ľ…ôyy…ôn edil…ôn dill…ôr…ô m√ľ…ôyy…ôn edilmiŇü m…ôlumatlarńĪn m…ônf…ô…ôti verir, 2 il…ô ortalama t…ôcr√ľb…ôsi monodil modell…ôri il…ô m√ľ…ôyy…ôn edilir v…ô 3) Finlandiya, FransńĪzca v…ô Ňěvediyada …ôvv…ôlki n√∂qt…ôsin sonu√ßlarńĪnńĪ √ßox yaxŇüńĪlaŇüdńĪrńĪr. ∆Źn yaxŇüńĪ sonu√ßlar √ßoxlu dil XLM-R modeli il…ô baŇüarńĪlńĪr. M…ôlumatlar kimi, CORE korpus serisini istifad…ô edirik ki, m√ľ…ôyy…ôn edilm…ômiŇü web t…ôr…ôfind…ôn m…ôlumatlarńĪ m…ôlumatlarńĪnńĪ bel…ô g√∂st…ôrir.', 'ca': "Aquest article estudia la classificació de documents de la web sense restriccions, com articles de notícies o blogs d'opinió, en un entorn multilingüi, explorant tant el benefici de l'entrenament en múltiples llengües com les capacitats de transfer ència translingüística de fotografies zero. Mentre que la gran varietat de variacions lingüístices trobadas a la web posen reptes per a la classificació del registre, estudis recents han demostrat que es poden aconseguir bons nivells de transfer ència translingüística del corps anglès CORE a altres llengües. En aquest estudi, demostram que la formació en múltiples llengües 1) beneficia les llengües amb quantitats limitades de dades anotats en registre, 2) aconsegueix un rendiment mitjà igual que els models monolingües, i 3) millora molt en finlandès, francès i suec els resultats anteriors de fotografia zero. Els millors resultats s'aconsegueixen amb el model XLM-R multilingüe. Com a dades, utilitzem la sèrie CORE corpus amb registres de dades anotates de la Web sense restriccions.", 'bs': 'Ovaj članak proučava registraciju klasifikacije dokumenta iz neograničene mreže, kao što su novinski članovi ili blogovi mišljenja, u multijezičkom stanju, istražujući i korist obuke na višestrukim jezicima i sposobnosti za prebacivanje jezika nulog snimka. Iako širok niz lingvističkih varijacija nalaženih na internetu predstavlja izazove za klasifikaciju registracija, nedavno ispitivanje pokazalo je da se mogu postići dobar nivo prekograničnog prijenosa iz širokog engleskog korpusa CORE na druge jezike. U ovom ispitivanju pokazujemo da obuka na višestrukim jezicima 1) koristi jezike sa ograničenim količinama podataka o registraciji, 2) na prosjeku postiže učinkovitost na par sa monojezičkim modelima, a 3) veoma poboljšava na prethodnim rezultatima nule snimanja na finskom, francuskom i švedskom. Najbolji rezultati su postignuti sa multijezičkim XLM-R modelom. Kao podaci, koristimo seriju CORE korpusa koja uključuje registrirane annotirane podatke iz neograničene mreže.', 'cs': 'Tento článek studuje klasifikaci dokumentů z neomezeného webu, jako jsou zpravodajské články nebo blogy o názorech, ve vícejazyčném prostředí, zkoumá jak přínosy školení na více jazycích, tak možnosti nulového přenosu mezi jazyky. Zatímco široká škála jazykových variací nalezených na webu představuje výzvu pro klasifikaci registrů, nedávné studie ukázaly, že lze dosáhnout dobré úrovně přenosu mezi jazyky z rozsáhlého anglického korpusu CORE do jiných jazyků. V této studii ukazujeme, že školení na více jazycích 1) přináší prospěch jazyků s omezeným množstvím dat v registru, 2) v průměru dosahuje výkonu stejného jako monojazyčné modely a 3) výrazně zlepšuje předchozí nulové výsledky ve finštině, francouzštině a švédštině. Nejlepších výsledků dosahuje vícejazyčný model XLM-R. Jako data používáme korpusovou řadu CORE obsahující registrovaná data z neomezeného webu.', 'fi': 'Tässä artikkelissa tutkitaan rajattomasta verkosta peräisin olevien asiakirjojen, kuten uutisartikkeleiden tai mielipitebloggien, rekisteriluokittelua monikielisessä ympäristössä, ja tutkitaan sekä monikielisen koulutuksen hyötyjä että mahdollisuuksia nollakieliseen siirtoon. Vaikka verkossa esiintyvä kielellinen vaihtelu asettaa haasteita rekisteriluokittelulle, viimeaikaiset tutkimukset ovat osoittaneet, että laaja englanninkielinen CORE-korpus voi siirtyä hyvin eri kielille. Tässä tutkimuksessa osoitetaan, että monikielinen koulutus 1) hyödyttää kieliä, joilla on rajallinen määrä rekisterimerkintöjä, 2) saavuttaa keskimäärin suorituskykyä yksikielisten mallien kanssa ja 3) parantaa huomattavasti aikaisempia nollatuloksia suomeksi, ranskaksi ja ruotsiksi. Parhaat tulokset saavutetaan monikielisellä XLM-R-mallilla. Käytämme aineistona CORE-korpussarjoja, joissa on rekisterimerkinnällä merkitty tieto rajattomasta verkosta.', 'et': 'Käesolevas artiklis uuritakse piiramatu veebi dokumentide, näiteks uudisteartiklite või arvamusblogide registreerimist mitmekeelses keskkonnas, uurides nii mitmekeelse koolituse eeliseid kui ka võimalusi mitmekeelseks ülekandmiseks. Kuigi veebis leiduv keeleline varieeruvus tekitab registrite klassifitseerimisele probleeme, on hiljutised uuringud näidanud, et on võimalik saavutada hea taseme keeltevahelise ülekande ulatuslikust inglise CORE korpusest teistesse keeltesse. Käesolevas uuringus näitame, et mitme keele koolitus 1) toob kasu keeltele, kus on piiratud kogus registreeritud andmeid, 2) saavutab keskmiselt võrdse tulemuse ühekeelsete mudelitega ja 3) paraneb oluliselt varasematest nullkatse tulemustest soome, prantsuse ja rootsi keeles. Parimad tulemused saavutatakse mitmekeelse XLM-R mudeliga. Andmetena kasutame CORE korpuse seeriat, mis sisaldavad piiramatult veebist registreeritud annoteeritud andmeid.', 'jv': 'Artik iki diputara bener winih sing nggawe dokumen ning web sing or a bisa dianggawe, kaya Artik balêr nggawe winih dhéwé, lan uga sistem hukum kanggo sistem hukum sistem plural, ndheke dhéwé éntukno sistem sing gawe nguasai perusahaan langkung sampeyan sak Where\'s the last language Genjer-genjer diunting akeh akeh operasi nggawe ing web punika ingkang diputara winih sing nggawe winih sing, winih sing ngendalikno wong dhéwé kuwi tindakan luwih apik-langkung Nang barêng-barêng iki, kéné iso nglanggar tarjamahan kanggo langgar sapa luwih lan ingkang 1) kayané perusahaan kanggo mbangaké kantor kantor nggawe data, 2) ngono nglanggar tarjamahan kanggo nyenggap kanggo nyenggap tarjamahan karo model Monolyanse, lan 3) sing beraksi kanggo nyenggap tarjamahan liyané sing nyenggap kanggo nyenggap tarjamahan Laptop" and "Desktop Sampeyan data, kita gambar kelompus cor', 'he': 'המאמר הזה לומד רשום קליזציה של מסמכים מהרשת הלא מוגבלת, כמו מאמרים חדשות או בלוגים דעות, במסגרת רבות שפות, לחקור את היתרון של האימונים על שפות רבות וכל היכולות להעברת שפות דרך אפס צילומים. While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved.  במחקר הזה, אנחנו מראים שאימונים על שפות רבות 1) שפות מועילות עם כמויות מוגבלות של נתונים רשומים, 2) בממוצע משיגים ביצועים באותה מידה עם דוגמנים monolingual, 3) משתפרים באופן גדול על תוצאות אפס קודמות בפינים, צרפתיים ושוודיים. התוצאות הטובות ביותר ניתן להשיג עם מודל XLM-R רב-שפתי. בתור נתונים, אנו משתמשים בסדרת CORE corpus המכילה רשום נתונים מועטפים מהרשת ללא מוגבלות.', 'sk': 'Ta članek proučuje registrsko klasifikacijo dokumentov iz neomejenega spleta, kot so novinarski članki ali mnenjski blogi, v večjezičnem okolju, pri čemer raziskuje koristi usposabljanja o več jezikih in zmožnosti za brezposelni medjezični prenos. Medtem ko širok obseg jezikovnih različic, ki jih najdemo na spletu, predstavlja izzive za klasifikacijo registrov, so nedavne študije pokazale, da je mogoče doseči dobro raven medjezikovnega prenosa iz obsežnega angleškega korpusa CORE v druge jezike. V tej študiji smo pokazali, da usposabljanje na več jezikih 1) koristi jezikom z omejenimi količinami registriranih podatkov, 2) v povprečju doseže enako uspešnost kot enojezični modeli in 3) močno izboljša od prejšnjih ničelnih rezultatov v finščini, francoščini in švedščini. Najboljše rezultate dosežemo z večjezičnim modelom XLM-R. Kot podatki uporabljamo serijo korpusov CORE, ki vsebuje registrske podatke z oznakami iz neomejenega spleta.', 'ha': "Wannan makala na karanta fasalin takardar kwamfyutan da ba'a rubutu ba, kamar makarantar da masu basu'a da suniyoyi, a cikin muhalli na multi-lingui, sunã jarraba laban amfani da amfani da yin wa'anar wa masu yawa da abincin wa transfer na-nau'in-sifanci. A lokacin da kewayen variantun linguistic wanda aka sãmu a kan web yana da zane-zane wa tsarin lissafi, masu ƙarami sun nuna cewa, zane da zane-zane mai kyau na shige-linguin-na'ura daga makamps na Ingiriya ya faɗi zuwa wasu harshen, za'a iya sãmun. Daga wannan lõkaci, Munã nũna wa mafarinta a cikin wasu harshe 1) yana amfani da wasu zane da aka ƙayyade yawan data na rubũtar da-yanzu, 2) a kan kawaici, yana sãmun babban rabo a par da misãlai masu monoli'in, kuma 3) yana ƙaranci mai girma a gabanin jarrabo na sifanci ta farko a cikin Finnish, Faransiya da Iswidishki. An sami mafi kyaun matsala da misalin XLM-R-na'ura multi-lingui. Kama da data, za mu yi amfani da jumla'in COER na shirin wasu mutane na rubutun da aka sanar da shi daga tare webi wanda ba'a saɓa ba.", 'bo': 'This article studies register classification of documents from the unrestricted web, such as news articles or opinion blogs, in a multilingual setting, exploring both the benefit of training on multiple languages and the capabilities for zero-shot cross-lingual transfer. While the wide range of linguistic variation found on the web poses challenges for register classification, recent studies have shown that good levels of cross-lingual transfer from the extensive English CORE corpus to other languages can be achieved. In this study, we show that training on multiple languages 1) benefits with limited amounts of register-annotated data, 2) on average achieves performance on par with monolingual models, and 3) greatly improves in previous zero-shot results in Finnish, French and Swedish. A variety of languages on this study shows that training on multiple languages 1) benefits with limited amounts of register-annotated data, 2) on average achieves performance on par with monolingual models, and 3) greatly improve དབྱིབས་འབྲས་ཤོས་ཚད་མང་ཆེ་ཤོས་ཡོད་པའི་སྐད་རིགས་XLM-R་མ་དབྱིབས་ཡོད་པ་རེད། འུ་ཅག་གིས་སྐྱོན་འབྲེལ་མཐུད་མཁན་གྱི་ཞབས་ཞུ་སྤྱོད་བཞིན་པ་ལྟར་བཀོད་ཡོད་མེད་པའི་དྲ་རྒྱའི་ནང་ནས་གསལ་བཀོད'}
{'en': 'De-identification of Privacy-related Entities in Job Postings', 'ar': 'إلغاء تعريف الكيانات المتعلقة بالخصوصية في إعلانات الوظائف', 'es': 'Desidentificación de entidades relacionadas con la privacidad en las ofertas de empleo', 'pt': 'Desidentificação de entidades relacionadas à privacidade em anúncios de emprego', 'fr': "Anonymisation des entités liées à la confidentialité dans les offres d'emploi", 'ja': '求人でのプライバシー関連企業の識別解除', 'zh': '于招聘启事中罢私识别化', 'ru': 'Деидентификация объектов, связанных с конфиденциальностью, в объявлениях о вакансиях', 'hi': 'नौकरी पोस्टिंग में गोपनीयता से संबंधित संस्थाओं की डी-आइडेंटिफिकेशन', 'ga': 'Aonáin a bhaineann le Príobháideacht a Dhí-aithint i bPostálacha Poist', 'ka': 'სამუშაო წიგნაში პირადილურობის შესახებ ინტერტიტების განიდენტიფიკაცია', 'el': 'Αποταυτοποίηση οντοτήτων που σχετίζονται με το απόρρητο σε δημοσιεύσεις θέσεων εργασίας', 'hu': 'Az adatvédelemmel kapcsolatos szervezetek azonosítása álláshirdetésekben', 'it': 'Deidentificazione degli Enti connessi alla Privacy negli annunci di lavoro', 'kk': 'Тапсырма жіберілген жеке жіберілген нысандардың бөлек идентификациясы', 'lt': 'Su privatumu susijusių subjektų darbo vietose identifikavimo panaikinimas', 'ms': 'Nyahidentifikasi Entiti Berkaitan Pribadi dalam Posisi Kerja', 'mk': 'Деидентификација на ентитетите поврзани со приватноста во работните места', 'ml': 'ജോലിയുടെ പോസ്റ്റുകളില്\u200d സ്വകാര്യ ബന്ധപ്പെട്ട എന്റിറ്റികളുടെ സ്വകാര്യം തിരിച്ചറിയു', 'mt': 'Id-diidentifikazzjoni tal-Entitajiet relatati mal-Privatezza fl-Impjiegi', 'mn': 'Хувь холбогдолтой байгууллагуудыг ажлын захиргаанд тайлбарлах', 'no': 'Identifikasjon av personverktøyrelaterte einingar i jobbeposter', 'pl': 'Dezentyfikacja podmiotów związanych z prywatnością w ogłoszeniach pracy', 'ro': 'Deidentificarea entităților legate de confidențialitate în posturile publicate', 'so': 'Shaqooyin ku saabsan ganacsiga gaarka loo leeyahay', 'sr': 'Deidentifikacija područja povezanih sa privatnošću u poslovima', 'sv': 'Avidentifiering av integritetsrelaterade enheter i platsannonser', 'si': 'වැඩ පොස්ටන්ගේ පුද්ගලිකත්වය සම්බන්ධතාවක් ගැන පුද්ගලිකතාවක් අඳුන්න', 'ta': 'பணி புரியும் போது தனிப்பட்ட உள்ளீடுகளின் தனிப்பட்ட அடையாளம்', 'ur': 'جوب پوسٹینگ میں پرائیسی رابطہ دار ایٹینٹیوں کی غیر معلومات', 'uz': 'Comment', 'vi': 'Nhận dạng đơn vị bảo mật trong các công việc', 'hr': 'Unidentifikacija područja povezanih s privatnošću u poslovima', 'de': 'Entkennung datenschutzbezogener Entitäten in Stellenausschreibungen', 'bg': 'Деидентификация на субекти, свързани с неприкосновеността на личния живот, при обяви за работа', 'nl': 'De-identificatie van privacygerelateerde entiteiten in vacatures', 'da': 'Afidentifikation af privatlivsrelaterede enheder i jobopslag', 'fa': 'بازشناسایی شرکتهای مخصوصی در پست کارها', 'tr': 'Görev Göçirmeklerinde gizli zatlaryň habaryny tapylmady', 'af': 'Onbekende identifikasie van Privateit-verwante Entiteit in Werkspos', 'ko': '작업 공고에서 프라이버시 관련 실체의 표식을 취소하다', 'sq': 'De-identification of Privacy-related Entities in Job Postings', 'sw': 'Kutambuliwa kwa upande mwingine katika Makala ya Job yanayohusiana na faragha', 'id': 'Desidentifikasi Entitas Berkait Privasi dalam Posting Kerja', 'am': "ዶሴ `%s'ን ማስፈጠር አልተቻለም፦ %s", 'az': 'İş göndərmələrində gizli olaraq əlaqə edilən Entitələrin dəyişdirməsi', 'bn': 'কাজের পোস্টের মধ্যে প্রাইভেট সংক্রান্ত এন্টিটির স্বয়ংক্রিয়ভাবে পরিচিতি', 'cs': 'Zrušení identifikace subjektů souvisejících s ochranou osobních údajů v pracovních nabídkách', 'hy': 'De-identification of Privacy-related Entities in Job Postings', 'ca': 'De-identification of Privacy-related Entities in Job Postings', 'fi': 'Yksityisyyteen liittyvien yksiköiden tunnistaminen työpaikkailmoituksissa', 'et': 'Privaatsusega seotud üksuste identifitseerimise kustutamine töökohakorraldustes', 'bs': 'Deidentifikacija podataka povezanih sa privatnošću u poslovima', 'jv': 'Delokan Entité sing dibenadagaké pribadi kanggo Kemerdekaan Job', 'he': 'ניתוח זיהוי של יחידות קשורות לפרטיות בעמדות עבודה', 'sk': 'Odprava identifikacije subjektov, povezanih z zasebnostjo, pri objavah delovnih mest', 'ha': '@ action', 'bo': 'ལས་ཀ་གནས་ཡུལ་ནང་གི་མི་སྒེར་དབང་དང་འབྲེལ་བའི་རྣམ་གྲངས་མེད་པ'}
{'en': 'De-identification is the task of detecting privacy-related entities in text, such as person names, emails and contact data. It has been well-studied within the medical domain. The need for de-identification technology is increasing, as privacy-preserving data handling is in high demand in many domains. In this paper, we focus on job postings. We present JobStack, a new corpus for de-identification of personal data in job vacancies on Stackoverflow. We introduce baselines, comparing Long-Short Term Memory (LSTM) and Transformer models. To improve these baselines, we experiment with BERT representations, and distantly related auxiliary data via multi-task learning. Our results show that auxiliary data helps to improve de-identification performance. While BERT representations improve performance, surprisingly vanilla BERT turned out to be more effective than BERT trained on Stackoverflow-related data.', 'fr': "L'anonymisation est la tâche qui consiste à détecter les entités liées à la confidentialité dans le texte, telles que les noms de personnes, les e-mails et les données de contact. Il a fait l'objet de nombreuses études dans le domaine médical. Le besoin de technologies d'anonymisation augmente, car le traitement des données préservant la confidentialité est très demandé dans de nombreux domaines. Dans cet article, nous nous concentrons sur les offres d'emploi. Nous présentons JobStack, un nouveau corpus de désidentification des données personnelles dans les offres d'emploi sur Stackoverflow. Nous introduisons des lignes de base, en comparant les modèles de mémoire longue et courte durée (LSTM) et de transformateur. Pour améliorer ces lignes de base, nous expérimentons des représentations BERT et des données auxiliaires distantes via l'apprentissage multitâche. Nos résultats montrent que les données auxiliaires contribuent à améliorer les performances de désidentification. Alors que les représentations BERT améliorent les performances, le BERT «\xa0simple\xa0» s'est avéré plus efficace que le BERT formé sur des données liées à StackOverflow.", 'es': 'La desidentificación es la tarea de detectar entidades relacionadas con la privacidad en el texto, como nombres de personas, correos electrónicos y datos de contacto. Se ha estudiado bien en el ámbito médico. La necesidad de tecnología de desidentificación está aumentando, ya que el manejo de datos que preserve la privacidad tiene una gran demanda en muchos dominios. En este artículo, nos centramos en las ofertas de trabajo. Presentamos JobStack, un nuevo corpus para la desidentificación de datos personales en puestos vacantes en Stackoverflow. Presentamos líneas de base, comparando modelos de memoria a corto plazo (LSTM) y transformadores. Para mejorar estas líneas de base, experimentamos con representaciones BERT y datos auxiliares relacionados a distancia mediante el aprendizaje multitarea. Nuestros resultados muestran que los datos auxiliares ayudan a mejorar el rendimiento de la desidentificación. Si bien las representaciones de BERT mejoran el rendimiento, el BERT sorprendentemente «básico» resultó ser más eficaz que el BERT entrenado en datos relacionados con StackOverflow.', 'ar': 'إزالة الهوية هي مهمة الكشف عن الكيانات المتعلقة بالخصوصية في النص ، مثل أسماء الأشخاص ورسائل البريد الإلكتروني وبيانات الاتصال. لقد تمت دراستها جيدًا في المجال الطبي. تتزايد الحاجة إلى تقنية إزالة الهوية ، حيث إن معالجة البيانات التي تحافظ على الخصوصية مطلوبة بشدة في العديد من المجالات. في هذه الورقة ، نركز على إعلانات الوظائف. نقدم JobStack ، مجموعة جديدة لإلغاء تحديد البيانات الشخصية في الوظائف الشاغرة على Stackoverflow. نقدم خطوط الأساس ، ومقارنة الذاكرة طويلة المدى (LSTM) ونماذج المحولات. لتحسين هذه الخطوط الأساسية ، نجرب تمثيلات BERT والبيانات المساعدة ذات الصلة البعيدة عبر التعلم متعدد المهام. تظهر نتائجنا أن البيانات المساعدة تساعد على تحسين أداء إزالة الهوية. بينما تعمل تمثيلات BERT على تحسين الأداء ، من المدهش أن "الفانيليا" BERT تبين أنها أكثر فاعلية من BERT المدربة على البيانات المتعلقة بـ Stackoverflow.', 'pt': 'A desidentificação é a tarefa de detectar entidades relacionadas à privacidade no texto, como nomes de pessoas, e-mails e dados de contato. Tem sido bem estudado dentro do domínio médico. A necessidade de tecnologia de desidentificação está aumentando, pois o manuseio de dados que preserva a privacidade está em alta demanda em muitos domínios. Neste artigo, nos concentramos em anúncios de emprego. Apresentamos JobStack, um novo corpus para desidentificação de dados pessoais em vagas de emprego no Stackoverflow. Introduzimos linhas de base, comparando modelos de Long-Short Term Memory (LSTM) e Transformer. Para melhorar essas linhas de base, experimentamos representações BERT e dados auxiliares distantes por meio de aprendizado multitarefa. Nossos resultados mostram que os dados auxiliares ajudam a melhorar o desempenho da desidentificação. Embora as representações do BERT melhorem o desempenho, surpreendentemente o BERT “baunilha” acabou sendo mais eficaz do que o BERT treinado em dados relacionados ao Stackoverflow.', 'hi': 'डी-आइडेंटिफिकेशन पाठ में गोपनीयता से संबंधित संस्थाओं का पता लगाने का कार्य है, जैसे कि व्यक्ति के नाम, ईमेल और संपर्क डेटा। यह चिकित्सा डोमेन के भीतर अच्छी तरह से अध्ययन किया गया है। डी-आइडेंटिफिकेशन तकनीक की आवश्यकता बढ़ रही है, क्योंकि गोपनीयता-संरक्षण डेटा हैंडलिंग कई डोमेन में उच्च मांग में है। इस पेपर में, हम नौकरी पोस्टिंग पर ध्यान केंद्रित करते हैं। हम JobStack, Stackoverflow पर नौकरी रिक्तियों में व्यक्तिगत डेटा की डी-पहचान के लिए एक नया कॉर्पस प्रस्तुत करते हैं। हम बेसलाइन पेश करते हैं, लंबी अवधि की मेमोरी (एलएसटीएम) और ट्रांसफॉर्मर मॉडल की तुलना करते हैं। इन आधार रेखाओं को बेहतर बनाने के लिए, हम BERT प्रतिनिधित्व के साथ प्रयोग करते हैं, और बहु-कार्य सीखने के माध्यम से दूर से संबंधित सहायक डेटा। हमारे परिणाम बताते हैं कि सहायक डेटा डी-आइडेंटिफिकेशन प्रदर्शन को बेहतर बनाने में मदद करता है। जबकि BERT प्रतिनिधित्व प्रदर्शन में सुधार करते हैं, आश्चर्यजनक रूप से "वेनिला" BERT Stackoverflow से संबंधित डेटा पर प्रशिक्षित BERT की तुलना में अधिक प्रभावी हो गया।', 'ru': 'Деидентификация - это задача обнаружения связанных с конфиденциальностью объектов в тексте, таких как имена лиц, электронная почта и контактные данные. Она хорошо изучена в области медицины. Потребность в технологии деидентификации возрастает, поскольку во многих областях обработка данных, обеспечивающая конфиденциальность, пользуется большим спросом. В этой статье мы сосредоточимся на вакансиях. Мы представляем JobStack, новый корпус для деидентификации персональных данных в вакансиях на Stackoverflow. Мы вводим базовые линии, сравнивая модели с длительной короткой терминальной памятью (LSTM) и трансформаторами. Чтобы улучшить эти исходные данные, мы экспериментируем с представлениями BERT и удаленно связанными вспомогательными данными с помощью многозадачного обучения. Наши результаты показывают, что вспомогательные данные помогают улучшить эффективность деидентификации. В то время как представления BERT улучшают производительность, удивительно «ванильный» BERT оказался более эффективным, чем BERT, обученный данным, связанным со стековым потоком.', 'ja': '識別解除は、個人名、電子メール、連絡先データなど、テキストでプライバシーに関連するエンティティを検出する作業です。医療領域内で十分に研究されています。プライバシー保護データの取り扱いは多くのドメインで高い需要があるため、識別解除技術の必要性が高まっています。本稿では求人情報を中心に紹介する。私たちは、Stackoverflow上の求人情報の個人データの識別を解除するための新しいコーパスであるJobStackを提示します。LSTM （ Long - Short Term Memory ）モデルとトランスフォーマーモデルを比較して、ベースラインを紹介します。これらのベースラインを改善するために、私たちはBERT表現を実験し、マルチタスク学習を通じて遠隔に関連する補助データを実験しました。私たちの結果は、補助データが識別解除のパフォーマンスを向上させるのに役立つことを示しています。BERTの表現はパフォーマンスを向上させますが、驚くべきことに「バニラ」のBERTは、スタックオーバーフロー関連データで訓練されたBERTよりも効果的であることが判明しました。', 'zh': '去识别化者,检文本之私体也,如人名,电子邮件联系人数。 在医学领域得很好研究。 识别化术之求方增,以众领之私护数据处理之求高也。 本文中,将重点招聘启事。 我们介绍了JobStack,这是一个新的语料库,用于Stackoverflow上对职位缺中的人数去标识处理。 我们介绍了基线,较了长期记忆(LSTM)和变压器模样。 改善基线,试于多任务学BERT示远辅数也。 吾之的结果表明,佐数有助于崇识。 虽BERT示性能,而令人讶之,"香草"BERT比于Stackoverflow相关数BERT之效也。', 'ga': 'Is éard is dí-aithint ann ná aonáin a bhaineann le príobháideachas a bhrath i dtéacs, amhail ainmneacha daoine, ríomhphoist agus sonraí teagmhála. Tá dea-staidéar déanta air laistigh den réimse leighis. Tá méadú ag teacht ar an ngá atá le teicneolaíocht dí-aitheantais, mar go bhfuil éileamh ard ar láimhseáil sonraí a chaomhnú príobháideachta i go leor réimsí. Sa pháipéar seo, dírímid ar phostálacha poist. Cuirimid JobStack i láthair, corpas nua chun sonraí pearsanta i bhfolúntais a dhí-aithint ar Stackoverflow. Cuirimid bonnlínte isteach, ag déanamh comparáide idir samhlacha Cuimhne Fadtéarmach Gearrthéarmach (LSTM) agus Trasfhoirmeoirí. Chun na bunlínte seo a fheabhsú, déanaimid triail le huiríll BERT, agus sonraí cúnta a bhaineann i bhfad i gcéin trí fhoghlaim ilthasc. Léiríonn ár dtorthaí go gcuidíonn sonraí cúnta le feabhas a chur ar fheidhmíocht dí-aitheantais. Cé go bhfeabhsaítear feidhmíocht uiríll CRET, is ionadh go raibh “vanilla” BERT níos éifeachtaí ná mar a fuair BERT oiliúint ar shonraí a bhaineann le Cruach-thar-sreabhadh.', 'hu': 'Az azonosítás megszüntetése az adatvédelemmel kapcsolatos szervezetek szövegben történő felismerésének feladata, mint például személyek nevei, e-mailjei és kapcsolattartási adatai. Jól tanulmányozták az orvosi területen. Az azonosítástalanítási technológia iránti igény növekszik, mivel számos területen nagy igény van a magánélet megőrzését célzó adatkezelésre. Ebben az újságban az álláshirdetésekre összpontosítunk. Bemutatjuk a JobStack-et, egy új korpuszt, amely a személyes adatok azonosítására szolgál a Stackoverflow álláshelyeiben. Bemutatjuk az alapvonalakat, összehasonlítva a Long-Short Term Memory (LSTM) és a Transformer modelleket. Ezen alapvető vonalak javítása érdekében BERT reprezentációkkal és távolról kapcsolódó kiegészítő adatokkal kísérletezünk többfeladatos tanulással. Eredményeink azt mutatják, hogy a kiegészítő adatok segítenek javítani az azonosítási teljesítményt. Míg a BERT reprezentációk javítják a teljesítményt, meglepően a "vanília" BERT hatékonyabbnak bizonyult, mint a BERT a Stackoverflow-hoz kapcsolódó adatokra képzett.', 'ka': "განიდენტიფიკაცია არის ტექსტში პრივიაციის შესახებ ინტერტიების განახლება, როგორც ადამიანის სახელი, ელფოსტა და კონტაქტის მონაცემები. მას მედიცინური დემონიში კარგი სწავლია. განიდენტიფიკაციის ტექნოლოგიის უფრო მეტია, რადგან პრივიაციის შესახებ მონაცემების შესახებ უფრო მეტი მოთხოვრებაში. ამ დოკუნტში ჩვენ დავამუშავებთ სამუშაო წერტილებზე. ჩვენ ვამხსენებთ JobStack, ახალი კორპუსი პირადი მონაცემების განიცნობისთვის სამუშაო სამუშაო სამუშაო სამუშაო სტაკოoverflow. ჩვენ დავიყენებთ ბაზის ხაზები, მარტივი სიმხოლოების მეხსიერება (LSTM) და ტრანფორმაციის მოდელების შედგენება. ამ ბაზი ხაზების გასაკეთებლად, ჩვენ ექსპერიმენტით BERT-ის გამოსახულებებით, და მრავალური მონაცემების გამოსახულებლად დახმარებული მონაცემები. ჩვენი წარმოდგენები გამოჩვენებენ, რომ დახმარებული მონაცემები დახმარებს განიდენტიფიკაციის გამოსახულებას. მაგრამ BERT-ს გამოსახულებები უფრო უფრო უფრო უფრო ეფექტიურია, ამიტომ განსხვავებული 'vanilla' BERT იყო უფრო ეფექტიურია, ვიდრე BERT-ს, რომელსაც სტან", 'it': 'La deidentificazione è il compito di rilevare entità legate alla privacy nel testo, come nomi di persone, e-mail e dati di contatto. È stato ben studiato nell\'ambito medico. La necessità di tecnologie di de-identificazione è in aumento, poiché il trattamento dei dati che preservano la privacy è molto richiesto in molti settori. In questo articolo, ci concentriamo sugli annunci di lavoro. Vi presentiamo JobStack, un nuovo corpus per la de-identificazione dei dati personali nelle offerte di lavoro su Stackoverflow. Introducemo le linee di base, confrontando i modelli LSTM (Long-Short Term Memory) e Transformer. Per migliorare queste linee di base, sperimentiamo con rappresentazioni BERT e dati ausiliari correlati a distanza tramite l\'apprendimento multi-task. I nostri risultati mostrano che i dati ausiliari aiutano a migliorare le prestazioni di de-identificazione. Mentre le rappresentazioni BERT migliorano le prestazioni, sorprendentemente BERT "vanilla" si è rivelato più efficace di quanto BERT abbia addestrato sui dati relativi allo Stackoverflow.', 'lt': 'Identifikavimo panaikinimas yra užduotis nustatyti su privatumu susijusius subjektus tekste, pavyzdžiui, asmenų vardus, el. paštus ir kontaktinius duomenis. Jis buvo gerai ištirtas medicinos srityje. Reikalavimas panaikinti identifikavimo technologiją didėja, nes daugelyje sričių duomenų tvarkymas užtikrina privatumą. Šiame dokumente daugiausia dėmesio skiriame darbo komandiravimui. Mes pristatome JobStack, naują asmens duomenų identifikavimo panaikinimo korpusą laisvose Stackoverflow darbo vietose. Įdiegiame bazines linijas, palygindami ilgalaikį trumpalaikį prisiminimą (LSTM) ir transformatorių modelius. Siekiant pagerinti šias bazines vertes, eksperimentuojame su BERT atstovavimais ir toli susijusiais papildomais duomenimis pasitelkiant daugiafunkcinį mokymąsi. Mūsų rezultatai rodo, kad papildomi duomenys padeda gerinti identifikavimo panaikinimo veiksmingumą. Nors BERT atstovai pagerina veiklos rezultatus, staiga „vanilės“ BERT pasirodė veiksmingesnis nei BERT, apmokytas su Stackoverflow susijusiais duomenimis.', 'kk': "Қосымша идентификация - мәтіндегі жеке мәтіндегі нысандарды, мысалы адамдар, эл. пошта және контакт деректерін анықтау тапсырмасы. Бұл медицина доменінде жақсы оқылған. Деидентификациялау технологиясын өзгерту керек, өйткені жеке сақтау деректерді қамтамасыз ету көптеген домендерде жоғары талап етеді. Бұл қағазда жұмыс жіберуде көңіл береміз. Stack, Stackoverflow жұмысындағы жеке деректерді қайта анықтау үшін жаңа корпус береді. Біз негізгі сызықтарды, ұзын қысқа уақыт жады (LSTM) және түрлендіруші үлгілерін салыстырып көрдік. Бұл негізгі сызықтарды жақсарту үшін біз BERT келтірімдерімен тәжірибе және көп тапсырмаларды оқыту арқылы көмектесу мәліметтерімен тәжірибе жасаймы Біздің нәтижелеріміз көмектесетін деректеріміз идентификациялау мүмкіндігін өзгертуге көмектеседі. BERT деңгейлері жылдамдығын жақсартып жатқанда, 'vanilla' BERT Stackoverflow деген деректеріне оқылған BERT дегеннен артық болды.", 'mk': 'De-identification is the task of detecting privacy-related entities in text, such as person names, emails and contact data.  Беше добро проучена во медицинската област. Потребноста за технологија за деидентификација се зголемува, бидејќи управувањето со податоците за зачувување на приватноста е во висока побарувачка во многу домени. Во овој весник, се фокусираме на поставувањата на работа. Го претставуваме Џобстак, новиот корпус за деидентификација на личните податоци на слободните места на Стаковерлм. Внесуваме основни линии, споредувајќи ги долгорочните мемории (ЛСТМ) и трансформските модели. За да ги подобриме овие основни линии, експериментираме со претставувања на BERT, и далеку поврзани помошни податоци преку мултизадачно учење. Нашите резултати покажуваат дека помошните податоци помагаат во подобрувањето на деидентификацијата. И покрај тоа што претставувањата на БЕРТ ја подобруваат перформансата, изненадувачки „ванила“ БЕРТ се покажа дека е поефикасна отколку БЕРТ обучена за податоци поврзани со Стаковерфлекс.', 'ms': "Nyahidentifikasi adalah tugas untuk mengesan entiti berkaitan privasi dalam teks, seperti nama orang, e-mel dan data kenalan. Ia telah dipelajari dengan baik dalam bidang perubatan. Perlukan teknologi penyahidentifikasi meningkat, kerana pengendalian data penyimpanan privasi dalam permintaan tinggi dalam banyak domain. Dalam kertas ini, kita fokus pada posting kerja. Kami perkenalkan JobStack, korpus baru untuk mengesahkan pengenalan data peribadi dalam pekerjaan kosong di Stackoverflow. Kami memperkenalkan garis dasar, membandingkan ingatan jangka pendek panjang (LSTM) dan model Transformer. Untuk memperbaiki garis dasar ini, kami eksperimen dengan perwakilan BERT, dan data bantuan yang berkaitan jauh melalui pembelajaran multi-tugas. Hasil kami menunjukkan bahawa data bantuan membantu untuk meningkatkan prestasi pengenalan-nyah. Sementara mewakili BERT meningkatkan prestasi, mengejutkan 'vanilla' BERT ternyata lebih berkesan daripada BERT dilatih pada data berkaitan Stackoverflow.", 'ml': "വ്യക്തിക പേരുകള്\u200d, ഈമെയിലുകള്\u200d, വിലാസവിവരങ്ങള്\u200d എന്നിവയുടെ പേരുകള്\u200d പോലെ സ്വകാര്യ ബന്ധപ്പെട്ട വസ്തുക്കള്\u200d കണ്ടുപിട മെഡിക്കല്\u200d ഡോമെയിനില്\u200d ഇത് നന്നായി പഠിച്ചിരിക്കുന്നു. ഡി തിരിച്ചറിയാനുള്ള സാങ്കേതികവിദ്യയുടെ ആവശ്യം കൂടുതല്\u200d വര്\u200dദ്ധിപ്പിക്കുന്നു. സ്വകാര്യ ഡേറ്റാ സൂക്ഷിക് ഈ പത്രത്തില്\u200d ഞങ്ങള്\u200d ജോലി പോസ്റ്റുകളില്\u200d ശ്രദ്ധിക്കുന്നു. ജോബ് സ്റ്റാക്ക്, സ്റ്റാക്ക്\u200cഫ്ലോയിലെ ജോലിയുടെ വ്യക്തിപരമായ വിവരങ്ങള്\u200d തിരിച്ചറിയാന്\u200d പുതിയ കോര്\u200dപ്പു ഞങ്ങള്\u200d ബേസ്ലൈനുകളെ പരിചയപ്പെടുത്തുന്നു, ലോങ്ങ് ചെറുതായ ടെര്\u200dമ്മ് മെമ്മറി (LSTM) പിന്നെ ട്രാന്\u200dസ്ഫോര്\u200dമാ ഈ അടിസ്ഥാനങ്ങള്\u200d മെച്ചപ്പെടുത്താന്\u200d വേണ്ടി, ബെര്\u200dട്ടി പ്രതിനിധികളുമായി നമ്മള്\u200d പരീക്ഷിക്കുന്നു. അകലെയുള്ള വിവരങ്ങള നമ്മുടെ ഫലങ്ങള്\u200d കാണിച്ചു കൊണ്ടിരിക്കുന്നത് കൂടുതല്\u200d വിവരങ്ങള്\u200d തിരിച്ചറിയാന്\u200d സഹായിക്കുന്നു. ബെര്\u200dട്ടിന്റെ പ്രതിനിധികള്\u200d സ്റ്റാക്ക്\u200cഫ്ലോസ് പരിശീലന വിവരങ്ങളില്\u200d ബെര്\u200dട്ടിനെക്കാള്\u200d അത്ഭുതകരമായ 'വാനില്ല' പ്രദര്\u200dശനം മ", 'mt': "Id-diidentifikazzjoni hija l-kompitu li jinstabu entitajiet relatati mal-privatezza fit-test, bħall-ismijiet tal-persuni, l-e-mails u d-dejta tal-kuntatt. Ġie studjat tajjeb fil-qasam mediku. Il-ħtieġa għal teknoloġija ta’ diidentifikazzjoni qed tiżdied, peress li l-immaniġġjar tad-dejta li tippreserva l-privatezza huwa f’domanda għolja f’ħafna oqsma. F’dan id-dokument, niffokaw fuq l-istazzjonar tax-xogħol. Aħna nippreżentaw JobStack, korpus ġdid għad-diżidentifikazzjoni tad-dejta personali f’postijiet tax-xogħol battala fuq Stackoverflow. Aħna nintroduċu linji bażi, li jqabblu l-Memorja fuq terminu twil u qasir (LSTM) u l-mudelli tat-Transformer. To improve these baselines, we experiment with BERT representations, and distantly related auxiliary data via multi-task learning.  Ir-riżultati tagħna juru li d-dejta awżiljarja tgħin biex titjieb il-prestazzjoni tad-diidentifikazzjoni. Filwaqt li r-rappreżentazzjonijiet tal-BERT itejbu l-prestazzjoni, b'mod sorprendenti l-'vanilla' BERT deher li kien aktar effettiv minn BERT imħarreġ fuq dejta relatata mal-Stackoverflow.", 'no': 'Identifikasjon er oppgåva for å finna personnamn, e-postar og kontaktdata frå personverktøyrelaterte entiteter i tekst. Det er godt studiert i medisinske domene. Nødvendighet for å identifisera teknologien økar, sidan datahandtering for privat er i høg etterspørsel i mange domene. I denne papiret fokuserer vi på jobbposting. Stack, ei ny korpus for å identifisera personlege data i jobbvacancies on Stackoverflow. Vi introduserer grunnlinjer, samanliknar lang- kort- minne (LSTM) og transformeringsmodeller. For å forbetra desse grunnlinjene, eksperimenterer vi med BERT-representasjonar, og langt relaterte hjelpelinjer via fleire oppgåver-læring. Resultatet våre viser at hjelpedata hjelper til å forbetra identifiseringsfunksjonen. Mens BERT-representasjonar forbetrar utviklinga, så overraska «vanilla» BERT viste seg til å vera meir effektiv enn BERT-trent på Stackoverflow-relaterte data.', 'ro': 'Deidentificarea este sarcina de a detecta entitățile legate de confidențialitate în text, cum ar fi numele persoanelor, e-mailurile și datele de contact. A fost bine studiat în domeniul medical. Nevoia de tehnologie de dezidentificare este în creștere, deoarece prelucrarea datelor care protejează confidențialitatea este foarte solicitată în multe domenii. În această lucrare, ne concentrăm pe postările de locuri de muncă. Vă prezentăm JobStack, un nou corpus de dezidentificare a datelor cu caracter personal în posturile vacante pe Stackoverflow. Introducem liniile de bază, comparând modelele de memorie pe termen lung-scurt (LSTM) și Transformer. Pentru a îmbunătăți aceste linii de referință, experimentăm cu reprezentări BERT și date auxiliare legate la distanță prin învățarea multi-task. Rezultatele noastre arată că datele auxiliare ajută la îmbunătățirea performanței de identificare. În timp ce reprezentările BERT îmbunătățesc performanța, în mod surprinzător, BERT "vanilie" s-a dovedit a fi mai eficient decât BERT instruit cu privire la datele legate de Stackoverflow.', 'el': 'Η απομαγνητοποίηση είναι το καθήκον της ανίχνευσης οντοτήτων που σχετίζονται με το απόρρητο στο κείμενο, όπως ονόματα προσώπων, μηνύματα ηλεκτρονικού ταχυδρομείου και δεδομένα επικοινωνίας. Έχει μελετηθεί καλά στον ιατρικό τομέα. Η ανάγκη για τεχνολογία αφαίρεσης ταυτότητας αυξάνεται, καθώς ο χειρισμός δεδομένων διατήρησης της ιδιωτικής ζωής είναι σε μεγάλη ζήτηση σε πολλούς τομείς. Σε αυτή την εργασία, εστιάζουμε στις δημοσιεύσεις εργασίας. Παρουσιάζουμε ένα νέο σώμα για την απομαγνητοποίηση προσωπικών δεδομένων σε κενές θέσεις εργασίας στο Stackoverflow. Παρουσιάζουμε γραμμές βάσης, συγκρίνοντας μοντέλα Μακροπρόθεσμης Μνήμης (και Μετασχηματιστών). Για να βελτιώσουμε αυτές τις γραμμές βάσης, πειραματιζόμαστε με αναπαραστάσεις και απομακρυσμένα συναφή βοηθητικά δεδομένα μέσω μάθησης πολλαπλών εργασιών. Τα αποτελέσματά μας δείχνουν ότι τα βοηθητικά δεδομένα βοηθούν στη βελτίωση της απόδοσης αφαίρεσης ταυτότητας. Ενώ οι αναπαραστάσεις βελτιώνουν την απόδοση, παραδόξως το "βανίλια" αποδείχθηκε πιο αποτελεσματικό από ό, τι ο BERT εκπαιδευμένος σε δεδομένα που σχετίζονται με το Stackoverflow.', 'pl': 'De-identyfikacja to zadanie wykrywania w tekście podmiotów związanych z prywatnością, takich jak nazwiska osób, e-maile i dane kontaktowe. Został dobrze zbadany w dziedzinie medycznej. Zapotrzebowanie na technologię dezidentyfikacji rośnie, ponieważ w wielu dziedzinach zachowująca prywatność przetwarzanie danych jest dużym zapotrzebowaniem. W tym artykule skupiamy się na ogłoszeniach pracy. Przedstawiamy JobStack, nowy korpus do dezidentyfikacji danych osobowych w ofertach pracy na Stackoverflow. Wprowadzamy linie bazowe, porównując modele pamięci długoterminowej (LSTM) i transformatora. Aby poprawić te linie podstawowe, eksperymentujemy z reprezentacjami BERT i oddalonymi danymi pomocniczymi poprzez wielozadaniowe uczenie się. Nasze wyniki pokazują, że dane pomocnicze pomagają poprawić wydajność dezidentyfikacji. Podczas gdy reprezentacje BERT poprawiają wydajność, zaskakująco "waniliowy" BERT okazał się bardziej skuteczny niż BERT przeszkolony na danych związanych z Stackoverflow.', 'sr': "Deidentifikacija je zadatak otkrivanja entitata povezanih sa privatnošću u tekstu, poput imena osoba, e-maila i podataka o kontaktu. Bilo je dobro proučeno u medicinskom domenu. Potreba za deidentifikacijskim tehnologijama se povećava, jer je podrška podataka koji očuvaju privatnost u velikoj potrebi u mnogim domenama. U ovom papiru, fokusiramo se na poslove. Predstavljamo JobStack, novi korpus za dezidentifikaciju ličnih podataka u praznim mjestima na Stackoverflow. Predstavljamo osnovne linije, uspoređujući dugoročnu uspomenu (LSTM) i modele transformera. Da bismo poboljšali ove osnovne linije, eksperimentirali smo sa predstavljanjem BERT-a i daleko povezanim pomoćnim podacima putem multi task učenja. Naši rezultati pokazuju da pomoæni podaci pomažu da poboljšaju deidentifikaciju. Dok predstavljanja BERT poboljšavaju učinkovitost, iznenađujuće je da je 'vanilla' BERT efikasniji od BERT obučenih na podacima povezanim sa Stackoverflow.", 'mn': "Хүмүүсийн нэр, эл.шуудан, харилцааны өгөгдлийг мэдэх мэт хувийн амьдралын холбоотой байгууллагуудыг олох ажил юм. Эмнэлгийн хэсэгт сайн судалсан. Хувийн амьдралын хамгаалах өгөгдлийн ажиллагааг олон хэсэгт хүлээн зөвшөөрүүлэх технологи нь нэмэгдэж байна. Энэ цаасан дээр бид ажлын захиргаанд анхаарлаа хандуулдаг. Бид JobStack-г Stackoverflow-ын ажлын амралтын мэдээллийг өөрчлөгдөх шинэ корпус болгож байна. Бид урт богино хугацааны санамж (LSTM) болон шилжүүлэгч загварыг харьцуулдаг суурь шулуунуудыг танилцуулдаг. Эдгээр суурь шулуунуудыг сайжруулахын тулд бид БЕРТ илтгэлээр туршиж, олон ажлын сургалтын аргаар хол холбоотой тусламжтай мэдээллийг туршиж байна. Манай үр дүнд тусламжтай өгөгдлийн мэдээллийг өөрчлөлтийн үйл ажиллагааг сайжруулахад тусалдаг. BERT-ын илтгэл хөгжлийн үйл ажиллагааг сайжруулахад гайхалтай нь 'vanilla' BERT нь Stackoverflow-тэй хамааралтай мэдээллээс илүү үр дүнтэй болсон юм.", 'si': "පුද්ගලිකත්වය සම්බන්ධතාවක් පරීක්ෂණයේ පුද්ගලිකත් සම්බන්ධතාවක් පරීක්ෂණය වෙනුවෙන් වැඩක්, පුද්ගල ඒක හොඳින් අධ්\u200dයානය කරලා තියෙන්නේ. පුද්ගලික විශ්වාස කරන්න පුළුවන් තාක්ෂණික විශ්වාස කරන්න, පුද්ගලික විශ්වාස කරන්න දත්ත ප්\u200dරයෝජනය විසින මේ පත්තරේ අපි වැඩක් පොස්ට් එක්ක බලන්න. අපි ජෝබ් ස්ටැක් වෙනුවෙන්, අලුත් කෝර්පුස් එකක් ස්ටැක්වර්ෆ්ලෝව් විදියට පුද්ගලික දත්ත අඳුරණය අපි මූලික පැත්තක් පෙනුම් කරනවා, ලොකු කොටි වාර්තාවක් මතකය (LSTM) සහ වෙනස් කරනවා මෝඩේල්ස්. මේ මූලික ප්\u200dරධානය වැඩ කරන්න, අපි BERT ප්\u200dරධානය සඳහා පරීක්ෂණය කරනවා, සහ දුරටත් සම්බන්ධ වෙන්න ප්\u200dරධානය සඳහා  අපේ ප්\u200dරතිචාරය පෙන්වන්නේ උදව්වක් දත්ත පුළුවන් වෙන්න පුළුවන් වෙන්න. BERT ප්\u200dරතිනිධාන විශ්වාස කරනවා නමුත්, පුදුම විශ්වාසයෙන් 'vanilla' BERT ප්\u200dරතිචාරයෙන් ස්ටැක්වර්ෆලෝව් සම්බන්ධ", 'so': 'Shaqada aqoonsiga waxaa ku jira macluumaadka la xiriira arrimaha gaarka loo leeyahay, tusaale ahaan magaca qofka, emaylka iyo macluumaadka xiriirka. Guriga caafimaadka waxaa lagu bartay si wanaagsan. U baahnaanshaha teknolojiga aqoonsiga waa sii korodhsan, sababtoo ah daryeelka macluumaadka gaarka loo leeyahay waxay ka baahan tahay meelo badan. Warqadan waxaynu ku kalsoonaannaa boostada shaqada. We present JobStack, a new corpus for de-identification of personal data in job vacancies on Stackoverflow.  Waxaannu soo bandhignaa saldhigyada, oo isbarbardhiga xasuusta waqtiga dheer (LSTM) iyo samooyinka bedelka. Si loo hagaajiyo heerarkaas, waxaynu ku jirrabnaa wakiilayaasha BERT, iyo macluumaad dheeraad ah oo la xiriira waxbarashada shaqo badan. Abaalkayaga waxay muuqataa in macluumaad dheeraad ah ay caawinaysaa in horumariyo tababarka aqoonsiga. Inta lagu arago BERT waxyaabaha la yaab leh, waxaa looga muuqday inay ka shaqeeyo faa’iido ka badan tahay waxbarashada lagu baray macluumaadka la xiriira Stackoverflow.', 'sv': 'Avidentifiering är uppgiften att upptäcka integritetsrelaterade enheter i text, såsom personnamn, e-post och kontaktuppgifter. Det har studerats väl inom det medicinska området. Behovet av avidentifieringsteknik ökar, eftersom integritetsskyddande datahantering efterfrågas på många områden. I den här uppsatsen fokuserar vi på platsannonser. Vi presenterar JobStack, en ny korpus för avidentifiering av personuppgifter i lediga jobb på Stackoverflow. Vi introducerar baslinjer, jämför Long-Short Term Memory (LSTM) och Transformer modeller. För att förbättra dessa baslinjer experimenterar vi med BERT-representationer och fjärrrelaterade hjälpdata via multi-task learning. Våra resultat visar att hjälpdata bidrar till att förbättra avidentifieringsprestandan. Medan BERT-representationer förbättrar prestandan, visade BERT sig förvånansvärt "vanilj" vara effektivare än BERT utbildad på Stackoverflödesrelaterad data.', 'ta': "தனிப்பட்ட பொருள்களை உரையில் கண்டுபிடிப்பதற்கான செயல், நபர் பெயர்கள், மின்னஞ்சல்கள் மற்றும் தொடர்பு தரவுகள். மருத்துவ களத்தில் அதை நன்றாக படிக்கப்பட்டது. தெரியும் தொழில்நுட்பத்தின் தேவை அதிகரிக்கிறது, ஏனெனில் தனிப்பட்ட தரவு கையாளும் பல இடங்களில் அதிக தேவையில் உள்ளது. இந்த காகிதத்தில், நாம் வேலை நிறுவனங்களை கவனம் செலுத்துகிறோம். நாம் ஜாப்ஸ்டாக், ஒரு புதிய குறிப்பை காண்பிக்கிறோம் ஸ்டாக்ஃபோல் வேலை வெற்றியடையும் தனிப்பட்ட தரவுகளை தி நாம் அடிப்படைகோடுகளை அறிமுகப்படுத்துகிறோம், நீண்ட கால நேரம் நினைவகம் (LSTM) மற்றும் மாற்றி மாதிரிகளை ஒப்பிட இந்த அடிப்படை கோடுகளை மேம்படுத்த, பிரெட் குறிப்புகளுடன் நாம் பரிசோதிக்கிறோம், மற்றும் தொலைவான பல பணி கற்றுக்கொள்ளு நம்முடைய முடிவுகள் காட்டுகிறது அந்த அடையாளம் செயல்பாட்டை மேம்படுத்த உதவுகிறது. பிரெட் குறிப்புகள் செயல்பாட்டை மேம்படுத்தும் போது, ஆச்சரியமாக 'வானில்லா' பெர்ட் பெர்ட் பிரெட் ஸ்டாக்பர்ப்லோ தொடர்பு", 'ur': "فائدہ شناسایی ایک ٹیکسٹ میں خصوصی رابطہ داری واحدوں کی شناسایی کا کام ہے، جیسے شخصی نام، ایمیل اور کنٹ ڈیٹ. اسے پزشکی دامین میں بہترین تحقیق کی گئی ہے۔ غیر شناسایی ٹیکنالوجی کی ضرورت بڑھتی ہے، کیونکہ خصوصی حفاظت کرنے والی ڈاکنالوجی میں بہت سی دامنوں میں اچھی درخواست ہے. اس کاغذ میں ہم کام پوسٹینگ پر تمرکز کرتے ہیں۔ ہم JobStack کو پیش کرتے ہیں، Stackoverflow کے مطابق کاروبار واکنشیوں میں شخصی ڈیٹوں کی ناشناسایی کے لئے ایک نئی کورپوس. ہم بنسس لینڈ کو معلوم کرتے ہیں، لانگ کوٹر ٹر میمور (LSTM) اور ترنسفور موڈل کے مقایسہ کرتے ہیں. ان بنیاس لینوں کو بہتر کرنے کے لئے ہم BERT روشنی کے ذریعے آزمائش کرتے ہیں، اور بہت سے کام کی تعلیم کے ذریعے دور سے مددگار ڈیٹا کے ذریعے۔ ہمارے نتیجے دکھاتے ہیں کہ مددگار ڈیٹا ناشناسایی کامپیوتر کو بہتر کرنے کے لئے مدد کرتا ہے. While BERT representations improve performance, surprisingly 'vanilla' BERT turned out to be more effective than BERT trained on Stackoverflow-related data.", 'uz': 'Name It has been well-studied within the medical domain.  Shaxsiy maʼlumotni saqlash uchun foydalanish kerak, chunki shaxsiy maʼlumotni saqlash uchun ko\'pchilik davlatlarda juda katta talab qiladi. Bu qogʻozda, biz ishning postlariga foydalanamiz. Biz JobStack, Stackoverflow\'da shaxsiy maʼlumotni aniqlash uchun yangi korpus. Biz yozib qoʻyish xotira (LSTM) va Transformer modellarini qisqa tugmalar bilan birlashtiramiz. Bu asboblarni oshirish uchun biz BERT tashkilotlari bilan tajriba qilamiz, va bir necha vazifani o\'rganish orqali murakkab bog\'liq taʼminlovchi maʼlumotlar bilan bir necha vazifa o\'rganish orqali. Bizning natijalarimizni ko\'rsatadi, qo\'shimcha maʼlumotlar tasdiqlashni bajarishga yordam beradi. BERT tashkilotlari esa "vanillar" bajarishini yaxshi ko\'ra ko\'rsatadi, va "vanillar" BERT\'da Stackoverflow-bog\'liq maʼlumotga o\'rganishdan ko\'proq ishlaydi.', 'vi': "Nhận diện là nhiệm vụ phát hiện các thực thể liên quan đến riêng tư trong văn bản, như tên người, email và dữ liệu liên lạc. Nó đã được nghiên cứu kỹ trong lĩnh vực y học. Nhu cầu về công nghệ gỡ nhận dạng đang tăng lên, vì việc xử lý dữ liệu bảo vệ sự riêng tư rất yêu cầu trong nhiều lĩnh vực. Trong tờ giấy này, chúng tôi tập trung vào việc đăng tải. Chúng tôi giới thiệu JobStack, một tập đoàn mới để xác định các dữ liệu cá nhân trong việc bỏ trống ở chồng dãy Chúng tôi giới thiệu các nền, so sánh Ký Ức Dài Hạn (LSTM) và Các mô hình transformer. Để cải thiện bản căn cứ này, chúng tôi thử qua các biểu tượng của BERT, và các dữ liệu phụ liên quan xa lạ qua nhiều nhiệm vụ học. Kết quả của chúng tôi cho thấy dữ liệu phụ giúp cải thiện khả năng nhận diện. Trong khi trình diễn của BERT cải thiện hiệu quả, ngạc nhiên,'vani'BERT lại hóa ra hiệu quả hơn BERT được huấn luyện về dữ liệu về chồng quản.", 'bg': 'Деидентификацията е задачата за откриване на субекти, свързани с неприкосновеността на личния живот в текст, като имена на лица, имейли и данни за контакт. Той е добре проучен в областта на медицината. Необходимостта от технология за деидентификация нараства, тъй като обработката на данни за запазване на неприкосновеността на личния живот е силно търсена в много области. В тази статия се фокусираме върху обявяването на работни места. Представяме нов корпус за деидентификация на лични данни в свободните работни места в Стакоувърфлоу. Въвеждаме базови линии, сравнявайки моделите за дългосрочна краткосрочна памет и трансформатор. За да подобрим тези базови линии, експериментираме с представяне на BERT и отдалечени спомагателни данни чрез многофункционално обучение. Нашите резултати показват, че спомагателните данни спомагат за подобряване на ефективността при деидентификация. Докато представянето подобрява производителността, изненадващо "ваниловият" се оказа по-ефективен от обучения по данни, свързани със стакоувърфлоу.', 'nl': 'De-identificatie is de taak om privacygerelateerde entiteiten in tekst te detecteren, zoals namen van personen, e-mails en contactgegevens. Het is goed bestudeerd binnen het medische domein. De behoefte aan de-identificatietechnologie neemt toe, aangezien privacybehoud van gegevensverwerking in veel domeinen een grote vraag is. In dit artikel richten we ons op vacatures. We presenteren JobStack, een nieuw corpus voor de-identificatie van persoonsgegevens in vacatures op Stackoverflow. We introduceren basislijnen, waarbij we Long-Short Term Memory (LSTM) en Transformer modellen vergelijken. Om deze baselines te verbeteren, experimenteren we met BERT-representaties en op afstand gerelateerde hulpgegevens via multi-task learning. Onze resultaten tonen aan dat hulpgegevens helpen om de-identificatie prestaties te verbeteren. Hoewel BERT-representaties de prestaties verbeteren, bleek BERT verrassend effectiever te zijn dan BERT getraind op Stackoverflow-gerelateerde gegevens.', 'de': 'Bei der De-Identifikation handelt es sich um die Aufgabe, datenschutzbezogene Entitäten in Texten wie Personennamen, E-Mails und Kontaktdaten zu erkennen. Es wurde im medizinischen Bereich gut studiert. Der Bedarf an Entkennungstechnologie steigt, da datenschutzerhaltender Umgang in vielen Bereichen stark nachgefragt wird. In dieser Arbeit konzentrieren wir uns auf Stellenausschreibungen. Wir präsentieren JobStack, ein neues Korpus zur De-Identifikation von personenbezogenen Daten in Stellenangeboten auf Stackoverflow. Wir stellen Baselines vor und vergleichen Long-Short Term Memory (LSTM) und Transformer Modelle. Um diese Baselines zu verbessern, experimentieren wir mit BERT-Darstellungen und entfernten Hilfsdaten mittels Multi-Task Learning. Unsere Ergebnisse zeigen, dass Hilfsdaten helfen, die Entidentifizierungsleistung zu verbessern. Während BERT-Darstellungen die Leistung verbessern, erwies sich überraschenderweise "Vanille"-BERT als effektiver als BERT, das auf Stackoverflow-bezogenen Daten trainiert wurde.', 'da': 'Afidentifikation er opgaven med at registrere privatlivsrelaterede enheder i tekst, såsom personnavne, e-mails og kontaktdata. Det er blevet godt undersøgt inden for det medicinske område. Behovet for afidentifikationsteknologi er stigende, da databehandling, der beskytter privatlivets fred, er meget efterspurgt på mange områder. I denne artikel fokuserer vi på jobopslag. Vi præsenterer JobStack, et nyt korpus til afidentifikation af personoplysninger i ledige stillinger på Stackoverflow. Vi introducerer baselines, sammenligner Long-Short Term Memory (LSTM) og Transformer modeller. For at forbedre disse basislinjer eksperimenterer vi med BERT-repræsentationer og fjernrelaterede hjælpedata via multi-task learning. Vores resultater viser, at hjælpedata hjælper med at forbedre de-identifikations ydeevne. Mens BERT-repræsentationer forbedrer ydeevnen, viste BERT sig overraskende nok at være mere effektiv end BERT uddannet i Stackoverflow-relaterede data.', 'id': "Deidentifikasi adalah tugas untuk mendeteksi entitas berkaitan privasi dalam teks, seperti nama orang, email dan data kontak. Ini telah dipelajari dengan baik dalam bidang medis. Kebutuhan teknologi de-identifikasi meningkat, karena pengendalian data yang memelihara privasi dalam permintaan tinggi di banyak domain. Di koran ini, kita fokus pada posting pekerjaan. Kami memperkenalkan JobStack, korpus baru untuk mengidentifikasi data pribadi dalam pekerjaan kosong di Stackoverflow. Kami memperkenalkan garis dasar, membandingkan Memori jangka pendek panjang (LSTM) dan model Transformer. Untuk meningkatkan garis dasar ini, kami eksperimen dengan rappresentasi BERT, dan data bantuan yang berhubungan jauh melalui pembelajaran multi-tugas. Hasil kami menunjukkan bahwa data bantuan membantu memperbaiki prestasi de-identifikasi. Sementara perwakilan BERT meningkatkan prestasi, mengejutkan 'vanilla' BERT ternyata lebih efektif dari BERT dilatih pada data terkait Stackoverflow.", 'ko': "반신분 식별은 텍스트에서 프라이버시와 관련된 실체를 검출하는 작업이다. 예를 들어 사람 이름, 이메일과 연락처 데이터 등이다.그것은 의학 분야에서 매우 좋은 연구를 받았다.많은 분야에서 프라이버시를 보호하는 데이터 처리에 대한 수요가 점점 높아지면서 반식별 기술에 대한 수요도 증가하고 있다.본문에서 우리가 주목하는 것은 직위 공고이다.우리는 Stackoverflow에서 직위 공백의 개인 데이터를 식별하는 데 사용되는 새로운 자료 라이브러리 Job Stack을 제시했다.우리는 기선을 소개하고 장단시 기억(LSTM)과 변압기 모형을 비교했다.이러한 기선을 개선하기 위해 우리는 다중 임무 학습을 통해 버트가 표시한 원거리와 관련된 보조 데이터에 대해 실험을 진행했다.우리의 결과에 따르면 보조 데이터는 식별 성능을 향상시키는 데 도움이 된다고 한다.버트는 법이 성능을 향상시켰지만 놀랍게도 Stackoverflow 관련 데이터에서 훈련한 버트보다'보통'버트가 더 효과적이었다.", 'hr': "Unidentifikacija je zadatak otkrivanja subjekta povezanih s privatnošću u tekstu, poput imena osoba, e-maila i podataka o kontaktu. Dobar je proučen u medicinskom domenu. Povećava se potreba za tehnologijom de-identifikacije, jer se obrađuje podaci o očuvanju privatnosti u mnogim područjima. U ovom papiru, fokusiramo se na poslove. Predstavljamo JobStack, novi korpus za dezidentifikaciju osobnih podataka u praznim mjestima na Stackoverflow. Predstavljamo osnovne linije u usporedbi s dugoročnim uspomenom (LSTM) i modelima transformera. Da bismo poboljšali ove osnovne linije, eksperimentirali smo s predstavljanjem BERT-a i daleko povezanim pomoćnim podacima putem multi task učenja. Naši rezultati pokazuju da pomoćni podaci pomažu u poboljšanju djelotvornosti. Iako su predstavnici BERT poboljšali učinkovitost, iznenađujuće je da je 'vanilla' BERT učinkovitiji od BERT obučenih na podacima povezanim s Stackoverflow.", 'fa': "بازشناسایی وظیفه شناسایی واحدهای مخصوصی در متن است، مانند نام شخصی، ایمیل و داده های تماس. در دامنه پزشکی خوب مطالعه شده است. نیازی برای تکنولوژی بازشناسایی افزایش می\u200cیابد، زیرا در بسیاری از دامنهای محافظت داده\u200cهای خصوصی در نیازی بالا است. در این کاغذ، ما روی پست کارها تمرکز می کنیم. ما JobStack را پیشنهاد می\u200cکنیم، یک کورپوس جدید برای بازشناسایی داده\u200cهای شخصی در جایگاه\u200cهای کار در Stackoverflow. ما خطوط پایین را معرفی می کنیم، با مقایسه حافظه زمان کوتاه (LSTM) و مدل تغییر دهنده. برای بهبود کردن این خطوط بنیادی، ما با نمایش\u200cهای BERT آزمایش می\u200cکنیم، و داده\u200cهای کمک\u200cکننده\u200cای از طریق یادگیری\u200cهای بسیاری از کارهای دور و دور و دور رابطه می\u200cکنیم. نتیجه\u200cهای ما نشان می\u200cدهند که داده\u200cهای کمک کمک می\u200cکند تا عملکرد بازشناسایی را بهتر کند. While BERT representations improve performance, surprisingly 'vanilla' BERT turns out to be more effective than BERT trained on Stackoverflow-related data.", 'sw': "Utambulisho huo ni jukumu la kutafuta vifaa vinavyohusiana na faragha katika maandishi kama vile majina ya mtu, barua pepe na taarifa za mawasiliano. Imekuwa imesomwa vizuri ndani ya maeneo ya afya. Umuhimu wa teknolojia ya kutambuliwa unaongezeka, kwa sababu huduma za binafsi za kutunza taarifa zinaongezeka katika mahitaji ya juu katika maeneo mengi. Katika karatasi hii, tunajikita kwenye makala za kazi. Tunawasilisha JobStack, makampuni mpya kwa ajili ya kutambua takwimu binafsi katika nafasi za ajira katika Stackoverflow. Tunawasilisha mistari, ukilinganisha kumbukumbu ya muda mrefu (LSTM) na mifano ya zamani. Ili kuboresha misingi haya, tunajaribu kuwawakilisha BERT, na taarifa za ushirikiano kwa kiasi kikubwa kupitia kujifunza kwa kazi nyingi. Matokeo yetu yanaonyesha kuwa taarifa za ziada zinasaidia kuboresha utendaji wa utambulisho. Wakati wawakilishaji wa BERT huboresha utendaji, kwa kushangaza kwamba 'vanilla' BERT ilionekana kuwa na ufanisi zaidi ya mafunzo ya BERT kwenye taarifa zinazohusiana na Stackoverflow.", 'sq': "De-identification is the task of detecting privacy-related entities in text, such as person names, emails and contact data.  Është studiuar mirë brenda fushës mjekësore. The need for de-identification technology is increasing, as privacy-preserving data handling is in high demand in many domains.  Në këtë letër, ne përqëndrohemi në postimet e punës. Ne paraqesim JobStack, një korpus të ri për çidentifikimin e të dhënave personale në vendet e lira të punës në Stackoverflow. Ne prezantojmë linjat bazë, duke krahasuar Memory Long-Short Term (LSTM) dhe modelet Transformer. Për të përmirësuar këto linja bazë, ne eksperimentojmë me përfaqësimet e BERT, dhe të dhënat ndihmëse të lidhura larg nëpërmjet mësimit me shumë detyra. Our results show that auxiliary data helps to improve de-identification performance.  While BERT representations improve performance, surprisingly 'vanilla' BERT turned out to be more effective than BERT trained on Stackoverflow-related data.", 'am': "ዶሴ `%s'ን ማስፈጠር አልተቻለም፦ %s በመድኃኒት ድሞኔ ውስጥ መልካም ተማርቷል፡፡ የግልጽ መግለጫ ቴክኖጂ ያስፈልጋል ብዙዎች ሀገሮች ውስጥ የግል መረጃዎችን በመጠበቅ ጥያቄ ነው፡፡ በዚህ ገጽ ላይ የሥራ ጽሑፎችን እናስማራለን፡፡ JobStack፣ የራሳቸውን ዳታዎችን በስቴክፎሎ አዲስ ስራ ፍቃድ ለማግኘት አቀረብን፡፡ የረጅም ቆራጭ ጊዜ ማስታወሻ (LSTM) እና ትርጉም ሞዴላዎችን እናስተያየዋለን፡፡ እነዚህን መደበቂያዎች ለማድረግ፣ BERT መልዕክቶች እና በብዙ ስራ ትምህርት የተጠቃሚ ዳታዎችን እናፈትናለን፡፡ ፍሬዎቻችን የሚያሳየው የአጠቃላይ ዳታዎች የግንኙነቱን ድረግ ለማሻል ይረዳል፡፡ While BERT representations improve performance, surprisingly 'vanilla' BERT turned out to be more effective than BERT trained on Stackoverflow-related data.", 'tr': "Şahsy adlar, e-poçtalar we temas maglumaty ýaly metin içinde mahkumlyklary ahtarmak göresidir. Lukmanyň arasynda gowy okalýan. Otomatik gaýd etmek üçin teknoloji artýar, ýüzünlik gaýd etmek üçin ýagdaýy gaýd etmek üçin köp alanlarda ýokary isleýär. Bu kagyzda işiň pozylygyna üns berýäris. Stack'i, Stackoverflow'da şahsy maglumatlaryň tapylygyny çykarmak üçin täze bir korpus görkeýäris. Biz suçular Bu temel hatlary geliştirmek üçin, BERT temsilleri bilen test edip, birnäçe işi öwrenmek üzerinden uzaklaşdyrýan kömekleyici maglumatlar bilen test edip duruyoruz. Biziň netijelerimiz kömek maglumatymyz kimliklendirmekden çykarmak üçin kömekleýändigini görkezýär. BERT täzelikleri etkinleşigini gowurap ýöredýän bolsa, 'vanilla' BERT Stackoverflow bilen öwrenmeli maglumatlardan daha täsirli bolup göründi.", 'az': "Şəxsi adlar, e-maillər və məlumatlar kimi şəxsi tərəfindən əlaqə edilən məlumatları təşkil etmək məlumatıdır. Təbbəbi sahəsində çox gözəl təhsil edildi. Özünü təsdiqləmə teknolojisinin ehtiyacı artırır, çünki xəbərsizlik təhlükəsizlərini qorumaq üçün məlumatları çox alanda yüksək tələb edir. Bu kağızda iş göndərmələrinə odaqlanırız. Biz JobStack, Stackoverflow işlərində kişilər məlumatlarını dəyişdirmək üçün yeni bir korpusu göstəririk. Biz, uzun-qısqa Term Memory (LSTM) və Transformer modellerini karşılaşdırırıq. Bu temel çətinləri yaxşılaşdırmaq üçün, biz BERT göstəriciləri ilə imtahana çəkirik, və çoxlu işin öyrənməsi vasitəsilə uzaqlaşdırılmış yardım məlumatları ilə imtahana çəkirik. Sonuçlarımız belə göstərir ki, yardım verilən məlumatlar dəyişdirmək performansını yaxşılaşdırmağa kömək edir. BERT göstəriciləri performansını yaxşılaşdırmaq üçün, şaşırtıcı olaraq 'vanilla' BERT Stackoverflow ilə təhsil edilən məlumatlardan BERT'dən daha etkilidir.", 'bn': 'টেক্সটে গোপনীয়তার সংশ্লিষ্ট বস্তুগুলো সনাক্ত করার কাজ, যেমন ব্যক্তির নাম, ই-মেইল এবং যোগাযোগ তথ্য। মেডিকেল ডোমেইনের মধ্যে এটা ভালো গবেষণা করা হয়েছে। ডি চিহ্নিত প্রযুক্তির প্রযুক্তির প্রয়োজন বাড়ছে, কারণ বেশ কয়েকটি দেশে ব্যক্তিগত তথ্য সংরক্ষণের ব্যক্তিগত তথ্য হা এই পত্রিকায় আমরা কাজ পোস্টের উপর মনোযোগ দিচ্ছি। আমরা জোবস্ট্যাক, স্ট্যাকফ্লোতে কাজের ব্যক্তিগত তথ্য পুনরায় চিহ্নিত করার জন্য একটি নতুন কর্পুস। We introduce baselines, comparing Long-Short Term Memory (LSTM) and Transformer models.  এই বেসেলাইন উন্নত করার জন্য আমরা বেরেটি প্রতিনিধিত্বের সাথে পরীক্ষা করি এবং বহুকাজের শিক্ষার মাধ্যমে অনেক সংশ্লিষ্ট অনুসা আমাদের ফলাফল দেখা যাচ্ছে যে প্রতিযোগিতার তথ্য সাহায্য করে ডি-চিহ্নিত কর্মসূচি উন্নত করার জন্য। যদিও বের্টি প্রতিনিধিত্বের প্রতিনিধিত্ব উন্নতি করেছে, স্ট্যাকফ্লোস-সম্পর্কিত তথ্যের প্রশিক্ষণের চেয়ে বেরেট বেশি কার্যকর মন', 'bs': "Deidentifikacija je zadatak otkrivanja entitata povezanih sa privatnošću u tekstu, poput imena osoba, e-maila i podataka o kontaktu. Dobro je proučeno u medicinskoj domenu. Povećava se potreba za tehnologijom de-identifikacije, jer je podrška podataka koji očuvaju privatnost u visokim zahtjevima u mnogim domenama. U ovom papiru, fokusiramo se na poslove. Predstavljamo JobStack, novi korpus za dezidentifikaciju ličnih podataka u praznim mjestima na Stackoverflow. Predstavljamo osnovne linije, uspoređujući dugoročnu uspomenu (LSTM) i modele transformera. Da bi poboljšali ove osnovne linije, eksperimentiramo sa predstavljanjem BERT-a i daleko povezanim pomoćnim podacima putem multi task učenja. Naši rezultati pokazuju da pomocni podaci pomažu da poboljšaju deidentifikaciju. Iako su predstave BERT poboljšale učinkovitost, iznenađujuće je da je 'vanilla' BERT učinkovitiji od BERT obučenih na podacima povezanim sa Stackoverflow.", 'ca': "La desidentificació és la tasca de detectar entitats relacionades amb la privacitat en text, com els noms de persones, els correus electrònics i les dades de contacte. S'ha estudiat bé dins el domini mèdic. The need for de-identification technology is increasing, as privacy-preserving data handling is in high demand in many domains.  In this paper, we focus on job postings.  We present JobStack, a new corpus for de-identification of personal data in job vacancies on Stackoverflow.  We introduce baselines, comparing Long-Short Term Memory (LSTM) and Transformer models.  Per millorar aquestes línies de base, experimentem amb representacions BERT, i dades auxiliars remotament relacionades mitjançant aprenentatge multitascat. Our results show that auxiliary data helps to improve de-identification performance.  Mentre que les representacions BERT milloren el rendiment, sorprenentment 'vanilla' BERT va resultar ser més eficaç que BERT entrenat en dades relacionades amb Stackoverflow.", 'af': "De- identifikasie is die taak van beskrywing van privateitsverwante entiteite in teks, soos persoonnaams, e- pos en kontak data. Dit is goed onderwyser binne die mediese domein. Die benodig vir de-identifikasie-teknologie word vergroei, want privateit-beveiliging-data-handtering is in hoë versoek in baie domeine. In hierdie papier, ons fokus op werkspos. Stack,  'n nuwe korpus vir de- identifikasie van persoonlike data in werk vacancies op Stackoverflow. Ons introduseer basis lyne, vergelyking van Lang- Kort Term Geheue (LSTM) en Transformer modele. Om hierdie basisline te verbeter, eksperimenteer ons met BERT-voorstellings, en af verwante hulpdata deur multi-taak-leer. Ons resultate wys dat hulpdata hulp om de-identifikasie-prestasie te verbeter. Terwyl BERT verteenwoordings uitwerking verbeter, het 'vanilla' BERT uitgedraai om meer effektief te wees as BERT ontoefen op Stackoverflow-verwante data.", 'hy': "De-identification is the task of detecting privacy-related entities in text, such as person names, emails and contact data.  It has been well-studied within the medical domain.  The need for de-identification technology is increasing, as privacy-preserving data handling is in high demand in many domains.  In this paper, we focus on job postings.  Մենք ներկայացնում ենք ՋոբՍթաքը, մի նոր կորպոս, որն անձնական տվյալների բացահայտումն է Սթաքքուրֆլեյսի դատարկ աշխատատեղերում: We introduce baselines, comparing Long-Short Term Memory (LSTM) and Transformer models.  Այս հիմնական գծերը բարելավելու համար մենք փորձում ենք BER-ի ներկայացումների և հեռավորության հետ կապված օգնական տվյալների հետ բազմախնդիրների ուսումնասիրության միջոցով: Մեր արդյունքները ցույց են տալիս, որ օգնական տվյալները օգնում են բարելավել բացահայտումները: While BERT representations improve performance, surprisingly 'vanilla' BERT turned out to be more effective than BERT trained on Stackoverflow-related data.", 'fi': 'Tunnistamisen poisto on yksityisyyteen liittyvien tahojen havaitseminen tekstissä, kuten henkilönimet, sähköpostit ja yhteystiedot. Sitä on tutkittu hyvin lääketieteen alalla. Tunnistamisteknologian tarve kasvaa, koska yksityisyyttä säilyttävä tiedonkäsittely on monilla aloilla erittäin kysyntää. Tässä artikkelissa keskitytään työpaikkailmoituksiin. Esittelemme JobStackin, uuden korpusen henkilötietojen tunnistamiseksi avoimissa työpaikoissa Stackoverflossa. Esittelemme peruslinjat, joissa vertaillaan Long-Short Term Memory (LSTM) ja Transformer malleja. Parantaaksemme näitä lähtökohtia kokeilemme BERT-esityksiä ja etänä toisiinsa liittyvää apudataa monitehtäväoppimisen avulla. Tuloksemme osoittavat, että apudata auttaa parantamaan tunnistamisen suorituskykyä. Vaikka BERT-esitykset parantavat suorituskykyä, yllättävän "vanilla" BERT osoittautui tehokkaammaksi kuin BERT, joka on koulutettu Stackoverflow-dataan.', 'cs': 'De-identifikace je úkolem detekce subjektů souvisejících s ochranou osobních údajů v textu, jako jsou jména osob, e-maily a kontaktní údaje. Byla dobře studována v rámci lékařské oblasti. Potřeba technologie deidentifikace roste, neboť zpracování dat zachovávající soukromí je v mnoha oblastech vysoká poptávka. V tomto článku se zaměřujeme na nabídky pracovních míst. Představujeme JobStack, nový korpus pro deidentifikaci osobních údajů v pracovních místech na Stackoverflow. Představujeme základní linie, porovnáváme modely Long-Short Term Memory (LSTM) a Transformer. Pro zlepšení těchto základních linek experimentujeme s BERT reprezentacemi a vzdáleně souvisejícími pomocnými daty prostřednictvím víceúlohového učení. Naše výsledky ukazují, že pomocná data pomáhají zlepšovat výkon deidentifikace. Zatímco reprezentace BERT zlepšují výkon, překvapivě se BERT ukázalo být efektivnější než BERT trénovaný na datech souvisejících se Stackoverflow.', 'et': 'Identifitseerimise eemaldamine on eraelu puutumatusega seotud üksuste tuvastamise ülesanne tekstis, nagu isikuandmed, e-kirjad ja kontaktandmed. Seda on hästi uuritud meditsiini valdkonnas. Vajadus identifitseerimise eemaldamise tehnoloogia järele kasvab, kuna eraelu puutumatust säilitavate andmete käitlemine on paljudes valdkondades suur nõudlus. Selles dokumendis keskendume ametikohtade väljakuulutamisele. Tutvustame JobStacki, uut korpust isikuandmete identifitseerimiseks vabadel töökohtadel Stackoveflow\'is. Tutvustame alusjooni, võrreldes pika-lühiajalise mälu (LSTM) ja transformaatori mudeleid. Nende lähtejoonte täiustamiseks eksperimenteerime BERTi esitusi ja kaugelt seotud abiandmeid mitme ülesandega õppimise kaudu. Meie tulemused näitavad, et lisaandmed aitavad parandada identifitseerimist. Kuigi BERT-i esitused parandavad jõudlust, osutus üllatavalt "vanilla" BERT tõhusamaks kui BERT-i koolitatud Stackhoverflow-ga seotud andmetega.', 'sk': 'Deidentifikacija je naloga odkrivanja subjektov, povezanih z zasebnostjo, v besedilu, kot so imena oseb, e-pošta in kontaktni podatki. To je bilo dobro raziskano na področju medicine. Potreba po tehnologiji za odpravo identifikacije se povečuje, saj je ravnanje s podatki, ki ohranja zasebnost, na številnih področjih veliko povpraševanje. V tem članku se osredotočamo na objave delovnih mest. Predstavljamo vam JobStack, nov korpus za identifikacijo osebnih podatkov na prostih delovnih mestih na Stackoverflow. Predstavljamo osnovne linije, ki primerjajo modele dolgoročnega kratkoročnega pomnilnika (LSTM) in transformatorjev. Za izboljšanje teh izhodišč preizkušamo z BERT predstavitvami in daleč povezanimi pomožnimi podatki prek večopravilnega učenja. Naši rezultati kažejo, da pomožni podatki pomagajo izboljšati zmogljivost odkrivanja identifikacije. Medtem ko BERT predstavitve izboljšujejo učinkovitost, se je presenetljivo izkazalo, da je BERT učinkovitejši od BERT, ki je bil usposobljen za podatke, povezane s Stackhoverflowom.', 'ha': 'Shirin-Aiki yana da aikin ka gane abubuwa masu da ke da gane na farat ɗaya cikin matsayin, kamar sunan mutum, email da data na haɗi. An jarraba shi alhẽri a cikin gudan shida. Ana ƙara da haji ga zane-gane-zane-zane, kwani tsari na tsari data na da amfani da kuma yana cikin mataimaki mai girma a cikin jama\'a mãsu yawa. Ina cikin wannan takarda, tuna makini a kan wurãren aiki. Tuna halatar da JobStack, wata sabo na daban wa-gane data masu cikin shirin ajin da ba\'a komai ba. Tuna ƙaranar salon, sunã daidaita lokacin Lokaci-Long-Short (LstuM) da motsalan Transformer. To, domin improve waɗannan bangon, za\'a jarraba su da mabarancin BERT, kuma da data masu husũma a cikin bayan da za\'a sami mutane. Our results show that auxiliary data helps to improve de-identification performance.  A lokacin da mazaunin BERT suka improve performance, sai aka baya "sava" BERT ta zama mafi mai amfani ko da BERT aka yi wa tsaron data na Stackoverslowo-related.', 'jv': 'ID It has been wel- read in the therapist domain. Awak dhéwé ngéwangi sampek-sampek akeh luwih nêmên, kaya nguasai perusahaan-sampek data kang nggawe barang akeh sabên nêmên. Nang mapun iki, kéné supokoke ing layang-layang. Stack Awak dhéwé nambah tanggal-sistem, nggo nambah Label Ngawe ngubah Caret Rejalaké awakdhéwé ngerasakno ngono nggawe operasi tarjamahan BERT wis dipunangé kapan langgar nggawe geratan operasi, pasang pangan "fala" BERT kuwi kapan titik sing luwih apik sing katya BERT terus dipunangé kapan dadi Stacksuper-Flow.', 'he': 'ההזדמנות היא המשימה לגלות יחידות קשורות לפרטיות בטקסט, כמו שמות אדם, דואר אלקטרוני ומידע קשר. הוא למד היטב בתחום הרפואי. הצורך לטכנולוגיה להפסקת זיהוי עולה, כיוון ששמירת נתונים על פרטיות נמצאת בדרישה גבוהה בהרבה תחומות. בעיתון הזה, אנחנו מתמקדים בפרסומות עבודה. אנחנו מציגים את ג\'וב סטאק, קורפוס חדש להפסיק זיהוי של נתונים אישיים במשרדים ריקים על סטאקאובר. אנחנו מציגים קווי בסיס, משווה זיכרון לטווח קצר ארוך (LSTM) ומודלים טרנספורטרים. כדי לשפר את קווי הבסיס האלה, אנו מנסים עם מייצגים BERT, ומידע עזרי קשור מרחוק דרך לימוד ממשימות רבות. התוצאות שלנו מראות כי נתונים עזריים עוזרים לשפר ביצועי התיקון. בעוד מייצגים של BERT משתפרים ביצועים, באופן מפתיע "ואנילה" BERT התברר להיות יותר יעיל מאשר BERT מאומן על נתונים קשורים לסטאקאובר.', 'bo': 'མིང་མེད་དམིགས་གསལ་ནི་ཡི་གེའི་ནང་གི་མིང་དང་གློག་འཕྲིན་དང་འབྲེལ་བ་ཡོད་པའི་མིང་དང་གསལ་བཤད་ཀྱི་བྱ་སྤྱོད་རེད། སྨན་ཐོག་གི་ཁྱིམ་ཚང་ནང་ལ་ཕལ་ཆེར་ཞིབ་བྱས་པ་རེད། རང་ཉིད་ཀྱི་མིང་རྟོགས་ལག་ཁྱེར་ལ་ཉེན་དགོས་ཀྱི་ལས་འཕར་རིས།མི་སྒེར་གྱི་རང་བཞིན་གནས་སྟངས་ཉར་སྐྱོང་བའི་ཆ་འཕྲིན་ཡིག ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོས་བློ་གཏད་ཀྱི་གནས་ཚུལ་ལ་དམིགས་བསལ་བྱེད་ཀྱི་ཡོད། ང་ཚོས་Stack ས་(JobStack)ནང་དུ་Stackoverflow(Stackoverflow)ནང་གི་ལས་གནས་ཚུལ་གསང་མེད་པའི་མིང་གསར་པ་ཞིག་སྟོན་པ་ཡིན། ང་ཚོས་རྨང་གཞིའི་ཡིག འུ་ཅག་གི་རྨང་གཞི་འདི་དག་ཡར་དུ་གཏོང་དགོས། BERT་ལྟ་བུའི་སྟོན་ཚུལ་དང་། ཐག་རིང་གི་རྒྱལ་ཁབ་ཀྱི་ཐབས་ལམ་ནས་མཐུན་སྐྱོང ང་ཚོའི་གྲུབ་འབྲས་བ་དག་གི་ཡར་རྐྱེན་ཆེན་བྱེད་ཀྱི་གནས་ཚུལ་གསལ་བཤད་ཀྱི་ལས་འཕར་རིས། BERT སྟོན་འཛུགས་ཀྱི་ལས་འགན་འགྱུར་བ་སྐྱེལ་ཡར་རྒྱས་གཏོང་བྱུང་།'}
{'en': 'NLI Data Sanity Check : Assessing the Effect of Data Corruption on Model Performance', 'ar': 'فحص سلامة بيانات NLI: تقييم تأثير تلف البيانات على أداء النموذج', 'pt': 'Verificação de integridade de dados da NLI: avaliando o efeito da corrupção de dados no desempenho do modelo', 'es': 'Comprobación de la integridad de los datos de NLI: evaluación del efecto de la corrupción de datos en el rendimiento del modelo', 'fr': "NLI Data Sanity Check\xa0: évaluation de l'effet de la corruption des données sur les performances du modèle", 'ja': 'NLIデータ健全性チェック：モデルパフォーマンスに対するデータ破損の影響の評価', 'zh': 'NLI 数健全性检:评估数损对模型性也', 'hi': 'एनएलआई डेटा स्वच्छता की जांच करें: मॉडल प्रदर्शन पर डेटा भ्रष्टाचार के प्रभाव का आकलन', 'ru': 'Проверка достоверности данных NLI: оценка влияния повреждения данных на эффективность модели', 'ga': 'Seiceáil Sláintíochta Sonraí LNÉ: Éifeacht an Chaillithe Sonraí ar Fheidhmíocht Múnla a Mheas', 'hu': 'NLI adatok józansági ellenőrzése: az adatkárosodás modellteljesítményre gyakorolt hatásának értékelése', 'el': 'Έλεγχος υγιεινής δεδομένων Αξιολόγηση της επίδρασης της διαφθοράς δεδομένων στην απόδοση μοντέλων', 'ka': 'Name', 'kk': 'NLI деректердің қасиеттерін тексеру: Үлгі істеу үшін деректердің қасиеттерінің эффектін оқу', 'it': "Controllo sanitario dei dati NLI: valutazione dell'effetto della corruzione dei dati sulle prestazioni del modello", 'ms': 'Semak Kesehatan Data NLI: Mengesan Kesan Korupsi Data pada Performasi Model', 'ml': 'NLI ഡേറ്റാ സാനിറ്റി പരിശോധിക്കുക: മോഡല്\u200d പ്രവര്\u200dത്തനത്തിന്റെ ഡേറ്റാ നീശങ്ങളുടെ പ്രഭാവം പരിശോധിക്കു', 'mt': 'Kontroll tas-Sanità tad-Dejta NLI: Valutazzjoni tal-Effett tal-Korruzzjoni tad-Dejta fuq il-Prestazzjoni tal-Mudell', 'lt': 'NLI duomenų saugumo patikrinimas: duomenų korupcijos poveikio modeliui vertinimas', 'mk': 'Проверка на сигурноста на податоците на НЛИ: проценка на ефектот на корупцијата на податоците врз резултатите на моделот', 'mn': 'NLI өгөгдлийн сайн байдал шалгалт: Загварын үйл ажиллагааны нөлөөг шалгах', 'pl': 'NLI Data Sanity Check: Ocena wpływu korupcji danych na wydajność modelu', 'no': 'NLI Data Sanity Check: Assess the Effect of Data Corruption on Model Performance', 'so': 'Check the effects of Data Corruption on Model Performance', 'ro': 'Verificarea sanatatii datelor NLI: evaluarea efectului coruptiei datelor asupra performantei modelului', 'sr': 'NLI Provjera zdravstvenosti podataka: procjena učinka korupcije podataka na modelu', 'si': 'Name', 'sv': 'NLI Data Sanity Check: Bedömning av effekten av datakorruption på modellprestanda', 'ta': 'NLI தகவல் பரிசோதிப்பு', 'ur': 'Name', 'uz': 'Name', 'vi': 'Kiểm tra độ an toàn dữ liệu: đánh giá tác động của sự hư hỏng dữ liệu lên trình diễn mô hình', 'bg': 'Проверка на разумността на данните: оценка на ефекта от корупцията на данните върху производителността на моделите', 'da': 'NLI Data Sanity Check: Vurdering af virkningen af datakorruption på modellens ydeevne', 'nl': 'NLI Data Sanity Check: het effect van datacorruptie op modelprestaties beoordelen', 'de': 'NLI Data Sanity Check: Bewertung der Auswirkungen von Datenkorruption auf die Modellleistung', 'hr': 'Provjera zdravosti podataka NLI: procjena učinka korupcije podataka na provedbu modela', 'ko': 'NLI 데이터 건전성 검사: 데이터 손상이 모델 성능에 미치는 영향 평가', 'fa': 'بررسی پاکیزه داده NLI: ارزیابی اثر خرابی داده\u200cها روی عملکرد مدل', 'tr': "NLI Maglumat Azlygy Barlamak: Model Performance'de Maglumatyň Ewezini Barlamak", 'sw': 'Tatibu Usalama wa data NLI: Kupitia Madhara ya Ufisadi wa Data juu ya Utendaji wa Modeli', 'sq': 'Kontrolli i sigurisë së të dhënave të NLI: Vlerësimi i efektit të korrupsionit të të dhënave mbi performancën e modelit', 'id': 'Pemeriksaan Kesehatan Data NLI: Mengevaluasi Efek Korupsi Data pada Performasi Model', 'af': 'Name', 'am': 'image-action', 'hy': 'ՆԼԻ տվյալների առողջապահության ստուգելը. տվյալների կոռուպցիայի ազդեցության գնահատումը մոդելի գործողության վրա', 'bn': 'NLI ডাটা স্যানিটি পরীক্ষা: মোডেল কার্যকলাপে ডাটা দুর্নীতির প্রভাব পরীক্ষা করা হচ্ছে', 'ca': "Verifica de salut de les dades de NLI: Evaluació de l'efecte de la corrupció de les dades en el rendiment model", 'bs': 'NLI Provjera zdravosti podataka: procjena učinka korupcije podataka na modelu', 'az': 'NLI Veri Hizm…ôti Se√ßimi: Model Performanci √ľz…ôrind…ô Veri B…ôsasńĪnńĪn Etkinlińüini Assess…ô Et', 'et': 'NLI andmete tervislikkuse kontroll: andmete korruptsiooni mõju hindamine mudeli jõudlusele', 'cs': 'NLI Data Sanity Check: Posouzení vlivu poškození dat na výkon modelu', 'fi': 'NLI Data Sanity Check: Datakorruption vaikutuksen arviointi mallien suorituskykyyn', 'jv': 'NLI data Sanity Check:', 'ha': 'KCharselect unicode block name', 'he': 'בדיקת בריאות נתונים של NLI: הערכה של השפעה של השחיתות נתונים על ההפעלה של מודל', 'sk': 'Preverjanje zdravega stanja podatkov NLI: Ocena učinka korupcije podatkov na uspešnost modela', 'bo': 'NLI Data Sanity Check: Assessing the Effect of Data Corruption on Model Performance'}
{'en': 'Pre-trained neural language models give high performance on natural language inference (NLI) tasks. But whether they actually understand the meaning of the processed sequences is still unclear. We propose a new diagnostics test suite which allows to assess whether a dataset constitutes a good testbed for evaluating the models’ meaning understanding capabilities. We specifically apply controlled corruption transformations to widely used benchmarks (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs. If model accuracy on the corrupted data remains high, then the dataset is likely to contain statistical biases and artefacts that guide prediction. Inversely, a large decrease in model accuracy indicates that the original dataset provides a proper challenge to the models’ reasoning capabilities. Hence, our proposed controls can serve as a crash test for developing high quality data for NLI tasks.', 'fr': "Les modèles de langage neuronal pré-entraînés offrent des performances élevées pour les tâches d'inférence du langage naturel (NLI). Mais on ne sait toujours pas s'ils comprennent réellement la signification des séquences traitées. Nous proposons une nouvelle suite de tests de diagnostic qui permet d'évaluer si un ensemble de données constitue un bon banc d'essai pour évaluer les capacités de compréhension de la signification des modèles. Nous appliquons spécifiquement des transformations de corruption contrôlées à des repères largement utilisés (MNLI et ANLI), qui impliquent la suppression de classes de mots entières et conduisent souvent à des paires de phrases absurdes. Si la précision du modèle sur les données corrompues reste élevée, l'ensemble de données est susceptible de contenir des biais statistiques et des artefacts qui orientent les prévisions. Inversement, une diminution importante de la précision du modèle indique que le jeu de données d'origine constitue un défi approprié pour les capacités de raisonnement des modèles. Par conséquent, les contrôles que nous proposons peuvent servir de crash-test pour développer des données de haute qualité pour les tâches NLI.", 'pt': 'Modelos de linguagem neural pré-treinados oferecem alto desempenho em tarefas de inferência de linguagem natural (NLI). Mas se eles realmente entendem o significado das sequências processadas ainda não está claro. Propomos um novo conjunto de testes de diagnóstico que permite avaliar se um conjunto de dados constitui um bom banco de testes para avaliar as capacidades de compreensão de significado dos modelos. Aplicamos especificamente transformações de corrupção controlada a benchmarks amplamente utilizados (MNLI e ANLI), que envolvem a remoção de classes inteiras de palavras e muitas vezes levam a pares de frases sem sentido. Se a precisão do modelo nos dados corrompidos permanecer alta, é provável que o conjunto de dados contenha vieses estatísticos e artefatos que orientam a previsão. Inversamente, uma grande diminuição na precisão do modelo indica que o conjunto de dados original oferece um desafio adequado às capacidades de raciocínio dos modelos. Assim, nossos controles propostos podem servir como um teste de colisão para o desenvolvimento de dados de alta qualidade para tarefas NLI.', 'ar': 'توفر نماذج اللغة العصبية المدربة مسبقًا أداءً عاليًا في مهام استدلال اللغة الطبيعية (NLI). لكن ما إذا كانوا يفهمون بالفعل معنى التسلسلات المعالجة لا يزال غير واضح. نقترح مجموعة اختبارات تشخيصية جديدة تسمح بتقييم ما إذا كانت مجموعة البيانات تشكل اختبارًا جيدًا لتقييم قدرات فهم المعنى للنماذج. نحن نطبق على وجه التحديد تحولات الفساد الخاضعة للرقابة على المعايير المستخدمة على نطاق واسع (MNLI و ANLI) ، والتي تتضمن إزالة فئات الكلمات بأكملها وغالبًا ما تؤدي إلى أزواج من الجمل غير المنطقية. إذا ظلت دقة النموذج على البيانات التالفة عالية ، فمن المحتمل أن تحتوي مجموعة البيانات على تحيزات إحصائية ومصنوعات يدوية توجه التنبؤ. بشكل عكسي ، يشير الانخفاض الكبير في دقة النموذج إلى أن مجموعة البيانات الأصلية توفر تحديًا مناسبًا لقدرات النماذج المنطقية. وبالتالي ، يمكن أن تكون عناصر التحكم المقترحة بمثابة اختبار تصادم لتطوير بيانات عالية الجودة لمهام NLI.', 'es': 'Los modelos de lenguaje neuronal previamente entrenados ofrecen un alto rendimiento en las tareas de inferencia de lenguaje natural (NLI). Pero aún no está claro si realmente entienden el significado de las secuencias procesadas. Proponemos un nuevo conjunto de pruebas de diagnóstico que permite evaluar si un conjunto de datos constituye un buen banco de pruebas para evaluar las capacidades de comprensión del significado de los modelos. Aplicamos específicamente transformaciones de corrupción controlada a puntos de referencia ampliamente utilizados (MNLI y ANLI), que implican la eliminación de clases de palabras enteras y, a menudo, conducen a pares de oraciones sin sentido. Si la precisión del modelo en los datos dañados sigue siendo alta, es probable que el conjunto de datos contenga sesgos estadísticos y artefactos que guían la predicción. Por el contrario, una gran disminución en la precisión del modelo indica que el conjunto de datos original supone un desafío adecuado para las capacidades de razonamiento de los modelos. Por lo tanto, los controles que proponemos pueden servir como una prueba de choque para desarrollar datos de alta calidad para las tareas de NLI.', 'zh': '豫教者神经语言形于自然语言理(NLI)供高性能。 然其真解序之义未详也。 新诊试套件,套件许评数集理解能力良测平台。 专以受控腐败转用于广用之准(MNLIANLI),此涉删全单词之类,而常致无意之句是也。 若损数之形准确性犹高,则数集可通算差伪影。 反是,准确性之大幅降原始数据集对模型理得其宜。 故臣等言控制措施可为NLI开高质量数者触试。', 'ja': '事前にトレーニングされたニューラル言語モデルは、自然言語推論（ NLI ）タスクに高いパフォーマンスを提供します。しかし、彼らが実際に処理された配列の意味を理解しているかどうかはまだ不明である。データセットがモデルの意味理解能力を評価するための良いテストベースであるかどうかを評価することができる新しい診断テストスイートを提案します。私たちは、広く使用されているベンチマーク（ MNLIおよびANLI ）に制御された破壊変換を特に適用します。これには、単語クラス全体の削除が含まれ、多くの場合、非意味的な文のペアにつながります。破損したデータのモデルの精度が高いままである場合、データセットには、予測を導く統計的バイアスとアーチファクトが含まれている可能性が高い。逆に、モデルの精度が大きく低下することは、元のデータセットがモデルの推論能力に適切な挑戦を提供することを示します。したがって、提案されたコントロールは、NLIタスクのための高品質データを開発するためのクラッシュテストとして機能することができます。', 'hi': 'पूर्व-प्रशिक्षित तंत्रिका भाषा मॉडल प्राकृतिक भाषा अनुमान (एनएलआई) कार्यों पर उच्च प्रदर्शन देते हैं। लेकिन क्या वे वास्तव में संसाधित अनुक्रमों के अर्थ को समझते हैं, यह अभी भी स्पष्ट नहीं है। हम एक नए निदान परीक्षण सूट का प्रस्ताव करते हैं जो यह आकलन करने की अनुमति देता है कि क्या डेटासेट मॉडल के अर्थ को समझने की क्षमताओं का मूल्यांकन करने के लिए एक अच्छा टेस्टबेड का गठन करता है। हम विशेष रूप से व्यापक रूप से उपयोग किए जाने वाले बेंचमार्क (MNLI और ANLI) पर नियंत्रित भ्रष्टाचार परिवर्तनों को लागू करते हैं, जिसमें पूरे शब्द वर्गों को हटाना शामिल है और अक्सर गैर-संवेदी वाक्य जोड़े का कारण बनता है। यदि दूषित डेटा पर मॉडल सटीकता उच्च बनी हुई है, तो डेटासेट में सांख्यिकीय पूर्वाग्रहों और कलाकृतियों की संभावना है जो भविष्यवाणी का मार्गदर्शन करते हैं। इसके विपरीत, मॉडल सटीकता में एक बड़ी कमी इंगित करती है कि मूल डेटासेट मॉडल की तर्क क्षमताओं के लिए एक उचित चुनौती प्रदान करता है। इसलिए, हमारे प्रस्तावित नियंत्रण एनएलआई कार्यों के लिए उच्च गुणवत्ता वाले डेटा को विकसित करने के लिए एक क्रैश टेस्ट के रूप में काम कर सकते हैं।', 'ru': 'Предварительно обученные нейронные языковые модели дают высокую производительность при выполнении задач вывода естественного языка (NLI). Но действительно ли они понимают значение обработанных последовательностей, до сих пор неясно. Мы предлагаем новый набор диагностических тестов, который позволяет оценить, является ли набор данных хорошим испытательным стендом для оценки возможностей понимания смысла моделей. Мы конкретно применяем контролируемые коррупционные преобразования к широко используемым критериям (MNLI и ANLI), которые предполагают удаление целых классов слов и часто приводят к нечувствительным парам предложений. Если точность модели на искаженных данных остается высокой, то набор данных, вероятно, будет содержать статистические погрешности и артефакты, которые направляют прогнозирование. И наоборот, значительное снижение точности модели указывает на то, что исходный набор данных создает надлежащую проблему для возможностей моделирования. Таким образом, предлагаемые нами средства контроля могут служить в качестве краш-теста для разработки высококачественных данных для задач NLI.', 'ga': 'Tugann samhlacha néartheanga réamhoilte ardfheidhmíocht ar thascanna tátail nádúrtha teanga (NLI). Ach níl sé soiléir fós an dtuigeann siad i ndáiríre an bhrí atá leis na sraitheanna próiseáilte. Molaimid sraith tástála diagnóisice nua a cheadaíonn measúnú a dhéanamh ar cé acu an bhfuil tacar sonraí ina leaba tástála maith chun cumas tuisceana brí na samhlacha a mheas. Cuirimid claochluithe truaillithe rialaithe i bhfeidhm go sonrach ar thagarmharcanna a úsáidtear go forleathan (MNLI agus ANLI), lena mbaineann baint a bhaint as aicmí focal iomlána agus a mbíonn péirí abairtí neamhíogaire mar thoradh orthu go minic. Má tá cruinneas múnla na sonraí truaillithe fós ard, is dócha go mbeidh laofachtaí staitistiúla agus déantáin sa tacar sonraí a threoraíonn an tuar. Go contrártha, léiríonn laghdú mór ar chruinneas na samhla go dtugann an tacar sonraí bunaidh dúshlán ceart do chumais réasúnaíochta na samhlacha. Mar sin, is féidir lenár rialuithe molta feidhmiú mar thuarthástáil chun sonraí ardchaighdeáin a fhorbairt le haghaidh tascanna LNÉ.', 'hu': 'Az előre képzett neurális nyelvi modellek nagy teljesítményt nyújtanak a természetes nyelvi következtetés (NLI) feladatokban. De még mindig nem világos, hogy megértik-e a feldolgozott szekvenciák jelentését. Javasolunk egy új diagnosztikai tesztcsomagot, amely lehetővé teszi annak felmérését, hogy egy adatkészlet jó tesztágyat jelent-e a modellek jelentésértési képességeinek értékeléséhez. Kifejezetten ellenőrzött korrupciós transzformációkat alkalmazunk a széles körben használt referenciaértékekre (MNLI és ANLI), amelyek teljes szóosztályok eltávolításával járnak, és gyakran nem érzékeny mondatpárokhoz vezetnek. Ha a sérült adatok modellpontossága továbbra is magas, akkor az adatkészlet valószínűleg statisztikai előítéleteket és tárgyakat tartalmaz, amelyek irányítják az előrejelzést. Ellenkezőleg a modell pontosságának nagymértékű csökkenése azt jelzi, hogy az eredeti adatkészlet megfelelő kihívást jelent a modellek érvelési képességei számára. Ezért javasolt vezérlőink összeomlási tesztként szolgálhatnak az NLI feladatokhoz szükséges magas minőségű adatok kidolgozásához.', 'el': 'Τα προ-εκπαιδευμένα μοντέλα νευρωνικής γλώσσας παρέχουν υψηλή απόδοση σε εργασίες συμπερασμάτων φυσικής γλώσσας. Αλλά αν κατανοούν πραγματικά το νόημα των επεξεργασμένων ακολουθιών είναι ακόμα ασαφές. Προτείνουμε μια νέα σειρά διαγνωστικών δοκιμών που επιτρέπει να αξιολογηθεί εάν ένα σύνολο δεδομένων αποτελεί ένα καλό δοκιμαστικό πεδίο για την αξιολόγηση των δυνατοτήτων κατανόησης των εννοιών των μοντέλων. Εφαρμόζουμε συγκεκριμένα ελεγχόμενους μετασχηματισμούς διαφθοράς σε ευρέως χρησιμοποιούμενα κριτήρια αναφοράς (και τα οποία περιλαμβάνουν την αφαίρεση ολόκληρων κατηγοριών λέξεων και συχνά οδηγούν σε μη ευαίσθητα ζεύγη προτάσεων. Εάν η ακρίβεια του μοντέλου στα κατεστραμμένα δεδομένα παραμένει υψηλή, τότε το σύνολο δεδομένων είναι πιθανό να περιέχει στατιστικές προκαταλήψεις και αντικείμενα που καθοδηγούν την πρόβλεψη. Αντίστροφα, μια μεγάλη μείωση της ακρίβειας του μοντέλου δείχνει ότι το αρχικό σύνολο δεδομένων παρέχει μια κατάλληλη πρόκληση στις δυνατότητες συλλογισμού των μοντέλων. Ως εκ τούτου, οι προτεινόμενοι έλεγχοι μας μπορούν να χρησιμεύσουν ως δοκιμή συντριβής για την ανάπτυξη υψηλής ποιότητας δεδομένων για εργασίες NLI.', 'ka': 'პირველი განსწავლებული ნეიროლური ენის მოდელები უფრო დიდი გამოსახულება ჩვენი ენერგიის ინფრენციის (NLI) პარამეტრებზე. მაგრამ თუ ისინი ნამდვილად გავიგებენ პროცესირების ნიშნავს თუ არა უცნობი. ჩვენ ახალი დიაგონტიკური ტესტის სუტი, რომელიც შესაძლებელია გავაკეთოთ თუ მონაცემების კონფიგურაცია კარგი ტესტის შესაძლებლობად მოდელების ნიშნავს შესაძლებლობას გადა ჩვენ განსაკუთრებით კონტროლური კოლუპორციის გარემოქმედებას უფრო გამოიყენებული ბენქმარკებში (MNLI და ANLI), რომლებიც უფრო სიტყვების კლასის წაშლად და ზოგჯერ უფრო სიტ თუ კომპორტირებული მონაცემების მართლა მართლა უფრო მეტია, შემდეგ მონაცემების კომპორტიკური მონაცემები იქნება სტატისტიკური წინასწორება და არტეფე შემდეგ დიდი მოდელური წარმოდგენების შემცირება მონიშნავს, რომ ორიგინალური მონაცემების შესაძლებლობისთვის მარტივი გამოცდილება. ამიტომ, ჩვენი მოძლევებული კონტროლეციები შეუძლიათ გავაკეთოთ როგორც კონტროლეციის ტესტი NLI სამუშაო მონაცემებისთვის.', 'it': "Modelli di linguaggio neurale pre-addestrati offrono alte prestazioni sulle attività di inferenza del linguaggio naturale (NLI). Ma se effettivamente capiscono il significato delle sequenze elaborate non è ancora chiaro. Proponiamo una nuova suite di test diagnostici che consente di valutare se un set di dati costituisce un buon banco di prova per valutare le capacità di comprensione del significato dei modelli. Applichiamo specificamente trasformazioni di corruzione controllate ai benchmark ampiamente utilizzati (MNLI e ANLI), che comportano la rimozione di intere classi di parole e spesso portano a coppie di frasi non sensibili. Se l'accuratezza del modello sui dati danneggiati rimane elevata, è probabile che il set di dati contenga pregiudizi statistici e artefatti che guidano la previsione. Al contrario, una grande diminuzione dell'accuratezza del modello indica che il set di dati originale fornisce una sfida adeguata alle capacità di ragionamento dei modelli. Pertanto, i nostri controlli proposti possono servire da crash test per lo sviluppo di dati di alta qualità per le attività NLI.", 'lt': 'Pre-trained neural language models give high performance on natural language inference (NLI) tasks.  Bet ar jie supranta perdirbtų sekų reikšmę, vis dar neaišku. Siūlome naują diagnostikos bandymų rinkinį, kuris leistų įvertinti, ar duomenų rinkinys yra tinkamas bandymų pagrindas vertinant modelių reikšmės supratimo gebėjimus. Mes konkrečiai taikome kontroliuojamus korupcijos pokyčius plačiai naudojamiems lyginamiesiems rodikliams (MNLI ir ANLI), kurie apima visiškų žodžių klasių pašalinimą ir dažnai lemia nežinomas sakinių poras. Jei sugadintų duomenų modelio tikslumas išlieka didelis, tuomet duomenų rinkinyje tikriausiai bus statistinių sąlyčių ir objektų, kuriais vadovaujamasi prognozėmis. Priešingai, didelis modelio tikslumo sumažėjimas rodo, kad pradinis duomenų rinkinys yra tinkamas iššūkis modelių motyvavimo gebėjimams. Todėl mūsų siūlomos kontrolės priemonės gali būti avarijų bandymas rengiant aukštos kokybės duomenis NLI užduotims atlikti.', 'ms': 'Model bahasa saraf dilatih-terlatih memberikan prestasi tinggi pada tugas kesimpulan bahasa semulajadi (NLI). Tetapi sama ada mereka benar-benar memahami makna urutan yang diproses masih tidak jelas. Kami cadangkan suite ujian diagnostik baru yang membolehkan menilai sama ada set data merupakan tempat ujian yang baik untuk menilai kemampuan pemahaman makna model. Kami secara khusus menerapkan pengubahan korupsi terkawal kepada tanda referensi yang digunakan secara luas (MNLI dan ANLI), yang melibatkan membuang seluruh kelas perkataan dan sering membawa kepada pasangan kalimat yang tidak sensik. Jika ketepatan model pada data rosak tetap tinggi, maka set data mungkin mengandungi biases statistik dan artefakta yang memimpin ramalan. Sebaliknya, penurunan besar dalam ketepatan model menunjukkan bahawa set data asal menyediakan cabaran yang sesuai untuk kemampuan reasoning model. Oleh itu, kawalan yang kami cadangkan boleh berkhidmat sebagai ujian kemalangan untuk mengembangkan data kualiti tinggi untuk tugas NLI.', 'kk': 'Алдыңғы оқылған невралдық тіл үлгілері табиғи тілдерінің (NLI) тапсырмаларында жоғары әсер етеді. Бірақ олар өзгертілген реттеулердің мағынасын түсінбейді. Біз үлгілердің мәліметін түсіну мүмкіндіктері үшін жаңа диагностикалық сынақтар сәйкестігін ұсынып көрсетуге мүмкіндік береді. Біз әдеттегі мәліметті мәліметті мәліметті (MNLI және ANLI) қолданылатын мәліметтерге бақылау үшін қолданымыз. Бұл сөздер классын өшіріп, көбінде сенсиялық емес сөздер екеу Егер бұғатталған деректердің дұрыстығы үлгісі жоғары болса, деректер жиында статистикалық көздері мен артефакттары бар болуы мүмкін. Терістеп, үлгі дұрыстығын үлкен азайту үлгі деректер жиының бастапқы деректер жиыны үлгілердің сезімдеу мүмкіндіктеріне дұрыс өзгертуге болады. Сондықтан, NLI тапсырмаларының жоғары сапатты деректерін құру үшін қолданылатын контроллеріміз жаңылыс сынақтары болады.', 'ml': 'പ്രവർത്തിക്കപ്പെട്ട ന്യൂറല്\u200d ഭാഷ മോഡലുകള്\u200d സ്വാഭാവിക ഭാഷ അപകടത്തിന്\u200dറെ (NLI) ജോലികളില്\u200d ഉയര്\u200dന്ന പ്രദര്\u200dശ പക്ഷെ അവര്\u200d ശരിക്കും മനസ്സിലാക്കുന്നുണ്ടോ അതിന്റെ അര്\u200dത്ഥം ഇപ്പോഴും വ്യക്തമാണെന്ന്. ഞങ്ങള്\u200d ഒരു പുതിയ ഡിയോഗിസ്റ്റിക്സ് പരീക്ഷ സ്യൂട്ട് പരിശോധിക്കുന്നു. ഒരു ഡാറ്റാസെറ്റ് ഒരു നല്ല പരീക്ഷണസെറ്റ് ഉണ്ടോ എന്ന് നിരീക പ്രത്യേകിച്ച് നിയന്ത്രിക്കപ്പെട്ട ദുര്\u200dമാറ്റങ്ങള്\u200d പ്രയോഗിക്കുന്നത് വിശാലമായി ഉപയോഗിക്കുന്ന ബെന്\u200dമാര്\u200dക്കുകള്\u200dക്കും മാറ്റുന് If model accuracy on the corrupted data remains high, then the dataset is likely to contain statistical biases and artefacts that guide prediction.  പ്രത്യേകിച്ച്, മോഡലിലെ വലിയ തെളിവുകളില്\u200d വലിയ കുറയ്ക്കുന്നത് മാതൃകങ്ങളുടെ കാരണമായ കഴിവുകള്\u200dക്ക് യഥാര്\u200dത്ഥ ഡാറ്റാ അതുകൊണ്ട്, നമ്മുടെ പ്രൊദ്ദേശിക്കപ്പെട്ട നിയന്ത്രണം NLI ജോലികള്\u200dക്ക് ഉയര്\u200dത്തുന്ന വിവരങ്ങള്\u200d നിര്\u200dമ്മിക്', 'mk': 'Преобучените модели на нервен јазик даваат висока резултат на природната инференција на јазикот (NLI). Но дали всушност го разбираат значењето на процесираните секвенции, сé уште не е јасно. Предложуваме нов тест за дијагностика кој овозможува да се процени дали компјутерот на податоци претставува добар тест за проценка на способностите за разбирање на значењето на моделите. Специфично аплицираме контролирани корупциски трансформации на широко употребени референтни значки (МНЛИ и АНЛИ), кои вклучуваат отстранување на цели класи на зборови и честопати водат до несензични парови реченици. Ако точноста на моделот на корумпираните податоци остане висока, тогаш системот на податоци најверојатно ќе содржи статистички предрасуди и артефакти кои ги водат предвидувањата. Напротив, големото намалување на точноста на моделот покажува дека оригиналниот набор на податоци претставува соодветен предизвик за размислувачките способности на моделите. Затоа, нашите предложени контроли можат да служат како тест за несреќа за развој на висококвалитетни податоци за задачите на НЛИ.', 'mt': "Mudelli ta’ lingwa newrali mħarrġa minn qabel jagħtu prestazzjoni għolja fuq kompiti ta’ inferenza lingwistika naturali (NLI). Iżda jekk humiex fil-fatt jifhmux it-tifsira tas-sekwenzi pproċessati għadhom mhux ċari. We propose a new diagnostics test suite which allows to assess whether a dataset constitutes a good testbed for evaluating the models' meaning understanding capabilities.  Aħna applikaw speċifikament trasformazzjonijiet ikkontrollati tal-korruzzjoni għal punti ta’ riferiment użati ħafna (MNLI u ANLI), li jinvolvu t-tneħħija ta’ klassijiet sħaħ ta’ kliem u spiss iwasslu għal pari ta’ sentenzi mhux sensiċi. Jekk il-preċiżjoni tal-mudell tad-dejta korrotta tibqa’ għolja, imbagħad is-sett tad-dejta x’aktarx li jkun fih preġudizzji u artefatti statistiċi li jiggwidaw it-tbassir. Għall-kuntrarju, tnaqqis kbir fil-preċiżjoni tal-mudell jindika li s-sett tad-dejta oriġinali jipprovdi sfida xierqa għall-kapaċitajiet ta’ raġunament tal-mudelli. Għalhekk, il-kontrolli proposti tagħna jistgħu jservu bħala test ta’ inċidenti għall-iżvilupp ta’ dejta ta’ kwalità għolja għall-kompiti tal-NLI.", 'pl': 'Wstępnie przeszkolone modele języka neuronowego zapewniają wysoką wydajność w zakresie zadań wnioskowania języka naturalnego (NLI). Ale czy rzeczywiście rozumieją znaczenie przetwarzanych sekwencji jest wciąż niejasne. Proponujemy nowy zestaw testów diagnostycznych, który pozwala ocenić, czy zbiór danych stanowi dobry zestaw testowy do oceny możliwości rozumienia znaczenia modeli. Szczególnie stosujemy kontrolowane transformacje korupcyjne do powszechnie stosowanych standardów referencyjnych (MNLI i ANLI), które obejmują usunięcie całych klas słów i często prowadzą do niewrażliwych par zdań. Jeśli dokładność modelu uszkodzonych danych pozostaje wysoka, zbiór danych prawdopodobnie będzie zawierał uprzedzenia statystyczne i artefakty, które kierują się przewidywaniem. Odwrotnie, duży spadek dokładności modelu wskazuje, że oryginalny zestaw danych stanowi odpowiednie wyzwanie dla możliwości rozumowania modeli. Dlatego proponowane przez nas kontrole mogą służyć jako test awaryjny do opracowywania wysokiej jakości danych dla zadań NLI.', 'mn': 'Өмнөх сургалтын мэдрэлийн хэл загварууд байгалийн хэл халдвар (NLI) үйл ажиллагаанд өндөр ажиллагааг өгдөг. Гэхдээ тэд үйлдвэрлэлтийн дарааллын утгыг ойлгож чадахгүй эсэх. Бид шинэ диагностикийн шинжлэх ухааны шинжлэх ухааны шинжлэх ухаан загварын ойлголтын чадварыг тодорхойлох боломжтой эсэхийг шалгаж өгдөг. Бид ялангуяа хэрэглэгдсэн бэрхшээлийн өөрчлөлт (MNLI болон ANLI) дээр ашигладаг. Энэ нь бүх үгийн ангиудыг устгах болон ихэвчлэн мэдрэмжгүй өгүүлбэртэй холбоотой. Хэрвээ буруу өгөгдлийн загварын зөв байдал өндөр байвал, өгөгдлийн санг тодорхойлох статистикийн хувьсгал болон артефактуудыг агуулах магадлалтай. Эсвэл загварын тодорхойлолтын том багасгал нь эхний өгөгдлийн суурь нь загварын ойлголтын чадварыг зөв сорилт өгдөг. Тиймээс бидний санал дэвшүүлсэн удирдах нь NLI ажлын өндөр чанартай мэдээллийг хөгжүүлэх шалгалт болно.', 'sr': 'Pre obučeni neuralni jezički modeli daju visoke funkcije na zadatke prirodnog jezika infekcije (NLI). Ali da li zapravo razumeju značenje procesiranih sekvencija još uvek nije jasno. Predlažemo novi apartman za testove dijagnostike koji omogućava da proceni da li komplet podataka predstavlja dobar testovan za procjenu mogućnosti razumevanja modela. Posebno se primjenjujemo kontrolirane transformacije korupcije na široko korištene kriterije (MNLI i ANLI), koje uključuju uklanjanje celih klasa riječi i često vode do neosenzičnih rečenica. Ako je modela tačnost korumpiranih podataka ostala visoka, onda će sigurno kompleta podataka sadržati statističke predrasude i artefakte koje vode predviđanje. Naprotiv, velika smanjenja tačnosti model a ukazuje na to da je originalna seta podataka pružala odgovarajući izazov za razumne mogućnosti modela. Stoga, naši predloženi kontroli mogu služiti kao test nesreće za razvoj visokokvalitetnih podataka za NLI zadataka.', 'no': 'Førehandsvis neuralspråk-modeller gjev høg utvikling på naturspråk-infeksjonar (NLI). Men om dei faktisk forstår betydninga av prosesserte sekvensane er fortsatt uventa. Vi foreslår eit nytt diagnostisk test suite som kan vurdere om ein datasett er ein god testtesttesttesten for å evaluera målsettingar for å forstå. Vi bruker spesielt kontrollerte korrupsjonstransformasjonar til breidde brukte benchmarker (MNLI og ANLI), som involverer fjerning av heile ordklassar og ofte fører til ikkje-sensiske setningar. Dersom modellen er nøyaktig på dei øydelagte data er høg, vil datasettet sannsynlegvis innehalda statistiske forsikt og artefaktar som hjelper framsyning. Ein stor nedgang i modellenøyaktighet tyder på at den opprinnelige datasettet tilbyr eit rett utfordring for modellen. Dette kan derfor våre foreslåande kontrollar gjere som krasjtest for å utvikla høg kvalitetsdata for NLI- oppgåver.', 'ro': 'Modelele de limbaj neural pre-instruite oferă performanțe ridicate în sarcinile inferenței limbajului natural (NLI). Dar dacă ei înțeleg de fapt sensul secvențelor procesate este încă neclar. Propunem o nouă suită de teste de diagnosticare care permite evaluarea dacă un set de date constituie un bun pat de testare pentru evaluarea capacităților de înțelegere a sensului modelelor. Aplicăm în mod specific transformări controlate de corupție la criteriile de referință utilizate pe scară largă (MNLI și ANLI), care implică eliminarea claselor întregi de cuvinte și adesea duc la perechi de propoziții non-sensibile. Dacă acuratețea modelului asupra datelor corupte rămâne ridicată, atunci setul de date este probabil să conțină prejudecăți statistice și artefacte care ghidează predicția. Dimpotrivă, o scădere mare a acurateții modelului indică faptul că setul de date original oferă o provocare adecvată pentru capacitățile de raționament ale modelelor. Prin urmare, controalele noastre propuse pot servi drept test de accident pentru dezvoltarea de date de înaltă calitate pentru sarcinile NLI.', 'si': 'ප්\u200dරධාන භාෂාව මොඩේල් ස්වභාවික භාෂාව ප්\u200dරශ්නය (NLI) වැඩක් ගැන හොඳ ප්\u200dරශ්නයක් දෙනවා. ඒත් ඔවුන් ඇත්තටම තේරුම් ගන්නේ නැත්නම් ප්\u200dරක්\u200dරියාත්මක විධානය තියෙන්නේ නැත්නම්. අපි අළුත් පරීක්ෂණ පරීක්ෂණ සූටියක් ප්\u200dරයෝජනය කරනවා ඒකෙන් දත්ත සූටියක් හොඳ පරීක්ෂණයක් තියෙන්න පුළුවන් ත අපි විශේෂයෙන්ම පාලනය කරලා පාලනය කරලා තියෙන්නේ ප්\u200dරමාණය විනාශය (MNLI සහ ANLI) විසින් භාවිත කරලා තියෙන්නේ, මුළු වචන වර්ගය වැරදි දත්තේ මදුල සැකසුම් හරියට ඉන්නවනම්, දත්ත සෙට් විශේෂයෙන් ස්ථාපනය සහ ආර්ටිෆේක්ටස් අනුවෙන් තියෙන විරුද්ධයෙන්, මොඩල් හරියට විශාල විශාලයක් පෙන්වන්න පුළුවන් දත්ත සෙට් මොඩල් ගේ හොයාගන්න පුළුවන්ට හොඳ ඉතින්, අපේ ප්\u200dරශ්නයක් පාලනය කරන්න පුළුවන් NLI වැඩේ විශේෂ දත්ත විස්තර කරන්න පුළුවන්.', 'sv': 'Förtränade neurala språkmodeller ger hög prestanda på naturliga språkinferens (NLI) uppgifter. Men om de faktiskt förstår innebörden av de bearbetade sekvenserna är fortfarande oklart. Vi föreslår en ny testsvit för diagnostik som gör det möjligt att bedöma om en datauppsättning utgör en bra testbädd för att utvärdera modellernas meningsförståelseförmåga. Vi tillämpar specifikt kontrollerade korruptionsomvandlingar på allmänt använda riktmärken (MNLI och ANLI), vilket innebär att hela ordklasser tas bort och ofta leder till icke-känsliga meningspar. Om modellnoggrannheten på de skadade data förblir hög, kommer datauppsättningen sannolikt att innehålla statistiska fördomar och artefakter som styr förutsägelsen. Omvänt indikerar en stor minskning av modellnoggrannheten att den ursprungliga datauppsättningen utgör en lämplig utmaning för modellernas resonemang kapacitet. Därför kan våra föreslagna kontroller fungera som ett krocktest för att utveckla högkvalitativ data för NLI-uppgifter.', 'ta': 'முன்பயிற்சிக்கப்பட்ட புதிய மொழி மாதிரி மாதிரிகள் இயல்பான மொழி பாதிப்பின் செயல்களில் அதிக செயல்பாட்டை  ஆனால் அவர்கள் உண்மையில் செயல்படுத்தப்பட்ட வரிசைகளின் அர்த்தம் இன்னும் தெளிவாக இருக்கும். நாம் ஒரு புதிய கண்டுபிடிப்பு சோதனை முறையை பரிந்துரைக்கிறோம். இது தகவல் அமைப்பு மாதிரிகளின் புரிய புரிந்து கொள்ளும் தன்மைகளை  We specifically apply controlled corruption transformations to widely used benchmarks (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs.  அழிக்கப்பட்ட தரவின் மாதிரி சரியான திட்டம் அதிகமாக இருந்தால், தரவு அமைப்பு புள்ளிவிவரம் மற்றும் மாதிரிகள் முன்னோட்டிக் மாதிரி சரியில் பெரிய குறைவு முறைமையில் குறைக்கும் முதல் தரவுத்தளத்தை மாதிரிகளின் காரணங்களுக்கு ஒரு சரியான சவா அதனால், எங்கள் பரிந்துரைக்கப்பட்ட கட்டுப்பாடுகள் NLI பணிகளுக்கு உயர்தரவு தரவை உருவாக்க ஒரு கிராஷ் சோதனையாக சேவை', 'so': 'Tusaalada afka neurada ee horay loo tababaray waxay sameyn karaan shaqooyin dheer oo ku saabsan cudurada afka dabiiciga ah (NLI). Laakin in ay xaqiiqdii garanayaan micnihiisa kooban weli ma ahan. Waxaynu horumarinaynaa koob cusub oo imtixaan ah, kaas oo qiimeyn kara in kooxda macluumaadku uu yahay imtixaan wanaagsan si loo qiimeeyo awoodda waxgarashada micneheeda. Si gaar ah ayaannu u isticmaalnaa beddelaadka korrupada si badan loo isticmaalo (MNLI iyo ANLI), kaas oo ku saabsan in la guuro fasalka oo dhan iyo inta badan in loo soo jeedo labada nooc oo aan la filin. Haddii tusaale rasmi ah oo ku qoran macluumaadka kharribka ah uu sii korodo, markaas waxaa suurtagal ah in lagu haysto qalabka statisticada iyo waxyaabaha lagu hago wax la sii sheego. Inta badnaan waxaa tusaale ahaan ku dhaca saxda tusaale ahaan oo aad u weyn ayaa tusaale ahaan u muujinaya in kaalmeeyaha asalka ah uu qalabka ku haysto dhibaato sax ah awoodda sababta ah. Sidaas darteed, kontrollyadayada la soo jeeday waxay u adeegi karaan imtixaan crash ah oo u horumarinta macluumaadka takhasuska sare ee shaqada NLI.', 'ur': 'اس سے پہلے تربیت کی نیورال زبان کی مدلکوں نے طبیعی زبان ایفارنس (NLI) کے کاموں پر اچھی فعالیت عطا کی ہے. لیکن کیا وہ واقعی سمجھتے ہیں کہ پردازش کی تعریف یہاں تک بھی غیر معلوم ہے۔ ہم نے ایک نوی دیاگنٹیکس تست سوئٹ کی پیشنهاد کرتا ہے جس کی اجازت دیتی ہے کہ ایک ڈیٹسٹ نے ایک اچھی تست لیا ہے کہ مدلکوں کی معنی سمجھنے کے قابلیت کا ارزش کرنے کے لئے۔ ہم مخصوص طور پر کنٹرول فساد کی تغییرات (MNLI اور ANLI) کو وسیع طور پر استعمال کیے جاتے ہیں جو تمام کلاس کو ہٹا دیتے ہیں اور اکثر بغیر سنسی کلاس کے جوڑوں کو لے جاتے ہیں۔ اگر فساد دیٹا پر موڈل دقیق رہے تو ڈاٹ سٹ ایسے ہو سکتا ہے کہ اسٹیسٹ ایسٹیسٹ کے سامنے اور آرٹیفٹ کے سامنے موجود ہوں جو پیش بینی کی راہ دکھاتے ہیں. مخالف طرح، ایک بڑی کمی موڈل دقیق کی نشان دیتا ہے کہ اصلی ڈاٹ سٹ موڈل کے منطقی قابلیت کے لئے ایک اچھی چال پیش کرتا ہے. لہٰذا، ہماری پیشنهاد کی کنٹرول NLI تاسکیوں کے لئے اچھی کیفیت ڈیٹا ایجاد کرنے کے لئے کرسکتی ہے۔', 'vi': 'Những mô hình thần kinh được đào tạo trước cung cấp năng lượng cao cho các công việc ngụ ngôn tự nhiên. Nhưng vẫn chưa rõ họ có hiểu được ý nghĩa của dãy diễn được xử lý hay không. Chúng tôi đề xuất một phòng thí nghiệm chẩn đoán mới cho phép đánh giá xem một bộ dữ liệu có thể là một bộ thử tốt để đánh giá khả năng hiểu biết ý nghĩa của các mẫu. Chúng tôi đặc biệt áp dụng sự biến đổi tham nhũng bị kiểm soát với những tiêu chuẩn được sử dụng rộng rãi (MJ và ANLI), gồm việc loại bỏ to àn bộ các lớp từ và thường dẫn đến các cặp án chưa nhạy cảm. Nếu độ chính xác mô hình trên dữ liệu bị hỏng vẫn còn cao, thì bộ dữ liệu có thể chứa các giả lập và các động vật hướng dẫn dự đoán. Trái lại, một sự giảm chính xác mô- đun lớn cho thấy bộ dữ liệu gốc mang lại một thách thức thích đáng cho khả năng lập trình của các mô-đun. Do đó, những sự kiểm tra như thế này có thể là một thử nghiệm rơi để phát triển dữ liệu chất lượng cao cho hoạt động NIL.', 'uz': "Taʼminlovchi neyron tili modellari tabiiy tilning vazifalari (NLI) vazifalarini eng yuqori ishlaydi. Lekin ularning aslida harakat freymning ma'nosini tushunishi mumkin. Biz yangi diagnostik sinov usulini talab qilamiz. Bu maʼlumot sahifa modelning imkoniyatlarini qiymatish uchun yaxshi tizimni yaratishga ruxsat beradi. Biz oddiy boshqaruv bir qanchalik bir so'zlarni olib tashlash va oddiy so'zlarni butun sinfni olib tashlash va odatda hech qanday o'zgarishlarni qo'llash mumkin. @ info Name Hence, our proposed controls can serve as a crash test for developing high quality data for NLI tasks.", 'da': 'Forududdannede neurale sprogmodeller giver høj ydeevne på naturlige sproginference (NLI) opgaver. Men om de rent faktisk forstår betydningen af de behandlede sekvenser er stadig uklart. Vi foreslår en ny diagnostisk testpakke, der gør det muligt at vurdere, om et datasæt udgør et godt testbed til vurdering af modellernes betydningsforståelsesevner. Vi anvender specifikt kontrollerede korruptionstransformationer på bredt anvendte benchmarks (MNLI og ANLI), som indebærer fjernelse af hele ordklasser og ofte fører til ikke-sensitive sætningspar. Hvis modelnøjagtigheden på de beskadigede data forbliver høj, vil datasættet sandsynligvis indeholde statistiske fordele og artefakter, der styrer forudsigelse. Omvendt indikerer et stort fald i modelnøjagtigheden, at det oprindelige datasæt udgør en ordentlig udfordring for modellernes ræsonnement evner. Derfor kan vores foreslåede kontroller tjene som en crash test til udvikling af data af høj kvalitet til NLI-opgaver.', 'hr': 'Preobučeni modeli neuralnog jezika daju visoke funkcije na zadatke prirodne infekcije jezika (NLI). Ali da li zapravo razumiju značenje obrađenih sekvencija još uvijek nije jasno. Predlažemo novi apartman za testove dijagnostike koji omogućava procijeniti da li komplet podataka predstavlja dobar testovan za procjenu mogućnosti razumijevanja modela. Posebno se primjenjujemo kontrolirane transformacije korupcije na široko korištene kriterije (MNLI i ANLI), koje uključuju uklanjanje cijelih klasa riječi i često dovede do nesumjetne rečenice. Ako je modela preciznost korumpiranih podataka ostala visoka, onda će sigurno sadržati statističke predrasude i artefakte koje vode predviđanje. Naprotiv, velika smanjenja preciznosti model a ukazuje na to da je originalna seta podataka pružala odgovarajući izazov mogućnosti razumljivanja modela. Stoga, naši predloženi kontroli mogu služiti kao test nesreće za razvoj visokokvalitetnih podataka za zadatak NLI-a.', 'de': 'Vortrainierte neuronale Sprachmodelle liefern eine hohe Leistung bei Natural Language Inference (NLI)-Aufgaben. Ob sie die Bedeutung der verarbeiteten Sequenzen tatsächlich verstehen, ist jedoch noch unklar. Wir schlagen eine neue Diagnosetestsuite vor, mit der beurteilt werden kann, ob ein Datensatz ein gutes Testfeld für die Bewertung der Bedeutungsverständnisse der Modelle darstellt. Wir wenden gezielt kontrollierte Korruptionstransformationen auf weit verbreitete Benchmarks (MNLI und ANLI) an, die ganze Wortklassen entfernen und oft zu unsensiblen Satzpaaren führen. Wenn die Modellgenauigkeit der beschädigten Daten hoch bleibt, enthält der Datensatz wahrscheinlich statistische Verzerrungen und Artefakte, die die Vorhersage leiten. Umgekehrt deutet eine große Abnahme der Modellgenauigkeit darauf hin, dass der ursprüngliche Datensatz eine angemessene Herausforderung für die Argumentationsfähigkeit der Modelle darstellt. Daher können unsere vorgeschlagenen Kontrollen als Crashtest für die Entwicklung hochwertiger Daten für NLI-Aufgaben dienen.', 'bg': 'Предварително обучените невронни езикови модели дават висока производителност при задачи по естествен език (НЛИ). Но дали те наистина разбират значението на обработените последователности все още не е ясно. Предлагаме нов комплект тестове за диагностика, който позволява да се прецени дали даден набор от данни представлява добро тестово поле за оценка на способностите за разбиране на значението на моделите. Ние специално прилагаме контролирани корупционни трансформации към широко използваните показатели (които включват премахване на цели класове думи и често водят до нечувствителни двойки изречения. Ако точността на модела на повредените данни остава висока, тогава наборът от данни вероятно ще съдържа статистически отклонения и артефакти, които ръководят прогнозирането. Обратно, голямото намаляване на точността на модела показва, че оригиналният набор от данни осигурява подходящо предизвикателство за способностите на моделите за разсъждение. Следователно, предложените от нас контроли могат да служат като тест за катастрофа за разработване на висококачествени данни за задачите на НЛИ.', 'nl': 'Vooropgeleide neurale taalmodellen leveren hoge prestaties bij Natural Language Inference (NLI)-taken. Maar of ze de betekenis van de verwerkte sequenties daadwerkelijk begrijpen is nog onduidelijk. We stellen een nieuwe diagnostische testsuite voor waarmee kan worden beoordeeld of een dataset een goed testbed is voor het evalueren van de betekenisbegripmogelijkheden van de modellen. We passen gecontroleerde corruptie transformaties specifiek toe op veelgebruikte benchmarks (MNLI en ANLI), waarbij hele woordklassen worden verwijderd en vaak leiden tot niet-sensuele zinsparen. Als de modellnauwkeurigheid van de beschadigde gegevens hoog blijft, bevat de dataset waarschijnlijk statistische biases en artefacten die de voorspelling leiden. Omgekeerd wijst een grote afname in de nauwkeurigheid van het model erop dat de oorspronkelijke dataset een behoorlijke uitdaging vormt voor de redeneringsmogelijkheden van de modellen. Daarom kunnen onze voorgestelde controles dienen als een crashtest voor het ontwikkelen van hoogwaardige gegevens voor NLI-taken.', 'sw': "Mfano wa lugha za afya zilizojifunza kabla hutoa ufanisi mkubwa wa maambukizi ya lugha asili (NLI). Lakini kama wanaelewa maana ya mfululizo unaofanywa bado si wazi. Tunazipendekeza kituo kipya cha uchunguzi kinachoruhusu kutathmini kama seti ya taarifa inaleta mtihani mzuri ili kutathmini uwezo wa kuelewa. Tunatumia mabadiliko yanayodhibitiwa na rushwa kwa kutumia bendera mbalimbali (MNLI na ANLI), ambayo yanahusisha kuondoa darasa zote za maneno na mara nyingi huwapelekea wanaume wasio na hisia. Kama uhakika wa mifano juu ya data zilizorushwa bado unabaki kuwa juu, basi kituo cha takwimu kinaweza kuwa na upendeleo wa takwimu na viumbe vinavyoongoa utabiri. Inversely, a large decrease in model accuracy indicates that the original dataset provides a proper challenge to the models' reasoning capabilities.  Kwa hiyo, udhibiti wetu unapendekezwa unaweza kutumika kama jaribio la ajali kwa ajili ya kuendeleza data za kiwango kikubwa kwa kazi za NLI.", 'tr': 'Öňki bilinmeli näyral dil nusgalary tebigat dilleriniň azalyşygy (NLI) täzeliklerinde ýokary ukyp edýär. Emma olar aslynda işleýän hadylaryň nähili düşünmeseleri ýok. Biz täze bir dijagnostik testi takmynany teklif edip görýäris ki, bir veri seti modelleriň anlama kapasitelerini deňlemek üçin gowy testi diýip çalyşyrýar. Biz adatça ullanýan benchmarklara (MNLI we ANLI) kontrol edilen korupçylyk üýtgewlerini uygulapdyk. Bu käwagt bütin söz klaslaryny çykaryp gidýär we köplenç duýumsuz sözlem çiftlere ýüz tutýar. Eger korrup maglumatyň dakyklygyny örän ýokary bolsa, maglumaty ýygnak bilen statistik biçimleri we artefaktlary barlap biler. Munuň tersine, modelleriň dogrylygyny azaltmak üçin original veri setiriniň nusgasyna düzgün kynçylyk berer. Şonuň üçin biziň teklip eden kontrollerimiz NLI işleri üçin ýokary kalitede maglumatlary geliştirmek üçin ýarakma testisi bolup biler.', 'ko': '사전 훈련을 거친 신경언어모델은 자연언어추리(NLI) 임무에서 높은 성능을 가지고 있다.그러나 그들이 처리한 서열의 의미를 정말 이해했는지는 아직 분명하지 않다.우리는 새로운 진단 테스트 세트를 제시했다. 이 세트는 평가 데이터 집합이 평가 모델의 의미 이해 능력을 구성하는 좋은 테스트 플랫폼이 되는지 여부를 허용한다.우리는 광범위하게 사용되는 기준 테스트(MNLI와 ANLI)에 대해 제어된 손상 변환을 적용한다. 이것은 전체 단어의 삭제와 관련이 있고 비감각적인 문장의 쌍을 초래할 수 있다.만약 파손 데이터의 모델 정밀도가 여전히 매우 높다면 데이터 집합은 예측을 지도하는 통계 편차와 인공 제품을 포함할 수 있다.반면 모델의 정밀도가 크게 떨어진 것은 원시 데이터 집합이 모델의 추리력에 적당한 도전을 했다는 것을 보여준다.따라서 우리가 제시한 컨트롤러는 NLI 작업을 위해 고품질 데이터를 개발하는 붕괴 테스트로 사용할 수 있다.', 'fa': 'مدل\u200cهای زبان عصبی پیش آموزش داده شده\u200cاند که عملکرد بالا بر کار\u200cهای زبان طبیعی (NLI) انجام می\u200cدهند. اما آیا آنها واقعا معنی ترکیب\u200cهای پردازی را درک می\u200cکنند هنوز مشخص نیست. ما یک سایت تست تشخیص جدید را پیشنهاد می\u200cکنیم که اجازه می\u200cدهد تا بررسی کند که آیا یک مجموعه داده\u200cها یک تست خوبی برای ارزیابی توانایی درک معنی مدل است. ما مخصوصاً تغییرات فساد کنترل را به نقشه\u200cهای عمومی استفاده می\u200cکنیم (MNLI و ANLI) که شامل حذف کلاس کلاس\u200cهای کلمه\u200cها است و اغلب به جفت\u200cهای مجازات غیر حسی می\u200cشوند. اگر دقیق مدل روی داده های تباه باقی مانده باشد، پس مجموعه داده ها احتمالاً شامل تغییرات آمار و آرتیفات\u200cها است که پیش\u200cبینی را هدایت می\u200cکنند. در عوض، کاهش بزرگی دقیق مدل نشان می دهد که مجموعه داده های اصلی یک چالش مناسب برای توانایی منطقی مدل را پیشنهاد می دهد. بنابراین کنترل پیشنهاد ما می تواند به عنوان آزمایش تصادف برای توسعه داده های کیفیت بالا برای کار NLI خدمت کنند.', 'sq': 'Modelet e paratrajnuara të gjuhës nervore japin performancë të lartë në detyrat e inferencës natyrore të gjuhës (NLI). Por nëse ata kuptojnë kuptimin e sekuencave të procesuara është ende e paqartë. Ne propozojmë një grup të ri diagnostike që lejon të vlerësojë nëse një grup i të dhënave përbën një vend të mirë të testuar për vlerësimin e aftësive të kuptimit të kuptimit të modeleve. We specifically apply controlled corruption transformations to widely used benchmarks (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs.  Nëse saktësia e modelit në të dhënat e korruptuara mbetet e lartë, atëherë grupi i të dhënave ka gjasa të përmbajë paragjykime statistike dhe artefakte që udhëzojnë parashikimin. Përkundrazi, një rënie e madhe në saktësinë e modelit tregon se kompleti origjinal i të dhënave ofron një sfidë të përshtatshme ndaj aftësive arsyetimi të modeleve. Kështu, kontrolli ynë i propozuar mund të shërbejë si një test përplasjeje për zhvillimin e të dhënave të cilësisë së lartë për detyrat e NLI.', 'af': "Vorige onderwyse neuraltaal modele gee hoë prestasie op natuurlike taal inferensie (NLI) taak. Maar of hulle eintlik verstaan die betekening van die verwerking sekwensies nog onbekend is. Ons voorstel 'n nuwe diagnostike toets suite wat toelaat om te vure of 'n datastel 'n goeie toets bedoel is vir die evaluering van die modele se betekening verstaan kapasiteite. Ons het spesifieke beheerde korrupsietransformasies toegewend na baie gebruikte benchmarke (MNLI en ANLI), wat insluit die verwydering van hele woorde klasse en dikwels lei na non-sensieële setpaar. As model presies op die korrupte data bly hoog, dan is die datastel waarskynlik na bevat statistiese biases en artefakte wat voorskou gids. Die oorspronklike datastel verskaf 'n groot verdubbeling in model presies wat die oorspronklike datastel verskaf 'n regte uitdrukking aan die modelle se redekening kapasiteite. Daarom kan ons voorgestelde kontroles as 'n krag toets dien vir die ontwikkeling van hoë kwaliteit data vir NLI taak.", 'id': 'Model bahasa saraf yang dilatih sebelumnya memberikan prestasi tinggi pada tugas bahasa alami (NLI). But whether they actually understand the meaning of the processed sequences is still unclear.  Kami mengusulkan sebuah suite tes diagnostik baru yang memungkinkan untuk menilai apakah set data merupakan tempat yang baik untuk mengevaluasi kemampuan pemahaman arti model. Kami secara khusus menerapkan transformasi korupsi terkendali ke benchmarks yang sangat digunakan (MNLI dan ANLI), yang melibatkan menghapus seluruh kelas kata dan sering menyebabkan pasangan kalimat yang tidak sensik. Jika akurasi model pada data korupsi tetap tinggi, maka dataset mungkin mengandung biases statistik dan artefakta yang memimpin prediksi. Sebaliknya, penurunan besar dalam akurasi model menunjukkan bahwa set data asli menyediakan tantangan yang tepat untuk kemampuan pemikiran model. Oleh karena itu, kontrol kami yang diusulkan dapat melayani sebagai tes kecelakaan untuk mengembangkan data kualitas tinggi untuk tugas NLI.', 'am': 'የቀድሞው የደዌብ ቋንቋ ሞዴሌዎች የፍጥረቱ ቋንቋ ድህነት (NLI) ስራ ላይ ከፍ ከፍ የሚያደርጉ ናቸው፡፡ ነገር ግን የሥርዓት ግንኙነት ማድረግ ምንም አይችልም፡፡ አዲስ ዶሴ ፍትሕት መፍትወት እናስገድዳለን፡፡ በተለየንም የሥርዓት ሁኔታ ለውጦችን በመስጠት የሚጠቀሙትን የቃላትን ክፍሎች ማነሳቅስ እና ብዙ ጊዜም የስህተት ግንኙነት አካባቢ እና የአንሊ (MNLI እና ANLI) የሚጠይቅ ነው፡፡ የረከሰው ዳታዎች ላይ የሞዴል እርግዝነት ቢኖር የዳታ ሰርቨሮች ቅድሚያ የሚለውጥ የstatistical ሁኔታዎችን እና አርጤክስቶችን በመቀበል ይችላል፡፡ በጥያቄ፣ በሞዴል እርግጠኛ የሚያሳፍር ትልቅ የዳታ ማዕከላዊው የሞዴል አካላዊ ችሎታዎችን የሚያሳውቅ ነው፡፡ ስለዚህ የተዘጋጀው ግንኙነታችን የNLI ስርዓቶች ከፍተኛ ጥሩ ዳታዎችን ለማግኘት የcrash ፈተና እንዲያገለግል ይችላል።', 'hy': 'Pre-trained neural language models give high performance on natural language inference (NLI) tasks.  Բայց արդյոք նրանք իրականում հասկանում են վերլուծված հաջորդականությունների իմաստը դեռևս անհասկանալի է: Մենք առաջարկում ենք նոր ախտորոշման փորձարկումների համակարգ, որը թույլ է տալիս գնահատել, թե արդյոք տվյալների համակարգը լավ փորձարկումներ է կազմում մոդելների իմաստալից հասկանալու ունակությունների գնահատ Մենք հատկապես կիրառում ենք կոռուպցիայի վերահսկվող վերափոխությունները լայնորեն օգտագործված համեմատային նպատակների (MNSI և Անլիի) համար, որոնք ներառում են բառերի ամբողջ դասերը հեռացնելը և հաճախ հանգեցնում են ոչ զգայական նախադասու Եթե կոռուպված տվյալների մոդելի ճշգրտությունը շարունակում է բարձր լինել, ապա տվյալների համակարգը հավանաբար կպարունակի վիճակագրական կողմնականություններ և արտեֆեկտներ, որոնք ուղղորդում են կանխատեսումը: Փոխարենը, մոդելի ճշգրտության մեծ նվազում ցույց է տալիս, որ սկզբնական տվյալների համակարգը հարմար մարտահրավեր է տալիս մոդելների մտածողական հնարավորություններին: Այսպիսով, մեր առաջարկած կառավարումը կարող է օգտագործվել որպես անհաջողության թեստ, որպեսզի զարգացնենք բարձր որակային տվյալներ ՆԼԻ-ի առաջադրանքների համար:', 'bs': 'Preobučeni modeli neuralnog jezika daju visoke funkcije na zadatke prirodnog jezika infekcije (NLI). Ali da li zapravo razumiju značenje procesiranih sekvencija još uvijek nije jasno. Predlažemo novi apartman za testove dijagnostike koji omogućava procijeniti da li komplet podataka predstavlja dobar testovan za procjenu mogućnosti razumijevanja modela. Posebno se primjenjujemo kontrolirane transformacije korupcije na široko korištene kriterije (MNLI i ANLI), koje uključuju uklanjanje cijelih klasa riječi i često dovede do nedosetljivih parova rečenica. Ako je modela preciznost korumpiranih podataka ostala visoka, onda će sigurno kompleta podataka sadržati statističke predrasude i artefakte koje vode predviđanje. Naprotiv, velika smanjenja preciznosti model a ukazuje na to da je originalna seta podataka pružala odgovarajući izazov mogućnosti razumljivanja modela. Stoga, naše predložene kontrole mogu služiti kao test nesreće za razvoj visokokvalitetnih podataka za zadatak NLI-a.', 'az': 'Əvvəlcə təhsil edilmiş nöral dil modelləri təbiətli dil infeksiyonu (NLI) işlərində yüksək performans verir. Lakin onlar həqiqətən də işlədilən sıraların anlamını anlamırlar. Biz yeni diagnostik testi suitini təklif edirik ki, bir veri seti modellərin anlama kapasitələrini değerləşdirmək üçün yaxşı testlər yaratdığını müəyyən edər. Biz müəyyən edilmiş korkunç dəyişikliklərini geniş istifadə edilmiş benchmarklara (MNLI və ANLI) uygulayırıq. Bunlar bütün sözlər dəyişikliklərini silmək və çox sıxıntılı cümlələr çiftlərə yol açar. Əgər fəsad verilən məlumatların modellərinin doğruluğu yüksək qalsa, verilən qurğuları tədbir göstərən statistik tədbirləri və artefaktlarını daxil edə bilər. Əslində, modellərin doğruluğunun böyük düşüşünü göstərir ki, orijinal verilən qurğunun modellərin razılıq qabiliyyətlərinə uyğun bir çətinlik verir. Beləliklə, təklif edilmiş kontrollərimiz NLI işləri üçün yüksək kaliteli məlumatları təhsil etmək üçün istifadə edə bilər.', 'bn': 'Pre-trained neural language models give high performance on natural language inference (NLI) tasks.  কিন্তু তারা সত্যিই বুঝতে পারে যে প্রক্রিয়ার সেকেন্সের মানে এখনো অজ্ঞ। আমরা একটি নতুন ডিয়াগনিস্টিক্স পরীক্ষার স্যুট প্রস্তাব করছি যা মূল্য করতে পারে ডেটাসেটের মানে বুঝতে পারে কি ভালো পরীক্ষা করা হয়। আমরা বিশেষ করে নিয়ন্ত্রিত দুর্নীতির পরিবর্তন প্রয়োগ করি ব্যাপকভাবে ব্যবহার করা বেনম্যাক্ক (এমএনলি এবং এনলি), যা পুরো শব্দ শ্রেণীকে সরিয়ে ন যদি দুর্নীতিগ্রস্ত তথ্যের মোডেলের সঠিকভাবে উচ্চতা থাকে, তাহলে ডাটাসেটটি পরিসংখ্যানের বৈষম্য এবং প্রতিষ্ঠানের মধ্য পরিবর্ততনভাবে মডেলের সঠিকভাবে একটি বিশাল কমে যাচ্ছে যে মূল ডাটাসেট মডেলের কারণের ক্ষমতার জন্য একটি সঠিক চ্যালেঞ্জ প্ তাই আমাদের প্রস্তাবিত নিয়ন্ত্রণ এনলির কাজের উচ্চ মানের তথ্য উন্নয়নের জন্য একটি ক্রাশ পরীক্ষা হিসেবে সেবা করা য', 'ca': "Els models de llenguatges neurals pré-entrenats donen un gran rendiment en les tasques de inferència de llenguatges naturals (NLI). Però si realment entenen el significat de les seqüències processades encara no és clar. Proposem una nova suite de tests de diagnòstics que permet evaluar si un conjunt de dades constitueix un bon banc de prova per evaluar les capacitats d'entendre el significat dels models. Aplicam específicament transformacions de corrupció controlades a punts de referència generalitzats (MNLI i ANLI), que impliquen eliminar classes de paraules senceres i sovint portan a parells de frases no sensuals. Si la precisió del model de les dades corruptes segueix elevada, és probable que el conjunt de dades contenga biases estadístics i artefactes que guien la predicció. Inversament, una gran disminució de la precisió del model indica que el conjunt de dades original ofereix un repte adequat a les capacitats de raonament dels models. Per tant, els nostres controls proposats poden servir de test d'accident per desenvolupar dades d'alta qualitat per a les tasques de l'INL.", 'et': 'Eelnevalt koolitatud närvikeele mudelid annavad looduskeele järelduste (NLI) ülesannetele suure jõudluse. Kuid kas nad tegelikult mõistavad töödeldud jadade tähendust, on ikka veel ebaselge. Pakume välja uue diagnostika testikomplekti, mis võimaldab hinnata, kas andmekogum on hea katseplats mudelite tähenduste mõistmise võimekuse hindamiseks. Konkreetselt rakendame kontrollitud korruptsioonimuundusi laialdaselt kasutatavatele võrdlusnäitajatele (MNLI ja ANLI), mis hõlmavad tervete sõnaklasside eemaldamist ja viivad sageli mittetundlike lausepaarideni. Kui vigastatud andmete mudeli täpsus jääb kõrgeks, sisaldab andmekogum tõenäoliselt statistilisi kallutusi ja esemeid, mis juhivad prognoosimist. Vastupidiselt näitab mudeli täpsuse suur vähenemine, et algne andmekogum pakub mudelite arutlusvõimekusele õiget väljakutset. Seega võivad meie kavandatud kontrollid olla kokkupõrketest, et töötada välja kvaliteetseid andmeid NLI ülesannete jaoks.', 'fi': 'Esikoulutetut neurokielimallit antavat korkean suorituskyvyn luonnollisen kielen päättelytehtävissä (NLI). Mutta ymmärtävätkö he prosessoitujen sekvenssien merkityksen, on vielä epäselvää. Ehdotamme uutta diagnostiikkatestikokonaisuutta, jonka avulla voidaan arvioida, muodostaako aineisto hyvän testialustan mallien merkityksen ymmärtämiseen. Sovellamme kontrolloituja korruption muunnoksia laajalti käytettyihin vertailuarvoihin (MNLI ja ANLI), jotka sisältävät kokonaisten sanaluokkien poistamisen ja johtavat usein epäherkiin lausepareihin. Jos mallin tarkkuus vioittuneessa datassa pysyy korkeana, aineistossa on todennäköisesti tilastollisia harhaluuloja ja ennusteita ohjaavia artefakteja. Mallin tarkkuuden suuri lasku puolestaan osoittaa, että alkuperäinen aineisto on oikea haaste mallien päättelykyvylle. Siksi ehdotetut kontrollit voivat toimia törmäystestinä korkealaatuisen datan kehittämiseksi NLI-tehtäviin.', 'cs': 'Předškolené neuronové jazykové modely poskytují vysoký výkon při úlohách inference přirozeného jazyka (NLI). Ale zda skutečně chápou význam zpracovaných sekvencí, je stále nejasné. Navrhujeme novou sadu diagnostických testů, která umožňuje posoudit, zda datová sada představuje dobré testovací místo pro hodnocení schopností porozumění významu modelů. Konkrétně aplikujeme řízené korupční transformace na široce používané benchmarky (MNLI a ANLI), které zahrnují odstranění celých slovních tříd a často vedou k necitlivým větovým párům. Pokud přesnost modelu poškozených dat zůstává vysoká, pak datová sada pravděpodobně obsahuje statistické předsudky a artefakty, které řídí predikci. Naopak, velký pokles přesnosti modelu naznačuje, že původní datová sada představuje správnou výzvu pro schopnosti uvažování modelů. Proto naše navržené ovládací prvky mohou sloužit jako nárazový test pro vývoj vysoce kvalitních dat pro úkoly NLI.', 'jv': 'model sing prelimin maneh da bukane nggawe barang nggawe luwih nggambar aturan (NLI) Nanging, yo uwis mesthi luwih ngerasai perkara sing dikarepaké sekondirne kuwi ora bisa pasar. Awak dhéwé ngerasah sistem anyar nggo diangéntike nggawe barang beraksi data set Anyone special Laptop" and "Desktop drawable-action Kaya, awak dhéwé nggunakake sistem iso ngejaraké tentang kanggo nggawe data sing wis ngawe kaliwat kanggo nggawe task NLI.', 'he': 'דוגמני שפת עצבית מאומנים מראש נותנים ביצועים גבוהים על משימות השפה הטבעית (NLI). אבל אם הם באמת מבינים את המשמעות של השורות המעבדות עדיין לא ברורות. אנו מציעים סוויטה חדשה לבדיקות אבחנה שמאפשרת להעריך אם קבוצת נתונים מייצרת מיטה טובה לבדיקות יכולות הבנה של המודלים. אנו מפעילים במיוחד שינויים שולטים בשחיתות למונחים משתמשים רחוקים (MNLI ו ANLI), שכוללים להסיר שיעורי מילים שלמות ולפעמים מובילים לזוגי משפטים לא רגשיים. אם מדויקת הדוגמא על הנתונים המושחתים נשארת גבוהה, אז סביר להניח שהקבוצת הנתונים תכיל חיזוקים סטטיסטיים וארטפקטים שמדריכים את החזוי. ההפך, שינוי גדול בדיקת הדוגמא מצביע על כך שהקבוצת המידע המקורית מספקת אתגר מתאים ליכולות ההיגיון של הדוגמאים. לכן, השליטה המוצעת שלנו יכולה לשמש בתור מבחן התרסקות לפיתוח נתונים איכות גבוהה למשימות NLI.', 'sk': 'Predhodno usposobljeni modeli nevronskega jezika zagotavljajo visoko učinkovitost pri nalogah sklepanja naravnega jezika (NLI). Vendar, ali dejansko razumejo pomen obdelanih zaporedij, še vedno ni jasno. Predlagamo nov diagnostični testni paket, ki omogoča oceno, ali je nabor podatkov dobra testna plošča za ocenjevanje sposobnosti razumevanja pomena modelov. Kontrolirane korupcijske transformacije uporabljamo posebej za široko uporabljene referenčne vrednosti (MNLI in ANLI), ki vključujejo odstranitev celotnih razredov besed in pogosto vodijo do neobčutljivih stavkov. Če natančnost modela na poškodovanih podatkih ostaja visoka, bo nabor podatkov verjetno vseboval statistične pristranskosti in artefakte, ki vodijo napoved. Nasprotno pa veliko zmanjšanje natančnosti modela kaže, da prvotni nabor podatkov predstavlja ustrezen izziv za zmogljivost modelov. Zato lahko naše predlagane kontrole služijo kot preskus trka za razvoj visokokakovostnih podatkov za naloge NLI.', 'ha': "Modellun harshen na farko da aka yi wa aikin neural, za'a sami aikin yin sauri a kan aikin na'ura (NLI). Amma kõ da sun fahimta fassarar rayuwar da aka yi aiki, ba ta kasancẽwa baya ba. Tuna goyyar da wata jigon diagnostics don ya yarda ka ƙayyade ko an sami tsarin a danne da wata mai kyau dõmin ka ƙaddara abincin hankalin misãlai. Tuna amfani da canza za'a lissafa ɓarnata da aka yi controlled zuwa manyan kayan amfani da (MNLI da ANLI), wanda ke ƙunsa da tafiyar da kure duk maganar, kuma ko da yawa za'a shige nau'i biyu wanda ba'a sani ba. Idan tsari na motel a kan data da aka ƙẽtare ya ƙara, za'a yiwu ya ƙunsa da surar ƙayyade da matsayi wanda ke shiryar da shi. A kansa, ƙarami babba cikin tsari na motel yana nuna cewa, tsarin danne na farko yana samar da kima mai inganci zuwa abincin masu inganci na motel. Kayya, kandinmu da ake amfani da shi yana iya amfani da jarrabi mai ɓarna dõmin ya buɗe data masu girma wa aikin NLI.", 'bo': 'སྔོན་གྱི་སྔོན་འཛིན་གྱི་སྒེར་གྱི་ནུས་པ་གཟུགས་རིས་ཀྱི་དཔེ་དབྱིབས་ནི་natural language inference (NLI)བསམ་བློ་གཏོང་ཚད་ལ་མཐོ་ འོན་ཀྱང་། ཁོང་ཚོས་དངོས་ཡོད་ཐོག་པའི་གྲངས་རིམ་གྱི་གསལ་བཤད་ནི་ཡང་མིན་རྟོགས་པ་ཡིན། ང་ཚོས་རང་ཉིད་སྒྲིག་ཆ་གནད་ཅིག་ལྟ་ཞིབ་བྱེད་པའི་རྣམ་པ་གསར་བ་ཞིག་འཆར་ཡོད་མིན་ན། ང་ཚོས་དམིགས་གསལ་བཀལ་ནས་དབུས་བཟོ་བྱེད་ཀྱི་བཟོ་བཅོས་ལ་ལག་ལེན་འཐབ་ཡོད་པའི་benchmarks (MNLI་དང ANLI)ལ་གསལ་བརྗོད་ཀྱི་ནང་དུ་ཡོད་པའི་ཐ་སྙད་ག If model accuracy on the corrupted data remains high, then the dataset is likely to contain statistical biases and artefacts that guide prediction. ལྡོག་དང་འགྱུར་བ། མ་དབྱིབས་བདེན་ཚད་ལ་ཆུང་བའི་ཚད་གྱིས་ཐོག་མའི་གནས་སྟངས དེར་བརྟེན། ང་ཚོའི་དམིགས་འཛུགས་ཀྱི་ཚད་འཛིན་ནི་NLI བྱ་འགུལ་ལ་སྐྱོན་བརྟག་ཞིག་བྱེད་པའི་ལྟ་བུ་ཞིག'}
{'en': 'Towards cross-lingual application of language-specific PoS tagging schemes', 'fr': 'Vers une application multilingue de schémas de balisage des bons de commande spécifiques à la langue', 'pt': 'Para a aplicação multilíngue de esquemas de marcação PoS específicos de idioma', 'es': 'Hacia la aplicación multilingüe de esquemas de etiquetado de PoS específicos del idioma', 'ar': 'نحو تطبيق متعدد اللغات لأنظمة تعليم نقاط البيع الخاصة بلغة معينة', 'zh': '向特定语言 PoS 记方案语用', 'hi': 'भाषा-विशिष्ट पीओएस टैगिंग योजनाओं के क्रॉस-भाषी अनुप्रयोग की ओर', 'ja': '言語固有のPoSタグ付けスキームのクロスリンガルアプリケーションに向けて', 'ru': 'На пути к межъязыковому применению схем тегов PoS для конкретных языков', 'ga': 'I dtreo chur i bhfeidhm tras-teangach scéimeanna clibeála PoS a bhaineann go sonrach le teanga', 'ka': 'სიტყვის სპექტიფიკალური POS- სქემების მრავალური პროგრამის მიხედვით', 'el': 'Προς τη διασυνοριακή εφαρμογή γλωσσικών συστημάτων επισήμανσης PoS', 'hu': 'A nyelvspecifikus PoS-címkézési rendszerek többnyelvű alkalmazása felé', 'it': "Verso un'applicazione multilingue di sistemi di etichettatura PoS specifici per le lingue", 'kk': 'Тілді ерекше PoS тегтер сұлбаларының бірнеше тілді қолданбаларына қарсы', 'mk': 'Надвор кон прекујазичната апликација на јазички специфични шеми за означување PoS', 'lt': 'Kalbėms skirtų PoS ženklinimo sistemų tarpkalbinio taikymo link', 'mn': 'Холбоо-тодорхойлолтой PoS-тодорхойлолтын төлөвлөгөөний олон хэлний хэрэглээ рүү', 'ms': 'Ke arah aplikasi saling bahasa skema penandaan PoS spesifik bahasa', 'ml': 'ഭാഷ- പ്രത്യേകിച്ച പോസ് ടാഗിങ്ങ് സ്ക്രീമുകളുടെ മുകളിലേക്കു് ക്രിസ്ലാങ്ങ് പ്രയോഗം', 'mt': 'Lejn applikazzjoni translingwistika ta’ skemi ta’ tikkettar PoS speċifiċi għall-lingwa', 'pl': 'W kierunku wielojęzycznego stosowania specyficznych dla języka systemów oznaczania PoS', 'ro': 'Către aplicarea translingvistică a schemelor de etichetare PoS specifice limbilor', 'sr': 'Prema međujezičkom primjenu šema označavanja PoS-a specifičnih jezika', 'si': 'භාෂාව-විශේෂ පොස් ටැග් ස්කම්ස් වලින් භාෂාව භාෂාව', 'no': 'Gå til fleirspråksprogram av språkspesifiske PoS-merkingsschema', 'so': 'Codsiga luqadaha kala duduwan ee qorshaha tagsiga PoS ee luuqadaha gaarka ah', 'sv': 'Mot flerspråkig tillämpning av språkspecifika PoS-märkningssystem', 'ta': 'மொழி- குறிப்பிட்ட போஸ் ஒட்டும் முறைமைகளின் மேல் மொழியின் பயன்பாடு', 'ur': 'زبان کے مختلف پوس ٹاگ سیمیوں کی مختلف زبان کی کاربرد کی طرف', 'uz': 'Name', 'vi': 'Để áp dụng ngôn ngữ khác nhau về phương pháp ám ảnh', 'bg': 'Към междуезично прилагане на специфични за езика схеми за маркиране на PoS', 'nl': "Naar een meertalige toepassing van taalspecifieke PoS-taggingschema's", 'da': 'Mod tværsproget anvendelse af sprogspecifikke PoS-mærkningsordninger', 'hr': 'Prema međujezičkom primjenu programa označavanja PoS-a specifičnih jezika', 'de': 'Auf dem Weg zur sprachübergreifenden Anwendung sprachspezifischer PoS-Tagging-Schemata', 'ko': '특정 언어의 어성 표기 방안을 위한 다중 언어 응용', 'fa': 'به سمت کاربرد متفاوت زبانی از نقشه\u200cهای نقشه\u200cهای نقاشی PoS خاص زبان', 'id': 'Menuju aplikasi saling bahasa skema penandaian PoS spesifik bahasa', 'tr': 'Diller takyklaýan PoS taýýarlama şemalarynyň cross-dilli uygulamasy üçin', 'af': 'Gaan teen kruistale toepassing van taal spesifieke PoS merking skeme', 'sw': 'Tokea matumizi ya lugha mbalimbali ya mipango ya viungo vya PoS', 'sq': 'Për zbatimin ndërgjuhësor të skemave specifike të etiketave PoS për gjuhën', 'am': 'ቋንቋ-ቋንቋ', 'hy': 'Ի ուղղություն PoS-ի լեզվին հատուկ տախտավորման ծրագրերի միջլեզվային կիրառումը', 'az': 'Dil m√ľxt…ôlif PoS etiketl…ôm…ô Ňü…ôkill…ôrinin √ßox dilli uyńüulamasńĪna t…ôr…ôf', 'bn': 'ভাষা নির্দিষ্ট পোস ট্যাগিং বিন্যাসের প্রোগ্রামের উপরে', 'bs': 'Prema međujezičkom primjenu šema označavanja PoS-a specifičnih jezika', 'cs': 'Ke vícejazyčnému uplatňování jazykově specifických PoS tagovacích schémat', 'ca': 'Towards cross-lingual application of language-specific PoS tagging schemes', 'et': 'Keelespetsiifiliste PoS märgistusskeemide keeleülene rakendamine', 'fi': 'Kohti kielikohtaisten PoS-merkintäjärjestelmien monikielistä soveltamista', 'jv': 'Tulung langkang banter-langgar aplikasi kanggo langgambar-langgar stir', 'ha': '@ action', 'sk': 'Za večjezično uporabo jezikovnih shem označevanja PoS', 'he': 'לכיוון שימוש דרך שפתיים של מערכות תוויות PoS ספציפיות לשפה', 'bo': 'སྐད་ཡིག་དམིགས་འཛུགས་ཅན་གྱི་PoS་མཚོན་རྟགས་ལས་འགུལ་གྱི་སྐད་ཆ་ལུགས་ཀྱི་ཉེར་སྤྱོད་ཀྱི་འགྲོ'}
{'en': 'We describe the process of conversion between the PoS tagging schemes of two languages, the Icelandic MIM-GOLD tagging scheme and the Faroese Sosialurin tagging scheme. These tagging schemes are functionally similar but use separate ways to encode fine-grained morphological information on tokenised text. As Faroese and Icelandic are lexically and grammatically similar, having a systematic method to convert between these two tagging schemes would be beneficial in the field of language technology, specifically in research on transfer learning between the two languages. As a product of our work, we present a provisional version of Icelandic corpora, prepared in the Faroese PoS tagging scheme, ready for use in cross-lingual NLP applications.', 'ar': 'نحن نصف عملية التحويل بين مخططات وضع العلامات PoS لغتين ، مخطط علامات MIM-GOLD الأيسلندي ونظام علامات Faroese Sosialurin. أنظمة وضع العلامات هذه متشابهة وظيفيًا ولكنها تستخدم طرقًا منفصلة لتشفير المعلومات المورفولوجية الدقيقة على النص المرمز. نظرًا لأن الفاروية والأيسلندية متشابهة معجميًا ونحويًا ، فإن وجود طريقة منهجية للتحويل بين هذين النظامين سيكون مفيدًا في مجال تكنولوجيا اللغة ، وتحديداً في البحث حول نقل التعلم بين اللغتين. كنتيجة لعملنا ، نقدم نسخة مؤقتة من المؤسسات الأيسلندية ، التي تم إعدادها وفقًا لنظام العلامات في Faroese PoS ، وهي جاهزة للاستخدام في تطبيقات البرمجة اللغوية العصبية متعددة اللغات.', 'fr': "Nous décrivons le processus de conversion entre les schémas de marquage PoS de deux langues, le schéma de marquage islandais MIM-GOLD et le schéma de marquage féroïen Sosialurin. Ces schémas de balisage sont fonctionnellement similaires mais utilisent des méthodes distinctes pour coder des informations morphologiques fines sur du texte tokenisé. Le féroïen et l'islandais étant similaires sur le plan lexical et grammatical, disposer d'une méthode systématique de conversion entre ces deux systèmes de marquage serait bénéfique dans le domaine de la technologie linguistique, en particulier dans la recherche sur l'apprentissage par transfert entre les deux langues. Dans le cadre de nos travaux, nous présentons une version provisoire des corpus islandais, préparés selon le schéma de marquage des PoS féroïens, prêt à être utilisé dans les applications de PNL multilingues.", 'pt': 'Descrevemos o processo de conversão entre os esquemas de marcação PoS de duas línguas, o esquema de marcação islandês MIM-GOLD e o esquema de marcação Faroese Sosialurin. Esses esquemas de marcação são funcionalmente semelhantes, mas usam maneiras separadas para codificar informações morfológicas refinadas em texto tokenizado. Como o feroês e o islandês são lexical e gramaticalmente semelhantes, ter um método sistemático para converter entre esses dois esquemas de marcação seria benéfico no campo da tecnologia linguística, especificamente na pesquisa sobre transferência de aprendizado entre as duas línguas. Como produto de nosso trabalho, apresentamos uma versão provisória de corpora islandês, preparado no esquema de marcação PoS das Ilhas Faroé, pronto para uso em aplicações de PNL multilíngue.', 'es': 'Describimos el proceso de conversión entre los esquemas de etiquetado PoS de dos idiomas, el esquema de etiquetado MIM-GOLD islandés y el esquema de etiquetado sosialurín feroés. Estos esquemas de etiquetado son funcionalmente similares, pero utilizan formas separadas para codificar información morfológica detallada en texto tokenizado. Como el feroés y el islandés son léxica y gramaticalmente similares, contar con un método sistemático para convertir entre estos dos esquemas de etiquetado sería beneficioso en el campo de la tecnología del lenguaje, específicamente en la investigación sobre la transferencia del aprendizaje entre los dos idiomas. Como producto de nuestro trabajo, presentamos una versión provisional de corpus islandeses, preparada en el esquema de etiquetado PoS de las Islas Feroe, lista para su uso en aplicaciones de PNL multilingües.', 'zh': '述二语者PoS表其转换,冰岛MIM-GOLD其方法罗语Socialolurin。 此方在功能上相似,而用异法编码记化本细粒度形息。 法罗语与冰岛语相似于词汇语法,故有一统之法,转换于语言之间,尤在两言之迁学。 为吾事者,建一冰岛语料库临时版本,当语料库法罗群岛PoS标记方案所备,可施于跨语NLP应用程序。', 'ja': 'アイスランド語のMIM - GOLDタグ付けスキームとフェロー語のSosialurinタグ付けスキームの2つの言語のPoSタグ付けスキーム間の変換プロセスについて説明します。これらのタグ付けスキームは機能的に類似していますが、別々の方法を使用して、トークン化されたテキスト上の微細な形態情報をエンコードします。フェロー語とアイスランド語は語彙的にも文法的にも類似しているため、これら2つのタグ付けスキームを変換する体系的な方法を持つことは、言語技術の分野、特に2つの言語間の転移学習の研究において有益である。私たちの仕事の成果として、フェロー語PoSタグ付けスキームで作成された、クロスリンガルNLPアプリケーションで使用する準備ができたアイスランドのコーラの仮バージョンを提示します。', 'hi': 'हम दो भाषाओं की पीओएस टैगिंग योजनाओं, आइसलैंडिक एमआईएम-गोल्ड टैगिंग योजना और फैरोइस सोसियालुरिन टैगिंग योजना के बीच रूपांतरण की प्रक्रिया का वर्णन करते हैं। ये टैगिंग योजनाएं कार्यात्मक रूप से समान हैं, लेकिन टोकनीकृत पाठ पर ठीक-ठीक रूपात्मक जानकारी को एन्कोड करने के लिए अलग-अलग तरीकों का उपयोग करती हैं। चूंकि फैरोइस और आइसलैंडिक शाब्दिक और व्याकरणिक रूप से समान हैं, इसलिए इन दो टैगिंग योजनाओं के बीच परिवर्तित करने के लिए एक व्यवस्थित विधि होना भाषा प्रौद्योगिकी के क्षेत्र में फायदेमंद होगा, विशेष रूप से दो भाषाओं के बीच स्थानांतरण सीखने पर शोध में। हमारे काम के एक उत्पाद के रूप में, हम आइसलैंडिक कॉर्पोरेट का एक अनंतिम संस्करण प्रस्तुत करते हैं, जो फैरोइस पीओएस टैगिंग योजना में तैयार किया गया है, जो क्रॉस-लिंगुअल एनएलपी अनुप्रयोगों में उपयोग के लिए तैयार है।', 'ru': 'Мы описываем процесс преобразования между схемами маркировки PoS двух языков, исландской схемой маркировки MIM-GOLD и фарерской схемой маркировки Sosialurin. Эти схемы тегирования функционально схожи, но используют отдельные способы кодирования мелкозернистой морфологической информации на токенизированном тексте. Поскольку фарерский и исландский языки лексически и грамматически схожи, наличие систематического метода преобразования между этими двумя схемами маркировки было бы полезно в области языковой технологии, особенно в исследованиях по переводу обучения между двумя языками. В качестве продукта нашей работы мы представляем предварительную версию исландских корпусов, подготовленную в схеме маркировки фарерских PoS, готовую для использования в межязычных приложениях NLP.', 'ga': 'Déanaimid cur síos ar an bpróiseas comhshó idir na scéimeanna clibeála PoS do dhá theanga, an scéim chlibeála Íoslainnis MIM-GOLD agus an scéim chlibeála Faróis Sosialurin. Tá na scéimeanna clibeála seo comhchosúil ó thaobh feidhme de ach úsáideann siad bealaí ar leith chun faisnéis mhín-mhioneolaíoch moirfeolaíoch ar théacs tokenáilte a ionchódú. Toisc go bhfuil cosúlachtaí idir an Fharó agus an Íoslainnis ó thaobh na foclóireachta agus na gramadaí de, bheadh sé tairbheach i réimse na teicneolaíochta teanga go mbeadh modh córasach le comhshó idir an dá scéim chlibeála seo, go háirithe i dtaighde ar fhoghlaim aistrithe idir an dá theanga. Mar tháirge dár gcuid oibre, cuirimid i láthair leagan sealadach de chorpora Íoslainnis, a ullmhaíodh sa scéim chlibeála Faróis PoS, réidh le húsáid in feidhmeanna tras-teangacha NLP.', 'hu': 'Leírjuk a két nyelv PoS címkézési sémája, az izlandi MIM-GOLD címkézési séma és a feröeri Sosialurin címkézési séma közötti konverziós folyamatot. Ezek a címkézési sémák funkcionálisan hasonlóak, de külön módszereket használnak a finom szemű morfológiai információk tokenizált szövegen történő kódolására. Mivel a feröer és az izlandi nyelv lexikai és nyelvtani szempontból hasonló, a két címkézési rendszer közötti átalakítás szisztematikus módszere előnyös lenne a nyelvtechnológia területén, különösen a két nyelv közötti transzfertanulás kutatásában. Munkánk eredményeként bemutatjuk az izlandi kormányok ideiglenes változatát, amely a Feröer PoS címkézési rendszerben készült, többnyelvű NLP alkalmazásokban való használatra kész.', 'el': 'Περιγράφουμε τη διαδικασία μετατροπής μεταξύ των συστημάτων επισήμανσης PoS δύο γλωσσών, του ισλανδικού συστήματος επισήμανσης MIM-GOLD και του συστήματος επισήμανσης Sosialurin των Φερόων. Αυτά τα συστήματα επισήμανσης είναι λειτουργικά παρόμοια, αλλά χρησιμοποιούν διαφορετικούς τρόπους για την κωδικοποίηση λεπτόκοκκων μορφολογικών πληροφοριών σε κείμενο με επισήμανση. Δεδομένου ότι τα φαροϊκά και τα ισλανδικά είναι λεξικά και γραμματικά παρόμοια, η ύπαρξη μιας συστηματικής μεθόδου μετατροπής μεταξύ αυτών των δύο συστημάτων σήμανσης θα ήταν επωφελής στον τομέα της γλωσσικής τεχνολογίας, ιδίως στην έρευνα για τη μάθηση μεταφοράς μεταξύ των δύο γλωσσών. Ως προϊόν της εργασίας μας, παρουσιάζουμε μια προσωρινή έκδοση ισλανδικών σωμάτων, παρασκευασμένα με το σύστημα επισήμανσης PoS των Φερόων, έτοιμα για χρήση σε γλωσσικές εφαρμογές NLP.', 'ka': 'ჩვენ აღწერეთ პროცესი, რომელიც ორი ენების PoS-ის მაგრამების შემების განცვლების პროცესი, ისლანდიური MIM-GOLD მაგრამების სქემი და ფეროს სოციალურინის მაგრამების შემი. ეს მართლა სქემები ფუნქციალურად იგივეა, მაგრამ გამოყენება განსაკუთრებული გზები, რომლებიც ტექსტის შესახებ მოპოროლოგიური ინფორმაცია კოდირება რადგან ტაპოსური და თლანდიული ლექსიკურად და გრამიკურად განსხვავებულია, რომ ამ ორი მაგრამის სქემების შორის შეცვლის სისტემატიკურად გამოიყენებული იქნება ენის ტექნოლოგიის ფორმაში, განსაკუთრებით ორ როგორც ჩვენი სამუშაო პროდისტი, ჩვენ აჩვენებთ თლანდიური კოპორაის პროდუმენტის გარეშე, რომელიც გავაკეთებული ფეროსური PoS-ის სქემიში, რომელიც დავიყენებთ გამოყენებას კრე', 'kk': 'Біз PoS тегтер сұлбаларының, Исландиялық MIM-GOLD тегтер сұлбаларының және Фарос социалурин тегтер сұлбаларының аудару процесін таңдадық. Бұл белгілеу сұлбалары функциялық түрде ұқсас, бірақ таңдалған мәтіннің морфологиялық мәліметін кодтау үшін бөлек арқылы қолданыңыз. Фарос және Исландия лексикалық және грамматикалық ұқсас болғанда, бұл екі тегтер сұлбаларының ортасында аудару жүйесінің әдісі, тіл технологиялық өрісінде, осымен қатар, екі тіл арасындағы аудару оқыту тал Жұмыстың продукты ретінде, біз Исландия корпорасының provisional нұсқасын таңдаймыз, Фарос PoS тегтер сұлбасына дайын, көптеген тілді NLP қолданбаларында қолдану үшін дайын.', 'it': "Descriviamo il processo di conversione tra gli schemi di tagging PoS di due lingue, lo schema islandese di tagging MIM-GOLD e lo schema di tagging Sosialurin Faroese. Questi schemi di tagging sono funzionalmente simili ma utilizzano modi separati per codificare informazioni morfologiche a grana fine sul testo tokenizzato. Poiché il faroese e l'islandese sono lessicamente e grammaticalmente simili, avere un metodo sistematico di conversione tra questi due schemi di marcatura sarebbe utile nel campo della tecnologia linguistica, in particolare nella ricerca sull'apprendimento di trasferimento tra le due lingue. Come prodotto del nostro lavoro, presentiamo una versione provvisoria di corpora islandese, preparata nello schema di marcatura PoS Faroese, pronta per l'uso in applicazioni cross-lingual NLP.", 'lt': 'Apibrėžiame dviejų kalbų PoS ženklinimo schemų, Islandijos MIM-GOLD ženklinimo schemos ir Farerų salų socialinės lurinos ženklinimo schemos perskaičiavimo procesą. Šios ženklinimo sistemos yra funkciškai panašios, tačiau naudojami atskiri būdai koduoti smulkiai grūdintą morfologinę informaciją apie ženklintą tekstą. Kadangi farerų ir islandų kalbos tekstiškai ir gramatiškai panašios, sistemingas šių dviejų ženklinimo schemų keitimo metodas būtų naudingas kalbų technologijų srityje, ypač moksliniuose tyrimuose dėl mokymosi tarp dviejų kalbų perdavimo. Kaip mūsų darbo produktas, pristatome laikiną islandų korporo versiją, parengtą Farerų salų PoS ženklinimo schemoje, paruoštą naudoti tarpkalbinėse NLP programose.', 'ms': 'Kami menggambarkan proses pertukaran antara skema penanda PoS dua bahasa, skema penanda MIM-GOLD Iceland dan skema penanda Sosialurin Faroese. Skema tag ini secara berfungsi serupa tetapi guna cara terpisah untuk mengekod maklumat morfologik-benih halus pada teks tokenised. Kerana Bahasa Faroese dan Icelandic secara leksikal dan grammatik sama, mempunyai kaedah sistemik untuk menukar antara dua skema tag ini akan berguna dalam bidang teknologi bahasa, khususnya dalam kajian tentang pemindahan belajar antara dua bahasa. Sebagai produk kerja kami, kami memperkenalkan versi sementara dari korpra Iceland, disediakan dalam skema penanda PoS Faroese, bersedia untuk digunakan dalam aplikasi NLP saling bahasa.', 'ml': 'രണ്ടു ഭാഷകളുടെ പോസ് ടാഗിങ്ങ് സ്ക്യൂമിനുമിടയില്\u200d മാറ്റുന്ന പ്രക്രിയം, ഇസ്ലാന്\u200dഡിക് MIM-GOLD ടാഗിങ്ങ് പദ്ധതിയും ഫാറോസ് സോസില്ല ഈ ടാഗിങ്ങ് സ്ക്രീമുകള്\u200d പ്രവര്\u200dത്തനപ്പെടുത്തിയിരിക്കുന്നത് പോലെയാണ്, പക്ഷെ സുന്ദരമായ മോര്\u200dഫോളിക്കല്\u200d വിവരങ് ഫാറോയിസും ഐസ്ലാന്റിക്കും ലെക്സിക്കിക്കും സാമ്രാമിക്കും ഒരുപോലെയാണ്, ഈ രണ്ട് ടാഗിങ്ങ് സ്ക്രീമിക് സ്ക്രീമിക് ഉണ്ടാക്കാന്\u200d ഒരു രീതിയുണ്ട്, ഭാഷ സ As a product of our work, we present a provisional version of Icelandic corpora, prepared in the Faroese PoS tagging scheme, ready for use in cross-lingual NLP applications.', 'mk': 'Го опишуваме процесот на конверзија помеѓу шемите за означување на PoS на два јазици, исландската шема за означување на MIM-GOLD и шемата за означување на Фароешките социјалурини. Овие шеми за означување се функционално слични, но користат одделни начини за кодирање на морфолошки информации за означен текст. Бидејќи фарските и исландските се лексички и граматички слични, имањето систематски метод за конверт помеѓу овие две шеми на означување би било корисно во областа на јазичката технологија, посебно во истражувањето за трансфер на учење помеѓу двата јазици. As a product of our work, we present a provisional version of Icelandic corpora, prepared in the Faroese PoS tagging scheme, ready for use in cross-lingual NLP applications.', 'mn': 'Бид PoS-ийн хоёр хэл, Исландийн MIM-GOLD-ын маркингийн схемийн хоорондын өөрчлөлтийг тайлбарладаг. Эдгээр тагжингийн схемүүд нь функционалтай адилхан, гэхдээ тодорхойлогдсон текст дээр тодорхойлогдсон морфологик мэдээллийг кодлох өөр арга хэрэглэдэг. Фарос болон Исландийн хоёр маркингийн схемүүдийн хоорондоо шилжүүлэх систематикийн арга нь хэлний технологийн талбар дээр ашигтай болно. Ялангуяа хоёр хэлний хоорондын суралцах судалгааны тухай. Бид ажлын бүтээгдэхүүний хувьд Исландийн корпораны хувилбар болгож, Фарос PoS-ын маркингийн схемд бэлдэн, олон хэлний NLP хэрэглээнд ашиглах бэлэн.', 'pl': 'Opisujemy proces konwersji pomiędzy schematami tagowania PoS dwóch języków, islandzkim schematem tagowania MIM-GOLD i faeryjskim schematem tagowania Sosialurin. Te schematy tagowania są funkcjonalnie podobne, ale używają oddzielnych sposobów kodowania drobnoziarnistych informacji morfologicznych na tekście tokenizowanym. Ponieważ język wysoki i islandzki są leksycznie i gramatycznie podobne, posiadanie systematycznej metody konwersji między tymi dwoma schematami tagowania byłoby korzystne w dziedzinie technologii językowych, w szczególności w badaniach nad nauką transferu między dwoma językami. Jako produkt naszej pracy prezentujemy tymczasową wersję islandzkich korpusów, przygotowaną w schemacie tagowania PoS, gotową do wykorzystania w aplikacjach NLP wielojęzycznych.', 'no': 'Vi beskriver prosessen for konvertering mellom PoS-merkingsschema med to språk, islandsk MIM-GOLD-merkingsschema og faroesisk sosialurin-merkingsschema. Desse merkingsskjemalane er funksjonell liknande, men bruk eige måtar for å koda finn-korn morfologisk informasjon om tokniserte tekst. Som faroesisk og islandsk er leksisk og gramatisk liknande, har eit systematisk metode for å konvertera mellom desse to merkingsskjema, vil det være nyttig i språk-teknologien, spesielt i forskning om læring mellom dei to språka. Som eit produkt av arbeidet vårt, presenterer vi ei provisional versjon av islandske korpora, forberedt i Faroese PoS-merkingsplanen, klart for bruk i fleirspråk NLP-program.', 'sr': 'Opišemo proces preobraćanja između šema označavanja PoS-a od dva jezika, Islandske šeme označavanja MIM-GOLD-a i šeme označavanja Faroese socijalurina. Ove šeme označavanja su funkcionalno slične ali koristite odvojene načine za kodiranje fino zrno morfološke informacije o tokeniranom tekstu. Dok su Faroese i Islandski leksički i gramatički slični, imati sistematičnu metodu preobraćanja između tih dva šema označavanja bilo bi korisno u oblasti jezičke tehnologije, posebno u istraživanju o učenju prevođenja između dva jezika. Kao proizvod našeg rada, predstavljamo privremenu verziju islandskog korpora, pripremljenu u shēmu oznake Faroese PoS, spremnu za upotrebu kroz jezičke aplikacije NLP.', 'si': 'අපි පොස් ටැගින් ස්කීම් දෙකක්, අයිස්ලෑන්ඩික MIM-GOLD ටැගින් ස්කීම් සහ ෆරෝස් සොසියාලුරින් ටැගින් ස්කීම් අතර පරිවර්තනය Name ෆරෝයිස් සහ අයිස්ලෑන්ඩික් ලෙක්සික් සහ ග්\u200dරාමාටික් වගේ වගේ, මේ ටැග් ස්කම් දෙකක් අතර පරිවර්තනයක් තියෙන්නේ භාෂාත්මක තාක්ෂණයේ  අපේ වැඩේ ප්\u200dරදේශයක් විදිහට, අපි අයිස්ලන්ඩික් කොර්පෝරාගේ ප්\u200dරදේශයක් තියෙනවා, ෆාරෝස් පෝස් ටැග් සැකසුමේ සූදානම් කරලා', 'so': 'Waxaynu qoraynaa koorasyada bedelka ee kooxda tagging ee PoS oo laba luqadood ah, qorshaha tagging ee Icelandic MIM-GOLD iyo qorshaha tagging ee Faroese Sosialurin. These tagging schemes are functionally similar but use separate ways to encode fine-grained morphological information on tokenised text.  Sida Faroese iyo Icelandi ay u eg yihiin lexic iyo grammatik, oo ay haystaan qaab systemic ah oo ku beddelaya labadan qorshaha tagsiga ayaa faa’iido u leh beerta teknolojiga luqada, si gaar ah waxbarashada beddelinta labada luqadood. Sida dhakhtarka shaqadeenna ah, waxaynu soo bandhignaynaa warqad ka soo baxa shirkadda Icelandika, oo lagu diyaariyey qorshaha tagging ee Faroese PoS, oo loo diyaariyey isticmaalka codsiga afka NLP.', 'sv': 'Vi beskriver omvandlingsprocessen mellan PoS-märkningssystemen på två språk, det isländska MIM-GOLD-märkningssystemet och Färöiska Sosialurin-märkningssystemet. Dessa märkningsscheman är funktionellt likartade men använder separata sätt att koda finkornig morfologisk information på tokeniserad text. Eftersom färöiska och isländska är lexiskt och grammatiskt likartade skulle det vara fördelaktigt att ha en systematisk metod för att konvertera mellan dessa två märkningssystem inom språkteknik, särskilt inom forskning om överföring av lärande mellan de två språken. Som en produkt av vårt arbete presenterar vi en provisorisk version av isländska corpora, framtagen i Färöiska PoS-märkningssystemet, klar för användning i tvärspråkiga NLP-applikationer.', 'ta': 'நாம் போஸ் ஒட்டுதல் திட்டத்திற்கு இடையே மாற்றும் செயல்பாட்டை விவரிக்கிறோம், ஐஸ்லாந்திக் MIM-GOLD ஒட்டும் திட்டத்தை மற்றும் ஃபார இந்த ஒட்டும் திட்டங்கள் செயல்படுத்தப்பட்டுள்ளது போன்ற ஆனால் செயல்படுத்தப்பட்ட உரையில் நன்றாக பிடிக்கப்பட்ட மாதிரி பாரோஸ் மற்றும் ஐஸ்லாந்திக்கும் முறைமையாக இருக்கிறது என்றால், இந்த இரண்டு குறிக்கும் திட்டத்திற்கும் இடையே மாற்ற ஒரு அமைப்பு முறையில் இருக்கும், மொ எங்கள் வேலையின் ஒரு பொருளாக, நாங்கள் இஸ்லாந்திக் நிறுவனத்தின் தற்காலிக பதிப்பை கொண்டுவருகிறோம், ஃபாரோஸ் போஸ் ஒட்டும் திட்டத்தில் தய', 'ro': 'Descriem procesul de conversie între schemele de etichetare PoS a două limbi, schema islandeză de etichetare MIM-GOLD și schema feroeză de etichetare Sosialurin. Aceste scheme de etichetare sunt similare din punct de vedere funcțional, dar utilizează modalități separate de codificare a informațiilor morfologice fine pe textul tokenizat. Deoarece limba feroeză și islandeză sunt similare din punct de vedere lexical și gramatical, ar fi benefică o metodă sistematică de conversie între aceste două scheme de etichetare în domeniul tehnologiei lingvistice, în special în cercetarea privind transferul de învățare între cele două limbi. Ca produs al activității noastre, vă prezentăm o versiune provizorie a corporei islandeze, pregătită în schema de etichetare PoS feroeză, gata pentru utilizare în aplicații cross-lingve PNL.', 'ur': 'ہم دو زبانوں کے پیس ٹاگ کرنے کی تصویروں کے درمیان تغییر کی پروسیس کو توصیح دیتے ہیں، اسلاندی MIM-GOLD ٹاگ کرنے کی تصویر اور فاروس سوسیلورین ٹاگ کرنے کی تصویر۔ These tagging schemes are functionally similar but use separate ways to encode fine-grained morphological information on tokenized text. جس طرح فاروس اور اسلاندی لکھی اور گراماتیکی طریقے کے مطابق مشابہ ہیں، ان دونوں ٹاگنگ طریقوں کے درمیان تبدیل کرنے کا سیستماتیک طریقہ ہے، زبان تکنولوژی کے کھیلے میں، مخصوصاً دو زبانوں کے درمیان تدریس کی تعلیم کے بارے میں تحقیق میں ہمارے کام کا ایک محصول ہے، ہم اسلاندی کورپورا کا ایک مقررہ نسخہ پیش کریں گے، جو فاروس پوس ٹاگنگ طریقے میں تیار ہے، کروست زبان کے NLP کاربریوں میں استعمال کے لئے تیار ہے.', 'mt': 'We describe the process of conversion between the PoS tagging schemes of two languages, the Icelandic MIM-GOLD tagging scheme and the Faroese Sosialurin tagging scheme.  Dawn l-iskemi tat-tikkettar huma funzjonalment simili iżda jużaw modi separati biex jikkodifikaw l-informazzjoni morfoloġika bi ħbub fin fuq it-test tokenizzat. Peress li l-Faroese u l-Islandiż huma lexikament u grammatikament simili, jekk ikun hemm metodu sistematiku għall-konverżjoni bejn dawn iż-żewġ skemi ta’ tikkettar ikun ta’ benefiċċju fil-qasam tat-teknoloġija tal-lingwi, speċifikament fir-riċerka dwar it-tagħlim tat-trasferiment bejn iż-żewġ lingwi. As a product of our work, we present a provisional version of Icelandic corpora, prepared in the Faroese PoS tagging scheme, ready for use in cross-lingual NLP applications.', 'uz': "Biz ikki tillar orasidan PoS taging qoliplari, Icelandic MIM-GOLD tagging qolipi va Faroese Sosylurin tagging qolipini ajratib turamiz. @ info Faroese va Icelandik bu ikki tillar orqali o'rganish uchun tizim usuli mavjud. Bu ikkita tagg qoliplarning orasidagi tillar teknologiya sohalatida, hususan ikkita tillar orqali o'rganishni o'rganishda foydali bo'ladi. Biz ishimizning produkti deb hisoblanamiz, Icelandik kompaniyasi taqdimotimiz, Faroese PoS teging qolipida tayyorlangan, bir necha tildagi NLP dasturlarida ishlatiladigan tayyor.", 'vi': 'Chúng tôi mô tả quá trình chuyển đổi giữa các kế hoạch định vị của Pog, ở hai ngôn ngữ, Hệ thống đánh dấu MIM-GOld Iceland và Hệ thống đánh dấu kiểu Pháp. Những bộ định vị này có chức năng tương tự nhưng dùng cách khác nhau để mã hóa thông tin lịch sự ổn định về văn bản đã bật. Bởi vì người Pháp và Iceland là ngôn ngữ văn học và theo ngữ pháp tương tự, có một phương pháp tổng thống để hoán cải giữa hai phương pháp đánh dấu này sẽ có lợi cho lĩnh vực công nghệ ngôn ngữ, cụ thể là trong nghiên cứu về học cách giao tiếp giữa hai ngôn ngữ. Chúng tôi là sản phẩm của chúng tôi, chúng tôi giới thiệu một phiên bản tạm thời của hạ sĩ Iceland, được chuẩn bị sẵn sàng để sử dụng cho các ứng dụng ngôn ngữ lục.', 'bg': 'Описваме процеса на преобразуване между схемите за маркиране на два езика, исландската схема за маркиране MIM-GOLD и фарьорската схема за маркиране на сосиалурин. Тези схеми за маркиране са функционално подобни, но използват отделни начини за кодиране на фина морфологична информация върху токенезиран текст. Тъй като фарьорският и исландският език са лексически и граматически сходни, наличието на систематичен метод за преобразуване между тези две схеми за маркиране би било от полза в областта на езиковите технологии, по-специално в изследванията за трансферно обучение между двата езика. Като продукт на нашата работа представяме временна версия на исландски корпори, изготвена в схемата за маркиране на Фарьорски поС, готова за използване в междуезични приложения за НЛП.', 'hr': 'Opišemo proces preobraćanja između šema označavanja PoS-a od dva jezika, Islandskog šema označavanja MIM-GOLD-a i šema označavanja Faroese socijalurina. Ovi šemi označavanja su funkcionalno slični ali koristite odvojene načine za kodiranje fino zrno morfoloških informacija o tokeniranom tekstu. Budući da su Faroese i Islandski leksički i gramatički slični, imati sistematski metod preobraćanja između tih dva programa označavanja bio bi korisno u oblasti jezičke tehnologije, posebno u istraživanju o učenju prijenosa između dva jezika. Kao proizvod našeg rada, predstavljamo privremenu verziju islandskog tijela, pripremljenu u shēmu označavanja Faroese PoS-a, spremnu za upotrebu u međujezičkim primjenama NLP-a.', 'da': 'Vi beskriver processen med konvertering mellem PoS-mærkningsskemaerne på to sprog, den islandske MIM-GOLD-mærkningsskema og den færøske Sosialurin-mærkningsskema. Disse mærkningsskemaer er funktionelt ens, men bruger separate måder at kode finkornede morfologiske oplysninger på tokeniseret tekst. Da færøsk og islandsk er leksikologisk og grammatisk ens, ville det være gavnligt at have en systematisk metode til at konvertere mellem disse to mærkningsordninger inden for sprogteknologi, især inden for forskning i overførsel af læring mellem de to sprog. Som et produkt af vores arbejde præsenterer vi en foreløbig version af islandske korpora, udarbejdet i den færøske PoS taggesystem, klar til brug i tværsprogede NLP applikationer.', 'nl': "We beschrijven het proces van conversie tussen de PoS tagging schema's van twee talen, het IJslandse MIM-GOLD tagging schema en het Faeröer Sosialurin tagging schema. Deze taggingschema's zijn functioneel vergelijkbaar, maar gebruiken aparte manieren om fijnkorrelige morfologische informatie op tokenised tekst te coderen. Aangezien het Faeröer en het IJslands lexicaal en grammaticaal op elkaar lijken, zou een systematische methode voor omzetting tussen deze twee taggingschema's nuttig zijn op het gebied van taaltechnologie, met name in onderzoek naar transferleren tussen de twee talen. Als product van ons werk presenteren we een voorlopige versie van IJslandse corpora, voorbereid in het Faeröer PoS tagging schema, klaar voor gebruik in cross-lingual NLP applicaties.", 'de': 'Wir beschreiben den Konvertierungsprozess zwischen den PoS-Tagging-Schemata zweier Sprachen, dem isländischen MIM-GOLD-Tagging-Schema und dem färöischen Sosialurin-Tagging-Schema. Diese Tagging-Schemata sind funktional ähnlich, verwenden jedoch getrennte Wege, um feinkörnige morphologische Informationen auf tokenisiertem Text zu kodieren. Da Färöisch und Isländisch lexikalisch und grammatikalisch ähnlich sind, wäre eine systematische Methode zur Umwandlung dieser beiden Tagging-Schemata im Bereich der Sprachtechnologie von Vorteil, insbesondere in der Forschung zum Transferlernen zwischen den beiden Sprachen. Als Produkt unserer Arbeit präsentieren wir eine vorläufige Version isländischer Korpora, die nach dem färöischen PoS-Tagging-Schema erstellt wurde und für den Einsatz in mehrsprachigen NLP-Anwendungen bereit ist.', 'id': 'Kami menggambarkan proses konversi antara skema penanda PoS dua bahasa, skema penanda MIM-GOLD Islandia dan skema penanda Sosialurin Faroese. Skema penandaan ini secara fungsional mirip tapi menggunakan cara terpisah untuk mengkode informasi morfologi berwarna gandum pada teks tokenised. Karena bahasa Faroese dan Islandia secara lexik dan gramatik sama, memiliki metode sistematis untuk mengubah antara dua skema tagging ini akan berguna dalam bidang teknologi bahasa, khususnya dalam penelitian tentang transfer belajar antara dua bahasa. Sebagai produk dari pekerjaan kami, kami mempersembahkan versi sementara dari korpra Islandia, siap dalam skema tagging PoS Faroese, siap untuk digunakan dalam aplikasi NLP saling bahasa.', 'ko': '우리는 아이슬란드 MIM-GOLD 표기 방안과 파라다이스 Sosialurin 표기 방안 두 언어의 어성 표기 방안 간의 전환 과정을 묘사했다.이러한 표기 방안은 기능적으로 비슷하지만 서로 다른 방식으로 표기 텍스트의 세립도 형태 정보를 인코딩한다.파라다이스어와 아이슬란드어는 어휘와 문법적으로 비슷하기 때문에 이 두 가지 표기 모델 사이를 전환하는 체계적인 방법은 언어 기술 분야, 특히 두 언어 간의 이동 학습 연구에 도움이 될 것이다.우리의 작업의 성과로서, 우리는 임시 버전의 아이슬란드 어료 라이브러리를 제공했다. 이것은 팔로제도 어성 표기 방안에서 작성된 것으로, 크로스 언어 NLP 응용에 사용할 수 있다.', 'fa': 'ما فرایند تبدیل بین نقشه\u200cهای نقشه\u200cهای نقشه\u200cهای نقشه\u200cهای نقشه\u200cهای نقشه\u200cای از دو زبان، نقشه\u200cهای نقشه\u200cهای نقشه\u200cهای نقشه\u200cهای MIM-GOLD اسلاندی و نقشه\u200cهای نقشه\u200cهای نقشه\u200cهای نقشه\u200cهای سوسیلا این نقشه\u200cهای نقشه\u200cگیری به طور عملی شبیه\u200cاند ولی از راه\u200cهای جدا استفاده کنید تا اطلاعات مورفولوژیکی\u200cهای پاکیزه\u200cای در متن\u200cشناخته\u200cشده را رمز کنید. در حالی که فاروس و اسلاندی به زبان و زبان مشابه دارند، با یک روش سیستمی برای تبدیل بین این دو برنامه نقاشی نقاشی نقاشی در زمینه تکنولوژی زبان، مخصوصا در تحقیقات درباره یادگیری بین دو زبان، منافع خواهد بود. به عنوان محصول کارمون، ما یک نسخه موقتی از شرکت اسلاندی را پیشنهاد می کنیم، که در برنامه نشان\u200cگیری POS فاروس آماده شده است، آماده برای استفاده در کاربردهای NLP متوسط زبان.', 'sw': 'Tunaelezea mchakato wa mabadiliko kati ya mipango ya viungo vya PoS kwa lugha mbili, Mpango wa wimbo wa MIM-GOLD wa Icelandic MIM-GOLD na mpango wa wimbo wa Sosialurin wa Faroea. Mpango huu wa viungo vinafanana kwa kazi lakini wanatumia njia tofauti ili kuweka taarifa nzuri za kifolojia kuhusu ujumbe ulioandikwa. Wakati wa-Faroea na Iceland wanafanana na hali ya kejeli na kwa kiasi kikubwa, wakiwa na njia ya mfumo ya kubadilisha kati ya mipango hii miwili ya viungo vitakuwa na faida katika teknolojia ya lugha, hususani katika utafiti wa kujifunza kati ya lugha mbili. Kama bidhaa ya kazi zetu, tunaweka toleo la muda wa makampuni ya Iceland, lililotengenezwa katika mpango wa mabango ya PoS wa Faroea, tayari kwa matumizi katika matumizi ya NLP yenye lugha mbalimbali.', 'tr': 'Biz PoS etiketleýän sahypalaryň arasynda, Isländçe MIM-GOLD etiketleýän sahypalaryny we Faroese sosialurin etiketleýän sahypalaryny tassyýarlapdyk. Bu tägleme şemalar funksiýaly meňzeşdir ýöne metin üstünde çizgin titizlikleri kodlamak üçin ayrı yollary ullan Faroese we Isländçe meňzeş bolup görä, bu iki etiketleme şemalary arasynda üýtgetmek üçin sistematik yöntemi ýeterli dil tehnologiýasynda has gowy bolar. Çalışmalarymyzyň önümi bolup, Isländçe korporanyň provisional bir wersiýasyny görkezip, Faroese PoS etiketleme taslamasynda taýýarlanýar, cross-lingual NLP uygulamalarynda ulanmak üçin taýýar.', 'af': "Ons beskryf die proses van omskakeling tussen die PoS merking skeme van twee tale, die Islandse MIM-GOLD merking skema en die Faroese sosialurin merking skema. Hierdie merkelingsskeme is funksioneel gelykbaar maar gebruik aparte maniere om fyn- graan morfologiese inligting op tokeniseerde teks te kodeer. Soos Faroese en Islandse leksies en grammatiese gelykbaar is, het 'n sistematiese metode om tussen hierdie twee merking skeme te omskakel sou voordeel wees in die veld van taal teknologie, spesifieke in ondersoek om onderwerp te leer tussen die twee tale. As 'n produkt van ons werk, voorsien ons 'n provisionele weergawe van Islandse korpora, gereed in die Faroese PoS-merking skema, gereed vir gebruik in kruistale NLP-toepassings.", 'am': 'በሁለት ቋንቋዎች መካከል የPoS ተቃውሞ ፕሮግራሙን፣ የኢስላንድያ MIM-GOLD ተለቀፍ ፕሮግራሙን እና የፋሮሲ ሶስዩላንት ማተሚያ ፕሮግራሙን እናሳውቃለን፡፡ እነዚህም ማተሚያ ፕሮግራሞች በተለየ ይተካከላሉ ግን በተለየ መንገዶችን ይጠይቃሉ፡፡ ፋሮዛዊ እና እስክላንድኛ በተለይ እና በተለይ እንደሆነ፣ እነዚህ ሁለቱ ተቃውሞ ዘዴዎች መካከል ለመለወጥ የስርዓት ሥርዓት ሆኖ በቋንቋ ቴክኖጂ እርሻ ውስጥ በተለይም በሁለቱ ቋንቋዎች መካከል ለመማር በተጠቃሚ ትምህርት ለመጠቀም ይጠቅማል፡፡ የሥራችንን አቀማመጥ የኢስላንድ ኮርፖርት የፊርዮስ ፖስስ ተግባር ፕሮግራሙን በተዘጋጀ የልዩ ቋንቋ የNLP ፕሮግራሙን ተዘጋጅተናል፡፡', 'hy': 'Մենք նկարագրում ենք PoS-ի երկու լեզվի նշանակման ծրագրերի, Իսլանդական MIM-ԳՈԼԴ նշանակման ծրագրի և Ֆարոցի սոցիալյուրինի նշանակման ծրագրի փոխարինման գործընթացը: Այս նշանների ծրագրերը ֆունկցիոնալ նման են, բայց օգտագործում են առանձին ձևեր, որպեսզի կոդավորեն նշանված տեքստի վերաբերյալ պատրաստված մորֆոլոգիական տեղեկատվությունը: Որովհետև ֆարովացիները և իսլանդացիները լեքսիկապես և գրամատիկապես նման են, ունենալը այս երկու գրաֆիկների միջև փոխակերպելու սիստեմատիկ մեթոդ կօգնի լեզվի տեխնոլոգիաների ոլորտում, հատկապես երկու լեզվի միջև սովորելու փոխանցման հետազոտություններում Որպես մեր աշխատանքի արդյունք, մենք ներկայացնում ենք իսլանդացի մարմնի ժամանակակից տարբերակը, որը պատրաստված է Ֆարոցի PoS-ի նշանակման ծրագրում, պատրաստ է օգտագործելու երկլեզվով ՆԼՊ-ի ծրագրերում:', 'bs': 'Opišemo proces preobraćanja između šema označavanja PoS-a od dva jezika, Islandske šeme označavanja MIM-GOLD-a i šeme označavanja Faroese socijalurina. Ove šeme označavanja su funkcionalno slične, ali koristite odvojene načine za kodiranje finokograđenih morfoloških informacija o tokeniranom tekstu. Dok su Faroese i Islandski leksički i gramatički slični, imati sistematski metod preobraćanja između tih dva šema označavanja bilo bi korisno u oblasti jezičke tehnologije, posebno u istraživanju o učenju prevođenja između dva jezika. Kao proizvod našeg posla, predstavljamo privremenu verziju islandskog korpora, pripremljenu u shēmu označavanja Faroese PoS-a, spremnu za upotrebu u međujezičkim aplikacijama NLP-a.', 'sq': 'We describe the process of conversion between the PoS tagging schemes of two languages, the Icelandic MIM-GOLD tagging scheme and the Faroese Sosialurin tagging scheme.  Këto skema etiketash janë funksionalisht të ngjashme por përdorin mënyra të veçanta për të koduar informacionin morfologjik të hollësishëm mbi tekstin e tokenizuar. Ndërsa farezët dhe islandezët janë lexikisht dhe grammatikisht të ngjashme, të ketë një metodë sistematike për të konvertuar midis këtyre dy skemave të etiketave do të jetë e dobishme në fushën e teknologjisë gjuhësore, veçanërisht në kërkimin mbi transferimin e mësimit midis dy gjuhëve. Si një produkt i punës sonë, ne paraqesim një version të përkohshëm të korprës islandeze, të përgatitur në skemën e etiketave PoS Faroese, gati për përdorim në aplikimet NLP ndërgjuhësore.', 'az': 'Biz PoS etiketlərinin iki dil, İslandiya MIM-GOLD etiketlərinin və Faroesi sosialurin etiketlərinin taslağı arasında dönüşünü təsdiqləyirik. Bu etiketləmə şəkilləri fərqli olaraq bənzəyirlər, amma tokenilən metin barəsində sünbülli morfolojik məlumatları kodlamaq üçün ayrı yollar istifadə edin. Faroese və İslandiyac leksifik və gramatik olaraq eyni olduğu üçün, bu iki etiketləmə şəkillərin arasında sistematik bir yöntemi olmaq dil teknolojisi sahəsində faydalı olardı, özlərinə də iki dil arasındakı öyrənmə haqqında təhsil etmək barəsində. Bizim işimizin məhsulları olaraq, İslandiya korporasının provisional versiyonunu göstəririk, Faroese PoS etiketi taslağında hazırlanmış, çox dilli NLP uyğulamalarında istifadə etmək üçün hazırlanmış.', 'bn': 'আমরা দুই ভাষার পোস ট্যাগিং বিন্যাস, আইসল্যান্ডিক মিএম-GOLD ট্যাগিং পরিকল্পনা এবং ফারোজ সোসিলুরিন ট্যাগিং পরিকল্পনার মধ্যে প্রক্রিয়া এই ট্যাগিং পরিকল্পনাগুলো কার্যকভাবে একই রকম কিন্তু সুনির্দিষ্ট ট টেক্সটের ব্যাপারে ভিন্ন উপায় ব্যবহার করুন। যেহেতু ফারোজি এবং আইস্লান্ডিকেরা লেক্সিক এবং গ্রামাটিকভাবে একই রকম, এই দুই ট্যাগিং পরিকল্পনার মধ্যে রাখার একটি সিস্টেমিক পদ্ধতি থাকবে ভাষার প্রযুক্তির আমাদের কাজের একটি পণ্য হিসেবে আমরা আইসল্যান্ডিক কোর্পোরার একটি প্রাক্তন সংস্করণ উপস্থাপন করছি, যা ফারোজ পস ট্যাগিং পরিকল্পনায় প্রস্তুত, যা ক্রিশ ভ', 'ca': "Descrivem el procés de conversió entre els esquemes d'etiquetage PoS de dues llengües, l'esquema islàndic d'etiquetage MIM-GOLD i l'esquema de etiquetage de la Sosialurin fàrea. Aquests esquemes d'etiquetage són funcionalment semblants però utilitzen maneres diferents per codificar informació morfològica fina sobre text etiquetat. Com que els fàeros i islàndics són lèxicament i gramàticament similars, tenir un mètode sistemàtic de conversió entre aquests dos esquemes d'etiquetage seria beneficiós en el camp de la tecnologia del llenguatge, específicament en la recerca sobre transfer ència d'aprenentatge entre les dues llengües. Com a producte de la nostra feina, presentem una versió provisional de corpora islàndica, preparata en l'esquema d'etiquetage PoS de Faroese, disposada a ser utilitzada en aplicacions NLP translingües.", 'cs': 'Popisujeme proces konverze mezi PoS tagovacími schématy dvou jazyků, islandským MIM-GOLD tagovacím schématem a faerským Sosialurin tagovacím schématem. Tato tagovací schémata jsou funkčně podobná, ale používají samostatné způsoby kódování jemně zrnitých morfologických informací na tokenizovaném textu. Vzhledem k tomu, že faerská a islandština jsou lexicky a gramaticky podobná, mít systematickou metodu převodu mezi těmito dvěma značkovacími schématy by bylo přínosné v oblasti jazykových technologií, zejména při výzkumu transferového učení mezi oběma jazyky. Jako produkt naší práce představujeme prozatímní verzi islandských korpusů, připravenou v faerském PoS taggingovém schématu, připravenou k použití v cross-jazyčných NLP aplikacích.', 'fi': 'Kuvaamme kahden kielen PoS-merkintäjärjestelmien, islantilaisen MIM-GOLD-merkintäjärjestelmän ja Färsaarten Sosialurin-merkintäjärjestelmän, muuntamisprosessia. Nämä tunnisteet ovat toiminnallisesti samankaltaisia, mutta ne käyttävät erillisiä tapoja koodata hienojakoisia morfologisia tietoja tokenisoituun tekstiin. Koska Färsaarten ja Islannin kieli ovat sanastollisesti ja kieliopillisesti samankaltaisia, systemaattinen menetelmä näiden kahden merkintäjärjestelmän välillä olisi hyödyllinen kieliteknologian alalla, erityisesti näiden kahden kielen välistä siirtooppimista koskevassa tutkimuksessa. Työn tuloksena esittelemme alustavan version islantilaisista korpusista, jotka on laadittu Färsaarten PoS-merkintäjärjestelmässä, valmiina käytettäväksi monikielisissä NLP-sovelluksissa.', 'et': 'Kirjeldame teisendamise protsessi kahe keele PoS märgistusskeemide vahel, Islandi MIM-GOLD märgistusskeemi ja Fääri saarte sosialuriini märgistusskeemi vahel. Need märgistusskeemid on funktsionaalselt sarnased, kuid kasutavad eraldi viise, kuidas kodeerida peeneteralist morfoloogilist teavet tokeniseeritud tekstile. Kuna Fääri saarte ja islandi keel on leksikaliselt ja grammatiliselt sarnased, oleks süstemaatiline meetod nende kahe märgistusskeemi vahel kasulik keeltehnoloogia valdkonnas, eriti nende kahe keele vahelise siirdeõppe uuringutes. Oma töö tulemusena esitleme Fääri saarte PoS märgistusskeemi alusel valmistatud islandi korpuste esialgset versiooni, mis on valmis kasutama keeleülestes uue õppeprogrammi rakendustes.', 'ha': "Tuna bayyana muhallin ayuka tsakanin tagogi na PoS cikin lugha biyu, shirin tagogi na Icaland MIM-GELD da shirin tagogi na Faro Sosialurin. These tagging schemes are functionally similar but use separate ways to encode fine-grained morphological information on tokenised text.  Ki da Farofoyi da Icandaki suka zama daidai, da kuma a kamata, yana da wata hanyor wa'urar da za'a iya mayar da su tsakanin wannan zanen tagogi biyu, zai yi amfani da su cikin field of technical lugha, hasa'a, a cikin karatun da za'a sanar da za'a shige a tsakanin harshen biyu. Kama da wani marãyi na aikinmu, tuna gabatar da wata tsohon da aka yi tattalin a cikin shirin tagogi na Faroyi-PoS, na yi tattalin amfani da shiryoyin ayukan NLP masu cikin lugha masu tsakanin-daraja.", 'jv': 'Awak dhéwé ngerwih perusahaan nggunakake tarjamahan gar nggambar Pung liyane duruh, akeh nggambar SMM-GOLT lan palet nggawe Tarjamahan politenessoffpolite"), and when there is a change ("assertivepolite"), and when there is a change ("assertivepoliteness Taning Warga awak dhéwé éntuk nggawé, awak dhéwé ngewehku perusahaan anyar iki, nik awak dhéwé éntuk nggawe gerangkat oleh terakhi, nggawe ngubah kanggo nggawe aplikasi NLP luwih-luwih.', 'sk': 'Opisujemo proces pretvorbe med shemami označevanja PoS dveh jezikov, islandsko MIM-GOLD označevanja in fersko označevanje Sosialurin. Te sheme označevanja so funkcionalno podobne, vendar uporabljajo ločene načine za kodiranje drobnozrnatih morfoloških informacij na žetoniziranem besedilu. Ker sta Ferski in islandski jezik leksično in slovnično podobni, bi bila sistematična metoda pretvorbe med tema dvema shemama označevanja koristna na področju jezikovne tehnologije, zlasti pri raziskavah o prenosu učenja med obema jezikoma. Kot produkt našega dela predstavljamo začasno različico islandskih korpusov, pripravljeno v forski shemi označevanja PoS, pripravljeno za uporabo v medjezičnih aplikacijah NLP.', 'he': 'אנחנו מתארים את תהליך השינוי בין מערכות התגים של POS של שתי שפות, מערכת התגים של MIM-GOLD האיסלנדית ולמערכת התגים של Sosialurin הפארואי. These tagging schemes are functionally similar but use separate ways to encode fine-grained morphological information on tokenised text.  כיוון שהפארואים והאיסלנדים דומים באופן לקסי וגרמטי, שיש שיטה מערכתית להפוך בין שתי תכניות התגים האלה תהיה מועיל בתחום טכנולוגיה לשפה, במיוחד במחקר על ההעברה ללמוד בין שתי השפות. כתוצאה מהעבודה שלנו, אנו מציגים גרסה זמנית של גופרה איסלנדית, מוכנה בתכנית התגים של פוס הפארואי, מוכנה להשתמש בתוכניות NLP בין שפות.', 'bo': 'We describe the process of conversion between the PoS tagging schemes of two languages, the Icelandic MIM-GOLD tagging scheme and the Faroese Sosialurin tagging scheme. These tagging schemes are functionally similar but use separate ways to encode fine-grained morphological information on tokenised text. ཕོ་རིས་དང་འཁྱཊ་གླིང་སྐད་དབྱིབས་ཡིན་པ་ལས་སྐད་རིགས་དང་ལས་རྟགས་འགྱུར་བ As a product of our work, we present a provisional version of Icelandic corpora, prepared in the Faroese PoS tagging scheme, ready for use in cross-language NLP applications.'}
{'en': 'Exploring the Importance of Source Text in Automatic Post-Editing for Context-Aware Machine Translation', 'ar': 'استكشاف أهمية النص المصدر في التحرير التلقائي اللاحق للترجمة الآلية الواعية بالسياق', 'pt': 'Explorando a importância do texto de origem na pós-edição automática para tradução automática sensível ao contexto', 'es': 'Exploración de la importancia del texto de origen en la postedición automática para la traducción automática sensible al contexto', 'fr': "Exploration de l'importance du texte source dans la post-édition automatique pour la traduction automatique contextuelle", 'ja': 'コンテキスト意識機械翻訳のための自動ポスト編集におけるソーステキストの重要性の探求', 'zh': '寻源文本在上下文感知机器翻译自译后编辑之要', 'hi': 'प्रसंग-जागरूक मशीन अनुवाद के लिए स्वचालित पोस्ट-संपादन में स्रोत पाठ के महत्व की खोज', 'ru': 'Изучение важности исходного текста в автоматическом постредактировании для контекстно-зависимого машинного перевода', 'ga': "Tábhacht Téacs Foinse a Iniúchadh in Iar-Eagarthóireacht Uathoibríoch d'Aistriúchán Meaisín Feasach ar an gComhthéacs", 'el': 'Εξερεύνηση της σημασίας του αρχικού κειμένου στην αυτόματη μετα-επεξεργασία για μηχανική μετάφραση με επίγνωση του περιβάλλοντος', 'hu': 'A forrásszöveg fontosságának feltárása az automatikus utószerkesztésben a kontextustudatos gépi fordításhoz', 'kk': 'Контексті қалайтын машинаның аудармасының автоматты түрде өңдеу мәтінің маңыздылығын зерттеу', 'ka': 'ტექსტის მნიშვნელობის გამოყენება, რომელიც კონტექსტის შეცდომა მაქსინის გადასტრუქციისთვის ავტომატური პოსტი რედაქტირებაში', 'it': "Esplorare l'importanza del testo sorgente nel post-editing automatico per una traduzione automatica consapevole del contesto", 'ml': 'എക്സ്റ്റെന്റ്- അറിയുന്ന മെഷീന്\u200d പരിഭാഷപ്പെടുത്തുന്നതിനുള്ള സോര്\u200dസ്സ് പദാവലിയുടെ പ്രാധാന്യം', 'lt': 'Exploring the Importance of Source Text in Automatic Post-Editing for Context-Aware Machine Translation', 'mn': 'Context-Aware Machine Translation-ын автоматжуулалтын дараа захирах эх үүсвэрийн текст чухал байдлыг судлах', 'no': 'Utforskar viktigheten av kjeldeteksten i automatisk post- redigering for omsetjinga til kontekst- vekk maskinen', 'pl': 'Badanie znaczenia tekstu źródłowego w automatycznej edycji post-edycyjnej dla kontekstowych tłumaczeń maszynowych', 'ro': 'Explorarea importanței textului sursă în posteditarea automată pentru traducerea automată conștientă de context', 'mk': 'Истражување на важноста на изворниот текст во автоматското постуредување за превод на машина на свесност за контекст', 'sr': 'Izgleda važnosti izvornog teksta u automatskom posledišnjem redakciji za prevod mašine koji su upozoreni kontekstom', 'si': 'ස්වයංක්\u200dරියාත්මක පස්ස සංපාදනය සඳහා ස්වයංක්\u200dරියාත්මක පස්ස සංපාදනය සඳහා මුළු පාළුම් පා', 'ms': 'Menjelaskan Kepentingan Teks Sumber dalam Penyunting-Setelah Automatik untuk Terjemahan Mesin Tersedar-Konteks', 'so': 'Exploring the importance of Source Text in Automatic Post-Editing for Context-aware Machine Translation', 'sv': 'Utforska betydelsen av källtext i automatisk efterredigering för kontextmedveten maskinöversättning', 'mt': 'L-esplorazzjoni tal-Importanza tat-Test tas-Sors fl-Edizzjoni Awtomatika wara l-Edizzjoni għat-Traduzzjoni tal-Makkinarju Konxju mill-Kuntest', 'ur': 'کنٹیکسٹ-آگاہ ماشین ترجمہ کے لئے سٹورس ٹیکسٹ کی اثبات کی تحقیق کرتی ہے', 'ta': 'தானியங்கி தொகுப்பு பின்தொகுப்பில் மூல உரையின் முக்கியத்தை ஏற்றுமதி செய்கிறது', 'uz': 'Name', 'vi': 'Khám phá sự quan trọng của Văn bản Nguồn trong việc tự động sửa đổi máy tạo thức trao đổi...', 'bg': 'Изследване на значението на изходния текст в автоматичното следредактиране за машинен превод, осъзнаващ контекста', 'da': 'Udforsk vigtigheden af kildetekst i automatisk efterredigering til kontekstbevidst maskinoversættelse', 'de': 'Untersuchung der Bedeutung von Quelltext in der automatischen Nachbearbeitung für kontextbasierte maschinelle Übersetzung', 'id': 'Menjelaskan Kepentingan Teks Sumber Dalam Pengeditan Otomatis Setelah Terjemahan Mesin yang Diketahui Konteks', 'nl': 'Het belang van brontekst onderzoeken bij automatische nabewerking voor contextbewuste machinevertaling', 'ko': '탐색 텍스트가 상하문 감지 기계 번역 자동 후기 편집에서의 중요성', 'sw': 'Kueleza umuhimu wa maandishi ya vyanzo katika Tafsiri ya Uhariri wa Habari baada ya Kuhusu Utafiri wa Mashine yanayofahamika', 'fa': 'توسعه مهم متن منبع در توسعه بعد ویرایش خودکار برای ترجمه ماشین آگاهی متن', 'hr': 'Ispitivanje važnosti izvornog teksta u automatskom posledišnjem redakciji za prevod uređaja poznavanja konteksta', 'af': 'Voorskou die belangrikheid van Bron Teks in Outomatiese Post- Redigering vir Konteks- Aware Masjien Vertaling', 'am': 'ምርጫዎች', 'sq': 'Exploring the Importance of Source Text in Automatic Post-Editing for Context-Aware Machine Translation', 'hy': 'Հետազոտել աղբյուրների տեքստի կարևորությունը համատեքստի գիտակցած մեքենայի թարգմանման ավտոմատիկ հետխմբագրման մեջ', 'az': '䭯湴數琭䅷慲攠䵡歩湥泉饲楮⁔즙牣쎼淉饳椠쎼쎧쎼渠䅶瑯浡瑩欠偯獴ⵅ摩瑩湧⁍즙瑮楮\u20c3陭狃뱮쎼渠쎖浲쎼滃밠叉饦즙慬污얟擄녲쒱爊', 'bn': 'স্বয়ংক্রিয়ভাবে পোস্ট সম্পাদনার জন্য সোর্স টেক্সটের গুরুত্বপূর্ণ অনুবাদ', 'bs': 'Izračunajući važnost izvornog teksta u automatskom posledišnjem redakciji za prevod uređaja koji znaju kontekst', 'ca': 'Exploring the Importance of Source Text in Automatic Post-Editing for Context-Aware Machine Translation', 'cs': 'Zkoumání významu zdrojového textu v automatickém posteditování pro kontextový strojový překlad', 'et': 'Lähteteksti olulisuse uurimine kontekstiteadliku masintõlke automaatses järelredigeerimises', 'fi': 'Lähdetekstin merkityksen selvittäminen kontekstitietoisen konekäännöksen automaattisessa jälkimuokkauksessa', 'tr': 'Kontekst-Görkezilen Maşynyň Terjimesi üçin Otomatik Poz Editlenýän Metiniň wajyplygyny Exploring', 'jv': 'Jejaring', 'sk': 'Raziskovanje pomena izvornega besedila pri samodejnem popurejanju strojnega prevoda, ki se zaveda konteksta', 'ha': '@ action', 'he': 'מחקר חשיבות טקסט המקור בעורך לאחר העורך אוטומטי לתרגום מכונת מודעת לקונקסט', 'bo': 'རང་འགུལ་གྱིས་ཁྱད་ནས་བསྒྱུར་བཅོས་བྱེད་པའི་ཐོག་ཁུངས་ཡི་གེ་གལ་ཆེན་དག་སྟོན་པ'}
{'en': 'Accurate translation requires document-level information, which is ignored by sentence-level machine translation. Recent work has demonstrated that document-level consistency can be improved with automatic post-editing (APE) using only target-language (TL) information. We study an extended APE model that additionally integrates source context. A human evaluation of fluency and adequacy in EnglishRussian translation reveals that the model with access to source context significantly outperforms monolingual APE in terms of adequacy, an effect largely ignored by automatic evaluation metrics. Our results show that TL-only modelling increases fluency without improving adequacy, demonstrating the need for conditioning on source text for automatic post-editing. They also highlight blind spots in automatic methods for targeted evaluation and demonstrate the need for human assessment to evaluate document-level translation quality reliably.', 'fr': "Une traduction précise nécessite des informations au niveau du document, qui sont ignorées par la traduction automatique au niveau de la phrase. Des travaux récents ont démontré que la cohérence au niveau du document peut être améliorée grâce à la post-édition automatique (APE) utilisant uniquement les informations de la langue cible (TL). Nous étudions un modèle APE étendu qui intègre également le contexte source. Une évaluation humaine de la fluidité et de l'adéquation de la traduction anglais-russe révèle que le modèle avec accès au contexte source surpasse de manière significative l'APE monolingue en termes d'adéquation, un effet largement ignoré par les mesures d'évaluation automatique. Nos résultats montrent que la modélisation TL seule augmente la fluidité sans améliorer l'adéquation, ce qui démontre la nécessité de conditionner le texte source pour une post-édition automatique. Ils mettent également en évidence les angles morts des méthodes automatiques d'évaluation ciblée et démontrent la nécessité d'une évaluation humaine pour évaluer de manière fiable la qualité de la traduction au niveau du document.", 'pt': 'A tradução precisa requer informações no nível do documento, que são ignoradas pela tradução automática no nível da frase. Trabalhos recentes demonstraram que a consistência em nível de documento pode ser melhorada com pós-edição automática (APE) usando apenas informações do idioma de destino (TL). Estudamos um modelo APE estendido que integra adicionalmente o contexto de origem. Uma avaliação humana da fluência e adequação na tradução inglês-russo revela que o modelo com acesso ao contexto de origem supera significativamente o APE monolíngue em termos de adequação, um efeito amplamente ignorado pelas métricas de avaliação automática. Nossos resultados mostram que a modelagem somente TL aumenta a fluência sem melhorar a adequação, demonstrando a necessidade de condicionamento no texto fonte para pós-edição automática. Eles também destacam pontos cegos em métodos automáticos para avaliação direcionada e demonstram a necessidade de avaliação humana para avaliar a qualidade da tradução em nível de documento de forma confiável.', 'es': 'La traducción precisa requiere información a nivel de documento, que la traducción automática a nivel de oración ignora. Un trabajo reciente ha demostrado que la coherencia a nivel de documento se puede mejorar con la postedición automática (APE) utilizando solo información del idioma de destino (TL). Estudiamos un modelo APE extendido que, además, integra el contexto de origen. Una evaluación humana de la fluidez y la adecuación de la traducción del inglés al ruso revela que el modelo con acceso al contexto de origen supera significativamente al APE monolingüe en términos de adecuación, un efecto ignorado en gran medida por las métricas de evaluación automática. Nuestros resultados muestran que el modelado solo de TL aumenta la fluidez sin mejorar la adecuación, lo que demuestra la necesidad de condicionar el texto de origen para la postedición automática. También destacan los puntos ciegos de los métodos automáticos para la evaluación específica y demuestran la necesidad de una evaluación humana para evaluar la calidad de la traducción a nivel de documento de forma fiable.', 'ar': 'تتطلب الترجمة الدقيقة معلومات على مستوى المستند ، والتي يتم تجاهلها من خلال الترجمة الآلية على مستوى الجملة. أظهر العمل الأخير أنه يمكن تحسين التناسق على مستوى المستند من خلال التحرير التلقائي اللاحق (APE) باستخدام معلومات اللغة الهدف (TL) فقط. ندرس نموذج APE الممتد الذي يدمج سياق المصدر بالإضافة إلى ذلك. يكشف تقييم بشري للطلاقة والكفاية في الترجمة الإنجليزية-الروسية أن النموذج الذي يتمتع بإمكانية الوصول إلى سياق المصدر يتفوق بشكل كبير على نموذج APE أحادي اللغة من حيث الملاءمة ، وهو تأثير تم تجاهله إلى حد كبير بواسطة مقاييس التقييم التلقائي. تظهر نتائجنا أن نمذجة TL فقط تزيد من الطلاقة دون تحسين الكفاية ، مما يدل على الحاجة إلى تكييف النص المصدر للتحرير التلقائي اللاحق. كما أنها تسلط الضوء على النقاط العمياء في الأساليب التلقائية للتقييم المستهدف وتوضح الحاجة إلى التقييم البشري لتقييم جودة الترجمة على مستوى المستند بشكل موثوق.', 'zh': '译者须文档级信息,而句机器翻译忽之。 近事明白,唯以语言 (TL) 信息者译后辑 (APE) 可以崇文档级之一致性。 考其APE,成其上下文。 夫人英语 - 俄语译之流畅性,充分性之评,于充分性,可以访源上下文之形,显优于单语APE,其于大者,为自料指标所忽。 吾之的结果表明,唯TL建模不崇充分性者流畅性,明须对源文本条件反射以自译后辑也。 又显针对性估自术之盲点,明须人工评乃可质文档译也。', 'hi': 'सटीक अनुवाद के लिए दस्तावेज़-स्तर की जानकारी की आवश्यकता होती है, जिसे वाक्य-स्तर की मशीन अनुवाद द्वारा अनदेखा किया जाता है। हाल के कार्य ने प्रदर्शित किया है कि दस्तावेज़-स्तर की संगतता को केवल लक्ष्य-भाषा (TL) जानकारी का उपयोग करके स्वचालित पोस्ट-एडिटिंग (APE) के साथ सुधारा जा सकता है। हम एक विस्तारित एपीई मॉडल का अध्ययन करते हैं जो अतिरिक्त रूप से स्रोत संदर्भ को एकीकृत करता है। अंग्रेजी-रूसी अनुवाद में प्रवाह और पर्याप्तता के एक मानव मूल्यांकन से पता चलता है कि स्रोत संदर्भ तक पहुंच के साथ मॉडल पर्याप्तता के मामले में मोनोलिंगुअल एपीई को काफी हद तक बेहतर बनाता है, एक प्रभाव जो स्वचालित मूल्यांकन मीट्रिक द्वारा काफी हद तक अनदेखा किया जाता है। हमारे परिणामों से पता चलता है कि टीएल-केवल मॉडलिंग पर्याप्तता में सुधार के बिना प्रवाह को बढ़ाती है, स्वचालित पोस्ट-संपादन के लिए स्रोत पाठ पर कंडीशनिंग की आवश्यकता का प्रदर्शन करती है। वे लक्षित मूल्यांकन के लिए स्वचालित तरीकों में अंधे धब्बों को भी उजागर करते हैं और विश्वसनीय रूप से दस्तावेज-स्तर के अनुवाद की गुणवत्ता का मूल्यांकन करने के लिए मानव मूल्यांकन की आवश्यकता को प्रदर्शित करते हैं।', 'ja': '正確な翻訳には文書レベルの情報が必要ですが、文レベルの機械翻訳では無視されます。最近の研究では、ターゲット言語（ TL ）情報のみを使用した自動ポストエディット（ APE ）で文書レベルの一貫性を改善できることが示されています。ソースコンテキストをさらに統合した拡張APEモデルを研究しています。英語とロシア語の翻訳における流暢性と妥当性の人間による評価は、ソースコンテキストにアクセスできるモデルが妥当性の点で単語の類人猿を大幅に上回っていることを明らかにし、この効果は自動評価指標によってほとんど無視されている。TLのみのモデリングは、妥当性を向上させることなく流動性を向上させ、自動ポスト編集のためのソーステキストのコンディショニングの必要性を示しています。また、ターゲットを絞った評価のための自動方法の盲点を強調し、文書レベルの翻訳品質を確実に評価するための人間の評価の必要性を示します。', 'ru': 'Для точного перевода требуется информация на уровне документа, которая игнорируется машинным переводом на уровне предложения. Недавняя работа показала, что согласованность на уровне документов может быть улучшена за счет автоматического постредактирования (APE) с использованием только целевой языковой (TL) информации. Мы изучаем расширенную модель ОБЕЗЬЯНЫ, которая дополнительно интегрирует исходный контекст. Оценка человеком беглости и адекватности в англо-русском переводе показывает, что модель с доступом к исходному контексту значительно превосходит одноязычную ОБЕЗЬЯНУ с точки зрения адекватности, эффект в значительной степени игнорируется автоматическими оценочными показателями. Наши результаты показывают, что моделирование только TL повышает беглость без улучшения адекватности, демонстрируя необходимость кондиционирования на исходном тексте для автоматического постредактирования. Они также выделяют слепые зоны в автоматических методах целевой оценки и демонстрируют необходимость оценки человеком для надежной оценки качества перевода на уровне документов.', 'ga': 'Teastaíonn faisnéis ar leibhéal na gcáipéisí le haghaidh aistriúcháin chruinn, rud nach ndéantar neamhaird de ag aistriúchán meaisín ar leibhéal na habairte. Tá sé léirithe ag obair a rinneadh le déanaí gur féidir comhsheasmhacht ar leibhéal doiciméad a fheabhsú trí iar-eagarthóireacht uathoibríoch (APE) agus úsáid á baint as faisnéis sprioctheanga (TL) amháin. Déanaimid staidéar ar shamhail leathnaithe APE a chomhtháthaíonn comhthéacs foinseach freisin. Nochtann meastóireacht dhaonna ar líofacht agus ar leorgacht san aistriúchán Béarla-Rúisis go sáraíonn an tsamhail a bhfuil rochtain ar chomhthéacs foinseach aici go mór le APE aonteangach i dtéarmaí leorgacht, éifeacht nach bhfuil mórán airde ag méadracht mheastóireachta uathoibríoch air. Léiríonn ár dtorthaí go méadaíonn samhaltú TL-amháin líofacht gan feabhas a chur ar leorgacht, rud a léiríonn an gá atá le riochtú ar bhuntéacs le haghaidh iar-eagarthóireacht uathoibríoch. Aibhsíonn siad freisin spotaí dalla i modhanna uathoibríocha meastóireachta spriocdhírithe agus léiríonn siad an gá atá le measúnú daonna chun cáilíocht an aistriúcháin ar leibhéal doiciméad a mheas go hiontaofa.', 'el': 'Η ακριβής μετάφραση απαιτεί πληροφορίες σε επίπεδο εγγράφου, οι οποίες αγνοούνται από τη μηχανική μετάφραση σε επίπεδο πρότασης. Πρόσφατες εργασίες έχουν δείξει ότι η συνέπεια σε επίπεδο εγγράφου μπορεί να βελτιωθεί με την αυτόματη μετα-επεξεργασία (APE) χρησιμοποιώντας μόνο πληροφορίες γλώσσας-στόχου (TL). Μελετάμε ένα εκτεταμένο μοντέλο που ενσωματώνει επιπλέον το πλαίσιο πηγής. Μια ανθρώπινη αξιολόγηση της ευκρίνειας και της επάρκειας της αγγλικής-ρωσικής μετάφρασης αποκαλύπτει ότι το μοντέλο με πρόσβαση στο πλαίσιο πηγής ξεπερνά σημαντικά την μονόγλωσση από την άποψη της επάρκειας, γεγονός που αγνοείται σε μεγάλο βαθμό από τις αυτόματες μετρήσεις αξιολόγησης. Τα αποτελέσματά μας δείχνουν ότι η μοντελοποίηση μόνο αυξάνει την ευκρίνεια χωρίς να βελτιώνει την επάρκεια, καταδεικνύοντας την ανάγκη προσαρμογής του αρχικού κειμένου για αυτόματη μετα-επεξεργασία. Επισημαίνουν επίσης τα τυφλά σημεία σε αυτόματες μεθόδους στοχευμένης αξιολόγησης και καταδεικνύουν την ανάγκη ανθρώπινης αξιολόγησης για την αξιόπιστη αξιολόγηση της ποιότητας της μετάφρασης σε επίπεδο εγγράφων.', 'hu': 'A pontos fordítás dokumentumszintű információkat igényel, amelyeket a mondatszintű gépi fordítás figyelmen kívül hagy. A legutóbbi munkák kimutatták, hogy a dokumentum szintű konzisztenciája javítható az automatikus utószerkesztéssel (APE), amely kizárólag célnyelvi (TL) információkat használ. Tanulmányozunk egy kiterjesztett APE modellt, amely továbbá integrálja a forráskörnyezetet. Az angol-orosz fordítás folyékonyságának és megfelelőségének emberi értékelése azt mutatja, hogy a forráskörnyezethez való hozzáféréssel rendelkező modell jelentősen felülmúlja az egynyelvű APE-t a megfelelőség tekintetében, amit az automatikus értékelési mutatók nagyrészt figyelmen kívül hagynak. Eredményeink azt mutatják, hogy a csak TL modellezés növeli a folyékonyságot anélkül, hogy javítaná a megfelelőséget, bizonyítva, hogy szükséges a forrásszöveg automatikus utószerkesztés. Emellett kiemelik a vakfoltokat a célzott értékelés automatikus módszereiben, és bebizonyítják, hogy az emberi értékelés szükséges a dokumentumszintű fordítási minőség megbízható értékeléséhez.', 'it': "Una traduzione accurata richiede informazioni a livello di documento, ignorate dalla traduzione automatica a livello di frase. Recenti lavori hanno dimostrato che la coerenza a livello di documento può essere migliorata con il post-editing automatico (APE) utilizzando solo informazioni in lingua di destinazione (TL). Studiamo un modello APE esteso che integra ulteriormente il contesto sorgente. Una valutazione umana della fluidità e dell'adeguatezza nella traduzione inglese-russa rivela che il modello con accesso al contesto sorgente supera significativamente l'APE monolingue in termini di adeguatezza, un effetto ampiamente ignorato dalle metriche di valutazione automatica. I nostri risultati mostrano che la modellazione solo TL aumenta la fluidità senza migliorare l'adeguatezza, dimostrando la necessità di condizionare il testo sorgente per il post-editing automatico. Essi evidenziano anche i punti ciechi nei metodi automatici di valutazione mirata e dimostrano la necessità di una valutazione umana per valutare in modo affidabile la qualità della traduzione a livello di documento.", 'ka': 'მარტივი გადაწყვეტილება მოჭირდება დოკუმენტის დოკუმენტის ინფორმაცია, რომელიც სიტყვების დოკუმენტის გადაწყვეტილებით იგნორიურ მიმდინარე სამუშაო მუშაო დამუშაობა, რომ დოკუმენტის დოკუმენტის კონსტენსტირება შესაძლებელია ავტომატური დარედაქტირებით (APE) მხოლოდ მიზეზ ჩვენ შევსწავლოთ გაფართებული APE მოდელი, რომელიც დამატებით კონტექსტის კონტექსტის ინტერგურაცია. ადამიანის განსაზღვრება სინამდვილეობის და სამართლეობის განსაზღვრება ინგლისური-პროსიის განსაზღვრებაში აღმოჩნდება, რომ მოდელი, რომელიც მიყვარება მსოფლიო კონტექსტში მნიშვნელოვანია მონოლენგური ჩვენი შედეგი გამოჩვენება, რომ მხოლოდ TL მოდელირება გაზრდება ფუნქციას მარტივის გაზრდებით, მარტივის შესაბამისი გაზრდებით, რომელიც გამოჩვენება საჭირო შესაბამისი ტექს ისინი ასევე აღწერებენ ბლეო წერტილები ადამიანის განსაზღვრებისთვის მოსაზღვრებისთვის ავტომატური მეტოვებისთვის და გამოწერებენ ადამიანის განსაზღვრებისთვის საჭიროა', 'kk': 'Дұрыс аудару құжаттың деңгейінің мәліметі керек, бұл сөз деңгейінің машинаның аудармасынан елемейді. Жуырдағы жұмыс тек мақсатты тіл (TL) мәліметін қолдану арқылы құжат деңгейінің тәуелсіздігін автоматты түрде өңдеу (APE) арқылы жақсарту мүмкіндігін көрсету Біз көзінің контексті қосымша кеңейтілген APE үлгісін зерттейміз. Ағылшын- рус аудармасындағы адамдардың жылдамдылығын және адамдардың оқиғасы, көзгерту контекстіне қатынау үлгісі монолингілік APE-нің адамдардың қасиеттеріне көп жеткізу үлгісін, автоматты оқиға метрикал Біздің нәтижелеріміз тек TL модельдің өзгерту үшін автоматты түрде өзгерту үшін көзгертілген мәтіннің шарттарын көрсетеді. Олар сондай-ақ таңдау әдістерін автоматты түрде көрсетеді және құжат деңгейінің аудармалардың сапатын сенімді оқу үшін адамның оқу қажеттігін көрсетеді.', 'ml': 'Accurate translation requires document-level information, which is ignored by sentence-level machine translation.  അടുത്ത പ്രവര്\u200dത്തനങ്ങള്\u200d കാണിച്ചിരിക്കുന്നുവെങ്കില്\u200d രേഖയുടെ നില സ്ഥിതിയുടെ സ്വയം എപ്പിയില്\u200d മെച്ചപ്പെടുത്താന്\u200d സാധ നമ്മള്\u200d ഒരു വിശാലമായ ഏപിഇ മോഡല്\u200d പഠിക്കുന്നു. അതിന്റെ സോര്\u200dസ് കോണ്\u200dസ്റ്റെക്സ്റ്റെക്സെന് ഇംഗ്ലീഷ്-റഷ്യന്\u200d പരിഭാഷയില്\u200d സ്വാധീനവും പരിശോധനവും മനുഷ്യന്റെ വിലാസവും വ്യക്തമാക്കുന്നു. സോര്\u200dസ് കോണ്\u200dട്ടെക്സ്റ്റെക്സ്റ്റെന്\u200dറ് സമ്മതിക്കുന്ന മോഡല നമ്മുടെ ഫലങ്ങള്\u200d കാണിക്കുന്നത് ടിഎല്\u200d മാത്രം മാത്രം മാത്രം മാത്രം പരിശോധിപ്പിക്കുന്നതിനുള്ള ഫ്ലൈവിഷന്\u200d വളര്\u200dത്തുന്നതാണ്, സോ നിരീക്ഷിക്കുന്നതിനുള്ള സ്വന്തം രീതികളില്\u200d അവര്\u200d കുരുടന്മാരുടെ സ്ഥാനങ്ങളെയും പ്രദര്\u200dശിപ്പിക്കുന്നു. രേഖയുടെ നിലവിലെ വി', 'lt': 'Tikrai vertimas reikalauja informacijos dokumento lygmeniu, kuri ignoruojama vertimo raštu lygmeniu. Recent work has demonstrated that document-level consistency can be improved with automatic post-editing (APE) using only target-language (TL) information.  Mes tiriame išplėstą APE model į, kuris papildomai integruoja šaltinio kontekstą. Žmogaus atliktas lankstumo ir tinkamumo vertimo anglų ir rusų kalbomis vertinimas rodo, kad modelis, turintis galimybę susipažinti su šaltinio aplinkybėmis, gerokai viršija vienkalbinę APE tinkamumo požiūriu, o poveikis dažniausiai ignoruojamas automatiniais vertinimo rodikliais. Mūsų rezultatai rodo, kad modeliavimas tik TL padidina lankstumą nepagerinant tinkamumo, ir tai rodo būtinybę pritaikyti prie išorinio teksto, skirto automatiniam po redagavimo. Taip pat automatiniuose tikslinio vertinimo metoduose pabrėžiami aklūs taškai ir įrodoma, kad žmogaus vertinimas yra būtinas siekiant patikimai įvertinti dokumentų vertimo kokybę.', 'ms': 'Terjemahan yang tepat memerlukan maklumat aras dokumen, yang diabaikan oleh terjemahan mesin aras kalimat. Recent work has demonstrated that document-level consistency can be improved with automatic post-editing (APE) using only target-language (TL) information.  We study an extended APE model that additionally integrates source context.  A human evaluation of fluency and adequacy in English-Russian translation reveals that the model with access to source context significantly outperforms monolingual APE in terms of adequacy, an effect largely ignored by automatic evaluation metrics.  Hasil kami menunjukkan bahawa pemodelan TL-sahaja meningkatkan keseluruhan tanpa meningkatkan keperluan, menunjukkan keperluan untuk berkundisi pada teks sumber untuk post-edisi automatik. Mereka juga menyatakan titik buta dalam kaedah automatik untuk penilaian sasaran dan menunjukkan keperluan penilaian manusia untuk menilai kualiti terjemahan-aras dokumen dengan yakin.', 'mk': 'Точниот превод бара информации на ниво на документ, кои се игнорираат со машински превод на ниво на реченици. Неодамнешната работа покажа дека конзистенцијата на нивото на документот може да се подобри со автоматско постуредување (APE) користејќи само информации за јазикот на целта (TL). We study an extended APE model that additionally integrates source context.  A human evaluation of fluency and adequacy in English-Russian translation reveals that the model with access to source context significantly outperforms monolingual APE in terms of adequacy, an effect largely ignored by automatic evaluation metrics.  Нашите резултати покажуваат дека моделирањето само за ТЛ ја зголемува течноста без подобрување на соодветноста, демонстрирајќи ја потребата за условување на изворниот текст за автоматско постуредување. Тие, исто така, ги истакнуваат слепите точки во автоматските методи за целосна проценка и ја покажуваат потребата од човечка проценка за доверливо проценка на квалитетот на превод на ниво на документи.', 'mt': 'It-traduzzjoni preċiża teħtieġ informazzjoni fil-livell tad-dokument, li hija injorata mit-traduzzjoni bil-magna fil-livell tas-sentenza. Xogħol reċenti wera li l-konsistenza fil-livell tad-dokument tista’ tittejjeb b’informazzjoni awtomatika ta’ wara l-edizzjoni (APE) bl-użu biss tal-lingwa fil-mira (TL). Aħna nistudjaw mudell estiż tal-APE li jintegra wkoll il-kuntest tas-sors. A human evaluation of fluency and adequacy in English-Russian translation reveals that the model with access to source context significantly outperforms monolingual APE in terms of adequacy, an effect largely ignored by automatic evaluation metrics.  Ir-riżultati tagħna juru li l-immudellar tat-TL biss iżid il-fluwenza mingħajr ma jtejjeb l-adegwatezza, u dan juri l-ħtieġa għall-kundizzjonar fuq it-test tas-sors għal post-edizzjoni awtomatika. Huma jenfasizzaw ukoll punti għamja f’metodi awtomatiċi għal evalwazzjoni mmirata u juru l-ħtieġa għal valutazzjoni umana biex tiġi evalwata b’mod affidabbli l-kwalità tat-traduzzjoni fil-livell tad-dokument.', 'pl': 'Dokładne tłumaczenie wymaga informacji na poziomie dokumentu, które są ignorowane przez tłumaczenie maszynowe na poziomie zdania. Ostatnie prace wykazały, że spójność na poziomie dokumentów można poprawić dzięki automatycznej edycji post-edycji (APE) wykorzystującej tylko informacje dotyczące języka docelowego (TL). Badamy rozszerzony model APE, który dodatkowo integruje kontekst źródłowy. Ludzka ocena płynności i adekwatności tłumaczenia angielsko-rosyjskiego ujawnia, że model z dostępem do kontekstu źródłowego znacznie przewyższa jednojęzyczne APE pod względem adekwatności, co w dużej mierze ignorowane przez automatyczne wskaźniki oceny. Nasze wyniki pokazują, że modelowanie tylko TL zwiększa płynność bez poprawy adekwatności, wskazując na konieczność uwarunkowania tekstu źródłowego do automatycznej edycji. Podkreślają one również martwe punkty w automatycznych metodach oceny ukierunkowanej i wykazują potrzebę oceny przez człowieka w celu rzetelnej oceny jakości tłumaczeń na poziomie dokumentów.', 'mn': 'Дууссан орчуулалт баримтын түвшинд мэдээллийг хэрэгтэй, өгүүлбэр-түвшинд машины орчуулалтын шаардлагатай. Саяхан ажил баримтуудын түвшин тогтвортой байдал нь зөвхөн зорилготой хэл (TL) мэдээлэл ашиглан автоматжуулан цахилгаанаас (APE) сайжруулж чадна гэдгийг харуулсан. Бид эх үүсвэрийн тухай нэмэлтэй нэгтгэдэг APE загварыг судалж байна. Хүн төрөлхтөн Англи-Орос хэлний орчуулалтын шингэн, адилхан байдлын үнэлгээ нь эх үүсвэртэй байдлын тухай загвар нь ганц хэлний APE-г зөвхөн адилхан байдлын тухай илэрхийлж чадна. Энэ нь автоматжуулалтын үнэлгээ метри Бидний үр дүнд зөвхөн ТЛ-ийн загварын загварын дараа автоматжуулахын тулд автоматжуулахын тулд эх үүсвэрийн текст дээр нөхцөл байдлыг сайжруулахгүй ихэсгэдэг. Тэд мөн дүрслэлтэй оюутнуудын автоматжуулалтын арга замыг тодорхойлж, хүний оюутнуудын шалгалтыг баримт түвшингийн орчуулалтын чадварыг итгэлтэй үнэлэх хэрэгтэй гэдгийг харуулдаг.', 'no': '@ title: window Nyleg har arbeidet demonstrert at konsistens for dokumentnivå kan forbetrast med automatisk postredigering (APE) med berre målspråk (TL) informasjon. Vi studerer eit utvida APE-modell som tillegg integrerer kjeldekonteksten. Ein menneskelig evaluering av fluktet og adekvitet i engelsk-russisk omsetjing viser at modellen med tilgang til kjeldekonteksten utfører signifikat monospråk APE i uttrykk av adekvitet, eit effekt som hovudsakelig ignorerer av automatiske evalueringsmetrikar. Resultatet våre viser at bare TL-modellering øker flukten utan å forbetra adekvitet, som viser nødvendighet for å kondisera kjeldeteksten for automatisk postredigering. Dei markerer også blinde punkt i automatiske metodar for målrett evaluering og demonstrerer nødvendighet for å evaluera kvaliteten på dokumentnivå på sikkert.', 'ro': 'Traducerea exactă necesită informații la nivel de document, care sunt ignorate de traducerea automată la nivel de frază. Lucrările recente au demonstrat că coerența la nivel de document poate fi îmbunătățită cu ajutorul posteditării automate (APE), utilizând numai informații referitoare la limba țintă (TL). Studiem un model APE extins care integrează în plus contextul sursei. O evaluare umană a fluenței și adecvării în traducerea engleză-rusă relevă faptul că modelul cu acces la contextul sursă depășește semnificativ APE monolingv în ceea ce privește adecvarea, un efect ignorat în mare măsură de evaluarea automată. Rezultatele noastre arată că modelarea numai TL crește fluența fără a îmbunătăți adecvarea, demonstrând necesitatea condiționării textului sursă pentru post-editarea automată. Ele evidențiază, de asemenea, punctele moarte în metodele automate de evaluare orientată și demonstrează necesitatea evaluării umane pentru a evalua în mod fiabil calitatea traducerii la nivel de document.', 'sr': 'Taèni prevod zahteva informacije na nivou dokumenta, koje se ignorira prevodom mašine na nivou rečenica. Nedavni rad je pokazao da se konsekvencija na nivou dokumenta može poboljšati sa automatskim posledicama (APE) koristeći samo informacije o ciljnom jeziku (TL). Proučavamo produžen model APE koji dodatno integriše izvorni kontekst. Ljudska procjena tečnosti i adekvatnosti na engleskom-ruskom prevodu pokazuje da model s pristupom izvornom kontekstu značajno iznosi monojezičku APE u smislu adekvatnosti, učinak koji je u velikoj meri ignorisan automatskim procjenama. Naši rezultati pokazuju da samo modeliranje TL povećava tečnost bez poboljšanja adekvatnosti, pokazujući potrebu za kondicionacijom izvornog teksta za automatski posteditanje. Oni takođe naglašavaju slepe tačke u automatskim metodama za ciljnu procjenu i pokazuju potrebu za ljudske procjene da bi se pouzdano procenila kvalitet prevoda na nivou dokumenta.', 'so': "Turjumista si xisaabta ah waxaa loo baahan yahay macluumaad heerka dokumenta, kaas oo lagu diido turjumista machine-level. Shaqoda ugu dambeysay waxay caddaysay in la horumariyo heerka dukumentiga oo isku xiran karo oo lagu isticmaali karo macluumaad luuqada goal-dhigista oo kaliya (TL). Waxaynu baranaynaa model dheer oo APE ah, kaas oo si dheer u wada qabsada kooxda asalka. Qiimeynta aqoonta iyo ku filan afka ingiriiska-Ruushka waxaa muujiya in modelka aad u isticmaali karta taariikhda sourceedku uu si muhiim ah u sameeyaa APE, si saxda ah, sababtoo badan waxaa loo jeedaa qiimeynta iskumar ah. Abaalkayaga waxaa tusaya in tusaalaha TL oo kaliya uu kordhiyaa faa'iido aan horumarinaynin si ku filan, wuxuuna muujiyaa baahida shuruudaha ku saabsan qoraalka suuradda ee dib-editidda. Waxay sidoo kale ku iftiimiyaan meelo indhaha la' si ay u baahan yihiin qiimeynta waxyaabaha lagu hagayo, waxayna muujiyaan baahida qiimeynta dadka in lagu qiimeeyo qiimeynta turjumidda qoraalka-heerka si aamin ah.", 'sv': 'Korrekt översättning kräver information på dokumentnivå, vilket ignoreras av maskinöversättning på meningsnivå. Det senaste arbetet har visat att konsistensen på dokumentnivå kan förbättras med automatisk efterredigering (APE) med endast målspråk (TL) information. Vi studerar en utökad APE-modell som dessutom integrerar källkontext. En mänsklig utvärdering av flytande och lämplighet i engelsk-rysk översättning visar att modellen med tillgång till källkontext avsevärt överträffar enspråkiga APE när det gäller lämplighet, en effekt som till stor del ignoreras av automatiska utvärderingsmetoder. Våra resultat visar att TL-only modellering ökar flytande utan att förbättra adekvaten, vilket visar behovet av konditionering av källtext för automatisk efterredigering. De belyser också blinda vinklar i automatiska metoder för riktad utvärdering och visar behovet av mänsklig bedömning för att på ett tillförlitligt sätt utvärdera översättningskvaliteten på dokumentnivå.', 'si': 'හරියට පරිවර්තනය අවශ්\u200dය විදිහට ලේවල් තොරතුරු තොරතුරු අවශ්\u200dය වෙනවා, ඒක වාර්තාව- ලේවල් පරි මුලින් වැඩේ ප්\u200dරකාශ කරලා තියෙන්නේ ලේඛන- තත්වය ස්වයංක්\u200dරීය පස්ස සංපාදනය (APE) එක්ක ස්වයංක්\u200dරීය ප්\u200dරවෘතිය අපි ප්\u200dරශ්නයක් අධ්\u200dයානය කරනවා ප්\u200dරශ්නයක් සම්බන්ධයෙන් සම්බන්ධය කරනවා. ඉංග්\u200dරීසිය-රුසියානු වාර්තාවේ මිනිස්සු විශ්ලේෂණය සහ හැකියාවක් ඉංග්\u200dරීසියානු භාවිතාවේ ප්\u200dරවේශය ප්\u200dරවේශයෙන් ප්\u200dරවේශයෙන් ප්\u200dර අපේ ප්\u200dරතිචාරය පෙන්වන්නේ TL- විතරයි මොඩිලින් විතරයි ස්වයංක්\u200dරියාවක් වැඩ කරන්නේ නැති විතරයි, ස්වයංක්\u200dරියාවක් පස ඔවුන් ස්වයංක්\u200dරීය විශ්වාස කරන්න ස්වයංක්\u200dරීය විධානයෙන් ප්\u200dරකාශ කරනවා වගේම මිනිස්සු විශ්වාස කරන්න අවශ්\u200dය වි', 'ta': 'சரியான மொழிபெயர்ப்பு ஆவண- மட்டத்தின் தகவல் தேவைப்படுகிறது, அது வாக்கு- மட்டத்தின் மொழிமாற்றி புறக்கணிக சமீபத்தில் செயல் தெரிவித்துள்ளது ஆவண- மட்டத்தின் ஒத்திசையை தானாகவே திருத்தும் பின் தொகுப்பு (APE) தகவலை மட்டும் சேர்க்கும நாம் ஒரு நீட்டப்பட்ட APE மாதிரியை படிக்கிறோம். அதை கூடுதலாக மூலத்தின் சூழலை ஒன்றாக்குகிறது. ஆங்கிலம்- ரஷ் மொழிபெயர்ப்பில் ஒரு மனித மதிப்பும் மற்றும் தேவையும் மதிப்பும் குறிப்பிடுகிறது மூலத்திற்கு அணுகல் மாதிரியும் மாதிரியின் மூலம் சரியான எங்கள் முடிவுகள் தானியங்கி தொகுப்புக்கு தானாகவே TL மட்டும் மாதிரிப்பு தேர்வு செய்யும் பின்னர் தேர்வு செய்ய தேவையை அதிகரி அவர்களும் சேர்க்கப்பட்ட செயல்பாட்டிற்கு தானாகவே குருட்டுப் புள்ளிகளை முன்னிலைப்படுத்துகிறார்', 'ur': '丿賯蹖賯 鬲乇噩賲蹃 讴蹝 賲胤丕亘賯 丿賮鬲乇-爻胤丨 賲毓賱賵賲丕鬲 讴蹖 囟乇賵乇鬲 蹃蹝貙 噩爻蹝 sentence-level 賲丕卮蹖賳 鬲乇噩賲蹃 讴蹝 匕乇蹖毓蹃 睾蹖乇 睾賵乇 讴蹖丕 噩丕鬲丕 蹃蹝. 丕趩诰丕 讴丕賲 丿讴诰丕蹖丕 诏蹖丕 蹃蹝 讴蹃 丿讴诰丕賳蹝 賵丕賱蹖 爻胤丨 賲孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬丕賱孬 蹃賲 丕蹖讴 倬诰蹖賱丕卅蹖 APE 賲賵趫賱 讴賵 倬趹诰鬲蹝 蹃蹖诤 噩賵 爻賵乇噩 讴賳俟賳爻 賲蹖诤 丕囟丕賮蹃 讴乇鬲丕 蹃蹝蹟 丕賳诏賱蹖爻蹖-乇賵爻蹖 鬲乇噩賲蹃 賲蹖诤 丕蹖讴 丕賳爻丕賳 讴蹖 賲胤丕亘賯 丕賵乇 賲賳丕爻亘 鬲乇噩賲蹃 讴蹖 賲胤丕亘賯 馗丕蹃乇 讴乇鬲丕 蹃蹝 讴蹃 賲賵趫賱 爻賵乇噩 讴賳俟蹖讴爻 讴蹝 賱卅蹝 倬蹃賳趩賳蹝 讴蹝 賱卅蹝 丕蹖讴 夭亘丕賳 APE 讴賵 賲賳丕爻亘 胤乇蹖賯蹝 爻蹝 夭蹖丕丿蹃 夭蹖丕丿蹃 丕囟丕賮蹃 讴乇鬲丕 蹃蹝貙 丕蹖讴 丕孬乇 噩賵 丕俟賵俟蹖讴爻 丕乇夭卮 賲鬲乇蹖讴 讴蹝 匕乇蹖毓蹃 爻蹝 睾丕賮賱 蹃賲丕乇蹝 賳鬲蹖噩蹝 丿讴诰丕鬲蹝 蹃蹖诤 讴蹃 氐乇賮 俟蹖賱 賲賵趫賱蹖賳诏 讴蹝 匕乇蹖毓蹃 爻賮丕乇卮蹖 讴蹝 亘睾蹖乇 爻賮丕乇卮蹖 讴賵 丕囟丕賮蹃 讴乇鬲丕 蹃蹝貙 丕賵乇 丕爻 讴蹝 爻丕鬲诰 爻賮丕乇卮蹖 讴蹝 倬蹖趩诰蹝 爻賲噩诰丕賳蹝 讴蹝 賱卅蹝 爻賮丕乇卮蹖 讴蹖 囟乇賵乇鬲 讴賵 丿讴诰丕鬲蹝 蹃蹖诤. 蹖蹃 亘诰蹖 丕賳丿诰蹝 賳賯胤蹝 賲賵噩賵丿 丕乇夭蹖丕亘蹖 讴蹝 賱卅蹝 賲賵噩賵丿 胤乇蹖賯蹝 賲蹖诤 蹃丿丕蹖鬲 讴乇乇蹃蹝 蹃蹖诤 丕賵乇 賱賵诏賵诤 讴蹖 丕乇夭蹖丕亘蹖 讴蹖 囟乇賵乇鬲 讴賵 賲胤賲卅賳 讴乇乇蹃蹝 蹃蹖诤 讴蹃 爻賳丿 爻胤丨 讴蹖 鬲乇噩賲蹃 讴蹖賮蹖鬲 讴賵 賲胤賲卅賳 讴乇蹖诤蹟', 'vi': 'Dịch chính xác đòi hỏi thông tin cấp tài liệu, mà bị bỏ qua bằng cách dịch chuyên bản án. Công việc gần đây đã chứng minh rằng mức độ đồng minh của tài liệu có thể cải thiện bằng việc hiệu chỉnh tự động sau (APE) chỉ thông tin ngôn ngữ đích (TLS). Chúng tôi nghiên cứu một mô hình APE mở rộng mà cũng liên kết với ngữ cảnh nguồn. Một đánh giá nhân loại về tính năng khéo léo và độ chuẩn trong dịch ngữ Anh-Nga cho thấy rằng mô hình có quyền truy cập vào ngữ cảnh nguồn khả năng cao hơn thực hiện APE ngôn ngữ, một hiệu quả bị bỏ qua bởi âm lượng đánh giá tự động. Kết quả của chúng tôi cho thấy rằng chỉ mô tả TLS sẽ tăng tốc không sửa đổi độ phù hợp, hiển thị nhu cầu điều hòa trên chữ nguồn cho việc chỉnh tự động sau. Chúng cũng nhấn mạnh điểm mù trong các phương pháp tự động để đánh giá mục tiêu và hiển thị nhu cầu đánh giá con người để đánh giá chất lượng dịch văn bản.', 'uz': "@ info Yaqinda ishni koʻrsatadi, faqat target- language (TL) maʼlumotidan foydalanish avtomatik tahrirlash (APE) bilan hujjat- darajasini birlashtirish mumkin. Biz kengaytirilgan APE modelini o'rganamiz, bu ko'proq manba tarkibini birlashtiradi. Ingliz- Ruscha tarjima tilidagi inson suvtagan qiymatni anglatadi. Ushbu model manba tarjima bo'lgan moddalarni qiymatga ega boʻlishi mumkin bo'lganligiga monolingual APE'ni o'xshash qiladi. Bizning natijalarimiz faqat TL modelining suhbatini oshirish mumkin va avtomatik tahrirlash uchun manba matnning muhimligini koʻrsatish kerak. Ular dokument- darajasi tarjima sifatini qiymatish uchun obʼektlarni avtomatik usullarda ko'rsatadi.", 'bg': 'Точният превод изисква информация на ниво документ, която се игнорира от машинния превод на ниво изречение. Последната работа показа, че съгласуваността на ниво документ може да бъде подобрена с автоматичното пост-редактиране (APE), като се използва само информация за целевия език (TL). Изследваме разширен модел, който допълнително интегрира контекста на източника. Човешката оценка на плавността и адекватността на англо-руския превод разкрива, че моделът с достъп до контекста на източника значително превъзхожда моноезичния по отношение на адекватността, ефект до голяма степен пренебрегван от автоматичните измервания. Нашите резултати показват, че моделирането само на ТЛ увеличава плавността, без да подобрява адекватността, демонстрирайки необходимостта от кондициониране на изходния текст за автоматична пост-редактиране. Те също така подчертават слепите точки в автоматичните методи за целенасочена оценка и демонстрират необходимостта от човешка оценка, за да се оцени надеждно качеството на превода на ниво документ.', 'nl': 'Nauwkeurige vertaling vereist informatie op documentniveau, die wordt genegeerd door machinevertaling op zinsniveau. Recent onderzoek heeft aangetoond dat consistentie op documentniveau kan worden verbeterd met automatische post-editing (APE) waarbij alleen informatie over doeltaal (TL) wordt gebruikt. We bestuderen een uitgebreid APE model dat bovendien broncontext integreert. Een menselijke evaluatie van vloeiendheid en adequaatheid in Engels-Russische vertaling toont aan dat het model met toegang tot broncontext aanzienlijk beter presteert dan eentalige APE in termen van adequaatheid, een effect dat grotendeels genegeerd wordt door automatische evaluatiestatistieken. Onze resultaten tonen aan dat TL-only modellering de vloeiendheid verhoogt zonder de adequaatheid te verbeteren, wat aantoont dat er behoefte is aan conditionering op brontekst voor automatische post-editing. Ze wijzen ook op blinde vlekken in automatische methoden voor gerichte evaluatie en tonen aan dat menselijke beoordeling nodig is om de vertaalkwaliteit op documentniveau betrouwbaar te evalueren.', 'da': 'Nøjagtig oversættelse kræver oplysninger på dokumentniveau, som ignoreres af maskinoversættelse på sætningsniveau. Det seneste arbejde har vist, at konsistens på dokumentniveau kan forbedres med automatisk efterredigering (APE) ved hjælp af kun målsprogsoplysninger (TL). Vi studerer en udvidet APE model, der derudover integrerer kildekontekst. En menneskelig evaluering af flydenhed og tilstrækkelighed i engelsk-russisk oversættelse afslører, at modellen med adgang til kildekontekst betydeligt overgår ensproget APE med hensyn til tilstrækkelighed, en effekt, der stort set ignoreres af automatiske evalueringsmålinger. Vores resultater viser, at TL-only modellering øger flydende uden at forbedre tilstrækkeligheden, hvilket viser behovet for konditionering af kildetekst til automatisk efterredigering. De fremhæver også blinde vinkler i automatiske metoder til målrettet evaluering og viser behovet for menneskelig vurdering for at evaluere oversættelseskvaliteten på dokumentniveau pålideligt.', 'de': 'Eine genaue Übersetzung erfordert Informationen auf Dokumentenebene, die bei der maschinellen Übersetzung auf Satzebene ignoriert werden. Neuere Arbeiten haben gezeigt, dass die Konsistenz auf Dokumentenebene durch automatische Nachbearbeitung (APE) verbessert werden kann, die nur zielsprachige Informationen (TL) verwendet. Wir untersuchen ein erweitertes APE-Modell, das zusätzlich Quellkontext integriert. Eine menschliche Bewertung der Fließfähigkeit und Angemessenheit der englisch-russischen Übersetzung zeigt, dass das Modell mit Zugang zum Quellenkontext die einsprachige APE in Bezug auf die Angemessenheit deutlich übertrifft, was von automatischen Auswertungsmetriken weitgehend ignoriert wird. Unsere Ergebnisse zeigen, dass nur TL-Modellierung die Fließfähigkeit erhöht, ohne die Angemessenheit zu verbessern. Dies zeigt die Notwendigkeit, Quelltext für die automatische Nachbearbeitung zu konditionieren. Sie zeigen auch blinde Flecken in automatischen Methoden zur zielgerichteten Bewertung auf und zeigen, dass eine menschliche Bewertung erforderlich ist, um die Übersetzungsqualität auf Dokumentenebene zuverlässig zu bewerten.', 'id': 'Terjemahan yang tepat membutuhkan informasi tingkat dokumen, yang diabaikan oleh terjemahan mesin tingkat kalimat. Pekerjaan baru-baru ini telah menunjukkan bahwa konsistensi tingkat dokumen dapat diperbaiki dengan post-edit otomatis (APE) menggunakan hanya informasi target-language (TL). We study an extended APE model that additionally integrates source context.  Sebuah evaluasi manusia fluency dan adekwatesi dalam terjemahan bahasa Inggris-Rusia mengungkapkan bahwa model dengan akses ke konteks sumber secara signifikan melebihi APE monobahasa dalam terma adekwatesi, efek yang sebagian besar diabaikan oleh metrik evaluasi otomatis. Hasil kami menunjukkan bahwa model TL-only meningkatkan fluency tanpa meningkatkan keperluan, menunjukkan kebutuhan untuk kondisi pada teks sumber untuk post-edisi otomatis. Mereka juga menyatakan titik buta dalam metode otomatis untuk evaluasi sasaran dan menunjukkan kebutuhan penilaian manusia untuk mengevaluasi kualitas terjemahan tingkat dokumen dengan yakin.', 'ko': '정확한 번역은 문서급의 정보를 필요로 하지만 문장급의 기계번역은 이 점을 소홀히 한다.최근의 작업에 따르면 대상 언어(TL) 정보의 자동 사후 편집(APE)만 사용하면 문서 수준의 일치성을 높일 수 있다.우리는 확장된 APE 모델을 연구했는데, 이 모델은 원본 상하문도 통합했다.인류가 영국과 러시아의 번역 유창성과 충분성에 대한 평가에 따르면 충분성 측면에서 원시 언어 환경에 접근할 수 있는 모델은 단어 APE보다 현저히 우수한데 이런 영향은 어느 정도에 자동 평가 지표에 의해 무시되었다.우리의 연구 결과에 따르면 TL 모델링만 유창성을 높였지만 충분성을 높이지 못했다. 이것은 원본 텍스트를 조건화 처리하여 자동 후기 편집을 해야 한다는 것을 나타낸다.그것들은 또한 맞춤형 평가의 자동 방법 중의 맹점을 강조하고 인위적인 평가를 통해 문서의 등급의 번역 품질을 믿을 수 있게 평가해야 한다는 것을 증명했다.', 'sw': 'Tafsiri halisi inahitaji taarifa ya kiwango cha dokumentano, ambacho kinapuuzwa na tafsiri ya mashine ya kigezo. Kazi ya hivi karibuni imeonyesha kuwa muunganiko wa kiwango cha dokumentari unaweza kuboreshwa kwa kutumia taarifa za lugha ya malengo tu (TL). Tunafoma muundo wa APE ulioendelea kwa kiasi kikubwa unaunganisha muktadha wa vyanzo. Utafiti wa ufanisi na usawa wa kibinadamu katika tafsiri ya Kiingereza na Urusi unaonyesha kuwa mtindo wa upatikanaji wa muktadha wa vyanzo unaonyesha matumizi makubwa ya APE ya kimapenzi kwa kiasi kikubwa, madhara yanayopuuzwa kwa kiasi kikubwa na mbinu za uchunguzi. Matokeo yetu yanaonyesha kuwa mifano pekee ya TL inaongezea ufanisi bila kuboresha ufanisi wa kutosha, wakionyesha haja ya kupata mazingira kwenye maandishi ya vyanzo vya habari kwa ajili ya kuhariri baada ya kujitolea. Pia wanaonyesha maeneo ya kipofu katika njia za kujitegemea kwa ajili ya uchunguzi wa lengo na kuonyesha haja ya kutathmini kiwango cha tafsiri kwa kiwango cha nyaraka kwa uaminifu.', 'hr': 'Tačni prevod zahtijeva informacije o razini dokumenta, koje ignorira prevod stroja na razini rečenica. Skorašnji rad pokazao je da se konsekvencija razine dokumenta može poboljšati s automatskim posledicama (APE) koristeći samo informacije o ciljnom jeziku (TL). Proučavamo prošireni model APE koji dodatno integrira izvorni kontekst. Ljudska procjena tekućine i adekvatnosti na engleskom-ruskom prevodu pokazuje da model s pristupom izvornom kontekstu značajno iznosi monojezičku APE u smislu adekvatnosti, učinak koji se u velikoj mjeri ignorira automatska procjena metrika. Naši rezultati pokazuju da samo modeliranje TL povećava tečnost bez poboljšanja adekvatnosti, pokazujući potrebu za uvjetovanje izvornog teksta za automatski postured. Oni također naglašavaju slijepe tačke u automatskim metodama ciljne procjene i pokazuju potrebu za ljudske procjene za pouzdanu procjenu kvalitete prevoda na nivou dokumenta.', 'af': "Redigeer vertaling benodig dokument-vlak inligting, wat word geïgnoreer deur sentence-vlak masjien vertaling. Onlangse werk het bevestig dat dokumentvlak-konsistensie kan wees verbeter met outomatiese post-redigeering (APE) gebruik slegs doel-taal (TL) inligting. Ons studeer 'n uitgebreide APE model wat bygevoeg bronkontekstek integreer. 'n Mense evaluasie van fluidigheid en adekviteit in Engels-Russiese vertaling vertel dat die model met toegang tot bronkontekstek betekenlik monolinglike APE in terms van adekviteit uitvoer, 'n effek groot geïgnoreer deur outomatiese evaluasie metrike. Ons resultate wys dat slegs TL-modelling fluiditeit vergroot sonder om adekuasie te verbeter, wys die behoefte vir voorwaarding op bron teks vir outomatiese post-redigeering. Hulle verlig blinde plekke ook in outomatiese metodes vir doelgemaakte evaluasie en wys die behoefte vir menslike evaluasie om dokumentvlak vertaling kwaliteit betroubaar te evalueer.", 'fa': 'ترجمه دقیق نیاز به اطلاعات سطح سند است که توسط ترجمه دستگاه سطح جمله نادیده می\u200cگیرد. کارهای اخیرا نشان داده است که هماهنگی سطح سند می\u200cتواند با استفاده از فقط اطلاعات هدف\u200cزبان (TL) با ویرایش بعد از خود (APE) بهتر شود. ما یک مدل APE گسترده را مطالعه می کنیم که اضافه\u200cاش محیط منبع را جمع می\u200cکند. یک ارزیابی انسان از مایع و قابلیت در ترجمه\u200cهای انگلیسی و روسیه نشان می\u200cدهد که مدل با دسترسی به محیط منبع زیادی APE یک زبان به عنوان قابلیت، تاثیر بزرگ توسط متریک ارزیابی خودکار نادیده می\u200cشود. نتیجه\u200cهای ما نشان می\u200cدهند که تنها نمونه\u200cسازی TL بی\u200cنیازی به مناسب افزایش می\u200cدهد، و نیازی برای شرایط متن منبع برای ویرایش\u200cسازی بعد از خود نشان می\u200cدهد. آنها همچنین نقطه\u200cهای کور را در روش\u200cهای خودکار برای ارزیابی هدف و نیازی برای ارزیابی انسان برای ارزیابی کیفیت ترجمه\u200cهای سطح سند به قابل اطمینان نشان می\u200cدهند.', 'am': 'የፋይል መረጃ የአሁኑ ስራ ሰነድ - ደረጃው ተቃውሞ በፖስታ ማቀናጃ (APE) የተጠቃሚ-ቋንቋ (TL) መረጃ የተጠቀመ ሆኖ ይሻላል፡፡ የአፍሪካ ምሳሌ በተጨማሪም የኩነታችንን ክፍል እናሳድራለን፡፡ በንግግሊዝና-ራሽኛ ትርጓሜ በሚያስፈልገው የሰው ድምፅ ማረጋገጫ በሙሉ ጉዳይ ላይ ማግኘት የሚችል ሞዴል በክፍለ ግንኙነት የሞላዊ ቋንቋ አካባቢ አካባቢ ነው፡፡ ፍጥረታችን የTL-ብቻ ምሳሌ ፍጥረትን ሳይበዛ ማሳየትን እንዲያበዛ፣ የኩነቶች ጽሑፎችን ለራሱ ማቀናጃ ያስፈልጋል፡፡ ሰነድ-ደረጃ ትርጉም ብዛት እንዲያስተካክሉ የሰው ማስታወቂያ የሚያስፈልገውን እና በአስማትነታቸው የዕውሮችን ስፍራዎች በተመሳሳይ አካባቢ እንዲያሳዩ ነው፡፡', 'tr': "Dyggat terjime etmek üçin desktap derejesi maglumaty gerek. Şahsy düzümler makine terjime görä terjime edilsin. Ýakyndaky işlem diňe maksady dil (TL) maglumaty ullanýan sened derejesini otomatik edip edip biljekdigini görkezildi. Biz çeşme kontekstini ekleýän ABE nusgasyny öwrenýärik. İnsanyň Iňlis-Russiýa terjimede nähili ýeterlik we ýeterlik değerlendirmegi görkezýär ki, çeşme kontekstüne erişen nusga adalatsyzlyk bilen monolingual APE'i ýeterlik ýöredip, awtomatik değerlendirme metrikleri tarapyndan ýok etýän täsiri täsir edýär. Biziň netijelerimiz diňe TL modellerinden ýeterlik gowurak etmek üçin arzuwyny artýandygyny görkezýär, awtomatik taýdan soňra metin üçin çeşme metiniň saklamagyny görkezýär. Olar hem maksady deňlemek üçin kör noktalary awtomatik taýýarlandyrylýar we adamlaryň deňlemesi üçin sened derejesini ynamly terjime etmek üçin gerekligini kanıtlaýarlar.", 'bn': 'সঠিক অনুবাদের তথ্য প্রয়োজন ডকুমেন্ট- স্তরের তথ্য প্রয়োজন, যা ক্রেণ-স্তর মেশিন অনুবাদ দ্বারা উপেক্ষা  Recent work has demonstrated that document-level consistency can be improved with automatic post-editing (APE) using only target-language (TL) information.  আমরা একটি বিস্তৃত APE মডেল গবেষণা করছি যা সোর্সের প্রেক্ষাপটে একত্রিত করে। ইংরেজি-রাশিয়ান অনুবাদে প্রভাব এবং যথাযথ মানুষের মানুষের মূল্য প্রকাশ করেছে যে উৎস প্রদানের প্রেক্ষাপটে প্রবেশ করার মডেল গুরুত্বপূর্ণ ভাষায় মোটোলি আমাদের ফলাফল দেখা যাচ্ছে যে টিএল-শুধুমাত্র মডেলিং তাদের প্রভাব বাড়িয়ে দেয় যথেষ্ট বৃদ্ধি ছাড়া, স্বয়ংক্রিয় পরিবর্তনের জন্য সোর তারা স্বয়ংক্রিয় পদ্ধতিতে অন্ধ স্থান উল্লেখ করে লক্ষ্য করা মূল্যের জন্য এবং মানুষের মান মূল্যের প্রয়োজন বিশ্বস্তি প্রদর্শন করে নথ', 'hy': 'Accurate translation requires document-level information, which is ignored by sentence-level machine translation.  Վերջին աշխատանքները ցույց են տալիս, որ փաստաթղթի մակարդակի համապատասխանությունը կարող է բարելավվել ավտոմատիկ խմբագրման հետևից (APE) օգտագործելով միայն նպատակային լեզու (TL) տեղեկատվություն: Մենք ուսումնասիրում ենք ընդլայնված APE մոդել, որը նաև ինտեգրում է աղբյուրի կոնտեքստը: Անգլերեն-ռուսերեն թարգմանության ճկունության և ճկունության մարդկային գնահատումը ցույց է տալիս, որ աղբյուրի կոնտեքստին հասանելիության մոդելը կարևորաբար գերազանցում է միալեզվական APE-ը ճկունության տեսանկյունից, ազդեցությունը, որը հիմնականում անտեսվում Մեր արդյունքները ցույց են տալիս, որ միայն տեխնոլոգիաների մոդելը բարձրացնում է ճկունությունը առանց բարելավելու համապատասխանատվությունը, ցույց տալով աղբյուրի տեքստի պայմանավորման կարիքը ավտոմատիկ հետխմբագրման համար: Նրանք նաև ներկայացնում են կոյր կետերը նպատակային գնահատման ավտոմատիկ մեթոդներում և ցույց են տալիս մարդկային գնահատման կարիքը փաստաթղթի մակարդակի թարգմանության որակը վստահելի գնահատելու համար:', 'ca': "Una traducció exacta requereix informació a nivell de document, que es ignora per traducció màquina a nivell de frases. La feina recent ha demostrat que la consistencia a nivell de documentos es pot millorar amb la postedició automàtica (APE) utilitzant només la informació del llenguatge alvo (TL). Estudem un model ampliat d'APE que integrat addicionalment el context de fonts. A human evaluation of fluency and adequacy in English-Russian translation reveals that the model with access to source context significantly outperforms monolingual APE in terms of adequacy, an effect largely ignored by automatic evaluation metrics.  Els nostres resultats demostren que la modelació exclusivament TL augmenta la fluïtat sense millorar la adequació, demostrant la necessitat de condicionar-se amb el text de font per a la postedició automàtica. També destaquen punts cecs en mètodes automàtics d'evaluació mirada i demostren la necessitat d'una evaluació humana per evaluar la qualitat de traducció a nivell de documentos de manera fiable.", 'bs': 'Tačni prevod zahtijeva informacije o nivou dokumenta, koje ignorira prevod stroja na nivou rečenica. Nedavni rad je pokazao da se konsekvencija na nivou dokumenta može poboljšati s automatskim posledicama (APE) koristeći samo informacije o ciljnom jeziku (TL). Proučavamo produžen model APE koji dodatno integrira izvorni kontekst. Ljudska procjena tekućine i adekvatnosti na engleskom-ruskom prevodu pokazuje da model s pristupom izvornom kontekstu značajno iznosi monojezičku APE u smislu adekvatnosti, učinak koji je u velikoj meri ignoriran automatskim procjenama. Naši rezultati pokazuju da samo modeliranje TL povećava tečnost bez poboljšanja adekvatnosti, pokazujući potrebu za kondicioniranjem izvornog teksta za automatski posteditanje. Oni također naglašavaju slijepe tačke u automatskim metodama ciljne procjene i pokazuju potrebu za ljudskom procjenom da se pouzdano procjenjuje kvalitet prevoda na nivou dokumenta.', 'cs': 'Přesný překlad vyžaduje informace na úrovni dokumentu, které jsou ignorovány strojovým překladem na úrovni věty. Nedávné práce ukázaly, že konzistence na úrovni dokumentů lze zlepšit pomocí automatické post-editace (APE) používající pouze informace v cílovém jazyce (TL). Studujeme rozšířený APE model, který navíc integruje zdrojový kontext. Lidské hodnocení plynulosti a adekvátnosti v anglicko-ruském překladu ukazuje, že model s přístupem ke zdrojovému kontextu výrazně překonává monojazyčné APE z hlediska adekvátnosti, což efekt automatickými hodnotícími metrikami do značné míry ignoruje. Naše výsledky ukazují, že modelování pouze TL zvyšuje plynulost bez zlepšení adekvátnosti, což ukazuje potřebu upravit zdrojový text pro automatickou post editaci. Dále upozorňují na slepá místa v automatických metodách cíleného hodnocení a ukazují potřebu lidského hodnocení pro spolehlivé hodnocení kvality překladu na úrovni dokumentů.', 'et': 'Täpne tõlge nõuab dokumenditasemel teavet, mida lausetasemel masintõlge ignoreerib. Hiljutised tööd on näidanud, et dokumentide tasandil on võimalik parandada automaatse järeltöötluse (APE) abil, kasutades ainult sihtkeele teavet. Uurime laiendatud APE mudelit, mis lisaks integreerib lähtekonteksti. Inimlik hinnang inglise-vene tõlke sujuvusele ja piisavusele näitab, et allikakontekstile juurdepääsuga mudel on piisavuse osas märkimisväärselt parem kui ühekeelne APE, mida automaatsed hindamismeetodikud suuresti ignoreerivad. Meie tulemused näitavad, et ainult TL modelleerimine suurendab sujuvust ilma adekvaatsust parandamata, näidates vajadust konditsioneerida lähteteksti automaatseks järeltöötlemiseks. Samuti tõstetakse need esile pimedad nurgad automaatsete sihipärase hindamise meetodite puhul ning tõestatakse vajadust inimhindamise järele, et hinnata dokumentide tasandil tõlkekvaliteeti usaldusväärselt.', 'sq': 'Përkthimi i saktë kërkon informacion në nivelin e dokumentit, i cili injorohet nga përkthimi në nivelin e fjalëve në makinë. Puna e fundit ka demonstruar se konsistenca e nivelit të dokumentit mund të përmirësohet me informacionin automatik pas redigimit (APE) duke përdorur vetëm gjuhën-objektiv (TL). We study an extended APE model that additionally integrates source context.  Një vlerësim njerëzor i fluencës dhe përshtatshmërisë në përkthimin anglo-rusë zbulon se modeli me akses në kontekstin e burimit ekziston në mënyrë të konsiderueshme në APE monogjuhësore lidhur me përshtatshmërinë, një efekt i injoruar kryesisht nga metrikat automatike të vlerësimit. Rezultatet tona tregojnë se modelimi vetëm TL rrit fluencën pa përmirësimin e përshtatshmërisë, duke demonstruar nevojën për kushtëzim në tekstin e burimit për posteditimin automatik. Ata theksojnë gjithashtu pikat e verbra në metodat automatike për vlerësimin e synuar dhe demonstrojnë nevojën për vlerësimin njerëzor për vlerësimin e cilësisë së përkthimit në nivelin e dokumentit në mënyrë të besueshme.', 'fi': 'Tarkka käännös vaatii dokumenttitason tietoja, jotka lausetason konekäännös jättää huomiotta. Viimeaikaiset tutkimukset ovat osoittaneet, että dokumenttitason johdonmukaisuutta voidaan parantaa automaattisella jälkimuokkauksella (APE) käyttäen vain kohdekielitietoa (TL). Tutkimme laajennettua APE-mallia, joka lisäksi integroi lähdekontekstin. Ihmisen tekemä arvio englannin-venäjän käännöksen sujuvuudesta ja riittävyydestä paljastaa, että lähdekontekstiin pääsyn sisältävä malli suoriutuu merkittävästi monikielisestä APE:stä riittävyyden suhteen, mikä on suurelta osin sivuutettu automaattisissa arviointimittareissa. Tuloksemme osoittavat, että vain TL-mallinnus lisää sujuvuutta parantamatta sen riittävyyttä, mikä osoittaa, että lähdetekstin ehdollistaminen automaattista jälkimuokkausta varten on tarpeen. Ne myös korostavat kohdennetun arvioinnin automaattisissa menetelmissä sokeita kohtia ja osoittavat, että inhimillisen arvioinnin on tarpeen arvioida dokumenttitason käännöksen laatua luotettavasti.', 'az': "Tərcümə düzgün səviyyə məlumatı lazım edir, ki bu cümə-səviyyə maşına çevirilməsi ilə məhv edilir. Son işləri təkcə məqsəd dili (TL) məlumatlarını istifadə edərək təkrar-düzəltmə (APE) ilə təkrar-düzəltmə müəyyən ediləcəyini göstərdi. Biz çox uzaqlaşdırılmış APE modelini təhsil edirik ki, mənbə məlumatını artıq birləşdirir. İnsanın İngilizə-Rus çevirisində təsirlik və uyğunluğunun değerlendirməsi göstərir ki, mənbə kontekstünə istifadə edilən modellərin monodil APE'nin uyğunluğu ilə çox böyük bir təsiri, avtomatik değerlendirmə metrikləri ilə dəyişmədiyini göstərir. Bizim sonuçlarımız TL yalnız modellərimiz tətbiqliyi yaxşılaşdırmadan fərqli artırmağı göstərir, avtomatik post-editing üçün mənbə mətnlərinin tətbiqlərinin müəyyən edilməsini göstərir. Onlar həmçin in məqsədilə vuruşmaq üçün automatik məqsədilə kor nöqtələri də işıqlandırır və insanların ölçülərinin ehtiyacını təsdiqlənməsi üçün səviyyədə çevirilən qiymətini təsdiqləyirlər.", 'ha': "@ action: button Yin aikin da aka sani ya nuna cewa za'a samar da wata takardar-daraja mai daidaita da shiryarwa na farat-editi (PAET) da amfani da maɓallin maganar-language kawai (TL). Tuna karatun wani misali da aka shimfiɗa wa PAEki wanda ke haɗa kowacan maɓalli. Wata haƙin mutum na gaskiyar fassarar Ingiriya-Ruushi, yana bayyana cewa misalin da za'a iya haɗi zuwa maɓallin wuri mai girma, yana samar da amfani na masu nau'i da ɗai'a, mai fassarar amfani da metrikan ƙaddara farat ɗaya. MatamayinMu na nuna cewa, shirin TL-kawai yana ƙara furiya, kuma bã ya gyãra da inganci, kuma ya nuna muhamako da ake da muhimmin ka kan matsayin kwanan-editi farat ɗaya. Suna ƙayyade sunayen makanta cikin metoden farat-farat wa qiymati da ake amfani da shi, kuma suna nuna muhimmada wa mutum dõmin a ƙaddara sifar fassarar-daraja-takardar.", 'sk': 'Natančen prevod zahteva informacije na ravni dokumenta, ki jih strojni prevod na ravni stavka prezre. Nedavno delo je pokazalo, da je doslednost na ravni dokumenta mogoče izboljšati s samodejnim postnim urejanjem (APE) z uporabo samo informacij o ciljnem jeziku (TL). Preučujemo razširjeni model APE, ki dodatno vključuje vir konteksta. Človeška ocena tekočega in ustreznega prevajanja angleško-ruskega prevoda kaže, da model z dostopom do izvornega konteksta bistveno presega enojezično APE v smislu ustreznosti, kar je učinek v veliki meri ignoriran pri samodejnih meritvah ocenjevanja. Naši rezultati kažejo, da modeliranje samo TL povečuje tekočost brez izboljšanja ustreznosti, kar kaže na potrebo po kondicioniranju na izvornem besedilu za avtomatsko post-urejanje. Prav tako poudarjajo mrtve kote pri avtomatskih metodah ciljnega ocenjevanja in dokazujejo, da je treba ocenjevanje človeka zanesljivo oceniti kakovost prevodov na ravni dokumentov.', 'he': 'התרגום מדויק דורש מידע על רמת המסמכים, אשר מתעלם על ידי התרגום מכונת רמת המשפטים. העבודה האחרונה הוכיחה שיותרות רמת המסמכים יכולה להשתפר עם מידע לאחר העורה אוטומטי (APE) בשפה המטרה בלבד (TL). אנחנו לומדים מודל APE מורחב שמכיל באופן נוסף קונטקסט מקור. הערכה אנושית של נוזנות ומתאימות בתרגום אנגלי-רוסי מראה שהמודל עם גישה לקונקסט מקור יוצא משמעותית מעל APE מונושפת במונחים של מתאימות, השפעה שהתעלמו בעיקר על ידי מטריות הערכה אוטומטית. התוצאות שלנו מראות שדוגמניות בלבד TL מגבילות נוזלות בלי לשפר את התאימה, להוכיח את הצורך לתאים על טקסט מקור לאחר העורר אוטומטי. הם גם מזכירים נקודות עיוורות בשיטות אוטומטיות להערכה ממוקדת והוכיחו את הצורך להערכה אנושית כדי להעריך את איכות התרגום ברמה המסמכים באופן אמיתי.', 'bo': 'precise translation requires document-level information, which is ignored by sentence-level machine translation. ཉེ་ཆར་གྱི་ལས་ཀ་ལྟ་ཀློག་ནི་ཡིག་ཆའི་གནས་ཡུལ་ཁོ་ན་རང་འགུལ་གྱིས་བསྒྱུར་བཅོས་ཀྱི་གནས་ཡུལ་དང་ཐོག་ཏུ་ཡར་རྒྱས་ཡོད་པ ང་ཚོས་རྒྱ་བསྐྱེད་པའི་APE རྣམ་པ་ཞིག་ལ་ཁུངས་དང་ཁུངས་ཡོད་པ་དེ་འགྱུར་ཁུངས་ཁ་ཤས་བསྡད་ཡོད། A human evaluation of fluency and adequacy in English-Russian translation reveals that the model with access to source context significantly outperforms monolingual APE in terms of adequacy, an effect largely ignored by automatic evaluation metrics. The human evaluation of fluency and adequacy in English-Russian translation reveals that the model with access to source context ང་ཚོའི་rezultat They also highlight blind spots in automatic methods for targeted evaluation and demonstrate the need for human assessment to evaluate document-level translation quality reliably.', 'jv': 'translation text-editor-action Awak dhéwé ngerasai model apa sing luwih dumadhi, dadi nambah ngono kontèks kelas Wanggang wong liyane karo kapan pangan lan nganggo dolanan ingkang-Rusi Rejalekan nambarang kelas-modellung ung ung gak bener tentang karo akeh nesaturan, iso nggambar kelas nggawe layakno tentang kanggo kelas teks mbusun kanggo ngubah auto-Edit. Genjer-genjer saiki nglanggar bantuan liyane ing automatically kanggo tarjamahan kanggo kowé nggawe hasil karo nggawe hasil kanggo nggawe tarjamahan kanggo kuwi nggawe kalite itoleh dokumen banget.'}
{'en': 'Grapheme-Based Cross-Language Forced Alignment : Results with Uralic Languages', 'ar': 'المحاذاة الإجبارية عبر اللغات المستندة إلى Grapheme: النتائج مع اللغات الأورالية', 'es': 'Alineación forzada entre lenguajes basada en grafemas: resultados con lenguajes urálicos', 'pt': 'Alinhamento forçado entre idiomas baseado em grafema: resultados com idiomas urálicos', 'fr': 'Alignement forcé entre langues basé sur des graphèmes\xa0: résultats avec les langues ouraliennes', 'zh': '基于字素跨语强对齐:乌拉尔语也', 'ja': 'グラフェムベースのクロスランゲージ強制アライメント：ウラル言語との結果', 'hi': 'Grapheme-आधारित क्रॉस-लैंग्वेज मजबूर संरेखण: यूरालिक भाषाओं के साथ परिणाम', 'ru': 'Форсированное выравнивание на основе графемного языка: результаты с уральскими языками', 'ga': 'Ailíniú Éigeantach Trasteanga Bunaithe ar Ghraifí: Torthaí le Teangacha Úralacha', 'ka': 'Name', 'el': 'Διαγλωσσική αναγκαστική ευθυγράμμιση με βάση τα γραφήματα: Αποτελέσματα με ουρικές γλώσσες', 'hu': 'Grafémalapú, keresztnyelvű kényszerített igazítás: Eredmények az urális nyelvekkel', 'it': 'Allineamento forzato cross-language basato sul grafico: risultati con le lingue Urali', 'kk': 'Графим- негіздеген көздегі тілдерді түрлеу: Uralic тілдермен нәтижелер', 'lt': 'Grafomis grindžiamas tarpkalbinis suderinimas: rezultatai su miesto kalbomis', 'mk': 'Grapheme-Based Cross-Language Forced Alignment: Results with Uralic Languages', 'ms': 'Jajaran Salib Bahasa Berasas Graf: Hasil dengan Bahasa Ural', 'ml': 'ഗ്രാഫീമെ- അടിസ്ഥാനമായ ക്രോസ്- ഭാഷ ശക്തിയുള്ള ഏകീകരണം: ഉരാലിക് ഭാഷയുടെ ഫലങ്ങള്\u200d', 'no': 'Grafembasert krysspråk- forsterkte justering: Resultat med Uraliske språk', 'pl': 'Wymuszone wyrównanie oparte na grafamech między językami: wyniki z językami uralskimi', 'mt': 'Allinjament Fuq il-Lingwa Kontra l-Lingwa Ibbażat fuq Grafika: Riżultati bil-Lingwa Urali', 'ro': 'Alinierea forțată între limbi bazată pe grafime: rezultate cu limbile uralice', 'mn': 'Grapheme-based Cross-Language Forced Alignment: Results with Uralic Languages', 'sr': 'Na Grafemu bazirano krstojezičko prisiljeno ispravljanje: rezultati sa Uraličkim jezicima', 'si': 'Name', 'so': 'Luqada qasabka ah ee Grapheme-Based Cross-Language: Results with Uralic language', 'sv': 'Grafemebaserad tvärspråksbaserad tvångsanpassning: Resultat med uraliska språk', 'ta': 'கிராப்பெம்- அடிப்படையான கிராஸ்- மொழி உறுதிப்படுத்தப்பட்டது: முடிவுகள் Uralic மொழிகளுடன்', 'ur': 'Grapheme-Based Cross-Language Forced Alignment: Results with Uralic Languages', 'uz': 'Name', 'vi': 'Biểu tượng Nhân văn học:', 'bg': 'Графично базирано насилствено подравняване между езиците: резултати с уралските езици', 'da': 'Grafemebaseret tværsprog tvunget tilpasning: Resultater med Uralske sprog', 'nl': 'Grafeem-gebaseerde cross-language gedwongen uitlijning: resultaten met urale talen', 'hr': 'Na Grafemu bazirano krstojezičko prisiljeno ispravljanje: rezultati s Uraličkim jezicima', 'id': 'Alinjasi Terpaksa Berdasarkan Grafen: Hasil dengan Bahasa Ural', 'de': 'Graphembasierte Cross-Language Forced Alignment: Ergebnisse mit uralischen Sprachen', 'fa': 'تنظیمات مجبور به زبان\u200cهای عبور بر اساس گرافم: نتیجه\u200cها با زبانهای Uralic', 'ko': '도형 기반의 다중 언어 강제 정렬: 우랄어의 결과', 'sw': 'Matokeo ya lugha za Ki-Uraliki', 'tr': 'Görniş Opşenler:', 'af': 'Grapheme- Based Cross- Language Forced Alignment: Results with Uralic Languages', 'sq': 'Rregullimi i detyruar ndër-gjuhë bazuar në grafikë: Rezultatet me gjuhët Urale', 'am': 'ቋንቋ', 'hy': 'Գրաֆեմայի հիմնված խաչլեզվի ստիպված հարմարեցման. Արդյունքները ուրալական լեզուներով', 'az': 'Graphem-tabanl캼 칂ift Dili S톛vv톛l S톛fl톛t: Ural Dill톛ri il톛 N칬vb톛l톛r', 'bs': 'Na Grafemu bazirano krstojezičko prisiljeno ispravljanje: rezultati s Uraličkim jezicima', 'bn': 'গ্রাফেম- ভিত্তিক ক্রস- ভাষা বাধ্যিক স্থান: উরালিক ভাষার ফলাফল', 'ca': 'Allinjament forçat per llengües cruzades basat en gràfics: Resultats amb llengües uràliques', 'fi': 'Grafeemipohjainen monikielinen pakotettu linjaus: tulokset uralin kielillä', 'cs': 'Vynucené zarovnání mezi jazyky založené na grafému: výsledky s uralskými jazyky', 'et': 'Grafeemipõhine keeleülene sunnijoondamine: tulemused uurali keeltega', 'jv': 'Graffeme-Based Kros-Language forcement Align: Effects with Uralic Languages', 'he': 'התאמה כפויה על שפת צלב מבוססת על גראפים: תוצאות עם שפות אוראליות', 'ha': 'KCharselect unicode block name', 'sk': 'Grafemska prisilna poravnava med jeziki: rezultati z uralskimi jeziki', 'bo': 'Grapheme-Based Cross-Language Forced Alignment: Results with Uralic Languages'}
{'en': 'Forced alignment is an effective process to speed up linguistic research. However, most forced aligners are language-dependent, and under-resourced languages rarely have enough resources to train an acoustic model for an aligner. We present a new Finnish grapheme-based forced aligner and demonstrate its performance by aligning multiple Uralic languages and English as an unrelated language. We show that even a simple non-expert created grapheme-to-phoneme mapping can result in useful word alignments.', 'pt': 'O alinhamento forçado é um processo eficaz para acelerar a pesquisa linguística. No entanto, a maioria dos alinhadores forçados depende do idioma e os idiomas com poucos recursos raramente têm recursos suficientes para treinar um modelo acústico para um alinhador. Apresentamos um novo alinhador forçado baseado em grafema finlandês e demonstramos seu desempenho alinhando vários idiomas urálicos e o inglês como um idioma não relacionado. Mostramos que mesmo um simples mapeamento grafema para fonema criado por um não especialista pode resultar em alinhamentos de palavras úteis.', 'ar': 'المحاذاة القسرية هي عملية فعالة لتسريع البحث اللغوي. ومع ذلك ، فإن معظم أدوات التقويم القسرية تعتمد على اللغة ، ونادرًا ما تمتلك اللغات منخفضة الموارد موارد كافية لتدريب نموذج صوتي للمصفف. نقدم تقويمًا إجباريًا جديدًا يعتمد على حروف الحروف الفنلندية ونظهر أداءه من خلال محاذاة لغات أورالية متعددة والإنجليزية كلغة غير مرتبطة. نبيِّن أنه حتى رسم الخرائط البسيط الذي تم إنشاؤه من قبل شخص غير خبير من الحروف إلى الصوت يمكن أن يؤدي إلى محاذاة كلمات مفيدة.', 'fr': "L'alignement forcé est un processus efficace pour accélérer la recherche linguistique. Cependant, la plupart des aligneurs forcés sont dépendants de la langue, et les langues sous-dotées ont rarement assez de ressources pour entraîner un modèle acoustique pour un aligneur. Nous présentons un nouvel aligneur forcé basé sur le graphème finlandais et démontrons ses performances en alignant plusieurs langues ouraliennes et l'anglais en tant que langue indépendante. Nous montrons que même un simple mappage graphème-phonème créé par des non-experts peut entraîner des alignements de mots utiles.", 'es': 'La alineación forzada es un proceso eficaz para acelerar la investigación lingüística. Sin embargo, la mayoría de los alineadores forzados dependen del idioma, y los idiomas con pocos recursos rara vez tienen recursos suficientes para entrenar un modelo acústico para un alineador. Presentamos un nuevo alineador forzado finlandés basado en grafemas y demostramos su rendimiento alineando múltiples idiomas urálicos e inglés como idioma no relacionado. Demostramos que incluso un simple mapeo de grafema a fonema creado por expertos puede resultar en alineaciones de palabras útiles.', 'zh': '强对齐者,疾言之效也。 然大抵强制于齐器,资源贫乏之言鲜足以练齐器之声学。 展一新芬兰语字素之齐器,因齐之多乌拉尔语英语以展其能。 虽简非创字素音素映射可以有用单词齐。', 'ja': '強制アライメントは、言語学的研究を加速するための効果的なプロセスです。しかし、ほとんどの強制アライナーは言語に依存しており、リソース不足の言語はアライナーの音響モデルを訓練するのに十分なリソースを持つことはほとんどありません。新しいフィンランド語の文法ベースの強制アライナーを提示し、複数のウラル語と英語を無関係な言語としてアライメントすることで、そのパフォーマンスを実証します。私たちは、単純な非専門家が作成した文法から音素へのマッピングでさえ、有用な単語の配置をもたらすことができることを示しています。', 'hi': 'भाषाई अनुसंधान को गति देने के लिए मजबूर संरेखण एक प्रभावी प्रक्रिया है। हालांकि, अधिकांश मजबूर संरेखक भाषा-निर्भर होते हैं, और अंडर-रिसोर्स्ड भाषाओं में शायद ही कभी एक संरेखक के लिए ध्वनिक मॉडल को प्रशिक्षित करने के लिए पर्याप्त संसाधन होते हैं। हम एक नया फिनिश grapheme-आधारित मजबूर aligner प्रस्तुत करते हैं और एक असंबंधित भाषा के रूप में कई Uralic भाषाओं और अंग्रेजी को संरेखित करके इसके प्रदर्शन का प्रदर्शन करते हैं। हम दिखाते हैं कि यहां तक कि एक साधारण गैर-विशेषज्ञ ने ग्राफेम-टू-फोनम मैपिंग बनाई है, जिसके परिणामस्वरूप उपयोगी शब्द संरेखण हो सकते हैं।', 'ru': 'Вынужденное выравнивание является эффективным процессом для ускорения лингвистических исследований. Однако большинство принудительных выравнивателей зависят от языка, и языки, не обеспеченные достаточными ресурсами, редко располагают достаточными ресурсами для обучения акустической модели выравнивателя. Мы представляем новый форсированный выравниватель на основе финских графем и демонстрируем его эффективность, выравнивая несколько уральских языков и английский язык как несвязанный язык. Мы показываем, что даже простое, не созданное экспертами, отображение «графема-фонема» может привести к полезным выравниваниям слов.', 'ga': 'Is próiseas éifeachtach é ailíniú éigeantach chun taighde teanga a bhrostú. Mar sin féin, tá formhór na n-ailínithe faoi éigean ag brath ar theanga, agus is annamh a bhíonn a ndóthain acmhainní ag teangacha a bhfuil dóthain acmhainní acu chun samhail fhuaimiúil a oiliúint d’ailíniú. Cuirimid i láthair ailíniú éigean Fionlainnis bunaithe ar graifíne agus léirímid a fheidhmíocht trí iltheangacha Úralacha agus an Béarla a ailíniú mar theanga nach mbaineann le hábhar. Léirímid go bhféadfadh ailíniú focal úsáideach a bheith mar thoradh ar mhapáil graifim-go-fóinéim shimplí cruthaithe fiú gan saineolaithe.', 'el': 'Η αναγκαστική ευθυγράμμιση είναι μια αποτελεσματική διαδικασία για την επιτάχυνση της γλωσσικής έρευνας. Ωστόσο, οι περισσότερες καταναγκαστικές ευθυγραμμίσεις εξαρτώνται από τη γλώσσα και οι γλώσσες που δεν διαθέτουν επαρκείς πόρους σπάνια έχουν αρκετούς πόρους για να εκπαιδεύσουν ένα ακουστικό μοντέλο για έναν ευθυγραμμιστή. Παρουσιάζουμε μια νέα φινλανδική εξαναγκαστική ευθυγράμμιση βασισμένη στο γραφέμιο και καταδεικνύουμε την απόδοσή της ευθυγραμμίζοντας πολλαπλές ουρικές γλώσσες και τα αγγλικά ως μη συνδεδεμένη γλώσσα. Δείχνουμε ότι ακόμα και μια απλή χαρτογράφηση γραφήματος-φωνήματος που δεν έχει δημιουργήσει ειδικός μπορεί να οδηγήσει σε χρήσιμες ευθυγραμμίσεις λέξεων.', 'hu': 'A kényszerített összehangolás hatékony folyamat a nyelvi kutatás felgyorsítására. Azonban a legtöbb erőszakos szabályozó nyelvfüggő, és az alacsony forrásokkal rendelkező nyelvek ritkán rendelkeznek elegendő erőforrással ahhoz, hogy akusztikus modellt képezzenek egy szabályozó számára. Bemutatunk egy új finn graféma alapú kényszerített igazítót, és bemutatjuk teljesítményét több uráli nyelv és az angol, mint egy független nyelv összehangolásával. Megmutatjuk, hogy még egy egyszerű, nem szakértő által létrehozott graféma-fonéma leképezés is hasznos szóigazításokat eredményezhet.', 'it': "L'allineamento forzato è un processo efficace per accelerare la ricerca linguistica. Tuttavia, la maggior parte degli allineatori forzati dipendono dal linguaggio, e i linguaggi poco dotati di risorse raramente hanno risorse sufficienti per formare un modello acustico per un allineatore. Presentiamo un nuovo allineamento forzato basato sul grafema finlandese e dimostriamo le sue prestazioni allineando più lingue Uraliche e l'inglese come lingua non correlata. Mostriamo che anche una semplice mappatura grafema-fonema creata non da esperti può risultare in utili allineamenti di parole.", 'ka': 'ძალიან გადაწყვეტილება არის ეფექტიური პროცესი, რომელიც სისტემისტიკური შესწავლობა. მაგრამ, უფრო ძალიან ძალიან მხარდაჭირებული მხარდაჭირებელი ენაზე დაახლოებით, და რესურსურებული ენაზე არაფერად მხარდაჭირებული რესურსები არაფერად არაფერად არა ჩვენ ახალი ფინქური გრაფიმების დაბათებული ძალიან მხარდაჭირებული მხარდაჭირებელი ჩვენ გამოჩვენებთ და გამოჩვენებთ მისი წარმოდგენებას, რამდენიმე სურალიური ენები და ან ჩვენ გამოჩვენებთ, რომ კიდევ უფრო მხოლოდ არ-ექსპერტის შექმნა გრაფიმე-სა-ფონემის მაპრაფიკაცია შეიძლება გამოიყენება სიტყვების დასწ', 'mk': 'Принуденото прилагодување е ефикасен процес за забрзување на јазичното истражување. Сепак, повеќето принудени присилувачи се зависни од јазикот, а јазиците со недоволни ресурси ретко имаат доволно ресурси за обука на акустички модел за присилувач. Презентираме нов фински графем-базиран присилен аланирач и ја демонстрираме неговата резултат со прилагодување на повеќе уралички јазици и англиски јазик како неповрзан јазик. Ние покажуваме дека дури и едноставното неекспертско создадено мапирање графем до фоном може да резултира со корисни зборови.', 'ms': 'Penjajaran terpaksa adalah proses yang efektif untuk mempercepat kajian bahasa. Namun, kebanyakan penyesuaian terpaksa bergantung pada bahasa, dan bahasa bawah sumber jarang mempunyai sumber yang cukup untuk melatih model akustik untuk penyesuaian. Kami memperkenalkan penyesuaian paksa berbasis grafem Finlandia baru dan menunjukkan prestasinya dengan menyesuaikan berbagai bahasa Ural dan bahasa Inggeris sebagai bahasa yang tidak berkaitan. Kami menunjukkan bahawa walaupun petakan grafem-ke-fonem tidak-ahli yang mudah dicipta boleh menghasilkan penyesuaian perkataan yang berguna.', 'lt': 'Privalomas suderinimas yra veiksmingas kalbų mokslinių tyrimų spartinimo procesas. Tačiau dauguma priverstinių derintojų priklauso nuo kalbos, o nepakankamai išteklių turinčios kalbos retai turi pakankamai išteklių derintojo akustiniam modeliui parengti. Mes pristatysime naują Suomijos grafikos pagrindu pagrįstą priverstinį suderintuvą ir įrodome jo rezultatus suderindami daugelį uralinių kalbų ir anglų kaip nesusijusią kalbą. Mes parodome, kad net paprastas ne ekspertas sukurtas grafiko-telefono žemėlapiavimas gali lemti naudingus žodžių suderinimus.', 'kk': 'Түкірлі түзету - лингвистикалық зерттеулерді тегістеу үшін эффективні процес. Бірақ көпшілік түрлердің көпшілігі тілден тәуелді, және көпшілікті тілдердің көпшілігі көпшілікті түрлердің көпшілігін түрлеу үшін акустикалы Біз жаңа Финляндия графикасының негізінде күш түрлендірушісін таңдап, бірнеше ураллық тілдерді және ағылшын тілдерін бірнеше тіл ретінде түрлендіру арқылы көрсетедік. Біз графикалық және фонемді құрылған кәдімгі емес эксперт картасында пайдалы сөздерді түзетуге болады дегенді көрсетедік.', 'ml': 'നിര്\u200dബന്ധിതമാക്കപ്പെട്ടിരിക്കുന്നത് ഭാഷയിലെ പരിശോധന വേഗത്തിലാക്കുന്ന ഒരു പ്രക്രിയയാണ്. However, most forced aligners are language-dependent, and under-resourced languages rarely have enough resources to train an acoustic model for an aligner.  നമ്മള്\u200d ഒരു പുതിയ ഫിനിഷ്യന്\u200d ഗ്രാഫ്റ്റീമിന്റെ അടിസ്ഥാനമാക്കിയിരിക്കുന്നു. അതിന്റെ പ്രകടനം കാണിക്കുന്നു. പല ഉരാലിക്  നമ്മള്\u200d കാണിക്കുന്നുണ്ടെങ്കില്\u200d ഒരു ലളിതമായ വിശേഷിപ്പില്ലാത്ത ഗ്രാഫീമില്\u200d നിന്നും ഫോണിമേപ്പ് ഉണ്ടാക്ക', 'mn': 'Хүчирхэг тэгшитгэл нь хэлний судалгааны хурдан хурдан үр дүнтэй процесс юм. Гэхдээ ихэнх хүчирхэг зохион байгуулагч нь хэл хамааралтай, мөн хүчирхэг хэл бага байгуулагдсан хэл бага зэрэг зохион байгуулагч загварын загварыг суралцах хангалттай баялаг байдаг. Бид шинэ Финляндын график дээр суурилсан хүчирхэг зохион байгуулагч болгож, олон Уралик хэл, Англи хэл болон хоорондоо холбоотой хэл болгож үзүүлдэг. Бид энгийн биш мэргэжилтнүүд ч графим-т-фонем зураг бүтээсэн гэдгийг харуулж чадна.', 'mt': 'L-allinjament furzat huwa proċess effettiv biex titħaffef ir-riċerka lingwistika. However, most forced aligners are language-dependent, and under-resourced languages rarely have enough resources to train an acoustic model for an aligner.  Aħna nippreżentaw allinjatur furzat ġdid Finlandiż ibbażat fuq grafimu u nippruvaw il-prestazzjoni tiegħu billi nallinjaw diversi lingwi Uraliċi u l-Ingliż bħala lingwa mhux relatata. Aħna nuru li anke immappjar sempliċi mhux espert maħluq minn grafem għal fonem jista’ jirriżulta f’allinjamenti utli tal-kliem.', 'ro': 'Alinierea forțată este un proces eficient de accelerare a cercetării lingvistice. Cu toate acestea, majoritatea alinierelor forțate sunt dependente de limbă, iar limbile cu resurse insuficiente rareori au suficiente resurse pentru a instrui un model acustic pentru un alinier. Vă prezentăm un nou alinier forțat finlandez bazat pe grafem și demonstrăm performanța acestuia prin alinierea mai multor limbi uralice și engleza ca limbă neafiliată. Noi arătăm că chiar și un simplu non-expert creat graf-la-fonem cartografiere poate duce la alinieri utile de cuvinte.', 'pl': 'Wymuszone dostosowanie jest skutecznym procesem przyspieszenia badań językowych. Jednak większość wymuszonych alignerów zależy od języka, a języki niedostatecznie zasobów rzadko mają wystarczająco dużo zasobów, aby przeszkolować model akustyczny dla alignera. Przedstawiamy nowy fiński wymuszony aligner oparty na grafemie i demonstrujemy jego działanie poprzez dostosowanie wielu języków uralskich i angielskiego jako niepowiązanego języka. Pokazujemy, że nawet proste, nieeksperckie mapowanie grafem-fonem może skutkować użytecznymi wyrównaniami słów.', 'no': 'Forkravd justering er ein effektiv prosess for å raskare språkstiske forskning. Dei fleste forsterkte innstillingane er språk-avhengig, og underressurserte språk har ofte nok ressursar for å trenja ein akustisk modell for ei justeringsprogram. Vi presenterer ein ny finsk grafebasert forbunda justering og demonstrerer utviklinga ved å justera fleire Uralske språk og engelsk som eit ukjent språk. Vi viser at sjølv ein enkel ikkje-ekspertert oppretta grafikk-til-fonemekarting kan resultera i nyttig ordjustering.', 'si': 'බලාපොරොත්තු සංවේදනය තමයි භාෂාත්මක පරීක්ෂණය වේගය කරන්න ප්\u200dරශ්නයක්. නමුත්, බොහෝ බලාපොරොත්තු සම්බන්ධ කරුණු භාෂාව අවශ්\u200dය භාෂාව, සහ අඩු සම්බන්ධ භාෂාවට ප්\u200dරමාණයක් ති අපි අළුත් ෆින්නිෂ් ග්\u200dරාෆේම් අධිරූපයෙන් බලාපොරොත්තු සම්බන්ධකයෙක් පෙන්වන්න සහ ඔරාලික් භාෂාවක් ස අපි පෙන්වන්නේ සාමාන්\u200dය නොවිශ්වාසිකයෙක්ටත් ග්\u200dරාෆේම් එක්ක ෆෝනේම් සැකසුම් කරලා තියෙන්න පුළු', 'sr': 'Forced alignment is an effective process to speed up linguistic research. Međutim, većina prisiljenih poravnača zavisi od jezika, a manje resursa jezika retko imaju dovoljno resursa za obuku akustičkog modela za poravnača. Predstavljamo novog finskog grafejskog prisiljenog poravnača i pokazujemo svoju predstavu poravnajući višestruke Uraličke jezike i engleski kao nepovezan jezik. Pokazujemo da čak i jednostavan ne-stručnjak koji je stvorio mapiranje grafika na telefon može rezultirati korisnim poravnanjem reči.', 'sv': 'Tvingad anpassning är en effektiv process för att påskynda språkforskningen. Men de flesta tvångsaligner är språkberoende, och underresurser språk har sällan tillräckligt med resurser för att träna en akustisk modell för en aligner. Vi presenterar en ny finsk grafembaserad tvångsjustering och visar hur den fungerar genom att anpassa flera uraliska språk och engelska som ett icke-besläktat språk. Vi visar att även en enkel icke-expert skapad grafem-till-fonem kartläggning kan resultera i användbara ordjusteringar.', 'ur': 'زبان تحقیقات کو سرعت دینے کے لئے مضبوط تعمیر ایک اثرات طریقہ ہے۔ اگرچہ اکثر مضبوط الیزار زبان کے معاملہ میں ہیں، اور کم رسسورٹ زبانوں کے لئے بہت کم منابع ہیں کہ ایک الیزار کے لئے آہستیک موڈل کی تعلیم کریں. ہم نے ایک نئی فنلاندی گرافیٹ بنیاد رکھی ہوئی مجبور الیٹر کو پیش کیے اور اس کی عمدگی کو دکھاتے ہیں بہت سی اورالیک زبانیں اور انگلیسی زبانیں ایک بے ربط زبان کے طور پر۔ ہم دکھاتے ہیں کہ اگرچہ ایک ساده غیر استرس نے گرافیمیٹ-ٹ-فونیمیٹ نقشه پیدا کی ہے تو فائدہ پہنچانے کے نتیجے ہیں۔', 'so': 'Isbedelka lagu qasbay waa jarayo faa’iido leh oo u kordhiya baaritaanka luuqadaha. Si kastaba ha ahaatee ajaaniibta badankoodu waa kuwa ku qasban luqada ku xiran, oo luuqadaha aan la isticmaaleynna inta ugu yaraan waxay leeyihiin hanti ku filan si ay u tababaridaan tilmaamo ajaaniibta ah. Waxaannu soo bandhignaynaa sawir cusub oo ku saleysan afka Finnishka oo qasab ah, waxaana muujinnaa sameynta afkiisa Uraliyeed iyo Ingiriiska sida luuqad a an la sheekeysan. Waxaynu tusnaynaa in xittaa khabiir aan khaas aheyn oo sawir-sawir-sawir ah lagu sameeyo wuxuu sababi karaa isbedelyo faa’iido leh oo hadal faa’iido ah.', 'ta': 'நிறுவப்படுத்தப்பட்ட ஒழுங்குபடுத்தல் ஒரு செயல்பாடு மொழியின் ஆராய்ச்சியை வேகமாக்குவதற்கு. ஆனால், பெரும்பாலான விரிவாளர்கள் மொழியில் சார்ந்து இருக்கின்றனர், மற்றும் மூலத்திற்குக் கீழ் மொழிகளுக்கு குறைவாக போது நாங்கள் ஒரு புதிய பின்னிஷ் வரைப்படத்தை அடிப்படையாக்கி பிரித்துள்ளோம் மற்றும் அதில் செயல்பாட்டை காட்டுகிறோம் பல Uralic ம We show that even a simple non-expert created grapheme-to-phoneme mapping can result in useful word alignments.', 'uz': "Tayyorlashni tasdiqlash, tillarni tezlashtirish jarayoni ishlab chiqarish kerak. Lekin ko'pchilik taʼminlovchilar tilga ishlatuvchilar, va taʼminlovchi tilida taʼminlovchi modelni o'rganish uchun juda yetarli manbalar bor. Biz bir necha Uralik tilni va Inglizchaga bir necha tilni taʼminlov qilish bilan bir xil tilni ko'proq tilni ko'rsatdik. Biz oddiy ekspert yaratilmagan grammatik va foydalanuvchi diagrammasi foydalanuvchi soʻzlarni moslash mumkin.", 'vi': 'Sự phối hợp ép buộc là một tiến trình hiệu quả để thúc đẩy nghiên cứu ngôn ngữ. Tuy nhiên, hầu hết các đối tượng bị ép buộc đều phụ thuộc ngôn ngữ, và ngôn ngữ thiếu nguồn ít khi có đủ nguồn lực để huấn luyện một mô hình âm thanh cho sự liên minh. Chúng tôi giới thiệu một phương pháp hỗ trợ bắt buộc với mục sư Phần Lan mới và chứng minh khả năng của nó bằng cách hợp nhiều ngôn ngữ Uralic và Anh không liên quan đến. Chúng tôi cho thấy ngay cả bản đồ graphheme-to-honeme được tạo ra đơn giản cũng có thể kết quả tích hợp từ.', 'da': 'Tvunget tilpasning er en effektiv proces til at fremskynde sproglig forskning. Imidlertid er de fleste tvungne aligners sprogafhængige, og underressourcer sprog har sjældent ressourcer nok til at træne en akustisk model for en aligner. Vi præsenterer en ny finsk grafembaseret tvunget aligner og demonstrerer dens ydeevne ved at tilpasse flere uraliske sprog og engelsk som et ikke beslægtet sprog. Vi viser, at selv en simpel ikke-ekspert skabt grafem-til-fonem kortlægning kan resultere i nyttige ordjusteringer.', 'de': 'Forced Alignment ist ein effektiver Prozess, um die linguistische Forschung zu beschleunigen. Allerdings sind die meisten erzwungenen Aligner sprachabhängig und Sprachen mit wenig Ressourcen haben selten genug Ressourcen, um ein akustisches Modell für einen Aligner zu trainieren. Wir stellen einen neuen finnischen Graphem-basierten Zwangsrichter vor und demonstrieren seine Leistungsfähigkeit durch die Ausrichtung mehrerer uralischer Sprachen und Englisch als nicht verwandte Sprache. Wir zeigen, dass selbst eine einfache, nicht-Experte erstellte Graphem-zu-Phonem-Zuordnung zu nützlichen Wortausrichtungen führen kann.', 'nl': 'Gedwongen afstemming is een effectief proces om taalkundig onderzoek te versnellen. De meeste gedwongen aligners zijn echter taalafhankelijk en talen met weinig middelen hebben zelden genoeg middelen om een akoestisch model voor een aligner te trainen. We presenteren een nieuwe Finse grafeemgebaseerde geforceerde aligner en demonstreren de prestaties ervan door meerdere Uralische talen en Engels als een niet-verwante taal op elkaar af te stemmen. We laten zien dat zelfs een eenvoudige niet-deskundige grafiek-naar-foneem toewijzing kan resulteren in nuttige woorduitlijningen.', 'bg': 'Принуденото подравняване е ефективен процес за ускоряване на езиковите изследвания. Повечето принудителни подравняващи устройства обаче зависят от езика и езиците с недостатъчно ресурси рядко разполагат с достатъчно ресурси, за да обучават акустичен модел за подравняване. Представяме нов финландски графим базиран принудителен подравник и демонстрираме неговото представяне чрез подравняване на множество уралски езици и английски като несвързан език. Показваме, че дори проста, неекспертна, създадена графима-фонема картографиране може да доведе до полезни подравнявания на думите.', 'hr': 'Prisiljeno poravnanje je učinkovit proces za ubrzanje jezičkih istraživanja. Međutim, većina prisiljenih poravnača ovisi o jeziku, a manje resursa jezika rijetko imaju dovoljno resursa za obuku akustičkog modela za poravnača. Predstavljamo novog prisiljenog poravnača na temelju finskog grafema i pokazujemo svoj učinkovit usklađivanjem višestrukih Uraljskih jezika i engleskog jezika kao neizbježnog jezika. Pokazujemo da čak i jednostavan ne-stručnjak koji je stvorio mapiranje grafima na telefon može rezultirati korisnim poravnanjem riječi.', 'ko': '강제 정렬은 언어 연구를 가속화시키는 효과적인 방법이다.그러나 대다수 강제 대제기는 언어에 의존하고 자원이 부족한 언어는 대제기를 위해 성학 모델을 훈련할 충분한 자원이 적다.우리는 핀란드어 자형을 기반으로 한 새로운 강제 대조기를 제시했고, 다양한 우랄어와 영어를 상관없는 언어로 대조함으로써 그 성능을 보여 주었다.우리는 간단한 비전문가가 만든 글자인 음영사도 유용한 단어의 정렬을 만들어 낼 수 있다는 것을 발견했다.', 'tr': 'G체챌le첵채n 챌yzygla힊dyrma dillerini 챌alt etmek 체챌in etkinlik bir proses. 횦철ne k철p g체첵챌li 챌yzyg챌ylar dili baglanma첵arlar, we esasy dillerde gaty 챌yzyg챌ylar 체챌in akustik nusgasyny 철wretmek 체챌in 첵eterlik 챌e힊meler bar. Biz t채ze bir Fin챌e grafik챌e da힊ary 첵agda첵la힊lygyny g철rke첵채ris we onu흫 etkinle힊igini birn채챌e Ural dilini we i흫lis챌e baglanma첵an dil h철km체nde g철rke첵채ris. Biz esasy bir uzmany흫 grafik-to-foneme haritasyny 첵체ze 챌ykyp bardygyny g철rkezip bileris.', 'id': 'Alinjasi terpaksa adalah proses yang efektif untuk mempercepat penelitian bahasa. However, most forced aligners are language-dependent, and under-resourced languages rarely have enough resources to train an acoustic model for an aligner.  Kami mempersembahkan penggaris terpaksa berbasis grafem Finlandia baru dan menunjukkan prestasinya dengan menyesuaikan berbagai bahasa Ural dan bahasa Inggris sebagai bahasa yang tidak terkait. Kami menunjukkan bahkan peta grafim-ke-fonem yang tidak ahli yang sederhana dapat menyebabkan penyesuaian kata yang berguna.', 'af': "Verkragte aanpassing is 'n effektief proses om lingwisiese ondersoek te spoed. Maar mees versterkte aligners is taal-afhanklik, en onder-hulpbronne tale het selfs genoeg hulpbronne om 'n akustiese model vir 'n aligner te oefen. Ons stel 'n nuwe Finnish graafema-gebaseerde verdrukking en wys sy prestasie deur veelvuldige Uralike tale en Engels as 'n onverwante taal te aligneer. Ons wys dat selfs 'n eenvoudige non-eksperte geskep grafém-to-foneme kaart kan resultaat in gebruiklike woord-alignments.", 'am': 'የቋንቋ-ቋንቋ መረጃ ለመፍጠር የሚችል ጥያቄ ነው፡፡ ምንም እንኳን፣ አብዛኞቹ ብዙዎቹ የተገደሉት የቋንቋ ቋንቋዎች በመታሰሪያ እና ከሀብት ቋንቋዎች በታች ለብሔራዊ ተቃዋሚ ሞዴል ለማስተማር በሚበቃቸው ሀብት አለባቸው፡፡ አዲስ የፊንድ ቋንቋ-ቋንቋ እና እንግሊዘኛ ቋንቋን እንደተካፈለ አዲስ የፊንድ ቋንቋ አጋራጠርን እናሳየዋለን፡፡ እናሳያቸዋለን፣ ቀላል ያልሆነው አካባቢ ስልክ-ለፎቶ ማረፊያ ለጥቅም ቃላት ማቀናቀል እንዲችል ነው፡፡', 'sw': 'Uwekezaji wa nguvu ni mchakato wa ufanisi wa kuongeza utafiti wa lugha. Hata hivyo, wageni wengi waliolazimishwa ni wanategemea lugha, na lugha zisizo rasmi zina rasilimali za kutosha kufundisha muundo wa kigeni. We present a new Finnish grapheme-based forced aligner and demonstrate its performance by aligning multiple Uralic languages and English as an unrelated language.  Tunaonyesha kuwa hata mtu ambaye haina wataalam ambaye ametengeneza ramani ya picha kwa simu inaweza kusababisha usambazaji wa maneno.', 'fa': 'تنظیم مجبور شدن یک فرایند موثری برای سرعت تحقیقات زبان است. ولی بیشتر متصل کنندگان مجبور به زبان بستگی دارند، و زبانهای زیر منابع کمی برای آموزش یک مدل آکوستیک برای یک متصل کننده دارند. ما یک گرافیک جدید فنلاندی را به عنوان یک زبان بی ارتباط نشان می دهیم و عملکرد آن را با تنظیم زبان\u200cهای اورالیک و انگلیسی به عنوان یک زبان بی ارتباط نشان می دهیم. ما نشان می دهیم که حتی یک متخصص ساده غیر متخصص نقشه\u200cسازی گرافیم-به-فونیم ساخته می\u200cشود که نتیجه\u200cی تعیین\u200cسازی کلمات مفید است.', 'sq': 'Rregullimi i detyruar është një proces efektiv për të shpejtuar kërkimin gjuhësor. Megjithatë, shumica e allinjatorëve të detyruar janë të varur nga gjuha dhe gjuhët me pak burime rrallë kanë burime të mjaftueshme për të trajnuar një model akustik për një allinjator. Ne paraqesim një linjues të ri finlandez me bazë grafike të detyruar dhe demonstrojmë shfaqjen e tij duke alinuar gjuhët e shumta Urale dhe Anglishtin si një gjuhë të pa lidhur. Ne tregojmë se edhe një hartim i thjeshtë jo-ekspert i krijuar nga grafi në telefon mund të rezultojë në rregullime të fjalëve të dobishme.', 'hy': 'Forced alignment is an effective process to speed up linguistic research.  Այնուամենայնիվ, ստիպված հաղորդակցողների մեծ մասը լեզվից կախված է, և թերռեսուրսներ ունեցող լեզուները հազվադեպ ունեն բավարար ռեսուրսներ հաղորդակցողների համար ձայնային մոդել վարժեցնելու համար: Մենք ներկայացնում ենք նոր ֆինլական գրաֆեմայով հիմնված ստիպված հավասարման գործընթաց և ցույց ենք տալիս դրա արտադրողությունը հավասարեցնելով բազմաթիվ ուրալական լեզուներ և անգլերեն որպես անկապ լեզու: Մենք ցույց ենք տալիս, որ նույնիսկ պարզ ոչ մասնագետ ստեղծված գրաֆեմ-ֆոնեմ քարտեզագրությունը կարող է հանգեցնել օգտակար բառերի հավասարման:', 'az': 'Çətinli tərəfləmə dil araştırmalarını hızlandırmaq üçün effektiv bir prosedür. Ancaq çox qüvvətli müəyyən edənlər dildən bağlı və çox qüvvətli dillərdən az qala müəyyən etmək üçün akustik modeli təhsil etmək üçün yetər. Biz yeni Finlandiya grafümə tabanlı tərəfdarlığı göstərdik və onun performansını çoxlu Ural dillərini və İngilizce dillərini əlaqəsiz dil olaraq göstərdik. Biz sadəcə bir ekspertik olmayan grafik-to-foneme mapasının yaratdığını göstəririk ki, bu da faydalı sözlər tərəfləndirilməsinə səbəb olar.', 'ca': "L'allinjament forçat és un procés efectiu per accelerar la recerca lingüística. Tot i així, la majoria dels allineadors for çats depenen del llenguatge, i les llengües sota recursos rarament tenen suficients recursos per formar un model acústic per a un allineador. We present a new Finnish grapheme-based forced aligner and demonstrate its performance by aligning multiple Uralic languages and English as an unrelated language.  Mostrem que fins i tot un simple mapatge de gràfic a fònem creat sense experiència pot resultar en alliniaments útils de paraules.", 'bs': 'Forced alignment is an effective process to speed up linguistic research. Međutim, većina prisiljenih poravnača ovisi o jeziku, a manje resursa jezika rijetko imaju dovoljno resursa za obuku akustičkog modela za poravnača. Predstavljamo novog prisiljenog usklađivača na finskom grafiku i pokazujemo svoj učinkovit usklađivanjem višestrukih Uraličkih jezika i engleskog jezika kao nesavezan jezik. Pokazujemo da čak i jednostavan ne-stručnjak koji je stvorio mapiranje grafima na telefon može rezultirati korisnim poravnanjem riječi.', 'cs': 'Nucené sladění je účinným procesem urychlení jazykového výzkumu. Většina nucených zarovnávačů je však jazykově závislá a jazyky s nedostatkem zdrojů mají zřídka dostatek prostředků na vycvičení akustického modelu pro zarovnávač. Představujeme nový finský vynucený aligner založený na grafému a demonstrujeme jeho výkon sladěním několika uralských jazyků a angličtiny jako nesouvisejícího jazyka. Ukazujeme, že i jednoduché mapování grafem-foném vytvořené neprofesi může vést k užitečným zarovnáním slov.', 'et': 'Sunnitud ühtlustamine on tõhus protsess keeleliste uuringute kiirendamiseks. Enamik sunnijoondjaid sõltub siiski keelest ja alaressurssidega keeltel on harva piisavalt ressursse joondja akustilise mudeli koolitamiseks. Tutvustame uut soome grafeemil põhinevat sundjoondjat ja demonstreerime selle toimimist, ühtlustades mitmeid uurali keeli ja inglise keelt kui sõltumatut keelt. Me näitame, et isegi lihtne mitteakspertide loodud grafeem-foneem kaardistamine võib põhjustada kasulikke sõnajoondusi.', 'bn': 'বাধ্য করা হচ্ছে ভাষাগত গবেষণার জন্য একটি কার্যকর প্রক্রিয়া। তবে বেশীরভাগ বিজ্ঞানীরা ভাষার উপরে নির্ভর করে এবং অন্তরসম্পদের ভাষায় যথেষ্ট সম্পদ রয়েছে যারা এক বিজ্ঞানীকে প্রশিক্ষণ করতে পারে। আমরা একটি নতুন ফিনিশ ভিত্তিক গ্রাফিক ভিত্তিক প্রতিযোগিতাকে উপস্থাপন করি এবং বেশ কয়েকটি উরালিক ভাষা এবং ইংরেজি ভাষাকে অকার্য আমরা দেখাচ্ছি যে এমনকি একটি সাধারণ বিশেষজ্ঞ গ্রাফিম-থেকে ফোন ম্যাপিং তৈরি করেছে তার ফলে কার্যকর শব্দের প্রতিমান', 'fi': 'Pakollinen yhdenmukaistaminen on tehokas prosessi kielellisen tutkimuksen nopeuttamiseksi. Useimmat pakkooikomislaitteet ovat kuitenkin kieliriippuvaisia, ja aliresursseilla varustetuilla kielillä on harvoin riittävästi resursseja akustisen mallin kouluttamiseen oikomislaitteelle. Esittelemme uuden suomalaisen grafeemipohjaisen pakkolinjaimen ja osoitamme sen suorituskykyä yhdistämällä useita uraalin kieliä ja englantia toisiinsa liittymättömänä kielenä. Osoitamme, että jopa yksinkertainen ei-asiantuntija luotu grafeemi-foneemi kartoitus voi johtaa hyödyllisiin sanalinjauksiin.', 'jv': 'textattr politenessoffpolite, "), and when there is a change ("assertive Awak dhéwé éntuk nggawe barang nggawe Finis kuwi diangkat saben nggawe barang kelas kuwi nggawe barang Uralic lan Inggris kuwi wis ngerasakno. Awak dhéwé éntuk sistem sing paling-perusahaan seneng pakan-perusahaan karo nggambar gambar nggawe barang-pakan seneng bersatu.', 'sk': 'Prisilna uskladitev je učinkovit proces za pospešitev jezikovnih raziskav. Vendar pa je večina prisilnih poravnalnikov odvisna od jezika in jeziki s premalo virov redko imajo dovolj virov za usposabljanje akustičnega modela za poravnalnik. Predstavljamo nov finski grafemski prisilni poravnalnik in dokazujemo njegovo delovanje z uskladitvijo več uralskih jezikov in angleščine kot nepovezanega jezika. Pokažemo, da lahko celo preprost, neizkušen, ustvarjen grafem-fonem mapiranje povzroči koristne poravnave besed.', 'ha': "@ action: inmenu A lokacin da, masu buwãya da aka lazimta su zama ana dõgara da harshen, kuma da harshen da ke ƙaranci, bã su da rabo kaɗan da ɗan sha'ani wa ya sanar da wani misali mai akostic wa wani matangaki. Tuna halatar da wani mai sauri na karatun Finnish an da aka lazimta shi, kuma Muke nuna aikin sa da za'a sami wasu harshe na Uraliki da Ingiriya kamar wata harshe na'ura ba. We show that even a simple non-expert created grapheme-to-phoneme mapping can result in useful word alignments.", 'he': 'התאמה כפויה היא תהליך יעיל להאיץ מחקר שפתי. בכל אופן, רוב המאמנים הכרחיים תלויים בשפה, ולשפות מתחת למשאבים לעיתים נדירות יש מספיק משאבים כדי לאמן מודל אקוסטי למאמן. אנחנו מציגים מגרף כפוי חדש בסיס גרפים ופגינים את ביצועיו על ידי הציון של שפות אוראליות רבות ואנגלית כשפה לא קשורה. אנחנו מראים שאפילו מפת גראפם-לפונמה לא מומחית פשוטה יכולה להוביל בהתאמות מילים שימושיות.', 'bo': 'དབྱིབས་ཅན་གྱི་གྲལ་སྒྲིག་ནི་སྐད་རིགས་ལྟ་བརྟག་འཕྱུར་འགྲོས་ཀྱི་ལས་སྦྱོར་ཞིག་རེད། ཡིན་ནའང་། forced aligners་ཚང་ཆེ་ཤོས་ཀྱིས་སྐད་ཡིག་དང་མཉམ་དུ་རྒྱུ་དངོས་ཡིག་ཆ་ཉུང་ཡིན་ན། ང་ཚོས་Finnish grapheme་གསར་པ་ཞིག་ལ་བསྟེན་ནས་བཟོ་བཅོས་བྱེད་པའི་གྲལ་སྒྲིག་གཅིག་གནང་བ་དང་། སྒྲ་ཚིགས་ཀྱི་སྐད་ཡིག་དང་དབྱིན་ཡིག་རྐ ང་ཚོས་སྤྱོད་པའི་ཐ་སྙད་ཅིག་ལས་སྟབས་བདེ་བ་མེད་པའི་གྲངས་རིམ་ཅིག་ཀྱང་ལག་ལེན་པའི་གྲལ་སྒྲིག་འགོད'}
{'en': 'Decentralized Word2Vec Using Gossip Learning', 'fr': 'Word2Vec décentralisé utilisant Gossip Learning', 'es': 'Word2Vec descentralizado mediante el aprendizaje de chismes', 'ar': 'Word2Vec اللامركزية باستخدام تعلم القيل والقال', 'pt': 'Word2Vec descentralizado usando o Gossip Learning', 'zh': '用八卦学中心化Word2Vec', 'ja': 'Gossip Learningを使用した分散Word 2 Vec', 'hi': 'विकेंद्रीकृत Word2Vec गपशप सीखने का उपयोग कर', 'ru': 'Децентрализованное Word2Vec с использованием обучения сплетням', 'ga': 'Word2Vec díláraithe ag Úsáid Gossip Learning', 'hu': 'Decentralizált Word2Vec Gossip Learning használatával', 'it': 'Word2Vec decentralizzato usando Gossip Learning', 'el': 'Αποκεντρωμένο Word2Vec χρησιμοποιώντας την εκμάθηση κουτσομπολιού', 'ka': 'Deentralised Word2Vec Using Gossip Learning', 'mk': 'Децентрализиран Word2Vec користејќи учење на Gossip', 'ml': 'ഗോസിപ് പഠനം ഉപയോഗിക്കുന്ന വാര്\u200dഡ്2വിക്', 'lt': 'Name', 'mt': 'Word2Vec deċentralizzat bl-użu tat-Tagħlim tal-Gossips', 'kk': 'Үлкен оқыту үшін децентрализацияланған сөздер2Vec', 'no': 'Desentralisert ord 2Vec ved bruk av målæring', 'mn': 'Цахилгаан үг 2Vec', 'ro': 'Word2Vec descentralizat folosind Gossip Learning', 'pl': 'Zdecentralizowane Word2Vec przy użyciu plotek uczenia się', 'sr': 'Decentralizirani Word2Vec koristeći učenje Gossip', 'si': 'අවශ්\u200dය වචන 2Vec වෙනුවෙන් Gossip ඉගෙනීම භාවිත කරන්න', 'ms': 'Name', 'sv': 'Decentraliserad Word2Vec med Gossip Learning', 'so': 'Decentralized Word2Vec Using Gossip Learning', 'ur': 'جاسپ سیکھنے کے استعمال سے دکنٹرالیز کلمات 2ویک', 'ta': 'Name', 'uz': 'Name', 'vi': 'Tập trung Word2Vek Sử dụng việc học Gossip', 'nl': 'Gedecentraliseerd Word2Vec met behulp van roddelleren', 'bg': 'Децентрализиран Word2Vec Използване на Клюкарки Учене', 'hr': 'Decentralizirani Word2Vec koristeći učenje Gossip', 'da': 'Decentraliseret Word2Vec ved hjælp af Gossip Learning', 'de': 'Dezentrales Word2Vec mit Gossip Learning', 'ko': '팔괘 학습을 이용한 분산식Word2Vec', 'fa': 'استفاده از یادگیری Gossip', 'id': 'Word2Vec Deċentralisasi Menggunakan Belajar Gossip', 'sw': 'Decentralized Word2Vec Using Gossip Learning', 'tr': 'Otomatik öwrenmek ullanýar', 'hy': 'Comment', 'sq': 'Decentralized Word2Vec Using Gossip Learning', 'af': 'Desentraliseerde Woord2Vec gebruik Gossip Leer', 'am': 'Vec using Gossip Learn', 'bs': 'Decentralizirani Word2Vec koristeći učenje Gossip', 'az': 'Deentralized Word2Vec Used Gossip Learning', 'ca': 'Decentralized Word2Vec Using Gossip Learning', 'et': 'Detsentraliseeritud Word2Vec Kasutades Klatšimooniat', 'cs': 'Decentralizovaný Word2Vec pomocí učení drbů', 'bn': 'Name', 'fi': 'Hajautettu Word2Vec Käyttämällä Gossip Learning', 'jv': 'politenessoffpolite"), and when there is a change ("assertivepoliteness', 'sk': 'Decentraliziran Word2Vec z učenjem opravljanja', 'ha': 'KCharselect unicode block name', 'he': 'Name', 'bo': 'སྤྱིར་བཏང་བའི་ཚིག་གྲངས་2Vec སྤྱོད་བཞིན་པའི་བརྡ་སྤྱད་པ'}
{'en': 'Advanced NLP models require huge amounts of data from various domains to produce high-quality representations. It is useful then for a few large public and private organizations to join their corpora during training. However, factors such as legislation and user emphasis on data privacy may prevent centralized orchestration and data sharing among these organizations. Therefore, for this specific scenario, we investigate how gossip learning, a massively-parallel, data-private, decentralized protocol, compares to a shared-dataset solution. We find that the application of Word2Vec in a gossip learning framework is viable. Without any tuning, the results are comparable to a traditional centralized setting, with a loss of quality as low as 4.3 %. Furthermore, the results are up to 54.8 % better than independent local training.', 'es': 'Los modelos avanzados de PNL requieren enormes cantidades de datos de varios dominios para producir representaciones de alta calidad. Por lo tanto, es útil que algunas grandes organizaciones públicas y privadas se unan a sus cuerpos durante la capacitación. Sin embargo, factores como la legislación y el énfasis de los usuarios en la privacidad de los datos pueden impedir la orquestación centralizada y el intercambio de datos entre estas organizaciones. Por lo tanto, para este escenario específico, investigamos cómo el aprendizaje de chismes, un protocolo descentralizado, masivamente paralelo, privado de datos, se compara con una solución de conjunto de datos compartidos. Encontramos que la aplicación de Word2Vec en un marco de aprendizaje de chismes es viable. Sin ningún ajuste, los resultados son comparables a los de un ajuste centralizado tradicional, con una pérdida de calidad tan baja como el 4,3%. Además, los resultados son hasta un 54,8% mejores que la formación local independiente.', 'fr': "Les modèles NLP avancés nécessitent d'énormes quantités de données provenant de divers domaines pour produire des représentations de haute qualité. Il est alors utile que quelques grandes organisations publiques et privées rejoignent leur corpus pendant la formation. Cependant, des facteurs tels que la législation et l'accent mis par les utilisateurs sur la confidentialité des données peuvent empêcher l'orchestration centralisée et le partage des données entre ces organisations. Par conséquent, pour ce scénario spécifique, nous étudions comment l'apprentissage des potins, un protocole décentralisé massivement parallèle, privé des données, se compare à une solution de jeu de données partagées. Nous trouvons que l'application de Word2Vec dans un framework d'apprentissage des potins est viable. Sans aucun réglage, les résultats sont comparables à ceux d'un réglage centralisé traditionnel, avec une perte de qualité aussi faible que 4,3\xa0%. En outre, les résultats sont jusqu'à 54,8\xa0% supérieurs à ceux d'une formation locale indépendante.", 'ar': 'تتطلب نماذج البرمجة اللغوية العصبية المتقدمة كميات هائلة من البيانات من مجالات مختلفة لإنتاج تمثيلات عالية الجودة. من المفيد إذن أن ينضم عدد قليل من المؤسسات العامة والخاصة الكبيرة إلى هيئاتهم أثناء التدريب. ومع ذلك ، قد تمنع عوامل مثل التشريع وتركيز المستخدم على خصوصية البيانات التنسيق المركزي ومشاركة البيانات بين هذه المنظمات. لذلك ، بالنسبة لهذا السيناريو المحدد ، نتحرى كيف يقارن تعلم القيل والقال ، وهو بروتوكول لا مركزي متوازي بشكل كبير ، خاص بالبيانات ، مع حل مجموعة البيانات المشتركة. نجد أن تطبيق Word2Vec في إطار عمل تعلم القيل والقال قابل للتطبيق. بدون أي ضبط ، تكون النتائج قابلة للمقارنة مع الإعداد المركزي التقليدي ، مع فقدان جودة منخفض يصل إلى 4.3٪. علاوة على ذلك ، فإن النتائج تصل إلى 54.8٪ أفضل من التدريب المحلي المستقل.', 'pt': 'Modelos avançados de PNL exigem grandes quantidades de dados de vários domínios para produzir representações de alta qualidade. É útil, então, que algumas grandes organizações públicas e privadas se juntem aos seus corpora durante o treinamento. No entanto, fatores como legislação e ênfase do usuário na privacidade de dados podem impedir a orquestração centralizada e o compartilhamento de dados entre essas organizações. Portanto, para esse cenário específico, investigamos como o gossip learning, um protocolo massivamente paralelo, privado de dados e descentralizado, se compara a uma solução de conjunto de dados compartilhado. Achamos que a aplicação do Word2Vec em uma estrutura de aprendizado de fofocas é viável. Sem qualquer ajuste, os resultados são comparáveis a uma configuração centralizada tradicional, com uma perda de qualidade tão baixa quanto 4,3%. Além disso, os resultados são até 54,8% melhores do que o treinamento local independente.', 'ja': '高度なNLPモデルは、高品質の表現を生成するために様々なドメインからの膨大な量のデータを必要とします。その後、いくつかの大規模な公共団体と民間団体がトレーニング中にコーポラルに参加することは有用です。ただし、法律やユーザーがデータプライバシーを重視するなどの要因により、これらの組織間の一元化されたオーケストレーションやデータ共有が妨げられる可能性があります。したがって、この特定のシナリオでは、大規模に並行したデータプライベートな分散型プロトコルであるゴシップ学習が、共有データセットソリューションとどのように比較されるかを調査します。私たちは、ゴシップ学習フレームワークでWord 2 Vecを適用することは実行可能であることを発見しました。チューニングなしでは、結果は従来の集中型設定と同等であり、品質の損失は4.3 ％と低くなります。さらに、結果は独立した現地トレーニングよりも最大54.8 ％優れています。', 'zh': '高级NLP形,异域之大数而生高质量也。 故于大公共私人组织,培训入其语料库有用也。 然立法用户隐,或止其集数据共享。 故于此特定之场,究八卦之学(大并行、数私有、去中心化之议)与共集解决方案之比。 见Word2Vec学于八卦框架其可用可也。 无所调,比旧集中式,损低至4.3%。 此外,高于独立培训54.8%。', 'hi': 'उन्नत एनएलपी मॉडल को उच्च गुणवत्ता वाले प्रतिनिधित्व का उत्पादन करने के लिए विभिन्न डोमेन से बड़ी मात्रा में डेटा की आवश्यकता होती है। यह तब कुछ बड़े सार्वजनिक और निजी संगठनों के लिए प्रशिक्षण के दौरान अपने निगम में शामिल होने के लिए उपयोगी है। हालांकि, कानून और डेटा गोपनीयता पर उपयोगकर्ता के जोर जैसे कारक इन संगठनों के बीच केंद्रीकृत orchestration और डेटा साझाकरने को रोक सकते हैं। इसलिए, इस विशिष्ट परिदृश्य के लिए, हम जांच करते हैं कि गपशप सीखने, बड़े पैमाने पर समानांतर, डेटा-निजी, विकेंद्रीकृत प्रोटोकॉल, एक साझा-डेटासेट समाधान की तुलना कैसे करता है। हम पाते हैं कि एक गपशप सीखने के ढांचे में Word2Vec का आवेदन व्यवहार्य है। किसी भी ट्यूनिंग के बिना, परिणाम एक पारंपरिक केंद्रीकृत सेटिंग के लिए तुलनीय हैं, जिसमें गुणवत्ता का नुकसान 4.3% के रूप में कम है। इसके अलावा, परिणाम स्वतंत्र स्थानीय प्रशिक्षण की तुलना में 54.8% तक बेहतर हैं।', 'ru': 'Передовые модели NLP требуют огромных объемов данных из различных областей для получения высококачественных представлений. В этом случае полезно, чтобы несколько крупных государственных и частных организаций присоединились к своим корпорациям во время обучения. Однако такие факторы, как законодательство и акцент пользователей на конфиденциальность данных, могут препятствовать централизованной организации и обмену данными между этими организациями. Поэтому для этого конкретного сценария мы исследуем, как обучение сплетням, массово-параллельный, частный, децентрализованный протокол, сравнивается с решением с общим набором данных. Мы находим, что применение Word2Vec в рамках обучения сплетням является жизнеспособным. Без какой-либо настройки результаты сопоставимы с традиционными централизованными настройками, с потерей качества на уровне 4,3%. Кроме того, результаты до 54,8% лучше, чем независимая местная подготовка.', 'ga': 'Teastaíonn méideanna ollmhóra sonraí ó réimsí éagsúla ó mhúnlaí Casta NLP chun léiriúcháin ardcháilíochta a tháirgeadh. Tá sé úsáideach mar sin do roinnt eagraíochtaí poiblí agus príobháideacha móra a bheith páirteach ina gcorpora le linn oiliúna. Mar sin féin, d’fhéadfadh fachtóirí amhail reachtaíocht agus béim úsáideoirí ar phríobháideachas sonraí cosc a chur ar cheolfhoirne láraithe agus ar chomhroinnt sonraí i measc na n-eagraíochtaí sin. Dá bhrí sin, don chás sonrach seo, déanaimid imscrúdú ar an gcaoi a ndéantar foghlaim gossip, prótacal díláraithe atá fíor-chomhthreomhar, sonraí-príobháideach, i gcomparáid le réiteach tacair sonraí comhroinnte. Faighimid amach go bhfuil cur i bhfeidhm Word2Vec i gcreat foghlama gossip inmharthana. Gan aon tiúnadh, tá na torthaí inchomparáide le suíomh láraithe traidisiúnta, le caillteanas cáilíochta chomh híseal le 4.3%. Ina theannta sin, tá na torthaí suas le 54.8% níos fearr ná oiliúint áitiúil neamhspleách.', 'hu': 'A fejlett NLP modellek hatalmas mennyiségű adatot igényelnek különböző tartományokból a kiváló minőségű reprezentációk előállításához. Hasznos tehát néhány nagy állami és magánszervezet számára, hogy a képzés során csatlakozzon a corporákhoz. Azonban olyan tényezők, mint a jogszabályok és az adatvédelemre helyezett felhasználói hangsúly, megakadályozhatják az ilyen szervezetek közötti központosított hangszerelést és adatmegosztást. Ezért ebben a konkrét forgatókönyvben azt vizsgáljuk, hogy a pletyka tanulás, egy masszívan párhuzamos, adat-privát, decentralizált protokoll hogyan viszonyul egy megosztott adatkészlet megoldáshoz. Úgy találjuk, hogy a Word2Vec alkalmazása egy pletyka tanulási keretrendszerben életképes. Hangolás nélkül az eredmények összehasonlíthatók egy hagyományos központosított beállítással, a minőség csökkenése akár 4,3%. Továbbá az eredmények akár 54,8%-kal jobb, mint a független helyi képzés.', 'el': 'Τα προηγμένα μοντέλα απαιτούν τεράστιες ποσότητες δεδομένων από διάφορους τομείς για να παράγουν αναπαραστάσεις υψηλής ποιότητας. Είναι χρήσιμο λοιπόν για μερικούς μεγάλους δημόσιους και ιδιωτικούς οργανισμούς να ενταχθούν στα σώματά τους κατά τη διάρκεια της εκπαίδευσης. Ωστόσο, παράγοντες όπως η νομοθεσία και η έμφαση των χρηστών στο απόρρητο δεδομένων ενδέχεται να εμποδίσουν την κεντρική ενορχήστρωση και την ανταλλαγή δεδομένων μεταξύ αυτών των οργανισμών. Ως εκ τούτου, για αυτό το συγκεκριμένο σενάριο, ερευνούμε πώς η εκμάθηση κουτσομπολιού, ένα μαζικά-παράλληλο, ιδιωτικό, αποκεντρωμένο πρωτόκολλο δεδομένων, συγκρίνεται με μια λύση κοινού συνόλου δεδομένων. Βρίσκουμε ότι η εφαρμογή του σε ένα πλαίσιο εκμάθησης κουτσομπολιών είναι βιώσιμη. Χωρίς καμία ρύθμιση, τα αποτελέσματα είναι συγκρίσιμα με μια παραδοσιακή κεντρική ρύθμιση, με απώλεια ποιότητας έως 4.3%. Επιπλέον, τα αποτελέσματα είναι έως 54,8% καλύτερα από ό,τι η ανεξάρτητη τοπική κατάρτιση.', 'ka': 'განვითარებული NLP მოდელები უნდა განსხვავებული დემონიდან დიდი მონაცემები გამოსახულებლად გამოსახულებლად. მაშინ ეს საჭიროა რამდენიმე დიდი ადამიანი და პირადი ორგანიზაციებისთვის, რომ საკუთარი ორგანიზაციებისთვის გადაწყვეტილება. მაგრამ, როგორც წესები და მომხმარებელი მონაცემების პრივიაციას შეუძლიათ წინახოთ ცენტრალიზური ორკესტერება და მონაცემების გაყოფილი ამ ორგანიზაციებში ამიტომ, ამ განსაკუთრებული სენარიოსთვის, ჩვენ შევხედავთ, როგორ საკუთრებული სწავლობა, მასიგურად პარალელი, მონაცემები-პირადი, დეკენტრალიზებული პროტოკოლას, საკუთრებული მონა ჩვენ ვიფიქრობთ, რომ Word2Vec-ის პროგრამა სწავლის ფრამეტში უკეთესია. არაფერი კონფიგურაციის გარეშე შემდგომარებელია ტრადიციონალური ცენტრალიზური დანაწილებისთვის, როგორც კონფიგურაციის დასრულება 4,3%-ზე. დამატებით, შედეგები 54,8% უფრო უკეთესია, ვიდრე საერთო ლოკალური განსწავლება.', 'it': "I modelli NLP avanzati richiedono enormi quantità di dati da vari domini per produrre rappresentazioni di alta qualità. È utile quindi per alcune grandi organizzazioni pubbliche e private unirsi ai loro corpora durante la formazione. Tuttavia, fattori come la legislazione e l'enfasi degli utenti sulla privacy dei dati possono impedire l'orchestrazione centralizzata e la condivisione dei dati tra queste organizzazioni. Pertanto, per questo specifico scenario, analizziamo come l'apprendimento del gossip, un protocollo decentralizzato massivamente parallelo, privato di dati, si confronta con una soluzione di set di dati condivisi. Troviamo che l'applicazione di Word2Vec in un framework di apprendimento gossip sia fattibile. Senza alcuna sintonizzazione, i risultati sono paragonabili a una impostazione centralizzata tradizionale, con una perdita di qualità fino al 4,3%. Inoltre, i risultati sono fino al 54,8% migliori della formazione locale indipendente.", 'lt': 'Pažangiems NLP modeliams reikalingas didelis įvairių sričių duomenų kiekis, kad būtų galima parengti aukštos kokybės atstovavimus. Tada naudinga, kad kelios didelės viešosios ir privačios organizacijos prisijungtų prie savo korporacijos mokymo metu. Tačiau tokie veiksniai kaip teisės aktai ir vartotojų akcentavimas duomenų privatumui gali užkirsti kelią centralizuotam šių organizacijų orkestrui ir dalijimuisi duomenimis. Todėl šiame konkrečiame scenarijuje mes tiriame, kaip šūkių mokymasis, masiškai lygiagretus, duomenų-privatus, decentralizuotas protokolas, palyginamas su bendro duomenų rinkinio sprendimu. Mes manome, kad Word2Vec taikymas šventyklų mokymosi sistemoje yra gyvybingas. Be jokio koregavimo rezultatai yra panašūs į tradicinę centralizuotą aplinką, o kokybės praradimas yra mažesnis kaip 4,3 %. Be to, rezultatai 54,8 proc. geresni nei nepriklausomas vietos mokymas.', 'kk': 'Қосымша NLP үлгілері көптеген доменден көптеген деректер саны жоғары сапатты көрсету үшін керек. Содан кейін бірнеше көпшілік және жеке құрылғылар корпорасына қатынау үшін пайдалы. Бірақ, мәліметтер мен пайдаланушылар мәліметтердің жалғастырылығына маңызды факторлар бұл құрылымдар арасында оркестірілген оркестерді және деректерді ортақтастыру мүмкі Сондықтан бұл нақты сценарий үшін біз бөліскен деректер жиынының шешіміне қалай салыстыратынын зерттейміз. Біз Word2Vec бағдарламасының қолданбасы оқыту бағдарламасында тұратын. Бір баптаусыз, нәтижелер дәстүрлі орталық параметрлермен, сапасы жоғалуы 4, 3% деп төмен болады. Сонымен қатар, нәтижелер тәуелсіз жергілікті оқытудан 54,8% деп жақсы болады.', 'mk': 'Напредните NLP модели бараат огромни количини на податоци од различни домени за да се произведуваат висококвалитетни претставувања. Тогаш е корисно за неколку големи јавни и приватни организации да им се приклучат на нивните корпорации за време на обуката. However, factors such as legislation and user emphasis on data privacy may prevent centralized orchestration and data sharing among these organizations.  Затоа, за овој специфичен сценарио, ние истражуваме како учењето на зборови, масовно паралелно, приватно-податоци, децентрализиран протокол, се споредува со решение на заеднички набор податоци. Најдовме дека апликацијата на Word2Vec во рамка за учење на зборови е остварлива. Без никакво прилагодување, резултатите се споредливи со традиционално централизирано поставување, со загуба на квалитетот од 4,3 отсто. Покрај тоа, резултатите се до 54,8 отсто подобри од независната локална обука.', 'ml': 'മുന്\u200dഗണന എംഎല്\u200dപി മോഡലുകള്\u200dക്ക് വ്യത്യസ്ത ഡോമെന്\u200dമെനുകളില്\u200d നിന്നും വലിയ വിവരങ്ങള്\u200d ആവശ്യമുണ്ട്. ഉയര്\u200dന്ന സ്ഥാ അപ്പോള്\u200d കുറച്ചു പ്രധാനപ്പെട്ട സംഘടനകള്\u200dക്കും സ്വകാര്യ സംഘടനകള്\u200dക്കും അവരുടെ കോർപ്പോരിയില്\u200d ചേരാ എന്നാലും നിയമങ്ങളും ഉപയോക്താവിന്റെ സ്വകാര്യത്തില്\u200d ഉപയോക്താവിന്റെ കാരണങ്ങളും ഈ സംഘടനകള്\u200dക്കിടയില്\u200d വിഭാഗിക്കുന്ന വി അതുകൊണ്ട്, ഈ പ്രത്യേക സിനേറ്ററിയോയ്ക്ക്, നമ്മള്\u200d അന്വേഷിക്കുന്നത് എങ്ങനെയാണ് ഗോസിപ്പ് പഠിക്കുന്നത്, വലിയ പാരാളല്\u200d, ഡേറ്റാ സ്വകാര്യ വോര്\u200dഡ്2വെക്കിന്റെ പ്രയോഗം ഒരു ഗുസ്സിപ് പഠിക്കുന്ന ഫ്രെയിമ്പില്\u200d പ്രയോഗിക്കുന്നത് പ്രയോഗിക ഒന്നുമില്ലാതെ, ഫലങ്ങള്\u200d പാരമ്പര്യമായ ഒരു സെന്\u200dട്രാലിസ് സജ്ജീകരണത്തിന് തുല്യമാണ്, 4.3 ശതമാനത്തിന് കുറഞ്ഞതാണ്. അതിനുമുമ്പ്, ഫലങ്ങള്\u200d 54.8% വരെ സ്വാതന്ത്ര്യാദേശിക പരിശീലനത്തെക്കാള്\u200d ഉത്തമമാണ്.', 'mt': 'Mudelli avvanzati tal-NLP jeħtieġu ammonti kbar ta’ dejta minn diversi oqsma biex jipproduċu rappreżentazzjonijiet ta’ kwalità għolja. Għalhekk huwa utli li ftit organizzazzjonijiet pubbliċi u privati kbar jingħaqdu mal-korpora tagħhom waqt it-taħriġ. Madankollu, fatturi bħall-leġiżlazzjoni u l-enfasi tal-utent fuq il-privatezza tad-dejta jistgħu jipprevjenu orkestrazzjoni ċentralizzata u kondiviżjoni tad-dejta fost dawn l-organizzazzjonijiet. Għalhekk, għal dan ix-xenarju speċifiku, ninvestigaw kif it-tagħlim tal-gossip, protokoll massivament parallel, privat u deċentralizzat, jitqabbel ma’ soluzzjoni ta’ sett ta’ dejta kondiviż. Aħna nsibu li l-applikazzjoni ta’ Word2Vec f’qafas ta’ tagħlim ta’ gossips hija vijabbli. Mingħajr ebda a ġġustament, ir-riżultati huma komparabbli ma’ ambjent ċentralizzat tradizzjonali, b’telf ta’ kwalità baxx sa 4.3%. Barra minn hekk, ir-riżultati huma sa 54.8% aħjar mit-taħriġ lokali indipendenti.', 'ms': 'Model NLP lanjutan memerlukan jumlah besar data dari berbagai domain untuk menghasilkan perwakilan kualiti tinggi. Ia berguna kemudian untuk beberapa organisasi awam dan peribadi besar untuk bergabung dengan korpra mereka semasa latihan. Namun, faktor seperti undang-undang dan tekanan pengguna pada privasi data boleh mencegah orkestrasi pusat dan berkongsi data di antara organisasi-organisasi ini. Oleh itu, untuk skenario spesifik ini, kami menyelidiki bagaimana pembelajaran gosip, sebuah protokol secara berterusan, data-peribadi, yang ditentralkan, dibandingkan dengan penyelesaian set data berkongsi. Kami mendapati bahawa aplikasi Word2Vec dalam kerangka pembelajaran gosip adalah hidup. Tanpa apa-apa penyesuaian, hasilnya boleh dibandingkan dengan tetapan pusat tradisional, dengan kehilangan kualiti rendah 4.3%. Selain itu, keputusan adalah 54.8% lebih baik daripada latihan setempat bebas.', 'mn': 'Өндөрсөн NLP загварууд нь олон хэсэгт маш олон мэдээллийн хэрэгтэй, өндөр чанартай илтгэл гаргах боломжтой. Тэгээд хэдэн том, хувийн байгууллагууд сургалтын үед корпоратаа оролцох хэрэгтэй. Гэвч хууль, хэрэглэгчийн хувьд мэдээллийн жинхэнэ амьдралыг анхаарч байгаа хүчин зүйлс нь эдгээр байгууллагуудын хооронд төв зориулагдсан оркестер болон өгөгдлийг хуваалцах боломжто Тиймээс энэ тодорхой хувилбарын тулд бид хэрхэн ярианы суралцах талаар, маш параллел, өгөгдлийн хувилбар, децентрализацийн протоколыг судалж, хуваалцан өгөгдлийн сангийн шийдэлтэй харьцуулдаг вэ? Бид Word2Vec-ын хэрэглээ үндэслэлийн суралцах үйл ажиллагаанд амьдрах боломжтой гэдгийг олж мэднэ. Ямар ч тодорхойлолтгүй үр дүн нь 4.3% бага байдаг уламжлалтай төвөгтэй харьцуулагддаг. Үүний үр дүн нь тогтнолтой орон нутгийн сургалтын дасгал хөдөлгөөнаас илүү 54.8 хувь хүртэл байна.', 'pl': 'Zaawansowane modele NLP wymagają ogromnych ilości danych z różnych domen do tworzenia wysokiej jakości reprezentacji. Jest to przydatne dla kilku dużych organizacji publicznych i prywatnych, aby dołączyć do swoich korpusów podczas szkoleń. Jednak czynniki takie jak ustawodawstwo i nacisk użytkowników na prywatność danych mogą uniemożliwić scentralizowaną orkiestrację i udostępnianie danych między tymi organizacjami. Dlatego dla tego konkretnego scenariusza badamy, w jaki sposób nauka plotek, masowo-równoległy, prywatny, zdecentralizowany protokół, porównuje się z rozwiązaniem współdzielonych zbiorów danych. Uważamy, że zastosowanie Word2Vec w ramach nauki plotek jest opłacalne. Bez jakiegokolwiek strojenia wyniki są porównywalne do tradycyjnego scentralizowanego ustawienia, z utratą jakości nawet 4,3%. Ponadto wyniki są do 54,8% lepsze niż niezależne szkolenia lokalne.', 'sr': 'Napredni NLP modeli zahtevaju ogromne količine podataka iz različitih domena da bi proizveli visoke kvalitetne predstave. Onda je korisno za nekoliko velikih javnih i privatnih organizacija da se pridruže svojoj korporaciji tokom treninga. Međutim, faktori poput zakonodavstva i korisnika naglašavaju na privatnost podataka mogu spriječiti centraliziranu orkestruaciju i dijeljenje podataka među tim organizacijama. Stoga, za ovaj konkretni scenario, istražujemo kako učenje tračeva, masivno paralelno, privatno podatke, decentralizovano protokol uspoređuje sa rešenjem podataka. Nalazimo da je primjena Word2Vec u okviru učenja tračeva održiva. Bez ikakvih podešavanja, rezultati su usporedni sa tradicionalnim centraliziranim postavljanjem, sa gubitkom kvalitete niskim od 4,3%. Osim toga, rezultati su do 54,8% bolji od nezavisnog lokalnog treninga.', 'ro': 'Modelele avansate NLP necesită cantități uriașe de date din diferite domenii pentru a produce reprezentări de înaltă calitate. Este util atunci pentru câteva organizații publice și private mari să se alăture corporațiilor lor în timpul antrenamentului. Cu toate acestea, factori precum legislația și accentul utilizatorilor pe confidențialitatea datelor pot împiedica orchestrarea centralizată și partajarea datelor între aceste organizații. Prin urmare, pentru acest scenariu specific, investigăm modul în care învățarea bârfelor, un protocol masiv paralel, privat de date, descentralizat, se compară cu o soluție partajată de seturi de date. Considerăm că aplicarea Word2Vec într-un cadru de învățare a bârfelor este viabilă. Fără nici o reglare, rezultatele sunt comparabile cu o setare centralizată tradițională, cu o pierdere de calitate de până la 4,3%. În plus, rezultatele sunt cu până la 54,8% mai bune decât formarea locală independentă.', 'so': "Tusaalada horumarinta ee NLP waxay u baahan yihiin macluumaad badan oo ka mid ah deegaanka kala duduwan si ay u soo bandhigaan noocyada takhasuska sare. It is useful then for a few large public and private organizations to join their corpora during training.  Si kastaba ha ahaatee, sababaha sharciga iyo isticmaalayaasha ku qoran gaarka loo leeyahay macluumaadka waxay ka hortagi karaan ururadaas oo ku saabsan ururadaas qayb-qayb-gaar ah. Sidaa darteed waxaynu baaraynaa sidoo kale barbaarinta jimicsiga, si faro badan u kala duduwan, qaabka macluumaadka ee gaarka loo leeyahay, oo la barbaran karo xafiiska loo qeybinayo. Waxaynu ogaanaynaa in codsiga Word2Vec ku habboon jardiino waxbarasho oo muuqan ah. Xilliga la'aanta, resultiyadu waxay u eg yihiin xarunta caadiga ah, waxaana khasaara qiimada u yar 4.3 boqolkiiba. Intaas waxaa dheer in laga helaa 54.8% way ka wanaagsan yihiin waxbarasho xor ah oo degmada ah.", 'sv': 'Avancerade NLP-modeller kräver enorma mängder data från olika domäner för att producera högkvalitativa representationer. Det är då användbart för några stora offentliga och privata organisationer att ansluta sig till sina corpora under utbildningen. Faktorer som lagstiftning och användarbetoning på dataskydd kan dock förhindra centraliserad orkestrering och datadelning mellan dessa organisationer. Därför undersöker vi för detta specifika scenario hur skvallerinlärning, ett massivt parallellt, dataprivat, decentraliserat protokoll, jämför med en delad dataset lösning. Vi finner att tillämpningen av Word2Vec i ett ramverk för skvallerinlärning är genomförbar. Utan någon justering är resultaten jämförbara med en traditionell centraliserad inställning, med en kvalitetsförlust så låg som 4,3%. Dessutom är resultaten upp till 54,8% bättre än självständig lokal utbildning.', 'si': 'ප්\u200dරධානය NLP මෝඩේල් අවශ්\u200dය විවිධ ස්ථානයෙන් විශේෂ දත්ත ගොඩක් අවශ්\u200dය වෙනවා විශේෂ ස්වභාව ඒක ප්\u200dරයෝජනය වෙන්න පුළුවන් විශාල සහ පෞද්ගලික සංවිධානයක් ඔවුන්ගේ කොර්පෝරා එක්ක ප නමුත්, විධානය සහ ප්\u200dරයෝජකයෙන් දත්ත පෞද්ගලිකතාවට ප්\u200dරශ්නය කරනවා වගේම මෙම සංවිධානයෙන් මධ්\u200dයස්ථා ඉතින්, මේ විශේෂ සීනාරියෝ වෙනුවෙන්, අපි පරීක්ෂා කරනවා කොහොමද කියලා, ගොඩක් සමාන්\u200dය, දත්ත-පෞද්ගලික, විශේෂ ප්\u200dරතිකා අපිට හොයාගන්න පුළුවන් විදියට Word2Vec යුද්ධය ප්\u200dරවේශනයක් ප්\u200dරවේශනය වෙන්න පුළුවන් කියලා. කිසිම සංවිධානයක් නැතුව, ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරමාණයක් සංවිධානයක් තියෙන්න පුළුවන් ඒ වගේම, ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරධාන 54.8% වඩා හොඳයි.', 'ta': 'மேம்பட்ட NLP மாதிரிகள் உயர்தரமான குறிப்புகளை உருவாக்குவதற்கு பல களங்களிலிருந்து மிகப்பெரிய தரவு தேவை பிறகு பயிற்சியில் சில பெரிய பொது மற்றும் தனியார் நிறுவனங்களுக்கு பயனுள்ளது. ஆனால், நிறுவனம் மற்றும் பயனர் தனிப்பட்ட தரவுகளை போன்ற காரணிகள் இந்த நிறுவனங்களிடையே பகிர்ந்த தகவல்களை தடுக்கலாம். எனவே, இந்த குறிப்பிட்ட காட்சிக்கு, நாம் குழப்பமான கற்றுப் பார்க்க வேண்டும், அதிகமாக இணையாக, தரவு தனிப்பட்ட நெறிமுறையை ஒப்பிடுகிறோம், பக வார்ட்2வெக்கின் பயன்பாடு ஒரு கூட்டு கற்றுக் கொள்ளும் சட்டத்தில் இருக்கும் என்பதை நாம் கண்டுபிடிக Without any tuning, the results are comparable to a traditional centralized setting, with a loss of quality as low as 4.3%.  மேலும், முடிவுகள் 54.8% வரை சுதந்திரமான உள்ளூர் பயிற்சியை விட சிறந்தது.', 'no': 'Avanserte NLP-modeller krev stor mange data frå ulike domene for å produsera høg kvalitetssrepresentasjonar. Det er nyttig derfor for nokre store offentlige og private organisasjonar å bli med korpora sine under opplæring. Men faktorer som lovgivning og brukaren kan hindra sentralisert orkestrasjon og datadeling mellom desse organisasjonane. Derfor, for denne spesifikke scenarioen, er vi undersøkt korleis læring av gossip, ein massivt parallell, dataprivat, decentralisert protokoll, samanliknar med ei delt datasettløysing. Vi finn at programmet Word2Vec i eit læringsrammeverk er viktig. Utan nokon oppsett er resultatet sammenlignbar med ein tradisjonell sentralisert innstilling, med ein tap av kvaliteten så lav som 4,3%. I tillegg er resultatene opp til 54,8 % bedre enn uavhengig lokal trening.', 'ur': 'آسانس لیپ موڈل کے لئے مختلف ڈومین سے بہت بڑے اندازے ڈاٹ کی ضرورت ہے کہ ان کے لئے بالکل کیفیت کی نمونات پیدا کریں۔ اس کے بعد بہت بڑے عمومی اور خصوصی سازمانوں کے لئے فائدہ ہے کہ ان کے شرکتوں کی تعلیم میں مل جائیں۔ However, such as legislation and user emphasis on data privacy may prevent centralized orchestration and data sharing among these organizations. لہٰذا، اس خاص صحیح سناریو کے لئے ہم تحقیق کرتے ہیں کہ گوسپ کی تعلیم کس طرح کی مطابق ہے، بڑی مشابہ، ڈاٹا-خصوصی، ڈیٹا-منطقی پروٹروکال، شریک ڈاٹسٹ کے حل سے مطابق ہے. ہمیں معلوم ہے کہ Word2Vec کا کاروبار ایک گوسپی سیکھنے کا فرمود قابل ہے۔ کسی تنظیم کے بغیر، نتیجے ایک منطقی مرکزی تنظیم کے مطابق مطابق ہیں، جیسے 4.3% کم کیفیت کا خسارہ ہے. اس کے علاوہ، نتیجے 54.8% سے زیادہ اچھے ہیں.', 'uz': "Name Шундай бўлса, у камбағал ва шахс ташкилотлар ўз композиясига қўшилиш фойдаланади. Lekin, bunday qoidalar va foydalanuvchilar maʼlumot shaxsiyatlarida foydalanuvchilarga foydalanuvchi sabablar bu tashkilotlarning tarkibini kattalashtirish va maʼlumot bilan bog'lash mumkin. Шундай қилиб, бу махсус таркибида, биз махсус маълумотлар бажаришига, маълумот парламент, махсус протоколига нисбатан тўсиқ бажарилган маълумот манзилга эга бўлишини кўриб турамиз. Biz o'rganish qatlamlarida Word2Vec dasturini ishlatish mumkin. @ info: whatsthis Ko'rsatganda, natijalar taʼminlovchi lokal tajribadan 54.8% yaxshi.", 'vi': 'Những mô hình Chọc thế bục cao tuổi yêu cầu một lượng lớn dữ liệu từ các miền khác nhau để sản xuất các biểu tượng cao chất lượng. Đó là điều cần thiết cho vài tổ chức lớn công cộng và tư nhân tham gia với hạ sĩ của họ trong khi huấn luyện. Tuy nhiên, những nhân tố như luật pháp và người dùng nhấn mạnh về tính mạng cá nhân có thể ngăn chặn việc lập trình tập trung và chia sẻ dữ liệu giữa các tổ chức này. Vì vậy, với kịch bản cụ thể này, chúng ta điều tra làm thế nào việc học buôn chuyện, một giao thức dữ liệu-riêng-tư-thông-tin, so sánh với giải pháp bộ dữ liệu chia sẻ. Chúng tôi thấy rằng sử dụng Word2Vek trong một cơ sở học buôn chuyện là có thể. Không có độ chỉnh nào, kết quả có thể so sánh với một thiết lập tập trung truyền thống, với một mức độ thua thấp bằng 4.3. Hơn nữa, kết quả đã đạt đến 54.8 cao hơn cả một khóa huấn luyện địa phương độc lập.', 'bg': 'Разширените модели на НЛП изискват огромни количества данни от различни области, за да произвеждат висококачествени представяния. Тогава е полезно няколко големи публични и частни организации да се присъединят към корпорите си по време на обучението. Въпреки това фактори като законодателството и акцента на потребителите върху поверителността на данните могат да попречат на централизираното оркестриране и споделяне на данни между тези организации. Ето защо, за този конкретен сценарий, изследваме как изучаването на клюки, масово паралелен, частен, децентрализиран протокол, се сравнява с решение за споделен набор от данни. Намираме, че приложението на Word2Vec в рамка за обучение на клюки е жизнеспособно. Без никаква настройка, резултатите са сравними с традиционната централизирана настройка, с загуба на качество чак 4.3%. Освен това резултатите са с до 54,8% по-добри от независимото местно обучение.', 'nl': 'Geavanceerde NLP-modellen vereisen enorme hoeveelheden gegevens uit verschillende domeinen om representaties van hoge kwaliteit te produceren. Het is dan handig voor een paar grote publieke en private organisaties om tijdens de training deel te nemen aan hun corpora. Factoren zoals wetgeving en de nadruk van gebruikers op gegevensbescherming kunnen echter een gecentraliseerde orkestratie en gegevensdeling tussen deze organisaties voorkomen. Daarom onderzoeken we voor dit specifieke scenario hoe roddelleren, een massaal-parallel, data-privé, gedecentraliseerd protocol, zich verhoudt tot een shared-dataset oplossing. We vinden dat de toepassing van Word2Vec in een roddelleerkader haalbaar is. Zonder afstemming zijn de resultaten vergelijkbaar met een traditionele gecentraliseerde setting, met een kwaliteitsverlies tot 4.3%. Bovendien zijn de resultaten tot 54,8% beter dan onafhankelijke lokale opleidingen.', 'hr': 'Napredni NLP modeli zahtijevaju ogromne količine podataka iz različitih domena za proizvodnju visokokvalitetnih predstavljanja. Onda je korisno za nekoliko velikih javnih i privatnih organizacija pridružiti se svom tijekom obuke. Međutim, činjenici poput zakonodavstva i korisnika naglašavaju na privatnost podataka mogu spriječiti centraliziranu orkestruaciju i dijeljenje podataka među tim organizacijama. Stoga, za ovaj konkretni scenario, istražujemo kako učenje tračeva, masivno paralelno, privatno podatke, decentralizirani protokol uspoređuje s rješenjem podataka. Nalazimo da je primjena Word2Vec u okviru učenja tračeva održiva. Bez bilo kakvih mjerenja, rezultati su usporedbeni sa tradicionalnom centraliziranom postavkom, s gubitkom kvalitete niskim od 4,3%. Nadalje, rezultati su do 54,8% bolji od nezavisnog lokalnog treninga.', 'da': 'Avancerede NLP-modeller kræver enorme mængder data fra forskellige domæner for at producere repræsentationer af høj kvalitet. Det er så nyttigt for nogle få store offentlige og private organisationer at slutte sig til deres corpora under træningen. Faktorer som lovgivning og brugernes vægt på databeskyttelse kan dog forhindre centraliseret orkestrering og datadeling mellem disse organisationer. Derfor undersøger vi for dette specifikke scenarie, hvordan sladder learning, en massivt parallel, data-privat, decentraliseret protokol, sammenlignes med en delt datasæt løsning. Vi finder, at anvendelsen af Word2Vec i en sladder læring ramme er levedygtig. Uden nogen tuning er resultaterne sammenlignelige med en traditionel centraliseret indstilling, med et tab af kvalitet så lavt som 4,3%. Endvidere er resultaterne op til 54,8% bedre end uafhængig lokal uddannelse.', 'id': 'Model NLP maju membutuhkan jumlah besar data dari berbagai domain untuk menghasilkan representation kualitas tinggi. Maka berguna bagi beberapa organisasi publik dan pribadi besar untuk bergabung dengan korpora mereka selama latihan. Namun, faktor seperti undang-undang dan tekanan pengguna pada privasi data dapat mencegah orkestrasi dan berbagi data diantara organisasi-organisasi ini. Oleh karena itu, untuk skenario spesifik ini, kami menyelidiki bagaimana pembelajaran gosip, sebuah protokol yang sangat paralel, data-pribadi, yang disentralisasi, dibandingkan dengan solusi set data berbagi. Kami menemukan bahwa aplikasi Word2Vec dalam rangka pembelajaran gosip adalah rental. Tanpa pengaturan apapun, hasilnya bisa dibandingkan dengan pengaturan tradisional yang ditentralisasi, dengan kehilangan kualitas rendah 4,3%. Selain itu, hasilnya sampai 54,8% lebih baik dari pelatihan lokal independen.', 'de': 'Erweiterte NLP-Modelle erfordern große Datenmengen aus verschiedenen Domänen, um qualitativ hochwertige Darstellungen zu erzeugen. Es ist dann nützlich für einige große öffentliche und private Organisationen, ihre Korpora während des Trainings beizutreten. Allerdings können Faktoren wie Gesetzgebung und der Schwerpunkt der Nutzer auf Datenschutz eine zentralisierte Orchestrierung und den Datenaustausch zwischen diesen Organisationen verhindern. Daher untersuchen wir für dieses spezifische Szenario, wie Gossip Learning, ein massiv-paralleles, datenprivates, dezentrales Protokoll, mit einer Shared-Dataset-Lösung verglichen wird. Wir finden, dass die Anwendung von Word2Vec in einem Gossip Learning Framework praktikabel ist. Ohne jegliche Abstimmung sind die Ergebnisse vergleichbar mit einer traditionellen zentralisierten Einstellung, mit einem Qualitätsverlust von nur 4,3%. Darüber hinaus sind die Ergebnisse bis zu 54,8% besser als unabhängige lokale Schulungen.', 'fa': 'مدلهای NLP پیشرفته به اندازه\u200cهای داده\u200cهای بزرگی از دامنه\u200cهای مختلف نیاز دارند تا نمایش\u200cهای کیفیت بالا را تولید کنند. پس برای چند سازمان عمومی و خصوصی که در طول آموزش به شرکت خود ملحق می شوند، مفید است. ولی فعالیت\u200cهای مانند قانون و کاربر بر خصوصی داده\u200cها می\u200cتوانند جلوی شرکت و شرکت داده\u200cهای مرکزی بین این سازمان\u200cها را بگیرند. بنابراین، برای این سناریو خاص، ما تحقیق می\u200cکنیم که چگونه یادگیری گفتگو، یک پروتکل بسیار متفاوت، داده\u200cهای خصوصی، دکنترلایزی شده، با یک راه حل داده\u200cهای مشترک مقایسه می\u200cکند. ما پیدا می\u200cکنیم که کاربرد Word2Vec در یک چهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهارچهار بدون هیچ تنظیم، نتیجه\u200cها با یک تنظیم مرکزی سنتی مقایسه می\u200cشوند، با کمبود کیفیت به اندازه ۴.۳ درصد. علاوه بر این، نتایج تا 54.8% بهتر از آموزش محلی مستقل است.', 'ko': '고급 NLP 모델은 다양한 영역에서 대량의 데이터를 얻어 고품질의 표현을 생성해야 한다.일부 대형 공공과 개인 조직에 대해 말하자면, 훈련 기간에 그들의 어료 라이브러리에 가입하는 것은 매우 유용하다.그러나 입법과 사용자가 데이터 프라이버시에 대한 중시 등 요소는 이들 조직 간의 집중적인 조율과 데이터 공유를 방해할 수 있다.따라서 이 특정 장면에 대해 우리는 팔괘학습(대규모 병행, 데이터 사유, 분산 협의)과 공유 데이터 집합 솔루션의 비교를 연구할 것이다.우리는Word2Vec가 팔괘 학습 프레임워크에서의 응용이 가능하다는 것을 발견했다.아무런 조정 없이 전통적인 집중 설치와 비슷하게 품질 손실이 4.3%까지 낮아졌다.또 현지 독립훈련과 비교해 훈련 결과는 54.8%에 달했다.', 'tr': "Esasy NLP nusgalary görkezilýän sahypalardan gaty bir sany maglumat gerek. Şol wagt köp uly we hususiy guramlar üçin korporasyna okuw wagtynda gatnaşmak üçin peýdaly. Ýöne kanunlar we ullançylar ýaly maglumatyň hususiýeti barada wajyp çekýän faktörler, centralized orkestrasyon we maglumaty bölüşmegi gözden çykaryp biler. Şol sebäpli, bu spesifik senaryýa üçin geň öwrenmeleri nädip çykyp barlaşýarys, maglumatlar-hususizlik we decentralized protokollaryň çözümüne karşılaşýar. Biz sözleşik öwrenmek çerçevesinde Word2Vec'yň uygulamasy mümkin däldir. Hiç hili deňleşme bolmadyk, netijeler däpli ortalamaly düzümlenmiş düzümler bilen, 4.3% ýaly ýitilmek bilen deňleşmelidir. Munuň üstünde netijeler ýerleşdirim eğitimäniňden 54.8% gowydyr.", 'sw': 'Mradi wa NLP zilizopangwa unahitaji kiasi kikubwa cha data kutoka maeneo mbalimbali ili kutengeneza uwakilishi wa kiwango kikubwa. Ni muhimu sana kwa mashirika makubwa ya umma na binafsi kujiunga na kampuni yao wakati wa mafunzo. Hata hivyo, sababu kama vile sheria na mtumiaji wanasisitiza taarifa za faragha ya data zinaweza kuzuia ujasiriano na kusambaza taarifa kati ya mashirika haya. Kwa hiyo, kwa mtazamo huu maalum, tunachunguza jinsi elimu ya udanganyifu, mkataba wa kibinafsi wa taarifa, ukilinganisha na suluhisho la usambazaji wa data. Tunapata kwamba matumizi ya Word2Vec katika mfumo wa kujifunza maudhui ni muhimu. Bila kutangazwa, matokeo yanafanana na kituo cha kitamaduni kilichotengenezwa, na kupoteza kiwango cha chini cha asilimia 4.3. Zaidi ya hayo, matokeo yanafikia asilimia 54.8 bora kuliko mafunzo huru ya wenyeji.', 'af': "Gevorderde NLP modelles benodig groot hoeveelheid data van verskeie domeine om hoë- kwaliteit voorstellings te produseer. Dit is dan nuttig vir 'n paar groot openbare en privaat organisasies om hul korpora by die onderwerp te sluit. Maar faktore soos wetgewing en gebruiker bepaal op data privateit kan voorkom sentraliseerde orkestrasie en data deel onder hierdie organisasies. Daarom, vir hierdie spesifieke scenario, ondersoek ons hoe spesifieke leer,  'n massief-parallel, data-privaat, decentraliseerde protokol, vergelyk met 'n gedeelde-datastel oplossing. Ons vind dat die toepassing van Word2Vec in 'n spesifieke leer raamwerk is beskikbaar. Sonder enige toepassing, is die resultate vergelykbaar met 'n tradisionele sentraliseerde instelling, met 'n verlies van kwaliteit so lae as 4.3%. Verder is die resultate tot 54,8% beter as onafhanklike plaaslike onderwerp.", 'sq': 'Modelet e avancuara të NLP kërkojnë sasi të mëdha të dhënash nga fusha të ndryshme për të prodhuar përfaqësime të cilësisë së lartë. Atëherë është e dobishme për disa organizata të mëdha publike dhe private të bashkohen me korporatën e tyre gjatë trajnimit. Megjithatë, faktorë të tillë si legjislacioni dhe theksi i përdoruesit mbi privatësinë e të dhënave mund të parandalojnë orkestrimin e centralizuar dhe ndarjen e të dhënave midis këtyre organizatave. Prandaj, për këtë skenar specifik, ne hetojmë se si mesimi i gënjeshtrave, një protokoll masiv-parallel, privat-të dhënash, i decentralizuar, krahasohet me një zgjidhje të përbashkët-të dhënash. Ne zbulojmë se aplikimi i Word2Vec në një kuadër mësimi të gënjeshtrave është i jetueshëm. Pa ndonjë rregullim, rezultatet janë të krahasueshme me një vendosje tradicionale të centralizuar, me humbje cilësie sa 4.3%. Përveç kësaj, rezultatet janë deri në 54.8% më të mirë se trainimi i pavarur lokal.', 'am': 'የNLP ዓይነቶች ከፍተኛ ጥሩ መልዕክቶችን ለማግኘት ብዙ ዳታዎችን ያስፈልጋሉ። በዚያን ጊዜም ለጥቂት ትልቁ የህዝብ እና የብሔራዊ ሠራዊቶች ኮፖርቲካቸውን በመጠቀም ይጠቅማል፡፡ ነገር ግን፣ እንደሕግ እና ተጠቃሚው የዳታ ግልኙነት ላይ የሚያስፈልጉት ጉዳዮች እነዚህ ድርጅቶች መካከል ማዕከላዊ ኦርክስታርሽን እና ዳራዎችን ማጋራረጃ ሊከለክሉ ይችላል፡፡ ስለዚህም ለዚህ በተለያዩ ስታይና የግንኙነት ትምህርት፣ እንዴት እንደተለይ፣ የዳታ-የግልኙነት፣ የሥልጣን ፕሮግራም፣ በተካፈለው የዳታ-አካባቢ መፍትሄን እናሳውቃለን፡፡ We find that the application of Word2Vec in a gossip learning framework is viable.  ምንም ጉዳይ ሳይኖር ፍጥረቶቹ ለባሕላዊ ማዕከላዊ ማዕከላዊ ማስተካከል ይተያያይበታል፡፡ ከዚህም በላይ ፍሬዎቹ ወደ ነፃ ግንኙነት ትምህርት ትምህርት 54.8 በመቶ ይሻላል፡፡', 'az': "Əlavə olunan NLP modelləri müxtəlif domenalardan böyük məlumat lazımdır ki, yüksək kaliteli göstəricilər ürəkləsinlər. Bundan sonra bir neçə böyük və xüsusi organizasiya təhsil etdiyi vaxt corpora katılması faydalıdır. Lakin, bu organizasyonlar arasında məlumatlar və istifadəçilər tərzlərini məlumatların xəlifliyinə təsirləndirmək üçün məlumatlar və məlumatlar paylaşılmasını qadağan edə bilər. Beləliklə, bu xüsusi scenario üçün, biz sözlərin öyrənməsini, böyük parallel, məlumat-xüsusi, decentralized protokol kimi, paylaşılmış veri qutusu çətinliyinə qarşılaşdırırıq. Biz söhbət öyrənmə qaydasında Word2Vec'in uyğulaması mümkün olduğunu görürük. Heç bir tərzim olmadan, sonuçlar 4.3%-dən düşük olduğu tərzimlə, nəticələri nümunə etibarlı bir tərzimlə bərabər olar. Daha sonra, sonuçlar bağımsız yerli təhsil etməkdən daha yaxşıdır 54,8%.", 'hy': 'Advanced NLP models require huge amounts of data from various domains to produce high-quality representations.  Այն ժամանակ օգտակար է մի քանի մեծ հանրային և մասնավոր կազմակերպությունների համար միանալ իրենց կապորան ուսուցման ժամանակ: Այնուամենայնիվ, այնպիսի գործոններ, ինչպիսիք են օրենսդրությունը և օգտագործողների ուշադրությունը տվյալների գաղտնիության վրա, կարող են կանխել կենտրոնացված նվագախմբի և տվյալների կիսման այս կազմակերպությունների միջ Այսպիսով, այս կոնկրետ սցենարիայի համար մենք ուսումնասիրում ենք, թե ինչպես է խոսքերի ուսումնասիրությունը, զանգվածային-զուգահեռ, տվյալների-մասնավոր, դեկտրոնացված պրոտոկոլ համեմատությունը ընդհանուր տվյալների համակարգի լուծու Մենք կարծում ենք, որ Word2vec-ի կիրառումը խոսքերի ուսումնասիրության շրջանակում հնարավոր է: Առանց որևէ հարմարեցման, արդյունքները համեմատուկ են ավանդական կենտրոնացված միջավայրին, որի որակի կորստը այնքան ցածր է, որքան 4.3 տոկոսը: Ավելին, արդյունքները 54.8 տոկոսով ավելի լավ են, քան անկախ տեղական ուսումնասիրությունը:', 'bs': 'Napredni NLP modeli zahtijevaju ogromne količine podataka iz različitih domena da bi proizveli visoke kvalitetne predstave. Onda je korisno za nekoliko velikih javnih i privatnih organizacija pridružiti se svom tijekom treninga. Međutim, faktori poput zakonodavstva i korisnika naglašavaju na privatnost podataka mogu spriječiti centraliziranu orkestruaciju i dijeljenje podataka među tim organizacijama. Stoga, za ovaj konkretni scenario, istražujemo kako učenje tračeva, masivno paralelno, privatno podatke, decentralizirani protokol uspoređuje s rješenjem podataka. Nalazimo da je primjena Word2Vec u okviru učenja tračeva održiva. Bez ikakvih mjerenja, rezultati su usporedbeni sa tradicionalnom centraliziranom postavkom, sa gubitkom kvalitete niskom od 4,3%. Nadalje, rezultati su do 54,8% bolji od nezavisnog lokalnog treninga.', 'bn': 'Advanced NLP models require huge amounts of data from various domains to produce high-quality representations.  তারপর কিছু বিশাল জনগণ এবং ব্যক্তিগত প্রশিক্ষণের সময় তাদের কোর্পোরায় যোগ দেয়ার জন্য এটা উপকার। তবে আইন এবং ব্যবহারকারীদের ব্যক্তিগত তথ্যের ব্যাক্তিগত ব্যাপারে জোর দিয়েছে যেমন এই সংস্থাগুলোর মধ্যে কেন্দ্রীয় কাঠামো  সুতরাং এই বিশেষ দৃশ্যের জন্য আমরা তদন্ত করি গুপ্তচর শিক্ষা, ব্যাপক-প্যারালেল, ডাটা ব্যক্তিগত, নিয়ন্ত্রণের প্রোটোকল, শেয়ার করা ডাটাসেট সমাধ আমরা খুঁজে পাচ্ছি যে গুজব শিক্ষা ফ্রেমে ওয়ার্ড২ভেকের অ্যাপ্লিকেশন প্রয়োজন। কোন ধরনের কাজ ছাড়াই ফলাফল ঐতিহ্যবাহী কেন্দ্রীয় সেটের সাথে তুলনা করা হয়েছে, যার মান ৪. এছাড়াও, ফলাফল ৫৪. ৮% বেশি ভালো স্থানীয় প্রশিক্ষণের চেয়ে ভাল।', 'ca': "Els models avançats de NLP requereixen grans quantitats de dades de diversos dominis per produir representacions d'alta qualitat. És útil que algunes grans organitzacions públices i privades s'uneixin a la seva corpora durant l'entrenament. Tot i així, factors com la legislació i l'enfasi dels usuaris en la privacitat de les dades poden evitar l'orquestració centralitzada i el intercanvi de dades entre aquestes organitzacions. Per tant, per aquest escenari específic, investigam com l'aprenentatge de gossips, un protocol massivament paralèl·lel, privat i descentralitzat, es compara amb una solució compartida. Trobem que l'aplicació de Word2Vec en un marc d'aprenentatge de gossips és viable. Sense cap ajustament, els resultats són comparables a un entorn centralitzat tradicional, amb una pèrdua de qualitat tan baixa com el 4,3%. A més, els resultats són fins al 54,8% millors que la formació local independent.", 'fi': 'Kehittyneet NLP-mallit vaativat valtavia määriä dataa eri toimialoilta korkealaatuisten esitysten tuottamiseksi. Tällöin on hyödyllistä, että muutamat suuret julkiset ja yksityiset organisaatiot liittyvät corporaansa koulutuksen aikana. Lainsäädännön ja käyttäjien yksityisyyden korostamisen kaltaiset tekijät voivat kuitenkin estää keskitetyn organisoinnin ja tietojen jakamisen näiden organisaatioiden kesken. Tämän vuoksi tutkimme tässä skenaariossa, miten juoruoppimista, massiivisen rinnakkaista, datayksityistä, hajautettua protokollaa, verrataan jaetun datajoukon ratkaisuun. Mielestämme Word2Vecin soveltaminen juoruoppimiseen on kannattavaa. Ilman viritystä tulokset ovat verrattavissa perinteiseen keskitettyyn asetukseen, jossa laadun menetys on jopa 4,3%. Lisäksi tulokset ovat jopa 54,8 prosenttia parempia kuin riippumaton paikallinen koulutus.', 'et': 'Täiustatud NLP mudelid nõuavad suurt hulka andmeid erinevatest valdkondadest, et luua kvaliteetseid esitusi. Siis on kasulik, et mõned suured avalik-õiguslikud ja eraõiguslikud organisatsioonid liituksid koolituse ajal oma korporatsioonidega. Siiski võivad sellised tegurid nagu õigusaktid ja kasutajate rõhuasetus andmekaitsele takistada nende organisatsioonide keskset korraldamist ja andmete jagamist. Seetõttu uurime selle konkreetse stsenaariumi puhul, kuidas klatšimootor, massiliselt paralleelne, andmeprivaatne detsentraliseeritud protokoll, võrreldakse jagatud andmekogumi lahendusega. Leiame, et Word2Veci rakendamine kuulujutte õppe raamistikus on elujõuline. Ilma häälestamiseta on tulemused võrreldavad traditsioonilise tsentraliseeritud seadistusega, kusjuures kvaliteedi kaotus on nii madal kui 4,3%. Lisaks on tulemused kuni 54,8% paremad kui sõltumatu kohalik koolitus.', 'cs': 'Pokročilé modely NLP vyžadují obrovské množství dat z různých domén k vytvoření vysoce kvalitních reprezentací. Je tedy užitečné pro několik velkých veřejných a soukromých organizací, aby se připojily ke svým korpusům během školení. Faktory, jako jsou legislativa a důraz uživatelů na ochranu osobních údajů, však mohou bránit centralizované orchestrizaci a sdílení dat mezi těmito organizacemi. Proto pro tento konkrétní scénář zkoumáme, jak se drbové učení, masivně paralelní, data-privátní, decentralizovaný protokol, srovnává s řešením sdílených dat. Zjišťujeme, že aplikace Word2Vec v rámci učení drbů je životaschopná. Bez jakéhokoliv ladění jsou výsledky srovnatelné s tradičním centralizovaným nastavením, se ztrátou kvality až 4,3%. Kromě toho jsou výsledky až 54,8% lepší než samostatná místní školení.', 'jv': 'modells NLP Average Awak dhéwé iso nggawe kanggo kelas akeh Publik lan sakjané pribadi kanggo gabung ning acara cara-cara sing tukang. Nanging, cah-cah bener kanggo hukum lan uwong pengguna nguasai perusahaan data bisa nguasai perusahaan pengguna kuwi ngregani ora ono wektu nggawe sistem sing dadi iki. Nanging, nggo langkung sekenari iki, kita mulasah piye basa gambar nggambar, akeh paneelno-perusahaan, data-pribadi, desenalke protokol Awak dhéwé ngerasakno karo aplikasi word 2vec ning acara awak dhéwé kuwi nggawe barang. #Siji Label, dadi mbut sing wis luwih luwih saben kanggo', 'he': 'דוגמניות NLP מתקדמות דורשות כמויות עצומות של נתונים מתחומים שונים כדי לייצר ייצוגים איכות גבוהה. אז זה שימושי לכמה ארגונים ציבוריים ופרטיים גדולים להצטרף לקופורה שלהם במהלך האימונים. However, factors such as legislation and user emphasis on data privacy may prevent centralized orchestration and data sharing among these organizations.  לכן, עבור התרחיש המיוחד הזה, אנו חוקרים כיצד ללמוד רכילות, פרוטוקול מסיבי-מקביל, נתוני-פרטי, מרכז, משווה לפתרון של קבוצת נתונים משותפת. אנו מוצאים שהשימוש של Word2Vec במסגרת לימוד רכילות הוא אפשרי. ללא שינוי כלשהו, התוצאות שוואות לסטה מרכזית מסורתית, עם אובדן איכות נמוך עד 4.3%. בנוסף, התוצאות טובות עד 54.8% יותר מאימונים מקומיים עצמאיים.', 'ha': "Motsalar NLP da aka buɗa ana ƙayyade yawan data mai girma daga guda daban-daban dõmin ya ƙãga masu tsari da inganci. Ina da amfani a lokacin da masu jama'a da mashuki masu yawa na jamii da mutane su haɗa makampuni a lokacin da za su yi mafunzo. However, factors such as legislation and user emphasis on data privacy may prevent centralized orchestration and data sharing among these organizations.  Saboda haka, zuwa wannan na ƙayyade, muna karatun jinsi da ake karanta zane na gossiki, a tsakanin-da-ɗabi'a, da misalin zaɓallin da aka yi raba-danganta. Tuna gane cewa shirin ayuka na Wd2ve cikin wani firam na lõkaci na ƙara. Babu wani tunkuɗe, matsala za'a sami daidai da tsarin ɗabi'a, da haske mai nauyi kamar 4.3%. Da haka, matsala za ta isa zuwa 54.8% mafiya alhẽri daga shirin lokal da ba ta koma ba.", 'sk': 'Napredni modeli NLP zahtevajo ogromne količine podatkov iz različnih področij za izdelavo visokokakovostnih predstavitev. Potem je koristno, da se nekaj velikih javnih in zasebnih organizacij pridruži svojim korpusom med usposabljanjem. Vendar pa dejavniki, kot sta zakonodaja in poudarek uporabnikov na zasebnosti podatkov, lahko preprečijo centralizirano organizacijo in izmenjavo podatkov med temi organizacijami. Zato za ta specifičen scenarij raziskujemo, kako se učenje čenčev, množično vzporedni, podatkovno zasebni, decentralizirani protokol primerja z rešitvijo skupnega nabora podatkov. Ugotovili smo, da je uporaba Word2Vec v okviru učenja opravljanja izvedljiva. Brez uglaševanja so rezultati primerljivi s tradicionalno centralizirano nastavitvijo, z izgubo kakovosti že 4,3%. Poleg tega so rezultati do 54,8% boljši od neodvisnega lokalnega usposabljanja.', 'bo': 'མཐོ་རིམ་གྱི་NLP མིག་དབྱིབས་སྤྱོད་པ་སྤྱོད་མཁན་དབྱིབས་ཆེ་བའི་ཚད་ཆེ་བ་དགོས་ཡོད་པ་ལ་རང་ཉིད་ཀྱི་ཁྱེར་སྤྱོད་དང་། དེ་ལས་སྤྱི་ཚོགས་ཆེན་པོ་ཞིག་དང་སྒེར་གྱི་ཚོགས་ལས་དབུགས་འབྲེལ་མཐུད་བྱེད་ནི་ཕན་ཚུལ་ཆེན་ཡོད། ཡིན་ནའང་། སྒྲིག་འགོད་དང་སྤྱོད་མཁན་གྱི་མི་སྒེར་གྱི་ཆ་འཕྲིན་ལ་གསལ་བཤད་ཀྱི་ཆ་རྐྱེན་པས། དབུས་གཞུང་དང་སྒྲིག་འཛིན་འདི་ཚོ་ Therefore, for this specific scenario, we investigate how gossip learning, a massively-parallel, data-private, decentralized protocol, compares to a shared-dataset solution. ང་ཚོས་ཤེས་པའི་གྲངས་སྐོར་གྱི་ནང་དུ་Word2Vec(Word2Vec)ཡི་བྱ་རིམ་དེ་ལས་ཕན་ཆེར་བ་ཐུབ་པ་ཡིན། ཞུན་བཤེར་མ་བྱེད་པ་ཞིག་ནི་དབུས་གཞུང་གི་སྒྲིག་སྟངས་དང་མཐུན་རྐྱེན་སྒྲིག མ་ཟད། གྲུབ་འབྲས་འབྲས་འདི་རང་ཁུལ་གྱི་སྐད་ཡུལ་སྒྲིག་འགོད་ལས་ཉེར་སྤྱོད་པ་ལས་ཉེན་བརྗོད།'}
{'en': 'Multilingual ELMo and the Effects of Corpus Sampling', 'es': 'eLMO multilingüe y los efectos del muestreo de corpus', 'fr': "ElMo multilingue et les effets de l'échantillonnage de corpus", 'ar': 'ELMo متعدد اللغات وآثار أخذ عينات الجسم', 'pt': 'ELMo multilíngue e os efeitos da amostragem de corpus', 'hi': 'बहुभाषी ELMo और कॉर्पस नमूनाकरण के प्रभाव', 'ja': '多言語ELMoとコーパスサンプリングの効果', 'zh': '多言 ELMo 与语料库采样', 'ga': 'ELMO Ilteangach agus Éifeachtaí Samplála Corpais', 'ru': 'Многоязычный ELMo и последствия отбора проб организма', 'ka': 'Name', 'el': 'Πολυγλωσσική ΕΛMo και οι επιπτώσεις της δειγματοληψίας σώματος', 'hu': 'Többnyelvű ELMo és a Corpus Mintavétel hatásai', 'kk': 'Көп тілді ELMo және корпус үлгісінің эффекттері', 'lt': 'Daugiakalbis ELMo ir Corpus mėginių ėmimo poveikis', 'it': 'ELMo multilingue e gli effetti del campionamento Corpus', 'mk': 'Мултијазична ЕЛМО и ефектите на примерок на корпус', 'ms': 'ELMo berbilang bahasa dan Kesan Sampel Corpus', 'mn': 'Олон хэлний ELMo болон Corpus Sampling нөлөө', 'ml': 'കോര്\u200dപ്പുസ് മാമ്പിളിങ്ങിന്റെ പ്രഭാഷണങ്ങളും', 'mt': 'ELMo multilingwi u l-Effetti tat-Teħid ta’ Kampjuni ta’ Corpus', 'pl': 'Wielojęzyczne ELMo i efekty pobierania próbek korpusowych', 'no': 'Fleirspråk ELMo og effektar for korpussamling', 'ro': 'ELMo multilingvă și efectele eșantionării Corpus', 'sr': 'Višejezički ELMo i efekti uzoraka korpusa', 'si': 'Name', 'so': 'ELMo iyo Effections of Corpus sameynta', 'sv': 'Flerspråkig ELMo och effekterna av Corpus Sampling', 'ta': 'பல மொழி ELMo மற்றும் கார்புஸ் மாதிரியும் விளைவுகள்', 'ur': 'Multilingual ELMo and the effects of Corpus Sampling', 'uz': 'Name', 'vi': 'Truyền thuyết đa ngôn ngữ và hiệu ứng của Tập Đoàn mẫu', 'bg': 'Многоезична ELMO и ефектите от вземането на проби от корпус', 'nl': 'Meertalige ELMo en de effecten van Corpus Sampling', 'da': 'Flersproget ELMo og virkningerne af Corpus Sampling', 'ko': '다국어 ELMo와 자료 라이브러리 추출 효과', 'id': 'ELMo berbilang bahasa dan Efek Sampel Corpus', 'fa': 'ELMo Multilingual و Effects of Corpus Sampling', 'hr': 'Višejezički ELMo i učinak uzoraka korpusa', 'sw': 'Multilingual ELMo and the Effects of Corpus Sampling', 'af': 'Name', 'sq': 'ELMo shumëgjuhës dhe efektet e kampionatit të Corpus', 'de': 'Mehrsprachige ELMo und die Auswirkungen von Corpus Sampling', 'az': 'Multilingual ELMo və Corpus Sampling Etkileri', 'tr': 'Çoklu dilli ELMo ve korpus örneklerinin etkisi', 'hy': 'Բազլեզու ELMo-ը և Կորպուսի նմուշների ազդեցությունները', 'am': 'ቋንቋዎች', 'cs': 'Vícejazyčné ELMo a účinky vzorkování korpusu', 'bn': 'বহুভাষায় ELMo এবং কোর্পাস সাম্পলিং এর প্রভাব', 'et': 'Mitmekeelne ELMo ja korpuse proovivõtmise mõju', 'bs': 'Višejezički ELMo i učinke uzoraka korpusa', 'ca': 'ELMo multilingüe i els efectes de la mostra de Corpus', 'fi': 'Monikielinen ELMO ja Corpus-näytteenoton vaikutukset', 'jv': 'Mulitping elMo lan Effects of corpus Sampling', 'he': 'ELMo רבות שפות וההשפעות של דגימת קורפוס', 'ha': 'KCharselect unicode block name', 'sk': 'Večjezični ELMO in učinki vzorčenja korpusa', 'bo': 'སྐད་རིགས་ཀྱི་སྣ་ཚོགས་ELMo དང་གཟུགས་ཀྱི་དཔེ་དབྱེ་བ།'}
{'en': 'Multilingual pretrained language models are rapidly gaining popularity in NLP systems for non-English languages. Most of these models feature an important corpus sampling step in the process of accumulating training data in different languages, to ensure that the signal from better resourced languages does not drown out poorly resourced ones. In this study, we train multiple multilingual recurrent language models, based on the ELMo architecture, and analyse both the effect of varying corpus size ratios on downstream performance, as well as the performance difference between monolingual models for each language, and broader multilingual language models. As part of this effort, we also make these trained models available for public use.', 'ar': 'تكتسب النماذج اللغوية متعددة اللغات شعبية بسرعة في أنظمة البرمجة اللغوية العصبية للغات غير الإنجليزية. تتميز معظم هذه النماذج بخطوة مهمة لأخذ عينات المجموعة في عملية تجميع بيانات التدريب بلغات مختلفة ، للتأكد من أن الإشارة من اللغات ذات الموارد الأفضل لا تغرق اللغات ضعيفة الموارد. في هذه الدراسة ، نقوم بتدريب العديد من نماذج اللغة المتكررة متعددة اللغات ، بناءً على بنية ELMo ، ونحلل تأثير نسب حجم الجسم المتغيرة على أداء المصب ، بالإضافة إلى اختلاف الأداء بين النماذج أحادية اللغة لكل لغة ، ونماذج اللغة متعددة اللغات الأوسع. . كجزء من هذا الجهد ، نجعل هذه النماذج المدربة متاحة للاستخدام العام.', 'es': 'Los modelos lingüísticos multilingües preentrenados están ganando popularidad rápidamente en los sistemas de PNL para idiomas distintos del inglés. La mayoría de estos modelos presentan un paso importante de muestreo de corpus en el proceso de acumulación de datos de entrenamiento en diferentes idiomas, para garantizar que la señal de los idiomas con mejores recursos no ahogue a los que tienen pocos recursos. En este estudio, entrenamos múltiples modelos lingüísticos recurrentes multilingües, basados en la arquitectura eLMO, y analizamos tanto el efecto de las diferentes proporciones de tamaño de los cuerpos en el rendimiento posterior, como la diferencia de rendimiento entre los modelos monolingües para cada idioma y los modelos lingüísticos multilingües más amplios. . Como parte de este esfuerzo, también hacemos que estos modelos capacitados estén disponibles para el uso público.', 'pt': 'Modelos de idiomas pré-treinados multilíngues estão rapidamente ganhando popularidade em sistemas de PNL para idiomas diferentes do inglês. A maioria desses modelos apresenta uma importante etapa de amostragem de corpus no processo de acumulação de dados de treinamento em diferentes idiomas, para garantir que o sinal de idiomas com melhores recursos não sufoque os com poucos recursos. Neste estudo, treinamos vários modelos de idiomas recorrentes multilíngues, com base na arquitetura ELMo, e analisamos o efeito de proporções de tamanho de corpus variáveis no desempenho downstream, bem como a diferença de desempenho entre modelos monolíngues para cada idioma e modelos de idiomas multilíngues mais amplos . Como parte desse esforço, também disponibilizamos esses modelos treinados para uso público.', 'fr': "Les modèles linguistiques préformés multilingues gagnent rapidement en popularité dans les systèmes de PNL pour les langues autres que l'anglais. La plupart de ces modèles comportent une étape importante d'échantillonnage de corpus dans le processus d'accumulation de données d'apprentissage dans différentes langues, afin de s'assurer que le signal provenant de langues mieux dotées n'étouffe pas celles qui disposent de ressources insuffisantes. Dans cette étude, nous formons plusieurs modèles linguistiques récurrents multilingues, basés sur l'architecture ElMo, et analysons à la fois l'effet des différents ratios de taille de corpus sur les performances en aval, ainsi que la différence de performance entre les modèles monolingues pour chaque langue, et les modèles linguistiques multilingues plus larges . Dans le cadre de cet effort, nous mettons également ces modèles formés à la disposition du public.", 'ja': '多言語の事前訓練された言語モデルは、非英語のためのNLPシステムで急速に人気を博しています。これらのモデルのほとんどは、よりリソースの乏しい言語からの信号がリソースの乏しい言語に浸されないようにするために、異なる言語でトレーニングデータを蓄積するプロセスにおける重要なコーパスサンプリングステップを特徴としています。この研究では、ELMoアーキテクチャに基づいて、複数の多言語再帰言語モデルをトレーニングし、下流のパフォーマンスに対するさまざまなコーパスサイズの比率の効果、および各言語の単語モデルとより広範な多言語モデルとの間のパフォーマンスの違いの両方を分析します。この取り組みの一環として、私たちはこれらの訓練されたモデルを公衆が利用できるようにします。', 'zh': '多言预训语言模型在非英语语言者NLP统中速普。 大抵积言练数,有大语料库采样步骤,以保其善言不没其乏也。 于此论之,吾于ELMo架构多习语言循环语言模样,并分别了不同语料库大小比率对下流性能的影响,及每种语言的单语模形和更广的多言语模形的性能差异。 以此为事者,吾以此给公众。', 'hi': 'बहुभाषी पूर्व-प्रशिक्षित भाषा मॉडल गैर-अंग्रेजी भाषाओं के लिए एनएलपी प्रणालियों में तेजी से लोकप्रियता हासिल कर रहे हैं। इनमें से अधिकांश मॉडल विभिन्न भाषाओं में प्रशिक्षण डेटा जमा करने की प्रक्रिया में एक महत्वपूर्ण कॉर्पस नमूना चरण की सुविधा देते हैं, यह सुनिश्चित करने के लिए कि बेहतर संसाधन वाली भाषाओं से सिग्नल खराब संसाधनों वाले लोगों को डूब न जाए। इस अध्ययन में, हम ईएलएमओ आर्किटेक्चर के आधार पर कई बहुभाषी आवर्तक भाषा मॉडल को प्रशिक्षित करते हैं, और डाउनस्ट्रीम प्रदर्शन पर अलग-अलग कॉर्पस आकार अनुपात के प्रभाव, साथ ही साथ प्रत्येक भाषा के लिए मोनोलिंगुअल मॉडल और व्यापक बहुभाषी भाषा मॉडल के बीच प्रदर्शन अंतर दोनों का विश्लेषण करते हैं। इस प्रयास के हिस्से के रूप में, हम इन प्रशिक्षित मॉडलों को सार्वजनिक उपयोग के लिए भी उपलब्ध कराते हैं।', 'ru': 'Многоязычные предварительно обученные языковые модели быстро набирают популярность в системах NLP для языков, отличных от английского. Большинство из этих моделей представляют собой важный этап отбора проб в процессе накопления учебных данных на различных языках, с тем чтобы сигнал от языков с лучшими ресурсами не заглушал те языки, которые плохо обеспечены ресурсами. В этом исследовании мы обучаем несколько многоязычных рекуррентных языковых моделей, основанных на архитектуре ELMo, и анализируем как влияние различных соотношений размеров корпуса на производительность нижележащего потока, так и разницу в производительности между одноязычными моделями для каждого языка и более широкими многоязычными языковыми моделями. В рамках этих усилий мы также делаем эти обученные модели доступными для публичного использования.', 'ga': 'Tá múnlaí teanga ilteangacha réamhoilte ag dul i méid go tapa i gcórais NLP do theangacha nach Béarla iad. Gné thábhachtach de shampláil corpais atá sa chuid is mó de na samhlacha seo sa phróiseas le sonraí oiliúna a bhailiú i dteangacha éagsúla, lena chinntiú nach báthaíonn an comhartha ó theangacha a bhfuil acmhainní níos fearr acu cinn a bhfuil acmhainní laga acu. Sa staidéar seo, cuirimid oiliúint ar ilmhúnlaí teanga athfhillteacha ilteangacha, bunaithe ar an ailtireacht ELMo, agus déanaimid anailís ar an éifeacht atá ag cóimheasa éagsúla méideanna corpais ar fheidhmíocht iartheachtacha, chomh maith leis an difríocht feidhmíochta idir samhlacha aonteangacha do gach teanga, agus samhlacha teanga ilteangacha níos leithne. . Mar chuid den iarracht seo, cuirimid na múnlaí oilte seo ar fáil don phobal freisin.', 'ka': 'Multilingual pretrained language models are rapidly gaining popularity in NLP systems for non-English languages. ამ მოდელების უფრო მეტი შესაძლებელია მნიშვნელოვანი კორპოსს გამოყენების ნაწილის მონაცემები განსხვავებული ენების პროცესში, რომ დარწმუნოთ, რომ სიგნალე უფრო მეტი რესურსურსურსურსურს ამ კვლევაში, ჩვენ მრავალენგური რეკურენტი ენის მოდელების მოწყობილობა, ELMo არქტიქტურაზე დაბათებული, და ანალიზაცით ორივე კორპუსს ზომის განსხვავებული პარაციების გამოსახულებაზე, და მონოლენგური მოდელების განსხვავება ყოველ ენ როგორც პროცესის ნაწილი, ჩვენ ასევე ამ მოდელების შესაძლებლობად გავაკეთებთ.', 'hu': 'A többnyelvű előképzett nyelvi modellek gyorsan népszerűsödnek a nem angol nyelvű NLP rendszerekben. E modellek többsége fontos korpuszmintázási lépést jelent a különböző nyelveken történő képzési adatok gyűjtésének folyamatában annak biztosítása érdekében, hogy a jobb erőforrásokkal rendelkező nyelvek jelei ne fojtsák ki a rossz erőforrásokkal rendelkezőket. Ebben a tanulmányban több, többnyelvű visszatérő nyelvi modellt készítünk az ELMo architektúra alapján, és elemezzük mind a különböző korpuszméret arányok hatását a downstream teljesítményre, mind az egyes nyelvek egynyelvű modelljei közötti teljesítménykülönbséget, mind pedig a szélesebb körű többnyelvű nyelvi modelleket. Ennek részeként ezeket a képzett modelleket nyilvános használatra is elérhetővé tesszük.', 'el': 'Πολυγλωσσικά προσχεδιασμένα γλωσσικά μοντέλα κερδίζουν γρήγορα δημοτικότητα στα συστήματα για μη-αγγλικές γλώσσες. Τα περισσότερα από αυτά τα μοντέλα διαθέτουν ένα σημαντικό βήμα δειγματοληψίας σωμάτων στη διαδικασία συσσώρευσης δεδομένων κατάρτισης σε διαφορετικές γλώσσες, ώστε να διασφαλιστεί ότι το σήμα από γλώσσες με καλύτερες πηγές δεν θα πνίξει τις γλώσσες με φτωχούς πόρους. Στην παρούσα μελέτη, εκπαιδεύουμε πολλαπλά πολύγλωσσα μοντέλα επαναλαμβανόμενης γλώσσας, βασισμένα στην αρχιτεκτονική του και αναλύουμε τόσο την επίδραση των ποικίλων αναλόγων μεγέθους σώματος στις μεταγενέστερες επιδόσεις, όσο και τη διαφορά απόδοσης μεταξύ μονογλωσσών μοντέλων για κάθε γλώσσα και ευρύτερων πολυγλωσσικών μοντέλων. Στο πλαίσιο αυτής της προσπάθειας, θέτουμε επίσης αυτά τα εκπαιδευμένα μοντέλα διαθέσιμα για δημόσια χρήση.', 'it': "I modelli linguistici multilingue pre-addestrati stanno rapidamente guadagnando popolarità nei sistemi NLP per le lingue non inglesi. La maggior parte di questi modelli presenta un'importante fase di campionamento del corpo nel processo di accumulazione dei dati di formazione in lingue diverse, per garantire che il segnale proveniente da lingue più dotate di risorse migliori non affoghi quelle con risorse scarse. In questo studio, formiamo più modelli multilingue ricorrenti, basati sull'architettura ELMo, e analizziamo sia l'effetto dei diversi rapporti di dimensione del corpo sulle prestazioni a valle, sia la differenza di prestazioni tra modelli monolingue per ogni lingua, sia modelli multilingui più ampi. Come parte di questo sforzo, rendiamo disponibili anche questi modelli addestrati per uso pubblico.", 'kk': 'Көптеген тілдер үлгілері NLP тілдерінде ағылшын тілдері емес тілдерінің мәліметті жетілдіреді. Бұл үлгілердің көпшілігі басқа тілдерде оқыту деректерін біріктіру процесінде маңызды корпус баптау қадамы болады. Бұл үлгілердің көпшілігі жақсы ресурстар тілдерінің сигналының көпшілігін қа Бұл зерттеулерде, ELMo архитектурасына негізделген бірнеше тілді қайталану үлгілерін үйренеміз және қайталану үлгілерінің көптеген корпус өлшемінің көптеген нәтижесін төмендету әрекетінде анализ, әрбір тіл үлгілерінің монолингілі модел Бұл әрекеттердің бір бөлігі ретінде бұл оқылған үлгілерді қолдану үшін қол жеткіземіз.', 'lt': 'Daugiakalbiai ikimokomi kalbų modeliai sparčiai didina populiarumą NLP sistemose ne anglų kalboms. Daugumoje šių modelių imamas svarbus mėginių ėmimo etapas rengiant mokymo duomenis skirtingomis kalbomis, siekiant užtikrinti, kad signalas iš geresnių išteklių turinčių kalbų nebūtų nuskendęs iš prastai išteklių turinčių kalbų. Šiame tyrime rengiame kelis daugiakalbius pasikartojančius kalbų modelius, grindžiamus ELMo architektūra, ir analizuojame įvairių korpuso dydžio santykių poveikį tolesniam veiklos rezultatui, taip pat kiekvienos kalbos vienkalbių modelių ir platesnių daugiakalbių modelių veiklos skirtumą. Šiomis pastangomis mes taip pat viešai naudojame šiuos apmokytus modelius.', 'mk': 'Мултијазичните предобучени јазички модели брзо добиваат популарност во системите на НЛП за неанглиски јазици. Повеќето од овие модели претставуваат важен чекор на примерок на корпус во процесот на акумулација на податоци за обука на различни јазици, со цел да се осигури дека сигналот од подобри ресурси јазици не се дави од лошо ресурсирани јазици. Во оваа студија, тренираме повеќе мултијазични рецидентни јазички модели, базирани на архитектурата на ЕЛМО, и го анализираме ефектот на различните проценти на големината на корпусот на понатамошните резултати, како и разликата на резултатите помеѓу монојазичните модели за секој јазик и пошироките Како дел од овие напори, ги поставуваме и овие обучени модели достапни за јавна употреба.', 'ms': 'Model bahasa berbilang yang dilatih dahulu dengan cepat meningkat popularitas dalam sistem NLP untuk bahasa bukan bahasa Inggeris. Kebanyakan model ini mengandungi langkah sampel corpus yang penting dalam proses mengumpulkan data latihan dalam bahasa yang berbeza, untuk memastikan isyarat dari bahasa yang mempunyai sumber sumber yang lebih baik tidak tenggelam keluar yang mempunyai sumber yang tidak baik. In this study, we train multiple multilingual recurrent language models, based on the ELMo architecture, and analyse both the effect of varying corpus size ratios on downstream performance, as well as the performance difference between monolingual models for each language, and broader multilingual language models.  Sebahagian daripada usaha ini, kami juga membuat model terlatih tersedia untuk penggunaan awam.', 'ml': 'ഇംഗ്ലീഷ് അല്ലാത്ത ഭാഷകള്\u200dക്കുള്ള NLP സിസ്റ്റത്തില്\u200d പ്രധാനപ്പെട്ട മോഡലുകള്\u200d പെട്ടെന്ന് വരുന്നു. ഈ മോഡലുകളില്\u200d മിക്കവാറും വ്യത്യസ്ത ഭാഷകളില്\u200d പരിശീല വിവരങ്ങള്\u200d ശേഖരിക്കുന്ന പ്രക്രിയയില്\u200d പ്രധാനപ്പെട്ട ഒരു പ്രധാനപ്പെട്ട കോര്\u200dപ്പുസ്  ഈ പഠനത്തില്\u200d നമ്മള്\u200d ഒരുപാട് പല ഭാഷ മോഡലുകള്\u200d പരിശീലിപ്പിക്കുന്നു, എഎല്\u200dമോ ആര്\u200dക്കിട്ടറിക്ക് അടിസ്ഥാനത്ത്, കോര്\u200dപ്പുസിന്റെ വലിപ്പത്തിന്റെ വ്യത്യസ്ത വിഭവങ്ങള്\u200d താഴെ പ്രകടനത്തിന്റ ഈ പരിശീലനത്തിന്റെ ഭാഗമായി നമ്മള്\u200d ഈ പരിശീലന മോഡലുകള്\u200d പൊതുവില്\u200d ഉപയോഗിക്കാന്\u200d ലഭ്യമാക്കുന്നു.', 'mt': 'Il-mudelli multilingwi tal-lingwi mħarrġa minn qabel qegħdin jiksbu popolarità malajr fis-sistemi NLP għal-lingwi mhux Ingliżi. Il-biċċa l-kbira ta’ dawn il-mudelli għandhom pass importanti ta’ teħid ta’ kampjuni korpus fil-proċess ta’ akkumulazzjoni ta’ dejta ta’ taħriġ f’lingwi differenti, biex jiġi żgurat li s-sinjal minn lingwi b’riżorsi aħjar ma jnaqqafx dawk b’riżorsi baxxi. F’dan l-istudju, aħna nħarrġu diversi mudelli multilingwi rikorrenti tal-lingwa, ibbażati fuq l-arkitettura tal-ELMo, u tanalizzaw kemm l-effett ta’ proporzjonijiet differenti tad-daqs tal-corpus fuq il-prestazzjoni downstream, kif ukoll id-differenza fil-prestazzjoni bejn mudelli monolingwi għal kull lingwa, u mudelli usa’ multilingwi. Bħala parti minn dan l-isforz, nagħmlu dawn il-mudelli mħarrġa disponibbli wkoll għall-użu pubbliku.', 'no': 'Fleirspråksprøvemodular får raskt popularitet i NLP-systemet for ikkje-engelske språk. Dei fleste av desse modelane har ein viktig korpussamling i prosessen for å akumulera opplæringsdata på ulike språk, for å sikre at signalen frå bedre ressurserte språk ikkje døyrer ut slik feil ressurserte. I denne studien treng vi fleire språk gjentakingsmodular, basert på ELMo-arkitekturen, og analyser begge effekten av forskjellige korpusstorleiksforholdet på nedstrekende utviklingar, og forskjellen mellom monospråk-modeller for kvar språk, og breire fleirspråk-modeller. Som del av denne innsatsen gjer vi også desse trengte modelane tilgjengelege for offentlig bruk.', 'pl': 'Wielojęzyczne wstępnie trenowane modele językowe szybko zyskują popularność w systemach NLP dla języków nieangielskich. Większość z tych modeli zawiera ważny etap próbkowania korpusów w procesie gromadzenia danych treningowych w różnych językach, aby zapewnić, że sygnał z lepiej zaopatrzonych języków nie zatopi słabo zasobnych. W niniejszym opracowaniu szkolimy wielojęzyczne modele językowe powtarzające się, oparte na architekturze ELMo, i analizujemy zarówno wpływ różnych współczynników wielkości korpusu na wydajność w dalszym szczeblu, jak i różnicę wydajności pomiędzy modelami jednojęzycznymi dla każdego języka, a szerszymi modelami językowymi wielojęzycznymi. W ramach tego wysiłku udostępniamy również te przeszkolone modele do użytku publicznego.', 'ro': 'Modelele lingvistice pre-instruite multilingve câștigă rapid popularitate în sistemele PNL pentru limbile non-engleze. Majoritatea acestor modele prezintă o etapă importantă de eșantionare a corpurilor în procesul de acumulare a datelor de formare în diferite limbi, pentru a se asigura că semnalul din limbi cu resurse mai bune nu înecă cele cu resurse slabe. În acest studiu, instruim mai multe modele lingvistice recurente multilingve, bazate pe arhitectura ELMo, și analizăm atât efectul diferitelor rapoarte de dimensiune a corpului asupra performanței din aval, cât și diferența de performanță dintre modelele monolingve pentru fiecare limbă, cât și modelele lingvistice multilingve mai largi. Ca parte a acestui efort, punem, de asemenea, aceste modele instruite disponibile pentru uz public.', 'sr': 'Mnogi jezički modeli preliječenih jezika brzo dobijaju popularnost u NLP sistemima za ne-engleski jezici. Većina ovih modela ukazuje na važan korak uzoraka korpusa u procesu okupljanja podataka obuke na različitim jezicima, kako bi se osigurala da se signal iz boljih izvornih jezika ne udavi loše resurse. U ovoj studiji treniramo višestruke jezičke rekonstruirane modele, bazirane na arhitekturi ELMo-a, i analiziramo oba učinka različitih koeficijenata veličine korpusa na pokretnoj funkciji, kao i razliku učinka između monojezičkih modela za svaki jezik i šire multijezičke modele. Kao deo ovog napora, takođe ćemo i ovi obučeni modeli biti dostupni za javnu korist.', 'so': 'Tusaalooyinka luuqadaha afka kala duwan ee lagu soo hor jeeday waxay si dhaqso ah ugu kordhaan nidaamka afka Ingiriiska ee NLP. Tusaaladan intooda badan waxay leeyihiin tusaale ahaan muhiim ah marka lagu soo ururiyo macluumaadka waxbarashada luuqadaha kala duduwan, si uu u xaqiijiyo in sawirka laga soo jeedo luqadaha aad ku habboon tahay uusan u maansheynin midiidinnada caadi ah. Waxbarashadan ayaannu ku tababarinnaa tusaalooyin luuqado ah oo kala duduwan, taasoo ku saleysan dhismaha ELMo, waana baaritaannaa saamaynta farsamada tirada kooxaha ah oo ku saabsan tababarka hoose-socodka, iyo kala duwanaashada muuqashada muuqashada af kasta oo noocyo luqad ah, iyo samooyin luuqad kala duduwan oo kala duduwan. Qayb ka mid ah hawlahan, waxaynu sameynaa qaababkan la tababaray oo lagu isticmaalo guud.', 'si': 'ගොඩක් භාෂාවක් ප්\u200dරීට්\u200dරීන් භාෂාවක් මොඩේල්ස් වල NLP පද්ධතියේ ඉංග්\u200dරීසි භාෂාවක් නැති විදි මේ මොඩල් වලින් ගොඩක් ප්\u200dරශ්ණයක් විවිධ භාෂාවට ප්\u200dරශ්ණ දත්ත සම්පූර්ණ කරනවා විවිධිය භාෂාවල් වල සංඥාවක් හොඳ සම්ප මේ පරීක්ෂණයේදී, අපි ගොඩක් භාෂාවක් ප්\u200dරතික්\u200dරියාත්මක භාෂාවක් මොඩේල්ස්, ELMo විද්\u200dයාත්මක විද්\u200dයාපිත විශේෂ කරනවා, සහ හ හැම භාෂාවක් විද්\u200dයාත්මක ව මේ උත්සහේ කොටසක් විදිහට, අපි මේ ප්\u200dරශ්නයක් ප්\u200dරයෝජනයට පාවිච්චි කරන්න පුළුවන් වෙනවා.', 'sv': 'Flerspråkiga förkränade språkmodeller ökar snabbt popularitet i NLP-system för icke-engelska språk. De flesta av dessa modeller innehåller ett viktigt provtagningssteg i processen med att samla träningsdata på olika språk, för att säkerställa att signalen från bättre resursbundna språk inte dränker dem som har dåligt resurs. I denna studie tränar vi flera flerspråkiga återkommande språkmodeller, baserade på ELMo-arkitekturen, och analyserar både effekten av olika korpustorleksförhållanden på nedströms prestanda, liksom prestandafördelningen mellan enspråkiga modeller för varje språk, och bredare flerspråkiga språkmodeller. Som en del av detta arbete gör vi också dessa utbildade modeller tillgängliga för allmänheten.', 'ta': 'மொழி மொழி மாதிரி மாதிரிகள் NLP முறைமைகளில் மக்களை விரைவாக அதிகரிக்கும். இந்த மாதிரிகளில் பெரும்பாலான ஒரு முக்கியமான கோப்ஸ் மாதிரி படியை வேறு மொழிகளில் பயிற்சி தரவை சேகரிக்கும் போது, சிறந்த மூலங்களிலிருந்து  இந்த ஆராய்ச்சியில், நாம் ELMo உருவாக்கத்தை அடிப்படையில் பல மொழி மொழி மாதிரி மாதிரிகளை பயிற்சி செய்து, மற்றும் விளைவின் வித்தியாசம் கார்ப்ஸ் அளவு விகிதத்தை தாழ்வின் மேலும் ஒவ இந்த முயற்சியின் பகுதியாக, இந்த பயிற்சி மாதிரிகளை பொது பயன்பாட்டிற்கு கிடைக்கும்.', 'ur': 'بہت سی زبان کی پرٹرین زبان کی موڈل NLP سیستموں میں غیر انگلیسی زبانوں کے لئے مثبت حاصل کر رہے ہیں۔ ان کی اکثریت مدلکوں میں سے ایک اہم کورپوس نمونٹ پلیٹ پڑھنے کی پروسس میں مختلف زبانوں میں آموزش دادہ جمع کرنے کے لئے، اس لئے مطمئن ہونا کہ بہترین رسسورٹ زبانوں سے سیگنالک برابر رسسورٹ نہیں کر سکتا۔ اس تحقیقات میں ہم بہت سی زبان کی دوبارہ تکرار کی زبان مدل کی تعلیم دیتے ہیں، ELMo معماری پر بنیاد رکھتے ہیں، اور ان دونوں کی مختلف کورپوس کی اندازہ نسبتوں کا تفسیر ڈال دیتے ہیں، اور ہر زبان کے ایک زبان کی مدل کے درمیان ایک زبان کے متفاوت تفاوت، اور بہت زیادہ زبان کی مدل کے اس تلاش کی ایک حصہ کے طور پر، ہم نے ان ترسیم نمونڈوں کو عمومی استعمال کے لئے بھی موجود بنایا ہے.', 'mn': 'Ихэнх хэлний хувьд хэл загварууд нь англи хэл биш болон NLP системд нэр хүндтэй болж байна. Эдгээр загваруудын ихэнх нь өөр хэл дээр сургалтын өгөгдлийг цуглуулахын тулд чухал корпус зураг цуглуулах алхам юм. Энэ судалгаанд бид ELMo архитектурын үндсэн олон хэлний дахин дахин хэл загварын загварыг суралцаж, багасгах үйл ажиллагаанд корпус хэмжээний ялгаатай харьцааны нөлөөг, хэл бүрийн нэг хэл загварын ялгааг, олон хэл загварын загварын ялгааг шинжилдэг. Энэ хичээлийн нэг хэсэг болгон бид эдгээр сургалтын загваруудыг олон нийтийн хэрэглээ ашиглах боломжтой болгодог.', 'vi': 'Các mô hình ngôn ngữ đa ngôn ngữ trước đã nhanh chóng trở nên phổ biến trong hệ thống ngôn ngữ không-Anh. Hầu hết các mô hình này đều có một bước tiến quan trọng trong quá trình tích tụ dữ liệu đào tạo ở các ngôn ngữ khác nhau, để đảm bảo tín hiệu từ những ngôn ngữ có nguồn lực tốt hơn không bị thiếu nguồn lực. Trong nghiên cứu này, chúng tôi đào tạo nhiều mô- đun ngôn ngữ đệ nhất, dựa trên kiến trúc ElMo, và phân tích cả ảnh hưởng của tỉ lệ quy mô hình thể khác nhau trên chiều theo dòng, cũng như sự khác biệt hiệu suất giữa các mô- ngôn ngữ ngữ cho mỗi ngôn ngữ, và các mô hình ngôn ngữ đa dạng rộng hơn. Trong nỗ lực này, chúng tôi cũng đưa những mô hình được đào tạo này ra cho công chúng.', 'uz': "Bir necha tillar o'zgarilgan tillar modellari ingliz tillar uchun NLP tizimlarida juda tez yaxshi ko'paytadi. Ushbu modellarning ko'pchiligi boshqa tillarda ta'lim maʼlumotni birlashtirish jarayonida muhim kompyuterga ega bo'ladi, va yaxshi murakkab boʻlgan tillardan signalni o'rganish uchun juda ko'p murakkab narsalarni o'chirib boʻlmaydi. Bu taʼminotda, biz ELMo arkitekturi asosida bir nechta tildagi takrorlanadigan tilning modellarini o'rganamiz, va ko'pchilik oʻlchami rasmlarining natijasini o'zgartirib o'rganamiz, va har bir tillar uchun monolingual modellar orasidagi diqqat o'zgarishni va ko'plab tillar modellarini o'zgartirish. Bu jarayonning bir qismi sifatida, biz bu ta'lim modellarni public foydalanish uchun imkoniyat qilamiz.", 'bg': 'Многоезичните предварително обучени езикови модели бързо набират популярност в системите за НЛП за чужди езици. Повечето от тези модели включват важна стъпка за вземане на проби от корпуси в процеса на натрупване на данни за обучение на различни езици, за да се гарантира, че сигналът от по-добре ресурсирани езици не заглушава слабо ресурсираните. В това проучване ние обучаваме множество многоезични повтарящи се езикови модели, базирани на архитектурата и анализираме както ефекта от вариращите съотношения на размера на корпуса върху ефективността надолу по веригата, така и разликата в ефективността между едноезичните модели за всеки език, и по-широките многоезични езикови модели. Като част от това усилие ние също така правим тези обучени модели достъпни за обществено ползване.', 'nl': 'Meertalige voorgetrainde taalmodellen winnen snel aan populariteit in NLP-systemen voor niet-Engelse talen. De meeste van deze modellen hebben een belangrijke corpussampling stap in het proces van het verzamelen van trainingsgegevens in verschillende talen, om ervoor te zorgen dat het signaal van beter uitgeruste talen niet overspoelt de slecht uitgeruste talen. In deze studie trainen we meerdere meertalige terugkerende taalmodellen, gebaseerd op de ELMo architectuur, en analyseren we zowel het effect van verschillende corpusgrootteverhoudingen op downstreamprestaties, als het prestatieverschil tussen eentalige modellen voor elke taal, en bredere meertalige taalmodellen. In dit kader stellen we deze getrainde modellen ook beschikbaar voor publiek gebruik.', 'da': 'Flersprogede prætrænede sprogmodeller vinder hurtigt popularitet i NLP-systemer til ikke-engelske sprog. De fleste af disse modeller indeholder et vigtigt skridt i forbindelse med indsamling af træningsdata på forskellige sprog for at sikre, at signalet fra sprog med bedre ressourcer ikke drukner dem med dårlig ressource. I denne undersøgelse træner vi flere flersprogede tilbagevendende sprogmodeller, baseret på ELMo-arkitekturen, og analyserer både effekten af varierende korpustørrelsesforhold på downstream performance, såvel som performance forskellen mellem ensprogede modeller for hvert sprog, og bredere flersprogede sprogmodeller. Som en del af denne indsats stiller vi også disse uddannede modeller til rådighed for offentlig brug.', 'hr': 'Mnogi jezički modeli preliječenih jezika brzo dobijaju popularnost u NLP sustavima za non-engleski jezici. Većina ovih modela ukazuje na važan korak uzoraka korpusa u procesu okupljanja podataka obuke na različitim jezicima kako bi se osigurala da se signal iz boljih izvornih jezika ne utopi loše izvornih jezika. U ovom ispitivanju treniramo višestruke višejezičke rekonstruirane jezičke modele na temelju arhitekture ELMo-a i analiziramo učinak različitih koeficijenata veličine korpusa na pokretnoj funkciji, kao i razliku učinka između monojezičkih modela za svaki jezik i šire višejezičke modele. Kao dio tih napora, također činimo te obučene modele dostupnim za javnu korist.', 'de': 'Mehrsprachige vortrainierte Sprachmodelle gewinnen in NLP-Systemen für nicht-englische Sprachen schnell an Popularität. Die meisten dieser Modelle verfügen über einen wichtigen Korpus-Sampling-Schritt bei der Ansammlung von Trainingsdaten in verschiedenen Sprachen, um sicherzustellen, dass das Signal aus besser ausgestatteten Sprachen nicht die schlecht ausgestatteten Sprachen übertönt. In dieser Studie trainieren wir mehrere mehrsprachige wiederkehrende Sprachmodelle, basierend auf der ELMo-Architektur, und analysieren sowohl den Einfluss unterschiedlicher Korpusgrößenverhältnisse auf die nachgelagerte Leistung als auch den Leistungsunterschied zwischen einsprachigen Modellen für jede Sprache und breiteren mehrsprachigen Sprachmodellen. Im Rahmen dieser Bemühungen stellen wir diese trainierten Modelle auch der Öffentlichkeit zur Verfügung.', 'ko': '다중 언어 예비 훈련 언어 모델은 비영어 NLP 시스템에서 신속하게 유행한다.이들 모델 중 대부분은 중요한 자료 라이브러리 샘플링 절차를 가지고 서로 다른 언어의 훈련 데이터를 축적하는 과정에서 자원이 좋은 언어로부터의 신호가 자원이 나쁜 언어를 침몰시키지 않도록 확보한다.이 연구에서 우리는 ELMo 체계 구조를 바탕으로 여러 개의 다중 언어 순환 언어 모델을 훈련시키고 서로 다른 언어 자료 라이브러리의 크기 비율이 하위 성능에 미치는 영향, 그리고 각 언어의 단일 언어 모델과 더욱 광범위한 다중 언어 모델 간의 성능 차이를 분석했다.이 작업의 일부로서, 우리는 교육을 받은 이 모델들을 대중에게 사용하도록 제공할 것이다.', 'sw': 'Mradi wa lugha mbalimbali wa lugha unaongezeka kwa haraka kupata umaarufu katika mfumo wa NLP kwa lugha isiyo ya Kiingereza. Mifano mingi hii ina hatua muhimu ya sampuli katika mchakato wa kukusanya taarifa za mafunzo kwa lugha tofauti, ili kuhakikisha kuwa ishara kutoka lugha nzuri zinazorasiliwa hazitazami rasilimali mbaya. Katika utafiti huu, tunafundisha mifano mingi ya lugha inayoendelea kwa lugha mbalimbali, kwa kutumia muundo wa ujenzi wa ELMo, na uchambuzi wa matokeo mbalimbali ya kiwango cha viwango vya makampuni kwenye utendaji wa mito ya chini, pamoja na tofauti ya utendaji kati ya mifano ya lugha za kimonolinguli kwa kila lugha, na mitindo ya lugha mbalimbali zaidi. Kama sehemu ya juhudi hizi, pia tunatengeneza mifano hii ya mafunzo yanayopatikana kwa matumizi ya umma.', 'tr': 'Çoklu diller öňünden gelen dil nusgalary NLP sistemlerinde iňlisçe ýok diller üçin täzelikde welinýärler. Bu nusgalaryň köp bölegi başga dillerde okuw maglumaty toplamak prosesinde örän möhüm bir korpus örnekleri çykarmak üçin, gowy bilim sisteminden sözleri iň gowy görnüş ýüze çykarmaýar. Bu okuwçyda, biz birnäçe dilli tekrarly dil nusgalaryny öwredýäris, ELMo arhitekturyna daýanýan, we köpüs ululyk nusgalarynyň täsirini aşak täsirinde çykyp bilýäris, we her dil üçin mono dil nusgalarynyň täsirini we uly dil nusgalarynyň täsirini çykarýarys. Bu kynçylygyň bir bölegi bolsa, biz hem bu bilim sistemasy nusgalary halk ulanmak üçin ulaşaýarys.', 'fa': 'مدلهای زبانی زیادی پیش\u200cفرض زبان به سرعت در سیستم\u200cهای NLP برای زبانهای غیر انگلیسی پیدا می\u200cکنند. بیشتر از این مدلها یک قدم نمونه\u200cهای مهم در فرایند آموزش داده\u200cهای آموزش در زبان\u200cهای مختلف را مشخص می\u200cکنند تا مطمئن شود که سیگنال از زبان\u200cهای بهترین استفاده از زبان\u200cهای استفاده\u200cشده بد\u200cترین منابع را غرق نمی\u200cکند. در این مطالعه، ما مدل های متعدد زبان تکرار را آموزش می کنیم، بر اساس معماری ELMo، و تاثیر اندازه های متفاوت کورپوس را بر عملکرد پایین\u200cترین، و تفاوت عملکرد بین مدل\u200cهای متعدد زبان برای هر زبان، و مدل\u200cهای متعدد زبان بیشتری را تحلیل می\u200cکنیم. به عنوان بخشی از این تلاش، ما این مدل های آموزش را برای استفاده عمومی در دسترس می دهیم.', 'af': "Veelvuldige praatspraak-modelles kry vinnig populariteit in NLP-stelsels vir nie-Engels tale. Die meeste van hierdie modele funksie 'n belangrike korpus versameling stap in die proses van opvoering data in verskillende tale, om te verseker dat die sein van beter hulpbron tale nie slegs hulpbron uitdroog nie. In hierdie studie, ons tref veelvuldige multilinglike herhaalde taal modele, gebaseer op die ELMo-arkitektuur, en analiseer beide die effek van verskillende korpusgrootte verhouding op onderstreem prestasie, asook die prestasie verskil tussen monolinglike modele vir elke taal, en breideer multilinglike taal modele. As deel van hierdie versoek, maak ons ook hierdie opgelei modele beskikbaar vir publieke gebruik.", 'sq': 'Modelet e gjuhës me shumëgjuhë të mësuara përpara janë duke fituar shpejt popullaritetin në sistemet NLP për gjuhët jo-angleze. Shumica e këtyre modeleve paraqesin një hap të rëndësishëm mosmarrëveshjeje të trupit në procesin e akumulimit të të dhënave të trainimit në gjuhë të ndryshme, për të siguruar se sinjali nga gjuhët më të burimeve nuk mbytet nga ato me burime të keqe. Në këtë studim, ne trajnojmë shumë modele gjuhësh të përsëritura shumëgjuhësore, bazuar në arkitekturën e ELMo, dhe analizojmë si efektin e proportave të ndryshme të madhësisë së korpusit në performancën poshtë, si dhe dallimin e performancës midis modeleve monogjuhësore për çdo gjuhë dhe modeleve më të gjera shumëgjuhësore të gjuhës. Si pjesë e këtij përpjekjeje, ne gjithashtu i bëjmë këto modele të stërvitur në dispozicion për përdorim publik.', 'hy': 'Multilingual pretrained language models are rapidly gaining popularity in NLP systems for non-English languages.  Այս մոդելների մեծ մասը ներկայացնում է մի կարևոր մարմնի նմուշներ վերցնելու քայլ տարբեր լեզուներով սովորեցման տվյալների հավաքելու գործընթացում, որպեսզի ապահովենք, որ ավելի լավ ռեսուրսների լեզուներից ստացված ազդանշանը վատ ռեսուրսներ չջնջ Այս ուսումնասիրության ընթացքում մենք ուսումնասիրում ենք բազմալեզու կրկնվող լեզվի մոդելներ, հիմնված ELMo ճարտարապետության վրա, և վերլուծում ենք մարմնի տարբեր չափերի հարաբերությունների ազդեցությունը հաջորդ շարժման արտադրողության վրա, ինչպես նաև յուրաքանչյուր լեզվի միալեզվի մոդելների և ավելի լա Այս ջանքերի մի մասը մենք նաև հանրային օգտագործման համար հասանելի ենք դարձնում այս վարժեցված մոդելները:', 'id': 'Model bahasa berbagai bahasa yang dilatih sebelumnya dengan cepat meningkat popularitas dalam sistem NLP untuk bahasa bukan bahasa Inggris. Kebanyakan dari model ini memiliki langkah sampel corpus yang penting dalam proses mengumpulkan data pelatihan dalam bahasa yang berbeda, untuk memastikan bahwa sinyal dari bahasa yang memiliki sumber daya yang lebih baik tidak menenggelamkan sumber daya yang buruk. Dalam studi ini, kami melatih banyak model bahasa berbagai bahasa yang berkurang, berdasarkan arsitektur ELMo, dan menganalisis efek dari ukuran corpus yang berbeda pada prestasi turun, serta perbedaan prestasi antara model monobahasa untuk setiap bahasa, dan model bahasa berbagai bahasa yang lebih luas. Sebagai bagian dari usaha ini, kami juga membuat model terlatih tersedia untuk penggunaan publik.', 'az': 'İngilizci dili olmayan NLP sistemlərində çoxlu dil öyrənmiş dil modelləri tez gəlir. Bu modellərin əksəriyyəti müxtəlif dillərdə təhsil məlumatlarını toplamaq üçün möhüm bir korpus nümunələri təhsil etmək üçün, daha yaxşı qüvvətli dillərdən sinyal pis qüvvətli dillərdən istifadə etməyə çalışır. Bu təhsil içində, ELMo arhitektüsünə dayanan çoxlu dil tekrarlı modellərini təhsil edirik və hər dilin monodil modellərinin və çoxlu dil modellərinin müxtəlif ölçülərinin təsirini analiz edirik. Bu çabaların bir parçası olaraq, bu təhsil modelləri də halkı istifadə etmək üçün faydalandırırıq.', 'bn': 'Multilingual pretrained language models are rapidly gaining popularity in NLP systems for non-English languages.  এই মডেলগুলোর বেশীরভাগ গুরুত্বপূর্ণ কোর্পাসের নমুনা প্রক্রিয়া বিভিন্ন ভাষায় প্রশিক্ষণের তথ্য সংগ্রহ করার প্রক্রিয়ায় একটি গুরুত্বপূর্ণ এই গবেষণায় আমরা ইএলমো সংস্কৃতি ভিত্তিতে বেশ কয়েকটি মাল্টিভাষার প্রশিক্ষণ প্রশিক্ষণ প্রদান করি এবং নীচের প্রদর্শনের উপর বিভিন্ন কোর্পাসের আকারের প্রভাব ব বিশ্লেষণ করি, এবং প্রতি ভাষার জন্য মো এই প্রচেষ্টার অংশ হিসেবে আমরা এই প্রশিক্ষিত মডেলগুলো জনগণের ব্যবহারের জন্য পাওয়া যাচ্ছি।', 'cs': 'Vícejazyčné předtrénované jazykové modely rychle získávají popularitu v NLP systémech pro neanglické jazyky. Většina těchto modelů představuje důležitý krok vzorkování korpusů v procesu shromažďování tréninkových dat v různých jazycích, aby se zajistilo, že signál z lepších zdrojů jazyků neutopí špatně zdrojované jazyky. V této studii trénujeme vícejazyčné recidivující jazykové modely založené na architektuře ELMo a analyzujeme jak vliv různých poměrů velikosti korpusu na následný výkon, tak i výkonnostní rozdíl mezi jednojjazyčnými modely pro každý jazyk a širšími vícejazyčnými modely jazyka. V rámci tohoto úsilí zpřístupníme také tyto školené modely pro veřejné použití.', 'bs': 'Mnogi jezički modeli preliječenih jezika brzo dobijaju popularnost u NLP-ovim sistemima za ne-engleski jezici. Većina ovih modela ukazuje na važan korpus uzoraka u procesu akumulacije podataka obuke na različitim jezicima, kako bi se osiguralo da se signal iz boljeg izvođenog jezika ne utopi loše resurse. U ovom studiju treniramo višejezičke rekonstruirane jezičke modele, bazirane na arhitekturi ELMo-a, i analiziramo i učinak različitih koeficijenata veličine korpusa na pokretnoj funkciji, kao i razliku učinka između monojezičkih modela za svaki jezik, i šire multijezičke modele. Kao dio ovog napora, također činimo ove obučene modele dostupnim za javnu korist.', 'ca': "Els models multilingües de llenguatges pré-entrenats estan guanyant ràpidament popularitat en els sistemes NLP per a llenguatges no angleses. La majoria d'aquests models representa un pas important de recolliment de mostres en el procés d'acumulació de dades d'entrenament en diferents llengües, per assegurar que la senyal de llengües amb millors recursos no s'ofegui a les que no tenen gaire recursos. En aquest estudi, entrenem múltiples models de llenguatge recurrents multilingües, basats en l'arquitectura ELMo, i analitzem tant l'efecte de variacions en les proporcions de mida del corpus en el rendiment avall, com la diferència de rendiment entre models monolingües per cada llenguatge, com models de llenguatge més amplis. Com part d'aquest esforç, també fem disponibles aquests models entrenats per utilitzar al públic.", 'fi': 'Monikieliset esikoulutetut kielimallit ovat nopeasti saamassa suosiota muiden kuin englannin kielten NLP-järjestelmissä. Useimmissa malleissa on tärkeä korpusnäytteenottovaihe eri kielillä tapahtuvan koulutustiedon keräämisessä, jotta paremmin resurssoitujen kielten signaali ei hukkaisi heikosti resurssoituja kieliä. Tässä tutkimuksessa koulutamme useita monikielisiä toistuvia kielimalleja, jotka perustuvat ELMo-arkkitehtuuriin, ja analysoimme sekä korpusen koon vaihtelun vaikutusta loppupään suorituskykyyn että yksittäisten kielten monikielisten mallien ja laajemmien monikielisten kielimallien suorituskykyeroa. Osana tätä työtä tarjoamme myös nämä koulutetut mallit julkisesti käyttöön.', 'am': 'የቋንቋ ቋንቋዎች ምሳሌዎች በNLP ስርዓቶች ውስጥ የግልፅ ቋንቋዎች ሳይሆን ፈጥኖ አግኝተዋል፡፡ Most of these models feature an important corpus sampling step in the process of accumulating training data in different languages, to ensure that the signal from better resourced languages does not drown out poorly resourced ones.  በዚህ ትምህርት፣ የELMo መሠረታዊ እናሳውቃለን፣ የቆርፓስ መጠቀሚያ ክፍላቸውን በታችኛው ውጤት ላይ እናስተምርማለን፣ እናም በሞሎልቋል ምሳሌዎች መካከል ለቋንቋ እና የበለጠ የቋንቋ ምሳሌዎችን እናሳውቃለን፡፡ እንደዚህ ስርዓት ክፍል፣ እነዚህን ተማሪዎችን የህዝብ ተጠቃሚዎች እናደርጋለን፡፡', 'et': 'Mitmekeelsed eelõpetatud keelemudelid muutuvad kiiresti populaarseks mitte-inglise keelte NLP süsteemides. Enamik neist mudelitest kujutab endast olulist korpuse proovivõtu etappi koolitusandmete kogumisel erinevates keeltes, tagamaks, et paremate ressurssidega keelte signaal ei summutaks välja halvasti ressurssidega keeli. Käesolevas uuringus koolitame ELMo arhitektuuril põhinevaid mitmekeelseid korduvkeelemudeleid ning analüüsime nii korpuse suuruse erinevate suhete mõju järgnevale jõudlusele kui ka erinevust iga keele ühekeelsete mudelite ja laiemate mitmekeelsete keelemudelite vahel. Nende jõupingutuste osana teeme need koolitatud mudelid avalikuks kasutamiseks kättesaadavaks.', 'sk': 'Večjezični predtrenirani jezikovni modeli hitro postajajo priljubljeni v sistemih NLP za ne-angleške jezike. Večina teh modelov predstavlja pomemben korpusni korpus vzorčenja v procesu zbiranja podatkov o usposabljanju v različnih jezikih, da se zagotovi, da signal iz boljših virov jezikov ne izgubi slabo virov. V tej študiji usposabljamo večjezične ponavljajoče se jezikovne modele, ki temeljijo na arhitekturi ELMo, in analiziramo učinek različnih razmerj velikosti korpusa na uspešnost v nadaljnjem delu, kot tudi razliko v uspešnosti med enojezičnimi modeli za vsak jezik in širšimi večjezičnimi jezikovnimi modeli. V okviru tega prizadevanja ti usposobljeni modeli dajemo na voljo tudi za javno uporabo.', 'he': 'דוגמני שפות רבות שפות מתאמנות מראש מקבלים באופן מהיר פופולריות במערכות NLP לשפות לא אנגליות. רוב הדוגמנים האלה מחזיקים צעד דוגמא חשוב של קורפוס בתהליך האספה של נתונים אימונים בשפות שונות, כדי להבטיח שהאות משפות משאבים טובות יותר לא טבע את אלה שפות לא טובות. במחקר הזה, אנו מאמנים דוגמנים רבים של שפות רבות שפות מתחזרות, מבוססים על ארכיטקטורת ELMo, ונבחן את השפעה של יחסי גודל גודל שונים של קורפוס על ההופעה למטה, כמו גם את ההבדל בין דוגמנים מונושפות לכל שפה, ומדוגמנים רבים שפות רחבים יותר. כחלק מהמאמץ הזה, אנחנו גם מקבלים את הדוגמנים המאמנים האלה זמינים לשימוש ציבורי.', 'jv': 'Mulalapun anyar lengkang dipunangguna kuwi nggawe popularan kanggo sistem NLP kanggo nganggo lengkang Inggris. Banyak kudu model iki, akeh langkung wiyane kuwi nggawe gerakan sampeyan sak tentang ing data nggawe ing langkung sampeyan, kanggo ngerayakno sapa sistem sing luwih apik sing luwih apik dhéwé. Nang barêng-barêng iki, kéné mulalah akeh model sing luwih akeh lang sampeyan karo paké, sing basa sak architecture elMo, lan ujileh sistem banjure nggawe sistem sing dipunangé dimulatan karo paké, lan akeh dhéwé kuwi wis dipunangé sampeyan karo paké, sampeyan ngono kuwi nggawe model sing berarti barêng kuwi nggawe lang sampeyan Nambah ning acara iki, kita lagi nggawe model sing tek gawe nggawe para sa jenengan pengguna', 'ha': "Motolin harshe na ɗabi'a masu fara ƙari cikin tsarin NLP don harshen Ingiriya. Babu yawa daga waɗannan misalin suna da wani matsayi mai muhimu wa nau'in misãlai cikin aikin haɗa data na amfani da harshen dabam-daban, dõmin a yi hakar da alama daga lugha masu da amfani da shi ba ya nutsar masu zartar da masu nau'i. A cikin wannan lõkaci, Munã sanar da misãlai masu cikin harshen da aka dace wa multi-lingui, a kan salon da aka sanya shirin ELMo, kuma munã rarraba mai ƙidãya matsayin rabon nau'in da suka sãɓã wa rabon fassarar nau'in da ke ƙarami, da kuma diɓallin muhimmanci a tsakanin misãlai na monoli-harshe wa kõwane harshe, da kuma masu shimfiɗaɗe misãlai na harshen multilala. Kami rabin wannan aikin, za mu sami waɗannan misãlai waɗanda aka yi wa lõkaci na amfani ga mutane.", 'bo': 'སྐད་ཡིག་ཆ་ལའང་ཕན་ངལ་ཆེ་བའི་སྐད་ཡིག Most of these models feature an important corpus sampling step in the process of accumulating training data in different languages, to ensure that the signal from better resourced languages does not drown out poorly resourced ones. In this study, we train multiple lingual recurrent language models, based on the ELMo architecture, and analyse both the effect of varying corpus size ratios on downstream performance, as well as the performance difference between monolingual models for each language, and broader multilingual language models. The སྤྱོད་བརྩོན་བྱས་ཆེན་དེ་ལྟར་ང་ཚོས་རྒྱ་ནག་གི་མིག་དཔེ་གཟུགས་རིས་དེ་ཚོ་སྤྱད་ནས་མང་གི་བེད་སྤྱོད'}
{'en': 'The Danish Gigaword Corpus', 'ar': 'مجموعة Gigaword الدنماركية', 'fr': 'Le corpus danois Gigaword', 'pt': 'O Gigaword Corpus Dinamarquês', 'es': 'El Corpus danés de Gigaword', 'ja': 'デンマークのギガワード・コーパス', 'zh': '丹麦千兆语料库', 'hi': 'डेनिश गीगावर्ड कॉर्पस', 'ru': 'The Danish Gigaword Corpus', 'ga': 'Corpas Gigaword na Danmhairge', 'ka': 'Name', 'el': 'Το Δανικό Σώμα Gigaword', 'hu': 'A dán Gigaword Corpus', 'it': 'Il corpo danese Gigaword', 'lt': 'Danijos Gigaword Corpus', 'mk': 'Данскиот Гигаворд Корпус', 'ml': 'ഡാനിഷ് ഗിഗാവോര്\u200dഡ് കോര്\u200dപ്പുസ്', 'kk': 'Дания Gigaword CorpusName', 'mt': 'Il-Gigaword Corpus Daniż', 'ms': 'The Danish Gigaword Corpus', 'pl': 'Duński Korpus Gigasłów', 'mn': 'Данийн гигавардын корпус', 'no': 'Den danske Gigaword- korpusName', 'ro': 'Corpus danez Gigaword', 'sr': 'Danska Gigaword Corpus', 'si': 'Name', 'ta': 'Name', 'so': 'Korpus', 'sv': 'Den danska Gigaword Corpus', 'ur': 'The Danish Gigaword Corpus', 'uz': 'Name', 'vi': 'Công ty Gigaword Corpus.', 'nl': 'Het Deense Gigaword Corpus', 'bg': 'Датският Гигауорд Корпус', 'da': 'Det danske Gigaword Corpus', 'hr': 'Danska Gigaword Corpus', 'de': 'Das dänische Gigaword Corpus', 'id': 'The Danish Gigaword Corpus', 'ko': '덴마크 Gigaword 자료 라이브러리', 'fa': 'Name', 'sw': 'Kikosi cha Kidenishi cha Gigaword', 'tr': 'Dança Gigaword Korpus', 'af': 'Name', 'sq': 'Korpus i Gigawordit Danez', 'am': '瘠ｴ瘧甫教瘧甫扱 瘡眼結word 瘧ｮ瘉ｭ瘢乍扱', 'bn': 'ড্যানিশ গিগাওয়ার্ড কোর্পাস', 'bs': 'Danska Gigaword Corpus', 'hy': 'Դանիացի Գիգավարդ Կորպուսը', 'ca': 'El Gigaword Corpus danès', 'az': 'Danca Gigaword Corpus', 'et': 'Taani gigasõna korpus', 'cs': 'Dánský Gigaword Corpus', 'fi': 'Tanskan jättisanakorpus', 'jv': 'Kerpus Dino Gegaword', 'sk': 'Danski korpus gigabesed', 'he': 'הקורפוס הגיגאוורד הדני', 'ha': 'KCharselect unicode block name', 'bo': 'Danish Gigaword Corpus'}
{'en': 'Danish language technology has been hindered by a lack of broad-coverage corpora at the scale modern NLP prefers. This paper describes the Danish Gigaword Corpus, the result of a focused effort to provide a diverse and freely-available one billion word corpus of Danish text. The Danish Gigaword corpus covers a wide array of time periods, domains, speakers’ socio-economic status, and Danish dialects.', 'ar': 'تعرقلت تكنولوجيا اللغة الدنماركية بسبب الافتقار إلى التغطية الواسعة للمؤسسات على النطاق الذي تفضله البرمجة اللغوية العصبية الحديثة. تصف هذه الورقة مجموعة Gigaword Corpus الدنماركية ، وهي نتيجة جهود مركزة لتوفير مجموعة متنوعة ومتاحة مجانًا من مليار كلمة من النص الدنماركي. تغطي مجموعة Gigaword الدنماركية مجموعة واسعة من الفترات الزمنية والمجالات والوضع الاجتماعي والاقتصادي للمتحدثين واللهجات الدنماركية.', 'fr': "La technologie de la langue danoise a été entravée par l'absence de corpus à large couverture à l'échelle préférée par la PNL moderne. Cet article décrit le Corpus danois Gigaword, le résultat d'un effort ciblé visant à fournir un corpus diversifié et librement disponible d'un milliard de mots de texte danois. Le corpus danois Gigaword couvre un large éventail de périodes, de domaines, de statut socio-économique des locuteurs et de dialectes danois.", 'es': 'La tecnología del idioma danés se ha visto obstaculizada por la falta de corpus de cobertura amplia a la escala que prefiere la PNL moderna. Este artículo describe el Danish Gigaword Corpus, el resultado de un esfuerzo centrado en proporcionar un corpus diverso y de libre acceso de mil millones de palabras de texto danés. El corpus de Gigaword danés cubre una amplia gama de períodos de tiempo, dominios, estatus socioeconómico de los hablantes y dialectos daneses.', 'pt': 'A tecnologia da língua dinamarquesa foi prejudicada pela falta de corpora de ampla cobertura na escala que a PNL moderna prefere. Este artigo descreve o Gigaword Corpus dinamarquês, o resultado de um esforço concentrado para fornecer um corpus de texto dinamarquês diversificado e disponível gratuitamente com um bilhão de palavras. O corpus Gigaword dinamarquês abrange uma ampla gama de períodos de tempo, domínios, status socioeconômico dos falantes e dialetos dinamarqueses.', 'ja': 'デンマーク語のテクノロジーは、現代のNLPが好む規模の広範囲なコーパスの欠如によって妨げられてきた。この論文では、デンマーク語のテキストの多様で自由に利用できる10億語のコーパスを提供するための集中的な努力の結果であるデンマーク語のギガワードコーパスについて説明します。デンマーク語のGigawordコーパスは、幅広い期間、領域、話者の社会経済的地位、デンマーク語の方言をカバーしています。', 'zh': '丹麦语术以乏今世NLP所目之广覆语料库而见阻。 本文述丹麦语Gigaword语料库,此供多样化而免费10亿字丹麦语文本语料库也。 丹麦语 Gigaword 语料库涵盖广时间段、域、言者位丹麦方言。', 'hi': 'डेनिश भाषा प्रौद्योगिकी को आधुनिक एनएलपी पसंद के पैमाने पर व्यापक कवरेज निगम की कमी से बाधित किया गया है। यह पेपर डेनिश गीगावर्ड कॉर्पस का वर्णन करता है, जो डेनिश पाठ के विविध और स्वतंत्र रूप से उपलब्ध एक अरब शब्द कॉर्पस प्रदान करने के लिए एक केंद्रित प्रयास का परिणाम है। डेनिश गीगावर्ड कॉर्पस समय अवधि, डोमेन, वक्ताओं की सामाजिक-आर्थिक स्थिति और डेनिश बोलियों की एक विस्तृत सरणी को कवर करता है।', 'ru': 'Датской языковой технологии препятствует отсутствие широко распространенных корпораций в масштабах, которые предпочитает современная НЛП. В этом документе описывается датский Gigaword Corpus, результат целенаправленных усилий по предоставлению разнообразного и свободно доступного одного миллиарда слов корпуса датского текста. Датский корпус Гигаворда охватывает широкий спектр периодов времени, областей, социально-экономического статуса говорящих и датских диалектов.', 'ga': 'Tá bac ar theicneolaíocht na Danmhairge mar gheall ar easpa corpora clúdaithe leathan ar an scála is fearr leis an NLP nua-aimseartha. Déanann an páipéar seo cur síos ar Chorpas Gigaword na Danmhairge, toradh ar iarracht dhírithe chun corpas de théacs Danmhairge atá éagsúil agus atá ar fáil go héasca, atá éagsúil ó thaobh billiún focal a sholáthar. Clúdaíonn corpas Gigaword na Danmhairge raon leathan tréimhsí ama, fearainn, stádas socheacnamaíoch na gcainteoirí, agus canúintí na Danmhairge.', 'ka': 'დენიური ენის ტექნოლოგია შეუძლებელია, რომელიც განსაკუთრებული კოპორაციის განმავლობაში მდგომარე NLP-ის განმავლობაში. ამ დოკუნტის შესახებ დენიური ჯიდავარის კორპოსს, განსხვავებული და თავისუფალურად მიღებული ერთ მილიარდი სიტყვების კორპოსს დენიური ტექსტის შესახებ. ეანჟკთწრ დთდავაპე კჲპოსჟ ოჲკპთგა ოყლნთ გპვმვნთ, ეჲმვნთ, ჟჲუთჲ-ვკონჲმთფვჟკთ ჟრარსრ ნა დჲგჲპთრვლთრვ თ ეანჟკთ ეთალვკრთ.', 'hu': 'A dán nyelvtechnológiát akadályozta a modern NLP által előnyben részesített méretű, széles körű lefedettségű corpora hiánya. Ez a tanulmány bemutatja a dán Gigaword Corpust, amely egy különböző és szabadon hozzáférhető, egymilliárd szós dán szövegkorpuszt kínál. A dán Gigaword korpusz számos időszakot, tartományt, a beszélők társadalmi-gazdasági státuszát és a dán dialektusokat fedi le.', 'el': 'Η δανική γλωσσική τεχνολογία έχει παρεμποδιστεί από την έλλειψη εταιρικών σωμάτων ευρείας κάλυψης στην κλίμακα που προτιμά το σύγχρονο NLP. Η παρούσα εργασία περιγράφει το Δανικό Σώμα Gigaword, αποτέλεσμα μιας επικεντρωμένης προσπάθειας για την παροχή ενός ποικίλου και ελεύθερα διαθέσιμου ενός δισεκατομμυρίου λέξεων από δανικό κείμενο. Το δανικό σώμα καλύπτει ένα ευρύ φάσμα χρονικών περιόδων, τομέων, κοινωνικοοικονομικής κατάστασης των ομιλητών και δανικών διαλέκτων.', 'it': 'La tecnologia della lingua danese è stata ostacolata dalla mancanza di corpora ad ampia copertura alla scala che la NLP moderna preferisce. Questo articolo descrive il danese Gigaword Corpus, il risultato di uno sforzo mirato per fornire un corpus di testo danese diversificato e liberamente disponibile per un miliardo di parole. Il corpus danese Gigaword copre una vasta gamma di periodi di tempo, domini, status socio-economico dei parlanti e dialetti danesi.', 'kk': 'Дания тілінің технологиясы көзгертілген NLP көзгертілген көзгертілік корпорасының жоқ болмасын сақталды. Бұл қағаз Дания Gigaword Corpus дегенді таңдайды. Дания мәтінінің бір миллиард мәтінді корпус үшін көздеген жұмыс істеудің нәтижесі. Даниялық Gigaword корпус көп уақыт күні, домендер, орындаушылардың социологиялық экономикалық күйі және Даниялық диалекттері жазылады.', 'lt': 'Danijos kalbos technologijoms trukdė plataus masto korporacijos trūkumas, kurį labiausiai teikia šiuolaikinė NLP. Šiame dokumente aprašomas Danijos Gigaword Corpus, kurio rezultatas – sutelktos pastangos suteikti įvairią ir laisvai prieinamą milijardo žodžių Danijos teksto korpusą. Danijos Gigaword korpus apima įvairius laikotarpius, sritis, kalbėtojų socialinį ir ekonominį status ą ir Danijos dialektus.', 'mk': 'Данската технологија на јазик е попречена од недостатокот на широко покривање на корпора на маштабот кој го преферира модерниот НЛП. Овој весник го опишува Данскиот Гигаворд Корпус, резултат на фокусираните напори за обезбедување различна и слободно достапна една милијарда зборни корпус на Дански текст. Данскиот Гигаворд корпус покрива широк вид на временски периоди, домени, социоекономски статус на говорниците и дански дијалекти.', 'ml': 'ഡാനിഷ് ഭാഷ സാങ്കേതികവിദ്യ തടസ്സപ്പെടുത്തിയിരിക്കുന്നു ഈ പത്രത്തില്\u200d ഡാനിഷ് ഗിഗാവോര്\u200dഡ് കോര്\u200dപ്പുസിനെ വിശദീകരിക്കുന്നു. ഒരു ബില്യണ്\u200d വാക്കുകള്\u200d ഡാനിഷ് പദാവലിയുടെ കോര്\u200dപ്പുസിനെ  ഡാനിഷ് ഗിഗാവോര്\u200dഡ് കോര്\u200dപ്പുസ് വിശാലമായ കാലഘട്ടങ്ങള്\u200d, ഡൊമെന്\u200dസ്, സംസാരിക്കുന്നവരുടെ സാമൂഹിക സ്ഥിതിയും ഡാനിഷ് സംസ', 'mt': 'It-teknoloġija tal-lingwa Daniża ġiet imxekkla minn nuqqas ta’ korpora ta’ kopertura wiesgħa fuq l-iskala li jippreferi l-NLP moderna. This paper describes the Danish Gigaword Corpus, the result of a focused effort to provide a diverse and freely-available one billion word corpus of Danish text.  Il-korpus Daniż Gigaword ikopri firxa wiesgħa ta’ perjodi ta’ żmien, dominji, status soċjoekonomiku tal-kelliema, u dijaletti Daniżi.', 'ms': 'Teknologi bahasa Danish telah dihalangi oleh kekurangan corpora lapisan luas pada skala NLP modern lebih suka. Kertas ini menggambarkan Gigaword Corpus Denmark, hasil usaha fokus untuk menyediakan satu bilion kata corpus teks Danish yang berbeza dan tersedia secara bebas. Korpus Gigaword Denmark meliputi sejumlah masa yang luas, domain, status sosio-ekonomi pembicara, dan dialekt Denmark.', 'mn': 'Данийн хэл технологийг орчин үеийн NLP-ын сонголт дэлгэрэнд шинэ мэдээллийн корпора байхын тулд зогсоосон. Энэ цаас Данийн Гигааров Корпус гэдгийг тайлбарлаж байна. Данийн текстүүдийн нэг тэрбум мянган үг корпус гэх мэт төвлөрсөн төвлөрсөн хичээлийн үр дүнд. Данийн гигавардын корпус цаг хугацааны, газрын, яриачдын нийгмийн эдийн засгийн байр суурь болон Данийн диалектуудыг ихэвчлэн хадгалдаг.', 'no': 'Den danske språk-teknologien har blitt hindra av mangling av breiddekkekorpora i skala for moderne NLP-innstillingar. Denne papiret beskriver den danske Gigaword Corpus, resultatet av ein fokusert forsøk for å gjera ein ulike og fritt tilgjengeleg ord korpus av danske tekst. Den danske Gigaword-korpusen dekker eit breidde rekkjefølgje av tidspunkt, domene, samfunn-økonomisk status for taler og danske dialektar.', 'pl': 'Duńska technologia językowa utrudnia brak korpusów o szerokim zasięgu w skali preferowanej przez nowoczesne NLP. Niniejszy artykuł opisuje duński korpus gigantyczny, który jest wynikiem skupionych starań mających na celu zapewnienie zróżnicowanego i wolno dostępnego miliarda słów duńskiego tekstu. Duński korpus Gigaword obejmuje szeroki wachlarz okresów czasowych, domen, statusu społeczno-ekonomicznego mówców i dialektów duńskich.', 'sr': 'Danska jezička tehnologija je spriječila nedostatak širokog korporacije na skali modernog NLP-a. Ovaj papir opisuje Danski Gigaword Corpus, rezultat fokusiranog napora da pruži različite i slobodno dostupne jednu milijardu riječi korpus danskog teksta. Danski Gigaword korpus pokriva širok niz vremenskih perioda, domena, socio-ekonomski status govornika i danskih dijalekata.', 'ro': 'Tehnologia limbii daneze a fost împiedicată de lipsa corpurilor largi de acoperire la scara preferată de PNL modern. Această lucrare descrie Corpus Gigaword danez, rezultatul unui efort concentrat de a oferi un corpus divers și liber disponibil de un miliard de cuvinte de text danez. Corpusul danez Gigaword acoperă o gamă largă de perioade de timp, domenii, statutul socio-economic al vorbitorilor și dialecte daneze.', 'so': 'Teknolojiilka afka Dannishka waxaa laga hor jeeday baahida shirkado ballaadhan oo ku saabsan daryeelka hore ee NLP. Kanu wuxuu ku qoraa Korpus ee Daanish Gigaword, taas oo ka mid ah jarrab aad u fiirsan karto inuu soo daayo hal billion oo eray oo af ah oo ku saabsan qoraalka Daanishka. Qoriga Gigaword ee Daanishku wuxuu qariyaa xilliyo badan oo kala duduwan, xaalada bulshada iyo dhaqaalaha dhaqaalaha ee dadka hadla iyo afka Daanishka ah.', 'sv': 'Den danska språktekniken har hindrats av avsaknad av storföretag med bred täckning i den skala som moderna NLP föredrar. Denna uppsats beskriver den danska Gigaword Corpus, resultatet av ett fokuserat arbete med att tillhandahålla en mångfaldig och fritt tillgänglig en miljard ord korpus av dansk text. Den danska Gigaword-korpusen täcker ett brett spektrum av tidsperioder, domäner, talares socioekonomiska status och danska dialekter.', 'si': 'ඩැනිෂ් භාෂා තාක්ෂණාත්මක තාක්ෂණාත්මක විශාල ප්\u200dරමාණ කරණාත්මක කාර්පෝරාවක් නැති විදියට අ මේ පැත්තේ ඩැනිෂ් ජිගාවර්ඩ් කෝර්පුස් විස්තර කරනවා, ඩැනිෂ් පාළුවේ විවිධ වචන කෝර්පුස් එක බිලියර් වචන කෝර්ප ඩැනිෂ් ජිගාවර්ඩ් කොර්පුස් එක්ක වෙලාවක් අවසානයක්, ඩෝමේන්, සාමාජික-අර්ථාත්මක ස්ථිතිය, ඩැනිෂ් ඩායි', 'ta': 'டானிஷ் மொழி தொழில்நுட்பம் பிரித்துக் கொண்டிருக்கிறது பிரிவான மறைப்பு நிறுவனத்தின் குறைப்பிட்டுள்ளது  இந்த காகிதத்தை டானிஷ் கிகாவோர்ட் கார்ப்ஸ் விளக்குகிறது, ஒரு வேறு பில்லியன் வார்த்தைகள் டானிஷ் உரையின் கோர்ப்புஸ் கொண் டேனிஷ் கிகாவோர்ட் குறிப்புகள் நிறைய நேர காலங்கள், களங்கள், பேசுபவர்களின் சமூக- பொருளாதார நிலைமை', 'ur': 'دنیا کی زبان تکنولوژی کی ترجیح دینے والی NLP کے مطابق وسیع کاپور کی کمپانی سے روکا گیا ہے. یہ کاغذ دینی جیگاورڈ کورپوس کو توصیح دیتا ہے، اس کے نتیجہ میں ایک مختلف اور آزاد طور پر موجود ایک میلیارڈ کلمات کا کورپوس دینا ہے۔ دنیا کی جگوارد کورپوس ایک گھیری مدت، ڈومین، صحبت کرنے والوں کی اجتماعی-اقتصادی موقعیت اور دنیا کی دیالکت پر پورے ہوتے ہیں.', 'uz': 'Name Bu qogʻoz Danish Gigaword Korpus (Danish Ggaword Korpus) haqida yaratadi. Bu qogʻoz Danish matnning bir milliard soʻzni boshqarish uchun foydalanuvchi jarayon natijasi. Name', 'vi': 'Công nghệ ngôn ngữ Đan Mạch bị cản trở bởi sự thiếu đông cơ sở hạ đẳng theo quy mô mà Đảng tư bản hiện đại thích. Bài viết này mô tả Cơ thể Gigaword Corpus, là kết quả của một nỗ lực tập trung để cung cấp một tập hợp chữ tiếng Đan Mạch rộng rãi và tự do có được hàng tỉ từ. The Danish Gigaword Corpus bao gồm một loạt các mảng thời gian, miền, địa bàn, địa phương xã hội-kinh tế của người phát biểu, và thổ ngữ Đan Mạch.', 'da': 'Dansk sprogteknologi er blevet hæmmet af mangel på bredt dækkende corpora i den skala moderne NLP foretrækker. Denne artikel beskriver det danske Gigaword Corpus, resultatet af en fokuseret indsats for at levere et alsidigt og frit tilgængeligt 1 milliard ord korpus af dansk tekst. Det danske Gigaword korpus dækker en lang række tidsperioder, domæner, taleres socioøkonomiske status og danske dialekter.', 'bg': 'Технологията на датския език е възпрепятствана от липсата на корпуси с широко покритие в мащаба, който предпочита съвременната НЛП. Настоящата статия описва датския корпус, резултат от фокусирани усилия за предоставяне на разнообразен и свободно достъпен един милиард думи корпус от датски текст. Корпусът обхваща широк спектър от времеви периоди, области, социално-икономическия статус на говорещите и датски диалекти.', 'hr': 'Danskoj jezičkoj tehnologiji su spriječili nedostatak širokog pokrivanja korporacije na skali modernog NLP-a. Ovaj papir opisuje Danski Gigaword Corpus, rezultat usredotočenog napora kako bi pružio različite i slobodno dostupne jednu milijardu riječi korpus danskog teksta. Danski Gigaword korpus pokriva širok niz vremenskih perioda, domena, socio-ekonomski status govornika i danskih dijalekata.', 'nl': 'Deense taaltechnologie is gehinderd door een gebrek aan brede dekking corpora op de schaal die moderne NLP verkiest. Dit artikel beschrijft het Deense Gigaword Corpus, het resultaat van een gerichte inspanning om een divers en vrij beschikbaar één miljard woord corpus Deense tekst te bieden. Het Deense Gigaword corpus bestrijkt een breed scala aan tijdsperiodes, domeinen, socio-economische status van sprekers en Deense dialecten.', 'ko': '덴마크 언어 기술은 현대 NLP가 선호하는 대규모 광범위한 커버 자료 라이브러리가 부족해 장애를 받고 있다.본고는 덴마크 Gigaword 자료 라이브러리를 묘사하는데 이것은 집중적인 노력의 결과로 다양하고 무료로 제공되는 10억 단어의 덴마크 텍스트 자료 라이브러리를 제공하는 데 목적을 둔다.덴마크 기가워드 어료고는 광범위한 시간대, 분야, 말하는 사람들의 사회적 경제적 지위와 덴마크 사투리를 포함한다.', 'fa': 'تکنولوژی زبان دانشی توسط ناتوانی شرکت گسترده در مقیاس NLP مدرن ترجیح داده شده است. این کاغذ کورپوس ژیگاورد دانش را توصیف می\u200cکند، نتیجه تلاش تمرکز شده تا یک میلیارد کلمه کورپوس از متن دانش را فراهم کند. کورپوس ژيگاورد دنيايي يه مجموعه طولاني از مدت زمان، دامنه ها، وضعيت اجتماعي و اقتصادي صحبت کننده ها و ديالکت هاي دنيايي را پوشانده است.', 'de': 'Die dänische Sprachtechnologie wurde durch den Mangel an breitflächigen Korpora in der Größenordnung behindert, die moderne NLP bevorzugt. Dieser Beitrag beschreibt das dänische Gigaword Corpus, das Ergebnis einer konzentrierten Anstrengung, einen vielfältigen und frei verfügbaren Wortkorpus dänischer Texte bereitzustellen. Das dänische Gigaword-Korpus deckt eine Vielzahl von Zeiträumen, Domänen, sozioökonomischen Status der Sprecher und dänische Dialekte ab.', 'sw': 'Teknolojia ya lugha ya KiDanish imezuiwa na kukosekana kwa kampuni ya habari pana kwa kiasi kikubwa kinachopendelea NLP. Gazeti hili linaelezea Korpus ya KiDanish Gigaword, matokeo ya juhudi za kutangaza maneno mabilioni moja yenye uhuru. Kampuni ya KiDanish Gigaword inazungumza kipindi kikubwa cha muda, ndani, hali ya kijamii na kiuchumi ya wanazungumza, na lugha za Kidenishi.', 'id': 'Teknologi bahasa Danes telah diharamkan oleh kekurangan corpora lapisan luas pada skala yang NLP modern lebih suka. Kertas ini menggambarkan Gigaword Corpus Denmark, hasil dari usaha fokus untuk menyediakan satu miliar kata corpus dari teks Danish yang berbeda dan tersedia secara bebas. Korpus Gigaword Denmark meliputi berbagai periode waktu, domain, status sosio-ekonomi pembicara, dan dialekt Denmark.', 'tr': 'Dança dil teknolojisi modern NLP göz öňünde örän gabat korporasy ýok bolmagyndan saklanýar. Bu kagyz Dança Gigaword Korpusyny tassyklaýar, we bu netijede dürli we seretseňsiz bir milyar söz korpusy Daniýanyň metini saýlamak üçin üns berýär. Dança Gigaword korpus wagt periýatlaryny, sahypalaryny, gürleşýänleriň sosial-ekonomik durumyny we dança dialektalaryny daşyrýar.', 'sq': 'Danish language technology has been hindered by a lack of broad-coverage corpora at the scale modern NLP prefers.  Ky dokument përshkruan Korpin Danik Gigaword, rezultatin e një përpjekjeje të përqëndruar për të siguruar një korpus të ndryshëm dhe lirisht të disponueshëm një miliard fjalësh të tekstit Danik. Korpus danez Gigaword mbulon një sërë të gjerë periudhësh kohore, fusha, statusi socio-ekonomik të folësve dhe dialekte daneze.', 'am': 'የዴንስ ቋንቋ ቴክኖጂ አዲስ አዲስ NLP በሚፈለግ ክፍተት ሰፊ የኮርፖርት ጉዳይ እንዳይጎድል ተከልክሎአል፡፡ ይህም ሕገጽ የዳንስ ጊጋword ኮርፓስ፣ የዳንስ ጽሑፍ ልዩ ብልዮን እና ነፃነት የቻለ አንድ ቢልዮን ቃላት የቆርፓስ ክሮፕስ በሚያሳየው ውጤት ነው፡፡ የዳንስ ጊግአርድ ኮርፓስ ብዙ ዘመናት፣ አዳማጆች፣ የንግግራተኞች ማኅበራዊ-የኢኮኖሚ ግንኙነት እና የዳንስ ድምፅ ይሸፍናል፡፡', 'hy': 'Դանիացի լեզվի տեխնոլոգիան խոչընդոտում է լայն ծավալի կոպորատային բացակայությունը ժամանակակից ՆԼՊ-ը նախընտրում է: Այս հոդվածը նկարագրում է Դանիական Գիգավորդ Կորպուսը, ֆոկուսացված ջանքերի արդյունքը՝ տալու բազմազան և ազատ հասանելի մեկ միլիարդ բառ Դանիական տեքստի կորպուս: Դանիական Գիգավորդի կորպուսը ներառում է տարբեր ժամանակահատվածներ, ոլորտներ, խոսնակների սոցիալ-տնտեսական կարգավիճակ և Դանիական դիալեկտներ:', 'af': "Daanse taal tegnologie is verhinder deur 'n ontbreiding van breeddekkerkorpora by die skaal moderne NLP voorkeur. Hierdie papier beskrywe die Daniese Gigaword Corpus, die resultaat van 'n fokuseerde versoek om 'n verskeie en vry beskikbaar een biljoen woord korpus van Daniese teks te verskaf. Die Daniese Gigaword korpus bedek 'n wye aantal tydperke, domeine, sprekkers se sosiale-ekonomiese status en Daniese dialekte.", 'ca': "La tecnologia danesa ha estat obstruïda per la falta de corpora de gran cobertura a escala que prefereix la NLP moderna. Aquest article descriu el Gigaword Corpus danès, resultat d'un esforç centrat per proporcionar un corpus de mil milions de paraules de text danès divers i lliurament disponible. El corps danès Gigaword cobre una gran varietat de períodes de temps, dominis, estatus socioeconòmic dels oradors i dialectes danès.", 'cs': 'Dánské jazykové technologie byly bráněny nedostatkem širokého pokrytí korpusů v měřítku moderního NLP. Tento článek popisuje dánský Gigaword Corpus, výsledek zaměřeného úsilí poskytnout rozmanitý a volně dostupný korpus jedné miliardy slov dánského textu. Dánský korpus Gigaword pokrývá širokou škálu časových období, domén, sociálně-ekonomického postavení mluvčích a dánských dialektů.', 'bn': 'ড্যানিশ ভাষার প্রযুক্তি আধুনিক এনএলপি প্রার্থীতে ব্যাপক কাভারেজের অভাবের কারণে বাধা দেয়া হয়েছে। এই পত্রিকাটি ড্যানিশ গিগাওয়ার্ড কোর্পাসের বর্ণনা করেছে, যার ফলে দানিশ টেক্সটের একটি বিভিন্ন এবং স্বাধীনতা প্রদানের একটি বিলিয়ন শব ড্যানিশ গিগাওয়ার্ড কোর্পাস বিশাল সময়, ডোমেন, ভাষাকারীদের সামাজিক-অর্থনৈতিক অবস্থা এবং ড্যানিশ ভাষায় বিভিন্ন ভাষণ প', 'et': 'Taani keele tehnoloogiat on takistanud laiaulatuslike korporatsioonide puudumine tänapäeva uue õppekava eelistatud ulatuses. Käesolevas dokumendis kirjeldatakse taani gigasõna korpust, mis on tulemus keskendunud jõupingutustele pakkuda mitmekesist ja vabalt kättesaadavat miljard sõna korpust taani teksti. Taani gigasõna korpus hõlmab mitmesuguseid ajaperioode, valdkondi, kõnelejate sotsiaalmajanduslikku staatust ja taani murdeid.', 'fi': 'Tanskankielistä tekniikkaa on haitannut laaja-alaisten yritysten puute nykyaikaisen NLP:n suosimassa mittakaavassa. Tämä artikkeli kuvaa tanskalaista gigasanakorpusta, joka on tulosta keskitetystä pyrkimyksestä tarjota monipuolinen ja vapaasti saatavilla oleva miljardi sanakorpus tanskalaista tekstiä. Tanskan gigasanakorpus kattaa laajan valikoiman ajanjaksoja, toimialoja, puhujien sosioekonomista asemaa ja tanskalaisia murteita.', 'az': "Danca dil teknolojisi moderni NLP seçimlərində geniş örtük korporası yoxdur. Bu kağıt Danish Gigaword Corpus'u təsdiqləyir, danışın məktubunun müxtəlif və özgür olaraq bir milyard söz korpusu təsdiqlənməsi üçün təsirli çabaların sonuçlarını təsdiqləyir. Danlıq Gigaword korpusu uzun zaman periyodlarını, domeinləri, danışanların sosial-ekonomik durumu və danlıq dialektlərini örtür.", 'bs': 'Danska jezička tehnologija je spriječila nedostatak širokog korporacije na skali modernog NLP-a. Ovaj papir opisuje Danski Gigaword Corpus, rezultat usredotočenog napora za pružanje različitih i slobodno dostupnih jedne milijarde riječi korpusa danskog teksta. Danski Gigaword korpus pokriva širok niz vremenskih perioda, domena, socio-ekonomski status govornika i danskih dijalekata.', 'he': 'טכנולוגיה לשפה דנית הוגבלה על ידי חוסר גופורה רחב כיסוי בסולם המודרני NLP מעדיף. העיתון הזה מתאר את הקורפוס הגיגאוורד הדני, תוצאה של מאמץ ממוקד לספק קופוס מיליארד מילים של טקסט דני מגוון ומזמין בחינם. הקורפוס הגיגאוורד הדני מכסה מגוון רחב של תקופות זמן, שטחים, סטטוס סוציו-כלכלי של מדברים, ודיאלקטים דנים.', 'jv': 'Danis lengkang tekno-tekno sing wis ngerasakno nang ora ono akeh kapan-kapan sing paling apik dhéwé kuwi nggawe NLP maun. Perintah iki dadi gihgaword Kripus Danish Ngucap Danish Gegaword karo perusahaan bukane gawe ngupakan bukane kabeh nguwarni winih, winih, perusahaan-akèh lan dialecto Danish.', 'bo': 'ཌེན་ཡིག་གི་སྐད་རིགས་ལག་རྩལ་བ་ཞིག་དག་གིས་དུས་མཐུན་ཁང་ཆེ་བ་ཡོད། ཤོག་བྱང་འདིས་ཌེན་གི་Gigaword Corpus་ལ་བྲིས་བཤད་པ་ཡིན། གནད་དོན་ཡོད་པའི་གོ་སྐབས་སྔར་སྣ་ཚོགས་དང་རང་དབང་སྤྲོད་ཐུབ་པའི་ཚིག་ ཌེན་ཤི་གི་Gigaword སྐོར་གྱི་དུས་མཚམས་མང་པོ་ཞིག་ཡོད་པ་ལས་ཕར་ཆེན་བྱེད་ཀྱི་ཡོད།', 'ha': "An kange technical na harshen Danish na da kuma an ƙara wata makampuni mai faɗa ɗawa da ke samu'ar zaɓen NLP na yanzu. Wannan takardan na bayyana Corbas na Danish Gigaword, na ƙarshen aikin da ya yi zura a samar da wata takarda na dabam-daban da ba'a iya sãmun wata biliyan magana na dansi. Shiryar Gigaword na dandan yana rufe wasu shekara dabam-dabam, madaidaici, ma'abũcin masu faɗa da jami-jami-kiomi, kuma danish na yi taƙaita.", 'sk': 'Dansko jezikovno tehnologijo ovira pomanjkanje široko pokritih korpusov v obsegu, ki ga sodobna NLP želi. Ta prispevek opisuje danski korpus gigabesed, rezultat osredotočenega prizadevanja za zagotovitev raznolikega in prosto dostopnega korpusa z eno milijardo besed danskega besedila. Danski korpus gigabesed zajema široko paleto časovnih obdobij, domen, družbeno-ekonomski status govorcev in danskih narečij.'}
{'en': 'DanFEVER : claim verification dataset for Danish', 'ar': 'DanFEVER: مجموعة بيانات التحقق الخاصة بالمطالبة باللغة الدنماركية', 'fr': 'DanFever\xa0: ensemble de données de vérification des réclamations pour le', 'es': 'DanFever: conjunto de datos de verificación de reclamaciones para', 'pt': 'DanFEVER: conjunto de dados de verificação de reivindicações para dinamarquês', 'ja': 'DanFEVER:デンマーク語のクレーム検証データセット', 'zh': 'DanFEVER:丹麦语索赔验数集', 'hi': 'DanFEVER: डेनिश के लिए दावा सत्यापन डेटासेट', 'ru': 'DanFEVER: набор данных для проверки претензий по Дании', 'ga': 'DanFEVER: tacar sonraí fíoraithe éilimh don Danmhairgis', 'hu': 'DanFEVER: kárigény-ellenőrzési adatkészlet dán számára', 'el': 'DanFEVER: σύνολο δεδομένων επαλήθευσης απαιτήσεων για τη δανική', 'ka': 'DanFEVER', 'kk': 'DanFEVER: Дания тексеру деректер жиынын талап ету', 'lt': 'DanFEVER: claim verification dataset for Danish', 'it': 'DanFEVER: set di dati di verifica dei reclami per il danese', 'mk': 'DanFEVER: набор на податоци за верификација на тврдењата за дански', 'ml': 'DanFEVER: ഡാനിഷിനുള്ള ഡാറ്റാറ്റാ ഉറപ്പാക്കുക', 'ms': 'DanFEVER: set data pengesahan klaim untuk Danish', 'mt': 'DanFEVER: sett ta’ dejta dwar il-verifika tat-talbiet għad-Daniż', 'pl': 'DanFEVER: zbiór danych dotyczących weryfikacji roszczeń dla Danii', 'mn': 'Данийн шалгалтын өгөгдлийн санг', 'no': 'DanFEVER: tvar verifiseringsdata for dansk', 'ro': 'DanFEVER: set de date de verificare a cererilor pentru Danemarca', 'sr': 'Opasnik: prijavi podatke za verifikaciju za danski', 'so': 'DanFEVER: Shahaadada xaqiijinta ee Danish', 'si': 'DanFEVER: ඩැනිෂ් වෙනුවෙන් පරීක්ෂණ දත්ත සෙට් පිළිගන්න', 'ur': 'DanFEVER: دنیش کے لئے verification dataset کی تعریف کریں', 'ta': 'DanFEVER: claim verification dataset for Danish', 'sv': 'DanFEVER: dataset för skadekontroll för danska', 'uz': 'Dancha uchun tasdiqlash maò¥lumotini soò£rash', 'vi': 'Đan Mạch: bộ dữ liệu kiểm tra yêu cầu', 'nl': 'DanFEVER: gegevensset voor claimverificatie voor Deens', 'bg': 'DanFEVER: набор от данни за проверка на претенции за датски език', 'da': 'DanFEVER: datasæt til bekræftelse af krav for dansk', 'hr': 'Opasnik: prijavi podatke o provjeri za danski', 'ko': 'DanFEVER: 덴마크 청구 확인 데이터 세트', 'fa': 'DanFEVER: تقاضای مجموعه داده\u200cهای تصدیق برای دانش', 'de': 'DanFEVER: Datensatz zur Überprüfung der Ansprüche für Dänisch', 'id': 'DanFEVER: set data verifikasi klaim untuk Danish', 'tr': 'Danmarkça üçin barlamak maglumaty takyklaş', 'af': 'Gevaarder: versoek bevestiging datastel vir Daniese', 'sq': 'DanFEVER: grupi i të dhënave për verifikimin e pretendimeve për danisht', 'sw': 'DanFEVER: taarifa za kuthibitishwa kwa ajili ya Denmark', 'am': "ዶሴ `%s'ን ማስፈጠር አልተቻለም፦ %s", 'bn': 'DanFEVER: ড্যানিশের জন্য নিশ্চিত তথ্যের দাবি করুন', 'az': 'Danca üçün təsdiqləmə veri qurğunu iddia et', 'hy': 'Դանֆեվեր: Դանիացի կողմից հավասարման տվյալների համակարգ', 'et': 'DanFEVER: Taani keele nõuete kontrollimise andmekogum', 'bs': 'Danski podaci za provjeru', 'ca': 'DanFEVER: claim verification dataset for Danish', 'cs': 'DanFEVER: sada údajů o ověření žádostí pro dánsko', 'fi': 'DanFEVER: Tanskan väitteiden todentamisaineisto', 'jv': 'Danish', 'ha': 'QUnicodeControlCharacterMenu', 'he': 'DanFEVER: קבוצת מידע האמת טענות לדני', 'sk': 'DanFEVER: nabor podatkov o preverjanju zahtevkov za danščino', 'bo': 'DanFEVER: claim verification dataset for Danish'}
{'en': 'We present a dataset, DanFEVER, intended for multilingual misinformation research. The dataset is in Danish and has the same format as the well-known English FEVER dataset. It can be used for testing methods in multilingual settings, as well as for creating models in production for the Danish language.', 'ar': 'نقدم مجموعة بيانات ، DanFEVER ، مخصصة لبحوث المعلومات الخاطئة متعددة اللغات. مجموعة البيانات باللغة الدنماركية ولها نفس تنسيق مجموعة بيانات FEVER الإنجليزية المعروفة. يمكن استخدامه لاختبار الأساليب في إعدادات متعددة اللغات ، وكذلك لإنشاء نماذج في الإنتاج للغة الدنماركية.', 'es': 'Presentamos un conjunto de datos, DanFever, destinado a la investigación multilingüe de desinformación. El conjunto de datos está en danés y tiene el mismo formato que el conocido conjunto de datos inglés FEVER. Se puede utilizar para probar métodos en entornos multilingües, así como para crear modelos en producción para el idioma danés.', 'fr': 'Nous présentons un ensemble de données, DanFever, destiné à la recherche multilingue sur la désinformation. Le jeu de données est en danois et a le même format que le célèbre jeu de données anglais FEVER. Il peut être utilisé pour tester des méthodes dans des environnements multilingues, ainsi que pour créer des modèles en production pour la langue danoise.', 'pt': 'Apresentamos um conjunto de dados, DanFEVER, destinado à pesquisa multilíngue de desinformação. O conjunto de dados está em dinamarquês e tem o mesmo formato que o conhecido conjunto de dados inglês FEVER. Ele pode ser usado para testar métodos em configurações multilíngues, bem como para criar modelos em produção para o idioma dinamarquês.', 'ja': '当社は、多言語の誤った情報の研究を目的としたデータセットDanFEVERを提示しています。データセットはデンマーク語で、よく知られている英語の発熱データセットと同じ形式です。多言語環境でのメソッドのテストや、デンマーク語のモデルの作成に使用できます。', 'hi': 'हम एक डेटासेट, DanFEVER, बहुभाषी गलत सूचना अनुसंधान के लिए इरादा प्रस्तुत करते हैं। डेटासेट डेनिश में है और इसमें प्रसिद्ध अंग्रेजी बुखार डेटासेट के समान प्रारूप है। इसका उपयोग बहुभाषी सेटिंग्स में विधियों के परीक्षण के लिए किया जा सकता है, साथ ही डेनिश भाषा के लिए उत्पादन में मॉडल बनाने के लिए भी किया जा सकता है।', 'zh': '建一DanFEVER数集,施于多言错误信息。 其数集用丹麦语,式与名英语 FEVER 同。 其可试多言置中之法,及生地为丹麦语创模形。', 'ru': 'Мы представляем набор данных DanFEVER, предназначенный для многоязычного исследования дезинформации. Набор данных составлен на датском языке и имеет тот же формат, что и хорошо известный английский набор данных о ЛИХОРАДКЕ. Он может быть использован для тестирования методов в многоязычных условиях, а также для создания моделей в производстве для датского языка.', 'ga': 'Cuirimid i láthair tacar sonraí, DanFEVER, atá beartaithe le haghaidh taighde ilteangach faisnéise mícheart. Tá an tacar sonraí sa Danmhairgis agus tá an fhormáid chéanna air agus atá ar an tacar sonraí Béarla FEVER a bhfuil aithne air. Is féidir é a úsáid le haghaidh modhanna tástála i suíomhanna ilteangacha, chomh maith le samhlacha a chruthú i dtáirgeadh don Danmhairgis.', 'hu': 'Bemutatjuk a DanFEVER adatkészletet, amelyet többnyelvű félinformációs kutatásra szántak. Az adatkészlet dán nyelvű, és ugyanolyan formátumú, mint a jól ismert angol FEVER adatkészlet. Használható többnyelvű környezetben végzett tesztelési módszerekhez, valamint a dán nyelv gyártásában lévő modellek létrehozásához.', 'ka': 'ჩვენ მონაცემების კონფიგურაცია, DanFEVER, მრავალენგური შეცდომარებისთვის. მონაცემების კონფიგურაცია დიანიშში და იგივეა ფორმატი როგორც უცნობილი ინგლისური FEVER მონაცემების კონფიგურაცია. ეს შეიძლება გამოყენება მრავალენგური პარამეტრებში მეტისტესტის მეტისტესებისთვის, როგორც მოდელების შექმნა დიანური ენაზე.', 'it': 'Vi presentiamo un set di dati, DanFEVER, destinato alla ricerca multilingue sulla disinformazione. Il set di dati è in danese e ha lo stesso formato del noto set di dati FEVER inglese. Può essere utilizzato per testare metodi in contesti multilingue, nonché per creare modelli in produzione per la lingua danese.', 'el': 'Παρουσιάζουμε ένα σύνολο δεδομένων, που προορίζεται για πολύγλωσση έρευνα παραπληροφόρησης. Το σύνολο δεδομένων είναι στα δανικά και έχει την ίδια μορφή με το γνωστό αγγλικό σύνολο δεδομένων FEVER. Μπορεί να χρησιμοποιηθεί για τη δοκιμή μεθόδων σε πολυγλωσσικές ρυθμίσεις, καθώς και για τη δημιουργία μοντέλων στην παραγωγή για τη δανική γλώσσα.', 'mk': 'Презентираме податоци, DanFEVER, наменети за мултијазички погрешни информации истражување. Податоците се на Дански и имаат ист формат како и познатиот англиски ФЕВЕР податок. Истата може да се користи за тестирање на методите во мултијазични услови, како и за создавање модели во производството на данскиот јазик.', 'kk': 'Біз көптеген тілдерді дұрыс түрлендіру зерттеу үшін деректер жинағын DanFEVER таңдаймыз. Деректер жиыны Дания тілінде және белгілі ағылшын FEVER деректер жиынының бір пішімі бар. Ол көптілік параметрлерінде әдістерді сынақтау үшін қолданылады, сондай-ақ Дания тілінің шығару үшін моделдерді құру үшін қолданылады.', 'ms': 'Kami memperkenalkan set data, DanFEVER, yang bertujuan untuk kajian misinformasi berbilang bahasa. Set data dalam bahasa Danish dan mempunyai format yang sama dengan set data FEVER Inggeris yang diketahui. Ia boleh digunakan untuk menguji kaedah dalam tetapan berbilang bahasa, serta untuk mencipta model dalam produksi bahasa Danish.', 'lt': 'Mes pristatome DanFEVER duomenų rinkinį, skirtą daugiakalbiams netinkamai informaciniams tyrimams. The dataset is in Danish and has the same format as the well-known English FEVER dataset.  Jis gali būti naudojamas daugiakalbių metodų bandymui ir modelių kūrimui danų kalba.', 'ml': 'ഞങ്ങള്\u200d ഒരു ഡാറ്റാസേറ്റ് ഡാന്\u200dഫെവെര്\u200d കൊണ്ടുവരുന്നു, പല ഭാഷ വിവരങ്ങള്\u200d പരിശോധിക്കാന്\u200d ഉദ്ദേശിച്ചത്. ഡാനിഷിലുള്ള ഡാറ്റാസറ്റ് ഉണ്ടു് എന്നിട്ട് അറിയപ്പെട്ട ഇംഗ്ലീഷ് FEVER ഡാറ്റാസറ്റ് പോലെ ഒരേ രീതി പല ഭാഷകങ്ങളുടെ സജ്ജീകരണങ്ങളില്\u200d പരീക്ഷിക്കുന്ന രീതികള്\u200dക്കും, ഡാനിഷ് ഭാഷയ്ക്ക് ഉല്\u200dപാദിപ്പിക്കുന്ന മോഡലുകള്\u200d', 'mt': 'Aħna nippreżentaw sett ta’ dejta, DanFEVER, maħsub għal riċerka multilingwi ta’ informazzjoni ħa żina. Is-sett tad-dejta huwa bid-Daniż u għandu l-istess format bħas-sett tad-dejta magħruf bl-Ingliż FEVER. Jista’ jintuża għall-ittestjar ta’ metodi f’ambjenti multilingwi, kif ukoll għall-ħolqien ta’ mudelli fil-produzzjoni għall-lingwa Daniża.', 'mn': 'Бид олон хэл буруу мэдээллийн судалгаанд зориулагдсан өгөгдлийн санг DanFEVER-г тайлбарлаж байна. Данийн өгөгдлийн санд байгаа бөгөөд Англи FEVER өгөгдлийн сантай адилхан форматтай байдаг. Үүнийг олон хэл хэлний тохиолдолд шалгах арга болон Данийн хэлний үйлдвэрлэлд моделуудыг бүтээхэд ашиглаж болно.', 'pl': 'Przedstawiamy zbiór danych DanFEVER, przeznaczony do wielojęzycznych badań dezinformacji. Zestaw danych jest w języku duńskim i ma ten sam format co znany angielski zestaw danych FEVER. Może być wykorzystywany do testowania metod w ustawieniach wielojęzycznych, jak również do tworzenia modeli w produkcji dla języka duńskiego.', 'ro': 'Vă prezentăm un set de date, DanFEVER, destinat cercetării multilingve de dezinformare. Setul de date este în limba daneză și are același format ca și binecunoscutul set de date FEVER în limba engleză. Acesta poate fi utilizat pentru testarea metodelor în setări multilingve, precum și pentru crearea de modele în producție pentru limba daneză.', 'no': 'Vi presenterer ein dataset, DanFEVER, som skal brukast for forskning med fleirspråk feilformatering. Datasettet er i dansk og har det same format som det kjente engelske FEVER- datasettet. Dette kan brukast for testmetodar i fleirspråk innstillingar, og for å laga modeller i produksjon for danske språk.', 'sr': 'Predstavljamo set podataka, DanFEVER, namjeren za istraživanje multijezičkih pogrešnih informacija. Podaci su na danskom jeziku i imaju isti format kao i poznati engleski FEVER podaci. Može se koristiti za testiranje metoda u multijezičkim nastavama, kao i za stvaranje modela proizvodnje za danski jezik.', 'si': 'අපි දත්ත සෙට් එකක්, DanFEVER, ගොඩක් භාෂාවක් වැරදි විශ්වාස පරීක්ෂණය සඳහා තියෙන්නේ. දත්ත සෙට් ඩැනිෂ් වල ඉන්නවා ඒ වගේම ඉංග්\u200dරීසි FEVER දත්ත සෙට් වගේම එකම ප්\u200dරමාණය තියෙනවා. ඒක ගොඩක් භාෂාවක් සැකසුම් වලට පරීක්ෂණා විදිහට භාවිතා කරන්න පුළුවන්, ඩැනිෂ් භාෂාව සඳහා නිර්', 'ta': 'நாங்கள் ஒரு தரவு அமைப்பு, DanFEVER, பல மொழி தவறான தகவல் ஆராய்ச்சிக்கு எதிர்பார்க்கப்பட்டது. தரவு அமைப்பு டானிஷில் உள்ளது மற்றும் தெரியும் ஆங்கிலத்தின் FEVER தரவு அமைப்பு போல் அதே வடிவம் உள்ளது. பல மொழி அமைப்புகளில் சோதனை முறைகளை பயன்படுத்தலாம், மற்றும் டானிஷ் மொழிக்கு மாதிரிகளை உருவாக்க பயன்படுத்தலாம்.', 'so': 'Waxaynu soo bandhignaa koox macluumaad ah, DanFEVER, oo loo talo galay baaritaanka macluumaadka luuqadaha kala duduwan. Rugta macluumaadku waxay ku jirtaa Danish, waxayna leedahay qaab la mid ah oo la yaqaan ingiriisiga FEVER macluumaadkiisa. Sidoo kale waxaa lagu isticmaali karaa qaababka imtixaanka oo lagu tijaabiyo xarumaha luuqadaha kala duduwan iyo tusaalaha lagu sameeyo wax lagu soo saaro afka Danish.', 'sv': 'Vi presenterar ett dataset, DanFEVER, avsett för flerspråkig felinformationsforskning. Datauppsättningen är på danska och har samma format som den välkända engelska FEVER-datauppsättningen. Den kan användas för testmetoder i flerspråkiga miljöer, samt för att skapa modeller i produktion för det danska språket.', 'ur': 'ہم ایک ڈاٹ سٹ، DanFEVER، بہت سی زبان غلطی تحقیق کے لئے مطلب کیا گیا ہے۔ ڈاٹ سٹ ڈانیش میں ہے اور اس کے لئے ایک ہی فرمود ہے جو انگلیسی FEVER ڈاٹ سٹ کے مطابق معلوم ہے. یہ بہت سی زبان تنظیمات میں آزمائش طریقے کے لئے استعمال کئے جاتے ہیں، اور دنیا زبان کے لئے موڈل بنانے کے لئے بھی استعمال کئے جاتے ہیں.', 'uz': "Biz bir necha tilda xato haqida o'rganish uchun maʼlumot tizimi DanFEVER'ni hozirganamiz. Name Ko'p tillar moslamalarida sinov usullarni va Danish tili uchun modellarni yaratish uchun ishlatiladi.", 'vi': 'Chúng tôi cung cấp một bộ dữ liệu, DanFERE, được tạo ra để nghiên cứu trí tuệ đa dạng. Bộ dữ liệu theo tiếng Đan Mạch có cùng định dạng với bộ dữ liệu liên lạc của người Anh. Nó có thể được dùng để thử nghiệm phương pháp ở thiết lập đa dạng, cũng như để tạo ra các mô hình sản xuất cho ngôn ngữ Đan Mạch.', 'da': 'Vi præsenterer et datasæt, DanFEVER, beregnet til flersproget misinformation forskning. Datasættet er på dansk og har samme format som det velkendte engelske FEVER datasæt. Den kan bruges til testmetoder i flersprogede omgivelser samt til at skabe modeller i produktion til det danske sprog.', 'bg': 'Представяме набор от данни, предназначен за многоезични изследвания на дезинформацията. Наборът от данни е на датски език и има същия формат като известния набор от данни на английски. Тя може да се използва за тестване на методи в многоезични условия, както и за създаване на модели в производство за датския език.', 'hr': 'Predstavljamo sastanak podataka DanFEVER, namjeren za istraživanje multijezičkih pogrešnih informacija. Podaci su na danskom i imaju isti format kao i poznati podaci engleskog FEVERa. Može se koristiti za testiranje metoda u multijezičkim nastavama, kao i za stvaranje modela proizvodnje za danski jezik.', 'nl': 'We presenteren een dataset, DanFEVER, bedoeld voor meertalig misinformatie onderzoek. De dataset is in het Deens en heeft hetzelfde formaat als de bekende Engelse FEVER dataset. Het kan worden gebruikt voor het testen van methoden in meertalige instellingen, evenals voor het maken van modellen in productie voor de Deense taal.', 'id': 'Kami mempersembahkan dataset, DanFEVER, yang ditujukan untuk penelitian misinformasi berbagai bahasa. Set data dalam bahasa Danish dan memiliki format yang sama dengan set data FEVER Inggris yang terkenal. Ini dapat digunakan untuk menguji metode dalam pengaturan berbagai bahasa, serta untuk menciptakan model dalam produksi bahasa Danes.', 'sw': 'We present a dataset, DanFEVER, intended for multilingual misinformation research.  seti ya taarifa hiyo ni ya KiDanish na ina mfumo sawa na seti inayofahamika kwa Kiingereza FEVER. Inaweza kutumika kwa njia za kujaribu katika mazingira ya lugha mbalimbali, pamoja na kutengeneza mifano katika uzalishaji kwa lugha ya Kidenishi.', 'de': 'Wir präsentieren einen Datensatz, DanFEVER, der für mehrsprachige Fehlinformationsforschung bestimmt ist. Der Datensatz ist auf Dänisch und hat das gleiche Format wie der bekannte englische FEVER Datensatz. Es kann zum Testen von Methoden in mehrsprachigen Umgebungen sowie zum Erstellen von Modellen in der Produktion für die dänische Sprache verwendet werden.', 'fa': 'ما یک مجموعه اطلاعات، DanFEVER را پیشنهاد می\u200cکنیم که برای تحقیقات غلط کردن چند زبان است. مجموعه داده\u200cها در دانش هستند و با مجموعه داده\u200cهای انگلیسی FEVER شناخته شده\u200cاند. می\u200cتواند برای آزمایش روش\u200cهایی در تنظیمات زیادی زبان استفاده شود، همچنین برای ایجاد مدل\u200cهای تولید برای زبان دانش.', 'tr': 'Çoklu dillerde ýalňyşlyk barlamak üçin netijä bir veri setirini DanFEVER görkezip berýäris. Maglumat setiriniň Danmarkça we bilinen iňlis FEVER dataseti bilen meňzeş biçimlendir. Ol köp dilli düzümlerde ýoly barlamak üçin ullanyla bilýär, we Dança dilinde önümlerinde nusgala döretmek üçin ullanyla bilýär.', 'ko': '다중 언어 오류 정보 검토를 위한 데이터 세트 DanFEVER를 제공합니다.이 데이터 집합은 덴마크어를 사용하는데, 격식은 유명한 영어 발열 데이터 집합과 같다.이것은 다중 언어 환경에서 테스트 방법과 생산 과정에서 덴마크 언어를 위한 모델을 만드는 데 사용될 수 있다.', 'af': "Ons stel 'n datastel, DanFEVER, bedoel vir multitaalske verkeerde ondersoek. Die datastel is in Daniese en het dieselfde formaat as die bekende Engels FEVER datastel. Dit kan gebruik word vir toets metodes in multilinguele instellings, asook vir skep modele in produksie vir die Daniese taal.", 'sq': 'Ne paraqesim një grup të dhënash, DanFEVER, që synohet për kërkime shumëgjuhëse të gabuara informacioni. Të dhënat janë në danisht dhe kanë të njëjtin format si të dhënat e njohura angleze FEVER. Ajo mund të përdoret për testimin e metodave në rregullime shumëgjuhësore si dhe për krijimin e modeleve në prodhimin për gjuhën daneze.', 'am': 'ለብዙ ቋንቋዎች የስህተት መረጃ ትምህርት ለመጠየቅ የዳታ ሳተር እናቀርባታለን፡፡ የዳታ ሰርቨሮች በዳንንያዊ እና እንደ እንግሊዘኛ FEVER ዳታ ማተሚያ እንደሌላው ፎርማት አላቸው። በቋንቋ ቋንቋ እና ለዳንስ ቋንቋ ምሳሌዎችን ለመፍጠር ይችላል፡፡', 'hy': 'Մենք ներկայացնում ենք տվյալների համակարգ, Դանֆեվերը, որը նշանակում է բազմալեզու սխալ ինֆորմացիայի հետազոտություն: Տվյալների համակարգը Դաներենում է և ունի նույն ձևը, ինչ հայտնի անգլերեն ֆերմեր տվյալների համակարգը: Այն կարող է օգտագործվել բազմալեզու միջոցների փորձարկումների համար, ինչպես նաև Դանիացի լեզվի արտադրման մոդելներ ստեղծելու համար:', 'az': 'Biz çoxlu dil yanlış informasyon araştırmalarına uyğun verilən DanFEVER qurğunu göstəririk. Veri qurğuları Danca dilində və ünlü İngiliz FEVER veri qurğusu ilə eyni formada sahib. Bunu çoxlu dil ayarlarında imtahana çəkmək üçün istifadə edilə bilər, danlıq dilində də modellər yaratmaq üçün istifadə edilə bilər.', 'bs': 'Predstavljamo set podataka, DanFEVER, namjeren za multijezičke istraživanje nepravde. Podaci su na danskom jeziku i imaju isti format kao i poznati podaci engleskog FEVER. Može se koristiti za testiranje metoda u multijezičkim nastavama, kao i za stvaranje modela proizvodnje za danski jezik.', 'cs': 'Představujeme datovou sadu DanFEVER určenou pro vícejazyčný výzkum dezinformací. Datová sada je v dánštině a má stejný formát jako známá anglická datasada FEVER. Může být použit pro testování metod ve vícejazyčném nastavení, stejně jako pro vytváření modelů ve výrobě pro dánský jazyk.', 'ca': 'Presentam un conjunt de dades, DanFEVER, destinat a investigar malament les informacions multilingües. El conjunt de dades està en danès i té el mateix format que el conegut conjunt de dades anglès FEVER. Poden utilitzar-se per a provar mètodes en entorns multilingües, i per crear models en producció per a la llengua danesa.', 'et': 'Esitame andmekogumi DanFEVER, mis on mõeldud mitmekeelseks valeinformatsiooni uurimiseks. Andmekogum on taani keeles ja on samas vormingus kui tuntud inglise FEVER andmekogum. Seda saab kasutada mitmekeelsete meetodite testimiseks ning taani keele tootmise mudelite loomiseks.', 'bn': 'আমরা একটি ডাটাসেট ড্যানাফেভার উপস্থাপন করছি, যা বহুভাষায় ভুল তথ্য গবেষণার উদ্দেশ্য। ড্যাটাসেট ড্যানিশে আছে এবং সাথে পরিচিত ইংরেজী FEVER ডাটাসেটের মতো একই ধরনের ফরম্যাট রয়েছে। এটি বহুভাষার বৈশিষ্ট্যে পরীক্ষার পদ্ধতির জন্য ব্যবহার করা যাবে, আর ড্যানিশ ভাষার জন্য মডেল তৈরি করা যাবে।', 'fi': 'Esitämme DanFEVER-aineiston, joka on tarkoitettu monikieliseen harhatiedon tutkimukseen. Aineisto on tanskaksi ja siinä on sama muoto kuin tunnetussa English FEVER -aineistossa. Sitä voidaan käyttää monikielisten menetelmien testaamiseen sekä tuotantomallien luomiseen tanskan kielelle.', 'ha': "We present a dataset, DanFEVER, intended for multilingual misinformation research.  @ info: whatsthis Ana iya amfani da shi ga jarraba hanyoyin cikin tsarin mulki'ala, da kuma za'a ƙiƙira misãlai cikin manunufi wa harshen Dance.", 'jv': 'Awak dhéwé nambah dataset, danFeVer, supoyo kanggo resmi dadi pakan langgar. dataset iku Danis karo perangkat oleh dadi, dadi Bukak efer data set Inggris Digawe iso nggunakake nggo ujian maneh kanggo nggawe aturan luwih, lan nggo nggawe model kanggo nggawe barang Danis.', 'bo': 'ང་ཚོས་DanFEVER(DanFEVER)ཡི་སྒྲིག་ཆ་འཕྲིན་ཡིག་ཆ་སྣང་མེད་འཚོལ་ཞིབ་བྱེད་ཡོད། The dataset is in Danish and has the same format as the well-known English FEVER dataset. དེ་ནི་སྐད་རིགས་སྒྲིག་འགོད་ནང་གི་ཐབས་ལམ་ལྟར་བརྟག', 'sk': 'Predstavljamo podatkovni niz DanFEVER, namenjen večjezičnim raziskovanjem napačnih informacij. Zbirka podatkov je v danščini in ima enako obliko kot znana angleška zbirka podatkov FEVER. Uporablja se lahko za testiranje metod v večjezičnih okoljih, kot tudi za ustvarjanje modelov v proizvodnji za danski jezik.', 'he': 'אנחנו מציגים קבוצת נתונים, DanFEVER, המתכוונת למחקר מידע שגוי רב-שפתי. קבוצת המידע היא בדנית ויש לה אותו פורמט כמו קבוצת המידע האנגלית FEVER הידועה. It can be used for testing methods in multilingual settings, as well as for creating models in production for the Danish language.'}
{'en': 'NorDial : A Preliminary Corpus of Written Norwegian Dialect Use', 'es': 'NorDial: un corpus preliminar del uso del dialecto noruego escrito', 'ar': 'NorDial: مجموعة أولية لاستخدام اللهجة النرويجية المكتوبة', 'pt': 'NorDial: um corpus preliminar de uso escrito do dialeto norueguês', 'fr': "NorDial\xa0: un corpus préliminaire d'utilisation écrite du dialecte norvégien", 'ja': 'NorDial:ノルウェー語の方言使用の事前コーパス', 'ru': 'NorDial: Предварительный корпус письменного использования норвежского диалекта', 'zh': 'NorDial曰:挪威语方言初语料库', 'hi': 'NorDial: लिखित नॉर्वेजियन बोली उपयोग का एक प्रारंभिक कॉर्पस', 'ga': 'NorDial: Réamhchorp d’Úsáid Scríofa Chanúint na hIorua', 'ka': 'Name', 'hu': 'NorDial: Az írott norvég tárcsázás előzetes testülete', 'el': 'Ένα προκαταρκτικό σώμα γραπτής νορβηγικής χρήσης διαλέκτων', 'kk': 'Norwegian Dialection', 'lt': 'NorDial: A preliminary Corpus of Written Norwegian Dialect Use', 'mk': 'NorDial: Прелиминарен корпус со пишана норвешка дијалектна употреба', 'ms': 'NorDial: A Preliminary Corpus of Written Norwegian Dialect Use', 'mn': 'Norwegian Dialect Use', 'mt': 'NorDial: A Preliminary Corpus of Written Norwegian Dialect Use', 'it': 'NorDial: Un corpo preliminare di uso scritto norvegese dialetto', 'no': 'Nordmål: Eit første korpus av skriven norsk dialektbruk', 'sr': 'Nordial: Preliminarna korpusa napisanog Norveškog dijalekta', 'ml': 'NorDial: A Preliminary Corpus of Written Norwegian Dialect Use', 'ro': 'NorDial: Un corpus preliminar al utilizării dialectelor scrise norvegiene', 'pl': 'NorDial: Wstępny korpus pisemnego dialektu norweskiego', 'ta': 'Norwegian Dialect Use', 'si': 'Name', 'sv': 'NorDial: En preliminär korpus av skriftlig norsk dialekt användning', 'so': 'Nordial: A Preliminary Corpus of Written Norwegian Dialect Use', 'ur': 'Norwegian Dialect Use کی ایک پہلی جگہ', 'vi': 'Biên dịch: Tập Trước của cấu trúc cấu trúc cấu trúc cấu trúc cấu tạo', 'uz': 'Norvegcha diagramma foydalanish', 'bg': 'Предварителен корпус за писмено използване на норвежки диалект', 'hr': 'NorDial: Preliminarna korpusa napisanog Norveškog dijalekta', 'de': 'NorDial: Ein vorläufiges Korpus der schriftlichen norwegischen Dialektnutzung', 'nl': 'NorDial: Een voorlopig korpus van geschreven Noorse dialectgebruik', 'ko': '북유럽어: 노르웨이 사투리 서면 사용의 초보 어료 라이브러리', 'fa': 'Name', 'da': 'NorDial: Et foreløbigt korpus af skriftlig norsk dialekt brug', 'tr': 'Norwegiýa: ýazylan Norwegiýanyň öňki bölegi', 'af': "Noorddial: ' n Preliminary Corpus van Geskryf Noorweë Dialeksie Gebruik", 'sw': 'Nordial: Kikosi cha awali cha Utumiaji wa Tamko la Kiingereza', 'hy': 'Նորվեգական դիալեկտի գրված օգտագործման նախաձեռնական կորպուսը', 'sq': 'NorDial: A Preliminary Corpus of Written Norwegian Dialect Usage', 'bn': 'নরডায়াল: লেখা নরওয়েজিয়ান ডায়ালেক্টর ব্যবহারের প্রাথমিক কোর্পাস', 'az': 'Norveçə Çağırış: Yazılmış Norveçə Dialekt Qullanımının əvvəlki Corpusu', 'bs': 'NorDial: Preliminarna korpusa napisanog Norveškog dijalekta', 'cs': 'NorDial: Předběžný sbor písemného norského dialektu', 'id': 'NorDial: A Preliminary Corpus of Written Norwegian Dialect Use', 'et': 'NorDial: Norra kirjaliku dialekti kasutamise esialgne korpus', 'fi': 'NorDial: Alustava korpus kirjallisen norjalaisen dialektin käytöstä', 'am': 'NorDial: A Preliminary Corpus of Written Norwegian Dialect Use', 'ca': "NorDial: Un Corpus Preliminari d'Us Escrit de Dialecta Noruega", 'jv': 'Nordial', 'ha': 'KCharselect unicode block name', 'he': 'NorDial: A Preliminary Corpus of Written Norwegian Dialect Usage', 'sk': 'NorDial: Predhodni korpus pisane norveške uporabe dialekta', 'bo': 'སྤྱིར་བཏང་མྱོང་། རྩོམ་ཡིས་པའི་སྔོན་འཛུགས་ཀྱི་མདོག'}
{'en': 'Norway has a large amount of dialectal variation, as well as a general tolerance to its use in the public sphere. There are, however, few available resources to study this variation and its change over time and in more informal areas, on social media. In this paper, we propose a first step to creating a corpus of dialectal variation of written Norwegian. We collect a small corpus of tweets and manually annotate them as Bokml, Nynorsk, any dialect, or a mix. We further perform preliminary experiments with state-of-the-art models, as well as an analysis of the data to expand this corpus in the future. Finally, we make the annotations available for future work.', 'ar': 'تتمتع النرويج بقدر كبير من الاختلاف في اللهجات ، فضلاً عن التسامح العام لاستخدامها في المجال العام. ومع ذلك ، هناك القليل من الموارد المتاحة لدراسة هذا التباين وتغيره بمرور الوقت وفي المناطق غير الرسمية على وسائل التواصل الاجتماعي. في هذه الورقة ، نقترح الخطوة الأولى لإنشاء مجموعة متنوعة من اللهجات للنرويجية المكتوبة. نقوم بتجميع مجموعة صغيرة من التغريدات ونقوم بتعليقها يدويًا مثل Bokmål أو Nynorsk أو أي لهجة أو مزيج. نقوم كذلك بإجراء تجارب أولية على أحدث النماذج ، بالإضافة إلى تحليل البيانات لتوسيع هذه المجموعة في المستقبل. أخيرًا ، نجعل التعليقات التوضيحية متاحة للعمل في المستقبل.', 'pt': 'A Noruega tem uma grande quantidade de variação dialetal, bem como uma tolerância geral ao seu uso na esfera pública. Existem, no entanto, poucos recursos disponíveis para estudar essa variação e sua mudança ao longo do tempo e em áreas mais informais, nas mídias sociais. Neste artigo, propomos um primeiro passo para a criação de um corpus de variação dialetal da escrita norueguesa. Coletamos um pequeno corpus de tweets e os anotamos manualmente como Bokmål, Nynorsk, qualquer dialeto ou uma mistura. Realizamos ainda experimentos preliminares com modelos de última geração, bem como uma análise dos dados para ampliar esse corpus no futuro. Por fim, disponibilizamos as anotações para trabalhos futuros.', 'fr': "La Norvège présente de nombreuses variations dialectales, ainsi qu'une tolérance générale à l'égard de son utilisation dans la sphère publique. Il existe cependant peu de ressources disponibles pour étudier cette variation et son évolution au fil du temps et dans des domaines plus informels, sur les réseaux sociaux. Dans cet article, nous proposons une première étape pour créer un corpus de variations dialectales du norvégien écrit. Nous collectons un petit corpus de tweets et les annotons manuellement en bokmål, nynorsk, n'importe quel dialecte ou un mélange. Nous effectuons également des expériences préliminaires avec des modèles de pointe, ainsi qu'une analyse des données afin d'élargir ce corpus à l'avenir. Enfin, nous mettons les annotations à disposition pour les travaux futurs.", 'es': 'Noruega tiene una gran variación dialectal, así como una tolerancia general a su uso en la esfera pública. Sin embargo, hay pocos recursos disponibles para estudiar esta variación y su cambio a lo largo del tiempo y en áreas más informales, en las redes sociales. En este artículo, proponemos un primer paso para crear un corpus de variación dialectal del noruego escrito. Recopilamos un pequeño corpus de tuits y los anotamos manualmente como Bokmål, Nynorsk, cualquier dialecto o una mezcla. Además, realizamos experimentos preliminares con modelos de última generación, así como un análisis de los datos para ampliar este corpus en el futuro. Por último, hacemos que las anotaciones estén disponibles para trabajos futuros.', 'ja': 'ノルウェーには多くの方言バリエーションがあり、公共の場での使用にも一般的に容認されています。しかし、ソーシャルメディア上のより非公式な領域で、このバリエーションとその経時的な変化を研究するための利用可能なリソースはほとんどありません。本稿では、ノルウェー語で書かれた方言変形のコーパスを作成するための最初のステップを提案する。少量のツイートを収集し、手動でBokmål、Nynorsk、あらゆる方言、またはミックスとして注釈を付けます。さらに、最先端モデルの予備実験と、今後このコーパスを拡大するためのデータの分析を行います。最後に、注釈を今後の作業に利用できるようにします。', 'hi': 'नॉर्वे में बड़ी मात्रा में बोलचाल की भिन्नता है, साथ ही साथ सार्वजनिक क्षेत्र में इसके उपयोग के लिए एक सामान्य सहिष्णुता भी है। हालांकि, इस भिन्नता और समय के साथ और अधिक अनौपचारिक क्षेत्रों में, सोशल मीडिया पर इसके परिवर्तन का अध्ययन करने के लिए कुछ उपलब्ध संसाधन हैं। इस पेपर में, हम लिखित नॉर्वेजियन की बोली भिन्नता का एक कॉर्पस बनाने के लिए पहला कदम प्रस्तावित करते हैं। हम tweets का एक छोटा सा कॉर्पस इकट्ठा करते हैं और मैन्युअल रूप से उन्हें Bokmål, Nynorsk, किसी भी बोली, या एक मिश्रण के रूप में एनोटेट करते हैं। हम आगे अत्याधुनिक मॉडल के साथ प्रारंभिक प्रयोग करते हैं, साथ ही साथ भविष्य में इस कॉर्पस का विस्तार करने के लिए डेटा का विश्लेषण भी करते हैं। अंत में, हम भविष्य के काम के लिए एनोटेशन उपलब्ध कराते हैं।', 'ru': 'В Норвегии существует большое разнообразие диалектов, а также общая терпимость по отношению к их использованию в общественной сфере. Вместе с тем в социальных сетях имеется мало ресурсов для изучения этой вариации и ее изменения с течением времени и в более неформальных областях. В этой статье мы предлагаем первый шаг к созданию корпуса диалектных вариаций письменности норвежского языка. Мы собираем небольшой корпус твитов и вручную аннотируем их как Bokmål, Nynorsk, любой диалект или смесь. Далее мы проводим предварительные эксперименты с новейшими моделями, а также анализ данных для расширения этого корпуса в будущем. Наконец, мы предоставляем аннотации для будущей работы.', 'zh': '挪威有大言变化,及公共域之用遍容之。 然鲜可用之资,可以社交媒体论变化,与时推移,非正域之变也。 本文中,开创书挪威语方言变体语料库第一。 收一小段推文,手动注为博克马尔语,尼诺斯克语,一切方言混合语。 更用先进之初实验,析其数,以广语料库于未来。 最后,我们将注释将来的事务。', 'ga': 'Tá éagsúlacht mhór canúintí ag an Iorua, chomh maith le lamháltas ginearálta maidir lena húsáid sa sféar poiblí. Is beag acmhainní atá ar fáil, áfach, chun staidéar a dhéanamh ar an éagsúlacht seo agus ar an athrú atá air le himeacht ama agus i réimsí níos neamhfhoirmiúla, ar na meáin shóisialta. Sa pháipéar seo, molaimid an chéad chéim chun corpas d’éagsúlacht chanúintí den Ioruais scríofa a chruthú. Bailímid corpas beag tweets agus déanaimid anótáil orthu de láimh mar Bokmål, Nua-Ioruais, canúint ar bith, nó meascán. Déanaimid tuilleadh turgnaimh le samhlacha úrscothacha, chomh maith le hanailís ar na sonraí chun an corpas seo a leathnú amach anseo. Ar deireadh, cuirimid na nótaí ar fáil le haghaidh oibre amach anseo.', 'ka': 'ნორვეგიაში დიალექტალური განცვლების დიალექტალური რაოდენობა, როგორც საერთო ტელექტალური თავის გამოყენებას საშუალო სფერაში. მაგრამ არსებობს რამდენიმე ხელსაწყოფილი რესურსები, რომლებიც ამ ცვლილებას და მისი ცვლილებას დროს და უფრო ინფორმალური ფართებში, საზოგადო მედი ჩვენ პირველი ნაწილის შექმნა დიალექტალური განცვლების კორპუსს დავიქმნა. ჩვენ შევქმნით პატარა კორპუს ტივიტების და მანძილურად ისინი ბოკმალურად, ნინორსკურად, ყველა დიალექტი, ან სიმბოლოს. ჩვენ უკვე გავაკეთებთ პირველი ექსპერიმენტები, რომელიც მონაცემების ანალიზაცია, რომ ამ კორპუსს მომავალში გაფართოთ. ბოლოს, ჩვენ მომავალეთ სამუშაო სამუშაო წარმოდგენება.', 'hu': 'Norvégia nagy mennyiségű dialektuális változattal rendelkezik, valamint általános toleranciával rendelkezik a nyilvános szférában történő használatával szemben. Mindazonáltal kevés forrás áll rendelkezésre ennek a változásnak és annak időbeli változásának tanulmányozására, illetve informálisabb területeken, a közösségi médiában. Ebben a tanulmányban egy első lépést javasolunk az írott norvég dialektuális variációs korpusz létrehozásához. Összegyűjtünk egy kis korpuszt tweetekből, és manuálisan jegyzeteljük őket Bokmal, Nynorsk, bármilyen dialektus, vagy keverék. Továbbá előzetes kísérleteket végzünk korszerű modellekkel, valamint elemzést végzünk az adatokról a korpusz jövőbeni bővítése érdekében. Végül elérhetővé tesszük a megjegyzéseket a jövőbeli munkákhoz.', 'it': "La Norvegia ha una grande quantità di variazioni dialettali, così come una tolleranza generale al suo uso nella sfera pubblica. Sono però poche le risorse disponibili per studiare questa variazione e il suo cambiamento nel tempo e in aree più informali, sui social media. In questo articolo, proponiamo un primo passo per creare un corpus di variazione dialettale del norvegese scritto. Raccogliamo un piccolo corpus di tweet e li annotiamo manualmente come Bokmal, Nynorsk, qualsiasi dialetto, o un mix. Eseguiamo inoltre esperimenti preliminari con modelli all'avanguardia, così come un'analisi dei dati per espandere questo corpus in futuro. Infine, mettiamo a disposizione le annotazioni per lavori futuri.", 'lt': 'Norvegija turi didelį dialektinės nuokrypio dydį ir bendrą toleranciją jos naudojimui viešojoje sferoje. There are, however, few available resources to study this variation and its change over time and in more informal areas, on social media.  Šiame dokumente siūlome pirmą žingsnį kuriant rašytinio norvegų dialektinės variacijos korpusą. Surinkame mažą tweetų korpusą ir rankiniu būdu juos anotuojame kaip Bokmal, Nynorsk, bet kokį dialektą ar mišinį. We further perform preliminary experiments with state-of-the-art models, as well as an analysis of the data to expand this corpus in the future.  Galiausiai pateikiame pastabas būsimam darbui.', 'el': 'Η Νορβηγία έχει μεγάλη διαλεκτική διακύμανση, καθώς και γενική ανοχή στη χρήση της στη δημόσια σφαίρα. Ωστόσο, υπάρχουν λίγοι διαθέσιμοι πόροι για τη μελέτη αυτής της παραλλαγής και της αλλαγής της με την πάροδο του χρόνου και σε πιο ανεπίσημους τομείς, στα μέσα κοινωνικής δικτύωσης. Στην παρούσα εργασία, προτείνουμε ένα πρώτο βήμα για τη δημιουργία ενός σώματος διαλεκτικής παραλλαγής των γραπτών νορβηγικών. Συλλέγουμε ένα μικρό σώμα tweets και τα σχολιάζουμε χειροκίνητα ως Bokmal, Nynorsk, οποιαδήποτε διάλεκτο, ή ένα μείγμα. Περαιτέρω διεξάγουμε προκαταρκτικά πειράματα με μοντέλα τελευταίας τεχνολογίας, καθώς και ανάλυση των δεδομένων για την επέκταση αυτού του σώματος στο μέλλον. Τέλος, θέτουμε τα σχόλια διαθέσιμα για μελλοντικές εργασίες.', 'ms': 'Norway mempunyai variasi dialektal yang besar, serta toleransi umum terhadap penggunaannya di sfera awam. Namun, terdapat sedikit sumber yang tersedia untuk mempelajari variasi ini dan perubahannya melalui masa dan di kawasan yang lebih informal, pada media sosial. Dalam kertas ini, kami cadangkan langkah pertama untuk mencipta korpus variasi dialektal dari Norwegian tertulis. Kami mengumpulkan korpus kecil tweet dan secara manual anotasinya sebagai Bokmal, Nynorsk, mana-mana dialekt, atau campuran. We further perform preliminary experiments with state-of-the-art models, as well as an analysis of the data to expand this corpus in the future.  Finally, we make the annotations available for future work.', 'ml': 'നോര്\u200dവേയില്\u200d അതിന്റെ ഉപയോഗിക്കാന്\u200d ഒരു പൊതുവായ ക്ഷമ ഉണ്ട്. എന്നാലും ഈ വ്യത്യാസവും സമയത്തിനുമുമ്പ് അതിന്റെ മാറ്റങ്ങളും കൂടുതല്\u200d അപരിചിതമായ പ്രദേശങ്ങളിലും സോഷ്യല്\u200d മീഡിയിലും  ഈ പത്രത്തില്\u200d, നോര്\u200dവേജിയന്\u200d എഴുതിയ വ്യത്യാസങ്ങളുടെ കോര്\u200dപ്പുസ് ഉണ്ടാക്കാന്\u200d ഞങ്ങള്\u200d ആദ്യ പടിയാണ് പ്രായശ്ചിത് We collect a small corpus of tweets and manually annotate them as Bokmal, Nynorsk, any dialect, or a mix.  നമ്മള്\u200d പിന്നീട് ആദ്യ പരീക്ഷണങ്ങള്\u200d പ്രവര്\u200dത്തിപ്പിക്കുന്നു. ഭാവിയില്\u200d ഈ കോര്\u200dപ്പുസിനെ വികസിപ്പിക്കാനുള്ള ഡേറ്റാകളു അവസാനം, നമ്മള്\u200d ഭാവിയുടെ ജോലിക്ക് വേണ്ടി പ്രശ്നങ്ങള്\u200d ലഭിക്കുന്നു.', 'kk': 'Норвегия диалекталдық өзгерістер көп саны бар, және көпшілік сферада қолдану жалпы толерансы бар. Бірақ бұл өзгерістерді және өзгерістерді уақытта және көбірек аумақтарда, социалдық медиақтарда зерттеу үшін бірнеше ресурс бар. Бұл қағаздың бірінші қадамы Норвегияны жазылған диалектикалық айнымалылық корпус құру үшін қолданамыз. Біз кішкентай tweet корпус жинап, оларды бокмал, Нинорск, кез-келген диалект немесе аралас ретінде белгілеп береміз. Біз осы корпусты келесіде үлкейту үшін алдындағы тәжірибелерді жасаймыз. Соңында, келесі жұмыс үшін жазбаларды қол жеткіземіз.', 'mk': 'Norway has a large amount of dialectal variation, as well as a general tolerance to its use in the public sphere.  There are, however, few available resources to study this variation and its change over time and in more informal areas, on social media.  In this paper, we propose a first step to creating a corpus of dialectal variation of written Norwegian.  We collect a small corpus of tweets and manually annotate them as Bokmal, Nynorsk, any dialect, or a mix.  Понатаму спроведуваме прелиминарни експерименти со најнови модели, како и анализа на податоците за проширување на овој корпус во иднина. Конечно, ги поставуваме анотациите достапни за идната работа.', 'mn': 'Норвегид олон хэмжээний диалектикийн өөрчлөлт, нийтийн хэмжээнд ашиглах ерөнхий tolerance байдаг. Гэхдээ энэ өөрчлөлтийг судлах, хугацаанд өөрчлөлтийг, илүү мэдээллийн хэсэгт, нийгмийн хэвлэл дээр хэдэн баялаг байдаг. Энэ цаасан дээр бид норвегийн бичигдсэн диалектикийн өөрчлөлтийг бүтээх эхний алхам санал болгож байна. Бид жижиг tweet корпус цуглуулж, тэднийг Bokmal, Nynorsk, ямар ч диалект, эсвэл холбоотой гэдгийг гараараа илтгэдэг. Бид дараа нь урлагийн загвартай эхний туршилт хийдэг, мөн ирээдүйд энэ корпус-г нэмэгдүүлэхэд өгөгдлийн шинжилгээ хийдэг. Эцэст нь бид ирээдүйн ажлын тулд анзаарлыг ашиглаж чадна.', 'pl': 'Norwegia ma dużą różnorodność dialektalną, a także ogólną tolerancję na jej stosowanie w sferze publicznej. Istnieje jednak niewiele dostępnych środków, aby zbadać tę zmianę i zmiany w czasie oraz w bardziej nieformalnych obszarach, w mediach społecznościowych. W niniejszym artykule proponujemy pierwszy krok do stworzenia korpusu dialektalnej wariacji pisanego norweskiego. Zbieramy mały korpus tweetów i ręcznie adnotatujemy je jako Bokmal, Nynorsk, dowolny dialekt lub mix. Ponadto przeprowadzamy wstępne eksperymenty z najnowocześniejszymi modelami, a także analizę danych w celu rozszerzenia tego korpusu w przyszłości. Na koniec udostępniamy adnotacje do przyszłych prac.', 'ro': 'Norvegia are o mare varietate dialectală, precum și o toleranță generală la utilizarea sa în sfera publică. Cu toate acestea, există puține resurse disponibile pentru a studia această variație și schimbarea sa în timp și în domenii mai informale, pe rețelele de socializare. În această lucrare, propunem un prim pas spre crearea unui corpus de variații dialectale ale limbii norvegiene scrise. Colectăm un mic corp de tweet-uri și le adnotam manual ca Bokmal, Nynorsk, orice dialect sau un mix. În continuare, efectuăm experimente preliminare cu modele de ultimă generație, precum și o analiză a datelor pentru a extinde acest corp în viitor. În cele din urmă, punem adnotările la dispoziție pentru lucrările viitoare.', 'mt': 'Norway has a large amount of dialectal variation, as well as a general tolerance to its use in the public sphere.  There are, however, few available resources to study this variation and its change over time and in more informal areas, on social media.  F’dan id-dokument, qed nipproponu l-ewwel pass lejn il-ħolqien ta’ korpus ta’ varjazzjoni dijalettika ta’ Norveġiż bil-miktub. Aħna nġabru korpus żgħir ta’ tweets u nnutawhom manwalment bħala Bokmal, Nynorsk, kwalunkwe dijalekt, jew taħlita. Barra minn hekk, nagħmlu esperimenti preliminari b’mudelli moderni, kif ukoll analiżi tad-dejta biex dan il-korpus jiġi estiż fil-futur. Finally, we make the annotations available for future work.', 'sr': 'Norveška ima veliku količinu dijalektalnih varijacija, kao i generalnu toleranciju za njegovu upotrebu u javnoj sferi. Međutim, na društvenim medijima postoji nekoliko dostupnih resursa za proučavanje te varijacije i promjene tijekom vremena i u više neformalnih područja. U ovom papiru predlažemo prvi korak da stvorimo korpus dijalektne varijacije napisanog norveškog. Skupljamo mali korpus tweeta i ruèno ih annotiramo kao Bokmal, Nynorsk, bilo koji dijalekt ili miks. Dalje izvršavamo preliminarni eksperimenti sa modelima države umjetnosti, kao i analizu podataka za proširenje ovog korpusa u budućnosti. Konaèno, objavljujemo annotacije za buduæi posao.', 'so': 'Norway waxaa ku yaal isbedelyo badan oo kala duduwan, sidoo kale dulqaadashada guud ee isticmaalka. Si kastaba ha ahaatee waxaa ku jira in yar oo hanti ah oo la heli karo si ay u barato isbedelkan iyo beddelkeeda waqtiyada iyo meelaha kale oo aan sharciga laheyn waxay ku jiraan macluumaadka bulshada. Qoraalkan waxaynu soo jeedaynaa koob ugu horraysa si aan u abuurno koob kala duwan oo qoran Norwey. Waxaannu soo ururinnaa weelal yar oo Twitteetka ah, waxaana si gacan ah looga macaamiloonaa Bokmael, Nynorsk, cudur kasta ama isku qasnaan. We further perform preliminary experiments with state-of-the-art models, as well as an analysis of the data to expand this corpus in the future.  Ugu dambaysta, waxaynu sameynaa dhibaatooyin ay ku jiraan shaqada mustaqbalka.', 'sv': 'Norge har en stor mängd dialektiska variationer, liksom en allmän tolerans mot dess användning i den offentliga sfären. Det finns dock få resurser att studera denna variation och dess förändring över tid och på mer informella områden, på sociala medier. I denna uppsats föreslår vi ett första steg för att skapa en korpus av dialektisk variation av skriven norska. Vi samlar in en liten samling tweets och manuellt kommenterar dem som Bokmal, Nynorsk, vilken dialekt som helst eller en blandning. Vi utför vidare preliminära experiment med toppmoderna modeller samt en analys av data för att utöka denna korpus i framtiden. Slutligen gör vi anteckningar tillgängliga för framtida arbete.', 'ta': 'நார்வேயில் பொது காலத்தில் பயன்படுத்துவதற்கான பொதுவான பொறுமையும் உள்ளது. ஆனால், இந்த மாறுபாடு மற்றும் நேரத்திற்கு மேலும் மாற்றம் மற்றும் அதிக அரசாங்கமான பிரிவுகளில், சமூக ஊடகங்களில் உள இந்த காகிதத்தில், நாம் எழுதப்பட்ட நார்வெகிய மாற்றங்களை உருவாக்க ஒரு முதல் படியை பரிந்துரைக்கிறோம். நாம் ஒரு சிறிய கட்டுரையை சேகரித்து கைம்முறையாக அவர்களை போக்மால், நினோர்ஸ்க், எந்த விளக்கச்சீட்டு அல்லது கலந்து என்று க மேலும் நாம் முன்னிருப்பு சோதனைகளை மாதிரி மாதிரிகளுடன் செயல்படுத்துகிறோம், மற்றும் எதிர்காலத்தில் இந்த கோப்புகளை விரி இறுதியில், நாம் எதிர்கால வேலை கிடைக்கும் அறிவிப்புகளை செய்கிறோம்.', 'no': 'Norge har stor mengd dialektiske variasjonar, og ein generell tolerans til bruken i den offentlege sfera. Det finst imidlertid få tilgjengelege ressursar å studiera denne variasjonen og endringen over tid og i meir informalt område på sosiale media. I denne papiret foreslår vi ein første steg til å laga ein korpus av dialektiske variasjonar av skriven norsk. Vi samler ein liten korpus av tweeter og oppmerker dei manuelt som Bokmal, Nynorsk, eit dialekt eller ei mix. Vi utfører framtidige eksperimenter med kunstmodeller, og ein analyse av data for å utvida denne korpusen i framtida. Til slutt gjør vi notasjonane tilgjengelege for framtidige arbeid.', 'ur': 'نوروژ کے پاس بہت بڑی اندازہ ڈیلکتل تغییر ہے اور اس کے استعمال کے لئے عمومی ثابت ہے۔ لیکن اس تغییر اور اس کی تغییر کی تعلیم کے لئے بہت کم موجود منابع موجود ہیں، سوسیل میڈیا میں۔ اس کاغذ میں ہم ایک پہلی قدم کی پیشنهاد کریں کہ نوروژی کے نامۂ دائیلکتی تغییرات کی ایک جسم پیدا کریں۔ ہم ایک چھوٹے ٹیوٹ کے کورپوس جمع کرتے ہیں اور انہیں بوکمل، نینورسک، کوئی ڈیلکت یا میکس بناتے ہیں۔ ہم اس سے پہلے آزمائش کریں گے کہ اس کورپوس کو مستقبل میں پھیلائیں۔ آخر میں، ہم آگے کے کام کے لئے اظہار کرنے کے لئے موجود بناتے ہیں.', 'si': 'නෝර්වෙයියාට ලොකු ප්\u200dරමාණයක් තියෙනවා, සහ සාමාන්\u200dය ප්\u200dරමාණයක් තියෙනවා. නමුත්, මේ වෙනස් විදියට පරීක්ෂා කරන්න සහ ඒක වෙනස් වෙන්න සහ සාමාජික මාධ්\u200dයානයේ වඩා විස්තර විදියට ප්\u200d මේ පත්තරේ අපි ප්\u200dරථම පැත්තක් සැලසුම් කරනවා ලියපු නෝර්වේජියාන්ගේ කොර්පුස් එකක් නිර්මාණය කරන්න. අපි ට්විට් වල පුංචි කොර්පස් එකක් එකතු කරනවා ඒ වගේම ඔවුන්ව බොක්මල්, නිනෝර්ස්ක්, කිසිම දායිලෙක්ට් එකක් වගේම අපි තව ප්\u200dරධාන පරීක්ෂණාවක් කරනවා, අනාගතයේ මේ කොර්පුස් විස්තර කරන්න දත්තේ විශ්ලේෂණයක් වගේම. අන්තිමේදි, අපි අනාගතය වැඩේ වෙනුවෙන් ප්\u200dරවේශ කරන්න පුළුවන් වෙනවා.', 'uz': "Norvegiya har bir nechta dialektal variasjonening ko'p o'zgarishlari, va public sphereda ishlatish uchun umumiy tolerance bor. Lekin, bu variasjonen va vaqtning o'zgarishlari va ko'proq haqida jamiyat media'da o'rganish uchun juda qisqa mavjud resources mavjud. In this paper, we propose a first step to creating a corpus of dialectal variation of written Norwegian.  Biz bir kichkina xabarlarni olib qo'lbola'llamiz va ularni Bokmal, Nynorsk, hech qanday dialek yoki bir qanchalik ko'plab kelamiz. Biz birinchi birinchi tajribalarni holatning holati modellari bilan bajaramiz, va kelajakda bu korpusni ajratish uchun maʼlumotni analyzeriz. Endi biz kelajakdagi ish uchun tajribalarni bajaramiz.", 'vi': 'Na Uy có rất nhiều ngôn ngữ khác biệt, cũng như một sự khoan dung chung với việc sử dụng nó trong lĩnh vực công cộng. Tuy nhiên, có rất ít nguồn lực có thể nghiên cứu sự biến đổi này qua thời gian và các lĩnh vực còn chưa chính thức, về các phương tiện truyền thông xã hội. Trong tờ giấy này, chúng tôi đề nghị bước đầu tiên để tạo ra một tập thể biến đổi phương ngữ của người Na Uy viết. Chúng tôi sưu tập tập một tập nhỏ các tweet và tự ghi âm chúng là Bozman, Nynorsk, bất kỳ phương ngữ nào, hay một hỗn hợp. Chúng tôi còn tiến hành thí nghiệm sơ bộ với các mô hình hiện đại, cũng như một phân tích dữ liệu để mở rộng tập thể này trong tương lai. Cuối cùng, chúng tôi sẽ đưa chú thích cho công việc tương lai.', 'bg': 'Норвегия има голямо количество диалектални вариации, както и обща толерантност към използването му в обществената сфера. Има обаче малко налични ресурси за изучаване на тази вариация и нейната промяна във времето и в по-неформални области, в социалните медии. В настоящата статия предлагаме първата стъпка към създаването на корпус от диалектална вариация на писмения норвежки език. Събираме малък корпус от туитове и ръчно ги анотираме като Бокмал, Нинорск, всеки диалект или микс. Освен това извършваме предварителни експерименти с най-съвременни модели, както и анализ на данните за разширяване на този корпус в бъдеще. И накрая, ние правим анотациите достъпни за бъдеща работа.', 'da': 'Norge har en stor mængde dialektiske variationer, samt en generel tolerance over for dets brug i den offentlige sfære. Der er imidlertid få ressourcer til rådighed til at studere denne variation og dens ændringer over tid og på mere uformelle områder, på sociale medier. I denne artikel foreslår vi et første skridt til at skabe et korpus af dialektiske variationer af skrevet norsk. Vi indsamler et lille korpus af tweets og manuelt noterer dem som Bokmal, Nynorsk, enhver dialekt eller en blanding. Vi udfører desuden foreløbige eksperimenter med state-of-the-art modeller samt en analyse af data for at udvide dette korpus i fremtiden. Endelig stiller vi noterne til rådighed til fremtidigt arbejde.', 'de': 'Norwegen hat eine große dialektale Variation sowie eine allgemeine Toleranz gegenüber seiner Verwendung in der Öffentlichkeit. Es gibt jedoch nur wenige Ressourcen, um diese Variation und ihre Veränderung im Laufe der Zeit und in informelleren Bereichen in sozialen Medien zu untersuchen. In diesem Beitrag schlagen wir einen ersten Schritt zur Schaffung eines Korpus dialektaler Variation des geschriebenen Norwegischen vor. Wir sammeln einen kleinen Korpus von Tweets und kommentieren sie manuell als Bokmal, Nynorsk, irgendeinen Dialekt oder eine Mischung. Weiterhin führen wir Vorversuche mit modernsten Modellen durch und analysieren die Daten, um diesen Korpus zukünftig zu erweitern. Schließlich stellen wir die Anmerkungen für zukünftige Arbeiten zur Verfügung.', 'hr': 'Norveška ima veliku količinu dijalektalnih varijacija, kao i općeg tolerancije za njegovu primjenu u javnoj sferi. Međutim, na društvenim medijima postoji nekoliko dostupnih resursa za proučavanje te promjene tijekom vremena i u više neformalnih područja. U ovom papiru predlažemo prvi korak za stvaranje korpusa dijalektne varijacije napisanog norveškog. Skupljamo mali korpus tweeta i ručno ih annotiramo kao Bokmal, Nynorsk, bilo koji dijalekt ili mješaj. Dalje izvršavamo preliminarni eksperimenti s modelima države umjetnosti, kao i analizu podataka za proširenje ovog korpusa u budućnosti. Napokon, objavljujemo napomene za budući posao.', 'fa': 'نوروژه تعداد زیادی از تغییرات دیالکتی دارد، همچنین تحمل عمومی برای استفاده از آن در منطقه عمومی دارد. ولی کمی منابع موجود برای مطالعه این تغییر و تغییرش در طول زمان و در منطقه\u200cهای غیر معمولی، در رسانه\u200cهای اجتماعی وجود دارد. در این کاغذ، ما پیشنهاد می کنیم یک قدم اول برای ایجاد یک جسد از تغییرات دیالکتی نوروژی نوشته شده. ما یک کورپوس کوچک از توئیت جمع می\u200cکنیم و به طور دستی آنها را به عنوان بوکمل، نینورسک، هر دیالکت یا ترکیب می\u200cدهیم. ما آزمایشات پیشینیانی را با مدل های دولتی هنر انجام می دهیم، و تحلیل داده ها را برای گسترش این کورپوس در آینده انجام می دهیم. بالاخره، ما اخطار را برای کار آینده در دسترس می دهیم.', 'nl': 'Noorwegen heeft een grote hoeveelheid dialectische variatie, evenals een algemene tolerantie voor het gebruik ervan in de publieke sfeer. Er zijn echter weinig middelen beschikbaar om deze variatie en de verandering ervan in de loop van de tijd en op meer informele gebieden, op sociale media te bestuderen. In dit artikel stellen we een eerste stap voor om een corpus van dialectische variatie van geschreven Noors te creëren. We verzamelen een klein corpus tweets en annoteren ze handmatig als Bokmal, Nynorsk, elk dialect of een mix. Verder voeren we voorlopige experimenten uit met state-of-the-art modellen, evenals een analyse van de gegevens om dit corpus in de toekomst uit te breiden. Tot slot stellen we de annotaties beschikbaar voor toekomstig werk.', 'ko': '노르웨이는 대량의 사투리 변체를 가지고 있으며 공공 분야에서 사투리를 사용하는 것을 보편적으로 용인한다.그러나 소셜 미디어에서 이러한 변화와 시간에 따른 변화, 그리고 더 비공식적인 분야를 연구할 수 있는 자원이 거의 없다.본고에서 우리는 서면 노르웨이어 방언 변체 어료 라이브러리를 만드는 첫걸음을 제시했다.우리는 일부 추문을 수집하여 수동으로 그것들을 보크마르어, 니노르스크어, 어떤 사투리, 혼합어로 표시했다.우리는 더욱 선진적인 모델을 이용하여 초보적인 실험을 하고 데이터를 분석함으로써 미래에 이 어료 라이브러리를 확장할 것이다.마지막으로, 우리는 미래에 제공할 일을 주석할 것이다.', 'af': "Noorweë het 'n groot hoeveelheid dialektiese veranderinge, asook 'n algemene toleransie aan sy gebruik in die openbare sfer. Daar is tog 'n paar beskikbaar hulpbronne om hierdie veranderinge en sy veranderinge oor tyd en in meer onformele gebiede te ondersoek op sosiale media. In hierdie papier voorstel ons 'n eerste stap om 'n korpus van dialektiese verandering van geskrywe Noorweërs te skep. Ons versamel 'n klein korpus van tweets en hulle hand aanannoteer as Bokmal, Nynorsk, enige dialekte of 'n gemink. Ons verder uitvoer vooraf eksperimente met state-of-the-art modele, en ook 'n analiseer van die data om hierdie korpus in die toekoms te uitbrei. Eindelik maak ons die notasies beskikbaar vir toekomstige werk.", 'id': 'Norwegia memiliki banyak variasi dialektal, serta toleransi umum untuk penggunaannya di sfera publik. Namun, ada sedikit sumber daya yang tersedia untuk mempelajari variasi ini dan perubahannya melalui waktu dan di daerah yang lebih informal, di media sosial. In this paper, we propose a first step to creating a corpus of dialectal variation of written Norwegian.  We collect a small corpus of tweets and manually annotate them as Bokmal, Nynorsk, any dialect, or a mix.  Kami lebih lanjut melakukan percobaan awal dengan model terbaik, serta analisis data untuk memperluas tubuh ini di masa depan. Finally, we make the annotations available for future work.', 'sw': 'Norway in a kiasi kikubwa cha mabadiliko ya kidijitali, pamoja na uvumilivu mkubwa wa matumizi yake katika maeneo ya umma. Hata hivyo, kuna rasilimali chache zinazopatikana kusoma mabadiliko haya na mabadiliko yake kwa muda na katika maeneo mengine yasiyo rasmi, kwenye mitandao ya kijamii. Katika karatasi hii, tunapendekeza hatua ya kwanza ya kutengeneza viungo vya mabadiliko ya kidijitali ya KiNorway. Tunakusanya makampuni madogo ya twita na kuwakasirisha kwa mkono kama Bokmal, Nynorsk, lugha yoyote, au mchanganyiko. Tunafanya majaribio ya awali yenye mifano ya sanaa, pamoja na uchambuzi wa taarifa ili kueneza makampuni haya kwa siku za usoni. Mwisho, tunafanya matatizo yanayopatikana kwa ajili ya kazi ya baadaye.', 'sq': 'Norvegjia ka një sasi të madhe variacioni dialektal si dhe një tolerancë të përgjithshme ndaj përdorimit të saj në sferën publike. Megjithatë, ka pak burime në dispozicion për të studiuar këtë variacion dhe ndryshimin e saj gjatë kohës dhe në fusha më jozyrtare, në mediat sociale. Në këtë letër, propozojmë një hap të parë për të krijuar një korpus të variacionit dialektal të shkruar norvegjez. We collect a small corpus of tweets and manually annotate them as Bokmal, Nynorsk, any dialect, or a mix.  Ne kryejmë më tej eksperimente paraprake me modele të moderne si dhe një analizë të të dhënave për të zgjeruar këtë trup në të ardhmen. Më në fund, ne i bëjmë anotacionet në dispozicion për punën e ardhshme.', 'hy': 'Նորվեգիան ունի բազմաթիվ դիալեկտալ տարբերություններ, ինչպես նաև ընդհանուր թույլատրելիություն իր օգտագործման հանրային ոլորտում: Այնուամենայնիվ, չկա բազմաթիվ հասանելի ռեսուրսներ այս տարբերությունը ուսումնասիրելու համար ժամանակի ընթացքում և ավելի ոչ տեղեկատվական ոլորտներում, սոցիալական լրատվամիջոցներում: In this paper, we propose a first step to creating a corpus of dialectal variation of written Norwegian.  We collect a small corpus of tweets and manually annotate them as Bokmal, Nynorsk, any dialect, or a mix.  We further perform preliminary experiments with state-of-the-art models, as well as an analysis of the data to expand this corpus in the future.  Finally, we make the annotations available for future work.', 'tr': 'Norwegiýada köp dialektal üýtgeşikleri bar we jemgyýetçilik sferde ulanmagynyň tolygy bar. Ýöne bu üýtgeşikleri we üýtgeşiklerini wagtynda we esasy meýdançalarda, sosyal medýýatlarda öwrenmek üçin az resurslar bar. Bu kagyzda, biz Norwegça ýazylan dialektal üýtgeşikleriniň korpusyny bejermek üçin ilkinji adım teklip edip bilýäris. Biz tweets kiçijik köpüsini ýygnaýarys we olaryň Bokmal, Nynorsk, her bir dialekt ýa-da bir karışyk hökmünde elimizden duýýarys. Biz öňki möhüm nusgalar bilen öňki synaglary çykarýarys, we berüjilerin gelejekde üýtgetmek üçin barlag çözümlerini de çykarýarys. Soňunda biz gelejekki işiň üçin duýdurmalaryny mejbur edip bilýäris.', 'bs': 'Norveška ima veliku količinu dijalektalnih varijacija, kao i općeg tolerancije za njegovu upotrebu u javnoj sferi. Međutim, u društvenim medijima postoji malo dostupnih resursa za proučavanje te varijacije i promjene tijekom vremena i u više neformalnih područja. U ovom papiru predlažemo prvi korak za stvaranje korpusa dijalektne varijacije napisanog norveškog. Skupljamo mali korpus tweeta i ručno ih annotiramo kao Bokmal, Nynorsk, bilo koji dijalekt ili mješaj. Dalje izvršavamo preliminarni eksperiment s modelima države umjetnosti, kao i analizu podataka kako bi u budućnosti proširili ovaj korpus. Napokon, objavljujemo annotacije za budući posao.', 'ca': "Noruega té una gran varietat dialectal i una tolerancia general a l'ús en la esfera pública. There are, however, few available resources to study this variation and its change over time and in more informal areas, on social media.  In this paper, we propose a first step to creating a corpus of dialectal variation of written Norwegian.  Recollim un petit cos de tweets i els anotem manualment com Bokmal, Nynorsk, qualsevol dialecte o una combinació. També fem experiments preliminars amb models més avançats, com també una an àlisi de les dades per expandir aquest cos en el futur. Finally, we make the annotations available for future work.", 'am': 'ኖርዌ ብዙ ዳሌካዊ ልውጤት እና በህዝብ ጉዳይ ላይ የተጠቃሚ ትዕግስት አለበት፡፡ There are, however, few available resources to study this variation and its change over time and in more informal areas, on social media.  በዚህ ፕሮግራም፣ የኖርዌይያን የተጻፈውን የዳሌካዊ መለወጥ ለመፍጠር የመጀመሪያ ደረጃን ለመፍጠር እናስባለን፡፡ በትዊተር ትንሽ ቆርጦችን እንሰበስባለን፣ እንደቦቅኤል፣ እንደኒኖርስክ፣ ማንኛውም ዲያሌክ ወይም የተቀላቀል እናስታውቃቸዋለን፡፡ የቀድሞው መጀመሪያ ፈተናዎች በሥርዓት-የ-አርእስት ዓይነት እናደርጋለን፣ እናም ይህንን ኮፕስ ለመዘረጋት የዳታዎችን አስተያየት እናደርጋለን፡፡ በመጨረሻም ለኋለኛይቱ ሥራ የሚደረገውን ማስታወቂያ እናደርጋለን፡፡', 'az': 'Norveşdə çox dialektal dəyişiklikləri var, həmçin in halkı kürəsində istifadə edilməsi üçün genel tolerans də var. Lakin bu dəyişiklikləri və dəyişikliklərini zamanında və daha informal bölgelerde, sosyal mediyalarda təhsil etmək üçün çox az faydalanır. Bu kağızda, yazılmış Norveççenin dialektal dəyişiklikləri yaratmaq üçün ilk adım təklif edirik. Biz küçük bir twet korpusu toplayırıq və onları Bokmal, Nynorsk, hər bir dialekt və ya bir karışıq kimi məhv edirik. Biz daha öncə bu korpusu gələcəkdə genişlənmək üçün məlumatların analizi ilə əvvəlki təcrübələr edərik. Sonunda, gələcək işlər üçün təbliğ edirik.', 'cs': 'Norsko má velké množství dialektálních variací, stejně jako obecnou toleranci k jeho používání ve veřejné sféře. Existuje však málo dostupných zdrojů pro studium této variace a její změny v průběhu času a v neformálnějších oblastech na sociálních médiích. V tomto článku navrhujeme první krok k vytvoření korpusu dialektální variace psané norštiny. Shromáždíme malý korpus tweetů a ručně je anotujeme jako Bokmal, Nynorsk, libovolný dialekt nebo mix. Dále provádíme předběžné experimenty s nejmodernějšími modely, stejně jako analýzu dat pro rozšíření tohoto korpusu v budoucnu. Na závěr zpřístupníme anotace pro budoucí práci.', 'et': 'Norras on suur hulk dialektilisi variatsioone, samuti üldine tolerants selle kasutamise suhtes avalikus sfääris. Selle varieerumise ja selle muutumise uurimiseks aja jooksul ja mitteametlikumates valdkondades sotsiaalmeedias on siiski vähe ressursse. Käesolevas töös pakume välja esimese sammu kirjutatud norra dialektilise variatsiooni korpuse loomiseks. Kogume väikese säutsude korpuse ja märgistame neid käsitsi Bokmal, Nynorsk, mis tahes murde või segu. Lisaks teostame esialgseid katseid kaasaegsete mudelitega ning analüüsime andmeid, et seda korpust tulevikus laiendada. Lõpuks teeme märkused tulevaseks tööks kättesaadavaks.', 'bn': 'নরওয়ের বিশাল পরিমাণ ডায়ালেক্টাল বিভিন্ন পার্থক্য রয়েছে, একই সাথে জনস্থানে তাদের ব্যবহারের সাধারণ সহনশীল। তবে সামাজিক প্রচার মাধ্যমে এই বিভিন্ন পরিবর্তন এবং সময়ের পরিবর্তন এবং অন্যান্য অফিসিয়াল এলাকায় এই পরিবর্তন পড়ার জন্যে সামাজ এই কাগজটিতে আমরা প্রথম পদক্ষেপ প্রস্তাব করছি যেখানে লিখিত নরওয়েজিয়ান ভিন্ন ভিন্ন সৃষ্টি করার জন্য। আমরা টুইটের একটি ছোট্ট কোর্পাস সংগ্রহ করি এবং হাতে তাদের বোকমাল, নাইনোর্স্ক, যে কোন ডায়ালেক্টর অথবা মিশ্র। আমরা আরো প্রাথমিক পরীক্ষা করি রাষ্ট্র-অফ-শিল্প মডেলের সাথে এবং ভবিষ্যতে এই কোর্পাস বিস্তৃত করার জন্য তথ্যের বিশ্লেষণ। Finally, we make the annotations available for future work.', 'fi': 'Norjassa on suuri määrä dialektista vaihtelua sekä yleinen suvaitsevaisuus sen käyttöön julkisella alalla. Sosiaalisessa mediassa on kuitenkin vähän resursseja tutkia tätä vaihtelua ja sen muutoksia ajan myötä ja epävirallisemmilla alueilla. Tässä työssä ehdotamme ensimmäistä askelta kirjoitetun norjan dialektisen variaation korpusen luomiseen. Keräämme pienen twiittikorpusen ja merkitsemme niihin manuaalisesti Bokmal, Nynorsk, mikä tahansa murre tai sekoitus. Lisäksi teemme alustavia kokeiluja uusimmilla malleilla sekä analysoimme aineistoa tämän korpusen laajentamiseksi tulevaisuudessa. Lopuksi annamme huomautukset saataville tulevaa työtä varten.', 'he': 'לנורבגיה יש כמות גדולה של שינוי דיאלקטלי, כמו גם סבלנות כללית לשימושו בכדור הציבור. למרות זאת, יש מעט משאבים זמינים ללמוד את ההחלטה הזו והשינוי שלה במהלך הזמן ובאזורים לא רשמיים יותר, בתקשורת חברתית. בעיתון הזה, אנו מציעים צעד ראשון לייצור קורפוס של שוורציה דיאלקטלית של נורבגית כתובת. We collect a small corpus of tweets and manually annotate them as Bokmal, Nynorsk, any dialect, or a mix.  אנחנו מבצעים עוד ניסויים מקדימים עם מודלים חדשים, כמו גם ניתוח של הנתונים כדי להרחיב את הקורפוס הזה בעתיד. סוף סוף, אנחנו מקבלים את ההערות זמינות לעבודה עתידית.', 'sk': 'Norveška ima veliko dialektičnih razlik, kot tudi splošno toleranco do njene uporabe v javni sferi. Vendar pa je na voljo malo virov za preučevanje te variacije in njenih sprememb s časom in na bolj neformalnih področjih na družbenih omrežjih. V prispevku predlagamo prvi korak k oblikovanju korpusa dialektualne variacije pisanega norveščine. Zbiramo majhen korpus tweetov in jih ročno označimo kot Bokmal, Nynorsk, kateri koli narečje ali mešanica. Nadalje izvajamo predhodne eksperimente z najsodobnejšimi modeli ter analizo podatkov za razširitev tega korpusa v prihodnosti. Na koncu, opombe dajemo na voljo za prihodnje delo.', 'ha': "Umarniya yana da gwargwadon variant mai dialaka, da kuma mai haƙuri ga amfani da shi a cikin filin umma. A lokacin da, akwai kaɗan da wuri masu iya amfani da za su karanta wannan variant da musanyawa domin lokacin da ke cikin wurãren da masu takarda, a kan mita jami. Daga wannan takardan, Munã buɗa wata ta farko da za'a sami wata kalma na dabar-dabar-dabar na rubuta na Narayiya. Suna samun makaranti kaɗan na Twitter kuma Muke taƙaita su da hannayen takarda kamar Bokel, Nynorsk, ko duk cikin hoteli ko da haɗi. We further perform preliminary experiments with state-of-the-art models, as well as an analysis of the data to expand this corpus in the future.  Gani, za'a sami sunayen zuwa aikin nan gaba.", 'jv': 'Norwegian sing nduwe akeh akeh pisan dialectal sing dumadhi, kaya ngono tolerans jenerah kanggo nggawe barang mongkar obah. Awak dhéwé, karo nganggo sing paling-paling nêmên kanggo ngerasakno karo hal-hal kanggo ngerasakno iki lan neng pisan sing nyerané, ning media sotiki. Nang pepul iki, kita supoyo perusahaan kanggo nggawe mulasah karo hal-hal dialectal perusahaan ning Norse. Awak dhéwé ngregani mritik karo tiket lan nganggo ngomong nik Bokml, NyNors, dialectu uwong, opo nyimpen. Awak dhéwé éntuk mulai perusahaan neng sampek prelimian karo model state-of-the-arts, lan nganggep anaranil kuwi nggawe data kedhéwé kanggo mbawané iki ning kapus dianggap. Sokake, awak dhéwé ngejaraké mbukaké kanggo nggawe gerakno.', 'bo': 'སྤྱི་ཚོགས་ཀྱི་ཁྱད་པར་ཆེ་བའི་དབྱངས་འབྲེལ་བ་ཞིག་ཡོད། ཡིན་ནའང་སྤྱི་ཚོགས་འབྲེལ་མཐུད་དེ་ལ་ཁྱད་པར་འགྱུར་བ་དང་། དེ་དག་གནས་སྐབས་དང་མཉམ་དུ་ལས་འགྱུར་བ་མང་ཙམ་རྟོགས་གནས་སྟངས འུ་ཅག་གིས་ཤོག་བུ་འདིའི་ནང་དུ་རྨང་ཐེངས་དང་པོ་ཞིག་གསར་འཛུགས་བྱེད་པར་སྤྲོད་ཡོད། སྔོན་གྱི་རྩོམ་པ་ཞིག་ ང་ཚོས་དྲ་ཡིག་གི་མཛུབ་ཆུང་ཀུ་ཅིག་སྒྲིག་ནས་སྟངས་འཛིན་བྱེད་ཀྱི་ཡོད། ང་ཚོས་མི་མང་གི་སྔོན་གྱི་བརྟག་ཞིབ་བྱེད་པའི་གནས་སྟངས་གཙོ་རིམ་དང་མཐུན་རྐྱེན་པའི་བརྟག་ཞིབ་དང་ མཐའ་མར་མ་དེར། ང་ཚོས་མ་འོངས་པའི་བརྗོད་ཐོའི་ནང་དུ་འཇུག་ཐུབ་ཀྱི་ཡོད།'}
{'en': 'The Swedish Winogender Dataset', 'es': 'El conjunto de datos sueco de Winogender', 'pt': 'O conjunto de dados sueco de Winogen', 'ar': 'مجموعة بيانات Winogender السويدية', 'fr': 'Le jeu de données suédois Winogender', 'ja': 'スウェーデンのワイノゲンデータセット', 'hi': 'स्वीडिश विनोजेंडर डेटासेट', 'zh': '瑞典Winogender数据集', 'ru': 'Шведский набор данных Winogender', 'ga': 'An tacar sonraí Winogender na Sualainne', 'ka': 'Name', 'el': 'Το σουηδικό σύνολο δεδομένων Winogener', 'kk': 'Швеция виногендер деректері', 'lt': 'Švedijos vynuogių augintojų duomenų rinkinys', 'mk': 'Name', 'ms': 'The Swedish Winogender Dataset', 'ml': 'സ്വീഡിഷ് വിന്\u200dജോണ്\u200dഡര്\u200d ഡേറ്റാസെറ്റ്Name', 'mt': 'Id-Dataset Svediż tal-Winogender', 'mn': 'Шведийн хувьсагч өгөгдлийн санаа', 'it': 'Il set di dati Winogender svedese', 'hu': 'A svéd Winogender adatkészlet', 'no': 'Dataset for Sverige Winogender', 'pl': 'Szwedzki zestaw danych Winogender', 'si': 'Name', 'ro': 'Setul de date Winogender suedez', 'sv': 'Den svenska Winogender-datauppsättningen', 'sr': 'Švedski podatak Winogender', 'ta': 'The Swedish Winogender Dataset', 'so': 'Iswidishka Winogender', 'ur': 'Name', 'uz': 'Name', 'vi': 'Dữ liệu gen Thụy Điển', 'bg': 'Шведският набор от данни Winogender', 'nl': 'De Zweedse Winogender Dataset', 'da': 'Det svenske Winogender- datasæt', 'de': 'Der schwedische Winogender-Datensatz', 'id': 'The Swedish Winogender Dataset', 'ko': '스웨덴 Winogener 데이터 세트', 'hr': 'Švedski podaci Winogender', 'sw': 'The Swedish Winogender Dataset', 'fa': 'اطلاعات وینوژندر سوئدی', 'tr': 'Şwesiýa Winogender Maglumaty', 'af': 'Name', 'hy': 'Շվեդիական Վինոգենդերի տվյալների համակարգը', 'sq': 'Paketa e dhënash e Winogender Suedeze', 'az': 'İsveçə Winogender Dataseti', 'bn': 'Name', 'bs': 'Švedski krijumčarski datum', 'ca': 'El conjunt de dades sobre el vinògen suec', 'am': 'The Swedish Winogender Dataset', 'fi': 'Ruotsin Winogender-tietokanta', 'cs': 'Švédská datová sada Winogender', 'et': 'Rootsi Winogenderi andmekogum', 'jv': 'FindOK', 'ha': 'KCharselect unicode block name', 'sk': 'Švedski nabor podatkov Winogender', 'bo': 'སུའུ་ཌིས་ཁོའི་རྒོལ་བསྡད་པའི་ཆ་འཕྲིན་དང་།', 'he': 'קבוצת נתונים של וינוגנדר השוודית'}
{'en': 'We introduce the SweWinogender test set, a diagnostic dataset to measure gender bias in coreference resolution. It is modelled after the English Winogender benchmark, and is released with reference statistics on the distribution of men and women between occupations and the association between gender and occupation in modern corpus material. The paper discusses the design and creation of the dataset, and presents a small investigation of the supplementary statistics.', 'ar': 'نقدم مجموعة اختبار سويوينوجندر ، وهي مجموعة بيانات تشخيصية لقياس التحيز بين الجنسين في دقة المرجع. تمت صياغته على غرار مقياس Winogender الإنجليزي ، وتم إصداره بإحصاءات مرجعية حول توزيع الرجال والنساء بين المهن والعلاقة بين الجنس والمهنة في مواد المجموعة الحديثة. تناقش الورقة تصميم وإنشاء مجموعة البيانات ، وتقدم تحقيقًا صغيرًا في الإحصائيات التكميلية.', 'pt': 'Apresentamos o conjunto de testes SweWinogender, um conjunto de dados de diagnóstico para medir o viés de gênero na resolução de correferência. Ele é modelado a partir do benchmark inglês Winogenender e é divulgado com estatísticas de referência sobre a distribuição de homens e mulheres entre ocupações e a associação entre gênero e ocupação em material de corpus moderno. O artigo discute o desenho e a criação do conjunto de dados e apresenta uma pequena investigação das estatísticas suplementares.', 'es': 'Presentamos el conjunto de pruebas SwewinoGender, un conjunto de datos de diagnóstico para medir el sesgo de género en la resolución de correferencias. Se basa en el modelo de referencia inglés Winogender y se publica con estadísticas de referencia sobre la distribución de hombres y mujeres entre ocupaciones y la asociación entre género y ocupación en el material del corpus moderno. El documento discute el diseño y la creación del conjunto de datos, y presenta una pequeña investigación de las estadísticas suplementarias.', 'fr': "Nous présentons l'ensemble de tests SwewinOgender, un ensemble de données diagnostiques pour mesurer le biais sexiste dans la résolution de coréférence. Il est calqué sur le benchmark Winogender anglais et est publié avec des statistiques de référence sur la répartition des hommes et des femmes entre les professions et l'association entre le sexe et la profession dans les corpus modernes. L'article traite de la conception et de la création de l'ensemble de données et présente une petite enquête sur les statistiques supplémentaires.", 'ja': '私たちは、コアリファレンス分解能における性別バイアスを測定するための診断データセットであるSweWinogenderテストセットを紹介します。これは、英語のWinogenderベンチマークをモデルにしており、現代のコーパス資料における職業間の男女の分布と性別と職業の関連性に関する参考統計を参考にしてリリースされています。この論文では、データセットの設計と作成について説明し、補足統計の小規模な調査を紹介しています。', 'zh': '引入SweWinogender试集,此一诊数集也,以量共指征分辨率之偏见也。 以英国Winogender准为蓝本,发今世语料库男女职业之间,与性别相连者参统计数据。 本文议数集设创,并补统计数据小勘。', 'ru': 'Мы вводим набор тестов SweWinogender, диагностический набор данных для измерения гендерной предвзятости в разрешении ядра. Он составлен по образцу английского контрольного показателя Winogender и выпущен вместе со справочными статистическими данными о распределении мужчин и женщин по профессиям и связи между полом и профессией в современных корпусных материалах. В документе обсуждается структура и создание набора данных и приводится небольшое исследование дополнительных статистических данных.', 'hi': 'हम SweWinogender परीक्षण सेट, एक नैदानिक डेटासेट को सह-सम्मेलन संकल्प में लिंग पूर्वाग्रह को मापने के लिए पेश करते हैं। यह अंग्रेजी Winogender बेंचमार्क के बाद मॉडलिंग की गई है, और व्यवसायों के बीच पुरुषों और महिलाओं के वितरण और आधुनिक कॉर्पस सामग्री में लिंग और व्यवसाय के बीच संबंध पर संदर्भ आंकड़ों के साथ जारी किया गया है। पेपर डेटासेट के डिजाइन और निर्माण पर चर्चा करता है, और पूरक आंकड़ों की एक छोटी सी जांच प्रस्तुत करता है।', 'ga': 'Tugaimid isteach tacar tástála SweWinogender, tacar sonraí diagnóiseacha chun laofacht inscne a thomhas i réiteach croí-chomhdhála. Múnlaítear é i ndiaidh thagarmharc Winogender Shasana, agus eisítear é le staitisticí tagartha ar dháileadh na bhfear agus na mban idir gairmeacha agus an bhaint atá idir inscne agus gairm bheatha in ábhar corpais an lae inniu. Pléann an páipéar dearadh agus cruthú an tacair sonraí, agus cuireann sé imscrúdú beag i láthair ar na staitisticí forlíontacha.', 'el': 'Παρουσιάζουμε το σύνολο δοκιμών ένα διαγνωστικό σύνολο δεδομένων για τη μέτρηση της προκατάληψης φύλου στην επίλυση της διαφοράς. Έχει μοντελοποιηθεί με βάση το αγγλικό δείκτη αναφοράς και δημοσιεύεται με στατιστικά στοιχεία αναφοράς για την κατανομή ανδρών και γυναικών μεταξύ επαγγέλματος και τη συσχέτιση μεταξύ φύλου και επαγγέλματος στο σύγχρονο υλικό σώματος. Η εργασία συζητά τον σχεδιασμό και τη δημιουργία του συνόλου δεδομένων και παρουσιάζει μια μικρή έρευνα των συμπληρωματικών στατιστικών.', 'ka': 'ჩვენ SweWinogender ტესტის შესახებ, დიაგნოსტიკური მონაცემების შესახებ გენექტური წინასწორებაში წინასწორებლად გავამრავლეთ. ეს მოდელურია ინგლისური ვინგენდერის ბენქმარკის შემდეგ, რომელიც განსხვავებულია მაკაცების და ქალების განსხვავებაზე კომპოციაციის და კომპოციაციის განსხვავებაში. დოკუნტის შესახებ მონაცემების შექმნა და შექმნა და აღწერს პატარა პატარა პასუხი სტატისტიკის შესახებ.', 'hu': 'Bemutatjuk a SweWinogender tesztkészletet, egy diagnosztikai adatkészletet, amely a nemi elfogultság mérésére szolgál a coreferencia felbontásban. Az angol Winogender referenciaérték alapján készült, és referencia statisztikákkal jelenik meg a férfiak és nők foglalkozások közötti eloszlásáról, valamint a nemek és foglalkozás közötti kapcsolatról modern korpusz anyagokban. A tanulmány tárgyalja az adatkészlet kialakítását és létrehozását, valamint bemutatja a kiegészítő statisztikák kisebb vizsgálatát.', 'it': "Introducemo il set di test SweWinogender, un set di dati diagnostici per misurare il bias di genere nella risoluzione della coreferenza. È modellato sul benchmark inglese Winogender ed è pubblicato con statistiche di riferimento sulla distribuzione di uomini e donne tra occupazioni e l'associazione tra genere e occupazione nel corpus moderno. L'articolo discute la progettazione e la creazione del set di dati e presenta una piccola indagine sulle statistiche supplementari.", 'mk': 'Го воведуваме тестот SweWinogender, дијагнозички податок за мерење на половинската пристрасност во резолуцијата на соодветноста. Истата е моделирана по англиската референтна оценка на Виногендер и е објавена со референтни статистики за дистрибуцијата на мажите и жените помеѓу окупациите и асоцијацијата помеѓу полот и окупацијата во модерниот корпус материјал. The paper discusses the design and creation of the dataset, and presents a small investigation of the supplementary statistics.', 'lt': 'Įvedame SweWinogender bandymų rinkinį, diagnostinių duomenų rinkinį, skirtą įvertinti lyčių pusiausvyrą koreferencijos rezoliucijoje. Jis modeliuojamas pagal Anglijos Winogender lyginamąjį rodiklį ir skelbiamas su referenciniais statistiniais duomenimis apie vyrų ir moterų pasiskirstymą tarp profesijų ir lyčių bei profesijos ryšį šiuolaikinėje korpuso medžiagoje. Dokumente aptariamas duomenų rinkinio projektavimas ir kūrimas ir pateikiamas nedidelis papildomų statistinių duomenų tyrimas.', 'kk': 'Біз SweWinogender сынақтарын, диагностикалық деректер қорын, гендердің қарсылығын мөлдіру үшін, мәселелердің айырмашылығында. Бұл ағылшын Виногендердің белгісінен кейін модельді, және әйелдер және әйелдердің бөлімі арасындағы гендердің және мәселелер арасындағы бөлімінің статистикасы бойынша шығарылады. Қағаз деректер қорының құрылымын және құрылымын талқылады және қосымша статистиканы шағын іздеу.', 'ml': 'ഞങ്ങള്\u200d സ്വീവ്വിന്\u200dജോണ്\u200dഡര്\u200d ടെസ്റ്റ് സെറ്റിനെ പരിചയപ്പെടുത്തുന്നു, കോര്\u200dഫെന്\u200dസിന്\u200dറെ റിസ്റ്റലില്\u200d ലൈന്\u200dസ് ബ അത് ഇംഗ്ലീഷ് വിന്\u200dജോണ്\u200dഡര്\u200d ബെന്\u200dച്മാര്\u200dക്കിന് ശേഷം മോഡല്\u200d ചെയ്യപ്പെടുന്നു. പുരുഷന്\u200dമാരും സ്ത്രീകള്\u200dക്കും ഇടയിലുള്ള ജോലിക്കുകള്\u200dക്കും  ആ പത്രത്തില്\u200d ഡാറ്റാസെറ്റിന്റെ ഡിസൈനിവും സൃഷ്ടിക്കുന്നതും സംസാരിക്കുന്നു. അതിന്റെ കൂടുതല്\u200d പരിസ്ഥിതികങ', 'ms': 'Kami memperkenalkan set ujian SweWinogender, set data diagnostik untuk mengukur bias jenis dalam resolusi persamaan. Ia dipodelkan selepas benchmark Winogender Inggeris, dan dibebaskan dengan statistik rujukan tentang pengedaran lelaki dan perempuan antara pekerjaan dan persatuan antara jenis dan pekerjaan dalam bahan corpus modern. Kertas ini membincangkan rancangan dan penciptaan set data, dan memperkenalkan penyelidikan kecil tentang statistik tambahan.', 'mt': 'Aħna nintroduċu s-sett tat-test SweWinogender, sett ta’ dejta dijanjostika biex tkejjel il-preġudizzju bejn is-sessi fir-riżoluzzjoni tal-koreferenza. Hija mmudellata wara l-punt ta’ riferiment Ingliż Winogender, u hija rilaxxata b’statistika ta’ referenza dwar id-distribuzzjoni tal-irġiel u n-nisa bejn ix-xogħol u l-assoċjazzjoni bejn is-sess u l-impjieg f’materjal modern corpus. Id-dokument jiddiskuti d-disinn u l-ħolqien tas-sett tad-dejta, u jippreżenta investigazzjoni żgħira tal-istatistika supplimentari.', 'mn': 'Бид SweWinogender-ын шалгалтыг танилцуулж, сэтгэл хөдлөлийн шийдвэрлэлд гендерийн эсрэг байдлыг хэмжүүлэх диагностик өгөгдлийн сангуудыг танилцуулж байна. Энэ нь Англи Уиногендерийн багц хэмжээний дараа дүрслэгдсэн бөгөөд орчин үеийн корпус материалын эрчүүд, эмэгтэйчүүдийн ажиллагааны хуваалцааны, гендер болон ажиллагааны хоорондын холбоотой холбоотой статист Дасгал нь өгөгдлийн сангийн дизайныг, бүтээлийг ярьж, нэмэлт статистикийн жижиг шалгалтыг харуулдаг.', 'no': 'Vi introduserer SweWinogender-testsettet, eit diagnostisk dataset for å måle seks-forholdet i oppløysing av koreferansen. Det er modellert etter engelsk vinogender-benchmarket, og blir utgjeven med referansstatistikk om distribusjonen av menn og kvinner mellom okupacjonar og assosisjonen mellom seks og okupacjon i moderne korpusmateriell. Papiret diskuterer design and creation of the dataset, and presents a small investigation of the supplementary statistics.', 'ro': 'Introducem setul de teste SweWinogender, un set de date de diagnosticare pentru a măsura părtinirea sexului în rezoluția corefenței. Acesta este modelat după benchmark-ul englez Winogender și este publicat cu statistici de referință privind distribuția bărbaților și femeilor între ocupații și asocierea dintre gen și ocupație în materialul corpus modern. Lucrarea abordează proiectarea și crearea setului de date și prezintă o mică investigare a statisticilor suplimentare.', 'pl': 'Przedstawiamy zestaw testów SweWinogender, zestaw danych diagnostycznych służący do pomiaru uprzedzeń płci w rozdzielczości koreferencji. Został on wzorowany na angielskim porównaniu Winogender i został opublikowany wraz ze statystykami referencyjnymi dotyczącymi rozkładu kobiet i mężczyzn między zawodami oraz związku między płcią a zawodem w nowoczesnych materiałach korpusowych. W artykule omówiono projekt i tworzenie zbioru danych oraz przedstawiono niewielkie zbadanie statystyk uzupełniających.', 'si': 'අපි ස්වීන්ගෙන්ඩර් පරීක්ෂණා සැටුම් කරනවා, විශ්වාසික දත්ත සැටුම් විශ්වාස කරනවා, කෝරෙෆෙරෙන්ස්  ඒක ඉංග්\u200dරීසි වින්ගෝජෙන්ඩර් බෙන්ච්මාර්ක් පස්සේ මොඩල් වෙලා තියෙනවා, ඒ වගේම මිනිස්සු සහ ගැනිස්සු විතරය සහ ජීවිත ස පත්තුරේ දත්ත සැට් විද්\u200dයාපනය සහ නිර්මාණය කතා කරනවා, ඒ වගේම පුළුවන් පරීක්ෂණය සඳහා පොඩි පරීක්ෂණ', 'sr': 'Predstavljamo test SweWinogender a, set dijagnostičkih podataka za mjerenje spolnih predrasuda u rezoluciji pristojnosti. Modelirana je nakon konzerve engleskog vinogendera, i oslobađuje se s referentnim statistikom o distribuciji muškaraca i žena između okupacija i asocijacije između spola i okupacije u modernom korpusnom materijalu. U novinama se raspravlja o dizajnu i stvaranju seta podataka i predstavlja mala istraga dodatne statistike.', 'sv': 'Vi introducerar SweWinogender testset, ett diagnostiskt dataset för att mäta könsbias i coreference upplösning. Den är modellerad efter det engelska Winogender-riktmärket och publiceras med referensstatistik över fördelningen av män och kvinnor mellan yrken och sambandet mellan kön och yrke i modernt corpusmaterial. Uppsatsen diskuterar utformningen och skapandet av datauppsättningen och presenterar en liten undersökning av tilläggsstatistiken.', 'ur': 'ہم نے سوئوینګینڈر تست سٹ کو معلوم کیا ہے، ایک دیاگنٹیک ڈیٹ سٹ جنس کی مخالفت مزید کرنا چاہیے۔ یہ انگلیسی وینګینڈر بنچم مارک کے بعد نمادی کی جاتی ہے، اور مردوں اور عورتوں کے تقسیم کے معاملہ میں جنس اور عملہ کے درمیان تفریق کے معاملہ میں منتشر کئے جاتے ہیں. The paper discusses the design and creation of the dataset, and presents a small investigation of the supplementary statistics.', 'so': "Waxaynu soo bandhignaynaa kooxda imtixaanka SweWinogender, taasoo ah kooxda macluumaadka la yaqaan si aan u qiyaasno rabshadda jinsiga oo ku saabsan go'aanka xiliga. Waxaa lagu sameynayaa bangiga afka Ingiriiska Winogender kadib, waxaana lagu soo daabacaa takhasuska macluumaadka ku saabsan qaybinta ragga iyo dumarka u dhexeeya xirfadaha iyo ururka u dhexeeya jinsiga iyo shaqaalaha ku saabsan waxyaabaha hore ee korpuska. Warqaddu wuxuu ka sheekaynayaa sawirada iyo abuuridda sawirada, wuxuuna soo bandhigaa baaritaano yar oo tiyaatarka dheeraadka ah.", 'ta': 'நாம் ஸ்வீவின்ஜன்டர் சோதனையை அறிவிக்கிறோம், குறிப்பு தெளிவுத்திறனில் பெண்கள் குறிப்பிட்ட பிரச்சனையை அள இது ஆங்கிலம் வின்ஜோன்டர் பென்ந்திருப்பின் மாதிரியிடப்பட்டுள்ளது, மற்றும் குறிப்பிட்ட புள்ளிவிவரங்களில் ஆண்களுக்கும் பெண்களுக்கும காகிதம் தரவுத்தளத்தின் வடிவமைப்பு மற்றும் உருவாக்கத்தை விவாதம் செய்து, கூடுதல் புள்ளிவிவரங்கள் ஒரு சிறிய விச', 'uz': "Biz SweWinogender sinov tizimini ko'rib chiqaramiz, jinsiyalar tizimini ko'rsatish uchun diagnostic maʼlumotlar tartibi. Bu ingliz tilidagi Winogender benchmarkidan keyin modellanadi va mening va ayollar orasidagi tashkilotlar va yangi korpus materialidagi jinsiyalar va ishlash orasidagi birlashtirish statistikasi bilan ajratiladi. Qogʻoz maʼlumot sahifadagi dizayni va yaratishni talab qiladi va qoʻshimcha statistika qidirishni yaratadi.", 'vi': 'Chúng tôi sẽ giới thiệu bộ thử ra Swenson, một tập tin chẩn đoán để đo lường khuynh hướng giới tính trong nghị quyết. Nó được mô hình theo tiêu chuẩn nhà Lý gốc Anh Quốc, và được xuất bản bằng các thống kê về phân chia nam nữ giữa các giáo vụ và liên kết giữa giới tính và nghề nghiệp trong các vật liệu hữu cơ hiện đại. Đề tài thảo về thiết kế và cấu trúc của bộ dữ liệu, và đưa ra một cuộc điều tra nhỏ về các thống kê bổ sung.', 'da': 'Vi introducerer SweWinogender testsættet, et diagnostisk datasæt til måling af kønsbias i coreferenceopløsning. Den er modelleret efter den engelske Winogender benchmark, og udgives med referencestatistik over fordelingen af mænd og kvinder mellem erhverv og sammenhængen mellem køn og erhverv i moderne korpusmateriale. Artiklen diskuterer udformningen og skabelsen af datasættet og præsenterer en lille undersøgelse af de supplerende statistikker.', 'de': 'Wir stellen den SweWinogender Testsatz vor, einen diagnostischen Datensatz zur Messung von Gender Bias in Coreferenzauflösung. Es ist dem englischen Winogender Benchmark nachempfunden und wird mit Referenzstatistiken zur Verteilung von Männern und Frauen zwischen Berufen und der Assoziation zwischen Geschlecht und Beruf in modernen Korpusmaterialien veröffentlicht. Der Beitrag diskutiert das Design und die Erstellung des Datensatzes und stellt eine kleine Untersuchung der ergänzenden Statistiken vor.', 'id': 'Kami memperkenalkan set tes SweWinogender, set data diagnostik untuk mengukur bias jenis dalam resolusi koreferensi. Ini dipodelkan setelah benchmark Winogender Inggris, dan dilepaskan dengan statistik referensi tentang distribusi pria dan wanita antara pekerjaan dan asosiasi antara jenis dan pekerjaan dalam materi corpus modern. Kertas ini mendiskusikan desain dan penciptaan set data, dan mempersembahkan penyelidikan kecil tentang statistik tambahan.', 'nl': 'We introduceren de SweWinogender test set, een diagnostische dataset om gender bias in coreferentie resolutie te meten. Het is gemodelleerd naar de Engelse Winogender benchmark, en wordt gepubliceerd met referentiestatistieken over de verdeling van mannen en vrouwen over beroepen en de associatie tussen geslacht en beroep in modern corpusmateriaal. Het artikel bespreekt het ontwerp en de creatie van de dataset en presenteert een klein onderzoek naar de aanvullende statistieken.', 'bg': 'Представяме тестовия набор от диагностични данни за измерване на половото отклонение в резолюцията на кореференцията. Тя е моделирана по английския бенчмарк и е публикувана с референтна статистика за разпределението на мъжете и жените между професиите и връзката между пол и професия в съвременния корпус материал. Статията разглежда проектирането и създаването на набора от данни и представя малко проучване на допълнителната статистика.', 'hr': 'Upoznajemo sastanak SweWinogender test a, sastanak dijagnostičkih podataka za mjerenje spolnih pristrasnosti u rješavanju pristojnosti. Modelirana je nakon konzerve engleskog Winogendera i oslobađuje se s referentnim statistikom o distribuciji muškaraca i žena između okupacija i asocijacije između spola i okupacije u modernom korpusnom materijalu. U novinama se raspravlja o dizajnu i stvaranju kompleta podataka i predstavlja mala istraga dodatne statistike.', 'ko': '우리는 SweWinogender 테스트 세트를 소개했는데, 이것은 모두 소화 중인 성별 편견을 측정하는 데 사용되는 진단 데이터 세트이다.영국의 위너 기준을 모델로 삼아 현대 어료 라이브러리 자료에 직업 간 남성과 여성의 분포와 성별과 직업 간의 관련에 대한 참고 통계 데이터를 발표했다.본고는 데이터 집합의 설계와 창설을 토론하고 통계 데이터를 보충하는 소형 조사를 실시했다.', 'sw': 'We introduce the SweWinogender test set, a diagnostic dataset to measure gender bias in coreference resolution.  Inaongozwa baada ya bendera ya Kiingereza ya Winogender, na imeachiwa kwa takwimu za maoni kuhusu usambazaji wa wanaume na wanawake kati ya kazi na ushirikiano kati ya jinsia na kazi katika vifaa vya kisasa. Gazeti hilo linajadili ubunifu na kutengenezwa kwa seti ya taarifa hiyo, na inatoa uchunguzi mdogo wa takwimu za ziada.', 'tr': 'SweWinogender testi setini tanyşdyrýarys, seçgi öngörünüşlerini ölçürmek üçin bir diagnostik veri setini. Iňlisçe Winogender salgynyň soňra görkezilýär we erkekler we aýallaryň işgärleriň we jemgyýetleriň arasyndaky işgärleri bilen modern korpus materialynyň arasyndaky işgärleriň paýlaşmagy barada süýtgedilýär. Kagyzyň maglumaty düzenlemegini we bejerilmegini we ilatyň statistiki barada kiçi bir soragy çykýar.', 'af': "Ons introduiseer die SweWinogender toets stel, 'n diagnosiese datastel om gende bias te maak in koreferensieresolusie. Dit is model na die Engelse Winogender-benchmark, en is verlig met verwysing statistiek oor die verspreiding van manne en vroue tussen occupasies en die assosiasie tussen geneem en occupasie in moderne korpusmateriël. Die papier bespreek die ontwerp en skepping van die datastel en stel 'n klein ondersoek van die addisionele statistiek.", 'sq': 'Ne prezantojmë grupin e testit SweWinogender, një grup të dhënash diagnostike për të masuar paragjykimin e gjinisë në zgjidhjen e korreferencës. Ajo modelohet pas referencës angleze Winogender dhe publikohet me statistika referuese mbi shpërndarjen e burrave dhe grave midis punëve dhe shoqërimit midis gjinisë dhe punës në material modern corpus. Gazeta diskuton dizajnin dhe krijimin e grupit të të dhënave dhe paraqet një hetim të vogël të statistikave shtesë.', 'am': 'የውይይዌንጆንዳር ፈተና ሰርቨርስቲ፣ የሴቶችን ጥያቄ ለመለካት የሚችሉትን ዳታዎችን እናስታውቃለን፡፡ በንግግሊዝኛ አናውንድራር ገጽ ከተደረገ በኋላ፣ ወንዶችና ሴቶችን በሥርዓት እና በአዲስ ኮርፓስ ማህበረሰብ መካከል በሚያካፍኑት ስርዓት የተለየ ውጤት የተለየ ተሳሳይ ሲታሪክ በተለየ፡፡ ገጽ የዳታ ሳጥን እና መፍጠርን ይናገራል፡፡', 'fa': 'ما مجموعه آزمایش SweWinogender را معرفی می\u200cکنیم، مجموعه داده\u200cهای تشخیص برای اندازه\u200cگیری رعایت جنسی در راه حل رعایت. بعد از نقاشی انگلیسی وینوگنگر نموده می شود، و با استارتفاده از تفریح مردان و زنان بین شغل و ارتباط بین جنسی و شغل در مواد شرکت مدرن آزاد می شود. کاغذ در مورد طراحی و ایجاد مجموعه داده ها صحبت می کند و یک تحقیقات کوچک از آمار اضافه می کند.', 'hy': 'Մենք ներկայացնում ենք Սվեյվինոգենդերի փորձարկումների համակարգը, ախտորոշման տվյալների համակարգը, որպեսզի չափենք գենդերային կողմնականությունը հարաբերությունների լուծման մեջ: Այն կառուցվում է անգլերեն Վինոգենդերի հարաբերականի վրա և հրապարակում է տղամարդկանց և կանանց բաշխման վիճակագրությունը մասնագիտությունների միջև և սեռի և մասնագիտության կապը ժամանակակից կորպոս նյութերում: Այս թղթին քննարկում է տվյալների համակարգի դիզայնը և ստեղծումը, և ներկայացնում է մի փոքրիկ հետազոտություն կատարելագործման վիճակագրության մասին:', 'bn': 'আমরা সোয়েউইনজেন্ডারের পরীক্ষা সেট পরীক্ষা করে দেখাচ্ছি, যারা লিঙ্গের বিরুদ্ধে লিঙ্গের বিরুদ্ধে পরিমাপ করে। ইংরেজি উইন্জোন্ডার বেঞ্চম্যার্কের পর এটি মডেল করা হয় এবং আধুনিক কোর্পাসের মাধ্যমে পুরুষ ও নারীদের বিতরণের পরিসংখ্যানের পরিসংখ্যানে মুক এই পত্রিকাটি ডিজাইন এবং ডাটাসেটের সৃষ্টি নিয়ে আলোচনা করেছে এবং সংস্থার পরিসংখ্যানের একটি ছোট তদন্তের সামনে উপস্থা', 'az': 'Biz SweWinogender testi setini tanıdırıq, cins bias ölçüsü üçün bir diagnostik veri seti. İngilis Winogender benchmarkindən sonra modelləşdirilmiş və modern corpus material arasında cins və işçilik arasındakı şəkildə kişilər və qadınların dağıtılması haqqında müəyyən edilən statistik vasitəsilə yayınlışdırılmışdır. Kağıt verilənlərin dizaynı və yaradılışını mübahisə edir və ilahi statistik haqqında küçük bir xəbərdarlıq göstərir.', 'ca': "Introduïm el conjunt d'exàmens SweWinogender, un conjunt de dades diagnòstics per mesurar el bias de gènere en la resolució de coreferència. Es modela segons el valor de referència anglès Winogender, i es publica amb estadístiques de referència sobre la distribució d'homes i dones entre ocupacions i l'associació entre gènere i ocupació en material de corpus modern. El paper discute el disseny i la creació del conjunt de dades i presenta una petita investigació de les estadístiques suplementaris.", 'cs': 'Představujeme testovací sadu SweWinogender, diagnostický datový soubor pro měření genderového biasu v koreferenčním rozlišení. Je modelována podle anglického Winogenderova benchmarku a je vydána s referenčními statistikami o rozdělení mužů a žen mezi povoláními a asociaci mezi pohlavím a povoláním v moderním korpusovém materiálu. Článek pojednává o návrhu a tvorbě datové sady a představuje malé zkoumání doplňkových statistik.', 'et': 'Tutvustame SweWinogenderi testikomplekti, diagnostilist andmekogumit soolise kalduvuse mõõtmiseks kortereferentsi resolutsioonis. See on modelleeritud inglise Winogenderi võrdlusaluse järgi ning avaldatakse tänapäevases korpusmaterjalis viitestatistika meeste ja naiste jaotuse kohta ametite vahel ning soo ja ameti seose kohta. Töös käsitletakse andmekogumi kujundamist ja loomist ning esitatakse väike uuring täiendava statistika kohta.', 'bs': 'Predstavljamo set testova SweWinogender, set dijagnostičkih podataka za mjerenje pristrasnosti spola u rješavanju pristojnosti. Modelirana je nakon konzerve engleskog Winogendera, a oslobađena je s referentnim statistikom o distribuciji muškaraca i žena između okupacija i asocijacije između spola i okupacije u modernom korpusnom materijalu. U novinama se raspravlja o dizajnu i stvaranju seta podataka i predstavlja mala istraga dodatne statistike.', 'fi': 'Esittelemme SweWinogender-testisarjan, joka on diagnostinen aineisto sukupuolen vääristymisen mittaamiseksi koreferenssin resoluutiossa. Se on mallinnettu englanninkielisen Winogender-benchmarkin mukaan, ja se julkaistaan viitetilastojen avulla miesten ja naisten jakautumisesta ammattien välillä sekä sukupuolen ja ammatin välisestä yhteydestä nykyaikaisessa korpusaineistossa. Työssä käsitellään aineiston suunnittelua ja luomista sekä esitetään pieni selvitys täydentävistä tilastoista.', 'jv': 'Awak dhéwé nglanggar aturan Winog nggambar ujak, dataset iténtuan kanggo kuwi gewis biasane kanggo nggawe Resolusi layanan. Deweke iso disoleh nèng kelas bendhèk Winog nggawe lan ijol-ijolan wong kuwi nggawe dadi sing nyimpen karo nggawe uwong karo perusahaan karo perusahaan karo perusahaan karo hal-wong sing nggawe gerakan karo perusahaan karo perusahaan winih. Gambar nganggep nggawe design karo nggawe dataset, lan uga penti perusahaan kelas kanggo dadi supunter dadi.', 'sk': 'Predstavljamo SweWinogender test set, diagnostični nabor podatkov za merjenje spolne pristranskosti pri ločljivosti koreference. Modeliran je po angleškem Winogenderjevem referenčnem merilu in je objavljen z referenčno statistiko o porazdelitvi moških in žensk med poklici ter povezavi med spolom in poklicem v sodobnem korpusnem gradivu. Prispevek obravnava oblikovanje in ustvarjanje nabora podatkov ter predstavlja majhno preiskavo dopolnilne statistike.', 'ha': "Tuna fara shirin jarrabi na SWeWinojnder, da aka sami tsarin bayani na zafi don ya iya ƙayyade surori na jini cikin rabo. Ana motsa shi bayan bangon Ingiriya Windownder, kuma aka saka shi da statistics za'a rarraba maza da mãtã a tsakanin aikin da association tsakanin jini da akan aiki a cikin takardar nan yanzu. Gagon takarda yana jãyayya tsarin da aka ƙayyade shi, kuma yana bayana da ƙidãya masu ƙaranci.", 'he': "אנחנו מציגים את קבוצת הבדיקות של סווינוג'נדר, קבוצת נתונים אבחניים כדי למדוד ההתמחות מינית בפתרון התאמה. הוא מודל לאחר נקודת השיחה של וינוגנדר האנגלית, והופרסם עם סטטיסטיקות התייחסות על ההפצה של גברים ונשים בין המקצועים והקשר בין מין ומקצועים במחומר קורפוס מודרני. העיתון מדבר על עיצוב ויוצר המידע, ומציג חקירה קטנה של הסטטיסטיקה המיוחדת.", 'bo': 'ང་ཚོས་SweWinogender བརྟག་ཞིབ་ལྟར་བཤད་པ་ནི་དབྱེ་རིམ་གྱི་ཐབས་ལམ་ལུགས་གནད་མེད་སྤྲོད་དགོས་པ་ཡིན། It is modelled after the English Winogender benchmark, and is released with reference statistics on the distribution of men and women between occupations and the association between gender and occupation in modern corpus materials. ཤོག་བྱང་གིས་གསལ་བཟོ་བ་དང་བཟོ་བཅོས་གནད་སྡུད་ཡིག་ཆའི་ནང་དུ་གཏོང་ཞིབ་བྱེད་ཀྱི་ཡོད།'}
