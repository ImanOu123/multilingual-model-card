{'en': 'AMR-To-Text Generation with Graph Transformer', 'ar': 'إنشاء AMR إلى نص باستخدام محول الرسم البياني', 'es': 'Generación de AMR a texto con Graph Transformer', 'fr': 'Génération AMR en texte avec Graph Transformer', 'pt': 'Geração de AMR para Texto com Transformador de Gráfico', 'zh': '用图形转换器生成 AMR 至文本', 'ja': 'グラフ変圧器によるAMR - To - Text生成', 'ru': 'AMR-To-Text Генерация с графическим трансформатором', 'hi': 'ग्राफ ट्रांसफॉर्मर के साथ AMR-से-पाठ जनरेशन', 'ga': 'Giniúint AMR-Chun Téacs le Trasfhoirmeoir Graf', 'ka': 'Comment', 'el': 'Δημιουργία AMR-σε-κείμενο με μετασχηματιστή γραφήματος', 'hu': 'AMR-to-Text generĂ¡lĂ¡s grafikontranszformĂ¡torral', 'it': 'Generazione AMR-To-Text con trasformatore grafico', 'kk': 'График түрлендірушісімен AMR- мен- мәтінді құру', 'lt': 'Name', 'mk': 'AMR-To-Text Generation with Graph Transformer', 'ms': 'Penjanaan AMR- Ke- Teks dengan Penukar Graf', 'mt': 'Ġenerazzjoni ta’ AMR-Tekst bit-Trasferiment tal-Grafiki', 'ml': 'ഗ്രാഫ് മാറ്റുന്നതിനുമായി AMR- to- Text Generation', 'mn': 'График шилжүүлэгч', 'pl': 'Generowanie AMR do tekstu z transformatorem grafu', 'no': 'Generering av AMR- til- tekst med grafikktransformerer', 'ro': 'Generare AMR-To-Text cu transformator grafic', 'sr': 'Generacija AMR- na- tekst sa transformatorom grafika', 'so': 'AMR-To-Text Generation with Graph Transfer', 'si': 'ග්\u200dරාෆ් වෙනස් කරණාකරණය සමග AMR- ට- පාළුවට පරීක්ෂණය', 'sv': 'Generation av AMR-till-text med graftransformator', 'ta': 'AMR- to- Text Generation with Graph Transfer', 'ur': 'Comment', 'uz': 'Graph tarjima boʻyicha AMR- to- matn yaratish', 'vi': 'Máy tạo hình dạng dạng dạng dạng dạng AMR-tới-Đoạn với bộ biến hình.', 'bg': 'Създаване на AMR към текст с графичен трансформатор', 'nl': 'Generatie van AMR-naar-tekst met grafiektransformator', 'de': 'AMR-to-Text Generierung mit Graph Transformer', 'ko': '그래픽 변환기를 사용하여 AMR에서 텍스트로 생성', 'da': 'AMR- til- tekst- generering med graftransformator', 'id': 'Generasi AMR- Ke- Teks dengan Transformer Graph', 'sw': 'Uzalishaji wa matokeo ya AMR-To-Text with Transfer Graph', 'af': 'Name', 'hr': 'Generacija AMR- na- tekst sa transformacijom grafika', 'tr': 'AMR-Metin Üýtgedişi Grafik Üýtgedicisi bilen', 'am': 'undo-type', 'az': 'Grafik transformatörü ilə AMR-Metin Ünvanı', 'fa': 'تولید AMR- به متن با تبدیل گراف', 'bn': 'গ্রাফ ট্রান্সফার্নারের সাথে AMR- থেকে টেক্সট প্রজন্ম', 'bs': 'Generacija AMR- na- tekst sa transformacijom grafika', 'ca': 'generació AMR-a-text amb transformador de gràfics', 'cs': 'Generování AMR na text s grafovým transformátorem', 'hy': 'Comment', 'et': 'AMR- tekstiks genereerimine graafilise muunduriga', 'fi': 'AMR-tekstiksi luominen graafisen muuntajan avulla', 'sq': 'Gjenerimi AMR-në-Tekst me Transformuesin Grafik', 'jv': 'AM R-To-Text Generation with Graph Transformer', 'ha': 'KCharselect unicode block name', 'he': 'AMR-To-Text Generation with Graph Transformer', 'sk': 'Ustvarjanje AMR v besedilo s pretvornikom grafikona', 'bo': 'AMR-To-Text Generation with Graph Transformer'}
{'en': 'Abstract meaning representation (AMR)-to-text generation is the challenging task of generating natural language texts from AMR graphs, where ', 'fr': "La génération de représentation abstraite de signification (AMR) en texte est la tâche difficile de générer des textes en langage naturel à partir de graphes AMR, où les nœuds représentent des concepts et les arêtes indiquent des relations. Les méthodes actuelles de pointe utilisent des modèles de graphe à séquence\xa0; cependant, elles ne peuvent toujours pas surpasser de manière significative les modèles séquence à séquence ou les approches statistiques précédents. Dans cet article, nous proposons un nouveau modèle graphique-séquence (Transformateur graphique) pour répondre à cette tâche. Le modèle code directement les graphes AMR et apprend les représentations des nœuds. Une fonction d'interaction par paire est utilisée pour calculer les relations sémantiques entre les concepts. De plus, des mécanismes d'attention sont utilisés pour agréger les informations provenant des voisins entrants et sortants, ce qui aide le modèle à capturer efficacement les informations sémantiques. Notre modèle surpasse l'approche neuronale de pointe de 1,5 point BLEU sur LDC2015E86 et de 4,8 points BLEU sur LDC2017T10 et atteint de nouvelles performances de pointe.", 'es': 'La generación de representación de significado abstracto (AMR) a texto es la tarea desafiante de generar textos en lenguaje natural a partir de gráficos AMR, donde los nodos representan conceptos y las aristas denotan relaciones. Los métodos actuales de última generación utilizan modelos de gráfico a secuencia; sin embargo, todavía no pueden superar significativamente los modelos anteriores de secuencia a secuencia o enfoques estadísticos. En este artículo, proponemos un modelo novedoso de gráfico a secuencia (Graph Transformer) para abordar esta tarea. El modelo codifica directamente los gráficos AMR y aprende las representaciones de los nodos. Se utiliza una función de interacción por pares para calcular las relaciones semánticas entre los conceptos. Además, se utilizan mecanismos de atención para agregar la información de los vecinos entrantes y salientes, lo que ayuda al modelo a capturar la información semántica de manera efectiva. Nuestro modelo supera al enfoque neuronal de última generación en 1,5 puntos BLEU en LDC2015E86 y 4,8 puntos BLEU en LDC2017T10 y logra nuevos rendimientos de vanguardia.', 'ar': 'يمثل تمثيل المعنى المجرد (AMR) - إلى نصوص مهمة صعبة لتوليد نصوص لغة طبيعية من الرسوم البيانية AMR ، حيث تمثل العقد المفاهيم والحواف تشير إلى العلاقات. تستخدم الأساليب الحديثة الحالية نماذج الرسم البياني إلى التسلسل ؛ ومع ذلك ، لا يزالون غير قادرين على التفوق بشكل كبير على نماذج التسلسل إلى التسلسل أو الأساليب الإحصائية السابقة. في هذه الورقة ، نقترح نموذجًا جديدًا للرسم البياني إلى التسلسل (Graph Transformer) لمعالجة هذه المهمة. يشفر النموذج الرسوم البيانية AMR مباشرة ويتعلم تمثيلات العقدة. يتم استخدام وظيفة التفاعل الزوجي لحساب العلاقات الدلالية بين المفاهيم. علاوة على ذلك ، يتم استخدام آليات الانتباه لتجميع المعلومات من الجيران الواردة والصادرة ، مما يساعد النموذج على التقاط المعلومات الدلالية بشكل فعال. يتفوق نموذجنا على النهج العصبي الحديث بمقدار 1.5 نقطة BLEU على LDC2015E86 و 4.8 نقطة BLEU على LDC2017T10 ويحقق أداءً متطورًا جديدًا.', 'pt': 'A geração de representação abstrata de significado (AMR) para texto é a tarefa desafiadora de gerar textos em linguagem natural a partir de grafos AMR, onde os nós representam conceitos e as arestas denotam relações. Os métodos atuais de última geração usam modelos de gráfico para sequência; no entanto, eles ainda não podem superar significativamente os modelos anteriores de sequência a sequência ou abordagens estatísticas. Neste artigo, propomos um novo modelo gráfico-sequência (Graph Transformer) para resolver essa tarefa. O modelo codifica diretamente os gráficos AMR e aprende as representações dos nós. Uma função de interação pareada é usada para calcular as relações semânticas entre os conceitos. Além disso, mecanismos de atenção são usados para agregar as informações dos vizinhos de entrada e saída, o que ajuda o modelo a capturar a informação semântica de forma eficaz. Nosso modelo supera a abordagem neural de última geração em 1,5 pontos BLEU no LDC2015E86 e 4,8 pontos BLEU no LDC2017T10 e alcança novos desempenhos de última geração.', 'ja': '抽象意味表現（ AMR ）-テキスト生成は、AMRグラフから自然言語テキストを生成する難しいタスクであり、ノードは概念を表し、エッジは関係を表します。 現行の最先端の方法は、グラフからシーケンスへのモデルを使用するが、それらは依然として、以前のシーケンスからシーケンスへのモデルまたは統計的アプローチを著しく上回ることはできない。 本稿では、この課題に対処するための新たなグラフからシーケンスへのモデル（ Graph Transformer ）を提案する。 モデルは、AMRグラフを直接エンコードし、ノード表現を学習します。 対相互作用関数は、概念間の意味関係を計算するために使用される。 さらに、注目メカニズムは、入出力の隣接者からの情報を集約するために使用され、モデルが意味情報を効果的に取り込むのに役立つ。 当社のモデルは、LDC 2015 E 86で1.5 BLEUポイント、LDC 2017 T 10で4.8 BLEUポイントの最先端のニューラルアプローチを上回り、新しい最先端のパフォーマンスを実現します。', 'hi': 'अमूर्त अर्थ प्रतिनिधित्व (एएमआर) -टू-टेक्स्ट जनरेशन एएमआर ग्राफसे प्राकृतिक भाषा ग्रंथों को उत्पन्न करने का चुनौतीपूर्ण कार्य है, जहां नोड्स अवधारणाओं का प्रतिनिधित्व करते हैं और किनारे संबंधों को दर्शाते हैं। वर्तमान अत्याधुनिक तरीके ग्राफ-टू-अनुक्रम मॉडल का उपयोग करते हैं; हालांकि, वे अभी भी पिछले अनुक्रम-से-अनुक्रम मॉडल या सांख्यिकीय दृष्टिकोणों को काफी हद तक बेहतर नहीं कर सकते हैं। इस पेपर में, हम इस कार्य को संबोधित करने के लिए एक उपन्यास ग्राफ-टू-सीक्वेंस मॉडल (ग्राफ ट्रांसफॉर्मर) का प्रस्ताव करते हैं। मॉडल सीधे AMR रेखांकन encodes और नोड अभ्यावेदन सीखता है. एक युग्मवार इंटरैक्शन फ़ंक्शन का उपयोग अवधारणाओं के बीच शब्दार्थ संबंधों की गणना के लिए किया जाता है। इसके अलावा, ध्यान तंत्र का उपयोग आने वाले और आउटगोइंग पड़ोसियों से जानकारी एकत्र करने के लिए किया जाता है, जो मॉडल को शब्दार्थ जानकारी को प्रभावी ढंग से कैप्चर करने में मदद करता है। हमारा मॉडल LDC2015E86 पर 1.5 BLEU अंक और LDC2017T10 पर 4.8 BLEU अंकों द्वारा अत्याधुनिक तंत्रिका दृष्टिकोण को मात देता है और नए अत्याधुनिक प्रदर्शन प्राप्त करता है।', 'zh': '象曰:(AMR) 生于 AMR 自然语言文本之挑战性,节点曰名,边曰边。 目前最先进之法,用图形于序; 然犹不能显优于前序。 于本文,新图到序(转换器)以决其事。 其法直 AMR 图形编码而学节点为文。 成对交函数以数其语义也。 此外,机以合传邻之息,助模形而获语义息也。 吾形于LDC2015E86上出于先进之神经,1.5 BLEU分于LDC2017T10,4.8 BLEU于新先进之性。', 'ru': 'Абстрактное представление смысла (AMR)- генерация текста - это сложная задача генерации текстов на естественном языке из графиков AMR, где узлы представляют концепции, а ребра обозначают отношения. Современные методы используют межпоследовательные модели; однако они все еще не могут значительно превзойти предыдущие межпоследовательные модели или статистические подходы. В этой статье мы предлагаем новую модель граф-последовательности (Graph Transformer) для решения этой задачи. Модель непосредственно кодирует графики AMR и изучает представления узлов. Для вычисления семантических отношений между понятиями используется парная функция взаимодействия. Кроме того, механизмы внимания используются для агрегирования информации от входящих и исходящих соседей, что помогает модели эффективно захватывать семантическую информацию. Наша модель превосходит современный нейронный подход на 1,5 балла BLEU на LDC2015E86 и 4,8 балла BLEU на LDC2017T10 и достигает новых современных показателей.', 'ga': 'Léiriú brí teibí (AMR)-go-téacs is ea an tasc dúshlánach a bhaineann le téacsanna teanga nádúrtha a ghiniúint ó ghraif AMR, áit a seasann nóid do choincheapa agus seasann imill an chaidrimh. Úsáideann na modhanna úrscothacha reatha samhlacha graf-go-seicheamh; mar sin féin, ní féidir leo sárobair a dhéanamh fós ar na samhlacha seicheamh-go-seicheamh nó na cineálacha cur chuige staidrimh a bhí ann roimhe seo. Sa pháipéar seo, molaimid samhail ghraf-go-seicheamh nua (Graph Transformer) chun tabhairt faoin tasc seo. Ionchódaíonn an tsamhail na graif AMR go díreach agus foghlaimíonn sé na huiríll nód. Úsáidtear feidhm idirghníomhaíochta péirewise chun an caidreamh séimeantach idir na coincheapa a ríomh. Ina theannta sin, úsáidtear meicníochtaí aird chun an fhaisnéis ó na comharsana isteach agus amach a chomhiomlánú, rud a chabhraíonn leis an tsamhail an fhaisnéis shéimeantach a ghabháil go héifeachtach. Sáraíonn ár múnla an cur chuige néarúil úrscothach le 1.5 pointe BLEU ar LDC2015E86 agus 4.8 pointe BLEU ar LDC2017T10 agus baintear amach léirithe nua den scoth.', 'ka': 'აბსტრაქტური ნიშნავს გამოსახულება (AMR)-ტექსტის განვითარება არის მოსახულებელი რაოდენობა, რომელსაც კონფექტები და ბრძანები გამოსახულებენ მოსახულებელი ენის ტექსტის შექმნა AMR მიმდინარე სურათის შეცდომა მოდელების გამოყენება. მაგრამ ისინი არ შეუძლებელიან მნიშვნელოვანი წინა შემდეგ შემდეგ ან სტატისტიკური შემდეგ გავაკეთებთ. ამ დავალებში, ჩვენ პრომენტის გრაფიკის შემდეგ მოდელს (გრაფიკური ტრანფორმეტრის) ამ დავალების შესახებ. მოდელეში Direkt კოდირება AMR გრაფიკების და შესწავლის კოდის გამოსახულებები. კონცეტების შორის სემონტიკური შესახებ გამოყენება. დამატებით, მონაცემის მექანციები გამოიყენება ინფორმაციის აგგრებისთვის, რომლებიც მოდის და გადასვლებული საზოგადოებებიდან, რომლებიც მოდელს ეფექტიურად გამოყენება ჩვენი მოდელი 1,5 BLEU წერტილებით LDC2015E86 და 4.8 BLEU წერტილებით LDC2017T10 წერტილებით გააკეთება და ახალი წერტილების მონაცემებით გააკეთება.', 'hu': 'Az absztrakt jelentésreformáció (AMR)-szöveg generálása a természetes nyelvű szövegek létrehozásának kihívást jelentő feladata AMR grafikonokból, ahol a csomópontok fogalmakat képviselnek, a szélek pedig kapcsolatokat jelölnek. A jelenlegi korszerű módszerek gráf-szekvencia modelleket használnak; azonban még mindig nem tudják jelentősen felülmúlni a korábbi szekvencia-szekvencia modelleket vagy statisztikai megközelítéseket. Ebben a tanulmányban egy új gráf-szekvencia modellt (Graph Transformer) javasolunk ennek a feladatnak a megoldására. A modell közvetlenül kódolja az AMR grafikonokat és megtanulja a csomópont reprezentációit. Egy páros interakciós függvényt használunk a fogalmak közötti szemantikai kapcsolatok kiszámítására. Ezenkívül figyelemmel kísérő mechanizmusokat használnak a bejövő és kimenő szomszédok információinak összesítésére, amelyek segítenek a modell hatékonyan rögzíteni a szemantikai információkat. Modellünk az LDC2015E86-on 1,5 BLEU ponttal és az LDC2017T10-en 4,8 BLEU ponttal felülmúlja a korszerű neurális megközelítést, és új, korszerű teljesítményt ér el.', 'el': 'Η παραγωγή αφηρημένης συμβολικής αναπαράστασης (ΑΜR)-σε-κείμενο είναι το δύσκολο έργο της δημιουργίας κειμένων φυσικής γλώσσας από γραφήματα όπου οι κόμβοι αντιπροσωπεύουν έννοιες και οι άκρες υποδηλώνουν σχέσεις. Οι τρέχουσες σύγχρονες μέθοδοι χρησιμοποιούν μοντέλα γραφής-ακολουθίας· Ωστόσο, εξακολουθούν να μην μπορούν να υπερβούν σημαντικά τα προηγούμενα μοντέλα αλληλουχίας ή τις στατιστικές προσεγγίσεις. Στην παρούσα εργασία, προτείνουμε ένα νέο μοντέλο γραφής-ακολουθίας (μετασχηματιστής γραφήματος) για την αντιμετώπιση αυτού του έργου. Το μοντέλο κωδικοποιεί άμεσα τα γραφήματα και μαθαίνει τις αναπαραστάσεις κόμβων. Μια συνάρτηση αλληλεπίδρασης ζεύγους χρησιμοποιείται για τον υπολογισμό των σημασιολογικών σχέσεων μεταξύ των εννοιών. Επιπλέον, χρησιμοποιούνται μηχανισμοί προσοχής για τη συγκέντρωση των πληροφοριών από τους εισερχόμενους και εξερχόμενους γείτονες, οι οποίοι βοηθούν το μοντέλο να συλλάβει αποτελεσματικά τις σημασιολογικές πληροφορίες. Το μοντέλο μας ξεπερνά την υπερσύγχρονη νευρωνική προσέγγιση με τα σημεία 1.5 στα σημεία και επιτυγχάνει νέες επιδόσεις τελευταίας τεχνολογίας.', 'it': "La rappresentazione astratta del significato (AMR)-to-text generation è il compito impegnativo di generare testi in linguaggio naturale da grafici AMR, dove i nodi rappresentano concetti e i bordi indicano relazioni. Gli attuali metodi all'avanguardia utilizzano modelli da grafico a sequenza; Tuttavia, non possono ancora superare significativamente i precedenti modelli sequenza-sequenza o approcci statistici. In questo articolo, proponiamo un nuovo modello grafico-sequenza (Graph Transformer) per affrontare questo compito. Il modello codifica direttamente i grafici AMR e impara le rappresentazioni dei nodi. Per calcolare le relazioni semantiche tra i concetti viene utilizzata una funzione di interazione a coppie. Inoltre, i meccanismi di attenzione sono utilizzati per aggregare le informazioni dai vicini in entrata e in uscita, che aiutano il modello a catturare efficacemente le informazioni semantiche. Il nostro modello supera l'approccio neurale all'avanguardia con 1,5 punti BLEU su LDC2015E86 e 4,8 punti BLEU su LDC2017T10 e raggiunge nuove prestazioni all'avanguardia.", 'kk': 'Абстрактық мәліметті (AMR) мен мәтіннің құрылуы - табиғи тілдер мәтіндерін AMR графиктерінен құру үшін көпшілік тапсырмасы, бұл топтардың концепциялар мен шектері қатынастарды көрсетеді. Қолданыстағы суреттің күй- жай әдістері графика- ті реттеу үлгілерін қолданылады; amount in units (real) Бірақ олар әлі алдыңғы реттеу үлгілеріне не статистикалық жағдайларына көмектесе алмайды. Бұл қағазда, бұл тапсырманың адресін түсінуге (графикалық түрлендіруші) романдық графикалық түрлендіруші үлгісін таңдаймыз. Бұл үлгі AMR графиктерін тікелей кодтамасыз және тобының келтірімдерін үйренеді. Екі жүйелік интерфейс функциясы концепциялардың семантикалық қатынасын есептеу үшін қолданылады. Сонымен қатар, келесі және келесі соңғылардан мәліметті біріктіру үшін, бұл үлгілерді семантикалық мәліметті әсер етіп алу үшін қолданылады. Біздің үлгіміз LDC2015E86 және 4,8 BLEU нүктелерінде 1,5 BLEU нүктелерінде суреттік невралдық тәртібін жеткізеді және жаңа суреттік тәртібін жеткізеді.', 'mk': 'Апстрактно значење претставување (АМР) до текст генерација е предизвикувачката задача за генерација на природни јазични тексти од графиците на АМР, каде што јазолите претставуваат концепти и рабови ги одредуваат односите. Сегашните најнови методи користат графички до секвенциски модели; сепак, тие сé уште не можат значително да ги надминат претходните модели од секвенца до секвенца или статистичките пристапи. In this paper, we propose a novel graph-to-sequence model (Graph Transformer) to address this task.  Моделот директно ги кодира графиките на AMR и ги научи претставувањата на јазлите. Функција на интеракција на парови се користи за пресметка на семантичните односи помеѓу концептите. Покрај тоа, механизмите за внимание се користат за агрегирање на информациите од соседите кои доаѓаат и излегуваат, кои му помагаат на моделот ефикасно да ги заземе семантичните информации. Нашиот модел го надминува најсовремениот нервен пристап за 1,5 БЛЕУ поени на ЛДЦ2015Е86 и 4,8 БЛЕУ поени на ЛДЦ2017Т10 и постигнува нови најсовремени претстави.', 'lt': 'Abstract meaning representation (AMR)-to-text generation is the challenging task of generating natural language texts from AMR graphs, where nodes represent concepts and edges denote relations. Dabartiniai pažangiausi metodai naudoja grafiką iš eilės; tačiau jos vis dar negali gerokai viršyti ankstesnių sekos modelių ar statistinių metodų. Šiame dokumente siūlome naują grafikų eilės model į (grafikų transformatorių), kad būtų galima spręsti šią užduotį. Modelis tiesiogiai koduoja AMR grafikus ir sužino mazgų rodmenis. Apskaičiuojant semantinius sąvokų santykius naudojama pora sąveikos funkcija. Be to, teikiant informaciją iš atvykstančių ir išvykstančių kaimynų, naudojami dėmesio mechanizmai, kurie padeda modeliui veiksmingai surinkti semantinę informaciją. Mūsų modelis viršija pažangiausią neurologinį požiūrį 1,5 BLEU taško, palyginti su LDC2015E86, ir 4,8 BLEU taško, palyginti su LDC2017T10, ir pasiekia naujų pažangiausių rezultatų.', 'ml': 'എംആര്\u200d ഗ്രാഫ്റ്റുകളില്\u200d നിന്നും സ്വാഭാവിക ഭാഷ പദാവലികള്\u200d ഉണ്ടാക്കുന്നതിന്റെ പ്രതിനിധിയുടെ (AMR)- ലേഖന തലമുറയാണ് അസ്ട്രാക്ട്രാക ഇപ്പോഴത്തെ നിലവിലുള്ള- ആര്\u200dട്ടിന്റെ രീതികള്\u200d ഗ്രാഫിലേക്കു് മോഡലുകള്\u200d ഉപയോഗിക്കുന്നു; എങ്കിലും മുമ്പുള്ള സെക്കന്\u200dസ് മോഡലോ സ്റ്റാട്ടിസ്റ്റിക്കല്\u200d മാതൃകങ്ങളോ പ്രവര്\u200dത്തിപ്പിക്കാന്\u200d അവര്\u200dക്ക് പ് ഈ പത്രത്തില്\u200d, ഈ ജോലിയെ വിവരപ്പെടുത്തുവാന്\u200d നോവല്\u200d ഗ്രാഫ്റ്റോ സെക്കന്\u200dസ് മോഡല്\u200d (ഗ്രാഫ് ട്രാന്\u200dസ്ഫോര്\u200dമാര്\u200dഫ നേരിട്ട് AMR ഗ്രാഫ്റ്റുകള്\u200d എക്കോഡ് ചെയ്യുകയും നോഡ് പ്രതിനിധികള്\u200d പഠിക്കുകയും ചെയ്യുന്നു. ആശയങ്ങള്\u200dക്കിടയിലുള്ള സെമാന്റിക്ക് ബന്ധങ്ങള്\u200d കണക്ക് ചെയ്യാന്\u200d ഒരു ജോട്ട് ഇടപെടുത്തുന്നു. കൂടാതെ, അകത്തുവരുന്നതില്\u200d നിന്നും പുറത്തുപോകുന്ന അയല്\u200dക്കാരില്\u200d നിന്നും വിവരങ്ങള്\u200d കൂടുതല്\u200d വിവരങ്ങള്\u200d ചേര്\u200dക്കാന്\u200d ശ്രദ്ധിക്കു ഞങ്ങളുടെ മോഡല്\u200d ലെഡിസി2015E86 ല്\u200d നിന്നും 4. 8 ബില്ലൂ പോയിന്\u200dറുകളില്\u200d നിന്നും പുതിയ സ്റ്റേറ്റ് സ്റ്റേറ്റ് സ്റ്റേറ്റ് സ്റ്റേറ്റ് സ്റ്റേറ്റ് സ്റ്റേറ്', 'ms': 'Abstract meaning representation (AMR)-to-text generation is the challenging task of generating natural language texts from AMR graphs, where nodes represent concepts and edges denote relations.  Kaedah state-of-the-art semasa menggunakan model graf-to-sequence; bagaimanapun, mereka masih tidak dapat melebihi secara signifikan model urutan-ke-urutan sebelumnya atau pendekatan statistik. Dalam kertas ini, kami cadangkan model graf-ke-urutan (Transformer Graf) untuk mengatasi tugas ini. Model secara langsung mengekod graf AMR dan belajar perwakilan nod. A pairwise interaction function is used for computing the semantic relations between the concepts.  Selain itu, mekanisme perhatian digunakan untuk mengumpulkan maklumat dari jiran yang masuk dan keluar, yang membantu model untuk menangkap maklumat semantik secara efektif. Model kita melebihi pendekatan saraf terbaik dengan 1.5 titik BLEU pada LDC2015E86 dan 4.8 titik BLEU pada LDC2017T10 dan mencapai pertunjukan terbaik.', 'mt': 'Ir-rappreżentanza tat-tifsira assoluta (AMR) għat-test hija l-kompitu diffiċli li jiġu ġġenerati testi lingwistiċi naturali mill-grafiċi AMR, fejn in-nodi jirrappreżentaw kunċetti u t-truf jindikaw relazzjonijiet. Il-metodi l-aktar avvanzati bħalissa jużaw mudelli minn graff għal sekwenza; madankollu, xorta ma jistgħux jaqbżu b’mod sinifikanti l-mudelli minn sekwenza għal sekwenza preċedenti jew l-approċċi statistiċi. F’dan id-dokument, qed nipproponu mudell ġdid ta’ graff għal sekwenza (Trasferiment ta’ Graff) biex nindirizzaw dan il-kompitu. Il-mudell jikkodifika direttament il-grafiċi AMR u jitgħallem ir-rappreżentazzjonijiet tan-nodi. Funzjoni ta’ interazzjoni bejn il-pari tintuża għall-kalkolu tar-relazzjonijiet semantiċi bejn il-kunċetti. Barra minn hekk, jintużaw mekkaniżmi ta’ attenzjoni għall-aggregazzjoni tal-informazzjoni mill-ġirien li jidħlu u li joħorġu, li jgħinu lill-mudell jaqbad l-informazzjoni semantika b’mod effettiv. Il-mudell tagħna jaqbeż l-approċċ newrali l-aktar avvanzat b’1.5 punti BLEU fuq LDC2015E86 u 4.8 punti BLEU fuq LDC2017T10 u jilħaq prestazzjonijiet ġodda l-aktar avvanzati.', 'mn': 'Абстракт утгын үзүүлэлт (AMR)-ээс-текст бүтээлт нь байгалийн хэл хэлний текстүүдийг AMR графикаас бий болгох шаардлагатай ажил юм. Гэхдээ nodes нь ойлголт болон талуудыг харилцаа илэрхийлдэг. Одоогийн урлагийн төвшин аргыг график-т дарааллын загварыг ашигладаг. Гэхдээ тэд өмнөх дарааллаас дарааллаас эсвэл статистикийн арга загвараас илүү чухал болж чадахгүй. Энэ цаасан дээр бид энэ ажлыг зохицуулахын тулд шинэ график-т дарааллын загвар (график түвшигч) загварыг санал болгож байна. Загвар нь шууд AMR графиктүүдийг кодлож, node илтгэлийг сурдаг. Хоёр талаар харилцааны функц ойлголтын хоорондын семантик харилцааныг тооцоолох үед хэрэглэгддэг. Үүнээс ч анхаарлын механизмууд орлох болон гарах хөршүүдийн мэдээллийг цуглуулахын тулд хэрэглэгддэг. Энэ загвар нь семантик мэдээллийг эффективно авах тулд загвар юм. Бидний загвар нь LDC2015E86 болон 4.8 BLEU цэгүүдийн НРC2017T10 дээрх 1.5 BLEU цэгүүдийн урлагийн мэдрэлийн төвшин ойлголтыг дамжуулж, шинэ урлагийн үйл ажиллагааг гаргадаг.', 'no': 'Abstrakt betydning representasjon (AMR)-til-tekstgenerering er den vanskelege oppgåva for å laga naturspråkstar frå AMR- grafen, der noder representerer konseptar og kantar for forholdet. Den gjeldande metodane for kunsten brukar modeller for graf-til-sekvens; Men dei kan fortsatt ikkje utføra dei førre rekkjefølgjande modelane eller statistiske tilnærmingar. I denne papiret foreslår vi eit nytt graf- til- sekvensmodell (grafikktransformer) for å adressa denne oppgåva. Modellen kodar direkte AMR- grafen og lærer noderepresentasjonane. Ein par interaksjonsfunksjon vert brukt for å rekna ut semantiske forhold mellom konseptane. I tillegg vert oppmerkingsmekanismane brukte for å samla informasjonen frå innkommande og utgående naboane, som hjelper modellen til å henta semantiske informasjon effektivt. Vårt modell utfører tilstanden til kunsten i neuraltilnærming med 1,5 BLEU-punkt på LDC2015E86 og 4,8 BLEU-punkt på LDC2017T10 og oppnår nye kunstendige utføringar.', 'pl': 'Generowanie abstrakcyjnej reprezentacji znaczenia (AMR)-do tekstu jest wyzwaniem generowania tekstów językowych naturalnych z wykresów AMR, gdzie węzły reprezentują koncepcje, a krawędzie oznaczają relacje. Obecnie najnowocześniejsze metody wykorzystują modele wykresu-sekwencji; Jednakże nadal nie mogą one znacznie przewyższyć poprzednich modeli sekwencji-sekwencji lub podejść statystycznych. W niniejszym artykule proponujemy nowy model graf-to-sequence (Graph Transformer), aby sprostać temu zadaniu. Model bezpośrednio koduje wykresy AMR i uczy się reprezentacji węzłów. Funkcja interakcji parowych służy do obliczania relacji semantycznych między pojęciami. Ponadto do agregacji informacji z przychodzących i wychodzących sąsiadów wykorzystywane są mechanizmy uwagi, które pomagają modelowi skutecznie uchwycić informacje semantyczne. Nasz model przewyższa najnowocześniejsze podejście neuronowe o punkty 1.5 BLEU na LDC2015E86 i 4.8 BLEU na LDC2017T10 i osiąga nowe wydajności.', 'ro': 'Generarea abstractă de reprezentare a semnificației (AMR)-la-text este sarcina provocatoare de a genera texte în limbaj natural din grafice AMR, unde nodurile reprezintă concepte și marginile denotă relații. Metodele actuale de ultimă generaţie utilizează modele grafice-secvenţă; Cu toate acestea, acestea nu pot depăși în mod semnificativ modelele secvență-secvență sau abordările statistice anterioare. În această lucrare, propunem un nou model grafic-la-secvență (Graph Transformer) pentru a aborda această sarcină. Modelul codează direct graficele AMR și învață reprezentările nodurilor. O funcție de interacțiune pereche este utilizată pentru calcularea relațiilor semantice dintre concepte. Mai mult decât atât, mecanismele de atenție sunt utilizate pentru agregarea informațiilor de la vecinii intrați și ieșiți, care ajută modelul să capteze eficient informațiile semantice. Modelul nostru depășește abordarea neurală de ultimă generație cu 1,5 puncte BLEU pe LDC2015E86 și 4,8 puncte BLEU pe LDC2017T10 și obține noi performanțe de ultimă generație.', 'sr': 'Apstraktivno značenje predstavljanja (AMR)-na-tekst generacija je izazovni zadatak stvaranja prirodnih jezičkih tekstova iz grafika AMR-a, gde čvorovi predstavljaju koncept i ivice pokazuju odnose. Trenutne metode umetnosti koriste modele grafika do sekvence; međutim, oni još uvek ne mogu značajno izvršiti prethodne modele sekvence do sekvence ili statističke pristupe. U ovom papiru predlažemo novi model grafika do sekvence (Transformer grafika) da se obratimo ovom zadatku. Model direktno kodira AMR grafike i nauči zastupanje čvorova. Funkcija interakcije na parovima se koristi za raèunanje semantičkih odnosa između koncepta. Osim toga, mehanizami pažnje se koriste za okupljanje informacija od dolaznih i izlazećih suseda, koji pomognu modelu da efektivno uhvati semantičke informacije. Naš model iznosi državni neuralni pristup 1,5 BLEU točki na LDC2015E86 i 4,8 BLEU točki na LDC2017T10 i postiže nove postupke umetnosti.', 'si': 'ප්\u200dරතිශ්න අදහසක් ප්\u200dරතිශාලය (AMR)-to-text ප්\u200dරතිශාලය තමයි ස්වභාවික භාෂාව පාළුවන් නිර්මාණය කරපු වැඩය AMR ග්\u200dරාෆික්ස් ව ප්\u200dරස්තූත ස්ථිතිය- of- the- art විද්\u200dයාවය Graph- to- sequence විද්\u200dයාවය භාවිත කරන්න; ඒත්, ඔවුන්ට තවමත් වැදගත් වෙන්න බැරි වෙන්න පුළුවන් පිළිබඳ පරීක්ෂණය සහ සංඛ්\u200dයාත්මක ප්\u200dරවේශන මේ පැත්තේ, අපි මේ වැඩේ විශ්වාස කරන්න නියම් ග්\u200dරාෆ් එක්ක සැකසුම් මද්\u200dයාලයක් ප්\u200dරයෝජනය කරනවා. මොඩල් හරියටම AMR ග්\u200dරාෆ් එක සංකේතය කරනවා ඒ වගේම නෝඩ් ප්\u200dරතිනිධානය ඉගෙන ගන්නවා. Name එතකොට, අවධානය සැකසුම් භාවිතා වෙනවා ඇවිත් සහ එළියට ගිහින් යන්න තොරතුරු සම්බන්ධ වෙනුවෙන්, ඒකෙන් මොඩේල් එක්  අපේ මොඩේලය LDC2015E86 සහ LDC2017T10 වලින් අළුත් ස්ථානයක් ලැබෙනවා.', 'sv': 'Abstract meaning representation (AMR)-to-text generation är den utmanande uppgiften att generera naturligt språk texter från AMR grafer, där noder representerar begrepp och kanter betecknar relationer. De nuvarande toppmoderna metoderna använder diagram-till-sekvensmodeller. De kan dock fortfarande inte väsentligt överträffa tidigare sekvensmodeller eller statistiska metoder. I denna uppsats föreslår vi en ny graf-till-sekvensmodell (Graph Transformer) för att hantera denna uppgift. Modellen kodar AMR-graferna direkt och lär sig nodrepresentationerna. En parvis interaktionsfunktion används för att beräkna de semantiska relationerna mellan begreppen. Dessutom används uppmärksamhetsmekanismer för att aggregera information från inkommande och utgående grannar, vilket hjälper modellen att fånga in semantisk information effektivt. Vår modell överträffar den senaste neurala metoden med 1,5 BLEU-punkter på LDC2015E86 och 4,8 BLEU-punkter på LDC2017T10 och uppnår nya toppmoderna prestanda.', 'so': 'Qoraalka qoraalka (AMR)-ilaa-text-generation waa shaqada dhibaatada leh oo ka abuurta qoraalka afka asalka ah ee AMR, meesha nodes represents fekero iyo edges denote relations. Isku isticmaala tilmaamaha sawirka-sawirka Si kastaba ha ahaatee, marnaba ma sii wadi karaan tusaalooyinkii hore ee soo socday ama qaabab takhasuska ah. Warqadan waxaynu ku soo jeedaynaa model sawir-sawir-to-dabar (Graph Transformer) si aan u baaraandegiso shaqadaas. Tusaale wuxuu toos u kooban yahay sawirada AMR, wuxuuna baran karaa noocyada. Shaqooyin isku xiriir ah waxaa loo isticmaalaa in la xisaabiyo xiriirka semantika ee fikrada dhexdooda. Moreover, attention mechanisms are used for aggregating the information from the incoming and outgoing neighbors, which help the model to capture the semantic information effectively.  Tusaale ahaan ayaa sameynaya xaaladda farshaxanka ee farshaxanka ah 1.5 BLEU barta LDC2015E86 iyo 4.8 BLEU barta LDC2017T10, wuxuuna gaadhaa bandhigyada farshaxanka cusub.', 'ta': 'AMR வரைபடங்களில் இருந்து இயல்பான மொழி உரையை உருவாக்குவதற்கான சவாலிக்க செயல் தற்போதைய நிலை- கலை முறைமைகள் வரைப்படத்திற்கு தொடர்ந்து மாதிரிகளை பயன்படுத்து; ஆனால், அவர்கள் இன்னும் முந்தைய வரிசையில் இருந்து வரும் மாதிரிகளை அல்லது புள்ளிவிவரமான முறைமைகளை செயல்படுத்த முட இந்த காகிதத்தில், நாம் இந்த செயலை முகவரிப்பதற்கு ஒரு புதிய வரைப்படம்- to- தொடர்ந்து மாதிரியை பரிந்துரைக்கிறோம். இந்த மாதிரி AMR வரைபடங்களை நேரடியாக குறியிடுகிறது மற்றும் நுட்பம் குறிமுறைகளை கற்றுகொள்கிறது. கருத்துக்களுக்கிடையில் இருந்து பெரும்பாடு இணைப்பு செயல்பாடு கணக்கிட பயன்படுத்தப்படுகிறது. மேலும், கவனம் முறைமைகள் உள்வரும் மற்றும் வெளியேறும் அண்டைகளிலிருந்து தகவல்களை பெருகுத்துவதற்கு பயன்படுத்தப்படுகிறது, அது மாதிரி எங்கள் மாதிரி LDC2015E86 மற்றும் 4. 8 பிலியு புள்ளிகள் LDC2017T10 மற்றும் புதிய நிலை- கலை செயல்பாடுகளை அடைகிறது.', 'ur': 'مطلب معنی نمایش (AMR)-ٹ-ٹکسٹ نسل کی تعبیر AMR گراف سے طبیعی زبان کی پیغام پیدا کرنے کا مشکل کام ہے جہاں نوڈ تفصیل اور کنارے کی تعبیر کرتے ہیں۔ موجود حالت-آرت طریقے گراف-تا-سفارش مدل استعمال کرتے ہیں; لیکن وہ ابھی بھی اگلوں کی سفارش کے ذریعہ سے زیادہ اضافہ نہیں کر سکتے۔ اس کاغذ میں، ہم نے اس کام کے بارے میں ایک نئی گراف-to-sequence موڈل (گراف ترنسفور) کی پیشنهاد کریں۔ Model directly encode the AMR graphs and learns the node representations. ایک جوڑی اپنا اپنا کامپیوتر کامپیوتر کی کمپیوتر کے لئے استعمال کیا جاتا ہے. اور اس کے علاوہ، توجه کے مکانیزوں کا استعمال کیا جاتا ہے کہ آتے اور باہر جاتے ہوئے گھر والوں سے اطلاعات جمع کریں، جو مدل کو سیمنٹی اطلاعات کے مطابق اثبات کرنے کے لئے مدد کرتی ہے. ہمارا مدل LDC2015E86 اور 4.8 BLEU پوینٹوں میں LDC2017T10 پر 1.5 BLEU پوینٹوں کے ذریعے نئورل طریقے سے زیادہ اضافہ کرتا ہے اور نئی حالت کی عملیات کو پہنچاتا ہے.', 'uz': "Name The current state-of-the-art methods use graph-to-sequence models;  Lekin ular oldingi seksirlik modellari yoki statistika usullarni amalga oshirib boʻlmaydi. Bu hujjatda, biz bu vazifani boshqarish uchun novel grafik to sequence modeli (Graph Transfer). Name Name Ko'rib chiqish va chiqish murakkablaridan maʼlumotni ajratish uchun foydalanadi. Bu model semantik maʼlumotini tasdiqlash uchun foydalanadi. Bizning modelimiz LDC2015E86 va 4.8 BLEU nuqta LDC2017T10 davomida 1.5 BLEU nuqta boshlaydi va yangi San'atning vazifalarini bajaradi.", 'vi': 'Tóm tắt ý nghĩa đại diện (AMR) thành chữ là nhiệm vụ thử thách tạo ra các văn bản ngôn ngữ tự nhiên từ các biểu đồ AMR, nơi các nút đại diện cho các khái niệm và cạnh chỉ ra các mối quan hệ. Các phương pháp hiện đại sử dụng các mô hình đồ thị-tới-dãy; Tuy nhiên, chúng vẫn không thể vượt qua quy mô phân biệt trước hoặc các phương pháp thống kê trước đây. Trong tờ giấy này, chúng tôi đề xuất một mô hình đồ thị-tới-trình tự mới (Graph transformer) để thực hiện nhiệm vụ này. Mô hình này mã hóa trực tiếp các biểu đồ AMR và lấy được các biểu tượng nút. Một hàm tương tác kép được dùng để tính toán các quan hệ ngữ pháp giữa các khái niệm. Hơn nữa, các cơ quan chú ý được dùng để tổng hợp thông tin từ hàng xóm đến và đi, giúp mô hình thu thập thông tin ngữ pháp một cách hiệu quả. Cách của chúng tôi hoàn thành bước tiếp cận thần kinh hiện đại của 1', 'bg': 'Генерирането на абстрактно значение (АМР) към текст е предизвикателната задача за генериране на текстове на естествен език от АМР графики, където възлите представляват понятия, а ръбовете означават връзки. Съвременните методи използват модели графика-последователност; Въпреки това, те все още не могат значително да надминат предишните модели последователност към последователност или статистически подходи. В настоящата статия предлагаме нов модел графика-последователност (Граф трансформатор) за справяне с тази задача. Моделът директно кодира графиките и научава представянето на възлите. Функция за взаимодействие по двойки се използва за изчисляване на семантичните отношения между понятията. Освен това се използват механизми за внимание за агрегиране на информацията от входящите и изходящите съседи, които помагат на модела да улавя семантичната информация ефективно. Нашият модел превъзхожда най-съвременния невронен подход с 1.5 точки на и 4.8 точки на постига нови съвременни изпълнения.', 'da': 'Abstrakt betydning repræsentation (AMR)-til-tekst generation er den udfordrende opgave at generere natursprogtekster fra AMR grafer, hvor knuder repræsenterer begreber og kanter betegner relationer. De nuværende state-of-the-art metoder anvender graf-to-sekvensmodeller; De kan imidlertid stadig ikke væsentligt overgå de tidligere sekvens-til-sekvensmodeller eller statistiske metoder. I denne artikel foreslår vi en ny graf-til-sekvensmodel (Graph Transformer) til at løse denne opgave. Modellen koder direkte AMR graferne og lærer node repræsentationer. En parvis interaktionsfunktion bruges til at beregne de semantiske relationer mellem begreberne. Desuden bruges opmærksomhedsmekanismer til at aggregere oplysningerne fra de indgående og udgående naboer, hvilket hjælper modellen til at fange den semantiske information effektivt. Vores model overgår den avancerede neurale tilgang med 1,5 BLEU-punkter på LDC2015E86 og 4,8 BLEU-punkter på LDC2017T10 og opnår nye state-of-the-art præstationer.', 'hr': 'Abstrakt značenje predstavljanja (AMR)-na-tekst generacija je izazovni zadatak stvaranja prirodnih jezičkih tekstova iz grafika AMR-a, gdje čvorovi predstavljaju koncept i ivice pokazuju odnose. Trenutne metode umjetnosti koriste modele grafika do sekvence; međutim, oni još uvijek ne mogu značajno iznijeti prethodne modele u sekvenciji ili statističke pristupe. U ovom papiru predlažemo novi model grafika do sekvencije (Transformer grafika) da se riješi ovaj zadatak. Model direktno kodira AMR grafike i uči zastupanje čvorova. Funkcija interakcije na parovima se koristi za računalo semantičkih odnosa između koncepta. Osim toga, mehanizami pažnje se koriste za skupljanje informacija od dolaznih i izlazećih susjeda, koji pomažu modelu učinkovito uhvatiti semantičke informacije. Naš model iznosi državni neuralni pristup 1,5 BLEU točka na LDC2015E86 i 4,8 BLEU točka na LDC2017T10 i postiže nove postupke umjetnosti.', 'id': 'Perwakilan arti abstrak (AMR)-ke-teks generasi adalah tugas menantang untuk menghasilkan teks bahasa alami dari grafik AMR, di mana node mewakili konsep dan pinggir menunjukkan hubungan. Metode state-of-the-art saat ini menggunakan model grafik-ke-urutan; namun, mereka masih tidak dapat melampaui batas lebih dari model urutan ke urutan sebelumnya atau pendekatan statistik. Dalam kertas ini, kami mengusulkan model grafik ke urutan novel (Graph Transformer) untuk mengatasi tugas ini. Model langsung mengkode grafik AMR dan mempelajari representation node. Fungsi interaksi pasangan digunakan untuk menghitung hubungan semantis antara konsep. Selain itu, mekanisme perhatian digunakan untuk agregasi informasi dari tetangga yang masuk dan keluar, yang membantu model untuk menangkap informasi semantis secara efektif. Model kita melebihi pendekatan saraf terbaik dengan 1,5 poin BLEU pada LDC2015E86 dan 4,8 poin BLEU pada LDC2017T10 dan mencapai pertunjukan terbaik.', 'ko': '추상적 의미 표현(AMR) - 텍스트 생성은 AMR 그림에서 자연 언어 텍스트를 생성하는 도전적인 작업으로, 노드는 개념을 나타내고 모서리는 관계를 나타낸다.현재 가장 선진적인 방법은 도형을 사용하여 모델을 정렬한다.그러나 그것들은 여전히 이전의 서열 간 모델이나 통계 방법보다 현저하게 우수하지 않다.본고에서 우리는 이 문제를 해결하기 위해 새로운 그림에서 서열 모델(그림 변환기)을 제시했다.이 모델은 AMR 그림을 직접 인코딩하고 노드 표현을 학습합니다.상호작용 함수로 개념 간의 의미 관계를 계산하는 데 사용한다.또한 주의 메커니즘은 이웃으로부터 전해지고 전해지는 정보를 모으는 데 사용되며 모델이 의미 정보를 효과적으로 포착하는 데 도움이 된다.우리의 모델은 LDC2015E86과 LDC2017T10에서 각각 가장 선진적인 신경법보다 1.5개의 BLEU점과 4.8개의 BLEU점 높아 가장 선진적인 성능에 이르렀다.', 'nl': 'Abstracte betekenisrepresentatie (AMR)-naar-tekst genereren is de uitdagende taak van het genereren van natuurlijke taalteksten uit AMR-grafieken, waarbij knooppunten concepten vertegenwoordigen en randen relaties aanduiden. De huidige state-of-the-art methoden maken gebruik van grafiek-to-sequence modellen; Ze kunnen echter nog steeds niet significant presteren dan de vorige sequentiemodellen of statistische benaderingen. In dit artikel stellen we een nieuw graph-to-sequence model (Graph Transformer) voor om deze taak aan te pakken. Het model codeert direct de AMR grafieken en leert de node representaties. Een pairwise interactiefunctie wordt gebruikt voor het berekenen van de semantische relaties tussen de concepten. Bovendien worden aandachtsmechanismen gebruikt voor het aggregeren van de informatie van de inkomende en uitgaande buren, die het model helpen om de semantische informatie effectief vast te leggen. Ons model overtreft de state-of-the-art neurale benadering met 1.5 BLEU punten op LDC2015E86 en 4.8 BLEU punten op LDC2017T10 en bereikt nieuwe state-of-the-art prestaties.', 'de': 'Abstrakte Bedeutungsrepr瓣sentation (AMR)-zu-Text Generierung ist die herausfordernde Aufgabe, natursprachliche Texte aus AMR-Graphen zu generieren, bei denen Knoten Konzepte darstellen und Kanten Beziehungen kennzeichnen. Die derzeitigen Methoden verwenden Graph-zu-Sequenz-Modelle; Dennoch k繹nnen sie die bisherigen Sequenzmodelle oder statistischen Ans瓣tze nicht signifikant 羹bertreffen. In diesem Beitrag schlagen wir ein neuartiges Graph-to-Sequence-Modell (Graph Transformer) vor, um diese Aufgabe zu l繹sen. Das Modell codiert direkt die AMR-Graphen und lernt die Knotendarstellungen. F羹r die Berechnung der semantischen Beziehungen zwischen den Konzepten wird eine paarweise Interaktionsfunktion verwendet. Dar羹ber hinaus werden Aufmerksamkeitsmechanismen f羹r die Aggregation der Informationen aus den eingehenden und ausgehenden Nachbarn verwendet, die dem Modell helfen, die semantischen Informationen effektiv zu erfassen. Unser Modell 羹bertrifft den hochmodernen neuronalen Ansatz um 1.5 BLEU Punkte auf LDC2015E86 und 4.8 BLEU Punkte auf LDC2017T10 und erzielt neue State-of-the-Art Leistungen.', 'sw': 'Inamaanisha kuwakilisha (AMR)-hadi kizazi cha maandishi ni kazi ya changamoto ya kutengeneza maandishi ya lugha asili kutoka kwenye picha za AMR, ambapo nodes inawakilisha dhana na upande wa mahusiano ya kura. mbinu za sasa za sanaa zinatumia mifano ya picha kwa mfululizo; Hata hivyo, bado hawawezi kutekeleza mifano ya awali ya mfululizo au mbinu za takwimu. Katika karatasi hii, tunapendekeza modeli ya riwaya kwa mfululizo (Graph Transfer) kuhusiana na kazi hii. Mfano unajumuisha picha za AMR na kujifunza uwakilishi wa kipindi hiki. Mpango wa mahusiano yanatumiwa kwa ajili ya kuhesabu mahusiano ya kimapenzi kati ya dhana. Zaidi ya hayo, mfumo wa ufuatiliaji unatumiwa kuongeza taarifa kutoka kwa majirani zao zinazoingia na waliotoka nje, ambazo zinasaidia muundo wa kukamata taarifa za kimapenzi kwa ufanisi. Mfano wetu unaonyesha hali ya kisasa ya kisasa kwa alama 1.5 BLEU kwenye pointi za LDC2015E86 na 4.8 BLEU kwenye LDC2017T10 na kupata matukio mapya ya sanaa.', 'fa': 'نمایش معنی مطلق (AMR)-به متن، کار مشکلی برای تولید متن زبان طبیعی از گرافهای AMR است، جایی که گرافها نمایش مفهوم و لبه\u200cها رابطه\u200cها را نشان می\u200cدهند. روش\u200cهای موقعیت هنر فعلی از مدل\u200cهای گراف به ترکیب استفاده می\u200cکند. با این حال، هنوز نمی\u200cتوانند مدل\u200cهای پیشینی یا نزدیک\u200cهای آمار\u200cشناسی را به طور معنی برسانند. در این کاغذ، ما یک مدل نویس گراف به ترکیب (ترکیب گراف) را پیشنهاد می\u200cکنیم تا این کار را حل کند. مدل مستقیماً گرافیک AMR را رمزگذاری می\u200cکند و نمایش گراف را یاد می\u200cگیرد. یک عملکرد ارتباط با جفت طریق برای محاسبه رابطه\u200cهای semantic بین مفهوم استفاده می\u200cشود. علاوه بر این، مکانیسم توجه برای جمع اطلاعات از همسایه\u200cهای وارد می\u200cشوند و از بیرون می\u200cروند، که به مدل کمک می\u200cکنند تا اطلاعات semantic را به طور تاثیر بگیرند. مدل ما روش عصبی ایالت هنری را با نقطه ۱.۵ BLEU در LDC2015E86 و ۴.۸ BLEU نقطه\u200cهای LDC2017T10 انجام می\u200cدهد و به اجرای ایالت هنری جدید رسیده است.', 'tr': 'Abstrakt anlamlı temsil (AMR)-dan-metin döredilmesi, doğal dil metinleri AMR grafiklerinden oluşturmak üçin çözümli göreldir. Bu ýerde nodeler düşünceleri we çukurları ifade ediyor. Häzirki möhüm modler grafik üçin hat modellerini ullan; Ýöne, öňki hatlaryň hatlaryndan hatlaryň üstüne bir şekilde çykyp bilmeýärler. Bu kagyzda, biz bu işi çözmek üçin roman grafik üçin terjime nusgasyny teklip edýäris. Model AMR grafiklerini doğrudan kodlayır ve düğüm ifadelerini öğrenir. Bir çift sanal faýly düşünjeler arasyndaky semantik baglaýyşlary hesaplamak üçin ullanýar. Munuň ýagdaýynda, üns meýdançalary gelen we çykan goňşularyň maglumaty toplamak üçin ullanýar. Bu semantik maglumaty etkinleştirmek üçin nusgasyna kömek edýär. Biziň nusgymyz LDC2015E86 we 4.8 BLEU noktalary LDC2017T10 we täze möhüm taýýarlarynda çykýar.', 'am': 'የጽሑፍ ትውልድ (AMR)-ወደ-text ትውልድ የፍጥረት ቋንቋ ጽሑፎችን ከAMR ቀለሞች የመፍጠር ማድረግ የሚያስጨንቅ ስራ ነው፤ አዲስ ዶሴዎችን እና ድምፅ ግንኙነትን የሚያስተካክሉ አዲስ ግንኙነት ነው፡፡ የአሁኑ ሁኔታ ምንም እንኳን፣ የቀድሞውን የግንኙነት እና የግንኙነት ዓይነቶች እና የstatistical ግንኙነት ማድረግ አይችሉም፡፡ በዚህ ፕሮግራም፣ ይህንን ስራ ለመጠቀም የነጥብ graph-to-sequence model (Graph Transfer) እናሳልጋለን፡፡ The model directly encodes the AMR graphs and learns the node representations.  አዲስ ዶሴ ፍጠር ደግሞም፣ ማስታወቂያውን ከመግባትና ከሚወጡት ጎረቤቶች ማውጣት የተጠቃቀሙ ስልጣናዎች የሚጠቀሙ ናቸው፡፡ ሞዴሌያችን የ-የ-የ-የ-የ-የ-የ-የ-የ-የ-የ-ጥያቄውን 1.5 BLEU ጥያቄ በLDC2015E86 እና 4.8 BLEU points በLDC2017T10 እና አዲስ የ-የ-የ-የ-የ-የ-art ጥያቄዎችን አግኝቷል፡፡', 'sq': 'Përfaqësimi abstrakt i kuptimit (AMR)-në tekst gjenerata është detyra sfiduese e gjenerimit të teksteve të gjuhës natyrore nga grafikët AMR, ku nyjet përfaqësojnë koncepte dhe krahët tregojnë marrëdhënie. The current state-of-the-art methods use graph-to-sequence models;  megjithatë, ata ende nuk mund të kalojnë në mënyrë të konsiderueshme modelet e mëparshëm sekuencë-në-sekuencë apo qasjet statistike. Në këtë letër, propozojmë një model të ri grafik-në-sekuencë (Graph Transformer) për të trajtuar këtë detyrë. The model directly encodes the AMR graphs and learns the node representations.  A pairwise interaction function is used for computing the semantic relations between the concepts.  Përveç kësaj, mekanizmat e vëmendjes përdoren për mbledhjen e informacionit nga fqinjët e ardhshëm dhe të daljes, që ndihmojnë modelin për të kapur informacionin semantik efektivisht. Modeli ynë kryen qasjen nervore më të lartë me 1.5 pikë BLEU në LDC2015E86 dhe 4.8 pikë BLEU në LDC2017T10 dhe arrin shfaqje të reja më të larta.', 'az': 'Abstrakt anlama ifadəsi (AMR) ilə-mətn nəsili AMR grafiklərindən doğal dil mətnlərini oluşturmaq üçün çətin bir işdir. Bütün düymələr nəzəriyyətləri və kərpələri ifadəsi edirlər. Hazırkı sanat metodları graf-to-sequence modellərini kullanır; Ancaq onlar əvvəlki sıralama-sıralama modellərini və statistik tərzlərini daha çox üstün edə bilməzlər. Bu kağızda, bu işi çəkmək üçün yeni graf-to-sequence modeli (Graph Transformer) təklif edirik. Model doğrudan AMR grafiklərini kodlayır və düyüm göstəricilərini öyrənir. Bir çift təşkil fəaliyyəti düşüncələrin arasındakı semantik əlaqələri hesablamaq üçün istifadə edilir. Daha sonra, gözləmə mehanizmiləri gələn və çıxan qonşuların məlumatlarını toplamaq üçün istifadə edilir ki, modeli semantik məlumatlarını etkili olaraq almaq üçün yardım edir. Modelimiz LDC2015E86 və LDC2017T10 üzerində 4.8 BLEU noktaları olan 1.5 BLEU noktalarının nürol tərzindən başqa bir şey göstərir və yeni sanat performanslarını başa çatdırır.', 'bn': 'এএমআর গ্রাফ থেকে প্রাকৃতিক ভাষার টেক্সট তৈরি করার চ্যালেঞ্জের কাজ, যেখানে নোডের ধারণা এবং পার্শ্ববর্তী সম্পর্কের প্রতিনিধিত্ব করে। The current state-of-the-art methods use graph-to-sequence models;  তবে তারা এখনও পূর্ববর্তী সেকেন্স-থেকে সেকেন্স মডেল বা পরিসংখ্যানের ক্ষেত্রে গুরুত্বপূর্ণ কাজ করতে পার এই কাগজটিতে আমরা একটি নোভেল গ্রাফ-থেকে সেকেন্ড মডেল (গ্রাফ ট্রান্সফ্রান্সফার) প্রস্তাব করি এই কাজের কথা বলতে। এই মডেল সরাসরি এমআর গ্রাফের কোড করে এবং নোডের প্রতিনিধিত্ব শিখে। ধারণার মধ্যে সেম্পেন্টিক সম্পর্ক গণনা করার জন্য একটি জোড়া ইন্টারনেকশন ফাংশন ব্যবহার করা হয়। এছাড়াও, প্রতিবেশীদের তথ্য বৃদ্ধি করার জন্য মনোযোগ মাধ্যম ব্যবহার করা হয়েছে, যা এই মডেলে কার্যকর করে সেমেন্টিক তথ্য ধরার জন্য। আমাদের মডেল লিডিসি২০১৫ এবং ৪. ৮ বিলিউ বিন্দুকে লিডিসি২০১৭টিটি১০-এর উপর ১.', 'hy': 'Abstract meaning representation (AMR)-to-text generation is the challenging task of generating natural language texts from AMR graphs, where nodes represent concepts and edges denote relations.  Ներկայումս ամենաբարձր մեթոդները օգտագործում են գրաֆիկ-հաջորդականության մոդելներ, այնուամենայնիվ, նրանք դեռևս չեն կարող նշանակալիորեն գերազանցել նախորդ հաջորդականության մոդելները կամ վիճակագրական մոտեցումները: Այս թղթի մեջ մենք առաջարկում ենք գրաֆ-հաջորդականության նոր մոդել (Գրաֆ-տրանֆորմեր) այս խնդիրը լուծելու համար: Մոդելը անմիջապես կոդավորում է AMR գրաֆիկները և սովորում է հանգույցի ներկայացումները: Կամակի փոխազդեցության ֆունկցիան օգտագործվում է հասկացությունների միջև սեմանտիկ հարաբերությունների հաշվարկելու համար: Ավելին, ուշադրության մեխանիզմները օգտագործվում են եկած և դուրս գալիս հարևանների տեղեկատվությունը համախմբելու համար, որոնք օգնում են մոդելին արդյունավետ վերցնել սեմանտիկ տեղեկատվությունը: Մեր մոդելը գերազանցում է ամենահետաքրքիր նյարդային մոտեցումը 1.5 միավորով, որոնք գտնվում են ԼԴԿ2015E86 և 4.8 միավորով, որոնք գտնվում են ԼԴԿ2017T10-ում, և հասնում է նոր ամենահետաքրքիր արտահայտությունների:', 'ca': "La representació del significat abstract (AMR) a la generació de text és la tasca desafiadora de generar textos de llenguatge natural a partir de gràfics AMR, on nodos representan conceptes i bords indican relacions. Els mètodes actuals utilitzen models de gràfic a seqüència; no obstant això, encara no poden superar significativament els models de seqüència a seqüència anteriors o els enfocaments estadístics. En aquest article proposem un nou model gràfic a seqüència (Graph Transformer) per abordar aquesta tasca. El model codifica directament els gràfics AMR i aprene les representacions dels nodos. Una funció d'interacció entre parelles s'utilitza per calcular les relacions semàntiques entre els conceptes. Moreover, attention mechanisms are used for aggregating the information from the incoming and outgoing neighbors, which help the model to capture the semantic information effectively.  El nostre model supera l'enfocament neuronal d'última generació en 1,5 punts BLEU en LDC2015E86 i 4,8 punts BLEU en LDC2017T10 i aconsegueix noves actuacions d'última generació.", 'cs': 'Generování abstraktního významu (AMR)-na-text je náročným úkolem generování přirozených jazykových textů z AMR grafů, kde uzly představují koncepty a hrany označují vztahy. Současné nejmodernější metody využívají graf-to-sekvence modely; Nicméně stále nemohou výrazně překonat předchozí modely sekvence-sekvence nebo statistické přístupy. V tomto článku navrhujeme nový graf-to-sekvence model (Graph Transformer), který by tento úkol řešil. Model přímo kóduje AMR grafy a učí se reprezentace uzlů. Pro výpočet sémantických vztahů mezi koncepty se používá funkce párové interakce. Kromě toho se používají mechanismy pozornosti pro agregaci informací z příchozích a odchozích sousedů, které pomáhají modelu efektivně zachytit sémantické informace. Náš model překonává nejmodernější neuronový přístup o 1,5 BLEU body na LDC2015E86 a 4,8 BLEU body na LDC2017T10 a dosahuje nových nejmodernějších výkonů.', 'af': "Abstrakte betekening verteenwoording (AMR)-na-teks generasie is die aanvaardige taak van die genereer van natuurlike taal teks van AMR grafieke, waar nodes verteenwoordig konsepte en rande verteenwoordig relasies. Die huidige state- of- the- art metodes gebruik graph- to- sequence models; use graph- to- sequence models; do not translate the keyword between brackets (e. g. ServerName, ServerAdmin, etc.) maar hulle kan nog nie betekeurig uitvoer die vorige volgorde-na-volgorde modele of statistiese toegang nie. In hierdie papier, voorstel ons 'n novele graf- na- sekwensiemodel (Graf Transformer) om hierdie taak te adres. Die model kodeer direk die AMR grafieke en leer die node voorstellings. Name Ook word aandag mekanisme gebruik vir die inligting van die inkom en uitgaande naboeste, wat die model help om die semantiese inligting effektief te vang. Ons model uitvoer die state-of-the-art neural approach deur 1.5 BLEU punte op LDC2015E86 en 4.8 BLEU punte op LDC2017T10 en bereik nuwe staat-van-kunstens.", 'et': 'Abstraktse t瓣henduserepresentatsiooni (AMR) genereerimine tekstile on keeruline 羹lesanne luua looduskeelseid tekste AMR graafikutest, kus s繭lmed esindavad kontseptsioone ja servad t瓣histavad seoseid. Praegused kaasaegsed meetodid kasutavad graafikutest j瓣rjestusse suunatud mudeleid; Siiski ei suuda nad siiski oluliselt 羹letada varasemaid jada-jada mudeleid v繭i statistilisi l瓣henemisviise. K瓣esolevas t繹繹s pakume v瓣lja uue graafikust j瓣rjestusse mudeli (Graph Transformer) selle 羹lesande lahendamiseks. Mudel kodeerib otse AMR graafikud ja 繭pib s繭lmede esitused. M繭istete semantiliste suhete arvutamiseks kasutatakse paari羹hist interaktsioonifunktsiooni. Lisaks kasutatakse sissetulevate ja v瓣ljuvate naabrite teabe koondamiseks t瓣helepanumehhanisme, mis aitavad mudelil semantilist teavet t繭husalt koguda. Meie mudel 羹letab tipptasemel n瓣rvil瓣henemisviisi 1,5 BLEU punkti LDC2015E86 ja 4,8 BLEU punkti LDC2017T10 puhul ning saavutab uued tipptasemel esitused.', 'fi': 'Abstrakti merkitysrepresentaatio (AMR)-tekstiin generointi on haastava tehtävä luoda luonnonkielisiä tekstejä AMR-kaavioista, joissa solmut edustavat käsitteitä ja reunat tarkoittavat suhteita. Nykyaikaisissa menetelmissä käytetään graafista sekvenssiin perustuvia malleja. Ne eivät kuitenkaan edelleenkään pysty merkittävästi ylittämään aiempia sekvenssimalleja tai tilastollisia lähestymistapoja. Tässä työssä ehdotamme uutta graafista sekvenssiin -mallia (Graph Transformer) tämän tehtävän ratkaisemiseksi. Malli koodaa suoraan AMR-kaaviot ja oppii solmuesitykset. Käsitteiden semanttisten suhteiden laskennassa käytetään parinvaihtofunktiota. Lisäksi huomiomekanismeja käytetään keräämään tietoa saapuvilta ja lähteviltä naapureilta, jotka auttavat mallia kaappaamaan semanttisen tiedon tehokkaasti. Mallimme päihittää huipputason neurolähestymistavan 1,5 BLEU-pisteellä LDC2015E86:lla ja 4,8 BLEU-pisteellä LDC2017T10:lla ja saavuttaa uusia huipputason suorituksia.', 'bs': 'Abstraktivno značenje predstavljanja (AMR)-na-tekst generacija je izazovni zadatak stvaranja prirodnih jezičkih tekstova iz grafika AMR-a, gdje čvorovi predstavljaju koncept i ivice pokazuju odnose. Trenutne metode umjetnosti koriste modele grafika do sekvence; međutim, oni još uvijek ne mogu značajno izvršiti prethodne modele sekvence do sekvence ili statističke pristupe. U ovom papiru predlažemo novi model grafika do sekvence (Transformer grafika) da riješi ovaj zadatak. Model direktno kodira AMR grafike i nauči zastupanje čvorova. Funkcija interakcije na parovima se koristi za računalo semantičkih odnosa između koncepta. Osim toga, mehanizami pažnje se koriste za okupljanje informacija od dolaznih i izlazećih susjeda, koji pomaže modelu efektivno uhvatiti semantičke informacije. Naš model iznosi državni neuralni pristup 1,5 BLEU točka na LDC2015E86 i 4,8 BLEU točka na LDC2017T10 i ostvario nove postupke umjetnosti.', 'jv': 'AllProgressBarUpdates architecture politenessoffpolite"), and when there is a change ("assertive Nang pemilit iki, kita mulungi model tog graf-to-sekondir (Graph Transformer) kanggo nganggo nggawe barang iki. Display routing Coverage Mangka, meh dadi kapan ingkang dipunangé kanggo ngregani informasi nang ingkang ingkang sampeyan lan ijol-ijolan, sing nghelp model kanggo ngerewake informasi semantar kang efes. model sing wis nambah dadi aturan-kalungkur kanggo ngilanggar sampeyan ingkang 1.5 B luwih ingkang 1.5', 'sk': 'Generacija abstraktne predstavitve pomena (AMR) v besedilo je zahtevna naloga ustvarjanja besedil v naravnem jeziku iz AMR grafov, kjer vozlišča predstavljajo koncepte in robovi označujejo relacije. Trenutne najsodobnejše metode uporabljajo modele graf-to-zaporedje; Vendar pa še vedno ne morejo bistveno preseči prejšnjih modelov zaporedja do zaporedja ali statističnih pristopov. V prispevku predlagamo nov model graf-to-sequence (Graph Transformer) za reševanje te naloge. Model neposredno kodira AMR grafe in se nauči predstavitev vozlišč. Parna interakcijska funkcija se uporablja za izračun semantičnih relacij med koncepti. Poleg tega se uporabljajo mehanizmi pozornosti za zbiranje informacij iz dohodnih in odhodnih sosedov, ki modelu pomagajo učinkovito zajeti semantične informacije. Naš model presega najsodobnejši nevronski pristop za 1,5 točke BLEU na LDC2015E86 in 4,8 točke BLEU na LDC2017T10 ter dosega nove najsodobnejše predstave.', 'ha': "@ action: button Tsarin-halin da ake kai yanzu, za'a yi amfani da misãlai-zuwa-sequence; however, they still cannot significantly outperform the previous sequence-to-sequence models or statistical approaches.  Daga wannan takardan, muna buƙata wani misali na-grafi-zuwa-sequence (Graf Transformer) don mu yi amfani da wannan aikin. @ info: whatsthis An yi amfani da aikin interaction sau biyu don a lissafa haɗi na semantic tsakanin zato. Furan wannan, ana amfani da masu kiyayi ga ƙarfafa information daga matabbatar da ke shiga da fitarwa, wanda ke taimakon misalin ya kãma information masu sakan. Tuduniyarmu yana samar da halin-sanar neural na 1.5 BLEU points on LDC2015E86 da 4.8 BLEU points on LDC2017T10 kuma yana sãmun new state-of-the-art.", 'he': 'מייצג משמעות אבסטרטית (AMR) לדור טקסט הוא המשימה המתאגרת לייצור טקסטים שפות טבעיים מגרפים AMR השיטות המיוחדות הנוכחיות משתמשות בדוגמנים גרף לרצף; עם זאת, הם עדיין לא יכולים להעביר באופן משמעותי את דוגמני הרצף לרצף הקודמים או גישות סטטיסטיות. בעיתון הזה, אנו מציעים מודל גרף לרצף חדש (מעבר גרף) כדי להתמודד עם המשימה הזאת. הדוגמנית קודמת ישירות את הגרפים של AMR ולמדת את מייצגי העמוד. A pairwise interaction function is used for computing the semantic relations between the concepts.  חוץ מזה, מנגנוני תשומת לב משתמשים כדי לאסוף את המידע מהשכנים הנכנסים והיוצאים, אשר עוזרים למודל לתפוס את המידע הסמנטי בצורה יעילה. הדוגמא שלנו עולה על הגישה העצבית ביותר על ידי 1.5 נקודות BLEU על LDC2015E86 ו-4.8 נקודות BLEU על LDC2017T10 ושיגש הופעות חדשות ביותר.', 'bo': 'ལམ་སྟོན་པའི་གསལ་བརྗོད་པ(AMR)-to-text generation་ནི་ཕན་ཚུན་མེད་པའི་བྱ་རིམ་འདི་ནི་ཕན་ཚུན་གྱིས་མཐུན་རིམ་ཡིན་པའི་སྐད་རིགས་དབྱིབས་བཟོ་བ་ཡིན། The current state-of-the-art methods use graph-to-sequence models; use graph-to-sequence models; ཡིན་ནའང་། ཁོང་ཚོས་སྔོན་གྱི་གོ་རིམ་ལྟར་སྔོན་གྱི་དཔེ་དབྱིབས་དང་ཚད་རྩིས་མཐུན་སྟངས་མངོན་གསལ་བྱེད་མི་ཐུབ འོག Model directly encodes the AMR graphs and learns the node representations. མཐུན་སྒྲིག་བྱ་ཚུལ་གཉིས་པ་ཞིག་ནི་རྣམ་གྲངས་རྩིས་ཀྱི་རྩིས་འབྲེལ་འདི་བེད་སྤྱོད་ཐུབ་པ Moreover, attention mechanisms are used for aggregating the information from the incoming and outgoing neighbors, which help the model to capture the semantic information effectively. ང་ཚོའི་མ་དབྱིབས་འདིས་སྔར་སྒྲིག་ནས་དབྱིབས་ཅན་གྱི་ཕྱོགས་སྣང་ཕྱོགས་ཀྱི་ཐབས་ལམ་ལ་བཤད་པ་ལས་ ཉེན་རིས་༡༥ BLEU གནད་སྡུད་དང་ ༤.༨ BLEU གནད་སྡུད་ནང་དུ་'}
{'en': 'Membership Inference Attacks on Sequence-to-Sequence Models : Is My Data In Your Machine Translation System?', 'ar': 'هجمات استدلال العضوية على نماذج التسلسل إلى التسلسل: هل بياناتي موجودة في نظام الترجمة الآلية لديك؟', 'fr': "Attaques par inférence d'adhésion contre les modèles séquence à séquence\xa0: Mes données se trouvent-elles dans votre système de traduction automatique\xa0?", 'es': 'Ataques de inferencia de membresía en modelos de secuencia a secuencia: ¿están mis datos en su sistema de traducción automática?', 'pt': 'Ataques de inferência de associação em modelos de sequência a sequência: meus dados estão em seu sistema de tradução automática?', 'ja': 'シーケンスツーシーケンスモデルへのメンバーシップ推論攻撃：マイデータはあなたの機械翻訳システムにありますか？', 'zh': '推而击之:吾数在机器翻译统中否?', 'hi': 'अनुक्रम-से-अनुक्रम मॉडल पर सदस्यता अनुमान हमले: क्या मेरा डेटा आपकी मशीन अनुवाद प्रणाली में है?', 'ru': 'Вывод о членстве Атаки на модели последовательности-последовательности: мои данные в вашей системе машинного перевода?', 'ga': 'Ionsaithe Tátail Ballraíochta ar Mhúnlaí Seicheamh-go-Seicheamh: An Bhfuil Mo Shonraí I do Chóras Aistriúcháin Meaisín?', 'ka': 'ჩემი მონაცემები თქვენი მაქსინური გადატყვების სისტემაში არა?', 'el': 'Επίθεση συμπερασμάτων μέλους σε μοντέλα διαδοχής σε ακολουθία: Είναι τα δεδομένα μου στο σύστημα μηχανικής μετάφρασής σας;', 'kk': 'Менің деректерім Сіздің компьютеріңіздің аудармалы жүйеңізде емес пе?', 'hu': 'Tagsági fertőzési támadások sorozat-sorozat modelleken: Az adataim a gépi fordító rendszerében vannak?', 'lt': 'Narystės Inferencijos atakai į sekos modelius: Ar mano duomenys yra jūsų mašinos vertimo sistemoje?', 'it': "Attacchi di inferenza all'iscrizione su modelli sequenziali: I miei dati sono nel tuo sistema di traduzione automatica?", 'mk': 'Инференциски напади на членство на моделите од секвенција до секвенција: Дали моите податоци се во вашиот систем за преведување на машина?', 'ms': 'Serangan Inferensi Anggota pada Model Sejukan-ke-Sejukan: Adakah Data Saya dalam Sistem Terjemahan Mesin anda?', 'mn': 'Хувьсгалтын Хувьсгалтын Хувьсгалтын Хувьсгалтын Хувьсгалтын Хувьсгалтын Хувьсгалтын Хувьсгалтын Хувьсгалтын Хувьсгалтын Хувьсгалтын Хувьсгалтын Хувьсгалтын Хувьсгалт', 'no': 'Medlemssystemetillegg for medlemssystemet på sekvens- til- sekvensmodeller: Er data mitt i maskineoversettelsystemet ditt?', 'mt': 'Attakki ta’ Inferenza tal-Is ħubija fuq Mudelli minn Sekwenza għal Sekwenza: Id-Dejta Tiegħi fis-Sistema ta’ Traduzzjoni tal-Magna Tiegħek?', 'sr': 'Napadi članstva u infekciji na modeli sekvence do sekvence: Da li su moji podaci u vašem sistemu za prevod mašine?', 'ro': 'Atacuri de inferență ale membrilor asupra modelelor secvență-la-secvență: Datele mele sunt în sistemul tău de traducere automată?', 'ml': 'മെമ്മറിഷന്\u200d മെയിന്\u200dസിറ്റ് മോഡലുകളില്\u200d മുഴുവന്\u200d ആക്രമണങ്ങള്\u200d: എന്\u200dറെ ഡേറ്റാ നിങ്ങളുടെ മെഷീന്\u200d പരിഭാഷ സിസ്റ്റം ആണോ?', 'si': 'මධ්\u200dයස්ථානය සම්බන්ධ විදියට පරීක්ෂණ විදියට පරීක්ෂණය: මගේ දත්ත ඔයාගේ මධ්\u200dයස්ථාන පද්ධතියේ ඉන්නව', 'so': 'Xubnaha Inference Attacks on Sequence-to-Sequence Models: Miyey macluumaadkayga ku jiraan Interference System', 'pl': 'Czy moje dane są w Twoim systemie tłumaczeń maszynowych?', 'sv': 'Membership Inference Attacks p疇 sekvens-till-sekvensmodeller: Finns mina data i ditt maskin繹vers瓣ttningssystem?', 'ur': 'سکوئنس-تا-سکوئنس موڈل پر مجموعی انفرنس اٹاکس: کیا میرا ڈاٹا تمہاری ماشین ترجمہ سیسٹم میں ہے؟', 'ta': 'தொடர்ந்து வரும் மாதிரிகளில் நினைவகம் புகுநிரல் தேவைகள்: என் தகவல் உங்கள் இயந்திரத்தின் மொழிபெயர்ப்பு முறைமையில் உள', 'uz': '@ info: whatsthis', 'vi': 'Các đơn vị liên liên kết phụ vào chế độ lặp tự động: có phải dữ liệu của tôi trong máy của bạn?', 'hr': 'Napadi članstva u infekciji na modeli sekvence do sekvence: Je li moji podaci u sustavu za prevod mašine?', 'nl': 'Lidmaatschap Inference Aanvallen op Sequence-to-Sequence Modellen: Zijn mijn gegevens in uw Machine Translation System?', 'bg': 'Атаки за заключение на членството върху модели последователност към последователност: Моите данни в системата за машинен превод ли са?', 'da': 'Medlemskabsinferens angreb på sekvens-til-sekvens modeller: Er mine data i dit maskinoversættelsessystem?', 'de': 'Membership Inference Angriffe auf Sequenz-zu-Sequenz Modelle: Sind meine Daten in Ihrem maschinellen Übersetzungssystem?', 'id': 'Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?', 'ko': '시퀀스에서 시퀀스 모형까지의 구성원 추리 공격: 내 데이터가 당신의 기계 번역 시스템에 있습니까?', 'sw': 'Shambulio la Kuzungumzia Kumbukumbu katika Mitandao ya Kufuatiliwa kwa mara mbili: Je Taarifa zangu ni katika mfumo wa Tafsiri wa Mashiniki?', 'fa': 'Attacks of Membership Inference on Sequence-to-sequence Models: Is My Data In Your Machine Translation System?', 'tr': 'Görevliler Jawanlaryň Diňe-täsirli Modellerinde Azalar: Meniň Maglumatym Siziň Maşynyň Terjime Sistemiňde mi?', 'sq': 'Inferenca e anëtarësimit sulmon modelet sekuence-to-sequence: A janë të dhënat e mia në sistemin tuaj të përkthimit të makinës?', 'af': 'Memberskap Inferensie Aangaande op Sequence- to- Sequence Models: Is My Data in jou Masjien Vertaling Stelsel?', 'am': 'Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?', 'hy': 'Իմ տվյալները ձեր մեքենայի թարգմանման համակարգում են:', 'bn': 'Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?', 'az': 'Membership Inference Attacks on Sequence-to-sequence Models: Mənim Məlumatım sizin Makine Çeviri Sisteminizdə deyilmi?', 'ca': "Els atacs d'influència de la membresa en models de seqüència a seqüència: Les meves dades són al sistema de traducció de la vostra màquina?", 'cs': 'Útoky na modely sekvence na sekvenci: Jsou moje data ve vašem systému strojového překladu?', 'et': 'Liikmesuse järelduse rünnakud järjestusest järjestuseni mudelitele: kas minu andmed on teie masintõlke süsteemis?', 'fi': 'J채senyysp채채telm채 Hy철kk채ykset sekvenssimalleihin: Ovatko tietoni konek채채nn철sj채rjestelm채ss채si?', 'bs': 'Napadi članstva u infekciji na modeli sekvence do sekvence: Da li su moji podaci u vašem sustavu za prevod mašine?', 'jv': 'memberspace Info Attacks Nang Model Seyte-to-Seyte: Ubah data Nang Majin Terjamahan Sistemu ?', 'he': 'התקפות התקפות של חברות על דוגמני רצף לרצף: האם המידע שלי במערכת ההתרשמות של המכונה שלך?', 'ha': 'Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?', 'sk': 'Napadi na modele zaporedja do zaporedja: Ali so moji podatki v vašem sistemu strojnega prevajanja?', 'bo': 'Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?'}
{'en': 'Data privacy is an important issue for ', 'ar': 'خصوصية البيانات هي قضية مهمة لمقدمي "التعلم الآلي كخدمة". نحن نركز على مشكلة هجمات استدلال العضوية: بالنظر إلى عينة بيانات والوصول إلى الصندوق الأسود إلى واجهة برمجة التطبيقات الخاصة بالنموذج ، حدد ما إذا كانت العينة موجودة في بيانات التدريب الخاصة بالنموذج. مساهمتنا هي التحقيق في هذه المشكلة في سياق نماذج التسلسل إلى التسلسل ، والتي تعتبر مهمة في تطبيقات مثل الترجمة الآلية وتعليق الفيديو. نحدد مشكلة استدلال العضوية لتوليد التسلسل ، ونوفر مجموعة بيانات مفتوحة تستند إلى أحدث نماذج الترجمة الآلية ، ونبلغ عن النتائج الأولية حول ما إذا كانت هذه النماذج تسرّب معلومات خاصة مقابل عدة أنواع من هجمات استدلال العضوية.', 'es': 'La privacidad de los datos es un tema importante para los proveedores de «aprendizaje automático como servicio». Nos centramos en el problema de los ataques de inferencia de membresía: dada una muestra de datos y acceso de caja negra a la API de un modelo, determine si la muestra existía en los datos de entrenamiento del modelo. Nuestra contribución es una investigación de este problema en el contexto de los modelos de secuencia a secuencia, que son importantes en aplicaciones como la traducción automática y la subtitulación de vídeo. Definimos el problema de inferencia de membresía para la generación de secuencias, proporcionamos un conjunto de datos abierto basado en modelos de traducción automática de última generación e informamos de los resultados iniciales sobre si estos modelos filtran información privada contra varios tipos de ataques de inferencia de membresía.', 'fr': "La confidentialité des données est une question importante pour les fournisseurs de «\xa0machine learning as a service\xa0». Nous nous concentrons sur le problème des attaques par inférence d'appartenance\xa0: à partir d'un échantillon de données et d'un accès en boîte noire à l'API d'un modèle, déterminez si l'échantillon existait dans les données d'entraînement du modèle. Notre contribution consiste à étudier ce problème dans le contexte des modèles séquence à séquence, qui sont importants dans des applications telles que la traduction automatique et le sous-titrage vidéo. Nous définissons le problème d'inférence d'appartenance pour la génération de séquences, fournissons un ensemble de données ouvert basé sur des modèles de traduction automatique de pointe et communiquons les premiers résultats indiquant si ces modèles divulguent des informations privées contre plusieurs types d'attaques par inférence d'appartenance.", 'pt': 'A privacidade dos dados é uma questão importante para os provedores de “aprendizagem de máquina como serviço”. Nós nos concentramos no problema dos ataques de inferência de associação: Dada uma amostra de dados e acesso de caixa preta à API de um modelo, determine se a amostra existia nos dados de treinamento do modelo. Nossa contribuição é uma investigação desse problema no contexto de modelos sequência a sequência, que são importantes em aplicações como tradução automática e legendagem de vídeo. Definimos o problema de inferência de associação para geração de sequência, fornecemos um conjunto de dados aberto baseado em modelos de tradução automática de última geração e relatamos resultados iniciais sobre se esses modelos vazam informações privadas contra vários tipos de ataques de inferência de associação.', 'ja': 'データプライバシーは、「サービスとしての機械学習」プロバイダーにとって重要な問題です。メンバーシップ推論攻撃の問題に焦点を当てています。データサンプルとモデルのAPIへのブラックボックスアクセスを使用して、サンプルがモデルのトレーニングデータに存在するかどうかを判断します。私たちの貢献は、機械翻訳やビデオキャプションなどのアプリケーションで重要なシーケンスツーシーケンスモデルの文脈におけるこの問題の調査です。シーケンス生成のためのメンバーシップ推論問題を定義し、最先端の機械翻訳モデルに基づいてオープンデータセットを提供し、これらのモデルがいくつかの種類のメンバーシップ推論攻撃に対してプライベート情報をリークするかどうかについて初期結果を報告します。', 'zh': '数隐者,机器学之提供商也。 注于成人推理攻击:给定数样本访于对模型API之黑盒,定样本存乎模形之数。 吾献在序背景,此机器翻译与视频字幕为重。 我们定义了序列生成的资格推理,给了一个基于最先进的机器翻译模形的开数据集,并告诉了这些模样多员资格推理攻击漏泄私信息的初步结果。', 'hi': 'डेटा गोपनीयता "एक सेवा के रूप में मशीन लर्निंग" प्रदाताओं के लिए एक महत्वपूर्ण मुद्दा है। हम सदस्यता अनुमान हमलों की समस्या पर ध्यान केंद्रित करते हैं: एक मॉडल के एपीआई के लिए डेटा नमूना और ब्लैक-बॉक्स एक्सेस को देखते हुए, यह निर्धारित करें कि नमूना मॉडल के प्रशिक्षण डेटा में मौजूद था या नहीं। हमारा योगदान अनुक्रम-से-अनुक्रम मॉडल के संदर्भ में इस समस्या की जांच है, जो मशीन अनुवाद और वीडियो कैप्शनिंग जैसे अनुप्रयोगों में महत्वपूर्ण हैं। हम अनुक्रम पीढ़ी के लिए सदस्यता अनुमान समस्या को परिभाषित करते हैं, अत्याधुनिक मशीन अनुवाद मॉडल के आधार पर एक खुला डेटासेट प्रदान करते हैं, और प्रारंभिक परिणामों की रिपोर्ट करते हैं कि क्या ये मॉडल कई प्रकार के सदस्यता अनुमान हमलों के खिलाफ निजी जानकारी लीक करते हैं।', 'ru': 'Конфиденциальность данных является важным вопросом для поставщиков услуг «машинного обучения как услуги». Мы фокусируемся на проблеме атак на выводы о членстве: учитывая выборку данных и доступ к API модели, определите, существует ли выборка в обучающих данных модели. Нашим вкладом является исследование этой проблемы в контексте моделей последовательности к последовательности, которые важны в таких приложениях, как машинный перевод и видео субтитры. Мы определяем проблему вывода о членстве для генерации последовательностей, предоставляем открытый набор данных, основанный на современных моделях машинного перевода, и сообщаем первоначальные результаты о том, протекают ли эти модели частной информации против нескольких видов атак вывода о членстве.', 'ga': 'Is ceist thábhachtach í príobháideacht sonraí do sholáthróirí “foghlaim meaisín mar sheirbhís”. Dírímid ar fhadhb na n-ionsaithe tátal ballraíochta: Nuair a chuirtear sampla sonraí agus rochtain bhosca dubh ar API múnla san áireamh, cinntigh an raibh an sampla i sonraí oiliúna an mhúnla. Is é ár rannchuidiú le himscrúdú ar an bhfadhb seo i gcomhthéacs samhlacha seicheamh-go-seicheamh, atá tábhachtach in feidhmeanna ar nós aistriúchán meaisín agus fotheidealú físeáin. Sainmhínímid an fhadhb tátal ballraíochta maidir le giniúint seicheamh, soláthraímid tacar sonraí oscailte bunaithe ar mhúnlaí aistriúcháin meaisín úrscothacha, agus tuairiscímid na torthaí tosaigh maidir le cibé an scaoileann na samhlacha sin faisnéis phríobháideach i gcoinne roinnt cineálacha ionsaithe tátail bhallraíochta.', 'hu': 'Az adatvédelem fontos kérdés a "gépi tanulás mint szolgáltatás" szolgáltatók számára. A tagsági következtetési támadások problémájára összpontosítunk: Adatmintával és egy modell API-jához való fekete dobozos hozzáféréssel határozzuk meg, hogy a minta létezett-e a modell képzési adataiban. Közreműködésünk ennek a problémának a vizsgálata sorozat-sorozat modellek összefüggésében, amelyek fontosak az olyan alkalmazásokban, mint a gépi fordítás és a videofeliratozás. Meghatározzuk a tagsági következtetési problémát a szekvencia generálásához, nyílt adatkészletet biztosítunk a legkorszerűbb gépi fordítási modellek alapján, és jelentjük a kezdeti eredményeket arról, hogy ezek a modellek szivárogtatnak-e magáninformációkat többféle tagsági következtetési támadás ellen.', 'el': 'Το απόρρητο δεδομένων αποτελεί σημαντικό ζήτημα για τους παρόχους "μηχανικής μάθησης ως υπηρεσία". Εστιάζουμε στο πρόβλημα των επιθέσεων συμπερασμάτων μέλους: Με δεδομένο ένα δείγμα δεδομένων και πρόσβαση μαύρου κουτιού στην API ενός μοντέλου, καθορίστε εάν το δείγμα υπήρχε στα δεδομένα εκπαίδευσης του μοντέλου. Η συμβολή μας είναι η διερεύνηση αυτού του προβλήματος στο πλαίσιο μοντέλων ακολουθίας σε ακολουθία, τα οποία είναι σημαντικά σε εφαρμογές όπως η μηχανική μετάφραση και η τιτλοποίηση βίντεο. Καθορίζουμε το πρόβλημα συμπερασμάτων μέλους για τη δημιουργία ακολουθιών, παρέχουμε ένα ανοικτό σύνολο δεδομένων βασισμένο σε σύγχρονα μοντέλα μηχανικής μετάφρασης και αναφέρουμε αρχικά αποτελέσματα σχετικά με το αν αυτά τα μοντέλα διαρρέουν ιδιωτικές πληροφορίες ενάντια σε διάφορα είδη επιθέσεων συμπερασμάτων μελών.', 'ka': "მონაცემების პრივისატები არის მნიშვნელოვანი პრობლემა 'მაქინის სწავლება როგორც სერვისი' მომხმარებისთვის. ჩვენ მოდილის API-ს მონაცემებისთვის მონაცემების გამოყენების პრობლემაზე დავუყენებთ: მოდილის მონაცემების მონაცემების მონაცემებში მონაცემების მონაცემებში არსებობს თუ არა. ჩვენი დამატება არის ამ პრობლემას შემოწმება, რომელიც მნიშვნელოვანია პრობლემაში, რომელიც მაქსინური გადაწყვება და ვიდეო შესახებ. ჩვენ განსაზღვრებით წიგნის ინფრენციის პრობლემა სკენექციის განვითარებისთვის, გახსნილი მონაცემების შესახებ მაქსინის განვითარებისთვის მოდელზე, და დავიწყებით პირველი შედეგების შესახებ თუ არა ეს მოდელები", 'it': 'La privacy dei dati è una questione importante per i fornitori di "machine learning as a service". Ci concentriamo sul problema degli attacchi di inferenza dell\'adesione: dato un campione di dati e l\'accesso in scatola nera all\'API di un modello, determinare se il campione esisteva nei dati di formazione del modello. Il nostro contributo è un\'indagine di questo problema nel contesto dei modelli sequenza-sequenza, che sono importanti in applicazioni come la traduzione automatica e la didascalia video. Definiamo il problema dell\'inferenza di appartenenza per la generazione di sequenze, forniamo un set di dati aperto basato su modelli di traduzione automatica all\'avanguardia e riportiamo i risultati iniziali sulla perdita di informazioni private da parte di questi modelli contro diversi tipi di attacchi di inferenza di appartenenza.', 'kk': 'Деректердің жалпы мәселесі - қызмет ретінде оқыту үшін маңызды мәселе. Біз бөлімдік инференциялардың мәселелеріне назар аударамыз: Деректер үлгісі мен қара жағындағы API үлгісіне қатынау үшін, үлгісінің оқыту деректерінде мәселе бар ма екенін анықтаймыз. Біздің көмегіміз осы мәселеді реттеу үлгілерінің контексті зерттеу. Бұл машинаны аудару мен видео айдарылығы секілді қолданбаларда маңызды. Біз реттеулерді құру үшін бөлшектердің инференциясының мәселесін анықтаймыз, механикалық аудару үлгілеріне негізделген ашық деректер жиынын таңдаймыз, және бастапқы нәтижелері бұл үлгілер бірнеше түрлі бөлшектердің', 'lt': 'Duomenų privatumas yra svarbus klausimas „mašininio mokymosi kaip paslaugų teikėjų“ atžvilgiu. Mes daugiausia dėmesio skiriame narystės išvadų išpuolių problemai: atsižvelgiant į duomenų mėginį ir galimybę juodoje langelyje susipažinti su modelio API, nustatyti, ar mėginys egzistuoja modelio mokymo duomenise. Mūsų indėlis yra šios problemos tyrimas, susijęs su sekos po sekos modeliais, kurie yra svarbūs tokiose programose kaip mašin in is vertimas ir vaizdo įrašas. Mes apibrėžiame narystės išvadų problem ą sekos kūrimui, pateikiame atvirą duomenų rinkinį, pagrįstą naujausiais mašinų vertimo modeliais, ir pranešame apie pradinius rezultatus, ar šie modeliai skleidžia privačią informaciją prieš kelių rūšių narystės išvadų išpuolius.', 'ms': "Kepribadian data adalah isu penting untuk 'mesin belajar sebagai penyedia perkhidmatan'. Kami fokus pada masalah penyerangan kesimpulan ahli: Mengingat sampel data dan akses kotak hitam ke API model, tentukan sama ada sampel wujud dalam data latihan model. Kontribusi kami adalah penyelidikan masalah ini dalam konteks model urutan-ke-urutan, yang penting dalam aplikasi seperti terjemahan mesin dan captioning video. Kami menentukan masalah kesimpulan anggota untuk generasi jujukan, menyediakan set data terbuka berdasarkan model terjemahan mesin state-of-the-art, dan laporkan keputusan awal sama ada model ini bocorkan maklumat peribadi melawan beberapa jenis serangan kesimpulan anggota.", 'ml': "'മെഷിന്\u200d പഠിക്കുന്നത് സേവനം ഉപയോഗിക്കുന്നവരായി പഠിക്കുന്നതിനായി ഡേറ്റാ സ്വകാര്യം പ്രധാന നമ്മള്\u200d മെമ്മറിപ്പിറ്റിയിലെ അപകടത്തിന്റെ പ്രശ്നത്തിലേക്ക് ശ്രദ്ധിക്കുന്നു: ഒരു ഡേറ്റാ മാതൃകയും കറുത്ത പെട്ടിയുടെ പ്രവേശനവും കൊണ്ട്,  നമ്മുടെ ഭാഗം ഈ പ്രശ്നത്തിന്റെ അന്വേഷണമാണ്. സെക്കന്\u200dസ് മോഡലുകളുടെ കൂട്ടത്തില്\u200d, അത് മെഷീന്\u200d പരിഭാഷണവും വീഡിയോ പ്രയോഗത്തിന്റെ പ്രയ സെക്കന്\u200dസ് തലമുറകള്\u200dക്ക് വേണ്ടി മെമ്മറിഷന്\u200d അപരിഹാരം പ്രശ്നത്തെ ഞങ്ങള്\u200d വിശദീകരിക്കുന്നു. ഒരു തുറന്ന ഡാറ്റാസെറ്റ് സ്റ്റേറ്റ് ഓഫ്-ആർട്ട് മെഷീന്\u200d പരിഭ", 'mk': 'Приватноста на податоците е важно прашање за обезбедувачите на „машинско учење како сервис“. Се фокусираме на проблемот со нападите на конференцијата за членство: Со оглед на примерок на податоци и пристап во црна кутија до API на моделот, одредете дали примерокот постоеше во податоците за обука на моделот. Нашиот придонес е истрага за овој проблем во контекст на модели од секвенца до секвенца, кои се важни во апликациите како што се машински превод и видео наслов. Ние го дефинираме проблемот со конференцијата на членство за генерацијата на секвенца, обезбедуваме отворен податок базиран на најсовремените машински преводни модели и известуваме за првичните резултати за тоа дали овие модели протекуваат приватни информации против неколку видови на конференциски', 'mt': 'Il-privatezza tad-dejta hija kwistjoni importanti għall-fornituri tat-“tagħlim bil-magna bħala servizz”. Aħna niffokaw fuq il-problem a tal-attakki tal-inferenza tas-s ħubija: Minħabba kampjun tad-dejta u a ċċess black-box għall-API ta’ mudell, niddeterminaw jekk il-kampjun kienx jeżisti fid-dejta tat-taħriġ tal-mudell. Il-kontribut tagħna huwa investigazzjoni ta’ din il-problema fil-kuntest ta’ mudelli minn sekwenza għal sekwenza, li huma importanti f’applikazzjonijiet bħat-traduzzjoni tal-magna u l-intestatura tal-vidjo. Aħna niddefinixxu l-problema tal-inferenza tas-sħubija għall-ġenerazzjoni tas-sekwenzi, nipprovdu sett ta’ dejta miftuħ ibbażat fuq mudelli ta’ traduzzjoni tal-magni l-aktar avvanzati, u nirrappurtaw ir-riżultati inizjali dwar jekk dawn il-mudelli jnixxux informazzjoni privata kontra diversi tipi ta’ attakki ta’ inferenza tas-sħ', 'pl': 'Prywatność danych jest ważną kwestią dla dostawców "uczenia maszynowego jako usługi". Skupiamy się na problemie ataków wnioskowania o członkostwo: biorąc pod uwagę próbkę danych i dostęp do interfejsu API modelu czarną skrzynkę, ustalić, czy próbka istniała w danych treningowych modelu. Naszym wkładem jest zbadanie tego problemu w kontekście modeli sekwencji-sekwencji, które są ważne w aplikacjach takich jak tłumaczenie maszynowe i napisy wideo. Definiujemy problem wnioskowania członkostwa dla generowania sekwencji, dostarczamy otwarty zestaw danych oparty na najnowocześniejszych modelach tłumaczenia maszynowego oraz raportujemy wstępne wyniki dotyczące tego, czy modele te wyciekają prywatne informacje przeciwko kilku rodzajom ataków wnioskowania członkostwa.', 'mn': "Өгөгдлийн хувийн байдал бол 'машин суралцах үйлчилгээний хувьд' хангамжигчийн чухал асуудал юм. Бид гишүүн халдварын шигдээний асуудлыг анхаарлаа хандуулж байна: өгөгдлийн жишээ болон хар хайрцаг загварын API-д хүртэл ашиглаж байгаа учраас загварын сургалтын өгөгдлийн хувьд жишээ байгааг тодорхойлох. Бидний зориулалт бол энэ асуудлыг дарааллаар дарааллаар давтагдсан загваруудын тухай судалгаа юм. Энэ нь машины орчуулалт болон видео загваруудын тухай чухал хэрэгжүүлэлт юм. Бид дарааллын үеийн гишүүн халдварын асуудлыг тодорхойлож, урлагийн машины хөрөнгө оруулах загварын үндсэн нээлттэй өгөгдлийн санг өгдөг. Эдгээр загварууд хэдэн төрлийн гишүүн халдварын дайралтын эсрэг хувийн мэдээллийг хувийн мэдээллийг ши", 'sr': 'privatnost podataka je važan problem za pružatelje "mašine učenja kao usluga". Fokusiramo se na problem napada infekcije članstva: s obzirom na uzorak podataka i pristup crnoj kutiji u API model a, utvrdimo je li uzorak postojao u podacima obuke modela. Naš doprinos je istraga ovog problema u kontekstu modela sequence-to-sequence, koji su važni u aplikacijama poput prevoda mašine i snimka. Definiramo problem infekcije članstva za generaciju sekvence, pružamo otvoren podatak na osnovu modela prevoda mašine za državu umjetnosti, i prijavimo prve rezultate o tome da li ovi modeli cure privatne informacije protiv nekoliko vrsta napada na infekciju članstva.', 'no': 'Data privat er eit viktig problem for « maskinelæring som teneste » - tilbydar. Vi fokuserer på problemet med mellomslagsinfeksjonsmakkar: Given eit dataprøve og svart boks tilgang til API til ein modell, bestemmer om prøva eksisterer i opplæringsdata i modellen. Bidraget vårt er ein undersøking av dette problemet i konteksten av sekvens-til-sekvensmodeller, som er viktig i program som maskinsomsetjing og video-tittel. Vi definerer problemet med medlemsstateinfeksjonen for sekvensgenerasjon, gir ei opna dataset basert på modeller for omsetjing av maskinen i tilstanden til kunsten, og rapporterer første resultat om desse modelane løyser privat informasjon mot fleire typar medlemsstateinfeksjonsmakkar.', 'so': 'Inta gaarka loo leeyahay macluumaadku waa arrin muhiim ah oo ku saabsan waxbarashada maskaxda sida adeegga. Waxaynu ku kalsoonaynaa dhibaatooyinka weerarka cudurka xubnaha: Sida uu sameynta macluumaadka iyo qaabka madowga u helo tusaale ahaan API, waxaynu ogaanaynaa in sameyntu ay ku jirto macluumaadka waxbarashada modellka. Sharcigayagu waa baaritaanka dhibaatadan marka lagu sameeyo tusaalaha isbedelka dabar-dabarka, taasoo muhiim u ah codsiga sida turjumidda machine iyo bandhigyada fiidiyowga. Waxaannu qoraynaa dhibaatada xubnaha ka mid ah dhibaatada qarniga dabaasha, waxaynu siinaynaa sawir furan oo ku saleysan tusaalaha turjumidda muusikada farshaxanka, waxaana wargelinaynaa resultiyada bilowga ah in modelladan ay ku lealaayaan macluumaad gaar ah oo ka gees ah weerarka cudurada xubnaha ah.', 'sv': 'Dataintegritet är en viktig fråga för leverantörer av maskininlärning som tjänster. Vi fokuserar på problemet med angrepp för medlemskapsinferens: Med ett dataprov och en svart låda tillgång till en modells API, bestäm om provet fanns i modellens träningsdata. Vårt bidrag är en undersökning av detta problem inom ramen för sekvensmodeller, som är viktiga i tillämpningar som maskinöversättning och bildtext. Vi definierar problemet med medlemsinferens för sekvensgenerering, tillhandahåller ett öppet dataset baserat på toppmoderna maskinöversättningsmodeller och rapporterar inledande resultat om dessa modeller läcker privat information mot flera typer av medlemsinferens attacker.', 'ta': "'இயந்திரம் கற்றல் சேவை' வழங்குபவராக தகவல் தனிப்பட்ட பிரச்சினையாகும். நாம் உறுப்பினர் பிரச்சனைகளை கவனம் செலுத்துகிறோம்: ஒரு தரவு மாதிரி மற்றும் கருப்பெட்டி மாதிரியின் API அணுகல் கொண்டு, மாதிரியின் பயிற்ச எங்கள் பங்கு என்பது தொடர்ந்து வரும் மாதிரிகளின் மூலம் இந்த பிரச்சனையின் ஒரு விசாரணையாகும், இது இயந்திர மொழிபெயர்ப்பு மற்றும் வீ We define the membership inference problem for sequence generation, provide an open dataset based on state-of-the-art machine translation models, and report initial results on whether these models leak private information against several kinds of membership inference attacks.", 'ro': 'Confidențialitatea datelor este o problemă importantă pentru furnizorii de servicii de învățare automată. Ne concentrăm asupra problemei atacurilor de inferență a membrilor: Având în vedere un eșantion de date și accesul în cutie neagră la API-ul unui model, determinați dacă eșantionul a existat în datele de instruire ale modelului. Contribuția noastră este o investigare a acestei probleme în contextul modelelor secvență-secvență, care sunt importante în aplicații precum traducerea automată și subtitrarea video. Definim problema inferenței membrilor pentru generarea secvențelor, furnizăm un set de date deschis bazat pe modele de traducere automată de ultimă oră și raportăm rezultatele inițiale cu privire la dacă aceste modele scurge informații private împotriva mai multor tipuri de atacuri inferenței membrilor.', 'si': "දත්ත පෞද්ගලිකතාවය 'පණිවිඩයේ ඉගෙන ගන්න' සේවාදායක් විදියට වැදගත් ප්\u200dරශ්නයක්. අපි මණ්ඩලයේ ප්\u200dරශ්නයක් වෙනුවෙන් ප්\u200dරශ්නයක් බලන්න: මණ්ඩලයේ ප්\u200dරශ්නයක් සහ කළු පෙට්ටියක් ප්\u200dරශ්නයක් තියෙන්නේ, මණ්ඩ අපේ ප්\u200dරයෝජනය තමයි මේ ප්\u200dරශ්නයේ පරීක්ෂණයක් පරීක්ෂණයක්, පරීක්ෂණයෙන් පරීක්ෂණයෙන් ප්\u200dරශ්නයක්, මෙචින් පරික අපි සාමාන්\u200dය විශාල ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් විශ්වාස කරනවා, සාමාන්\u200dය විශාල ප්\u200dරශ්නයක් සඳහා විවෘත දත්ත සූදානයක් ප්\u200dරශ්නයක් කරන්න, සහ පටන", 'ur': "ڈاٹا خصوصی ایک اہم مسئلہ ہے 'مشین سیکھنے کی طرح' سفارش دینے والوں کے لئے۔ ہم نے مدل کے API کے لئے ایک نمونہ نمونہ اور سیاه باکس کے دسترسی کے ذریعے مطابق مشکل پر تمرکز کیا ہے کہ نمونہ نمونہ نمونہ کی تطابق دادہ میں موجود ہے؟ ہمارا حصہ یہ مسئلہ کا تحقیق ہے کہ سطح کے ذریعہ سے سطح کے مدلکوں میں ہے، جو مہینی ترجمہ اور ویڈیو کاپٹینگ کے مطابق اہم ہیں۔ ہم تعریف نسل کے لئے مغلوم کے انفارینس مسئلہ کی تعریف کرتے ہیں، ایک کھولی ڈاٹ سٹ کے ذریعہ آرتی ماشین کی ترجمہ موڈل پر بنیاد رکھتے ہیں، اور آغاز نتائج کے بارے میں راپورٹ کرتے ہیں کہ یہ موڈل مختلف طریقے مغلوم کے انفارینس حملہ کے خلاف خص", 'vi': 'Dữ liệu riêng tư là một vấn đề quan trọng đối với việc "học máy làm dịch vụ". Chúng tôi tập trung vào vấn đề liên quan đến các vụ tấn công vào hội viên: với mẫu dữ liệu và quyền truy cập hộp đen vào API của mô hình, xác định xem mẫu có trong dữ liệu huấn luyện của mô-đun không. Sự đóng góp của chúng tôi là một cuộc điều tra vấn đề này trong các mô hình lặp lại, rất quan trọng trong các ứng dụng như dịch bản máy và đoạn video... Chúng tôi xác định vấn đề ám chỉ hội viên cho sản xuất chuỗi, cung cấp một bộ dữ liệu mở dựa trên các mô hình dịch cỗ máy hiện đại, và báo cáo kết quả đầu tiên về việc những mô hình này có rò rỉ thông tin cá nhân vào nhiều loại tấn công liên quan.', 'uz': "Name Bu modelning API dasturiga qaramadigan muammolarni foydalanamiz: Maʼlumot misol va qora qutisi asbob modelidagi asboblar modelning taʼminlovchi maʼlumot mavjudligini aniqlash mumkin. Bu muammolarning qidirishlarimiz, bir necha xil modellarda qidirish mumkin, bu mashina tarjima va video ta'sirishi kabi dasturlarda muhim. Biz cheksiz foydalanuvchi davomida o'smirlash muammolarini aniqlamadik, o'sha shaxsiy mashina tarjima modellari asosida ochiq maʼlumotlar tizimini aniqlab, va bu modellar bir necha xil bir xil xil bog'lanuvchilar haqida shaxsiy maʼlumot yozib olishlari haqida xabar beramiz.", 'bg': 'Поверителността на данните е важен въпрос за доставчиците на "машинно обучение като услуга". Фокусираме се върху проблема с атаките за заключение на членството: Като се има предвид извадка от данни и достъп в черна кутия до API на модела, определете дали извадката е съществувала в данните за обучение на модела. Нашият принос е изследване на този проблем в контекста на моделите последователност към последователност, които са важни в приложения като машинен превод и видео надпис. Определяме проблема с изводите за членство за генериране на последователност, предоставяме отворен набор от данни въз основа на най-съвременните модели на машинен превод и докладваме първоначалните резултати за това дали тези модели изтичат лична информация срещу няколко вида атаки за изводи на членство.', 'hr': 'privatnost podataka je važan problem za pružatelje pružatelja "učenje strojeva kao usluga". Usredotočili smo se na problem napada infekcije članstva: s obzirom na uzorak podataka i pristup crnoj kutiji u API model a, utvrditi postoji li uzorak u podacima obuke modela. Naš doprinos je istraga ovog problema u kontekstu modela sequence-to-sequence, koji su važni u aplikacijama poput prevoda strojeva i snimka. Definiramo problem infekcije članstva za generaciju sekvencije, pružamo otvorenu podatku na temelju modela prevoda uređaja u stanju umjetnosti i prijavimo početne rezultate o tome da li ovi modeli cure privatne informacije protiv nekoliko vrsta napada infekcije članstva.', 'nl': "Gegevensbescherming is een belangrijk thema voor aanbieders van 'machine learning as a service'. We concentreren ons op het probleem van lidmaatschapsinferentieaanvallen: met behulp van een datasample en black-box toegang tot de API van een model, bepaalt u of de sample bestond in de trainingsgegevens van het model. Onze bijdrage is een onderzoek naar dit probleem in de context van sequence-to-sequence modellen, die belangrijk zijn in toepassingen zoals machinevertaling en video ondertiteling. We definiëren het lidmaatschapsinferentieprobleem voor het genereren van sequenties, bieden een open dataset op basis van state-of-the-art machinevertaalmodellen en rapporteren eerste resultaten over of deze modellen privé informatie lekken tegen verschillende soorten lidmaatschapsinferentieaanvallen.", 'da': 'Databeskyttelse af personlige oplysninger er et vigtigt spørgsmål for udbydere af maskinlæring som tjeneste. Vi fokuserer på problemet med medlemskabsdeference angreb: Med en dataprøve og sort boks adgang til en models API, skal du bestemme, om prøven eksisterede i modellens træningsdata. Vores bidrag er en undersøgelse af dette problem i forbindelse med sekvens-til-sekvensmodeller, som er vigtige i applikationer som maskinoversættelse og videobilledtekster. Vi definerer problemet med medlemskabsdeference for sekvensgenerering, leverer et åbent datasæt baseret på state-of-the-art maskinoversættelsesmodeller og rapporterer første resultater af, om disse modeller lækker private oplysninger mod flere former for medlemskabsdeference angreb.', 'de': 'Datenschutz ist ein wichtiges Thema für Anbieter von "Machine Learning as a Service". Wir konzentrieren uns auf das Problem von Membership Inference Attacken: Bestimmen Sie anhand einer Datenprobe und eines Black-Box-Zugriffs auf die API eines Modells, ob die Probe in den Trainingsdaten des Modells vorhanden war. Unser Beitrag ist eine Untersuchung dieses Problems im Kontext von Sequenz-zu-Sequenz Modellen, die für Anwendungen wie maschinelle Übersetzung und Video-Untertitelung wichtig sind. Wir definieren das Mitgliederinferenzproblem für die Sequenzgeneration, stellen einen offenen Datensatz auf Basis modernster maschineller Übersetzungsmodelle bereit und berichten erste Ergebnisse darüber, ob diese Modelle private Informationen gegen verschiedene Arten von Mitgliederinferenzangriffen auslaufen.', 'id': "Privasi data adalah masalah penting untuk 'belajar mesin sebagai penyedia layanan'. Kami fokus pada masalah penyerangan dari anggota: mengingat sampel data dan akses kotak hitam ke API model, menentukan apakah sampel ada dalam data latihan model. Kontribusi kami adalah penyelidikan masalah ini dalam konteks model urutan-urutan, yang penting dalam aplikasi seperti terjemahan mesin dan captioning video. Kami mendefinisikan masalah kesimpulan anggota untuk generasi urutan, menyediakan set data terbuka berdasarkan model terjemahan mesin terbaik, dan melaporkan hasil awal apakah model ini bocor informasi pribadi melawan beberapa jenis serangan kesimpulan anggota.", 'fa': 'خصوصی داده ها مسئله مهم برای پردازگاران "یادگیری ماشین به عنوان خدمت" است. ما روی مشکل حمله های عضو عضو عضو حضور تمرکز می کنیم: با توجه به نمونه نمونه داده و دسترسی جعبه سیاه به API یک مدل، مشخص می کنیم آیا نمونه در داده های آموزش مدل وجود دارد. شرکت ما تحقیق این مشکل در محیط مدل\u200cهای ردیابی به ردیابی است که در کاربردهای مثل ترجمه\u200cهای ماشین و ویدئویی مهم است. ما مشکل عضویت عضویت را برای نسل\u200cهای مختلف تعریف می\u200cکنیم، مجموعه داده\u200cهای باز را بر اساس مدل\u200cهای ترجمه\u200cی ماشین\u200cهای هنر روشن می\u200cکنیم، و نتیجه\u200cهای اولیه را گزارش می\u200cدهیم که آیا این مدل\u200cها اطلاعات خصوصی را بر خلاف چند نوع حمله\u200c', 'ko': '데이터 프라이버시는 기계 학습 즉 서비스 제공자의 중요한 문제이다.우리는 구성원의 추리 공격 문제에 주목한다. 데이터 샘플과 모델 API에 대한 블랙박스 접근을 정하고 샘플이 모델의 훈련 데이터에 존재하는지 확인한다.우리의 공헌은 서열에서 서열 모델까지의 배경에서 이 문제를 연구하는 것이다. 이것은 기계 번역과 영상 자막 등 응용에서 매우 중요하다.우리는 서열 생성의 구성원 신분 추리 문제를 정의했고 가장 선진적인 기계 번역 모델을 바탕으로 개방된 데이터 집합을 제공했으며 이 모델들이 몇 가지 구성원 신분 추리 공격에 대한 개인 정보를 누설했는지의 초보적인 결과를 보고했다.', 'sw': "faragha ya data ni suala muhimu kwa 'kujifunza mashine kama watoa huduma'. Tunakabilia tatizo la mashambulizi ya maambukizi ya wanachama: Kutokana na sampuli ya data na tasnia za kiusingi za upatikanaji wa chama cha API, kuamua kama sampuli hiyo ilikuwepo katika data za mafunzo ya modeli. Mchango wetu ni uchunguzi wa tatizo hili katika muktadha wa mifano ya mfululizo wa mfululizo, ambayo ni muhimu katika matumizi kama vile tafsiri ya mashine na kuchapisha video. Tunaweza kufafanua tatizo la kutokuwepo kwa wanachama kwa vizazi vya mfululizo, kutoa seti ya taarifa huru kwa kutumia mifano ya utafsiri wa mashine ya sanaa, na kuripoti matokeo ya mwanzo kuhusu ikiwa mifano hii huchapisha taarifa binafsi dhidi ya aina kadhaa ya mashambulizi ya wanachama.", 'am': "የዳታ የግል ግልገት ግልፅ ለ ' ማኪኖችን ማምረጥ' አገልግሎት የሚሰጠው ጉዳይ ነው፡፡ የአንባቢነት ሳንቆስ መከራ ላይ እናስከጅላለን: የዳታ ምሳሌ እና ጥቁር-box የሞዴል API ማግኘት ሲሰጠን ምሳሌው በሞዴል ትምህርት ዳታ ውስጥ ቢኖር እንደሆነ አስተውል፡፡ አካባቢነታችን የዚህ ጉዳይ ምርመራ ነው፡፡ የአደራዊ ስህተት መከራን ለግንኙነት ትውልድ እናሳውቃለን፤ የክፈት ዳታ ማተርሚናል በሀገር-የart ትርጉም ዓይነቶች ላይ የተመሳሳይ እናደርጋለን፡፡", 'tr': "Maglumat gizemliýeti 'enjamyň öwrenmesi häzirki häzirki häzirki meseledir'. Biz üýtgeşik alyklamasynyň kynçylyklarynyň üstüne üns berýäris: maglumatyň örnegini we gara-box nusgalarynyň API'e gollanan ýagdaýyny çykarýarys. Biziň gaýşartymyz bu meseläni sequence-to-sequence modelleriň kontekstinde barlamakdyr. Bu program maşynyň terjime we video käpşenleri ýaly möhüm. Biz bu nusgalar üýtgeşik alçaklarynyň sanlary üçin hasaplanýarys, aýdym alçaklarynyň durumynda-aýdym maşynynyň terjime modellerine daýan ýar we bu nusgalaryň birnäçe nusgalaryna garşy hasaplanýandygyny barada başlangıç netijesini çykarýarys.", 'sq': "Data privacy is an important issue for 'machine learning as a service' providers.  We focus on the problem of membership inference attacks: Given a data sample and black-box access to a model's API, determine whether the sample existed in the model's training data.  Our contribution is an investigation of this problem in the context of sequence-to-sequence models, which are important in applications such as machine translation and video captioning.  We define the membership inference problem for sequence generation, provide an open dataset based on state-of-the-art machine translation models, and report initial results on whether these models leak private information against several kinds of membership inference attacks.", 'az': "Məlumat təhlükəsizlik 'maşın öyrənməsi Servis kimi' təmin edənlər üçün vacib bir məsələdir. Membership inference saldıqlarının problemlərinə odaklanırıq: model'in təhsil məlumatlarında nümunə və siyah qutusuna istifadə edilməsini təmin edirik. Bizim səbəbimiz bu problemi seçmə-seçmə modellərin məlumatında, maşın çeviri və video başlığı kimi proqramlarda mövcuddur. Biz seçmə nəsillərinin üyelik infeksyon problemini belə təyin edirik, sanat makinelərin qurğulama modellərinə dayanan açıq verilən qurğuları təyin edirik, və bu modellərin bəzi cür üyelik infeksyon saldıqlarına qarşı xüsusi məlumatları təhrif edir.", 'bn': "Data privacy is an important issue for 'machine learning as a service' providers.  আমরা সদস্যদের আক্রমণের ব্যাপারে মনোযোগ দিয়ে মনোযোগ দিচ্ছি: একটি ডাটা নম্বর এবং কালো বাক্স মডেলের API-এ প্রবেশের কারণে মডেলের প্রশিক্ষণের তথ্য আমাদের অবদান হচ্ছে এই সমস্যার একটি তদন্ত, যা মেশিন অনুবাদ এবং ভিডিও ক্যাপ্টেশনের মতো গুরুত্বপূর্ণ অ্যাপ্লিকেশনে গুরুত্বপূর্ণ। আমরা সেকেন্ড প্রজন্মের জন্য সদস্যদের অনফিসের সমস্যা নির্ধারণ করি, একটি উন্মুক্ত ডাটাসেট প্রদান করি যা রাষ্ট্র-অফ-শিল্প মেশিন অনুবাদ মডেলের ভিত্তিতে ভিত্তি", 'bs': 'privatnost podataka je važno pitanje za pružatelje "učenje strojeva kao usluga". Fokusiramo se na problem napada infekcije članstva: s obzirom na uzorak podataka i pristup crnoj kutiji u API model a, utvrđujemo da li je uzorak postojao u podacima o obuci modela. Naš doprinos je istraga ovog problema u kontekstu modela sequence-to-sequence, koji su važni u aplikacijama poput prevoda mašine i snimka. Definiramo problem infekcije članstva za generaciju sekvence, pružamo otvorenu podatku na temelju modela prevoda uređaja stanja umjetnosti, i prijavimo početne rezultate o tome da li ovi modeli cure privatne informacije protiv nekoliko vrsta napada na infekciju članstva.', 'ca': "La privacitat de les dades és un tema important per als proveedors d'aprenentatge màquina com a servei. Ens centrem en el problem a dels atacs de la inferència de la membresa: Dant una mostra de dades i l'accés a una caixa negra a l'API d'un model, determinar si la mostra existia en les dades d'entrenament del model. La nostra contribució és una investigació d'aquest problema en el context de models seqüència a seqüència, que són importants en aplicacions com la traducció màquina i el captionisme de vídeo. Defineixem el problema de la inferència de membres per a la generació de seqüències, proporcionem un conjunt de dades obert basat en models de traducció màquina d'última generació, i informem dels resultats inicials sobre si aquests models filtren informació privada contra diversos tipus d'atacs de inferència de membres.", 'cs': 'Ochrana osobních údajů je důležitou otázkou pro poskytovatele strojového učení jako služby. Zaměřujeme se na problém útoků na závěr členství: Vzhledem k datovému vzorku a přístupu k rozhraní API modelu v černé skříňce určíme, zda vzorek existoval v tréninkových datech modelu. Naším příspěvkem je zkoumání tohoto problému v kontextu modelů sekvence na sekvenci, které jsou důležité v aplikacích jako strojový překlad a video titulky. Definujeme problém inference členství pro generování sekvencí, poskytujeme otevřenou sadu dat založenou na nejmodernějších modelech strojového překladu a nahlásíme první výsledky toho, zda tyto modely unikají soukromé informace proti několika druhům inferenčních útoků členství.', 'et': 'Andmete privaatsus on masinõppe kui teenuse pakkujate jaoks oluline küsimus. Keskendume liikmesuse järelduste rünnakute probleemile: arvestades andmevalimit ja mustkasti juurdepääsu mudeli API-le, tehke kindlaks, kas valim oli mudeli koolitusandmetes. Meie panus on selle probleemi uurimine jada-jada mudelite kontekstis, mis on olulised sellistes rakendustes nagu masintõlge ja video pealdiste. Määratleme järjestuse genereerimiseks liikmesuse järelduse probleemi, pakume kaasaegsetel masintõlkemudelitel põhinevat avatud andmekogumit ja anname esialgseid tulemusi selle kohta, kas need mudelid lekivad privaatset teavet mitmesuguste liikmesuse järeldusrünnakute vastu.', 'af': "Data privateit is 'n belangrike probleem vir 'masjien leer as 'n diens' verskaffer. Ons fokus op die probleem van medelskap inferensie-atake: Gien 'n data voorbeeld en swart-boks toegang tot 'n model se API, bepaal of die voorbeeld in die model se onderwerp data bestaan het. Ons bydrang is 'n ondersoek van hierdie probleem in die konteks van sekwensie-na-sekwensie modele, wat belangrik is in toepassings soos masjien vertaling en video-kapsie. Ons definieer die medelskap inferensie probleem vir sekwensiegenerasie, verskaf 'n oop datastel gebaseer op staat-van-die-kunstens masjien vertalingsmodele, en rapporteer inisiale resultate op of hierdie modele privaat inligting uitlei teen verskeie soorte medelskap inferensie-atake.", 'fi': 'Tietosuoja on tﾃ､rkeﾃ､ kysymys koneoppimisen palveluna -tarjoajille. Keskitymme jﾃ､senjohtohyﾃｶkkﾃ､ysten ongelmaan: Kun otetaan huomioon tietonﾃ､yte ja mustan laatikon pﾃ､ﾃ､sy mallin sovellusliittymﾃ､ﾃ､n, mﾃ､ﾃ､ritﾃ､, oliko malli olemassa mallin koulutustiedoissa. Meidﾃ､n panoksemme on tutkia tﾃ､tﾃ､ ongelmaa sekvenssimallien kontekstissa, jotka ovat tﾃ､rkeitﾃ､ sovelluksissa, kuten konekﾃ､ﾃ､nnﾃｶs ja videotekstitys. Mﾃ､ﾃ､rittelemme jﾃ､senpﾃ､ﾃ､ttelyongelman sekvenssien luomiselle, tarjoamme avoimen datajoukon, joka perustuu viimeisimpiin konekﾃ､ﾃ､nnﾃｶsmalleihin, ja raportoimme alustavat tulokset siitﾃ､, vuotavatko nﾃ､mﾃ､ mallit yksityistﾃ､ tietoa erilaisten jﾃ､senpﾃ､ﾃ､ttelyhyﾃｶkkﾃ､ysten varalta.', 'hy': "Տվյալների գաղտնիությունը կարևոր խնդիր է «մեքենային սովորելու որպես ծառայություն» պարտադրողների համար: We focus on the problem of membership inference attacks: Given a data sample and black-box access to a model's API, determine whether the sample existed in the model's training data.  Մեր ներդրումը այս խնդրի ուսումնասիրությունն է հաջորդականության մոդելների կոնտեքստում, որոնք կարևոր են այնպիսի ծրագրերում, ինչպիսիք են մեքենային թարգմանությունը և տեսագրությունը: Մենք սահմանում ենք անդամների հետևանքների խնդիրը հաջորդականության սերունդների համար, ապահովում ենք բաց տվյալների համակարգ, որը հիմնված է ամենաբարձր մեքենային թարգմանման մոդելների վրա, և զեկուցում ենք սկզբնական արդյունքները այն մասին, թե արդյոք այս մոդելները մասնավոր", 'ha': "Primary data yana da wani muhimu wa 'Shirin' da za'a sanar da shi kamar mai amfani da shirin ayuka. Tuna fokus a kan masu shawarar masu iya zartar da shirin ayuka: Gida wani misali da aka samu-misali da matsayin duffai zuwa wani misali na kwamfyutan ayuka, ka ƙayyade misalin ya kasance a cikin data na amfani da shirin ayuka. Bayanmu da aikin mu ne yin ƙidãya a cikin misalin wannan masu cikin misalin-sauri-sauri, da masu muhimu a cikin shiryoyin ayuka kamar fassarar mashine da tsarin video. Kana bayyana matabbatar wa mai ƙidãya wa kiyayen kowace, don mu samar da tsarin bayani na-state-of-the-art masu fassarar mashine, kuma mu rarraba fassarar farko a kan musammalin, ko za'a leak information fara ɗaya kan abubuwa masu ƙaranci.", 'he': "Data privacy is an important issue for 'machine learning as a service' providers.  We focus on the problem of membership inference attacks: Given a data sample and black-box access to a model's API, determine whether the sample existed in the model's training data.  התרומה שלנו היא חקירה של הבעיה הזאת בתוך הקשר של דוגמנים רצף לרצף, שחשוב בתוכניות כמו תרגום מכונות וכתובת וידאו. אנו מגדירים את בעיית ההנחה של חברות לדור הרצף, מספקים קבוצת נתונים פתוחה מבוססת על מודלים התרגום המכונה המאוחרים ביותר, ודווחים על התוצאות הראשוניות על האם דוגמנים אלה דולפים מידע פרטי נגד מספר סוגים של תקיפות ההנחה של חברות.", 'sk': 'Zasebnost podatkov je pomembno vprašanje za ponudnike strojnega učenja kot storitev. Osredotočamo se na problem napadov sklepanja članstva: Glede na vzorec podatkov in dostop do API modela v črni škatli ugotovite, ali je vzorec obstajal v podatkih o usposabljanju modela. Naš prispevek je raziskava tega problema v kontekstu modelov zaporedja v zaporedje, ki so pomembni v aplikacijah, kot sta strojno prevajanje in video napisovanje. Opredelimo problem sklepanja članstva za generiranje zaporedja, zagotovimo odprt nabor podatkov na podlagi najsodobnejših modelov strojnega prevajanja in poročamo o začetnih rezultatih o tem, ali ti modeli puščajo zasebne informacije pred več vrstami napadov sklepanja članstva.', 'bo': "སྒེར་གྱི་ཆ་འཕྲིན་གྱི་ཆ་འཕྲིན་དེ་ 'མ་ལག་གི་ཁྱད་པར་ཞབས་ཞུགས་ཀྱི་བྱིས་ཆས་ལ་གལ་ཆེན་ཤིག་རེད། We focus on the problem of membership inference attacks: Given a data sample and black-box access to a model's API, determine whether the sample existed in the model's training data. ང་ཚོའི་གོ་སྤྲོད་ཀྱི་གོ་སྐབས་ཡུལ་གྱིས་དབྱེ་རིམ་ནང་གི་དཀའ་ངལ་འདི་ཞིབ་དཔྱད་ཡོད་པ་ལྟར་མཇུག་བསྡུས་ལས་དབྱེ་རིམ་ནང་ད We define the membership inference problem for sequence generation, provide an open dataset based on state-of-the-art machine translation models, and report initial results on whether these models leak private information against several kinds of membership inference attacks.", 'jv': 'User Awak dhéwé seneng nglanggar kuwi kesempal karo pernik nggawe barang urip: nyimpen sampler data lan nganggo pernik-box sing diranggo Ap model, dadi nunggo perusahaan model sing takot nang data atualizasi model Nhal nganggep ning gunakake perusahaan kanggo nyumbang iki ning sak model sing sekondirne ning sekondirne, sing mau basa ning aplikasi sing dibutuhke sistem lan video. Awak dhéwé nggawe sistem menyang karo perbudhakan kanggo ngerasai winih sing berarti, nyebute dataset sing bukum sistem sing basa supra model state-of-the-arts'}
{'en': 'SpanBERT : Improving Pre-training by Representing and Predicting Spans', 'es': 'Spanbert: mejora de la preformación mediante la representación y la predicción de intervalos', 'ar': 'سبانبيرت: تحسين التدريب المسبق من خلال تمثيل وتوقع الفترات', 'fr': 'SpanBert\xa0: améliorer la pré-formation en représentant et en prédisant les portées', 'pt': 'SpanBERT: Melhorando o pré-treinamento ao representar e prever períodos', 'ja': 'SpanBERT ：スパンの表現と予測による事前トレーニングの改善', 'zh': 'SpanBERT:示跨度以豫教也', 'hi': 'SpanBERT: प्रतिनिधित्व और भविष्यवाणी Spans द्वारा पूर्व प्रशिक्षण में सुधार', 'ru': 'SpanBERT: Совершенствование предварительного обучения путем представления и прогнозирования диапазонов', 'ga': 'SpanBERT: Réamhoiliúint a Fheabhsú trí Réise a Ionadaíocht agus a Thuar', 'el': 'Βελτίωση της προεκπαίδευσης με την εκπροσώπηση και την πρόβλεψη των διαστάσεων', 'ka': 'SpanBERT: გარეშე და გარეშე სპონტების გასაკეთება', 'hu': 'SpanBERT: Az előképzés javítása a feszültségek reprezentálásával és előrejelzésével', 'kk': 'SpanBERT: Алдыңғы оқыту және таңдау бойынша', 'mk': 'SpanBERT: подобрување на преобуката со претставување и предвидување на растенија', 'lt': 'SpanBERT: Parengiamojo mokymo gerinimas atstovaujant ir numatant išankstinius mokymus', 'it': 'SpanBERT: Migliorare la pre-formazione attraverso la rappresentazione e la previsione delle distanze', 'ms': 'SpanBERT: Menembak Latihan-Latihan Melalui Perwakilan dan Prediksi Span', 'ml': 'സ്പാന്\u200dബെര്\u200dട്ട്: റിസ്റ്റീന്\u200dസ് ചെയ്യുന്നതും മുന്\u200dകൂട്ടി പരിശീലിപ്പിക്കുന്നതും മുന്\u200dകൂട്ടുക', 'mt': 'SpanBERT: It-titjib tat-taħriġ minn qabel permezz tar-Rappreżentanza u t-Tbassir tal-Ispannijiet', 'mn': 'СПАНБЕРТ: Өмнөх сургалтын хөгжлийн төлөө', 'ro': 'SpanBERT: Îmbunătățirea pregătirii prin reprezentarea și predicția intervalelor', 'no': 'SpanBERT: Forbetra føreøving ved å representera og forventa mellomrom', 'pl': 'SpanBERT: Poprawa przedszkoleń poprzez reprezentowanie i przewidywanie rozpięć', 'sr': 'SpanBERT: Poboljšanje predobuke predstavljajući i predviđajući španjolske', 'si': 'spanBERT: ප්\u200dරධාන සහ ප්\u200dරධාන ස්පේන්ස් වලින් ප්\u200dරධාන ප්\u200dරශ්නයක් විස්තර කරන්න', 'so': 'SpanBERT: Improving pre-training by Representing and Preparation Spanish', 'ta': 'ஸ்பான்பெர்ட்: முன் பயிற்சியை மேம்படுத்துதல் மற்றும் முன்னேற்றுதல் மூலம்', 'ur': 'SpanBERT: Representing and Predicting Spans', 'sv': 'SpanBERT: Förbättra fortbildningen genom att representera och förutse sträckor', 'uz': 'Name', 'vi': 'Trình đầu huấn luyện tốt bằng cách đại diện và đoán trước', 'bg': 'Подобряване на предварителното обучение чрез представяне и прогнозиране на диапазоните', 'hr': 'SpanBERT: Poboljšanje predobuke predstavljajući i predviđajući španjolske prostore', 'nl': 'SpanBERT: Pre-training verbeteren door spans te representeren en voorspellen', 'da': 'SpanBERT: Forbedring af forudgående træning ved at repræsentere og forudsige spændinger', 'de': 'SpanBERT: Verbesserung des Pre-Trainings durch Darstellung und Vorhersage von Spannen', 'id': 'SpanBERT: Menembangkan Pra-pelatihan dengan Memperkembangkan dan Prediksi Span', 'ko': 'SpanBERT: 경계를 표시하고 예측함으로써 예비 훈련을 개선합니다', 'fa': 'SpanBERT: تحصیل پیش آموزش توسط نمایش و پیش\u200cبینی اسپانیایی', 'sw': 'SpanBERT: Kuboresha mafunzo ya Tayari kwa Kuwakilisha na Kujiandaa Hispania', 'sq': 'SpanBERT: Përmirësimi i paratrainimit nga përfaqësimi dhe parashikimi i Spanjave', 'tr': 'SpanBERT: Öňki okuwçylygy Representing and Predicting Spans tarapyndan gowylaşdyrylýar', 'af': 'SpanBERT: Verbeter vooraf- oefening deur voorstel en voorskou Spans', 'am': 'Spanish', 'hy': 'ԻսպանԲերթ՝ Առաջնական ուսուցման բարելավումը ներկայացնելով և կանխատեսելով', 'az': 'SpanBERT: √ñn-t…ôhsil t…ôhsil etm…ôk v…ô t…ôhsil etm…ôk vasit…ôsil…ô', 'bn': 'স্প্যান্বেরেট: প্রতিনিধি এবং প্রস্তুতি প্রশিক্ষণের মাধ্যমে প্রাপ্ত প্রশিক্ষণ উন্নতি করা হচ্ছে', 'cs': 'SpanBERT: Zlepšení předškolení reprezentováním a predikcí rozpětí', 'ca': 'SpanBERT: millorar la pré-capacitació representant i predicuent els espais', 'bs': 'SpanBERT: Poboljšanje predobuke predstavljajući i predviđajući španjolske', 'et': 'SpanBERT: Eelkoolituse parandamine piirkondade esindamise ja prognoosimise kaudu', 'fi': 'SpanBERT: Esikoulutuksen parantaminen edustamalla ja ennakoimalla rajoja', 'jv': 'SpanBERT: Ulihke Reyes', 'sk': 'SpanBERT: Izboljšanje predusposabljanja s predstavljanjem in predvidevanjem razponov', 'he': 'ספנברט: שיפור האימונים הקדמיים על ידי מייצג ומצפה ספנס', 'ha': 'KCharselect unicode block name', 'bo': 'SpanBERT: Representing and Predicting Spans ། Improving Pre-training by Representing and Predicting Spans'}
{'en': 'We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms ', 'ar': 'نقدم SpanBERT ، طريقة ما قبل التدريب المصممة لتمثيل وتوقع مسافات النص بشكل أفضل. يمتد نهجنا BERT من خلال (1) إخفاء الامتدادات العشوائية المتجاورة ، بدلاً من الرموز المميزة العشوائية ، و (2) تدريب تمثيلات حدود الامتداد للتنبؤ بالمحتوى الكامل للمدى المقنع ، دون الاعتماد على التمثيلات الرمزية الفردية داخلها. يتفوق SpanBERT باستمرار على BERT وخطوط الأساس التي تم ضبطها بشكل أفضل ، مع مكاسب كبيرة في مهام اختيار النطاق مثل الإجابة على الأسئلة وحل المرجع. على وجه الخصوص ، مع نفس بيانات التدريب وحجم النموذج مثل BERTlarge ، يحصل نموذجنا الفردي على 94.6٪ و 88.7٪ F1 على SQuAD 1.1 و 2.0 على التوالي. نحقق أيضًا حالة جديدة من الفن في مهمة تحليل المرجع المرجعي OntoNotes (79.6٪ F1) ، وأداء قوي في معيار استخراج علاقة TACRED ، وحتى مكاسب على GLUE.', 'es': 'Presentamos Spanbert, un método de preentrenamiento que está diseñado para representar y predecir mejor los intervalos de texto. Nuestro enfoque amplía BERT al (1) enmascarar los tramos aleatorios contiguos, en lugar de los tokens aleatorios, y (2) entrenando las representaciones de los límites del tramo para predecir todo el contenido del intervalo enmascarado, sin depender de las representaciones de los tokens individuales dentro de él. SpanBert supera constantemente a BERT y a nuestras líneas de base mejor ajustadas, con ganancias sustanciales en las tareas de selección de intervalos, como la respuesta a preguntas y la resolución de correferencias. En particular, con los mismos datos de entrenamiento y tamaño de modelo que BertLarge, nuestro modelo único obtiene un 94,6% y un 88,7% de F1 en sQuad 1.1 y 2.0 respectivamente. También logramos un nuevo estado del arte en la tarea de resolución de correferencias de OntoNotes (79,6% F1), un sólido rendimiento en el punto de referencia de extracción de relaciones TACRED e incluso ganancias en GLUE.1', 'fr': "Nous présentons SpanBert, une méthode de pré-formation conçue pour mieux représenter et prédire les étendues de texte. Notre approche étend le BERT en (1) masquant des plages aléatoires contiguës, plutôt que des jetons aléatoires, et (2) en entraînant les représentations de limites de portée pour prédire le contenu complet de la plage masquée, sans dépendre des représentations de jetons individuelles qu'elle contient. SpanBERT surpasse constamment BERT et nos lignes de base mieux ajustées, avec des gains substantiels sur les tâches de sélection de plage telles que la réponse aux questions et la résolution de coréférence. En particulier, avec les mêmes données d'entraînement et la même taille de modèle que BertLarge, notre modèle unique obtient 94,6\xa0% et 88,7\xa0% de F1 sur SQuad 1.1 et 2.0 respectivement. Nous atteignons également un nouvel état de l'art sur la tâche de résolution de coréférence OnToNotes (79,6% F1), de bonnes performances sur le benchmark d'extraction de relations TACRED et même des gains sur GLUE.1", 'pt': 'Apresentamos o SpanBERT, um método de pré-treinamento projetado para melhor representar e prever extensões de texto. Nossa abordagem estende o BERT (1) mascarando intervalos aleatórios contíguos, em vez de tokens aleatórios, e (2) treinando as representações de limite de intervalo para prever todo o conteúdo do intervalo mascarado, sem depender das representações de token individuais dentro dele. O SpanBERT supera consistentemente o BERT e nossas linhas de base melhor ajustadas, com ganhos substanciais em tarefas de seleção de span, como resposta a perguntas e resolução de correferência. Em particular, com os mesmos dados de treinamento e tamanho de modelo do BERTlarge, nosso modelo único obtém 94,6% e 88,7% de F1 no SQuAD 1.1 e 2.0, respectivamente. Também alcançamos um novo estado da arte na tarefa de resolução de correferência OntoNotes (79,6% F1), forte desempenho no benchmark de extração de relação TACRED e até ganhos em GLUE.1', 'ja': 'SpanBERTは、テキストのスパンをよりよく表現し、予測するように設計された事前トレーニング方法です。私たちのアプローチは、（ 1 ）ランダムなトークンではなく、連続したランダムなスパンをマスキングすること、および（ 2 ）マスキングされたスパンの全体的なコンテンツを予測するために、その中の個々のトークン表現に依存することなく、スパン境界表現をトレーニングすることによって、BERTを拡張します。SpanBERTは、常にBERTとより調整されたベースラインを上回り、質問の回答やコアリファレンスの解決などのSPAN選択タスクで実質的な利益を得ています。特に、BERTlargeと同じトレーニングデータとモデルサイズで、当社の単一モデルは、SQuAD 1.1と2.0でそれぞれ94.6 ％と88.7 ％のF 1を取得します。また、OntoNotesコアリファレンス解決タスク（ 79.6 ％ F 1 ）では最先端のパフォーマンスを実現し、TACREDリレーション抽出ベンチマークでは強力なパフォーマンスを実現し、GLUE 1ではさらに優れたパフォーマンスを実現しています。', 'hi': 'हम स्पैनबर्ट, एक पूर्व-प्रशिक्षण विधि प्रस्तुत करते हैं जिसे पाठ के स्पैन का बेहतर प्रतिनिधित्व करने और भविष्यवाणी करने के लिए डिज़ाइन किया गया है। हमारा दृष्टिकोण BERT को (1) यादृच्छिक टोकन के बजाय सन्निहित यादृच्छिक स्पैन को मास्किंग करके बढ़ाता है, और (2) नकाबपोश अवधि की पूरी सामग्री की भविष्यवाणी करने के लिए स्पैन सीमा प्रतिनिधित्व को प्रशिक्षित करता है, इसके भीतर व्यक्तिगत टोकन प्रतिनिधित्व पर भरोसा किए बिना। SpanBERT लगातार BERT और हमारे बेहतर ट्यून बेसलाइन, इस तरह के सवाल का जवाब और coreference संकल्प के रूप में अवधि चयन कार्यों पर पर्याप्त लाभ के साथ outperforms. विशेष रूप से, BERTlarge के रूप में एक ही प्रशिक्षण डेटा और मॉडल आकार के साथ, हमारा एकल मॉडल क्रमशः SQuAD 1.1 और 2.0 पर 94.6% और 88.7% F1 प्राप्त करता है। हम OntoNotes coreference रिज़ॉल्यूशन कार्य (79.6% F1), TACRED संबंध निष्कर्षण बेंचमार्क पर मजबूत प्रदर्शन, और यहां तक कि GLUE.1 पर भी लाभ पर कला की एक नई स्थिति प्राप्त करते हैं', 'zh': '言SpanBERT,预训练方法也,意在善占文本之跨度。 吾法以(1)屏蔽连续跨度非随机标记以广BERT,及(2)练跨度界以占蔽跨度之全部内容,而不恃其单表也。 SpanBERT 之为言也,始于 BERT ,与吾治之基线,择事于跨度(问与共指决)而得实质性益。 与BERTlarge同练数,吾等于SQuAD 1.1、2.0,各得94.6%88.7%之F1。 又于 OntoNotes 共分引解析务(79.6% F1)最先进之技术水平,于 TACRED 取基准以成强大,至于 GLUE 上亦有收益。', 'ru': 'Мы представляем SpanBERT, метод предварительного обучения, который предназначен для лучшего представления и прогнозирования диапазонов текста. Наш подход расширяет BERT, (1) маскируя смежные случайные пролеты, а не случайные токены, и (2) обучая представления границ пролета предсказывать все содержимое маскируемого пролета, не полагаясь на отдельные представления токенов внутри него. SpanBERT неизменно превосходит BERT и наши лучше настроенные базовые линии, с существенным выигрышем в задачах выбора диапазона, таких как ответы на вопросы и разрешение ядра. В частности, при тех же данных обучения и размере модели, что и BERTlarge, наша единая модель получает 94,6% и 88,7% F1 на SQuAD 1.1 и 2.0 соответственно. Мы также достигли нового уровня техники по задаче разрешения ядра OntoNotes (79,6% F1), высокой производительности по эталону извлечения отношений TACRED и даже выигрыша по GLUE.1', 'ga': 'Cuirimid i láthair SpanBERT, modh réamhoiliúna atá deartha chun réimsí téacs a léiriú agus a thuar ar bhealach níos fearr. Leathnaíonn ár gcur chuige CRET trí (1) réisí randamacha comhtheagmhálacha a chumhdach, seachas comharthaí randamacha, agus (2) oiliúint a chur ar léirithe teorann réise chun ábhar iomlán na réise chumhdaigh a thuar, gan a bheith ag brath ar na huiríll comharthaí aonair laistigh di. Is fearr le SpanBERT go seasta ná ár mbunlínte níos téite, le gnóthachain shuntasacha ar thascanna roghnaithe réise cosúil le freagairt ceisteanna agus réiteach croí-chomhdhála. Go háirithe, leis na sonraí oiliúna céanna agus an méid múnla céanna le BERTlarge, faigheann ár múnla aonair 94.6% agus 88.7% F1 ar SQuAD 1.1 agus 2.0 faoi seach. Bainimid amach freisin úrscothacht ar thasc réitigh croíchomhdhála OntoNotes (79.6% F1), feidhmíocht láidir ar thagarmharc eastósctha TACRED, agus fiú gnóthachain ar GLUE.1', 'hu': 'Bemutatjuk a SpanBERT-et, egy előkészítő módszert, amelyet arra terveztek, hogy jobban ábrázolja és megjósolja a szövegtartományokat. Megközelítésünk kiterjeszti a BERT-t (1) a véletlenszerű tokenek helyett egymással összefüggő véletlenszerű tartományok maszkolásával, és (2) a span határábrázolásával, hogy megjósolják a maszkos tartomány teljes tartalmát, anélkül, hogy az egyes tokenreprezentációkra támaszkodnánk. A SpanBERT következetesen felülmúlja a BERT-t és a jobban hangolt alapvonalainkat, és jelentős előnyöket jelent a tartományválasztási feladatokban, mint például a kérdésválasztás és a coreferencia megoldás. Különösen a BERTlarge edzési adataival és modellméretével egyetlen modellünk 94,6%, illetve 88,7% F1-t ér el az SQUAD 1.1 és 2.0 esetén. Az OntoNotes coreferencia felbontási feladat (79,6% F1) területén is új technológiát érünk el, erős teljesítményt érünk el a TACRED kapcsolatkivonási referenciaérték tekintetében, sőt a GLUE-nál is elérhető nyereséget. 1', 'el': 'Παρουσιάζουμε μια μέθοδο προενταξιακής εκπαίδευσης που έχει σχεδιαστεί για να αναπαριστά καλύτερα και να προβλέψει περιοχές κειμένου. Η προσέγγισή μας επεκτείνει το BERT με (1) τη συγκάλυψη συνεχών τυχαίων διαστάσεων, αντί για τυχαία σήματα, και (2) την εκπαίδευση των αναπαραστάσεων ορίων εύρους ώστε να προβλέπουν ολόκληρο το περιεχόμενο του καλυμμένου διαστήματος, χωρίς να βασίζονται στις μεμονωμένες αναπαραστάσεις συμβολαίων μέσα σε αυτό. Το ΣπανBERT ξεπερνά συνεχώς το BERT και τις καλύτερα συντονισμένες γραμμές βάσης μας, με σημαντικά κέρδη σε εργασίες επιλογής φάσματος, όπως η απάντηση σε ερωτήσεις και η επίλυση συναλλακτικών διαφορών. Ειδικότερα, με τα ίδια δεδομένα προπόνησης και το ίδιο μέγεθος μοντέλου με το μοναδικό μας μοντέλο αποκτά 94.6% και 88.7% F1 στο SQuAD 1.1 και 2.0 αντίστοιχα. Επιτυγχάνουμε επίσης μια νέα κατάσταση τεχνολογίας όσον αφορά την εργασία επίλυσης συναλλακτικών διαφορών (79.6% F1), ισχυρή απόδοση στο σημείο αναφοράς εξαγωγής σχέσεων TACRED, ακόμη και κέρδη στο GLUE. 1', 'ka': 'ჩვენ აჩვენებთ SpanBERT, პრე-სტრინციის მეტი, რომელიც განაზღვრებულია ტექსტის უფრო მეტად გამოსახულებლად და წინახულებლად. ჩვენი პროგორმაცია BERT-ს (1) მაქსირებით შემცირებული კონფიგური სინამდვილეების მაქსირებით, ვიდრე შემცირებითი სინამდვილეების გარეშე, და (2) მაქსირებითი სინამდვილეების გარეშე, რომ მაქსირებულ სინამდ SpanBERT მუშაობელია BERT და ჩვენი უკეთესი კონფიგურაციული ფესური ხაზები, რომლებიც სპენტურაციის მონიშნულებაში მნიშვნელოვანი მიღება, როგორც კითხვების პასუხი და კონფიგ განსაკუთრებით, იგივე მონაცემები და მოდელური ზომა, როგორც BERTlarge, ჩვენი ერთი მოდელი მიიღება 94,6% და 88,7% F1 SQuAD 1.1 და 2.0-ზე. ჩვენ ასევე გავაკეთებთ ახალი სურათის განსხვავება OntoNotes-ის კონფერენციის განსხვავებაზე (79.6% F1), ძალიან გავაკეთება TACRED-ის განსხვავებაზე და GLUE-ის კონფერენციის განსხვავებაზე. 1', 'kk': 'Біз SpanBERT-ті, мәтіннің кеңістіктерін көрсету және алдын- ала оқыту әдісін таңдаймыз. Біздің тәсіліміз BERT (1) кездейсоқ белгілерді қалқалау үшін кездейсоқ кездейсоқ белгілерді қалқалау үшін (2) кездейсоқ белгілерді қалқалау үшін белгілі белгілердің мазмұнын көрсету үшін кездейсоқ белгілерд SpanBERT BERT және біздің жақсы түрлендірілген негізгі жолдарымызға әсер етеді. Сұрақ жауап беру және мәселелердің айырмашылығы секілді көптеген таңдау тапсырмалардың көп жетілдері бар Әдетте, BERTlarge деген бір оқыту деректері мен үлгі өлшемі бар, біздің бір моделіміз SQuAD 1. 1 және 2. 0 дегенде 94, 6% және 88, 7% F1 болады. Сонымен қатар, OntoNotes қатынасының тапсырмасының жаңа күйін жеткіземіз (79.6% F1), TACRED қатынасының сәйкестігін тарқату белгісінің көмегімен және GLUE қатынасының қатынасы бар. 1', 'mk': 'Ние го претставуваме SpanBERT, метод за предобука кој е дизајниран за подобро да претставува и предвидува растојание на текст. Нашиот пристап го проширува БЕРТ со (1) маскирање континуирани случајни промени, наместо случајни знаци, и (2) тренирање на граничните претставувања на промената за предвидување на целата содржина на маскираниот промен, без да се потпира на индивидуалните претставувања на знаци во него. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution.  Особено, со истите податоци за обука и големина на моделот како Бертголем, нашиот единствен модел добива 94,6 отсто и 88,7 отсто Ф1 на SQuAD 1,1 и 2,0 отсто. Ние, исто така, постигнуваме нова техничка состојба во врска со задачата за резолуција на коференцијата на OntoNotes (79,6 отсто F1), силна резултат на референцијата за извлекување на односите со TACRED и дури и добивка на GLUE. 1', 'lt': 'Pateikiame SpanBERT, parengiamojo mokymo metodą, skirtą geriau atspindėti ir prognozuoti teksto apimtį. Mūsų metodas išplečia BERT (1) maskuodamas gretimus atsitiktinius intervalus, o ne atsitiktinius ženklus, ir (2) treniruodamas intervalo ribų atstovavimus, kad būtų galima prognozuoti visą maskuoto intervalo turinį, nepaisant atskirų ženklų atstovavimų jame. SpanBERT nuosekliai viršija BERT ir mūsų geriau pritaikytas bazines linijas, turint didelę naudą spektro atrankos užduotims, pavyzdžiui, atsakymui į klausimus ir atitikties sprendimui. Visų pirma, naudojant tokius pačius mokymo duomenis ir modelio dydį kaip ir BERTlarge, mūsų vienintelis modelis gauna atitinkamai 94,6 % ir 88,7 % F1 SQuAD 1,1 ir 2,0. Taip pat pasiekėme naują pažangą OntoNotes koreferencijos pertvarkymo užduotyje (79,6 % F1), stiprių rezultatų TACRED santykių išgavimo lyginamuoju rodikliu ir net GLUE naudos. 1', 'it': "Vi presentiamo SpanBERT, un metodo di pre-allenamento progettato per rappresentare e prevedere al meglio gli intervalli di testo. Il nostro approccio estende BERT (1) mascherando span casuali contigui, piuttosto che token casuali, e (2) addestrando le rappresentazioni dei confini di span per prevedere l'intero contenuto dell'span mascherato, senza contare sulle singole rappresentazioni di token all'interno di esso. SpanBERT supera costantemente BERT e le nostre linee di base meglio sintonizzate, con notevoli vantaggi nelle attività di selezione degli intervalli come la risposta alle domande e la risoluzione della coreferenza. In particolare, con gli stessi dati di allenamento e le stesse dimensioni del modello BERTlarge, il nostro modello singolo ottiene rispettivamente il 94,6% e l'88,7% F1 su SQUAD 1.1 e 2.0. Otteniamo anche un nuovo stato dell'arte nell'attività di risoluzione della coreferenza OntoNotes (79,6% F1), forti prestazioni sul benchmark di estrazione delle relazioni TACRED e persino guadagni su GLUE. 1", 'ms': 'Kami memperkenalkan SpanBERT, kaedah pralatihan yang direka untuk mewakili dan meramalkan jangkauan teks yang lebih baik. pendekatan kami melangkah BERT dengan (1) menutup jangkauan rawak berikut, daripada token rawak, dan (2) melatih perwakilan sempadan jangkauan untuk meramalkan seluruh kandungan jangkauan bertopeng, tanpa bergantung pada perwakilan token individu di dalamnya. SpanBERT secara konsisten melampaui BERT dan garis dasar yang lebih sesuai kami, dengan keuntungan yang besar pada tugas pemilihan jangkauan seperti jawapan soalan dan resolusi persamaan. Terutama, dengan data latihan dan saiz model yang sama dengan BERTlarge, model tunggal kita mendapatkan 94.6% dan 88.7% F1 pada SQuAD 1.1 dan 2.0 respectively. Kami juga mencapai kemajuan baru pada tugas resolusi koreferensi OntoNotes (79.6% F1), prestasi kuat pada tanda referensi ekstraksi hubungan TACRED, dan bahkan keuntungan pada GLUE. 1', 'mn': 'Бид SpanBERT-г илүү сайн үзүүлэх, таамаглах боломжтой арга загварын өмнө сургалтын арга загвар өгдөг. Бидний арга хэмжээнд BERT-г (1) хэлбэрээр газрын зураг, санамсаргүй тодорхойлолтуудын оронд газрын зураг, 2) хэлбэрээр газрын зураг дүрслэх хэмжээний бүтээгдэхүүнийг таамаглах боломжтой. СпанБерт БЕРТ болон бидний сайн зохицуулагдсан суурь шулуунуудыг ихэвчлэн нэмэгдүүлдэг. Ялангуяа, BERTlarge-тэй адилхан суралцах өгөгдлийн болон загварын хэмжээтэй бидний ганц загвар нь SQuAD 1.1 болон 2.0 дээр 94.6%, 88.7% F1-г авдаг. Мөн бид OntoNotes-ын зөвхөн шийдвэрлэлтийн ажил (79.6% F1) дээрх урлагийн шинэ байдал гаргаж чадна. TACRED-ын хамааралтай хамааралтай тэмдэглэл болон GLUE-д хүртэл ашиглаж чадна. 1', 'ml': 'ഞങ്ങള്\u200d സ്പാന്\u200dബെര്\u200dട്ടിനെ സമ്മാനിക്കുന്നു, ഒരു പ്രോഗ്രേനിക്കുന്ന രീതിയില്\u200d, അതിനെ നല്ല പ്രതിനിധിക്കാനും പ്ര നമ്മുടെ അടുത്തേക്ക് ബെര്\u200dട്ടിയെ മൂടുന്നത് (1) കുറച്ചുകൂടാതെ കഷ്ടപ്പെടുത്തുന്ന അടയാളങ്ങള്\u200dക്ക് പകരം മുഖം മൂടുന്നതിന്\u200dറെ ഉള്ളില്\u200d ആശ്രയിക്കാതെ മുഴുവന്\u200d പ സ്പാന്\u200dബെര്\u200dട്ട് ബെര്\u200dട്ടിയെയും നമ്മുടെ മെച്ചപ്പെട്ട അടിസ്ഥാനങ്ങളെയും പ്രവര്\u200dത്തിപ്പിക്കുന്നു. സ്പാന്\u200d തെരഞ്ഞെടുക്കുന്ന ജോലി പ്രത്യേകിച്ച്, ബെര്\u200dട്ടിഗ്രാപ്റ്റന്\u200d ഡേറ്റായും മോഡലിന്റെ വലിപ്പം, നമ്മുടെ ഒറ്റയ്ക്ക് മോഡല്\u200d 94. 6% എടുക്കുന്നു. 88. 7% F1 സ്ക്വാഡ്  ഓണ്\u200dടോനോട്ടുകളുടെ കോര്\u200dഫെന്\u200dസ് വിധിത്തീരുമാനത്തിന്\u200dറെ (79. 6% F1), ടാക്രിഡിയുടെ ബന്ധം പുറത്തെടുക്കുന്നതിന്\u200dറെ ശക്തിയുള്ള പ്രവര്\u200dത്തനങ്ങള്\u200d  1', 'mt': 'Aħna nippreżentaw SpanBERT, metodu ta’ qabel it-taħriġ li huwa mfassal biex jirrappreżenta u jipprevedi a ħjar firxa ta’ testi. L-approċċ tagħna jestendi l-BERT billi (1) jaħbi firxiet każwali kontigwi, minflok tokens każwali, u (2) jitħarrġu r-rappreżentazzjonijiet tal-limiti tal-firxa biex jipprevedu l-kontenut kollu tal-firxa maskrata, mingħajr ma jiddependu fuq ir-rappreżentazzjonijiet individwali tat-tokens fihom. SpanBERT b’mod konsistenti jaqbeż il-prestazzjoni tal-BERT u l-linji bażi tagħna aġġustati aħjar, b’kisbiet sostanzjali fil-kompiti tal-għażla tal-firxa bħalma huma t-tweġiba għall-mistoqsijiet u r-riżoluzzjoni tal-koreferenza. B’mod partikolari, bl-istess dejta ta’ taħriġ u daqs tal-mudell bħal BERTlarge, il-mudell uniku tagħna jikseb 94.6% u 88.7% F1 fuq SQuAD 1.1 u 2.0 rispettivament. Aħna nkisbu wkoll avvanz ġdid dwar il-kompitu tar-riżoluzzjoni tal-koreferenza OntoNotes (79.6% F1), prestazzjoni qawwija fuq il-parametru referenzjarju tal-estrazzjoni tar-relazzjoni TACRED, u saħansitra qligħ fuq GLUE. 1', 'ro': 'Vă prezentăm SpanBERT, o metodă de pre-antrenament concepută pentru a reprezenta mai bine și a prezice intervalele de text. Abordarea noastră extinde BERT prin (1) mascarea intervalelor aleatorii contigue, mai degrabă decât a jetoanelor aleatorii, și (2) instruirea reprezentărilor limitelor intervalului pentru a prezice întregul conținut al intervalului mascat, fără a se baza pe reprezentările individuale ale jetoanelor din interiorul acestuia. SpanBERT depășește în mod constant BERT și liniile noastre de bază mai bine reglate, cu câștiguri substanțiale în sarcinile de selecție a intervalului, cum ar fi răspunsul la întrebări și rezoluția coreferenței. În special, cu aceleași date de antrenament și dimensiuni ale modelului ca BERTlarge, modelul nostru unic obține 94,6% și 88,7% F1 pe SQUAD 1.1 și respectiv 2.0. De asemenea, obținem o nouă stare de tehnologie în ceea ce privește sarcina de rezoluție a corefenței OntoNotes (79,6% F1), performanțe puternice în ceea ce privește valoarea de referință de extracție a relațiilor TACRED și chiar câștiguri în ceea ce privește GLUE. 1', 'no': 'Vi presenterer SpanBERT, eit føreøvingsmetode som er designert for å bedre representera og forhåndsvisa mellomrom av tekst. Tilnærminga vårt utvidar BERT med (1) maskering av tilfeldige tilfeldige mellomrom, i staden for tilfeldige teikn, og (2) treng avstandgrenserepresentasjonane for å foregå hele innhaldet i maskerte område, utan å tilbakekalle på den individuelle representasjonane i det. SpanBERT utfører konsekvent BERT og våre bedre oppsett baselinjer, med stor forsøk på utvalet av spenner som spørsmål og oppløysing av koreferansen. I særskilt, med same opplæringsdata og modellstorleik som BERTlarge, får vår enkelt modell 94,6% og 88,7% F1 på SQuAD 1,1 og 2,0 respectively. Vi oppnår også ein ny tilstand til kunsten på oppgåva for koreferens-oppløysing av OntoNotes (79,6% F1), sterk utvikling på TACRED-relasjonsbenchmarken for utpakking, og til og med gjennomgang av GLUE. 1', 'pl': 'Prezentujemy SpanBERT, metodę przedtreningową, która ma na celu lepsze reprezentowanie i przewidywanie rozpięć tekstu. Nasze podejście rozszerza BERT poprzez (1) maskowanie przyległych losowych rozpięć, zamiast losowych tokenów, oraz (2) szkolenie reprezentacji granic rozpięcia, aby przewidywać całą zawartość maskowanego rozpięcia, bez polegania na indywidualnych reprezentacjach tokenów w nim. SpanBERT konsekwentnie przewyższa BERT i nasze lepiej dostrojone linie bazowe, przynosząc znaczne zyski w zakresie zadań wyboru zakresu, takich jak odpowiadanie na pytania i rozwiązywanie współdziałań. W szczególności, przy takich samych danych treningowych i wielkości modelu jak BERTlarge, nasz pojedynczy model uzyskuje odpowiednio 94.6% i 88.7% F1 na SQuAD 1.1 i 2.0. Osiągamy również nowy stan techniki w zakresie zadań rozdzielczości współdzielnej OntoNotes (79,6% F1), wysoką wydajność w zakresie ekstrakcji relacji TACRED, a nawet zyski na GLUE. 1', 'so': "Waxaynu soo wadaynaa SpanBERT, qaab ka horeysa waxbarasho oo loo qoray si aad ugu wanaagsan u taqaano oo aad u sii sheegtid qoraalka. Dhaqdhaqaalkayagu wuxuu ku fidiyaa BERT (1) waqtiyada dhibaatada ah, taas oo ka bedelan calaamado fudud, iyo (2) wax ku tababarida xuduudaha duurka si uu uga hor tago kooxda cirka maska oo dhan, iyadoo aan ku kalsoonayn qofka gaarka ah oo ku jira. SpanBERT wuxuu si wada jir ah u sameeyaa BERT iyo saldhigyadeena aad u fiican, waxaana lagu helaa faa'iido badan oo ku saabsan shaqooyinka doorashada span sida jawaabta iyo go’aanka kooban. Si gaar ah, marka lagu jiro macluumaadka waxbarashada iyo sida BERTlarge oo kale, qaababkayaga oo kaliya waxay u helaan qiyaas 94.6% iyo 88.7% F1 ee SQuAD 1.1 iyo 2.0. Sidoo kale waxaan sameynaa xaalad cusub oo farshaxan ku saabsan xafiiska kormeedka OntoNotes (79.6% F1), xittaa waxaynu heli karnaa shaqo xoog leh oo ku saabsan bangiga soo saarashada TACRED iyo xittaa waxyaabaha ku saabsan GLUE. 1", 'si': 'අපි ස්පැන්බෙර්ට් විදිහට ප්\u200dරධානයක් පෙන්වන්න පුළුවන් ප්\u200dරශ්නයක් තියෙනවා ඒක හොඳට ප්\u200dරධානය කරන්න ස අපේ ප්\u200dරවේශනය BERT විස්තර කරනවා (1) සම්පූර්ණයෙන් සම්පූර්ණයෙන් සම්පූර්ණයෙන් ක්\u200dරියාත්මක වෙනුවෙන් සම්පූර්ණයෙන් ස්පූර්ණයෙන් ස්පූර්ණය span BERT සාමාන්\u200dයයෙන්ම BERT සහ අපේ හොඳ සැකසුම් ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් සහ ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් වගේම ප්\u200dරශ්නයක් ප්\u200dරතික්\u200dර විශේෂයෙන්, BERTLarge විශේෂයෙන් එකම ප්\u200dරධාන දත්ත සහ මන්ඩල ප්\u200dරමාණය සමග, අපේ එකම මන්ඩල 94.6% සහ 88.7% F1 SQuAD 1.1 සහ 2.0 වලින්. අපි අලුත් ස්ථානයක් ලබාගන්නවා ඔන්ටෝනෝට්ස් කෝරෙෆරෙන්ස් විශේෂණ වැඩසටහන (79.6% F1), TACRED සම්බන්ධ සංවිධානය බෙන්ච්මාර්ක් ව 1', 'sv': 'Vi presenterar SpanBERT, en pre-training metod som är utformad för att bättre representera och förutsäga textomfång. Vårt tillvägagångssätt utökar BERT genom att (1) maskera sammanhängande slumpmässiga spännvidder, snarare än slumpmässiga polletter, och (2) träna spänngränsrepresentationer för att förutsäga hela innehållet i det maskerade spännvidden, utan att förlita sig på de enskilda symbolrepresentationerna inom det. SpanBERT presterar konsekvent bättre än BERT och våra bättre inställda baslinjer, med betydande vinster på spännviddsval som frågesvar och coreference resolution. Med samma träningsdata och modellstorlek som BERTlarge får vår enkelmodell 94,6% och 88,7% F1 på SQUAD 1.1 respektive 2.0. Vi uppnår också ett nytt toppmodernt resultat när det gäller OntoNotes coreference resolution uppgift (79,6% F1), stark prestanda på TACRED relationsextraktion benchmark och även vinster på GLUE. 1', 'sr': 'Predstavljamo SpanBERT, metodu predobuke koji je dizajniran za bolji predstavljanje i predviđanje prostora teksta. Naš pristup proširi BERT (1) maskiranjem slučajnih prostora, umjesto slučajnih znakova, i (2) obučavanjem graničnih predstavljanja da predvidimo cijeli sadržaj maskiranog prostora, bez oslanjanja na pojedinačne predstave znakova unutar njega. SpanBERT konsekventno iznosi BERT i našu bolju prilagođenu osnovnu liniju, sa značajnim dobicama na zadatke izbora spanovanja poput odgovora na pitanja i rješavanja pristojnosti. Posebno, sa istim podacima obuke i veličinom modela kao BERTlarge, naš jedini model dobija 94,6% i 88,7% F1 na SQuAD 1,1 i 2,0. Također postižemo novo stanje umjetnosti na zadatku rezolucije liječnosti OntoNotes (79,6% F1), jaku provedbu na TACRED povezanoj mjeri izvlačenja odnosa, a čak i dobitak na GLUE. 1', 'ta': 'நாங்கள் ஸ்பான்பெர்ட், ஒரு முன் பயிற்சி முறைமையை காண்பிக்கிறோம், அது உரையின் ஸ்பென்ஸ் முன்னேற்றும் முறை நம்முடைய அணுகல் BERT (1) தொடர்புள்ள குறிப்பிட்ட குறிப்புகளை மூடுவதற்கு பதிலாக தேவையான குறிப்புகளை மூடுவது, மற்றும் (2) முகத்தில் உள்ள அனைத்து குறிப்புகளை முழ ஸ்பான்பெர்ட் தொடர்ந்து BERT மற்றும் எங்கள் சிறந்த துண்டிக்கப்பட்ட அடிப்படைக்கோடுகளை வெளியேற்றுகிறது, கேள்வி பதில் மற்றும் குறிப்ப பெர்ட்பெர்ட் போன்ற ஒரே பயிற்சி தரவும் மாதிரி அளவும், எங்கள் ஒற்றை மாதிரி 94. 6% மற்றும் 88. 7% F1 கிடைக்கும் SquaAD 1. 1 மற்றும் 2. 0. நாம் ஒரு புதிய கலைப்பாட்டின் தெளிவுத்திறன் செயல்பாடு (79. 6% F1), TACRED தொடர்பு பிரிப்பு பிரிப்பு குறிப்புக்குறிப்பின் வலிமை செயல்பாடு, மற் 1', 'ur': 'ہم اسپانBERT کو پیش آموزش کا طریقہ پیش کریں گے جو بہترین نمایش اور تغییر کے لئے طراحی کی گئی ہے۔ ہمارا طریقہ BERT کو (1) مسکین کے ذریعہ مسکین کر رہا ہے، بغیر تصادفی علائم کے، اور (2) مسکین کے تمام منصوبات کی تدبیر کرنے کے لئے، بغیر اس کے اندر ایک شخصی علائم کی نشانیوں پر بھروسہ رکھتے ہیں. SpanBERT ہمیشہ BERT اور ہمارے بہترین سینڈ لینڈ سے زیادہ فائدہ اٹھائے جاتے ہیں، جیسے سوال جواب دینے اور مہربانی رخصت کے ذریعہ سے بہترین فائدہ اٹھائے جاتے ہیں. مخصوصا، BERTlarge کے ساتھ ویسی ترینس ڈیٹے اور موڈل سائز کے ساتھ ہماری ایک موڈل 94.6% اور 88.7% F1 کو SQuAD 1.1 اور 2.0 پر ملتا ہے۔ ہم نے بھی ایک نئی حالت پہنچائی OntoNotes coreference resolution task (79.6% F1) پر، TACRED ارتباط اٹھانے والی بانچم پر مضبوط عمل، اور بھی GLUE پر کمائی ہوتی ہے۔ 1" (msgctxt: "panel:showusername") to "1', 'uz': "Biz SpanBERT, matn spanlarini yaxshi taʼminlovchi oldingi usulni ko'rib chiqarish va oldin ishlash usuli. Bizning usuli BERT (1) cheksiz chegarasini o'zgartirib chiqaradi, chegara belgilaridan foydalanadi va (2) uning ichida bir necha belgilarni ishlatmaydi. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution.  Hullas, BERTbig kabi bitta taʼminlovchi maʼlumot va modelning oʻlchami bilan bizning bitta modelmiz boshqa SQuAD 1. 1 va 2. 0 bilan 94.6% va 88.7% F1 ga ega. Biz ደግሞ OntoNotes correspondence resolution вазифаси (79.6% F1), TACRED bogʻlamaning qismlari chegarasini qidirish imkoniyatini bajaramiz va xato GLUE haqida qo'shish imkoniyatini bajaramiz. 1", 'vi': 'Chúng tôi giới thiệu SpanBERT, một phương pháp trước khi huấn luyện được thiết kế nhằm hướng đến khả năng đại diện và dự đoán chi tiết văn bản. Cách tiếp cận của chúng ta có thể mở rộng ALT bằng (1) che đậy những chi tiết ngẫu nhiên liên tục, thay vì những hiệu ngẫu nhiên, và (2) huấn luyện các biểu đồ biên giới vượt thời gian để dự đoán to àn bộ nội dung đeo mặt nạ, mà không dựa vào các biểu tượng biểu tượng trên nó. Những sàn đấu thắng hoàn toàn vượt trội của thiếu sót sót do thiếu sót và thiết lập nền, với những lợi nhuận quan trọng trong các nhiệm vụ chọn lọc, như câu hỏi trả lời và nghị quyết. Đặc biệt, với dữ liệu huấn luyện và kích thước mô hình giống như BERTlarge, mô hình duy nhất của chúng ta có thể đạt được 94.6. và 88.7=. F1.1 và 2.0. Chúng tôi cũng đạt được một bước tiến mới trong nhiệm vụ giải quyết thuyết trình duy nhất OntoNotes (97.6=.* F1), khả năng đạt kết quả nghiêm trọng về các mối quan hệ chiết xuất tập kết TARED, và còn tăng lợi nhuận trên GLUE. L', 'bg': 'Представяме метод за предобучение, който е предназначен да представя и предсказва по-добре диапазоните на текста. Нашият подход разширява BERT чрез (1) маскиране на съседни случайни интервали, а не случайни символи, и (2) обучение на представителствата на границите на обхвата, за да предскаже цялото съдържание на маскирания интервал, без да разчита на индивидуалните символи в него. Постоянно превъзхожда BERT и нашите по-добре настроени базови линии, със значителни печалби в задачите за избор на обхват, като отговор на въпроси и решаване на съвместни референции. По-специално, със същите данни за обучение и размер на модела като нашия единичен модел получава съответно 94.6% и 88.7% Ф1 на SQuAD 1.1 и 2.0. Също така постигаме ново състояние на изкуството по задачата за разделяне на кореференцията (79,6% Формула), силни резултати по референтния показател за екстракция на отношенията и дори печалби по GLUE. 1', 'nl': 'We presenteren SpanBERT, een pre-training methode die is ontworpen om spanten van tekst beter weer te geven en te voorspellen. Onze aanpak breidt BERT uit door (1) aaneengesloten random spans te maskeren in plaats van willekeurige tokens, en (2) de span grens representaties te trainen om de volledige inhoud van de gemaskeerde span te voorspellen, zonder te vertrouwen op de individuele token representaties erin. SpanBERT presteert consequent beter dan BERT en onze beter afgestemde baselines, met aanzienlijke winsten op spanselectietaken zoals het beantwoorden van vragen en het oplossen van coreferenties. Met name, met dezelfde trainingsgegevens en modelgrootte als BERTlarge, behaalt ons single model respectievelijk 94.6% en 88.7% F1 op SQuAD 1.1 en 2.0. We bereiken ook een nieuwe state of the art op het gebied van de OntoNotes coreference resolution taak (79,6% F1), sterke prestaties op de TACRED relatie extractie benchmark en zelfs winsten op GLUE. 1', 'da': 'Vi præsenterer SpanBERT, en pre-training metode, der er designet til bedre at repræsentere og forudsige omfanget af tekst. Vores tilgang udvider BERT ved (1) at maskere sammenhængende tilfældige spænder, snarere end tilfældige tokens, og (2) at træne spændgrænserepræsentationerne til at forudsige hele indholdet af det maskerede spænd, uden at stole på de enkelte tokenrepræsentationer i det. SpanBERT overgår konsekvent BERT og vores bedre afstemte basislinjer med betydelige gevinster på spændingsudvælgelsesopgaver som spørgsmål besvarelse og coreference løsning. Med de samme træningsdata og modelstørrelse som BERTlarge får vores enkeltmodel henholdsvis 94,6% og 88,7% F1 på SQUAD 1.1 og 2.0. Vi opnår også en ny state of te art på OntoNotes coreferenceopløsningsopgave (79,6% F1), stærk ydeevne på TACRED relation ekstraktion benchmark og endda gevinster på GLUE. 1', 'hr': 'Predstavljamo SpanBERT, metodu predobuke koji je dizajniran za bolje predstavljanje i predviđanje mjera teksta. Naš pristup proširi BERT (1) maskiranjem kontaktivnih slučajnih prostora, umjesto slučajnih znakova, i (2) obučavanjem graničnih predstavljanja razdoblja za predviđanje cijelog sadržaja maskiranog razdoblja, bez oslanjanja na pojedinačne predstave znakova unutar njega. SpanBERT stalno iznosi BERT i naše bolje prilagođene osnovne linije, s značajnim dobicama na zadatke izbora razdoblja poput odgovora na pitanje i rješavanja pristojnosti. Posebno, s istim podacima o obuci i veličini modela kao BERTlarge, naš jedini model dobija 94,6% i 88,7% F1 na SQuAD 1,1 i 2,0. Također postignemo novo stanje umjetnosti o zadatku rezolucije liječnosti OntoNotes (79,6% F1), jakoj učinkovitosti na TACRED povezanoj mjeri za izvlačenje odnosa, a čak i dobiti GLUE. 1', 'fa': 'ما اسپانبرت را پیش آموزش طراحی می\u200cکنیم، که برای بهتر نمایش و پیش\u200cبینی فضای متن\u200cها طراحی شده است. روش ما BERT را با (1) ماسک کردن فضای تصادفی مختلف، به جای نشانه\u200cهای تصادفی، و (2) آموزش نمایش\u200cهای مرز طولانی برای پیش\u200cبینی تمام محتوای مدت ماسک، بدون اعتماد به نمایش\u200cهای نشانه\u200cهای شخصی در آن. SpanBERT همیشه BERT و خطوط پایین\u200cهای بهترین ترجیح داده شده\u200cایم، با پیروزی بسیار زیادی در کار انتخاب\u200cهای طولانی مانند پاسخ\u200cدادن سوال و حل\u200cسازی رضایت. مخصوصا با اطلاعات آموزش و اندازه مدل به عنوان BERTlarge، مدل تنها ما 94.6% و 88.7% F1 را در SQuAD 1.1 و 2.0 دریافت می\u200cکند. ما همچنین یک وضعیت جدید از هنر در وضعیت مقدمات رضایت OntoNotes (79.6% F1) رسیده ایم، عملکرد قوی در مقدمات ارتباط TACRED، و حتی دریافت GLUE. ۱', 'id': 'Kami memperkenalkan SpanBERT, metode pra-pelatihan yang dirancang untuk mewakili dan memprediksikan lebih baik jangkauan teks. pendekatan kita memperluas BERT dengan (1) menutupi jangkauan acak berikutnya, bukan token acak, dan (2) melatih representation batas jangkauan untuk memprediksi seluruh isi jangkauan bertopeng, tanpa bergantung pada representation token individu di dalamnya. SpanBERT secara konsisten melebihi BERT dan garis dasar kita yang lebih sesuai, dengan keuntungan yang besar pada tugas seleksi span seperti jawaban pertanyaan dan resolusi koreferensi. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively.  Kami juga mencapai state of the art baru pada tugas resolusi koreferensi OntoNotes (79,6% F1), prestasi kuat pada benchmark ekstraksi hubungan TACRED, dan bahkan keuntungan pada GLUE. 1', 'ko': 'SpanBERT를 소개했는데, 이것은 텍스트의 경계를 더욱 잘 표시하고 예측하기 위한 예비 훈련 방법이다.우리의 방법은 (1) 랜덤 표시가 아닌 연속적인 랜덤 표시를 차단하고 (2) 훈련 경계 표시를 통해 차단 경계의 전체 내용을 예측하며 그 중의 단일 표시 표시에 의존하지 않고 BERT를 확장한다.SpanBERT는 BERT와 우리가 더욱 잘 조정한 기선보다 시종일관 우수하여 문제에 대답하고 공통된 문제를 해결하는 등 크로스 선택 임무에서 실질적인 발전을 이루었다.특히 훈련 데이터와 모델 크기가 베틀라글과 같은 상황에서 우리 단일모델은 1.1팀과 2.0팀의 F1 성적이 각각 94.6%, 88.7%였다.우리는 또한 OntoNotes 공지 해소 임무(79.6%F1)의 최신 발전을 실현했고 TACRED 관계 추출 기준 테스트에서 뛰어난 활약을 보였으며 심지어 GLUE에서도 향상되었다.1', 'de': 'Wir präsentieren SpanBERT, eine Vortrainingsmethode, die entwickelt wurde, um Textspannen besser darzustellen und vorherzusagen. Unser Ansatz erweitert BERT durch (1) Maskieren zusammenhängender zufälliger Spannen anstelle von zufälligen Token und (2) Training der Spannengrenzrepräsentationen, um den gesamten Inhalt der maskierten Spanne vorherzusagen, ohne sich auf die einzelnen Token-Repräsentationen in ihr zu verlassen. SpanBERT übertrifft BERT und unsere besser abgestimmten Baselines konsequent und bietet erhebliche Verbesserungen bei den Spannenauswahlaufgaben wie der Beantwortung von Fragen und der Coreferenzlösung. Insbesondere mit den gleichen Trainingsdaten und Modellgrößen wie BERTlarge erreicht unser Einzelmodell 94,6% und 88,7% F1 auf SQuAD 1.1 bzw. 2.0. Wir erreichen auch einen neuen Stand der Technik bei der Kernauflösung von OntoNotes (79,6% F1), eine starke Leistung beim Benchmark für die Beziehungsextraktion von TACRED und sogar Gewinne bei GLUE. 1', 'sw': 'Tunawasilisha SpanBERT, njia ya mafunzo ya awali inayolengwa kuwakilisha vizuri na kutabiri nyanja za maandishi. Hatua yetu inaeneza BERT kwa (1) kuvaa siku za uso wenye uchungu, badala ya ishara za ajabu, na (2) kuwafundisha wakiwakilisha mipaka ya pande kutabiri maudhui yote ya spania hiyo, bila kutegemea maoni ya watu binafsi ya alama ndani yake. SpanBERT inafanya vizuri zaidi BERT na misingi yetu yenye mafanikio makubwa katika kazi za uchaguzi wa spania kama vile majibu na suluhisho la msingi. Hasa, kwa takwimu hizo za mafunzo na ukubwa wa mifano kama BERTlarge, mtindo wetu mmoja una asilimia 94.6 na 88.7% F1 kwenye SQuAD 1.1 na 2.0. Kadhalika tunapata hali mpya ya sanaa kwenye jukumu la maamuzi ya OntoNotes (79.6% F1), utendaji wa nguvu kwenye bendera ya utekelezaji wa TACRED, na hata kupata mafanikio kwenye GLUE. 1', 'tr': "Biz SpanBERT'i, metin alanlaryny gowy temsil etmek we öňünden geçirmek üçin tasarlanýan öňünden öňünden eğitim yöntemi görkeýäris. Bizim yaklaşımız BERT'i (1) tesadüf noktalar maskelendirmek yerine, tesadüf işaretleri yerine, ve (2) boyutlu sınır temsillerini maskeli gezinin bütün mahallini tahmin etmek için kullanarak uzaklaştırır. SpanBERT BERT we biziň gowy düzümlenmiş baz çyzgymlarymyzdan hemişe BERT'i üstüne getirir. Ýüklemek we seçmelik çözümleri ýaly çözümleme meselesinde örän wajyp gazanýar. Aýratyn bolsa BERTlarge bilen meňzeş bilim maglumaty we nusgasy bilen, biziň ýeke nusgasymyz 94.6% we 88.7% F1 SQuAD 1.1 we 2.0 diýip bolýar. Biz hem OntoNotes coreference çözümleriniň täze bir durumyny tapdyk (79.6% F1), TACRED ilişki çekişme salgysynda güýçli bir şekilde başarýarys we hatta GLUE üstüne gazanýarys. 1", 'af': "Ons stel SpanBERT voor 'n voor-oefening metode wat ontwerp is om beter te stel en voorskou spans van teks te voorskou. Ons toegang verleng BERT deur (1) die maskering van gemeenskaplike willekeurige spans, eerder as willekeurige tokens, en (2) die span grense voorstellings om die hele inhoud van die maskerde spans te voorskou, sonder om op die individuele token voorstellings binne te vertrou. SpanBERT konsistentlik uitvoer BERT en ons beter-tuned basis lyne, met substantiele verskaffings op span-keuse opdragte soos vraag antwoord en koreferensie oplossing. Spesifieke, met dieselfde onderwerking data en model grootte as BERTlarge, ons enkele model kry 94. 6% en 88. 7% F1 respectively op SQuAD 1. 1 en 2. 0. Ons het ook 'n nuwe staat van die kuns bereik op die OntoNotes koreferensie oplossing taak (79.6% F1), sterk prestasie op die TACRED verhouding van uittrekking benchmark, en selfs verkry op GLUE. 1", 'sq': 'We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text.  Përqasja jonë zgjeron BERT me (1) maskimin e zgjatjeve të rastësishme të përbashkëta, në vend të shenjave të rastësishme, dhe (2) trajnimin e përfaqësimeve të kufirit të zgjatjes për të parashikuar të gjithë përmbajtjen e zgjatjes së maskuar, pa mbështetur në përfaqësimet individuale të shenjave brenda saj. SpanBERT kalon vazhdimisht BERT dhe linjat bazë tona më të përshtatura, me fitime thelbësore në detyrat e zgjedhjes së fushës të tilla si përgjigjet e pyetjeve dhe zgjidhja e korreferencës. Në veçanti, me të njëjtat të dhëna treinimi dhe madhësinë e modelit si BERTlarge, modeli ynë i vetëm merr 94.6% dhe 88.7% F1 respektivisht në SQuAD 1.1 dhe 2.0. Ne arrijmë gjithashtu një gjendje të re të teknologjisë në detyrën e zgjidhjes së korreferencës OntoNotes (79.6% F1), performancë të fortë në standartin e nxjerrjes së marrëdhënieve TACRED dhe madje fitime në GLUE. 1', 'hy': 'Մենք ներկայացնում ենք Սպանբերթը, նախավարժման մեթոդը, որը ստեղծված է ավելի լավ ներկայացնելու և կանխատեսելու տեքստի տարածքները: Մեր մոտեցումը ընդլայնում է BER-ը (1) ծածկելով կողմնակի պատահական տարածքներ, ոչ թե պատահական նշաններ, և (2) վարժեցնելով տարածքների սահմանների ներկայացումները, որպեսզի կանխատեսենք ծածկված տարածքի ամբողջ բովանդակությունը, առանց հույս դնելու անհատական նշանների ներկայ ԻսպանԲերթը մշտապես գերազանցում է ԲԵՌԹը և մեր ավելի լավ կազմակերպված հիմնական գծերը, որն ունի նշանակալի շահույթ տարածքի ընտրության խնդիրների վրա, ինչպիսիք են հարցերի պատասխանը և հարցերի լուծումը: Մասնավորապես, BER-ի նման ուսուցման տվյալներով և մոդելի չափերով մեր միակ մոդելը ստանում է 94.6 և 88.7 տոկոս F1-ը SQUAD1-ի 1.1 և 2.0-ի միջոցով: Մենք նաև հասնում ենք նոր տեխնոլոգիայի վերաբերյալ Օնտոնոտեսի կորֆերենսի լուծումների խնդիրը (79.6 տոկոս F1), ուժեղ արտադրողականությունը ԹԱՔԵԴ հարաբերությունների վերաբերյալ վերաբերյալ և նույնիսկ GLUE-ի շահույթները: 1', 'am': 'የጽሑፉን ደቂቃዎች ለመመልከት እና ለመፍጠር የሚደረገውን ስፓንቢርቴን አቀረብን፡፡ የአካባቢው ግንኙነታችን BERT (1) በአካባቢ ምልክቶችን ከመጠቀም ይልቅ አካባቢ እና (2) የደረጃ ግንኙነቶችን በመጠቀም እና በዙሪያው በአካባቢው ግንኙነት ላይ ሳይታመኑ የደረጃ ግንኙነቶችን ለመጠቀም ያስተምራል፡፡ ስፓንባርቲ በጥያቄ መልስ እና የክፍለ ትርጉም ትምህርት በሚመስል የስፓን ምርጫ ስራዎችን በጥያቄ የBERT እና የበለጠ መሠረት ማድረግ ይደረጋል፡፡ In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively.  በኦንቶዎት የቆርጠኛ አዋጅ ትርጉም አድራሻ (79.6 በመቶ F1) ላይ አዲስ የዐርድ ጥናት እናደርጋለን፤ TACRED ግንኙነት የውጤት ገጽ እና በGLUE ላይ ትክክለኛ ነው፡፡ 1', 'bn': 'আমরা স্প্যান্বারেটের উপস্থাপন করছি, একটি প্রশিক্ষণের পূর্ববর্তী পদ্ধতি, যা লেখার প্রতিনিধিত্ব ও ভবিষ্যতের জন্য পরিকল্পন আমাদের প্রতিযোগিতা বিবেরেটিকে (১) দ্বারা পরিস্থিতির ক্ষেত্রে ক্ষতিগ্রস্ত অঙ্গীকারের পরিবর্তে মুখোশ প্রকাশ করে দিয়েছে, আর (২) মুখোশিত স্পেনের পুরো বিষ স্প্যান্বেরেট বিবের্ট এবং আমাদের ভালো সংক্রান্ত বেসারেন্টের বাইরে প্রকাশ করেছে, যেমন প্রশ্নের উত্তর এবং কোরেফেন্সের সিদ্ধান্তের মত বিশেষ করে, বের্টিব্যাপী একই প্রশিক্ষণের তথ্য এবং মডেল আকারের মাধ্যমে আমাদের একক মডেল ৯৪. ৬% এবং ৮৮. ৭% এফ১ স্কোয়াড ১. ১ এবং ২. এছাড়াও আমরা অনটনোটের কোরাফেন্স সিদ্ধান্তের কাজের নতুন পরিস্থিতি অর্জন করি (৭৯. ৬% এফ১), টাক্রিডি সম্পর্ক বেছে নেওয়ার ব্যাঙ্কমের উপর শক্তিশালী  ১', 'az': 'Biz SpanBERT\'i, daha yaxşı təhsil etmək və məktub müəyyən etmək üçün hazırlanmış bir təhsil metodu göstəririk. Bizim yaxınlığımız BERT (1) ilə müxtəlif müəyyən müəyyən müəyyən müəyyən müəyyən müəyyən müəyyən müəyyən müəyyən müddətlərlə, müəyyən müəyyən müəyyən müəyyən müəyyən etmək üçün müəyyən müəyyən müəyyən müəyyən müəyyən edir. SpanBERT BERT və bizim daha yaxşı fikirləşdirilmiş baz çətinliklərimizi sürəkləndirir, böyük səviyyə seçilmiş işlərdə sual cavab vermək və mərhəmət çözümü kimi böyük qənimətlər vardır. Özellikle, BERTlarge kimi aynı təhsil məlumatları və modellərin böyüklüyü ilə, tək modellərimiz SQuAD 1.1 və 2.0 üzerində 94.6% və 88.7% F1 alır. Biz də OntoNotes\'in coreference çözünürlük işlərinin yeni bir məlumatını başa çatdıq (79.6% F1), TACRED ilişkilərin əlaqəsi çıxartma benchmark ı və hətta GLUE əlaqəsində qələbə çatdıq. 1" (msgctxt: "panel:showusername") to "1', 'bs': 'Predstavljamo SpanBERT, metodu predobuke koji je dizajniran za bolji predstavljanje i predviđanje prostora teksta. Naš pristup proširi BERT (1) maskiranjem zajedničkih nasumičnih prostora, umjesto nasumičnih znakova, i (2) obučavanjem graničnih predstavljanja da predvidimo cijeli sadržaj maskiranog prostora, bez oslanjanja na pojedinačne predstave znakova unutar njega. SpanBERT konsekventno iznosi BERT i našu bolje prilagođenu osnovnu liniju, sa značajnim dobicama na zadatke izbora razdoblja poput odgovora na pitanja i rješavanja pristojnosti. Posebno, sa istim podacima o obuci i veličini modela kao BERTlarge, naš jedini model dobija 94,6% i 88,7% F1 na SQuAD 1,1 i 2,0. Također postignemo novo stanje umjetnosti o zadatku rezolucije liječnosti OntoNotes (79.6% F1), jakoj učinkovitosti na TACRED povezanoj mjeri izvlačenja odnosa, a čak i dobiti GLUE. 1', 'et': "Tutvustame SpanBERT-i, koolituseelset meetodit, mille eesmärk on paremini esindada ja ennustada teksti ulatusi. Meie lähenemisviis laiendab BERT-i (1) maskeerides juhuslike vahemike, mitte juhuslike märkide, ja (2) treenides ulatuse piiride esitusi, et ennustada kogu maskeeritud vahemiku sisu, toetumata selles sisalduvatele individuaalsetele märkide esitustele. SpanBERT jõuab pidevalt üle BERT-i ja meie paremini häälestatud lähtejooned, saavutades märkimisväärset kasu ulatuse valimise ülesannetes, nagu küsimustele vastamine ja ühise viite lahendamine. Täpsemalt, samade treeningandmete ja mudeli suurusega nagu BERTlarge, saavutab meie üksikmudel SQuAD 1.1 ja 2.0 puhul vastavalt 94,6% ja 88,7% F1. Samuti saavutame uue tehnika taseme OntoNotes'i kortereferentsi lahendamise ülesande osas (79,6% F1), tugeva tulemuse TACRED-suhte ekstraheerimise võrdlusalusel ja isegi GLUE-i kasu. 1", 'ca': "Presentem SpanBERT, un mètode de pré-entrenament dissenyat per representar i predir millor l'espai de text. El nostre enfocament s'estendrà a BERT amb (1) mascarant les extensions aleatòries contigues, en comptes de les fitxes aleatòries, i (2) capacitant les representacions de les fronteres de l'interval per predir tot el contingut de l'interval mascarat, sense confiar en les representacions individuals de les fitxes dins ell. SpanBERT supera constantment el BERT i les nostres línies de base millor ajustades, amb avanços substancials en tasques de selecció d'espai com la resposta a preguntes i la resolució de coreferencia. En particular, amb les mateixes dades d'entrenament i la mateixa mida de model que BERTlarge, el nostre model obté el 94,6% i el 88,7% F1 en SQuAD 1,1 i 2,0 respectivament. També aconsegueixem un nou avançat en la tasca de resolució de coreferència OntoNotes (79,6% F1), un fort rendiment en el punt de referència de l'extracció de relacions TACRED, i fins i tot guanyem en GLUE. 1", 'cs': 'Představujeme SpanBERT, metodu předškolení, která je navržena tak, aby lépe reprezentovala a předpovídala rozpětí textu. Náš přístup rozšiřuje BERT (1) maskováním souvisejících náhodných rozpětí namísto náhodných tokenů a (2) trénováním reprezentací hranic rozpětí tak, aby předpovídala celý obsah maskovaného rozpětí, aniž by se spoléhala na jednotlivé reprezentace tokenů uvnitř něj. SpanBERT konzistentně překonává BERT a naše lépe vyladěné základní linie, s výrazným ziskem v oblasti výběru rozsahu, jako je zodpovězení otázek a řešení koreferencí. Zejména se stejnými tréninkovými údaji a velikostí modelu jako BERTlarge získává náš jediný model 94,6% a 88,7% F1 na SQuAD 1.1 a 2.0. Dosahujeme také nového stavu techniky v oblasti úlohy rozlišení koreference OntoNotes (79,6% F1), silného výkonu v referenčním měřítku extrakce vztahů TACRED a dokonce zisků na GLUE. 1', 'fi': 'Esittelemme SpanBERT-menetelmän, joka on suunniteltu esittämään ja ennustamaan paremmin tekstin välejä. Lähestymistapamme laajentaa BERT:tä (1) peittämällä vierekkäisiä satunnaisia välejä satunnaisten polettien sijaan ja (2) kouluttamalla obsessirajaesityksiä ennustamaan koko maskitun spanin sisältöä luottamatta sen yksittäisiin tunnuksiin. SpanBERT suoriutuu jatkuvasti BERTin ja entistä paremmin viritetyn lähtölinjamme paremmin ja saavuttaa merkittäviä hyötyjä kattavuuden valintatehtävissä, kuten kysymyksiin vastaamisessa ja yhteisferenssin ratkaisussa. Erityisesti BERTlarge:n harjoitustietojen ja mallikoon ansiosta yksimallimme saa SQuAD 1.1:lla 94,6% ja 88,7% F1 2.0:lla. Saavutamme myös uuden tekniikan tason OntoNotes-koreferenssin ratkaisutehtävässä (79,6% F1), vahvan suorituskyvyn TACRED-suhdeuuttamisen vertailuarvossa ja jopa voitot GLUE:sta. 1', 'jv': '@item:inbox Awak dhéwé SpanBERT isih ngupakan ono nggawe BERT karo akeh basa sing luwih dumadhi, karo iso nggawe barang langkung sane ono nggawe gerasane nggawe gerasane kang dianggap karo Resolutioning Jucah-Jucah, karo nganggo langgambar kuwi data lan model kuwi BERT-luwih, model sing gawe lan alah-luwih kuwi Awak dhéwé éntuk sistem sing dibutuhke perusahaan kanggo nggawe gerakan kanggo nggawe Resolusi OntNotis 1', 'he': 'אנו מציגים את ספנברט, שיטת אימון ראשונה שמתכננת לייצג טוב יותר ולחזות אורך הטקסט. הגישה שלנו מארחת את BERT על ידי (1) מסיכת מרווחים אקראיים משותפים, במקום סימנים אקראיים, ו (2) מאמן את מייצגי הגבול של הרווחים כדי לחזות את כל התוכן של הרווחים המסויים, מבלי לסמוך על מייצגי סימנים בודדים בתוכו. ספנברט עולה על ביצועי BERT ועל קווי הבסיס הטובים ביותר שלנו, עם הרווחים משמעותיים על משימות הבחירה במרחק כגון תשובת שאלות ופתרון התאמה. במיוחד, עם אותו מידע אימון וגודל כמו BERTlarge, המודל היחיד שלנו מקבל 94.6% ו-88.7% F1 על SQuAD 1.1 ו-2.0 בהתאם. אנחנו גם מגיעים למצב חדש של האמנות על משימת הפתרון קופורנס אונטו נוטס (79.6% F1), ביצועים חזקים על נקודת הרישום של יחסי TACRED, ואפילו רווחים על GLUE. 1', 'sk': 'Predstavljamo vam SpanBERT, metodo pred usposabljanjem, ki je zasnovana za boljše predstavljanje in napovedovanje razponov besedila. Naš pristop razširja BERT tako, da (1) prikriva neprekinjene naključne razpone namesto naključnih žetonov, in (2) usposablja predstavitve meje razpona za napovedovanje celotne vsebine maskiranega razpona, ne da bi se zanašali na posamezne predstavitve žetonov znotraj njega. SpanBERT dosledno presega BERT in naše bolje prilagojene osnovne linije, z bistvenimi koristmi pri opravilih izbire obsega, kot sta odgovarjanje na vprašanja in reševanje skupnih referenc. Zlasti z enakimi podatki o usposabljanju in velikostjo modela kot BERTlarge, naš enojni model pridobi 94,6% F1 na SQuAD 1.1 oziroma 88,7% F1. Dosežemo tudi novo stanje tehnike pri nalogi ločevanja korerference OntoNotes (79,6% F1), močno uspešnost pri referenčni vrednosti za ekstrakcijo relacij TACRED in celo dobiček pri GLUE. 1', 'ha': 'Tuna halatar da spanBERT, wata metode ta gabã ɗaya wadda aka design ta fi zama mai kyau ga wakiyyar da kuma ke gabatar da spannin matsayin. TsarakanMu na shimfiɗa BERT da (1) mai rufe ranar kwanan da aka haife shi a ranar, kuma da (2) mai amfani da alamomi na spani dõmin ya yi bayani ga duk tsarin da aka rufe shi, kuma bã ya dõgara a kan wasu masĩfa guda. KCharselect unicode block name A cikin ƙayyade, da data da girmar tsari da girmar motsi kamar BERTbabba, shirin ayuka guda yana sãmu shekara 94.6% da 88.7% F1 na SQuAD 1.1 da 2.0. Tuna sami wani halin sanar da aka samu a kan aikin Core-Rawalt na OntoNotes (79.6% F1), mai ƙarfi a kan fanel na TACRED da dangijini na tsari, kuma kõ dã yana sami da GLUU. 1', 'bo': 'ང་ཚོས་SpanBERT(SpanBERT)ཡི་སྔོན་གྲངས་སྒྲིག་ཐབས་ལམ་ཞིག་བྲིས་མིན་པའི་ཡིག་གེ་ཚིག་གི་སྟོང་རྣམས་དང་སྔོན་གྲངས Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question response and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. ང་ཚོས་OntoNotes་ལ་གཟུགས་བརྟན་བཀོལ་སྤྱོད་ཀྱི་གནས་སྟངས་གསར་བ་ཞིག་གི་ཐོག་ལས་མཐུན་ཐུབ་པ་ཡིན། 1'}
{'en': 'A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation', 'ar': 'نموذج ما قبل التدريب المعزز بالمعرفة لتوليد قصة المشاع', 'fr': "Un modèle de préformation amélioré par les connaissances pour la génération d'histoires de bon sens", 'pt': 'Um modelo de pré-treinamento aprimorado pelo conhecimento para geração de histórias de senso comum', 'es': 'Un modelo de preentrenamiento mejorado con conocimientos para la generación de historias con sentido común', 'ja': '常識的なストーリー生成のための知識強化された事前トレーニングモデル', 'zh': '常识性故事增强型预训模形', 'hi': 'कॉमनसेंस स्टोरी जेनरेशन के लिए एक ज्ञान-संवर्धित प्रीट्रेनिंग मॉडल', 'ru': 'Модель предварительного обучения, основанная на знаниях, для генерации истории общего смысла', 'ga': 'Múnla Réamhoiliúint Feabhsaithe ar an Eolas do Ghiniúint Scéalta Commonsense', 'ka': 'Name', 'el': 'Ένα βελτιωμένο στη γνώση μοντέλο προετοιμασίας για τη δημιουργία ιστοριών κοινής λογικής', 'hu': 'A tudásbővített színlelési modell a Commonsense Story Generation számára', 'it': 'Un modello di preformazione avanzato della conoscenza per la generazione di storie commonsense', 'kk': 'Commonsense оқиғаны құру үшін білім- жетілдірілген претейнер үлгісіName', 'mk': 'Модел на преправање за генерација на заеднички приказни засилен од знаење', 'lt': 'Bendros istorijos sukūrimo žiniomis pagrįstas išankstinio mokymo modelis', 'ms': 'A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation', 'ml': 'കമോണ്\u200dസണ്\u200dസെന്\u200dസ് സ്റ്റോറി ജനിപ്പിക്കുന്നതിനുള്ള വിജ്ഞാനം മെഡോള്\u200d', 'mt': 'Mudell ta’ Tħarriġ minn Qabel imtejjeb bl-Għarfien għall-Ġenerazzjoni ta’ Stories Komuni', 'mn': 'Commonsense түүх бий болгох мэдлэг-нэмэгдүүлэгдэх загвар', 'no': 'Name', 'pl': 'Model wstępnego treningu oparty na wiedzy dla generowania opowieści', 'ro': 'Un model de prefacere îmbunătățit de cunoștințe pentru generarea poveștilor Commonsense', 'sr': 'Povećan model pretvaranja znanja za generaciju priče o Commonsense', 'si': 'Name', 'sv': 'En kunskapsförstärkt låtsas modell för Commonsense Story Generation', 'so': 'Qoraalka horumarinta aqoonta ee horumarinta taariikhda shirkadda', 'ta': 'ஒரு அறிவு- மேம்படுத்தப்பட்ட முன்பயிற்சி மாதிரி', 'ur': 'Name', 'uz': 'Name', 'vi': 'Một hình thức giả cường tri thức cho Thế Hệ Chuyện Bình thường', 'hr': 'Povećan model pretvaranja znanja za generaciju priče o Commonsense', 'da': 'En vidensforbedret foregivende model for Commonsense Story Generation', 'nl': 'Een kennisverbeterd voorbeeldmodel voor het genereren van Commonsense Story', 'bg': 'Модел за усъвършенстване на знанията за генериране на обикновени истории', 'id': 'Model Pretraining Enhanced Knowledge untuk Generasi Cerita Komunis', 'ko': '지식 강화의 상식 이야기 생성 예비 훈련 모델', 'de': 'Ein wissensbasiertes Vortrainingsmodell für die Generierung von Commonsense Story', 'fa': 'یک مدل تغییر زیاد دانش برای تولید داستان کمونس', 'sw': 'Mradi wa mafunzo ya Ujuzi kwa ajili ya Generation of Commons Stories', 'tr': 'Bir bilim-gelişmiş Pretraining Model for Commonsense Hikaye oluşturma', 'af': "'n Knowledge-Enhanced Pretraining Model vir Commonsense Story Generation", 'sq': 'Një model pretendimi i përmirësuar nga njohuritë për gjenerimin e historive të zakonshme', 'am': 'አቀማመጥ', 'hy': 'Գիտությունից բարելավված նախապատմությունների ստեղծման մոդելը', 'az': 'Commonsense hekay톛l톛ri M톛xluqat캼 칲칞칲n Bilim-Enhanced Pretraining Modeli', 'bn': 'কমিনসেন্স স্টোরি জেনারেশনের জন্য জ্ঞান-উন্নত প্রেট্রেনিং মডেল', 'bs': 'Povećan model pretvaranja znanja za generaciju priče o Commonsense', 'ca': 'Un model de pretensió millorat pel coneixement per a generar històries comunes', 'cs': 'Model předtréninku vylepšený znalostmi pro generování příběhů o zdravém smyslu', 'et': 'Teadmistele tugevdatud eelõpetamise mudel üldise mõistuse loomiseks', 'fi': 'Knowledge-Enhanced Esiharjoitusmalli Commonsense Story Generation', 'ha': '@ action', 'jv': 'Multiplikator ngregani soko akeh-ngregani soko urip nggawe Peringatan apakno Manual', 'sk': 'Model predusposabljanja z izboljšanim znanjem za ustvarjanje splošnih zgodb', 'bo': 'A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation', 'he': 'דוגמנית התאמנות משותפת במידע ליצירת סיפורים משותפים'}
{'en': 'Story generation, namely, generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant ', 'ar': 'يعد إنشاء القصة ، أي إنشاء قصة معقولة من سياق رائد ، مهمة مهمة ولكنها صعبة. على الرغم من النجاح في نمذجة الطلاقة والتماسك المحلي ، لا تزال نماذج توليد اللغة العصبية الحالية (على سبيل المثال ، GPT-2) تعاني من التكرار ، والصراعات المنطقية ، والافتقار إلى التماسك طويل المدى في القصص التي تم إنشاؤها. نعتقد أن هذا بسبب صعوبة ربط المعرفة المنطقية ذات الصلة ، وفهم العلاقات السببية ، وتخطيط الكيانات والأحداث بترتيب زمني مناسب. في هذه الورقة ، نبتكر نموذجًا للتدريب المسبق معززًا بالمعرفة لتوليد قصة منطقية. نقترح استخدام المعرفة المنطقية من قواعد المعرفة الخارجية لتوليد قصص معقولة. لمزيد من التقاط التبعيات السببية والزمنية بين الجمل في قصة معقولة ، نستخدم التعلم متعدد المهام ، والذي يجمع بين هدف تمييزي للتمييز بين القصص الحقيقية والمزيفة أثناء الضبط الدقيق. يُظهر التقييم الآلي واليدوي أن نموذجنا يمكن أن يولد قصصًا أكثر منطقية من خطوط الأساس الحديثة ، لا سيما من حيث المنطق والتماسك العالمي.', 'fr': "La génération d'une histoire, c'est-à-dire la génération d'une histoire raisonnable à partir d'un contexte de premier plan, est une tâche importante mais difficile. Malgré le succès de la modélisation de la fluidité et de la cohérence locale, les modèles de génération de langage neuronal existants (par exemple, GPT-2) souffrent toujours de répétitions, de conflits logiques et d'un manque de cohérence à long terme dans les histoires générées. Nous supposons que cela est dû à la difficulté d'associer des connaissances pertinentes de bon sens, de comprendre les relations de cause à effet et de planifier des entités et des événements dans un ordre temporel approprié. Dans cet article, nous concevons un modèle de pré-apprentissage amélioré par les connaissances pour la génération d'histoires de bon sens. Nous proposons d'utiliser les connaissances de bon sens issues de bases de connaissances externes pour générer des histoires raisonnables. Pour mieux saisir les dépendances causales et temporelles entre les phrases d'une histoire raisonnable, nous utilisons l'apprentissage multitâche, qui combine un objectif discriminant pour distinguer les vraies et les fausses histoires lors de la mise au point. L'évaluation automatique et manuelle montre que notre modèle peut générer des histoires plus raisonnables que les bases de référence de pointe, notamment en termes de logique et de cohérence globale.", 'pt': 'A geração de histórias, ou seja, gerar uma história razoável a partir de um contexto principal, é uma tarefa importante, mas desafiadora. Apesar do sucesso na modelagem de fluência e coerência local, os modelos de geração de linguagem neural existentes (por exemplo, GPT-2) ainda sofrem de repetição, conflitos lógicos e falta de coerência de longo alcance nas histórias geradas. Conjecturamos que isso se deve à dificuldade de associar conhecimento relevante do senso comum, entender as relações causais e planejar entidades e eventos com ordem temporal adequada. Neste artigo, desenvolvemos um modelo de pré-treinamento aprimorado pelo conhecimento para a geração de histórias de senso comum. Propomos utilizar o conhecimento de senso comum de bases de conhecimento externas para gerar histórias razoáveis. Para capturar ainda mais as dependências causais e temporais entre as sentenças em uma história razoável, usamos o aprendizado multitarefa, que combina um objetivo discriminativo para distinguir histórias verdadeiras e falsas durante o ajuste fino. A avaliação automática e manual mostra que nosso modelo pode gerar histórias mais razoáveis do que linhas de base de última geração, principalmente em termos de lógica e coerência global.', 'es': 'La generación de historias, es decir, generar una historia razonable a partir de un contexto principal, es una tarea importante pero desafiante. A pesar del éxito en el modelado de la fluidez y la coherencia local, los modelos de generación de lenguaje neuronal existentes (por ejemplo, GPT-2) todavía sufren de repetición, conflictos lógicos y falta de coherencia a largo plazo en las historias generadas. Conjeturamos que esto se debe a la dificultad de asociar el conocimiento de sentido común relevante, comprender las relaciones causales y planificar entidades y eventos con un orden temporal adecuado. En este artículo, diseñamos un modelo de preentrenamiento mejorado con conocimientos para la generación de historias con sentido común. Proponemos utilizar el conocimiento de sentido común de bases de conocimiento externas para generar historias razonables. Para capturar aún más las dependencias causales y temporales entre las oraciones de una historia razonable, utilizamos el aprendizaje multitarea, que combina un objetivo discriminativo para distinguir las historias verdaderas y falsas durante la puesta a punto. La evaluación automática y manual muestra que nuestro modelo puede generar historias más razonables que las líneas de base de última generación, particularmente en términos de lógica y coherencia global.', 'ja': 'ストーリーの生成、すなわちリーディングコンテキストから合理的なストーリーを生成することは、重要ですが、難しい課題です。 モデリングの流暢性及び局所的な一貫性において成功したにもかかわらず、既存のニューラル言語生成モデル（例えば、ＧＰＴ － ２ ）は、生成されたストーリーにおける繰り返し、論理対立、及び長距離の一貫性の欠如に依然として苦しむ。 これは、関連する常識的知識を関連付け、因果関係を理解し、エンティティやイベントを適切な時間的順序で計画することが困難であるためであると推測します。 本稿では、常識的なストーリー生成のための知識強化された事前トレーニングモデルを考案する。 外部の知識ベースからの常識的な知識を活用して、合理的なストーリーを生み出すことを提案します。 合理的なストーリーの文章間の因果関係と時間的依存関係をさらに捉えるために、私たちは微調整中に真実と偽のストーリーを区別するために差別的な目的を組み合わせたマルチタスク学習を使用します。 自動評価と手動評価は、特に論理とグローバルな一貫性の観点から、当社のモデルが最先端のベースラインよりも合理的なストーリーを生成できることを示しています。', 'zh': '故事成,先上下文生理,一重而有挑战性事也。 虽流利性局一致性建模成,而今神经语言成形(,GPT-2)犹重复,缉突成事无长距离一致性之患。 推此者,难以相关常识,解因果关系及规画实体,与时序相应也。 于本文中,设一知识增练模样,以成常识性事。 吾言以外知识库之常识生理者也。 句因果关系时依赖性,用多任务学问,以别真伪。 自与手动评,模形可以成先基线,特在逻辑全局一致性。', 'hi': 'कहानी पीढ़ी, अर्थात्, एक प्रमुख संदर्भ से एक उचित कहानी उत्पन्न करना, एक महत्वपूर्ण लेकिन चुनौतीपूर्ण कार्य है। मॉडलिंग प्रवाह और स्थानीय सुसंगतता में सफलता के बावजूद, मौजूदा तंत्रिका भाषा पीढ़ी मॉडल (उदाहरण के लिए, जीपीटी -2) अभी भी पुनरावृत्ति, तर्क संघर्ष, और उत्पन्न कहानियों में लंबी दूरी की सुसंगतता की कमी से पीड़ित हैं। हम अनुमान लगाते हैं कि यह प्रासंगिक कॉमनसेंस ज्ञान को जोड़ने, कारण संबंधों को समझने, और उचित अस्थायी आदेश के साथ संस्थाओं और घटनाओं की योजना बनाने की कठिनाई के कारण है। इस पेपर में, हम कॉमनसेंस कहानी पीढ़ी के लिए एक ज्ञान-संवर्धित प्रीट्रेनिंग मॉडल तैयार करते हैं। हम उचित कहानियों को उत्पन्न करने के लिए बाहरी ज्ञान के आधार से सामान्य ज्ञान ज्ञान का उपयोग करने का प्रस्ताव करते हैं। एक उचित कहानी में वाक्यों के बीच कारण और अस्थायी निर्भरताओं को आगे बढ़ाने के लिए, हम बहु-कार्य सीखने का उपयोग करते हैं, जो ठीक-ट्यूनिंग के दौरान सच्ची और नकली कहानियों को अलग करने के लिए एक भेदभावपूर्ण उद्देश्य को जोड़ता है। स्वचालित और मैनुअल मूल्यांकन से पता चलता है कि हमारा मॉडल अत्याधुनिक आधार रेखाओं की तुलना में अधिक उचित कहानियां उत्पन्न कर सकता है, विशेष रूप से तर्क और वैश्विक सुसंगतता के संदर्भ में।', 'ru': 'Создание истории, а именно создание разумной истории из ведущего контекста, является важной, но сложной задачей. Несмотря на успех в моделировании беглости и локальной согласованности, существующие модели генерации нейронного языка (например, GPT-2) все еще страдают от повторов, логических конфликтов и отсутствия согласованности на большом расстоянии в сгенерированных историях. Мы предполагаем, что это связано с трудностью связывания соответствующих здравомыслящих знаний, понимания причинно-следственных связей и планирования сущностей и событий с правильным временным порядком. В этой статье мы разрабатываем модель предварительного обучения, основанную на знаниях, для генерации сюжетов здравого смысла. Мы предлагаем использовать знания здравого смысла из внешних баз знаний для создания разумных историй. Чтобы дополнительно зафиксировать причинно-следственные и временные зависимости между предложениями в разумной истории, мы используем многозадачное обучение, которое сочетает в себе дискриминирующую цель различения истинных и фальшивых историй во время тонкой настройки. Автоматическая и ручная оценка показывает, что наша модель может генерировать более разумные истории, чем современные исходные данные, особенно с точки зрения логики и глобальной согласованности.', 'ga': 'Is tasc tábhachtach ach dúshlánach é scéalta a ghiniúint, eadhon, scéal réasúnta a ghiniúint ó chomhthéacs ceannasach. In ainneoin an ratha maidir le líofacht agus comhleanúnachas áitiúil a shamhaltú, tá samhlacha giniúna néaracha teanga atá ann cheana féin (m.sh., GPT-2) fós ag fulaingt ó athrá, coinbhleachtaí loighic, agus easpa comhleanúnachais fadraoin i scéalta ginte. Is dóigh linn gurb é an chúis atá leis seo ná an deacracht a bhaineann le heolas ábhartha chiallmhar a chomhcheangal, na caidrimh chúise a thuiscint, agus aonáin agus imeachtaí a phleanáil le hord cuí ama. Sa pháipéar seo, ceapaimid samhail réamhoiliúint fheabhsaithe le haghaidh scéalta ciallmhara a ghiniúint. Tá sé beartaithe againn eolas ciallmhar ó bhunachar eolais sheachtracha a úsáid chun scéalta réasúnacha a chruthú. Chun na spleáchais chúiseacha agus ama idir na habairtí i scéal réasúnta a ghabháil tuilleadh, úsáidimid foghlaim ilthasc, a chomhcheanglaíonn cuspóir idirdhealaitheach chun idirdhealú a dhéanamh idir scéalta fíor agus falsa le linn mionchoigeartaithe. Léiríonn meastóireacht uathoibríoch agus láimhe gur féidir lenár múnla scéalta níos réasúnta a ghiniúint ná na bunlínte nua-aimseartha, go háirithe i dtéarmaí loighic agus comhleanúnachas domhanda.', 'ka': 'ისტორიების წარმოდგენა, რომელიც აზრივი ისტორია, რომელიც წარმოდგენებული კონტექსტიდან, არის მნიშვნელოვანი, მაგრამ შესაძლებელი რაოდენობა. მოდელური ფუნქცია და ლოკალური კონსერენციის წარმატების შემდეგ, მსგავსი ნეიროლური ენერგიის მოდელები (მაგალითად, GPT-2) უკვე განახლებით, ლოგიური კონფლიქტებით და ძალიან განახლებელი კონსერენცი ჩვენ ვფიქრობთ, რომ ეს არის მნიშვნელოვანი საზოგადოებო მეცნიერების შესაძლებლობა, მიზეზი შესახებ და პლანეციის ინტერტიკების და მოვლენების შესაძლებლობის შესახებ. ჩვენ ამ დოკუნეში გავაკეთებთ ცნობიერების უფრო უფრო უფრო უფრო უფრო უფრო უფრო ცნობიერების მოდელს. ჩვენ მინდა გამოიყენოთ საზოგადოებო ცნობილების გამოყენება გარეშე ცნობილების ბაზებიდან, რომლებიც განსაკუთრებული ისტორიების შექმნა. უფრო მეტი მიზეზი და დრომალური დასარწმუნობების შესახებ წესების განსაკუთრებულ ისტორიაში, ჩვენ გამოყენებთ მრავალური დავასწავლობა, რომელიც დისკრიმინატიური მიზეზი, რომ განსაკუთრებოთ მართ ავტომატური და ხელსაწყოთა განსაზღვრება ჩვენი მოდელის შესაძლებელია უფრო პარამეტური ისტორიები, ვიდრე ხელსაწყოთა მხარეს, განსაკუთრებით ლოგიკის და გლობალური შესაძლებელობის', 'hu': 'A történet generálása, nevezetesen egy ésszerű történet generálása egy vezető kontextusból, fontos, de kihívást jelentő feladat. A folyékonyság és a helyi koherencia modellezésében elért sikerek ellenére a meglévő neurális nyelv generálási modellek (pl. GPT-2) még mindig ismétlődéstől, logikai konfliktusoktól és a generált történetek hosszú távú koherenciájának hiányától szenvednek. Feltételezzük, hogy ez azért van, mert nehéz a releváns közérzeti ismereteket összekapcsolni, megérteni az okozati összefüggéseket, valamint az entitásokat és eseményeket megfelelő időbeli renddel tervezni. Ebben a tanulmányban kidolgozunk egy tudásbővített előképzési modellt a közértelmes történetek generálására. Javasoljuk, hogy a külső tudásbázisokból származó közértelmes tudást használjuk fel ésszerű történetek létrehozására. Annak érdekében, hogy egy ésszerű történet mondatai közötti okozati és időbeli függőségeket tovább rögzítsük, többfeladatos tanulást használunk, amely egyesíti a diszkriminatív célkitűzést, hogy megkülönböztesse az igaz és a hamis történeteket finomhangolás során. Az automatikus és manuális értékelés azt mutatja, hogy modellünk ésszerűbb történeteket tud generálni, mint a legkorszerűbb alapvető értékek, különösen a logika és a globális koherencia tekintetében.', 'el': 'Η δημιουργία ιστοριών, δηλαδή η δημιουργία μιας λογικής ιστορίας από ένα ηγετικό πλαίσιο, είναι ένα σημαντικό αλλά προκλητικό έργο. Παρά την επιτυχία στη μοντελοποίηση της ρευστότητας και της τοπικής συνοχής, τα υπάρχοντα μοντέλα δημιουργίας νευρωνικών γλωσσών (π.χ., GPT-2) εξακολουθούν να υποφέρουν από επανάληψη, λογικές συγκρούσεις και έλλειψη μεγάλης εμβέλειας συνοχής στις δημιουργημένες ιστορίες. Υποθέτουμε ότι αυτό οφείλεται στη δυσκολία συσχέτισης της σχετικής γνώσης κοινής λογικής, της κατανόησης των αιτιακών σχέσεων και του σχεδιασμού οντοτήτων και γεγονότων με την κατάλληλη χρονική τάξη. Σε αυτή την εργασία, σχεδιάζουμε ένα μοντέλο προετοιμασίας για τη δημιουργία μιας κοινής λογικής ιστορίας. Προτείνουμε να χρησιμοποιήσουμε τη γνώση κοινής λογικής από εξωτερικές βάσεις γνώσης για να δημιουργήσουμε λογικές ιστορίες. Για να καταγράψουμε περαιτέρω τις αιτιώδεις και χρονικές εξαρτήσεις μεταξύ των προτάσεων σε μια λογική ιστορία, χρησιμοποιούμε εκμάθηση πολλαπλών εργασιών, η οποία συνδυάζει έναν διακριτικό στόχο για να διακρίνουμε αληθινές και ψεύτικες ιστορίες κατά τη διάρκεια του συντονισμού. Η αυτόματη και χειρωνακτική αξιολόγηση δείχνει ότι το μοντέλο μας μπορεί να δημιουργήσει πιο λογικές ιστορίες από ό,τι οι τελευταίες γραμμές βάσης, ιδίως όσον αφορά τη λογική και την παγκόσμια συνοχή.', 'it': "La generazione di storie, ovvero generare una storia ragionevole da un contesto di riferimento, è un compito importante ma impegnativo. Nonostante il successo nella modellazione della fluidità e della coerenza locale, i modelli esistenti di generazione del linguaggio neurale (ad esempio, GPT-2) soffrono ancora di ripetizione, conflitti logici e mancanza di coerenza a lungo raggio nelle storie generate. Presumiamo che ciò sia dovuto alla difficoltà di associare le conoscenze pertinenti di buon senso, comprendere le relazioni causali e pianificare entità ed eventi con un ordine temporale adeguato. In questo articolo, elaboriamo un modello di pre-formazione potenziato dalla conoscenza per la generazione di storie di buon senso. Proponiamo di utilizzare conoscenze di buon senso provenienti da basi di conoscenza esterne per generare storie ragionevoli. Per catturare ulteriormente le dipendenze causali e temporali tra le frasi in una storia ragionevole, utilizziamo l'apprendimento multi-task, che combina un obiettivo discriminatorio per distinguere storie vere e false durante la messa a punto. La valutazione automatica e manuale dimostra che il nostro modello può generare storie più ragionevoli rispetto a linee di base all'avanguardia, in particolare in termini di logica e coerenza globale.", 'kk': 'Журналды құру, мысалы, бастапқы контекстің оқиғасын құру - маңызды, бірақ әсер ету тапсырмасы. Модельдің жылдамдылығын және жергілікті теңдеу үшін, барлық невралдық тілдерді құру үлгілері (мысалы, GPT-2) қайталау, логикалық конфликттері және құрылған оқиғаларда ұзын аумағындағы теңдеу жоқ. Біз ойлаймыз, бұл көпшілікті білімдерді біріктіру, себепті қатынастарды түсіндіру, жоспарлау жағдайларды және оқиғаларды дұрыс уақытша ретінде біріктіру қиындығының себе Бұл қағазда білім көпшілікті оқиғаларды құру үшін көпшілікті өзгерту үлгісін құрамыз. Біз сыртқы білім негізінен көпшілікті білімдерді қолдану үшін ұсынамыз. Сөздер арасындағы маңызды және уақытша тәуелсіздіктерді бірнеше тапсырмаларды оқыту үшін бірнеше тапсырмаларды қолданамыз. Бұл дұрыс және жалғыз оқиғаларды түзету үшін дискриминациялық мақсатты бірі Автоматты және қолмен бағалау үлгіміздің оқиғаларымыздың негізгі сызықтардан артық оқиғаларды, осылақ логикалық және жалпы сәйкестіктердің қасиеттерінде жасауға болады.', 'lt': 'Story generation, namely, generating reasonable story from a leading context, is an important but challenging task. Nepaisant sėkmingo modeliavimo lankstumo ir vietos nuoseklumo, esami nervinių kalbų kūrimo modeliai (pvz., GPT-2) vis dar kenčia nuo kartojimo, loginių konfliktų ir ilgalaikio nuoseklumo sukauptose istorijose trūkumo. Mes darome prielaidą, kad tai yra dėl sunkumų susieti atitinkamas bendras žinias, suprasti priežastinius santykius ir planavimo subjektus bei įvykius tinkama laiko tvarka. Šiame dokumente sukuriame žiniomis pagrįstą išankstinio mokymo model į bendros istorijos kartai. Siūlome panaudoti bendrąsias žinias iš išorės žinių bazių, kad būtų sukurtos pagrįstos istorijos. Siekiant dar labiau suvokti priežastinę ir laikiną priklausomybę tarp sakinių pagrįstoje istorijoje, mes naudojame daugiafunkcinį mokymąsi, kuris derina diskriminacinį tikslą atskirti tikrąsias ir suklastotas istorijas tobulinant. Automatinis ir rankinis vertinimas rodo, kad mūsų modelis gali sukurti protingesnes istorijas nei naujausios bazės, ypač logikos ir pasaulinės nuoseklumo požiūriu.', 'mk': 'Генерацијата на приказни, имено, генерирањето разумна приказна од водечки контекст, е важна, но предизвикувачка задача. И покрај успехот во моделирањето на течноста и локалната кохеренција, постојните модели на генерација на невровни јазици (на пример, ГПТ-2) сé уште страдаат од повторувања, логични конфликти и недостаток на долга кохеренција во генерираните приказни. Претпоставуваме дека ова е поради тешкотијата да се поврзува релевантното заедничко знаење, разбирање на причинните врски, и планирање на ентитетите и настаните со соодветен временски ред. Во овој весник, ние создаваме модел за претренирање засилен на знаење за генерација на заеднички приказни. Предложуваме да се искористи заедничко знаење од надворешни бази на знаење за да се генерираат разумни приказни. За понатамошно да ги фатиме причинните и временските зависности помеѓу речениците во разумна приказна, користиме мултизадачно учење, кое комбинира дискриминативна цел за разлика на вистинските и лажните приказни за време на финетизирање. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence.', 'ms': 'Story generation, namely, generating a reasonable story from a leading context, is an important but challenging task.  Walaupun kejayaan dalam pemodelan keseluruhan setempat dan keseluruhan setempat, model generasi bahasa saraf yang ada (cth., GPT-2) masih menderita berulang-ulang, konflik logik, dan kekurangan keseluruhan jangkauan panjang dalam cerita yang dijana. Kami menduga bahawa ini adalah kerana kesulitan untuk mempersekutukan pengetahuan umum yang relevan, memahami hubungan penyebab, dan entiti merancang dan peristiwa dengan perintah sementara yang betul. Dalam kertas ini, kita merancang model latihan yang diperbaiki oleh pengetahuan untuk generasi cerita umum. Kami cadangkan untuk menggunakan pengetahuan umum dari pangkalan pengetahuan luar untuk menghasilkan cerita yang masuk akal. Untuk menangkap lebih lanjut dependensi penyebab dan sementara antara kalimat dalam cerita yang masuk akal, kami menggunakan pembelajaran multi-tugas, yang menggabungkan objektif diskriminatif untuk membedakan cerita yang benar dan palsu semasa penyesuaian. Evaluasi automatik dan manual menunjukkan bahawa model kita boleh menghasilkan cerita yang lebih masuk akal daripada garis dasar state-of-the-art, terutama dalam terma logik dan kesesuaian global.', 'ml': 'നേതാവിന്റെ സംസ്ഥാനത്തില്\u200d നിന്ന് വിവേകമുള്ള കഥ ഉണ്ടാക്കുന്നത് ഒരു പ്രധാനപ്പെട്ടതാണ്, പക്ഷെ വിലാല്\u200dക വിജയത്തിന്റെയും പ്രാദേശികമായ സംഘത്തിന്റെയും സ്ഥാനത്തിന്റെയും മാതൃകയില്\u200d വിജയം വരുത്തിയാല്\u200d നിലവിലുള്ള ന്യൂറല്\u200d ഭാഷ തലമുറകള്\u200d (ഉദാഹരണമായ ജിപിടി- 2) പി നമുക്ക് തോന്നുന്നത് ഇത് കാരണമാണെന്നാണ് വിചാരിക്കുന്നത്, പ്രധാനപ്പെട്ട കാര്യത്തിന്റെ പരിജ്ഞാനത്തിന്റെ കാരണമാണ്, കാരണ ബന ഈ പത്രത്തില്\u200d, നമ്മള്\u200d ഒരു പരിജ്ഞാനത്തിന്റെ മെഡല്\u200d നിര്\u200dമ്മിക്കുന്നു. കമോണ്\u200dസണ്\u200dസെന്\u200dസ് തലമുറതലമുറയ്ക് പുറത്തുള്ള അറിവുകളുടെ അടിസ്ഥാനത്തുനിന്ന് കമ്പന്\u200dസണ്\u200dസിന്\u200dറെ അറിവ് ഉപയോഗിക്കാന്\u200d ഞങ്ങള്\u200d ആലോചിക്കുന്നു.  വാക്കുകളുടെ ഇടയില്\u200d കാരണവും സമയത്തില്\u200d ആശ്രയിക്കുന്നതും കൂടുതല്\u200d പിടികൂടാന്\u200d വേണ്ടി, ഒരു ന്യായമായ കഥയില്\u200d ഞങ്ങള്\u200d പല ജോലിയുടെ പഠനം ഉപയോഗിക്കുന്നു. അത് സത് സ്വയമായും കൈകാര്യം വിലാസവും കാണിക്കുന്നു നമ്മുടെ മോഡല്\u200d സ്ഥാനത്തേക്കാള്\u200d വ്യക്തമായ കഥകള്\u200d ഉണ്ടാക്കാന്\u200d കഴിയും, പ്രത്യേകിച്ച് ലോഗിക', 'mt': "Il-ġenerazzjoni ta’ storji, jiġifieri, il-ġenerazzjoni ta’ storja raġonevoli minn kuntest ewlieni, hija kompitu importanti iżda ta’ sfida. Minkejja s-suċċess fl-immudellar tal-fluwenza u l-koerenza lokali, il-mudelli e żistenti tal-ġenerazzjoni tal-lingwi newrali (pereżempju GPT-2) g ħadhom ibatu minn ripetizzjoni, kunflitti loġiċi, u nuqqas ta’ koerenza fuq medda twila ta’ distanzi fi storji ġġenerati. We conjecture that this is because of the difficulty of associating relevant commonsense knowledge, understanding the causal relationships, and planning entities and events with proper temporal order.  F’dan id-dokument, a ħna nħejju mudell ta’ taħriġ minn qabel imsaħħaħ bl-għarfien għall-ġenerazzjoni ta’ storji komuni. Aħna nipproponu li nużaw għarfien komuni minn bażijiet esterni ta' għarfien biex niġġeneraw storji raġonevoli. Biex inqabdu aktar id-dipendenzi kawżali u temporali bejn is-sentenzi fi storja raġonevoli, a ħna nużaw tagħlim multikompitu, li jgħaqqad għan diskriminatorju biex jiddistingwi l-istoriji veri u falsi waqt l-irfinar. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence.", 'mn': 'Энэ түүх үеийнхээ хувьд, түүхийг эхлээд ойлголттой түүхийг бий болгох нь чухал, гэхдээ шаардлагатай ажил юм. Шингэл, орон нутгийн нийгэмдлийг загварчлах амжилтыг хүртэл суурилсан мэдрэлийн хэл төрөлхтний загварууд (жишээ нь GPT-2) дахин давтах, логик зөрчилдөөн, урт хугацааны нийгэмдлийг бий болгосон түүхдээс үргэлж зовж байна. Бид үүнийг ихэвчлэн ойлголттой мэдлэгийг холбох, шалтгаан холбоотой байдлыг ойлгохын тулд хэцүү байдаг гэдгийг ойлгож байна. Энэ цаасан дээр бид мэдлэгтэй нэмэгдүүлсэн түүх үеийн загварыг төсөөлж байна. Бид гадаад мэдлэгтэй суурь дээр мэдлэгийг ашиглах боломжтой түүх бий болгох гэсэн санал байна. Үүний шалтгаан болон хугацааны хамааралтай байдлыг илүү ойлгохын тулд олон ажлын сургалтыг ашигладаг. Энэ нь жинхэнэ, хуурамч түүхийг тодорхойлдох зорилго юм. Бидний загвар нь урлагийн суурь шулуунаас илүү ойлгомжтой түүхийг гаргаж чадна. Ялангуяа логик болон ертөнцийн нийгмийн холбоотой.', 'no': 'Generering av historiar, som er det viktig, men vanskeleg oppgåve, er å laga ein raskt historie frå ein første kontekst. I tillegg til suksessen i modellering av fluktens og lokale koherens, eksisterande modeller for å generera neuralspråk (f.eks. GPT-2) blir fortsatt påg ått frå gjentakinga, logiske konflikt og manglar langområde koherens i genererte historiar. Vi gjer at dette er på grunn av vanskeligheten for å tilknytte relevant fellesskap kunnskap, forstå grunnleggjande forhold, og planlegging av einingar og hendingar med rett tidsstyrke. I denne papiren finn vi eit kunnskap-forbetra pretreningsmodell for generering av vanlege historiar. Vi foreslår å bruka vanleg kunnskap frå eksterne kunnskapsbaser for å laga raskt historiar. For å få fram det grunnleggjande og tidlegare avhengigheten mellom setningane i eit raskt historie, bruker vi fleire oppgåver læring, som kombinerer ein diskriminasjonalt mål for å distinere sanne og falske historiar under finning. Automatisk og manuelt evaluering viser at modellen vårt kan laga meir rasjonlege historiar enn kunstbaselinjer, særleg i uttrykket av logikk og globalt samsvar.', 'pl': 'Generowanie opowieści, a mianowicie generowanie rozsądnej historii z wiodącego kontekstu, jest ważnym, ale wymagającym zadaniem. Pomimo sukcesu w modelowaniu płynności i lokalnej spójności, istniejące modele generacji języków neuronowych (np. GPT-2) nadal cierpią z powodu powtarzania, konfliktów logicznych i braku spójności długoterminowej w generowanych opowieściach. Domyślamy się, że wynika to z trudności związanej z powiązaniem istotnej wiedzy zdrowego rozsądku, zrozumieniem związków przyczynowych oraz planowaniem jednostek i zdarzeń z odpowiednim porządkiem czasowym. W niniejszym artykule opracowujemy model wstępnego treningu oparty na wiedzy dla generowania opowieści zdrowego rozsądku. Proponujemy wykorzystanie zdrowego rozsądku wiedzy z zewnętrznych baz wiedzy do generowania rozsądnych historii. Aby dalej uchwycić zależności przyczynowe i czasowe między zdaniami w rozsądnej opowieści, stosujemy wielozadaniowe uczenie się, które łączy dyskryminacyjny cel, aby rozróżnić prawdziwe i fałszywe historie podczas dostrajania. Automatyczna i ręczna ocena pokazuje, że nasz model może generować bardziej rozsądne historie niż najnowocześniejsze linie bazowe, zwłaszcza pod względem logiki i globalnej spójności.', 'ro': 'Generarea poveștii, și anume generarea unei povești rezonabile dintr-un context principal, este o sarcină importantă, dar provocatoare. În ciuda succesului în modelarea fluenței și coerenței locale, modelele existente de generare a limbajului neural (de exemplu, GPT-2) suferă încă de repetiții, conflicte logice și lipsa coerenței pe termen lung în poveștile generate. Presupunem că acest lucru se datorează dificultății asocierii cunoștințelor relevante de bun simț, înțelegerii relațiilor cauzale și planificării entităților și evenimentelor cu ordinea temporală corespunzătoare. În această lucrare, concepem un model de pregătire îmbunătățit de cunoaștere pentru generarea poveștilor de bun simț. Propunem să utilizăm cunoștințe de bun simț din baze de cunoștințe externe pentru a genera povești rezonabile. Pentru a surprinde în continuare dependențele cauzale și temporale dintre propozițiile dintr-o poveste rezonabilă, folosim învățarea multi-task, care combină un obiectiv discriminatoriu pentru a distinge poveștile adevărate și false în timpul reglării fine. Evaluarea automată și manuală arată că modelul nostru poate genera povești mai rezonabile decât cele de bază de ultimă generație, în special în ceea ce privește logica și coerența globală.', 'sr': 'Generacija priče, protiv, stvaranje razumne priče iz vodećeg konteksta, je važan ali izazovni zadatak. Uprkos uspjehu modeliranja tečnosti i lokalne konsekvencije, postojeći modeli generacije neuralnih jezika (npr. GPT-2) još uvijek pati od ponavljanja, logičkih konflikta i nedostatka dugog raspona koherencije u proizvedenim pričama. Pretpostavljamo da je to zbog teškoće povezivanja relevantnih znanja zajedničkog smisla, razumevanja uzrokovanih odnosa, planiranja entitata i događaja sa odgovarajućim vremenskim redom. U ovom papiru, razmišljamo o povećanom znanju modelu pretrenja za generaciju priče o običnim smislama. Predlažemo da iskoristimo zajedničko znanje iz vanjskih baza znanja da bi stvorili razumne priče. Da bismo dalje uhvatili uzrokovane i privremene zavisnosti između rečenica u razumnoj priči, koristimo više zadataka učenje, koja kombinuje diskriminacijski cilj da razlikuje prave i lažne priče tijekom isprave. Automatska i ručna procjena pokazuje da naš model može stvoriti razumne priče od početnih linija umetnosti, posebno u smislu logike i globalne konsekvence.', 'sv': 'Story generation, nämligen att generera en rimlig berättelse från ett ledande sammanhang, är en viktig men utmanande uppgift. Trots framgångarna i modellering av flytande och lokal samstämmighet lider existerande neurala språkgenereringsmodeller (t.ex. GPT-2) fortfarande av repetition, logikkonflikter och brist på långsiktig samstämmighet i genererade berättelser. Vi antar att detta beror på svårigheten att associera relevant allmännyttig kunskap, förstå orsakssamband och planera entiteter och händelser med rätt tidsordning. I den här uppsatsen utformar vi en kunskapsförbättrad förberedelsemodell för generering av allmännyttiga berättelser. Vi föreslår att använda allmännyttig kunskap från externa kunskapsbaser för att generera rimliga berättelser. För att ytterligare fånga orsaks- och tidsberoendet mellan meningarna i en rimlig berättelse använder vi multi-task learning, som kombinerar ett diskriminerande mål för att skilja sanna och falska berättelser under finjustering. Automatisk och manuell utvärdering visar att vår modell kan generera mer rimliga berättelser än state-of-the-art baselines, särskilt när det gäller logik och global sammanhållning.', 'so': 'Qarniga sawiridda, kaas oo ah abuuridda sheeko wanaagsan oo ka imaanaya xaalada hogaamiya, waa muhiim laakiin waa dhibaatooyin. Inkastoo ay ku liibaantay sameynta suurtagalnimada iyo sameynta deegaanka, samooyinka qaranka afka neurada ah (e.g. GPT-2) weli waxay ka xanuunsadaan dib u celinta, muran-qalaf jimicsiga, iyo baahida wadajirka aad u dheer oo ku qoran sheekooyin. Waxaynu malaynaynaa in taasi sababtoo ah dhibaatooyin ay ku xiriiraan aqoonta xiriirka shirkadda, ay fahamaan xiriirka sababaha ah, iyo qorsheynta dhaqaalaha iyo dhacdooyinka oo ku habboon xilliga waqtiga ah. Warqadan, waxaynu qoraynaa model aqoonta kordhisan oo u bedelanaya dabool roob-soo-dhigista qarniga shirkadda. Waxaynu soo jeedaynaa in aan aqoonta shirkadda ka isticmaalno aasaaska aqoonta dibadda ah si aan u sameeyo sheekooyin caqli ah. Si loo sii qabsado sababta iyo waqtiga lagu xiriiro hadalka saxda ah, waxaynu isticmaalnaa waxbarashada shaqada badan, taasoo isku xiriira goal takoorista ah si loo kala soocaa warqada runta ah oo been ah xilliga hagaajinta. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence.', 'ta': 'ஒரு முன்னேற்ற சூழலிலிருந்து ஒரு சரியான கதையை உருவாக்குவது ஒரு முக்கியமான ஆனால் சவால் செய்யும் செயல் மாதிரி வெற்றி மற்றும் உள்ளூர் இணைப்பில் வெற்றியை உருவாக்கும் போதும், தற்போது இருக்கும் புதிய நிரல் மொழி உருவாக்கும் மாதிரிகள் (உதாரணமாக GPT-2) திரும நாங்கள் நினைக்கிறோம் இது தான் தெரியும் தொடர்பு அறிவு, காரணத்தின் உறவுகளை புரிந்து கொள்ள, மற்றும் திட்டமைப்பு பொருள்கள் மற இந்த காகிதத்தில், நாம் ஒரு அறிவு மேம்படுத்தப்பட்ட மாதிரியை உருவாக்குகிறோம். தொழில்நுட்பம் கதை உருவாக் வெளி அறிவு அடிப்படையிலிருந்து தொழில் அறிவை பயன்படுத்துவதற்கு நாம் பரிந்துரைக்கிறோம் வெளியே அறிவு அடி காரணங்கள் மற்றும் தற்காலிக சார்ந்த சார்ந்த சார்புகள் இடையே மேலும் பிடித்துக் கொள்வதற்கு, நாம் பல பணி கற்றுக்கொள்கிறோம், அது சரியான முறையில் உண்ம தானாகவே மற்றும் கைமுறை evaluation shows that our model can produce more reasonable stories than state-of-the-art baselines, especially logic and global coherence in terms.', 'ur': 'کہانیاں کی نسل، یعنی ایک منطقی کہانیاں پیدا کرنے والی، ایک نہایت اہم بات ہے لیکن مشکل کام ہے. موڈلینگ فلونٹی اور محلی تعلق کی موفقیت کے باعث موجود ہے، موجود نیورل زبان کی نسل نمونڈل (جی.پی.ٹ.-2) دوبارہ تکرار کرنے سے، لوجیک اختلاف سے، اور پیدا کئے ہوئے کہانیاں میں طویل مدت کی تعلق کمی ہے. ہم سمجھتے ہیں کہ یہ تعلق معلوم علم کے شریک کرنے کی مشکل کے باعث ہے، دلیل رابطہ سمجھنے کی، اور سائل کی تدبیر اور حادثہ کو مستقیم مدت کے ساتھ سمجھنے کے باعث ہے. اس کاغذ میں ہم نے ایک علم سے زیادہ زیادہ اچھی طرح کی مثال سنانے کی نسل کے لیے بنائی ہے ہم اس بات کی پیشنهاد کرتے ہیں کہ باہر علم کی بنیاد سے عام سمجھ کا علم استعمال کریں کہ منطقی کہانیاں پیدا کریں۔ اس لئے کہ کلمات کے درمیان منطقی داستان کے درمیان کیسائل اور موقت اعتباری کو اضافہ کرنے کے لئے ہم بہت سے کام کی تعلیم کے مطابق استعمال کرتے ہیں، جو حق اور جھوٹی کہانیاں کو اضافہ کرنے کے لئے مختلف کرتا ہے۔ آٹوٹی اور مہمانی ارزیابی دکھاتی ہے کہ ہمارا موڈل اس سے زیادہ منطقی کہانیاں پیدا کر سکتا ہے جو ان کی حالت کی بنیاس لینوں سے زیادہ منطقی ہیں، مخصوصاً logic اور global coherence کے مطابق۔', 'si': 'කතාවක් පරීක්ෂණයක්, කියන්නේ, ප්\u200dරධාන සම්බන්ධයක් නිර්මාණය කතාවක්, වැදගත් නමුත් ප්\u200dරශ්නයක්. නිර්මාණය සහ ස්ථානික භාෂාව පරීක්ෂණයේ සාර්ථක සංවිධානයේ සාර්ථක විශ්වාස කරනවා නමුත්, තියෙනවා නිර්මාණය භාෂාව පරීක්ෂණය (උදාහරණය,  අපි හිතන්නේ මේක ප්\u200dරශ්නයක් තියෙන්නේ සම්බන්ධ සාමාන්\u200dය දන්නවය සම්බන්ධ වෙන්න අමාරුය නිසා, කාරණාත්මක සම්බන්ධත මේ පත්තරේ අපි දන්නවන්න පුළුවන් විශ්වාස කතාවක් ලොකුවෙන් සාමාන්\u200dය කතාවක් විසින් ප්\u200dරතිකාර අපි සාමාන්\u200dය දැනගන්න ප්\u200dරයෝජනය කරන්න ප්\u200dරයෝජනය කරන්න ප්\u200dරයෝජනය කතාවක් නිර්මාණය කරන්න. විශේෂ කතාවක් අතර කාරණාත්මක සහ කාරණාත්මක විශේෂතාවක් අතර අල්ලගන්න, අපි ගොඩක් කාර්ය ඉගෙනගන්න පාවිච්චි කරනවා, ඒකෙන් විශේෂ ස්වයංක්\u200dරියාත්මක සහ පුද්ගලික විශ්ලේෂණය පෙන්වන්නේ අපේ මොඩල් එක්ක තරම් හොඳ කතාවක් නිර්මාණය කරන්න පුළුවන් වෙයි', 'uz': "Story generation, namely, generating a reasonable story from a leading context, is an important but challenging task.  @ info: status Biz buni o'ylaymiz, buni ma'lumot bilan bog'liq qiyin, sababli munosabatlarni o'rganish qiyin sababchi, ma'lumotlarni o'rganish, ma'lumotlar va hodisalarni yaxshi vaqt tartibi bilan boshlash qiyin. Bu qogʻozda, biz bir tajriba tajriba yaratish modelini tasavvur qilamiz. Biz tashqi ilmiy asosidagi taʼminot asosidan foydalanishni talab qilamiz. Bu o'zlari haqiqiqiy hikoyalarni yaratish uchun. Bir necha vazifa o'rganish uchun gapirarning eng sabablar va vaqt o'zgarishlariga qo'shish uchun biz bir vazifa o'rganishdan foydalanamiz, bu haqiqiqiy va fak hikoyalarni o'zgartirish uchun haqiqiqiylik va to'g'ri qismini ajratish uchun bir qanday qiymatni birlashtiradi. Avtomatik va qoʻlbola qiymatlari koʻrsatiladi, modelimizning holatdagi shaxsiy asosiy asboblardan juda yaxshi qiymatlar yaratish mumkin, hususan logik va dunyo birlashtirish darajada.", 'vi': 'Chương trình phát triển một câu chuyện hợp lý từ ngữ cảnh dẫn đầu là một nhiệm vụ quan trọng nhưng đầy thử thách. Mặc dù thành công trong việc tạo ra hỗn loạn và sự phù hợp địa phương, các mô hình đời truyền ngôn ngữ thần kinh tồn tại (v.d. GPT-2) vẫn còn phải chịu đựng nhiều lần, xung đột logic, và thiếu sự đồng bộ lâu dài trong các câu chuyện được tạo ra. Chúng tôi cho rằng đây là vì khó khăn liên quan đến hiểu biết thông thường, hiểu được các mối quan hệ hệ hệ hệ hệ hệ hệ hệ hệ hệ hệ, và các tổ chức lập trình và các sự kiện theo đúng trật tự thời gian. Trong bài báo này, chúng ta lập ra một mô hình nâng cao kiến thức cho các câu chuyện cổ điển. Chúng tôi đề nghị sử dụng kiến thức thông thường từ căn cứ kiến thức bên ngoài để tạo ra những câu chuyện hợp lý. Để nắm bắt thêm các quan hệ hệ hệ hệ hệ hệ sinh tử và thời gian giữa các câu truyện trong một câu chuyện hợp lý, chúng tôi sử dụng các bài học đa nhiệm vụ, kết hợp một mục tiêu lệch lạc để phân biệt các câu chuyện thật và giả dối. Đánh giá tự động và tay hướng dẫn cho thấy mô hình của chúng ta có thể tạo ra những câu chuyện hợp lý hơn những bản căn cứ hiện đại, đặc biệt về mặt logic và sự đồng bộ toàn cầu.', 'bg': 'Генерирането на история, а именно генерирането на разумна история от водещ контекст, е важна, но предизвикателна задача. Въпреки успеха в моделирането на плавността и локалната съгласуваност, съществуващите модели за генериране на невронни езици (напр. GPT-2) все още страдат от повторение, логически конфликти и липса на дългосрочна съгласуваност в генерираните истории. Предполагаме, че това се дължи на трудностите за свързване на съответните разумни знания, разбиране на причинно-следствените връзки и планиране на обекти и събития с подходящ времеви ред. В тази статия ние разработваме усъвършенстван на знанието модел за предобучение за генериране на обикновени истории. Предлагаме да се използват разумни знания от външни бази знания, за да се генерират разумни истории. За по-нататъшно улавяне на причинно-следствените и времевите зависимости между изреченията в разумна история, ние използваме многозадача обучение, което съчетава дискриминационна цел да разграничи истинските и фалшивите истории по време на фината настройка. Автоматичната и ръчна оценка показва, че нашият модел може да генерира по-разумни истории от най-съвременните базови линии, особено по отношение на логиката и глобалната съгласуваност.', 'hr': 'Generacija priče, protiv, stvaranje razumne priče iz vodećeg konteksta, je važan ali izazovni zadatak. Uprkos uspjehu modeliranja tekućin e i lokalne konsekvencije, postojeći modeli generacije neuralnog jezika (npr. GPT-2) još uvijek pati od ponavljanja, logičkih sukoba i nedostatka dugogodišnje saskaņonosti u proizvedenim pričama. Pretpostavljamo da je to zbog teškoće povezivanja relevantnih znanja zajedničkog smisla, razumijevanja uzrokovanih odnosa, planiranja entitata i događaja s odgovarajućim vremenskim redom. U ovom papiru, razmišljamo o povećanom znanju modelu pretkivanja za generaciju priče o običnim smislama. Predlažemo iskoristiti zajedničke znanje iz vanjskih baza znanja kako bi stvorili razumne priče. Da bismo dalje uhvatili uzrokovane i privremene zavisnosti između rečenica u razumnoj priči, koristimo više zadataka učenje, koja kombinira diskriminacijski cilj da razlikuje prave i lažne priče tijekom isprave. Automatska i ručna procjena pokazuje da naš model može proizvesti razumne priče nego početne linije stanja umjetnosti, posebno u smislu logike i globalne konsekvence.', 'nl': 'Verhalen genereren, namelijk het genereren van een redelijk verhaal vanuit een leidende context, is een belangrijke maar uitdagende taak. Ondanks het succes in het modelleren van vloeiendheid en lokale coherentie, hebben bestaande neurale taalgeneratiemodellen (bijv. GPT-2) nog steeds last van herhaling, logica conflicten en gebrek aan langeafstandscoherentie in gegenereerde verhalen. We veronderstellen dat dit te wijten is aan de moeilijkheid om relevante kennis van gezond verstand te associëren, de causale relaties te begrijpen en entiteiten en gebeurtenissen te plannen met de juiste tijdsorde. In dit artikel ontwikkelen we een kennisverbeterd pretraining model voor het genereren van gezonde verhalen. We stellen voor om gezonde kennis van externe kennisbases te gebruiken om redelijke verhalen te genereren. Om de oorzakelijke en tijdelijke afhankelijkheden tussen de zinnen verder vast te leggen in een redelijk verhaal, gebruiken we multitask learning, dat een discriminerend doel combineert om echte en nepverhalen te onderscheiden tijdens finetuning. Automatische en handmatige evaluatie toont aan dat ons model redelijkere verhalen kan genereren dan state-of-the-art baselines, vooral wat betreft logica en mondiale samenhang.', 'da': 'Story generation, nemlig at generere en fornuftig historie ud fra en førende kontekst, er en vigtig, men udfordrende opgave. På trods af succesen med modellering af flydende og lokal sammenhæng lider eksisterende neurale sprog generationsmodeller (f.eks. GPT-2) stadig af gentagelse, logikkonflikter og mangel på lang rækkevidde sammenhæng i genererede historier. Vi formoder, at dette skyldes vanskeligheden ved at associere relevant almindelig viden, forstå årsagsforholdene og planlægge enheder og begivenheder med ordentlig tidsmæssig orden. I denne artikel udtænker vi en vidensforbedret foruddannelsesmodel for generering af almindelige historier. Vi foreslår at udnytte almindelig viden fra eksterne vidensbaser til at generere rimelige historier. For yderligere at fange årsagsrelaterede og tidsmæssige afhængigheder mellem sætningerne i en rimelig historie bruger vi multi-task learning, som kombinerer et diskriminerende mål for at skelne sande og falske historier under finjustering. Automatisk og manuel evaluering viser, at vores model kan generere mere fornuftige historier end state-of-the-art baselines, især hvad angår logik og global sammenhæng.', 'id': 'Generasi cerita, yaitu, menghasilkan cerita yang masuk akal dari konteks yang memimpin, adalah tugas yang penting tapi menantang. Meskipun sukses dalam modeling fluency dan koerensi lokal, model generasi bahasa saraf yang ada (contohnya GPT-2) masih menderita dari ulangan, konflik logika, dan kekurangan koerensi jangkauan panjang dalam cerita yang dihasilkan. Kami menduga bahwa hal ini karena kesulitan untuk mempersekutukan pengetahuan umum relevan, memahami hubungan penyebab, dan entitas rencana dan peristiwa dengan urutan waktu yang tepat. Dalam kertas ini, kami membuat model pretraining yang diperbaiki oleh pengetahuan untuk generasi cerita umum. Kami mengusulkan untuk menggunakan pengetahuan umum dari dasar pengetahuan luar untuk menghasilkan cerita yang masuk akal. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we use multi-task learning, which combines a discriminative objective to distinguish true and fake stories during fine-tuning.  Evaluasi otomatis dan manual menunjukkan bahwa model kita dapat menghasilkan cerita yang lebih masuk akal daripada garis dasar state-of-the-art, terutama dalam terma logika dan koherensi global.', 'de': 'Story-Generierung, nämlich eine vernünftige Story aus einem führenden Kontext zu generieren, ist eine wichtige, aber herausfordernde Aufgabe. Trotz des Erfolgs bei der Modellierung von Fluenz und lokaler Kohärenz leiden bestehende neuronale Sprachgenerationsmodelle (z.B. GPT-2) immer noch unter Wiederholung, Logikkonflikten und fehlender Kohärenz in generierten Geschichten. Wir vermuten, dass dies auf die Schwierigkeit zurückzuführen ist, relevantes gesundes Wissen zu verknüpfen, die kausalen Zusammenhänge zu verstehen und Entitäten und Ereignisse mit der richtigen zeitlichen Ordnung zu planen. In diesem Beitrag entwickeln wir ein wissensgestütztes Vortrainingsmodell für die Generierung gesunder Geschichten. Wir schlagen vor, gesundes Wissen aus externen Wissensbasen zu nutzen, um vernünftige Geschichten zu generieren. Um die kausalen und zeitlichen Abhängigkeiten zwischen den Sätzen in einer vernünftigen Geschichte weiter zu erfassen, verwenden wir Multi-Task Learning, das ein diskriminierendes Ziel kombiniert, wahre und gefälschte Geschichten während der Feinabstimmung zu unterscheiden. Automatische und manuelle Auswertung zeigt, dass unser Modell sinnvollere Geschichten generieren kann als State-of-the-Art Baselines, insbesondere in Bezug auf Logik und globale Kohärenz.', 'fa': 'نسل داستان، یعنی، تولید یک داستان منطقی از یک محیط رهبری، یک کار مهم ولی سخت\u200cکننده است. با وجود موفقیت در مدل\u200cسازی آلودگی و هماهنگی محلی، مدل\u200cهای نسل زبان عصبی وجود دارد (مثال GPT-2) هنوز از تکرار، اختلاف منطقی و کمبود هماهنگی طولانی در داستان\u200cهای تولید می\u200cشوند. ما تصور می\u200cکنیم که این به خاطر مشکلی است که در ارتباط با علم معمولی مربوط به تعلق داشته باشد، درک رابطه\u200cهای دلایل، و برنامه\u200cبندی\u200cها و اتفاقات با دستور موقتی مناسب باشد. در این کاغذ، ما یک مدل تغییر تغییر از علم برای نسل داستان معمولی طراحی می کنیم. ما پیشنهاد می\u200cکنیم که از پایگاه دانش بیرون اطلاعات معمولی استفاده کنیم تا داستان\u200cهای منطقی را تولید کنیم. برای further capture the causal and temporal dependencies between the sentences in a reasonable story, we use multi task learning, which combines a discriminative objective to distinguish true and false stories during fine-tuning. ارزیابی خودکار و دستی نشان می دهد که مدل ما می تواند داستانهای منطقی بیشتر از خطوط اصلی هنر را تولید کند، مخصوصا در مورد منطقی و هماهنگی جهانی.', 'ko': '이야기 생성은 중요하지만 도전적인 임무, 즉 주도적인 언어 환경에서 합리적인 이야기를 생성하는 것이다.비록 유창성과 국부 연관성 모델링에 성공했지만 기존의 신경 언어 생성 모델(예를 들어 GPT-2)은 여전히 중복, 논리적 충돌과 생성된 이야기의 장기적인 연관성이 결여된 문제가 존재한다.관련 상식지식을 연계해 인과관계를 이해하고 적절한 시간 순서로 실체와 사건을 기획하기 어려웠기 때문으로 추정된다.본고에서 우리는 지식이 강화된 상식 이야기 생성 예훈련 모델을 설계했다.우리는 외부 지식 라이브러리의 상식 지식을 이용하여 합리적인 이야기를 만들 것을 건의합니다.합리적인 이야기에서 문장 간의 인과와 시간 의존 관계를 한층 더 포착하기 위해 우리는 다중 임무 학습을 사용한다. 이것은 하나의 구분 목표를 결합시켜 미세하게 조정하는 과정에서 진위를 구분한다.자동과 수동 평가에 따르면 우리의 모델은 가장 선진적인 기선보다 합리적인 이야기를 만들 수 있으며 특히 논리와 전 세계의 일치성에 있어 더욱 합리적인 이야기를 만들 수 있다.', 'sw': 'Kizazi cha hadithi, kiitwacho, kutengeneza hadithi sahihi kutoka katika muktadha unaoendelea, ni muhimu lakini ni changamoto. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories.  Tunafikiri kwamba hii ni kwa sababu ya ugumu wa kuunganisha maarifa yanayohusiana na umma, kuelewa mahusiano yanayosababisha, na kupanga vitu na matukio kwa amri sahihi ya muda. Katika gazeti hili, tunapanga muundo wa kutengeneza ufahamu unaoongezeka kwa ajili ya kizazi cha habari za umma. Tunazipendekeza kutumia maarifa ya umma kutoka kwenye msingi wa maarifa ya nje ili kutengeneza hadithi zenye maana. Ili kuongeza zaidi sababu na wakati unategemea kati ya hukumu katika hadithi sahihi, tunatumia elimu ya kazi nyingi, ambayo inaunganisha lengo la kibaguzi ili kutofautisha habari za kweli na bandia wakati wa kutangaza vizuri. Tathmini za kibinafsi na manufaa zinaonyesha kuwa muundo wetu unaweza kutengeneza hadithi zenye ufanisi zaidi ya misingi ya hali ya sanaa, hususani kwa uhusiano wa kimataifa na ulimwengu.', 'tr': 'Hizra d철wletleri, i흫.ed첵채n ba힊lang캇챌dan makul heka첵a 체retmek, m철h체m 첵철ne 챌철z체mli zadyr. N채hili derejede g 철rkezil첵채n we 첵erli birle힊igi 체챌in 체st체nlik gazanyp bolan neural dil d철redilmeleri (mesel창, GPT-2) 첵ene-de tekrarlanmakdan, lojik 챌aganlardan so흫ra we 체retilen heka첵alarda uzyn d체z체mle힊melik 첵ok. Munu흫 n채hili d체힊체n첵채n bilim bilen bagla힊mak kyn챌ylygyny흫 seb채bi, seb채pli bagla첵y힊laryny d체힊체nmek we bolup durmu힊lary we ge챌irmek kyn챌ylygyny흫 seb채bi. Bu kagyzda, umumy du첵gun heka첵a d철wletlerini흫 체챌in bilim gowurak nusgasyny tasarl첵arys. Da힊arydaky bilim 체ss체nden d체힊체njeli heka첵alary d철retmek 체챌in umumy bilgi ulanmagy teklip ed첵채ris. S철zler arasyndaky seb채pli we wagt ba휓lyklaryny d체힊체njeli heka첵ada da힊yrmak 체챌in, biz k철p-t채bli 철wrenm채ge ulan첵arys. Bu dogry we 첵al흫y힊 d체zg체n d체zg체n durmu힊yny tapawutlamak 체챌in bir diskrimin maksadyny birle힊첵채r. Otomatik we el de흫le힊meleri bizi흫 modelimiz sanat baselini흫 durumyndan has ukyply heka첵alary d철redip biljekdigini g철rkez첵채r, 첵철ne logik we d체n첵채d채ki birle힊meleri barada.', 'sq': 'Gjenerimi i historive, veçan ërisht, duke krijuar një histori të arsyeshme nga një kontekst kryesor, është një detyrë e rëndësishme por sfiduese. Pavarësisht nga suksesi në modelimin e fluencës dhe koherencës lokale, modelet ekzistuese të gjenerimit të gjuhës nervore (për shembull GPT-2) ende vuajnë nga përsëritjet, konfliktet logjike dhe mungesa e koherencës në distancë të gjatë në historitë e gjeneruara. Ne supozojmë se kjo është për shkak të vështirësisë së lidhjes së njohurive të përbashkëta të rëndësishme, kuptimit të marrëdhënieve shkakuese, dhe planifikimit të njësive dhe ngjarjeve me rendin e duhur të përkohshëm. Në këtë letër, ne krijojmë një model parastërvitjeje të përmirësuar nga njohuria për gjeneratën e historive të zakonshme. We propose to utilize commonsense knowledge from external knowledge bases to generate reasonable stories.  Për të kapur më tej varësitë e shkakut dhe të përkohshëm midis dënimeve në një histori të arsyeshme, ne përdorim mësimin me shumë detyra, që kombinon një objektiv diskriminues për të dalluar historitë e vërteta dhe të rreme gjatë rregullimit të mirë. Vlerësimi automatik dhe manual tregon se modeli ynë mund të gjenerojë histori më të arsyeshme sesa linjat bazë më të larta, veçanërisht në lidhje me logjikën dhe koherencën globale.', 'am': 'ታሪክ ትውልድ፣ በመጀመሪያው ታሪክ በመፍጠር የሚያስፈልገው ነገር ግን አዋቂ ነው፡፡ ምንም እንኳን የውሃት እና የአገር ግንኙነት ማቀናቀል ቢሆንም፣ የአሁኑን የነዌብ ቋንቋ ትውልድ (ለምሳሌ GPT-2) ምሳሌዎች (የGPT-2) የሚቀሰቀሱ፣ የግንኙነት ግጭት እና የረጅም ዘመን ግንኙነት የጎደለበት ታሪክ ውስጥ ነው፡፡ ይህ በአካባቢው እውቀት፣ የውይይት ግንኙነትን በማስተዋል፣ አካባቢዎች እና ሁኔታዎችን በመጠቀም በጊዜው ትክክል በማስተካከል ስርዓት ማቀናቀል ምክንያት አስቸጋሪ ነው ብለን እናስባለን፡፡ በዚህ ካላት፣ እውቀት የበለጠ የዝርዝር ትውልድ መፍጠርን እናደርጋለን፡፡ ከውጭ እውቀት መቀመጫዎች የውጭ እውቀት ማውቀትን ለመጠቀም እናስባለን፡፡ በጥያቄ ታሪክ እና በክፍለ ዘመቻ እና በጥያቄ ታሪክ መካከል የሚደገመውን ማስተማር ለመያዝ፣ እውነተኛውን እና ሐሰተኛ ታሪኮችን ለመለየት በብዙ ስራ ትምህርት እናስጠጋለን፡፡ አውቶማቲካዊ እና ገጽ ማስታወቂያው ሞዴልናችን የዓለማዊ ግንኙነት እና የዓለማዊ ብሔራዊ ግንኙነት ከሀገር ክፍል ይልቅ የሚበልጥ ታሪኮችን ማፍጠር ይችላል፡፡', 'af': "Geskiedenis, naamlik, die genereer van 'n redelike storie van 'n ledende konteks, is 'n belangrike maar uitgelykende taak. Alhoewel die sukses in die modellering van fluiditeit en plaaslike koherens, bestaande neural e taal generasie modele (bv. GPT-2) nog steeds lyk van herhaal, logiese konflikte en mislukking van lang-omvang koherens in genereerde stories. Ons verwerp dat dit is vanweë die moeilikheid van toesluiting van relevante gemeenskaplike kennis, verstaan die oorsaaklike verhoudings en planeer entiteite en gebeurtenis met regte tydelike volgorde. In hierdie papier het ons 'n kennis-verbeterde pretraining model vir gemeenskapste storie generasie ontwerp. Ons voorstel om gemeenskaplike kennis te gebruik van eksterne kennis bases om redelike stories te genereer. Om die oorsaaklike en tydelike afhanklikhede tussen die setinge in 'n redelike storie verder te vang, gebruik ons veelvuldige taak leer, wat kombinieer 'n diskriminasiewe doel om waar en falsies stories te verkies tydens fyn-tuning. Outomatiese en manuele evaluering vertoon dat ons model meer redelike stories kan genereer as staat-van-kuns basisline, spesiaal in terms van logiek en globaal koherens.", 'hy': 'Պատմությունների սերունդը, հատկապես, խելամիտ պատմություն ստեղծելը առաջնորդների կոնտեքստից, կարևոր, բայց դժվար խնդիր է: Չնայած ճկունության և տեղական կոնցենցիայի մոդելավորման հաջողությանը, գոյություն ունեցող նյարդային լեզվի ստեղծման մոդելները (օրինակ GPT-2) դեռևս կրկնվում են, տրամաբանական հակամարտությունները և ստեղծված պատմություններում երկար հեռավորության պակաս: Մենք ենթադրում ենք, որ սա այն դժվարության պատճառով է, որ հարկավոր ընդհանուր գիտելիքներ կապված են, պատճառական հարաբերությունների հասկանալը, պլանավորման միավորները և իրադարձությունները ճիշտ ժամանակային կարգով: Այս թղթի մեջ մենք ստեղծում ենք ընդհանուր պատմությունների սերունդների համար գիտելիքներով բարելավված նախապատրաստման մոդել: Մենք առաջարկում ենք օգտագործել արտաքին գիտելիքների հիմքերից ընդհանուր գիտելիքներ՝ խելամիտ պատմություններ ստեղծելու համար: Որպեսզի ավելի լավ ընկալենք պատճառի և ժամանակական կախվածությունը դասությունների միջև, մենք օգտագործում ենք բազմախնդիրների ուսումնասիրություն, որը համադրում է խտրականության առարկաներ ճշմարտության և կեղծ պատմությունների տարբերակելու համար բարելավման ժամանակ: Ավտոմատիկ և ձեռքի գնահատումը ցույց է տալիս, որ մեր մոդելը կարող է ստեղծել ավելի խելամիտ պատմություններ, քան ամենաբարձր հիմքերը, հատկապես տրամաբանական և գլոբալ կոնցենսիայի առումով:', 'az': 'Hikayə nəsli, məsələn, ilk məlumatdan münasibətli hekayə yaratmaq, möhüm, amma çətin bir işdir. Modellənmək və yerli birləşmək modellərinin başarısızlığına baxmayaraq, məskənin nöral dil nəsli modellərinin (məsələn, GPT-2) hələ də tekrarlıqların, lojik münafiqlərin və ürəklənmiş hekayələrdə uzun səviyyədə bir birləşmək yoxdur. Biz zənn edirik ki, bunun məqsədilə müxtəlif bilikləri birləşdirmək, nəticə əlaqələrini anlamaq, planlamaq məqsədilə və olaraq müəyyən vaxtlı sıralamaq üçün çətindir. Bu kağızda, elmdən daha çox təsirli bir modeli düzəltdik. Biz münasibətli hekayələr yaratmaq üçün dış bilgi üssələrindən müxtəlif bilgi istifadə etməyi təklif edirik. Bu cümlələr arasındakı səbəb və müddətli bağlılıqları daha da artırmaq üçün çoxlu işin öyrənməsini istifadə edirik ki, həqiqəti və sahte hekayələri düzgün düzgün düzgün tərzdə ayırmaq üçün dəyişiklik məqsədilə birləşdirir. Avtomatik və manual değerlendirmək modellərimiz mövzudan daha çox mantıklı hekayələr yarada biləcəyini göstərir, özlərinə də logik və küresel birləşdirmək haqqında.', 'bn': 'একটি নেতৃত্ব থেকে একটি যৌক্তিক গল্প তৈরি করার জন্য গল্প গুরুত্বপূর্ণ কিন্তু চ্যালেঞ্জের কাজ। In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories.  আমরা ধারণা করছি যে এটা হচ্ছে সংশ্লিষ্ট কমিউনিসেন্সের জ্ঞানের সাথে যোগাযোগের কঠিন কারণে, কারণ সম্পর্ক বুঝতে পারে এবং সঠিক সময়ের আদেশের এই কাগজটিতে আমরা একটি জ্ঞান-বৃদ্ধিপূর্ণ মডেল চিন্তা করেছি কমন্সেন্সেস প্রজন্মের জন্য বৃষ্টির প্রজন্ম আমরা বাইরের জ্ঞানের বেস থেকে কমিউনিসেন্সের জ্ঞান ব্যবহার করার প্রস্তাব করছি যাতে যুক্ত গল্প তৈরি করা। আরো কারণ এবং সময়ের নির্ভর করার জন্য একটি যৌক্তিক গল্পের মধ্যে বাক্যের মাঝে নির্ভর করা, আমরা বহুকাজ শিক্ষা ব্যবহার করি, যা সঠিক ভাবে মিথ্যা এবং মিথ্যা কাহিনীকে  স্বয়ংক্রিয়ভাবে এবং হাতিয়াল মুল্যায়ন দেখাচ্ছে যে আমাদের মডেল রাষ্ট্র-শিল্পের বেসাইনের চেয়ে আরো বেশি যুক্ত গল্প তৈরি করতে পারে, বি', 'ca': "La generació de històries, és a dir, generar una història raonable d'un context líder, és una tasca important però desafiadora. Malgrat l'èxit en modelar la fluïtat i la coherencia local, els models existents de generació de llenguatges neurals (per exemple GPT-2) encara pateixen repetició, conflictes lògics i falta de coherencia a llarg alcance en les històries generades. Suposem que això és degut a la dificultat d'associar coneixements comuns pertinents, entendre les relacions causals, i organitzar entitats i esdeveniments amb ordre temporal adequat. En aquest article, dissenyem un model de pré-entrenament millorat pel coneixement per a generar històries comunes. Proposem utilitzar coneixements comuns de bases externes de coneixements per generar històries raonables. Per capturar més les dependencies causals i temporals entre les frases d'una història raonable, fem servir l'aprenentatge multitasca, que combina un objectiu discriminatiu per distingir històries veritables i falses durant l'ajustament. L'avaluació automàtica i manual mostra que el nostre model pot generar històries més raonables que les línies de base més avançades, especialment en termes de lògica i coherencia global.", 'bs': 'Generacija priče, protiv, stvaranje razumne priče iz vodećeg konteksta, je važan ali izazovni zadatak. Uprkos uspjehu modeliranja tečnosti i lokalne konsekvencije, postojeći modeli generacije neuralnih jezika (npr. GPT-2) još uvijek pati od ponavljanja, logičkih sukoba i nedostatka dugog raspona saskaņonosti u proizvedenim pričama. Pretpostavljamo da je to zbog teškoće povezivanja relevantnih znanja zajedničkog smisla, razumijevanja uzrokovanih odnosa, planiranja entitata i događaja s odgovarajućim vremenskim redom. U ovom papiru, razmišljamo o povećanom znanju modelu pretkivanja za generaciju priče o običnom smislu. Predlažemo da iskoristimo zajedničke znanje iz vanjskih baza znanja kako bi stvorili razumne priče. Da bismo dalje uhvatili uzrokovane i privremene zavisnosti između rečenica u razumnoj priči, koristimo više zadataka učenje, koja kombinuje diskriminacijski cilj da razlikuje prave i lažne priče tijekom fine-tuning. Automatska i ručna procjena pokazuje da naš model može proizvesti razumne priče nego početne linije države umjetnosti, posebno u smislu logike i globalne konsekvence.', 'cs': 'Generování příběhů, konkrétně generování rozumného příběhu z vedoucího kontextu, je důležitým, ale náročným úkolem. Navzdory úspěchu v modelování plynulosti a lokální koherence stále trpí stávajícími modely generace nervových jazyků (např. GPT-2) opakováním, logickými konflikty a nedostatkem dlouhodobé koherence generovaných příběhů. Domníváme se, že je to kvůli obtížnosti spojovat relevantní znalosti zdravého rozumu, porozumět příčinným vztahům a plánovat entity a události s řádným časovým řádem. V tomto článku navrhujeme model předškolení rozšířený znalostmi pro generování zdravého rozumu příběhů. Navrhujeme využít zdravého rozumu znalostí z externích znalostních bází k generování rozumných příběhů. Abychom dále zachytili příčinné a časové závislosti mezi větami v rozumném příběhu, používáme multi-tasking learning, který kombinuje diskriminační cíl rozlišovat pravdivé a falešné příběhy při jemném ladění. Automatické a ruční hodnocení ukazuje, že náš model může generovat rozumnější příběhy než moderní základní linie, zejména z hlediska logiky a globální soudržnosti.', 'et': 'Lugude loomine, nimelt mõistliku loo loomine juhtivast kontekstist, on oluline, kuid keeruline ülesanne. Vaatamata sujuva modelleerimise ja kohaliku sidususe edule kannatavad olemasolevad neurokeele genereerimise mudelid (nt GPT-2) ikka veel korduvuse, loogika konfliktide ja loodud lugude pikaajalise sidususe puudumise tõttu. Me oletame, et see on tingitud raskustest seostada asjakohaseid mõistlikke teadmisi, mõista põhjuslikke seoseid ning planeerida üksusi ja sündmusi õige ajalise järjekorraga. Käesolevas töös töötame välja teadmistepõhise eelõpetamise mudeli mõistliku loo genereerimiseks. Me teeme ettepaneku kasutada mõistlike lugude loomiseks välistest teadmistebaasidest pärit üldisi teadmisi. Mõistliku loo lausete põhjusliku ja ajalise sõltuvuse edasiseks jäädvustamiseks kasutame mitme ülesandega õppimist, mis ühendab diskrimineeriva eesmärgi eristada õigeid ja valelugusid peenhäälestuse ajal. Automaatne ja käsitsi hindamine näitab, et meie mudel võib luua mõistlikumaid lugusid kui kaasaegsed lähtejooned, eriti loogika ja globaalse sidususe osas.', 'fi': 'Tarinan luominen eli järkevän tarinan luominen johtavasta kontekstista on tärkeä mutta haastava tehtävä. Huolimatta sujuvasta mallintamisesta ja paikallisesta koherenssista, olemassa olevat neurokielen generointimallit (esim. GPT-2) kärsivät edelleen toistosta, logiikkaristiriidoista ja pitkän aikavälin koherenssin puutteesta luoduissa tarinoissa. Arvelemme, että tämä johtuu siitä, että on vaikeaa yhdistää asiaankuuluvaa yleistä järkeä koskevaa tietoa, ymmärtää syy-yhteyttä ja suunnitella kokonaisuuksia ja tapahtumia oikeaan aikajärjestykseen. Tässä artikkelissa kehitämme tietoon perustuvan esikoulutusmallin järjetöntä tarinankehitystä varten. Ehdotamme, että hyödynnämme ulkoisista tietokantoista saatavaa yleistä tietoa järkevän tarinan tuottamiseksi. Jotta lauseiden kausaali- ja ajalliset riippuvuudet saataisiin talteen järkevässä tarinassa, käytämme monitehtäväoppimista, jossa yhdistyy erotteleva tavoite erottaa oikeat ja väärennetyt tarinat hienosäädön aikana. Automaattinen ja manuaalinen arviointi osoittaa, että mallimme voi tuottaa järkeviä tarinoita kuin uusimmat lähtökohdat, erityisesti logiikan ja globaalin johdonmukaisuuden kannalta.', 'jv': 'Suara winih, mengko-nambah, njuk-nambah kuwi sampeyan sing mengko kontèks, sing dikareparahan ngupakan nggawe basa sing apik. Gak dhéwé nganggo perbudhakan langgar sampek lan sabanjuré, model sing paling nêran langgar sampek (isih, Gpta-2) isih durung tau matang sakjane kapan pangan, sukelan-sukelan lan ora sakjane sampek kapan nganggo sakjane kapan pangan. Awak dhéwé yaakaar iki ngono nggawé kuwi nggawé kuwi nggawe kesempatan kanggo awak dhéwé kesempatan, kuwi mau ngerasakno perusahaan, lan sampek kanggo awak dhéwé lan uwis. Nanging kuwi iki, awake dhewe jejer model sing paling awak dhuwur, ngono model kuwi nggawe Perintah Panjenengan Perintah kuwi mau. Awakdhéwé nggunakake ngerasakno nggawe barang nggawe barang-barang awak dhéwé nang ngerasakno sing luwih apik. Mbok iso nggawe akeh dolanan sing perusahaan lan sampeyan gewis dipulangan luwih dumadhi winih sing nganggo kuwi mau, awak dhéwé iso nggambar kelas multi-task, sing isinggo iso nggawe bukal sing apik tur dhéwé sing apik dhéwé kuwi mau Awakdhéwé lan manut kuwi nggawe modèl kuwi tindakan akeh basa luwih apik dhéwé karo hal-karat kuwi tindakan, ngomong sakjane logik lan tambah global.', 'ha': "Kishi kiyaye, yanzu, ka ƙãga wani lãbãri mai daidai daga wani muhimmi mai gabatar da shi, yana da muhimmi kuma yana gauraya. Babu da cin nasara a sami-samfani da lokal kodi, yana da misãlai masu danna cikin harshen neural (misali, GPT-2) kuma ana ƙari koshi mai tsawo cikin historin da aka ƙãga. Tuna cẽwa, wannan na yi nauyi a kan tãrayyar da ilmi masu husũma, bã su fahimta masu husũma da danganta, kuma yana yi mãkirci da masu adadi da kuma ma'anar al'amarin da ke daidaita. Ga wannan takardan, Munã ƙayyade wani misalin da aka samar da shi na bakwai wa 'ya'yan lãbãrin na'urar. Munã kwaɗayin mu yi amfani da ilmin mataimaki daga bakin saniya na bakin da za'a sami lãbãri masu inganci. To, dõmin ka kãma masu saurin da takarda da zaman mutane a tsakanin maganar da ke cikin wani lãbãri mai haƙƙanci, za mu yi amfani da sanar aiki masu yawa, wanda ke haɗa wani abu da ya yi rabo da gaɓanci dõmin ka rarraba masu gaskiya da ƙarya a lokacin da za'a yi amfani da shi. Ana ƙaddara farat ɗaya da manual yana nuna cewa misalinmu yana iya ƙiƙiro wasu lãbãri masu inganci mafi cancanta daga halin-rubutun-na-kunyar, hususanci, cikin masu haɗi da mazaɓa da duniya.", 'sk': 'Ustvarjanje zgodb, namreč ustvarjanje razumne zgodbe iz vodilnega konteksta, je pomembna, a zahtevna naloga. Kljub uspehu pri modeliranju tekočosti in lokalne koherence obstoječi modeli generacije nevronskih jezikov (npr. GPT-2) še vedno trpijo zaradi ponavljanja, logičnih konfliktov in pomanjkanja dolgoročne koherence v ustvarjenih zgodbah. Domnevamo, da je to zaradi težav pri povezovanju ustreznega splošnega znanja, razumevanju vzročnih odnosov ter načrtovanju entitet in dogodkov z ustreznim časovnim redom. V tem prispevku smo oblikovali model predšolskega usposabljanja za ustvarjanje dobrih zgodb. Predlagamo uporabo splošnega smisla znanja iz zunanjih baz znanja za ustvarjanje razumnih zgodb. Za nadaljnje zajemanje vzročnih in časovnih odvisnosti med stavki v razumni zgodbi uporabljamo večopravilno učenje, ki združuje diskriminativni cilj razlikovanja resničnih in lažnih zgodb med finim uravnavanjem. Avtomatsko in ročno ocenjevanje kaže, da lahko naš model ustvari bolj razumne zgodbe kot najsodobnejše osnovne linije, zlasti v smislu logike in globalne skladnosti.', 'bo': 'Story generation, namely, reasonable story from a leading context, is an important but challenging task. རྣམ་པ་དབྱིབས་མཐུན་དང་རང་ཁུལ་གྱི་མཉམ་མཐུན་གྱི་གྲུབ་འབྲས་ཀྱི་དཔེ་དབྱིབས་ཡོད་པ་ལྟར། ང་ཚོས་རྟོགས་པའང་འདིར་མཐུན་པའི་ཆ་རྐྱེན་གྱི་སྤྱི་ཚོགས་ཀྱི་དཀའ་ངལ་བ་རྐྱེན་གྱི་རྒྱུ་མཚན་ཡིན། འོག་གི་ཤོག་བུ་འདིའི་ནང་དུ་ང་ཚོས་མཐོང་སྣང་པ་ལྟར་ཡར་རྒྱས་བཤད་ཀྱི་ཐབས་ལམ་ཞིག་གསར་བསྐྲུན་ཡོད། We propose to use commonsense knowledge from external knowledge bases to generate reasonable stories. ཚིག་རྐང་ཐོག་མའི་རྒྱུ་མཚན་དང་དུས་མཚམས་ཀྱི་རྟེན་འབྲེལ་འདི་ཚོ་རྟོགས་བསམ་བློ་གཏོང་ནི་ལྟ་བུ་སྤྱད་ནས་གནད་དོན་མང་པོ་ཞིག་སྤྱོད་ཀྱི་ཡོད། རང་བཞིན་གྱིས་དང་ལག་བཟོས་ཡོད་པའི་ཞིབ་དཔྱད་ནི་ང་ཚོའི་མིག', 'he': 'דור סיפורים, במיוחד, ליצור סיפור הגיוני ממקשר מוביל, הוא משימה חשובה אבל מאתגרת. למרות ההצלחה בנוגע למודל הנוזל והתואם המקומי, דוגמני דורת שפת עצבית קיימים (למשל GPT-2) עדיין סובלים ממחזור, קונפליקטים הגיוניים, וחסר תואם לטווח ארוך בסיפורים שנוצרים. We conjecture that this is because of the difficulty of associating relevant commonsense knowledge, understanding the causal relationships, and planning entities and events with proper temporal order.  בעיתון הזה, אנחנו מתכננים מודל מחדש מידע משפר לדור סיפורים רגילים. אנחנו מציעים להשתמש בידע משותף מבסיסי ידע חיצוניים כדי ליצור סיפורים הגיוניים. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we use multi-task learning, which combines a discriminative objective to distinguish true and fake stories during fine-tuning.  הערכה אוטומטית והידנית מראה שהדוגמא שלנו יכולה ליצור סיפורים הגיוניים יותר מאשר קווי הבסיס המיוחדים ביותר, במיוחד במונחים של הגיוניקה וקיונציה גלובלית.'}
{'en': 'Improving Candidate Generation for Low-resource Cross-lingual Entity Linking', 'ar': 'تحسين جيل المرشحين لربط الكيانات متعدد اللغات منخفض الموارد', 'fr': "Améliorer la génération de candidats pour la liaison d'entités multilingues à faibles ressources", 'pt': 'Melhorando a geração de candidatos para vinculação de entidades multilíngues com poucos recursos', 'es': 'Mejorar la generación de candidatos para la vinculación de entidades multilingües de bajos', 'ja': '低リソースのクロスリンガルエンティティリンクの候補者生成の改善', 'hi': 'कम संसाधन क्रॉस-भाषी एंटिटी लिंकिंग के लिए उम्मीदवार जनरेशन में सुधार', 'zh': '改低资源跨言体链接之选成', 'ru': 'Улучшение генерации соискателей для межъязыковой привязки малоресурсных объектов', 'ga': 'Giniúint Iarrthóirí a Fheabhsú le haghaidh Nascála Aonáin Trasteangacha ar Acmhainn Íseal', 'ka': 'უფრო მეტი რესურსის კრისტალური ელემენტის შესაბამისათვის კანდიდის შექმნა', 'el': 'Βελτίωση της γενιάς υποψηφίων για τη διασύνδεση γλωσσών οντοτήτων με χαμηλούς πόρους', 'hu': 'A jelöltek generálásának javítása az alacsony erőforrású, többnyelvű szervezetek összekapcsolása érdekében', 'it': 'Migliorare la generazione di candidati per il collegamento interlinguale di entità a basse risorse', 'lt': 'Mažai išteklių turinčių tarpkalbinių subjektų tarpusavio ryšių kandidatų generacijos gerinimas', 'mk': 'Подобрување на генерацијата на кандидати за поврзување меѓујазични ентитети со ниски ресурси', 'kk': 'Төменгі ресурстардың көп тілді нысандарды сілтемелеу үшін кандидатты жасау үшін жақсарту', 'ml': 'കുറഞ്ഞ വിഭവങ്ങള്\u200dക്കുള്ള ക്രോസ്- ലിങ്കിങ്ങിനുള്ള വിന്യാപാര്\u200dത്ഥിക്കുന്ന സംഘത്തിനുള്ള പ്', 'ms': 'Menembak Jenerasi Candidate untuk Pautan Entiti Salib-Bahasa Sumber-rendah', 'mt': "Titjib fil-Ġenerazzjoni tal-Kandidati għal Rabta bejn Entitajiet Translingwi b'Riżorsi Bażi", 'no': 'Forbetra generering av kandidater for lenking av låg ressurs- krysspråk', 'mn': 'Хоёр бага баялаг боловсруулагч хэлний нэгж холбогдохын тулд', 'ro': 'Îmbunătățirea generației de candidați pentru conectarea interlingvă a entităților cu resurse reduse', 'pl': 'Poprawa generowania kandydatów na rzecz łączenia podmiotów o niskich zasobach', 'sr': 'Poboljšanje generacije kandidata za povezanje krstojezičkih jedinica s niskim resursima', 'si': 'ක්\u200dරොස් භාෂාත්මක අන්තර්ජාතාව සඳහා කැන්ඩියේට් නිර්මාණය වැඩි කරනවා', 'so': 'Improving Candidate Generation for Low-resource Cross-language Entity Linking', 'sv': 'Förbättra kandidatgenereringen för sammanlänkning av flerspråkiga enheter med låga resurser', 'ta': 'குறைந்த மூலத்தின் குறைந்த மொழி இணைப்புக்கான சேர்ப்பு உருவாக்கத்தை மேம்படுத்துகிறது', 'ur': 'کم-منبع کرس-زبان انٹیٹی لینک کے لئے کنڈیٹ پیدا کرنے کے لئے بہتر ہے', 'uz': 'Name', 'vi': 'Tăng cường hình ứng viên cho Liên kết ngôn ngữ thấp', 'bg': 'Подобряване на генерирането на кандидати за свързване на междуезични организации с ниски ресурси', 'nl': 'Verbetering van kandidaat-generatie voor laagdrempelige cross-lingual entity linking', 'da': 'Forbedring af kandidatgenerering for tværsproget enhedssammenkædning med lave ressourcer', 'hr': 'Poboljšanje generacije kandidata za povezanje krstojezičkih podataka s niskim resursima', 'de': 'Verbesserung der Kandidatengeneration für ressourcenschonende Cross-Lingual Entity Linking', 'id': 'Menembangkan Generasi Kandidat untuk Hubungan Entitas Selasa Bahasa dengan sumber daya rendah', 'fa': 'بهتر تولید تولید کاندیده برای ارتباط متحد زبان کم', 'ko': '저자원 다중 언어 실체 링크 후보 생성 개선', 'sw': 'Kuboresha kizazi cha wagombea kwa ajili ya Uunganishaji wa Kiasili cha Kupungua Rasilimali', 'tr': 'Az-Ressourt Çapraz Dilli Baglaýyşlar üçin Kandidat Jeşimlerini geliştir', 'af': 'Verbeter Kandidate Generasie vir Lae- hulpbron Kruistale Entiteit Linking', 'sq': 'Përmirësimi i gjenerimit të kandidatëve për lidhjen ndër-gjuhësore të njësisë me burime të ulta', 'am': 'አድራሻ', 'hy': 'Նվագ ռեսուրսների միջլեզվային միավորների կապերի առաջացման բարելավումը', 'bn': 'নিম্নলিখিত সম্পদ ক্রস-ভাষায় লিঙ্কিং-এর জন্য প্রার্থীর প্রজন্ম উন্নতি করা হচ্ছে', 'az': 'Aşağı-çox-çox dilli Entity Linking üçün Kandidata Üstünlüyünü yaxşılaşdırma', 'bs': 'Poboljšanje generacije kandidata za povezanje krstojezičkih jedinica s niskim resursima', 'ca': 'millorar la generació de candidats per un enllaç entre entitats translingües de baix recursos', 'cs': 'Zlepšení generace kandidátů pro propojení mezi jazyky s nízkými zdroji', 'et': 'Kandidaatide põlvkonna parandamine vähese ressursiga keeleüleste üksuste sidumiseks', 'fi': 'Ehdokaspolven parantaminen vähävaraisessa monikielisessä linkityksessä', 'he': 'שיפור מיוצר מועמדים לקשר יחסי בין יחידות דרך שפתיים עם משאבים נמוכים', 'sk': 'Izboljšanje generacije kandidatov za medjezično povezovanje subjektov z nizkimi viri', 'ha': 'KCharselect unicode block name', 'jv': 'Ngubah Kandidate Generation kanggo Ketok-Ressource Oblast-Linking', 'bo': 'རྒྱ་ཆེ་ཆུང་ལ་མཐུད་སྣུམ་མཐུད་ཀྱི་ཆ་རྐྱེན་ཆ་ལ་ཡར་རྒྱས་གཏོང'}
{'en': 'Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention. Approaches based on resources from ', 'ar': 'ربط الكيانات عبر اللغات (XEL) هو مهمة البحث عن المراجع في قاعدة معرفة اللغة الهدف (KB) للإشارات المستخرجة من نصوص لغة المصدر. تتمثل الخطوة الأولى في (X) EL في إنشاء مرشح ، والذي يسترد قائمة بالكيانات المرشحة المعقولة من قاعدة المعارف باللغة الهدف لكل ذكر. أثبتت الأساليب القائمة على الموارد من ويكيبيديا نجاحها في مجال اللغات عالية الموارد نسبيًا ، ولكن هذه لا تمتد بشكل جيد إلى اللغات منخفضة الموارد مع القليل من صفحات ويكيبيديا ، إن وجدت. في الآونة الأخيرة ، ثبت أن أساليب التعلم التحويلية تقلل من الطلب على الموارد باللغات منخفضة الموارد من خلال استخدام الموارد باللغات وثيقة الصلة ، لكن الأداء لا يزال متأخرًا كثيرًا عن نظرائهم ذوي الموارد العالية. في هذه الورقة ، نقوم أولاً بتقييم المشكلات التي تواجهها طرق إنشاء مرشح الكيان الحالي لـ XEL منخفض الموارد ، ثم نقترح ثلاثة تحسينات (1) تقلل من الانفصال بين إشارات الكيان وإدخالات KB ، و (2) تحسين متانة النموذج لسيناريوهات الموارد المنخفضة. الأساليب بسيطة ولكنها فعالة: نجرب نهجنا على سبع مجموعات بيانات XEL ووجدنا أنها تحقق ربحًا متوسطًا بنسبة 16.9٪ في أعلى 30 مرشحًا لاسترجاع الذهب ، مقارنةً بخطوط الأساس الحديثة. ينتج نموذجنا المحسّن أيضًا مكسبًا متوسطًا بنسبة 7.9٪ في دقة in-KB من طرف إلى طرف XEL.1', 'es': 'La vinculación de entidades multilingües (XEL) es la tarea de encontrar referentes en una base de conocimientos (KB) del idioma de destino para las menciones extraídas de textos en el idioma fuente. El primer paso de (X) EL es la generación de candidatos, que recupera una lista de entidades candidatas plausibles de la KB del idioma de destino para cada mención. Los enfoques basados en recursos de Wikipedia han demostrado ser exitosos en el ámbito de los idiomas de recursos relativamente altos, pero no se extienden bien a los idiomas de bajos recursos con pocas páginas de Wikipedia, si es que tienen alguna. Recientemente, se ha demostrado que los métodos de aprendizaje por transferencia reducen la demanda de recursos en los idiomas de bajos recursos mediante la utilización de recursos en idiomas estrechamente relacionados, pero el rendimiento sigue estando muy por debajo de sus homólogos de alto nivel de recursos. En este artículo, primero evaluamos los problemas a los que se enfrentan los métodos actuales de generación de candidatos de entidad para XEL de bajos recursos, luego proponemos tres mejoras que (1) reducen la desconexión entre las menciones de entidades y las entradas de KB, y (2) mejoran la solidez del modelo para escenarios de bajos recursos. Los métodos son simples, pero efectivos: experimentamos con nuestro enfoque en siete conjuntos de datos XEL y descubrimos que producen una ganancia promedio del 16,9% en la retirada de los 30 mejores candidatos de oro, en comparación con las líneas de base de última generación. Nuestro modelo mejorado también produce una ganancia media del 7,9% en la precisión en KB del XEL de extremo a extremo.', 'pt': 'Cross-lingual entity linking (XEL) é a tarefa de encontrar referências em uma base de conhecimento do idioma de destino (KB) para menções extraídas de textos do idioma de origem. A primeira etapa do (X)EL é a geração de candidatos, que recupera uma lista de entidades candidatas plausíveis da base de conhecimento do idioma de destino para cada menção. Abordagens baseadas em recursos da Wikipédia provaram ser bem sucedidas no domínio das linguagens de recursos relativamente altos, mas elas não se estendem bem a linguagens de poucos recursos com poucas páginas da Wikipédia, ou nenhuma. Recentemente, os métodos de aprendizagem por transferência demonstraram reduzir a demanda por recursos em idiomas com poucos recursos, utilizando recursos em idiomas intimamente relacionados, mas o desempenho ainda está muito atrás de suas contrapartes de recursos altos. Neste artigo, primeiro avaliamos os problemas enfrentados pelos métodos atuais de geração de candidatos de entidade para XEL de baixo recurso, então propomos três melhorias que (1) reduzem a desconexão entre menções de entidade e entradas de KB e (2) melhoram a robustez do modelo para cenários de poucos recursos. Os métodos são simples, mas eficazes: experimentamos nossa abordagem em sete conjuntos de dados XEL e descobrimos que eles geram um ganho médio de 16,9% no recall dos 30 candidatos a ouro, em comparação com linhas de base de última geração. Nosso modelo aprimorado também gera um ganho médio de 7,9% na precisão em KB do XEL de ponta a ponta.1', 'fr': "La liaison d'entités multilingues (XEL) consiste à trouver des référents dans une base de connaissances de la langue cible (KB) pour les mentions extraites de textes en langue source. La première étape de (X) EL est la génération de candidats, qui récupère une liste d'entités candidates plausibles à partir de la base de connaissances de la langue cible pour chaque mention. Les approches basées sur les ressources de Wikipédia ont fait leurs preuves dans le domaine des langues à ressources relativement élevées, mais elles ne s'étendent pas bien aux langues à faibles ressources avec peu de pages Wikipédia, voire aucune. Récemment, il a été démontré que les méthodes d'apprentissage par transfert réduisent la demande de ressources dans les langues à faibles ressources en utilisant des ressources dans des langues étroitement apparentées, mais les performances restent loin derrière leurs homologues à ressources élevées. Dans cet article, nous évaluons d'abord les problèmes rencontrés par les méthodes actuelles de génération de candidats d'entités pour les XEL à faible ressource, puis nous proposons trois améliorations qui (1) réduisent la déconnexion entre les mentions d'entité et les entrées de la base de connaissances, et (2) améliorent la robustesse du modèle pour les scénarios à faibles ressources. Les méthodes sont simples mais efficaces\xa0: nous expérimentons notre approche sur sept ensembles de données XEL et constatons qu'ils génèrent un gain moyen de 16,9\xa0% dans le rappel des 30 meilleurs candidats or, par rapport aux bases de référence les plus récentes. Notre modèle amélioré produit également un gain moyen de 7,9\xa0% en termes de précision en Ko du XEL.1 de bout en bout", 'ja': 'クロスリンガルエンティティリンク（ XEL ）は、ソース言語のテキストから抽出されたメンションのためのターゲット言語ナレッジベース（ KB ）で参照元を見つけるタスクです。 （ Ｘ ） ＥＬの第１のステップは、各言及についてターゲット言語ＫＢから妥当な候補エンティティのリストを取り出す候補生成である。 ウィキペディアからのリソースに基づいたアプローチは、比較的高いリソース言語の領域で成功していることが証明されていますが、これらは、ウィキペディアのページがほとんどない（もしあれば）低リソース言語にはうまく拡張できません。 最近では、密接に関連する言語のリソースを活用することで、低資源言語のリソース需要を減らす転送学習方法が示されているが、そのパフォーマンスは依然として高資源言語のリソースよりもはるかに遅れている。 本稿では、まず、低資源XELの現行のエンティティ候補生成手法が直面する問題点を評価し、次いで、(1)エンティティメンションとKBエントリの切断を減らす、(2)低資源シナリオへのモデルの堅牢性を向上させる、3つの改善点を提案する。 方法はシンプルですが効果的です。7つのXELデータセットでアプローチを実験したところ、最先端のベースラインと比較して、トップ30ゴールド候補のリコールで平均16.9 ％の利得が得られることがわかりました。 当社の改良されたモデルは、エンドツーエンドのXEL 1のインKB精度においても平均7.9%の利得をもたらします。', 'hi': 'क्रॉस-लिंगुअल एंटिटी लिंकिंग (XEL) स्रोत-भाषा ग्रंथों से निकाले गए उल्लेखों के लिए लक्ष्य-भाषा ज्ञान आधार (KB) में दिग्दर्शन खोजने का कार्य है। (एक्स) ईएल का पहला चरण उम्मीदवार पीढ़ी है, जो प्रत्येक उल्लेख के लिए लक्ष्य-भाषा केबी से प्रशंसनीय उम्मीदवार संस्थाओं की एक सूची प्राप्त करता है। विकिपीडिया से संसाधनों पर आधारित दृष्टिकोण अपेक्षाकृत उच्च-संसाधन भाषाओं के दायरे में सफल साबित हुए हैं, लेकिन ये कुछ, यदि कोई हो, तो विकिपीडिया पृष्ठों के साथ कम संसाधन वाली भाषाओं तक अच्छी तरह से विस्तारित नहीं होते हैं। हाल ही में, हस्तांतरण सीखने के तरीकों को बारीकी से संबंधित भाषाओं में संसाधनों का उपयोग करके कम-संसाधन भाषाओं में संसाधनों की मांग को कम करने के लिए दिखाया गया है, लेकिन प्रदर्शन अभी भी अपने उच्च-संसाधन समकक्षों से बहुत पीछे है। इस पेपर में, हम पहले कम-संसाधन XEL के लिए वर्तमान एंटिटी उम्मीदवार पीढ़ी विधियों द्वारा सामना की जाने वाली समस्याओं का आकलन करते हैं, फिर तीन सुधारों का प्रस्ताव करते हैं जो (1) एंटिटी उल्लेख और केबी प्रविष्टियों के बीच डिस्कनेक्ट को कम करते हैं, और (2) कम-संसाधन परिदृश्यों के लिए मॉडल की मजबूती में सुधार करते हैं। विधियां सरल हैं, लेकिन प्रभावी हैं: हम सात एक्सईएल डेटासेट पर हमारे दृष्टिकोण के साथ प्रयोग करते हैं और पाते हैं कि वे अत्याधुनिक बेसलाइन की तुलना में शीर्ष -30 सोने के उम्मीदवार को याद करने में 16.9% का औसत लाभ प्राप्त करते हैं। हमारे बेहतर मॉडल भी अंत-से-अंत XEL.1 की इन-केबी सटीकता में 7.9% का औसत लाभ प्राप्त करता है', 'zh': '语言实体链接 (XEL) 在语言知识库 (KB) 中寻源语言文本提引。 (X)EL 初为选生,每言知识库中检理者列表。 盖维基百科资源之法,高资之言,已成其功,然不能甚广维基百科页面少(有)之低资源言。 近者,迁学之法已验可用密语之资以损低资源言之求,然其性犹远后于高。 先料低资源XEL实体成法所临,然后举三改进,即(1)减实体提及知识库条目之间脱节,及(2)增模形对低资源之鲁棒性。 其法甚简,然颇效:吾于七XEL数据集上试吾法,视最先进之基线,其在前30名黄金候选召中均收益为16.9%。 更进模则端 XEL KB 内准确度均益至 7.9%。', 'ru': 'Межъязычное связывание сущностей (XEL) - это задача поиска ссылок в базе знаний целевого языка (КБ) для упоминаний, извлеченных из текстов на исходном языке. Первым шагом (X)EL является генерация кандидата, которая извлекает список правдоподобных объектов-кандидатов из КБ целевого языка для каждого упоминания. Подходы, основанные на ресурсах из Википедии, оказались успешными в области относительно высокоресурсных языков, но они не распространяются хорошо на малоресурсные языки с немногими, если таковые имеются, страницами Википедии. В последнее время было показано, что методы обучения методам перевода сокращают спрос на ресурсы на языках с ограниченными ресурсами за счет использования ресурсов на близких языках, однако показатели их использования по-прежнему значительно отстают от показателей их коллег, обладающих большими ресурсами. В этой статье мы сначала оцениваем проблемы, с которыми сталкиваются текущие методы генерации сущностей-кандидатов для малоресурсного XEL, а затем предлагаем три улучшения, которые (1) уменьшают разрыв связи между упоминаниями сущностей и записями KB и (2) улучшают устойчивость модели к сценариям с малоресурсами. Методы просты, но эффективны: мы экспериментируем с нашим подходом на семи датасетах XEL и обнаруживаем, что они дают средний прирост в 16,9% в топ-30 золотых отзывов кандидатов по сравнению с современными базовыми линиями. Наша улучшенная модель также дает средний прирост в 7,9% в КБ точности сквозного XEL.1', 'ga': "Is éard atá i gceist le nascadh aonán tras-teangach (XEL) tagairtí a aimsiú i mbunachar eolais ar an sprioctheanga (KB) le haghaidh tagairtí a bhaintear as téacsanna sa bhunteanga. Is í an chéad chéim de (X)EL ná giniúint iarrthóra, a dhéanann liosta d’eintitis iarrthóirí sochreidte a aisghabháil ón KB sa sprioctheanga do gach tagairt. D'éirigh go maith le cur chuige bunaithe ar acmhainní ón Vicipéid i réimse na dteangacha sách ard-acmhainní, ach ní shíneann siad seo go maith chuig teangacha íseal-acmhainne ar bheagán leathanach Vicipéid, más ann dóibh. Le déanaí, léiríodh go laghdaíonn modhanna foghlama aistrithe an t-éileamh ar acmhainní sna teangacha íseal-acmhainne trí úsáid a bhaint as acmhainní i dteangacha atá gaolmhar go dlúth leo, ach tá an fheidhmíocht fós i bhfad chun deiridh ar a gcomhghleacaithe ard-acmhainní. Sa pháipéar seo, déanaimid measúnú ar dtús ar na fadhbanna a bhíonn le sárú ag modhanna giniúna iarrthóirí aonáin reatha le haghaidh XEL íseal-acmhainne, ansin molaimid trí fheabhsú a (1) a laghdaíonn an dícheangal idir tagairtí eintitis agus iontrálacha KB, agus (2) feabhas a chur ar stóinseacht an mhúnla. do chásanna íseal-acmhainne. Tá na modhanna simplí, ach éifeachtach: Déanaimid tástáil lenár gcur chuige ar sheacht dtacar sonraí XEL agus faighimid amach go n-eascraíonn siad gnóthachan meánach de 16.9% in aisghlaoch iarrthóirí ór Barr-30, i gcomparáid leis na bunlínte úrscothacha. Cruthaíonn ár múnla feabhsaithe meánghnóthachan 7.9% freisin i gcruinneas in-KB de XEL.1 deireadh go deireadh", 'ka': 'მრავალენგური ინტერტის შეერთება (XEL) არის რეფერენტების ძებნა მიზეზი ენგური ცნობიერების ბაზაში (KB) განსხვავებაში, რომლებიც მხოლოდ ენგური ტექსტიდან გამოყენებულია. (X) EL-ის პირველი ნაგულისხმები არის კანდენტის განვითარება, რომელიც ყველა შესახებ კონდენტის მისაღებელი კანდენტის ელემენტის სია მიღება. Wikipedia-ის რესურსების ბაზედან მიღებები წარმატებულია, რომელიც უფრო მეტი მეტი რესურსურსურსურსური ენათების სამყაროში, მაგრამ ეს უფრო მცირე რესურსურსურსური ენათებისთვის, თუ არ მიმდინარე შემდეგ გასწავლების მეტი ჩვენებულია, რომ რესურსების შესაბამისათვის მარტივი რესურსების ენაში გამოყენებული რესურსების გამოყენებით, მაგრამ რესურსების შესაბამისათვის ენაში, მაგრამ კ ამ დომენტში, ჩვენ პირველად გავაკეთებთ პრობლემები, რომლებიც მიმდინარე ინტერტიკური კონდიდეტური განვითარება მეტისთვის XEL-ს, შემდეგ სამი შესაძლებლობა, რომლებიც (1) განვითარება ინტერტიკური განვითარება და KB-ის შესახებ,  მაგრამ ექსპერიმენტები უკეთესი, მაგრამ ექსპერიმენტები: ჩვენ ექსპერიმენტებით ჩვენი პროგორმაცია XEL მონაცემების შვიდი მონაცემებით და აღმოჩნეთ, რომ ისინი საშუალო 16,9% დაი ჩვენი მოდელის შესაძლებლობული მოდელიც სწორედ 7,9% უფრო კოლანდის კოლანდის დასრულებაში XEL-ის განმავლობა. 1', 'el': 'Η διασγλωσσική σύνδεση οντοτήτων (XEL) είναι το καθήκον της εύρεσης αναφορών σε μια βάση γνώσεων της γλώσσας-στόχου (ΚΒ) για αναφορές που εξάγονται από κείμενα της πηγής. Το πρώτο βήμα του (Χ)ΕΛ είναι η δημιουργία υποψηφίων, η οποία ανακτά έναν κατάλογο με πιθανές υποψήφιες οντότητες από την ΚΒ της γλώσσας-στόχου για κάθε αναφορά. Οι προσεγγίσεις που βασίζονται σε πόρους της Βικιπαίδειας έχουν αποδειχθεί επιτυχείς στο πεδίο των γλωσσών σχετικά υψηλών πόρων, αλλά αυτές δεν επεκτείνονται καλά σε γλώσσες χαμηλής περιεκτικότητας με λίγες, αν υπάρχουν, σελίδες της Βικιπαίδειας. Πρόσφατα, οι μέθοδοι μάθησης μεταφοράς έχουν αποδειχθεί ότι μειώνουν τη ζήτηση για πόρους στις γλώσσες χαμηλής περιεκτικότητας σε πόρους χρησιμοποιώντας πόρους σε στενά συνδεδεμένες γλώσσες, αλλά η απόδοση εξακολουθεί να υστερεί πολύ πίσω από τις αντίστοιχες με υψηλό δυναμικό. Σε αυτή την εργασία, αξιολογούμε πρώτα τα προβλήματα που αντιμετωπίζουν οι τρέχουσες μέθοδοι δημιουργίας υποψηφίων οντότητας για το XEL χαμηλού πόρου, στη συνέχεια προτείνουμε τρεις βελτιώσεις που (1) μειώνουν την αποσύνδεση μεταξύ αναφορών οντότητας και καταχωρίσεων ΚΒ, και (2) βελτιώνουν την ανθεκτικότητα του μοντέλου σε σενάρια χαμηλού πόρου. Οι μέθοδοι είναι απλές, αλλά αποτελεσματικές: πειραματιζόμαστε με την προσέγγισή μας σε επτά σύνολα δεδομένων και διαπιστώνουμε ότι αποδίδουν ένα μέσο κέρδος 16,9% στην ανάκληση χρυσού κορυφαίου 30, σε σύγκριση με τις τελευταίες γραμμές βάσης. Το βελτιωμένο μοντέλο μας αποδίδει επίσης ένα μέσο κέρδος 7,9% στην ακρίβεια σε KB του τέλους σε τέλος. 1', 'hu': 'A Cross-lingual entity linking (XEL) feladata, hogy referenciákat találjon egy célnyelvű tudásbázisban (KB) a forrásnyelvű szövegekből kivont említésekhez. Az (X)EL első lépése a jelöltek generációja, amely minden említésnél megkeresi a valószínűsíthető jelöltek listáját a célnyelvű KB-ból. A Wikipédia erőforrásain alapuló megközelítések sikeresnek bizonyultak a viszonylag nagy erőforrású nyelvek birodalmában, de ezek nem terjednek ki jól az alacsony erőforrású nyelvekre, amelyek kevés, ha vannak, Wikipédia oldalak. A közelmúltban kimutatták, hogy a transzfertanulási módszerek csökkentik az alacsony erőforrásokat igénylő nyelvek erőforrásainak igényét a szorosan hasonló nyelveken használt erőforrások felhasználásával, de a teljesítmény még mindig messze elmarad a nagy erőforrásokat igénylő társaiktól. Ebben a tanulmányban először felmérjük a jelenlegi entitásjelöltek generálási módszereinek problémáit az alacsony erőforrású XEL esetében, majd három olyan fejlesztést javasolunk, amelyek (1) csökkentik az entitásjelzések és KB bejegyzések közötti kapcsolatot, és (2) javítják a modell robusztusságát alacsony erőforrású forgatókönyvekre. A módszerek egyszerűek, de hatékonyak: A megközelítésünkkel hét XEL adatkészleten kísérletezünk, és úgy találjuk, hogy ezek átlagosan 16,9%-os nyereséget eredményeznek a Top-30 arany jelöltek visszahívásában, összehasonlítva a legkorszerűbb alapvonalakkal. Továbbfejlesztett modellünk átlagosan 7,9%-os növekedést eredményez az end-to-end XEL pontosságában. 1', 'it': "Il cross-lingual entity linking (XEL) è il compito di trovare referenti in una base di conoscenze in lingua di destinazione (KB) per le menzioni estratte da testi in lingua di origine. Il primo passo di (X)EL è la generazione dei candidati, che recupera un elenco di entità candidate plausibili dalla KB della lingua di destinazione per ogni menzione. Gli approcci basati sulle risorse di Wikipedia si sono dimostrati efficaci nel regno dei linguaggi con risorse relativamente elevate, ma questi non si estendono bene ai linguaggi con poche, se presenti, pagine Wikipedia. Recentemente, i metodi di apprendimento a trasferimento hanno dimostrato di ridurre la domanda di risorse nelle lingue a basso consumo utilizzando risorse in lingue strettamente correlate, ma le prestazioni sono ancora molto indietro rispetto alle loro controparti ad alto contenuto di risorse. In questo articolo, valutiamo prima i problemi affrontati dagli attuali metodi di generazione delle entità candidate per XEL a basso contenuto di risorse, quindi proponiamo tre miglioramenti che (1) riducono la disconnessione tra menzioni di entità e voci KB, e (2) migliorano la robustezza del modello a scenari a basso contenuto di risorse. I metodi sono semplici, ma efficaci: sperimentiamo il nostro approccio su sette dataset XEL e scopriamo che producono un guadagno medio del 16,9% nel richiamo dei candidati d'oro Top-30, rispetto alle linee di base all'avanguardia. Il nostro modello migliorato produce anche un guadagno medio del 7,9% in precisione in KB di XEL end-to-end. 1", 'kk': 'Тілді білім негізінде сілтемелерді табу (XEL) деген тапсырмасы көзі тілді мәтіндерден тарқату үшін мақсат тілді мәліметтерді табу тапсырмасы. (X) EL- нің бірінші қадамы - кандидаттарды құру. Бұл әрбір айтқанда КБ- тілінен келесі көмектесетін кандидаттардың тізімін алып тастайды. Википедиядың ресурстарына негізделген жағдайлар салыстырып көп ресурстар тілдерінде сәтті жеткілікті болды, бірақ бұл кейбірде Википедия парақтарында кейбір ресурстар тілдеріне жеткілікті болмайды. Жуырда оқыту арқылы ресурстарды төмен ресурстар тілдерінде көшірмелеу үшін ресурстарды жақын тілдерде пайдалану арқылы көрсетілді, бірақ оқыту арқылы ресурстардың көшірмесі жоғары ресурстардың қатынасына Бұл қағазда, біріншіден қазіргі бағдарламалардың XEL көп ресурстар үшін кандидаттарды құру әдістерінің мәселелерін бағалап, содан кейін (1) деген бағдарламалар мен КБ жазулар арасындағы байланыстығын азайтын үш жақсартуларды ұсынып, 2) үлгінің Бұл әдістер қарапайым, бірақ эффективті: біз жеті XEL деректер қорларына тәжірибемізді тәжірибеміз және олардың орташа 16,9% алтын кандидаттарының еске салыстырып жатқанын таптық. Біздің жақсы үлгіміз сондай-ақ, КБ-де соңғы-соңғы XEL деген орташа 7,9% көлеміне жеткізеді. 1', 'lt': 'Kelių kalbų subjektas, jungiantis (XEL), yra užduotis rasti referentus tikslinės kalbos žinių bazėje (KB), kad būtų galima paminėti iš tekstų šaltinio kalba. Pirmasis (X)EL žingsnis yra kandidatų karta, kuri kiekvienai paminėti iš tikslinės kalbos KB gauna tikėtinų kandidatų subjektų sąrašą. Pasirodė, kad Wikipedia ištekliais pagrįsti metodai yra sėkmingi palyginti didelių išteklių kalbų srityje, tačiau jie nėra tinkami mažai išteklių turinčioms kalboms su keliomis, jei yra, Wikipedia puslapiais. Pastaruoju metu įrodyta, kad perkėlimo mokymosi metodai mažina išteklių poreikį mažai išteklių turinčiomis kalbomis, naudojant išteklius glaudžiai susijusiomis kalbomis, tačiau rezultatai vis dar labai atsilieka nuo didelių išteklių turinčių šalių. Šiame dokumente pirmiausia vertiname problemas, su kuriomis susiduria dabartiniai subjekto kandidatų gamybos metodai mažai išteklių naudojančiam XEL, tada siūlome tris patobulinimus, kuriais (1) sumažinamas subjekto ir KB įrašų atskyrimas ir (2) gerinamas modelio patikimumas mažai išteklių naudojantiems scenarijams. Metodai paprasti, bet veiksmingi: eksperimentuojame su savo požiūriu į septynis XEL duomenų rinkinius ir nustatome, kad jie gauna vidutinį 16,9 % pelną geriausių 30 aukso kandidatų atšaukimo atveju, palyginti su naujausiomis bazinėmis linijomis. Mūsų patobulintas modelis taip pat suteikia vidutinį 7,9 % XEL tikslumo KB pabaigoje padidėjimą. 1', 'mk': 'Кросјазичниот ентитет кој поврзува (XEL) е задачата да се најдат референти во базата на знаење на јазикот на целта (KB) за спомени извадени од текстите на изворот. Првиот чекор од (X)EL е генерацијата на кандидати, која добива листа на веројатни кандидатски ентитети од клучниот јазик КБ за секое споменување. Пристапите базирани на ресурси од Википедија се покажаа успешни во областа на релативно високи јазици, но овие не се шират добро на јазици со ниски ресурси со неколку, ако постојат, страници од Википедија. Неодамна се покажа дека методите на трансфер на учење ја намалуваат побарувачката за ресурси во јазиците со ниски ресурси со користење на ресурсите во блиски поврзани јазици, но резултатот сé уште е далеку зад нивните колеги со високи ресурси. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios.  Методите се едноставни, но ефикасни: експериментираме со нашиот пристап на седум XEL датотеки и откриваме дека тие предизвикуваат просечна добивка од 16,9 отсто во враќањето на најдобрите 30 златни кандидати, во споредба со најсовремените бази линии. Нашиот подобрен модел, исто така, предизвикува просечен придобивка од 7,9 отсто во точноста во КБ на XEL од крај до крај. 1', 'ms': 'Entiti saling-bahasa yang menghubungkan (XEL) adalah tugas untuk mencari referens dalam pangkalan pengetahuan bahasa-sasaran (KB) untuk sebutan yang diekstrak dari teks bahasa-sumber. Langkah pertama dari (X)EL ialah generasi calon, yang mengambil senarai entiti calon yang boleh diterima dari KB bahasa sasaran untuk setiap sebutan. Approaches based on resources from Wikipedia have proven successful in the realm of relatively high-resource languages, but these do not extend well to low-resource languages with few, if any, Wikipedia pages.  Baru-baru ini, kaedah pembelajaran pemindahan telah menunjukkan untuk mengurangkan permintaan sumber dalam bahasa sumber rendah dengan menggunakan sumber dalam bahasa yang berkaitan dengan dekat, tetapi prestasi masih tertinggal jauh di belakang rekan-rekan sumber tinggi mereka. Dalam kertas ini, kita pertama-tama menilai masalah yang dihadapi oleh kaedah generasi calon entiti semasa untuk XEL sumber rendah, kemudian menyarankan tiga peningkatan yang (1) mengurangkan penghubungan antara sebutan entiti dan masukan KB, dan (2) meningkatkan kepekatan model kepada skenario sumber rendah. Kaedah ini mudah, tetapi berkesan: kita eksperimen dengan pendekatan kita pada tujuh set data XEL dan mencari bahawa mereka menghasilkan keuntungan rata-rata 16.9% dalam pengembalian calon emas Top-30, dibandingkan dengan garis dasar state-of-the-art. Model kami yang lebih baik juga memberikan pendapatan rata-rata 7.9% dalam keperluan in-KB XEL akhir-akhir. 1', 'ml': 'സ്രോതഭാഷ പദാവലികളില്\u200d നിന്നും പുറത്തെടുക്കപ്പെട്ട വിവരങ്ങള്\u200dക്കുള്ള ലക്ഷ്യമായ അറിവുകളുടെ അടിസ്ഥാനത്ത് രേഖകള്\u200d കണ്ടെത്തുന്നതിനുള് The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention.  വിക്കിപീഡിയയിലെ വിഭവങ്ങള്\u200d അടിസ്ഥാനമായി സമ്പാദിച്ചിരിക്കുന്നു. വിക്കിപിഡിയയുടെ താളുകള്\u200d വിജയിച്ചിരിക്കുന്നു. എന്നാല്\u200d ഇതൊക്കെ കുറച്ച അടുത്തിടെ വിഭവങ്ങള്\u200d അടുത്ത വിഭവങ്ങള്\u200d ഉപയോഗിക്കുന്നതില്\u200d വിഭവങ്ങള്\u200dക്കുള്ള വിഭവങ്ങള്\u200d കുറവാക്കാന്\u200d പഠിക്കുന്ന രീതികള്\u200d കാണിച്ചിരിക്കുന്നു. പക ഈ പത്രത്തില്\u200d, നിലവിലുള്ള പ്രാര്\u200dത്ഥിതിയുടെ തലമുറയുടെ പ്രശ്നങ്ങള്\u200d നമ്മള്\u200d ആദ്യം പരിഗണിക്കുന്നു. പിന്നീട് മൂന്നു മുന്നറിയിപ്പുകള്\u200d നോക്കുന്നു (1) വസ്തുവിന്റെ പ്രഖ്യാപന ഈ രീതികള്\u200d എളുപ്പമാണ്, പക്ഷെ സാധ്യതയാണ്: ഞങ്ങള്\u200d ഏഴ് എക്സെല്\u200d ഡാറ്റാസറ്റുകളില്\u200d നമ്മുടെ സാഹചര്യം പരീക്ഷിക്കുന്നു. അവയെല്ലാം മുകളില്\u200d 30 സ്വര്\u200dണ്ണ പ്രാര്\u200dത് നമ്മുടെ മെച്ചപ്പെട്ട മോഡല്\u200d എക്സിഎലിന്റെ അവസാനം എക്സില്\u200d 7. 9% വര്\u200dദ്ധിപ്പിക്കുന്നു. 1', 'no': 'Krysspråk- eininga som lenkjer (XEL) er oppgåva for å finna referanser i ein målspråk- kunnskapsbasen (KB) for å minna ut frå kjeldespråk- tekstar. Den første stegen på (X)EL er kandidatgenerasjonen, som hentar ei liste over tilgjengelege kandidateiningar frå målspråket KB for kvar minning. Tilnærmingar basert på ressursar frå Wikipedia har vist suksessfull i området av relativt høg ressursspråk, men desse utvidar ikkje godt til låg ressursspråk med få, viss det er noko, Wikipedia-sider. Nyleg har du vist metodar for å redusera etterspørselen for ressursar i låg ressursspråk ved å bruka ressursar i nært relaterte språk, men utviklinga er fortsatt langt bak dei høg ressurskompartirane. I denne papiret vurderer vi første problema som står med gjeldande entitetskandidatmetodar for låg ressurs XEL, og så foreslår vi tre forbedringar som (1) reduserer avkoplinga mellom entitetsnamn og oppføringar i KB, og (2) forbedrar kraftigheten av modellen til låg ressursscenarior. Metodane er enkle, men effektiv: Vi eksperimenterer med tilnærming vårt på syv XEL-datasett og finn at dei gjennomsnittlig får 16,9% i rekkjellen av topp30 gullskandidat, samanlikna med kunstbaselinjer. Vårt forbetra modell gjev også gjennomsnittlig forbetring av 7,9% i KB-nøyaktighet for end-to-end XEL. 1', 'mt': 'L-entità translingwistika li torbot (XEL) hija l-kompitu li ssib referenti f’bażi ta’ għarfien tal-lingwa fil-mira (KB) għal menzjonijiet estratti mit-testi tal-lingwa tas-sors. L-ewwel pass ta’ (X)EL huwa l-ġenerazzjoni ta’ kandidati, li tirċievi list a ta’ entitajiet kandidati plawżibbli mil-lingwa fil-mira KB għal kull referenza. Approċċi bbażati fuq riżorsi mill-Wikipedia wrew suċċess fil-qasam ta’ lingwi relattivament ta’ riżorsi għoljin, iżda dawn ma jestendux tajjeb għal lingwi b’riżorsi baxxi b’ftit, jekk ikun hemm, paġni tal-Wikipedia. Dan l-aħħar, intwera li l-metodi ta’ tagħlim ta’ trasferiment inaqqsu d-domanda għar-riżorsi fil-lingwi b’riżorsi baxxi billi jintużaw ir-riżorsi f’lingwi relatati mill-qrib, iżda l-prestazzjoni għadha ferm lura mill-kontropartijiet tagħhom b’riżorsi għoljin. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios.  Il-metodi huma sempliċi, iżda effettivi: Aħna ninsperimentaw bl-approċċ tagħna fuq seba’ settijiet ta’ dejta XEL u nsibu li jirriżultaw fi qligħ medju ta’ 16.9% f’ġbir lura ta’ l-ogħla 30 kandidat tad-deheb, meta mqabbel ma’ linji bażi l-aktar avvanzati. Il-mudell imtejjeb tagħna jagħti wkoll żieda medja ta’ 7.9% fil-preċiżjoni in-KB tal-XEL minn tmiem sa tmiem. 1', 'mn': 'Холбоотой хэлний нэгж (XEL) гэдэг бол эх үүсвэр хэлний текстүүдээс гаргасан хэлний мэдлэг суурь (КБ) дээр шинэчлэлүүдийг олох даалгавар юм. (X)EL-ын анхны алхам бол захирагчийн үеийн шагнал. Энэ нь хүн бүрт КБ-ын захирагчийн тусламжтай хүмүүсийн жагсаалт авдаг. Wikipedia-ын баялаг боловсруулалт нь харьцангуй өндөр боловсруулагдсан хэл дээр амжилттай болж байна. Гэхдээ эдгээр нь Wikipedia хуудас хэдэн бага боловсруулагдахгүй. Сүүлийн үед суралцах үйл ажиллагааны арга нь бага баялаг боловсролын хэлний хэрэгцээ багасгах боломжтой болсон. Гэхдээ үйл ажиллагаа нь тэдний өндөр боловсролын хамтрагчдын ард байдаг. Энэ цаасан дээр бид эхлээд орчин үеийн захирагчдын төлөө бага нөөцийн XEL-ын төлөө зориулагдсан асуудлыг шалгаж, дараа нь (1) биетийн нэр болон КБ-ийн нэр хоорондын холбоотой холбоотой гурван сайжруулалтыг багасгах болон (2) загварын хүчтэй байдал Эдгээр арга нь энгийн, гэхдээ үр дүнтэй: бид 7 XEL өгөгдлийн сангийн тухай туршилт хийж, тэд Top-30 алтан удирдагчдын дундаж 16.9% зарцуулдаг гэдгийг олж мэднэ. Бидний сайжруулсан загвар нь мөн КБ-ын төгсгөл-төгсгөл XEL-ын дундаж 7.9% өндөр нэмэгдүүлдэг. 1', 'pl': 'Crosslingual entity linking (XEL) to zadanie znalezienia referentów w bazie wiedzy języka docelowego (KB) dla wzmianek ekstraktowanych z tekstów językowych źródłowych. Pierwszym krokiem (X)EL jest generowanie kandydatów, które pobiera listę wiarygodnych podmiotów kandydujących z języka docelowego KB dla każdej wzmianki. Podejścia oparte na zasobach Wikipedii okazały się skuteczne w sferze języków stosunkowo wysokich zasobów, ale nie rozciągają się one dobrze na języki niskich zasobów z niewielką liczbą stron Wikipedii. Ostatnio wykazano, że metody uczenia się transferowego zmniejszają zapotrzebowanie na zasoby w językach o niskim zasobie poprzez wykorzystanie zasobów w ściśle powiązanych językach, ale wydajność wciąż pozostaje daleko za ich wysokim zasobem odpowiednikami. W niniejszym artykule najpierw oceniamy problemy, z którymi borykają się obecne metody generowania kandydatów podmiotów dla niskoszasobowego XEL, a następnie proponujemy trzy ulepszenia, które (1) zmniejszają odłączenie między wzmiankami o jednostkach a wpisami KB oraz (2) poprawiają solidność modelu do scenariuszy niskoszasobowych. Metody są proste, ale skuteczne: eksperymentujemy z naszym podejściem na siedmiu zbiorach danych XEL i stwierdzamy, że dają one średni zysk 16,9% w odzyskaniu złotych kandydatów Top-30 w porównaniu z najnowocześniejszymi liniami bazowymi. Nasz ulepszony model daje również średni przyrost 7,9% w dokładności w KB end-to-end XEL. 1', 'ro': 'Legătura interlingvă a entităților (XEL) este sarcina de a găsi referenți într-o bază de cunoștințe în limba țintă (KB) pentru mențiuni extrase din textele din limba sursă. Primul pas al (X)EL este generarea de candidați, care obține o listă de entități candidate plauzibile din limba țintă KB pentru fiecare mențiune. Abordările bazate pe resurse de pe Wikipedia s-au dovedit a fi de succes în domeniul limbilor cu resurse relativ mari, dar acestea nu se extind bine la limbile cu resurse reduse, cu puține, dacă există, pagini Wikipedia. Recent, s-a demonstrat că metodele de învățare transferată reduc cererea de resurse în limbile cu resurse reduse prin utilizarea resurselor în limbi strâns conexe, dar performanța rămâne mult în urma omologilor lor cu resurse ridicate. În această lucrare, evaluăm mai întâi problemele cu care se confruntă metodele actuale de generare a candidatelor entităților pentru XEL cu resurse reduse, apoi propunem trei îmbunătățiri care (1) reduc deconectarea dintre mențiunile entității și intrările KB și (2) îmbunătățesc robustețea modelului la scenarii cu resurse reduse. Metodele sunt simple, dar eficiente: experimentăm cu abordarea noastră pe șapte seturi de date XEL și constatăm că acestea generează un câștig mediu de 16,9% în recuperarea candidatului de aur Top-30, comparativ cu liniile de bază de ultimă generație. Modelul nostru îmbunătățit oferă, de asemenea, un câștig mediu de 7,9% în precizia in-KB a XEL end-to-end. 1', 'si': 'Name (X)EL ගේ පළමු පැත්තේ ප්\u200dරධානය තමයි ප්\u200dරධානයක් ප්\u200dරධානයක්, ඒකෙන් හැම කියන්න විශ්වාස කරන්න පුළුවන් ප්\u200dරධානයක් ල විකිපිඩියා වලින් ප්\u200dරධාන භාෂාවන් වලින් අධාරිත විකිපිඩියා වලින් ප්\u200dරධාන වලින් සමහර විශ්වාස කරලා තියෙනවා, ඒත් මේවා වික අලුත් වෙලාවේ, ඉගෙන ගන්න විදියට පෙන්වන්න පුළුවන් විදියට, අඩු සම්බන්ධ භාෂාවට අවශ්\u200dයය අඩු භාෂාවට අවශ්\u200dයය අඩු කරන්න පුළ මේ පත්තරේ අපි මුලින්ම විශ්වාස කරනවා දැන් ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් සම්බන්ධ වෙනුවෙන් ප්\u200dරශ්නය XEL විශ්වාස කරන්න, ඊට පස්සේ ප්\u200dරශ්නයක් තුනක් ප්\u200dරශ්නයක් තියෙ විධානය සාමාන්\u200dය, නමුත් ප්\u200dරභාවිත: අපි XEL දත්ත සේට් 7ක් ගැන ප්\u200dරයෝජනය කරනවා ඒ වගේම ඔවුන් සාමාන්\u200dය 16.9% විශේෂයෙන් සෝල්ඩ් 30ක් ප්\u200dරයෝ අපේ ප්\u200dරවෘත්ති මොඩේල් එක්කෙන් සාමාන්\u200dය විශේෂ 7.9% වලින් ප්\u200dරයෝජනයක් තියෙනවා. 1', 'sr': 'Krozjezička entiteta koja povezuje (XEL) je zadatak pronalaženja referenta u bazi znanja ciljevih jezika (KB) za spomena izvučene iz teksta izvornog jezika. Prvi korak (X)EL-a je generacija kandidata, koja dobija spisak uvjerljivih kandidatskih entitata iz ciljnog jezika KB za svaki spomen. Pristupi na temelju resursa iz Wikipedije dokazali su uspešni u regiji relativno visokih jezika resursa, ali to se ne proširi dobro na jezike niskih resursa sa nekoliko, ako ima, Wikipedijskih stranica. Nedavno su pokazali metode prevođenja učenja kako bi smanjili zahtev za resursima na jezicima niskih resursa koristeći resurse u bliskom odnosim jezicima, ali izvodnja još uvijek ostaje daleko iza njihovih kolega sa visokim resursima. U ovom papiru, prvo procjenjujemo probleme s kojima se suočavaju metode generacije kandidata za manje resurse XEL, a zatim predlažemo tri poboljšanja koje (1) smanjuju diskontaciju između spominjanja entiteta i ulaska KB-a, i (2) poboljšavaju robotu modela do scenarija niskih resursa. Metode su jednostavne, ali efikasne: eksperimentiramo sa našim pristupom na sedam XEL podataka i otkrijemo da su dobili prosječnu dobit od 16,9% u sjećanju zlatnog kandidata Top-30 u usporedbi sa početnim linijama umjetnosti. Naš poboljšan model takođe pruža prosječan dobitak od 7,9% u KB tačnosti kraja do kraja XEL-a. 1', 'so': "XEL waa shaqo ka raadinta dadka aqoonta luqada ku qoran (KB) oo loogu talogaley qoraalaha afka nooc ah. Xarunta ugu horeeya (X)EL waa qarniga kandida ah, kaas oo ka helaya list of wasaarayaal suurtagal ah oo laga helaa qoraal ka mid ah KB in kastoo lagu hadlo. Approaches based on resources from Wikipedia have proven successful in the realm of relatively high-resource languages, but these do not extend well to low-resource languages with few, if any, Wikipedia pages.  Muddo u dhowaad waxaa loo muujiyey hababka waxbarashada si ay u hoosayso baahida ku habboon luqadaha hoose-resourceyaasha, si ay ugu isticmaalaan hantida luuqadaha ku saabsan ee dhow, laakiin tababarku weli waxay ka dambeysaa dhinacyada rasmigooda sare. Qoraalkan waxaynu marka hore qiimeynaynaa dhibaatooyin ay ka hor jeedaan qaababka midowga dhaqaalaha XEL ee hoos-resource, kadibna waxaynu soo jeedaynaa saddex hagaajiya in (1) hoos u dhigo xiriirka u dhexeeya macluumaadka maandooriyaha iyo galayaasha KB, iyo (2) horumarinta dhaqaalaha modelka si uu u beddelo sawirada hoose-resource. Isticmaalku waa fudud, laakiin waa faa’iido: waxaynu ku tijaabinaynaa qaabilaada todobada XEL-databaseyaasha, waxaynu ogaannaa in ay ka soo baxaan faa'iido guud 16.9% oo ku jirta xasuusta kandidada dahabka Top-30, iyadoo la barbarbaranaya xaalad-qoriga ah. Tusaalkayaga horumarinta ah wuxuu sidoo kale dhalaa koritaanka ugu dhexeeya 7.9% ee saxda ugu dambaysta XEL. 1", 'sv': 'Cross-lingual entity linking (XEL) är uppgiften att hitta referenter i en kunskapsbas för målspråk (KB) för omnämnanden från källspråkstexter. Det första steget i (X)EL är kandidatgenerering, som hämtar en lista över sannolika kandidatenheter från målspråkets KB för varje omnämnande. Metoder baserade på resurser från Wikipedia har visat sig vara framgångsrika inom sfären av relativt höga resurser språk, men dessa sträcker sig inte bra till lågresursspråk med få, om några, Wikipedia sidor. Nyligen har överföringsinlärningsmetoder visat sig minska efterfrågan på resurser på lågresursspråk genom att använda resurser på nära besläktade språk, men prestandan ligger fortfarande långt efter deras högresursmotsvarigheter. I denna uppsats bedömer vi först problemen med nuvarande metoder för generering av entitetskandidater för XEL med låg resurs, och föreslår sedan tre förbättringar som (1) minskar kopplingen mellan entitetsomnämnanden och KB poster, och (2) förbättrar modellens robusthet till lågresursscenarier. Metoderna är enkla, men effektiva: Vi experimenterar med vårt tillvägagångssätt på sju XEL-dataset och finner att de ger en genomsnittlig vinst på 16,9% i Top-30 guldkandidatåterkallelse, jämfört med state-of-the-art baslinjer. Vår förbättrade modell ger också en genomsnittlig vinst på 7,9% i in-KB noggrannhet för end-to-end XEL. 1', 'ta': 'கிராஸ்- மொழி பொருள் இணைக்கப்பட்டது (XEL) மூலத்தின் மொழி உரைகளிலிருந்து வெளியேற்றப்பட்ட குறிப்புகளின் செயல் (X)EL இன் முதல் படி தான் தேர்ந்தெடுக்கப்பட்ட தலைமுறை, அது ஒவ்வொரு குறிப்புக்கும் சேர்க்கக்கூடிய தேர்ந்தெடுக்கப்படும் வி விகிபிடியாவில் இருந்து வளங்களை அடிப்படையில் தெளிவாக்கப்பட்டுள்ளது சார்ந்த உயர்ந்த மூலத்தின் மொழிகளின் ராஜாவில் வெற்றிகரமாக, ஆனால் இது சில ம சமீபத்தில், கற்றல் முறைகள் குறைந்த மூலங்களில் வளங்களின் தேவையை குறைக்க காண்பிக்கப்பட்டுள்ளது, மேலும் முறைமையில் தொடர்புடைய மொழிகளை பயன்படுத்தி மூ இந்த காக்கியத்தில், நாம் முதலில் தற்போதைய பொருள் தேர்ந்தெடுக்கப்பட்ட பிரச்சனைகளை முன்னோக்குகிறோம் குறைந்த மூலத்திற்கான உருவாக்க முறைமைகளை பின்னர் மூன்று முன்னேற்று முறைகள் எளிதாக இருக்கும், ஆனால் வெறுப்பாகும்: நாம் ஏழு XEL தரவுத்தளங்களில் சோதனைப்படுத்தினோம் மற்றும் அவர்கள் மேல்-30 தங்க வார்த்தையாளர் நினைவில் 16 எங்கள் மேம்படுத்தப்பட்ட மாதிரி ஒரு சராசரியான அதிகாரத்தை கொடுக்கிறது KB-ல் உள்ள சரியான சரியான XEL-ல் 7. 9% கிடைக்கும 1', 'ur': 'کرس-زبان ایٹینٹ لینک (XEL) کا کام ہے کہ ایک موجود زبان علم بنسس (KB) میں سراسر زبان متن سے اٹھایا گیا ہے۔ (X)EL کی پہلی قدم ہے کانڈیٹ نسل، جو ہر ذکر کے لئے موجود ہونے والی کانڈیٹ ایٹنیٹوں کی لکھی حاصل کرتا ہے. ویکیپیڈیا کے سرمایہ پر بنیاد ہوئے تقریبا بہت بالا سرمایہ زبانوں کی ملک میں موفق ہوگئے ہیں لیکن یہ کم سرمایہ زبانوں کے ساتھ بہت اچھی طرح نہیں پھیلاتے، اگر ایسا ہو تو ویکیپیڈیا صفحے کے ساتھ۔ اچھے سے، ترنسفور سیکھنے کے طریقے دکھائے گئے ہیں کہ کم منبع زبانوں میں سرمایہ کی خواہش کم کر دیں، لیکن عملکرد ان کے بلند منبع کنٹرپاٹوں کے پیچھے دور رہی ہے. اس کاغذ میں ہم پہلی بار موجود انٹیٹی کاندینٹ پیدائٹ کے مطابق مشکلات کی آزمائش کریں گے، پھر تین سودائش کی پیشنهاد کریں گے جو (1) انٹیٹی کے ذریعے اور کیب انٹریوں کے درمیان تفریق کو کم کریں گے، اور (2) مدل کی مضبوطی کو کم منبع سناریوں تک بڑھائیں گے۔ یہ طریقے ساده ہیں، لیکن اثبات ہیں: ہم سات XEL ڈیٹسٹ پر اپنے طریقے سے آزمائش کرتے ہیں اور دیکھتے ہیں کہ وہ بال-30 سونے کے کنڈیٹ کے ذریعے 16.9% کے متوسط فائدہ حاصل کر رہے ہیں، ہمارے بہترین موڈل نے کب میں آخر-to-end XEL کے مطابق 7.9% کے متوسط فائدہ اٹھاتا ہے۔ 1" (msgctxt: "panel:showusername") to "1', 'uz': "Name EL (X)EL' ning birinchi qadam - kandida yaratish, har bir necha xil uchun playin boʻladigan kandida fayllarning roʻyxatini olib tashlash. Wikipediya Resources asosida murakkablar qismlari juda katta rasmlar tilida muvaffaqiyatli topildi, lekin bu soʻzlarni bir nechta o'z tillarga yozib olib kelmaydi, agar hech qanday bo'lsa, Wikipedia sahifalar. Yaqinda o'rganishni o'rganish usullarini ko'rsatish mumkin, qisqa rasmlar tilida rasmlarni ishlatish mumkin, lekin ishlatish imkoniyatlarining eng yuqori manba kompyuterlaridan uzaydi. Bu qogʻozda, biz joriy tashkilot xolosi XEL uchun qo'llangan muammolarni tasavvur qilamiz, keyin uchta taʼminlov qilamiz, bu tub narsa xossalari va KB yozuvchilari орасида bogʻlanish imkoniyatini kamaytirish va (2) modelning ko'payligini kamaytirish imkoniyatini yaxshi ko'radi. Bu usullar oddiy, lekin effektiv: Biz yetti XEL maʼlumotlar satrlariga qaramamiz va ular yuqori-30 yuqori gull kandida 16.9% yordam sotuvchiligi bilan bir holat asosiy asosiy sonlariga qaraydi. Bizning o'zgarilgan modelimizda ko'paytirilgan XEL'ning oxiriga 7.9% ko'payishimizga ega bo'ladi. 1", 'vi': 'Liên kết ngôn ngữ rộng (Xâu) là nhiệm vụ tìm người đọc tại một căn cứ kiến thức ngôn ngữ đích (KB) để tìm những tên trích ra từ các văn bản ngôn ngữ nguồn. Bước đầu tiên của (X)el là thế hệ ứng cử viên, nó lấy được danh sách các thực thể ứng cử viên hợp lý từ ngôn ngữ đích KB cho mỗi lần được nhắc đến. Phương pháp dựa trên nguồn tài nguyên của Wikipedia đã chứng tỏ sự thành công trong lĩnh vực ngôn ngữ có nhiều nguồn lực tương đối lớn, nhưng những phương pháp này không tốt cho ngôn ngữ có rất ít, nếu có, trang Wikipedia. Gần đây, các phương pháp chuyển sang học đã được cho thấy để giảm nhu cầu nguồn lực trong ngôn ngữ ít tài nguyên bằng cách sử dụng nguồn lực ở các ngôn ngữ liên quan, nhưng kết quả vẫn chậm hơn so với các đối tác giàu có. Trong tờ giấy này, chúng ta đầu tiên đánh giá các vấn đề do các phương pháp tạo ứng cử viên hiện thời đối mặt với các phương pháp lưu lượng ít tài nguyên X, sau đó đề xuất ba cải tiến mà (1) giảm sự ngắt kết nối giữa các thực thể đề cập và các mục nhập của KB, và (2) nâng cao độ bền vững của mô hình với viễn cảnh ít tài nguyên. Phương pháp này đơn giản, nhưng hiệu quả: thử nghiệm với phương pháp của chúng ta trên bảy tập tin X-quang và tìm thấy chúng thu được một lợi nhuận trung bình trong triệu tập ứng viên vàng Top-30, so với nền tảng thời đại. Hệ thống cải tiến của chúng ta cũng cho lợi nhuận trung bình của 7.9=-trong-KB chính xác cao độ cao liên kết X-quang. L', 'bg': 'Междуезичното свързване на единици (КБ) е задачата да се намерят референти в база знания за целеви езици (КБ) за споменавания, извлечени от текстове на изходния език. Първата стъпка на (Х)EL е генериране на кандидати, което извлича списък на правдоподобни кандидатски субекти от КБ на целевия език за всяко споменаване. Подходите, базирани на ресурси от Уикипедия, се оказват успешни в областта на относително високоресурсните езици, но те не се разпространяват добре към езиците с нискоресурсни ресурси, с малко, ако има такива страници в Уикипедия. Напоследък методите за трансферно обучение намаляват търсенето на ресурси в езиците с нисък ресурс чрез използване на ресурси в тясно свързани езици, но резултатите все още изостават далеч от техните високоресурсни колеги. В тази статия първо оценяваме проблемите, пред които са изправени настоящите методи за генериране на кандидати за обекти за нискоресурсни КСЕЛ, след което предлагаме три подобрения, които (1) намаляват прекъсването между споменаването на обекти и записите в КБ и (2) подобряват устойчивостта на модела до сценарии с ниски ресурсни ресурси. Методите са прости, но ефективни: експериментираме с нашия подход върху седем набора от данни и откриваме, че те дават средна печалба от 16,9% в топ-30 златни кандидати, в сравнение с най-съвременните базови линии. Нашият подобрен модел също така дава средно увеличение от 7,9% в точността в КБ на КСЕЛ от край до край. 1', 'nl': "Cross-lingual entity linking (XEL) is de taak om referenten te vinden in een doeltaalkennisbank (KB) voor vermeldingen uit brontaalteksten. De eerste stap van (X)EL is het genereren van kandidaten, waarbij voor elke vermelding een lijst van plausibele kandidaat entiteiten uit de doeltaal KB wordt opgehaald. Aanpak's gebaseerd op bronnen uit Wikipedia zijn succesvol gebleken op het gebied van relatief hoge resource talen, maar deze strekken zich niet uit tot low resource talen met weinig of geen Wikipedia pagina's. Onlangs is aangetoond dat transferleermethoden de vraag naar hulpbronnen in de low-resource talen verminderen door gebruik te maken van hulpbronnen in nauw verwante talen, maar de prestaties blijven nog steeds ver achter bij hun high-resource tegenhangers. In dit artikel evalueren we eerst de problemen waarmee de huidige methoden voor het genereren van kandidaten voor entiteiten voor XEL met lage resources worden geconfronteerd, stellen we vervolgens drie verbeteringen voor die (1) de ontkoppeling tussen entiteitsvermeldingen en KB-vermeldingen verminderen, en (2) de robuustheid van het model ten opzichte van scenario's met lage resources verbeteren. De methoden zijn eenvoudig, maar effectief: we experimenteren met onze aanpak op zeven XEL datasets en vinden dat ze een gemiddelde winst opleveren van 16,9% in Top-30 gold candidate recall, vergeleken met state-of-the-art baselines. Ons verbeterde model levert ook een gemiddelde winst van 7,9% op in-KB nauwkeurigheid van end-to-end XEL. 1", 'hr': 'Krozjezička entiteta povezanja (XEL) je zadatak pronalaženja referenta u bazi znanja ciljeva jezika (KB) za spomenuti izvučene iz teksta izvornog jezika. Prvi korak (X)EL-a je generacija kandidata, koja prima popis uvjerljivih kandidatskih entitata iz ciljnog jezika KB za svaki spomen. Pristupi na temelju resursa iz Wikipedia dokazali su uspješni u regiji relativno visokih jezika resursa, ali to se ne proširi dobro na jezike niskih resursa sa malo, ako ima, Wikipedia stranica. Nedavno su pokazali metode prijenosnog učenja kako bi smanjili zahtjev za resursima na jezicima niskih resursa korištenjem resursa u bliskim jezicima, ali izvodnja još uvijek ostaje daleko iza njihovih kolega sa visokim resursima. U ovom papiru, prvo procjenjujemo probleme s kojima se suočavaju metode generacije kandidata za manje resurse XEL, a zatim predlažemo tri poboljšanja koje (1) smanjuju isključenje između spominjanja subjekta i ulaska KB-a, i (2) poboljšavaju snagu modela u scenarije niske resurse. Metode su jednostavne, ali učinkovite: eksperimentiramo svoj pristup na sedam podataka XEL-a i otkrijemo da su dobili prosječni dobit od 16,9% u sjećanju zlatnog kandidata Top-30 u usporedbi s početnim linijama umjetnosti. Naš poboljšan model također pruža prosječan dobitak od 7,9% u KB preciznosti kraja do kraja XEL-a. 1', 'da': 'Cross-lingual entity linking (XEL) er opgaven at finde referenter i en målsproget vidensbase (KB) for omtaler udvundet fra kildesprogede tekster. Det første trin i (X)EL er kandidatgenerering, som henter en liste over sandsynlige kandidatenheder fra målsproget KB for hver omtale. Tilgange baseret på ressourcer fra Wikipedia har vist sig at være vellykkede inden for relativt høj ressource sprog, men disse strækker sig ikke godt til lav ressource sprog med få, hvis nogen, Wikipedia-sider. For nylig har overførselsmetoder vist sig at reducere efterspørgslen efter ressourcer på de lave ressourcer sprog ved at udnytte ressourcer på nært beslægtede sprog, men resultaterne halter stadig langt bagefter deres høje ressourcer modparter. I denne artikel vurderer vi først de problemer, som de nuværende metoder til generering af enhedskandidater står over for for for XEL med lav ressource, og foreslår derefter tre forbedringer, der (1) reducerer afbrydelsen mellem enhedsnævnelser og KB-poster, og (2) forbedrer modellens robusthed til scenarier med lav ressource. Metoderne er enkle, men effektive: Vi eksperimenterer med vores tilgang på syv XEL datasæt og finder ud af, at de giver en gennemsnitlig gevinst på 16,9% i top-30 guldkandidattilbagekaldelse sammenlignet med state-of-the-art basislinjer. Vores forbedrede model giver også en gennemsnitlig gevinst på 7,9% i in-KB nøjagtighed af end-to-end XEL. 1', 'de': 'Cross-lingual entity linking (XEL) ist die Aufgabe, Referenzen in einer zielsprachlichen Wissensdatenbank (KB) für Erwähnungen aus quellsprachlichen Texten zu finden. Der erste Schritt von (X)EL ist die Kandidatengenerierung, die für jede Erwähnung eine Liste plausibler Kandidatentitäten aus der zielsprachlichen KB abruft. Ansätze, die auf Ressourcen aus Wikipedia basieren, haben sich im Bereich der relativ ressourcenreichen Sprachen bewährt, aber diese erstrecken sich nicht gut auf ressourcenarme Sprachen mit wenigen, wenn überhaupt, Wikipedia-Seiten. In letzter Zeit hat sich gezeigt, dass Transferlernmethoden den Bedarf an Ressourcen in den ressourcenarmen Sprachen verringern, indem Ressourcen in eng verwandten Sprachen genutzt werden, aber die Leistung liegt noch weit hinter den ressourcenreichen Gegenstücken zurück. In diesem Beitrag bewerten wir zunächst die Probleme, mit denen aktuelle Entitätskandidaten-Generierungsmethoden für ressourcenarme XEL konfrontiert sind, und schlagen dann drei Verbesserungen vor, die (1) die Trennung zwischen Entitätserwähnungen und KB-Einträgen verringern und (2) die Robustheit des Modells gegenüber ressourcenarmen Szenarien verbessern. Die Methoden sind einfach, aber effektiv: Wir experimentieren mit unserem Ansatz an sieben XEL-Datensätzen und stellen fest, dass sie einen durchschnittlichen Gewinn von 16,9% im Top-30 Gold Candidate Recall liefern, verglichen mit modernsten Baselines. Unser verbessertes Modell liefert auch einen durchschnittlichen Gewinn von 7,9% in der In-KB Genauigkeit von End-to-End XEL. 1', 'id': 'Entitas saling bahasa yang menghubungkan (XEL) adalah tugas untuk menemukan referen di dasar pengetahuan bahasa-sasaran (KB) untuk sebutan yang dikeluarkan dari teks bahasa-sumber. Langkah pertama dari (X)EL adalah generasi kandidat, yang mengambil daftar entitas kandidat yang dapat dipercaya dari bahasa-target KB untuk setiap sebutan. Pendekatan berdasarkan sumber daya dari Wikipedia telah terbukti sukses dalam bidang bahasa sumber daya relatif tinggi, tetapi ini tidak memperluas dengan baik ke bahasa sumber daya rendah dengan beberapa, jika ada, halaman Wikipedia. Baru-baru ini, metode belajar transfer telah menunjukkan untuk mengurangi permintaan sumber daya dalam bahasa sumber daya rendah dengan menggunakan sumber daya dalam bahasa yang berhubungan dekat, tetapi prestasi masih jauh di belakang rekan sumber daya tinggi mereka. Dalam kertas ini, kita pertama-tama menilai masalah yang dihadapkan oleh metode generasi kandidat entitas saat ini untuk sumber daya rendah XEL, kemudian mengusulkan tiga peningkatan yang (1) mengurangi ketidakberhubungan antara sebutan entitas dan masukan KB, dan (2) meningkatkan kepekatan model untuk skenario sumber daya rendah. Metodi ini sederhana, tapi efektif: kita eksperimen dengan pendekatan kita pada tujuh set data XEL dan menemukan bahwa mereka menghasilkan keuntungan rata-rata 16,9% dalam rekaman kandidat emas Top-30, dibandingkan dengan garis dasar yang terbaik. Our improved model also yields an average gain of 7.9% in in-KB accuracy of end-to-end XEL. 1', 'ko': '크로스 언어 엔티티 링크(XEL)의 임무는 대상 언어 지식 라이브러리(KB)에서 원본 언어 텍스트에서 추출한 참조를 찾는 것입니다.(X) EL의 첫 번째 단계는 대상 언어 지식 라이브러리에서 각 언급에 대해 합리적인 후보 엔티티 목록을 검색하는 후보 생성입니다.위키백과 자원을 바탕으로 하는 방법은 자원이 상대적으로 높은 언어 분야에서 성공한 것으로 증명되었지만 이러한 방법은 자원이 낮은 언어로 잘 확장되지 못하고 이런 언어는 위키백과 페이지가 거의 없다.최근 몇 년 동안 이동 학습 방법은 밀접한 관련 언어의 자원을 이용하여 저자원 언어의 자원 수요를 감소시켰지만 그 성능은 여전히 고자원 언어보다 훨씬 뒤떨어졌다.본고에서 우리는 먼저 현재 저자원 픽셀에 대한 실체 후보 생성 방법이 직면하고 있는 문제점을 평가한 다음에 세 가지 개선을 제시했다. (1) 실체 언급과 지식 라이브러리 항목 간의 불균형을 줄이고 (2) 모델이 저자원 장면에 대한 노봉성을 향상시킨다.이러한 방법은 간단하지만 효과가 있다. 우리는 7개의 XEL 데이터 집합에서 실험을 진행하였는데, 가장 선진적인 기선에 비해 30위권 금후보 리콜에서 평균 16.9%의 이익을 얻었다는 것을 발견하였다.우리가 개선한 모델은 끝에서 끝까지 픽셀의 KB 정밀도에서 7.9%의 평균 이득을 얻을 수 있다.1', 'fa': 'واحد متصل زبان (XEL) وظیفه پیدا کردن گزارش\u200cها در پایگاه علم هدف زبان (KB) برای اشاره\u200cهای استخراج از متن\u200cهای زبان منبع است. اولین مرحله از (X)EL نسل کاندیده است که یک لیست عنوان کاندیده\u200cهای قابل اعتماد از کلی زبان هدف برای هر اشاره می\u200cگیرد. نزدیک\u200cها بر اساس منابع از ویکیپدیا در سلطنت زبانهای نسبتا زیادی منابع ثابت شده\u200cاند، ولی این\u200cها به زبانهای کمی منابع با کمی، اگر باشد، صفحه ویکیپدیا خیلی خوب نمی\u200cافزایند. تازگی، روش\u200cهای یادگیری انتقال برای کاهش درخواست منابع در زبانهای کم منابع نشان داده شده است، با استفاده از منابع در زبانهای نزدیک ارتباط داشته باشد، ولی عملکرد هنوز پشت همکاران منابع بالا باقی مانده است. در این کاغذ، اولین بار مشکلات را که توسط طریق نسل کاندیداتی کاندیداتی فعلی برای کم منابع XEL روبرو می\u200cکنیم، سپس سه پیشنهاد پیشنهاد می\u200cکنیم که (۱) قطعه\u200cای بین یادآوری\u200cهای واحد و ورودهای KB را کاهش می\u200cدهد، و (۲) قوی مدل را به سناریو کم منابع کاه روش\u200cها ساده\u200cاند، اما موثر است: ما با روش\u200cهایمان روی هفت تنظیم داده\u200cهای XEL آزمایش می\u200cکنیم و می\u200cبینیم که آنها در مقایسه با خط\u200cهای پایین\u200cهنری، یک سود متوسط از ۱۶.۹ درصد از کاندیده\u200cهای طلا بالا-۳۰ در مقایسه با خط\u200cهای پا Model improved ما همچنین یک نفع متوسط از 7.9 درصد در دقیق آخر-به-پایان XEL را در کیب ب می دهد. ۱', 'sw': 'Ujumbe wa lugha inayounganisha (XEL) ni jukumu la kuwatafuta watu katika msingi wa maarifa ya lugha ya lengo (KB) kwa ajili ya taja zilizotolewa kutoka kwenye maandishi ya lugha ya asili. Hatua ya kwanza ya (X)EL ni kizazi cha mgombea, ambacho kinapata orodha ya vitu vya wagombea kutoka KB kwa kila taja. Kutokana na rasilimali kutoka Wikipedia imethibitisha kuwa na mafanikio katika ufalme wa lugha za rasilimali zilizo juu, lakini hizi hazitaongezeka vizuri kwa lugha ndogo ya rasilimali kwa machache, kama yoyote, kurasa za Wikipedia. Hivi karibuni, mbinu za kuhamisha elimu zimeonyeshwa kupunguza madai ya rasilimali kwa lugha ndogo ya rasilimali kwa kutumia rasilimali kwa lugha zinazohusiana na karibu, lakini utendaji bado unabaki mbali na wenzao wa rasilimali za juu. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios.  mbinu hizo ni rahisi, lakini ni yenye ufanisi: tunajaribu hatua yetu kwa takwimu za data saba za XEL na tunagundua kwamba wanatoa ongezeko la wastani wa asilimia 16.9 katika kumbukumbu ya mgombea wa dhahabu ya juu-30, ukilinganisha na hali ya msingi wa sanaa. Mradi wetu ulioboreshwa pia unaleta ongezeko la wastani wa asilimia 7.9 katika uhakika wa KB wa mwisho wa XEL. 1', 'tr': "(XEL) çeşme dilinde sözleri tapmak üçin bir maksadat-dil bilim tabasynda (KB) sözleri tapmak üçin zady. (X)EL'iň ilkinji adım kandidat döredilmesi, bu iňe her çykyş üçin Ullanylan kandidat zatlaryň listini alýar. Wikipediýany ň çeşmelerine daýanýan ýagdaýlar relativ ýokary çeşme dilleriniň ýerinde başarılygy bardylar, ýöne bu ýagdaýlar, Wikipediýa sahypalary kiçi-çeşme bilen azajyk hasaplamaýarlar. Soňky wagtlarda, öwrenme metodlary ýüzünde resurslary iň az resurslarda ýüz tutan dillerde çeşmeler ullanarak görkezildi, ýöne täsirler iň ýokary çeşmeleriniň arkasynda daşary galýar. Bu kagyzda, biz ilkinji gezek häzirki entitet kandidýasynyň iň-ressurs XEL üçin kynçylyk taýýarlanan meselelerini çözdirip, soňra üç üýtgeşmeleri (1) birnäçe we KB girdileriniň arasyndaky baglaýyşyny düşürmegi teklip edip, we (2) nusganyň güýçligini iň-ressurs senaryoňa ýig Metiller basit, ýöne täsirli: biz 7-nji XEL veri setirlerinde ýagdaýymyzy barlaýarys we olaryň üst-30 altyn kandidatyň ýagdaýynda ortalama gazanlygyny tapýarys. Biziň gelişmiş nusgamyz hem KB-iň soňunda XEL'iň ýüzünde ortalama gazanlygy bar. 1", 'af': "Kruistale entiteit wat koppel (XEL) is die taak van verwysing in 'n doel- taal kennis basis (Kb) vir verwysing uitgevoer van bron- taal teks. Die eerste stap van (X) EL is kandidate generasie, wat 'n lys van persoonlike kandidate entiteite ontvang van die doel- taal Kb vir elke bepaal. Aangaande gebaseerde op hulpbronne van Wikipedia het suksesvol bevestig in die realm van relativief hoë hulpbronne tale, maar hierdie verleng nie goed na lae hulpbronne tale met paar, indien enige, Wikipedia bladsye nie. Onlangs het die oordragmetodes vertoon om die versoek vir hulpbronne in die lae hulpbronne tale te verklein deur hulpbronne in naby verwante tale te gebruik, maar die prestasie staan nog ver agter hul hoë hulpbronne teen hulpbronne. In hierdie papier, ons vurig eerste die probleems met die aangesig van huidige entiteite kandidate generasie metodes vir lae hulpbron XEL, dan voorstel drie verbeteringe wat (1) die ontkoppeling tussen entiteite mentioneer en Kb inskrywings verminder, en (2) verbeter die kragtigheid van die model na lae hulpbron scenarios. Die metodes is eenvoudig, maar effektief: Ons eksperimenteer met ons toegang op sewe XEL datastelle en vind dat hulle 'n gemiddelde verkry van 16.9% in die top-30 goud kandidate herroep, vergelyk met staat van die kuns basisline. Ons verbeterde model gee ook 'n gemiddelde verskaf van 7.9% in Kb-presies van end-to-end XEL. 1", 'sq': 'Njësia ndërgjuhësore që lidh (XEL) është detyra e gjetjes së referentëve në një bazë njohurie gjuhës-objektiv (KB) për përmendimet e nxjerra nga tekstet e gjuhës-burimi. Hapi i parë i (X)EL është gjenerata kandidate, e cila merr një list ë të njësive të besueshme kandidate nga gjuha-objektiv KB për çdo përmendim. Approaches based on resources from Wikipedia have proven successful in the realm of relatively high-resource languages, but these do not extend well to low-resource languages with few, if any, Wikipedia pages. Kohët e fundit, metodat e transferimit të mësimit janë treguar të reduktojnë kërkesën për burime në gjuhët me burime të ulëta duke përdorur burimet në gjuhët e lidhura ngushtë, por performanca mbetet ende shumë prapa homologëve të tyre me burime të larta. Në këtë letër, ne së pari vlerësojmë problemet me të cilat ndeshen metodat e gjenerimit të njësisë së tanishme kandidate për burime të ulëta XEL, pastaj propozojmë tre përmirësime që (1) reduktojnë dallimin midis përmendimeve të njësisë dhe hyrjeve të KB-së dhe (2) përmirësojnë fuqinë e modelit në skenarët e burimeve të ulëta. Metodat janë të thjeshta, por efektive: ne eksperimentojmë me qasjen tonë në shtatë grupe të dhënash XEL dhe gjejmë se ato japin një fitim mesatar prej 16.9% në thirrjen e kandidatëve të art ë të lartë, krahasuar me linjat bazë më të moderne. Modeli ynë i përmirësuar jep gjithashtu një fitim mesatar prej 7.9% në saktësinë në KB të XEL nga fundi në fund. 1', 'am': 'የቋንቋ-ቋንቋ አካባቢ (XEL) የሚታያየው መልዕክት ነው፡፡  EL (X)EL የፊተኛው ደረጃ የአካባቢ ትውልድ ነው፤ ይህም የአካባቢ ቋንቋ KB የተደረገ የአካባቢ አካላትን ዝርዝር የሚያስፈልግ ነው፡፡ ከWikipedia ሀብት በመሠረት ላይ የተገኘ መግቢያዎች በአካባቢው ከፍ-resource ቋንቋዎች መንግሥት ውስጥ አግኝተዋል፤ ነገር ግን እነዚህ በጥቂቶች ቢሆኑ Wikipedia ገጾች በጥቂቶች አይዘጋጁም፡፡ በቅርብ ዘመን፣ የትምህርት ሥርዓት ማቀናቀል፣ የሀብትን ፈቃድ በአቅራቢያ በተቃራኒ ቋንቋዎች በመጠቀም፣ ነገር ግን የደረጃው ክፍል በተቃራኒ ቋንቋዎች ላይ ይቆያል፡፡ በዚህ ጽሑፍ፣ በመጀመሪያ የአሁኑ አካባቢ የክፍለ ትውልድ አካባቢ የሆኑን ትውልድ ልማድ የደረሰባቸውን ጉዳይ እና የኪB ግንኙነትን የሚያጎድል እና (2) የሞዴላውን የዝቅተኛ የክፍለ ክፍለ ዕይታ እንዲያበጅል ሦስት ድጋፍ ያሻራል፡፡ ዘዴዎች ቀላል ነገር ግን የሚጠይቁ ናቸው፤ በሰባት XEL ዳታተሮች ላይ በመፈተናችን እና በሀገር-የዐርስ-አርእስት መሠረት ላይ 16.9 በመቶ የሚታሰናክል ጥቅም እንዲያሰናከል እናገኛለን፡፡ Our improved model also yields an average gain of 7.9% in in-KB accuracy of end-to-end XEL. 1', 'hy': 'XEL ը (XELը) կապող միջլեզվային առանձնահատկությունն է հանձնարարություններ գտնելու նպատակային լեզվի գիտելիքի հիմքում (ԿԲ) աղբյուր լեզվի տեքստերից հանված նշանների համար: (X)EL ի առաջին քայլը թեկնածուների սերունդն է, որը յուրաքանչյուր հաղորդակցության համար ստանում է հավատալի թեկնածուների միավորների ցուցակ նպատակային լեզվից ԿԲ: Վիքիփեդիայի ռեսուրսների վրա հիմնված մոտեցումները հաջողակ են ապացուցել համեմատաբար բարձր ռեսուրսներ ունեցող լեզուների ոլորտում, բայց դրանք լավ չեն ընդլայնվում ցածր ռեսուրսներ ունեցող լեզուների հետ, եթե կա, Վիքիփեդիայի է Վերջերս ցույց է տվել, որ ուսուցման փոխանցման մեթոդները նվազեցնում են ցածր ռեսուրսների լեզուներում պահանջը, օգտագործելով ռեսուրսները սերտորեն կապված լեզուներում, բայց արդյունքները դեռևս շատ են մնում իրենց բարձր ռեսուրսների համագործակ Այս թղթի մեջ մենք սկզբում գնահատում ենք այն խնդիրները, որոնց հանդիպում են ներկայիս կազմակերպությունների թեկնածուների ստեղծման մեթոդները ցածր ռեսուրսների XEL ի համար, հետո առաջարկում ենք երեք բարելավումներ, որոնք (1) նվազեցնում են կազմակերպությունների և ԿԲ-ի գրքերի միջև անկապ Մեթոդները պարզ են, բայց արդյունավետ: Մենք փորձում ենք մեր մոտեցումը XEL ի եօթը տվյալների համակարգերի վրա և հայտնաբերում ենք, որ դրանք հասնում են 16.9 տոկոսի միջին շահույթ ամենաբարձր 30-ի ոսկու թեկնածուների վերադարձման մեջ, համեմատած ամենաբարձր Մեր բարելավված մոդելը նաև 7.9 տոկոսի միջին շահույթ է ստանում XEL ի վերջ-վերջ ճշգրտության մեջ: 1', 'az': 'XEL ilə bağlanan çoxlu dil məlumatı mənbə dil məlumatından çıxarılan məlumatları üçün məqsəd dil bilgi bazında referans tapmaq məlumatıdır. (X)EL\'nin ilk adı kandidát nəsilidir. Bu, hər bir anı üçün məqsəd dilindən sadəcə olaraq faydalanabilir kandidát nəsillərinin listesini alır. Wikipedia tərəfindən istifadə edilən məlumatlar çox yüksək ressurs dillərinin mülkündə başarılı oldu, amma bunlar Wikipedia sayfaları ilə çox az ressurs dillərinə çox uzatmaz. Son zamanlarda, öyrənmə metodlarının istifadə edilməsi üçün istifadə edilmişdir ki, tükəncü ressurs dillərində istifadə etmək üçün istifadə edilmişdir, amma performans hələ də yüksək ressurs yoldaşlarının arxasındadır. Bu kağıtda ilk dəfə hökmünün XEL-nin düşük kaynaqlar üçün hazırkı entitə kandidát nəsli metodlarının qarşısındakı problemlərini təmin edirik, sonra (1) bir entitə yaddaşları ilə KB girişi arasındakı bağlantı azaltmağı və (2) modelinin qüvvətini düşük kaynaqlar scenariyalarına dəyişdiririk. Bu metodlar basit, amma etkilidir: Biz yeddi XEL veri qutusu ilə təcrübə edirik və onların Top-30 altın kandidatının yadına salınması üçün ortalama 16,9%-in art ıqlığını görürük. Bizim yaxşılaşdırılmış modellərimiz həmçin in XEL-in sona qədər bitmiş XEL-nin ortalama %7,9 qədər artırır. 1" (msgctxt: "panel:showusername") to "1', 'bn': 'উৎস-ভাষার টেক্সট থেকে বের করা উল্লেখের জন্য ক্রস-ভাষায় লিঙ্কিং বস্তু (XEL) একটি লক্ষ্য-ভাষা জ্ঞানের বেস (কেবি) লিঙ্ক করার কাজ। (এক্স)EL এর প্রথম পদক্ষেপ হচ্ছে প্রার্থী প্রজন্ম, যা প্রত্যেক উল্লেখের জন্য প্রার্থী কেবি থেকে প্রার্থীর তালিকা পুনরায় উইকিপিডিয়ার সম্পদ ভিত্তিক ভিত্তিক প্রতিযোগিতা প্রমাণ করেছে যেখানে উইকিপিডিয়া পাতাগুলো সম্পূর্ণ উচ্চমূল্যের রাজ্যে সফল হয়েছে, কিন্তু এই সমস্ত ভ সম্প্রতি সম্প্রতি কাছাকাছি সম্পর্কিত ভাষায় সম্পদ ব্যবহার করে নিম্ন সম্পদের দাবি কমানোর জন্য শিক্ষার পদ্ধতি পরিবর্তন করা হয়েছে, কিন্তু তাদের উচ্চ সম্পদের প্রতিপক্ষ এই কাগজটিতে আমরা প্রথম মূল্য দিচ্ছি বর্তমান বস্তুর প্রার্থী প্রজন্মের মুখোমুখি সমস্যার সম্মুখীন, তারপর তিনটি উন্নতি প্রস্তাব করা যায় যে বস্তুর উল্লেখ এবং কেবি এন্ট্রির মধ্যে সংযোগ কমিয়ে  এই পদ্ধতি সাধারণ, কিন্তু কার্যকর: আমরা সাতটি XEL ডাটাসেটে আমাদের পদক্ষেপ নিয়ে পরীক্ষা করছি এবং তারা খুঁজে পাচ্ছি যে তারা স্বর্ণ প্রার্থীর স্মৃতির স্মৃত আমাদের উন্নয়ন মডেল এছাড়াও সাধারণত এক্সেলের ৭. ৯% কেবি-এর শেষ পর্যন্ত সঠিকভাবে অর্জন করে। ১', 'bs': 'Međujezička entiteta povezivanja (XEL) je zadatak pronalaženja referenta u bazi znanja ciljeva jezika (KB) za spomene izvučene iz teksta izvornog jezika. Prvi korak (X)EL-a je generacija kandidata, koja prikuplja popis uvjerljivih kandidatskih entitata iz ciljnog jezika KB za svaki spomen. Pristupi na temelju resursa iz Wikipedije dokazali su uspješni u regiji relativno visokih jezika resursa, ali to se ne proširi dobro na jezike niskih resursa sa nekoliko stranica Wikipedije. Nedavno su pokazali metode prijenosnog učenja kako bi smanjili zahtjev za resursima na jezicima niskih resursa koristeći resurse u bliskim jezicima, ali izvodnja još uvijek ostaje daleko iza njihovih kolega sa visokim resursima. U ovom papiru, prvo procjenjujemo probleme s kojima se suočavaju metode generacije kandidata za manje resurse XEL, a zatim predlažemo tri poboljšanja koje (1) smanjuju diskontaciju između spominjanja subjekta i ulaska KB-a, i (2) poboljšavaju robotu modela do scenarija niskih resursa. Metode su jednostavne, ali efikasne: eksperimentiramo sa svojim pristupom na sedam XEL podataka i otkrijemo da su dobili prosječnu dobit od 16,9% u sjećanju zlatnog kandidata Top-30 u usporedbi sa početnim linijama umjetnosti. Naš poboljšan model također pruža prosječan dobitak od 7,9% u KB preciznosti kraja do kraja XEL-a. 1', 'cs': 'Cross-lingual entity linking (XEL) je úkolem hledání referentů v cílové znalostní bázi (KB) pro zmínky extrahované z textů zdrojového jazyka. Prvním krokem (X)EL je generace kandidátů, která pro každou zmínku získá seznam věrohodných kandidátských entit z cílového jazyka KB. Přístupy založené na zdrojích Wikipedie se ukázaly úspěšně v oblasti jazyků s relativně vysokým obsahem zdrojů, ale tyto se dobře nevztahují na jazyky s nízkými zdroji s málo, pokud vůbec, stránkami Wikipedie. V poslední době bylo prokázáno, že metody transferového učení snižují poptávku po zdrojích v jazycích s nízkými zdroji využitím zdrojů v úzce příbuzných jazycích, ale výkon stále daleko zaostává za jejich protějšky s vysokými zdroji. V tomto článku nejprve zhodnotíme problémy, kterým čelí současné metody generování kandidátů entity pro XEL s nízkými zdroji, následně navrhneme tři vylepšení, která (1) snižují odpojení mezi zmínkami entity a položkami KB a (2) zlepšují robustnost modelu k scénářům s nízkými zdroji. Metody jsou jednoduché, ale efektivní: Experimentujeme s naším přístupem na sedmi XEL datových sadách a zjišťujeme, že ve srovnání s nejmodernějšími základními liniemi přinášejí průměrný zisk 16,9% při odvolání zlatých kandidátů Top-30. Náš vylepšený model také přináší průměrný zisk 7,9% v přesnosti v KB end-to-end XEL. 1', 'ca': "L'entitat translingüística que vincula (XEL) és la tasca de trobar referents en una base de coneixements de llenguatge alvo (KB) per a les mencions extraites dels textos de llenguatge fontal. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention.  Els enfocaments basats en recursos de Wikipedia han demostrat èxit en el camp de llengües relativament altas, però no s'estenen bé a llengües de baix recursos amb poques pàgines de Wikipedia. Recentment, s'ha demostrat que els mètodes d'aprenentatge de transfer ència redueixen la demanda de recursos en les llengües de baix recursos utilitzant recursos en llengües estretament relacionades, però el rendiment encara està molt enrere dels seus col·locutes d'alt recurso. En aquest paper, primer evaluem els problemes que enfronten els mètodes actuals de generació de candidats d'entitats per a XEL amb baix recursos, després proposem tres millores que (1) redueixin el desconnecte entre les mencions d'entitats i les entrades de KB, i (2) milloren la robustet del model a escenaris amb baix recursos. Els mètodes són senzills, però efectius: experimentem amb el nostre enfocament en set de dades XEL i descobrim que produeixen un guany mitjà de 16,9% en el recuperament dels candidats d'or dels 30 millors, comparat amb les línies de base més avançades. El nostre model millorat també produeix un guany mitjà de 7,9% en KB de precisió del XEL final a final. 1", 'fi': 'Cross-kielinen kokonaisuuslinkkaus (XEL) tarkoittaa viiteaineiden etsimistä kohdekielitietokannasta (KB) lähdekielistä teksteistä poimituille maininnoille. (X)EL:n ensimmäinen vaihe on ehdokassukupolvi, joka hakee luettelon uskottavista ehdokasyhteisöistä kohdekielen KB-tietokannasta jokaista mainintaa varten. Wikipedian resursseihin perustuvat lähestymistavat ovat osoittautuneet menestyksekkäiksi suhteellisen suuriresurssisten kielten alalla, mutta ne eivät ulotu hyvin vähäresurssisiin kieliin, joilla on vain vähän, jos yhtään Wikipedian sivuja. Viime aikoina siirtooppimismenetelmien on osoitettu vähentävän resurssien kysyntää vähävaraisilla kielillä hyödyntämällä resursseja läheisesti toisiinsa liittyvillä kielillä, mutta suorituskyky on edelleen paljon jäljessä niiden runsaat resurssit omaavista vastaavista kielistä. Tässä artikkelissa arvioimme ensin ongelmia, joita nykyiset entiteettiehdokkaiden luomismenetelmät kohtaavat vähäresurssisen XEL:n osalta, ehdotamme sitten kolmea parannusta, jotka (1) vähentävät entiteettimainintojen ja KB-merkintöjen välistä yhteyttä ja (2) parantavat mallin kestävyyttä vähäresurssisten skenaarioiden osalta. Menetelmät ovat yksinkertaisia, mutta tehokkaita: Kokeilemme lähestymistapaamme seitsemällä XEL-tietokannalla ja huomaamme, että ne tuottavat keskimäärin 16,9% voiton Top 30 -kultaehdokkaiden takaisinkutsussa verrattuna viimeisimpiin perusviivoihin. Parannettu mallimme tuottaa myös keskimääräisen 7,9%:n vahvistuksen päästä päähän XEL:n tarkkuudessa kilotavuissa. 1', 'et': 'Keeleülene olemilingimine (XEL) on ülesanne leida viiteid sihtkeele teadmistebaasist (KB) lähtekeeltest saadud märkustele. (X)EL esimene samm on kandidaatide generatsioon, mis hangib iga nimetuse kohta sihtkeele KB-st usutavate kandidaatide nimekirja. Vikipeedia ressurssidel põhinevad lähenemisviisid on osutunud edukaks suhteliselt suure ressursiga keeltes, kuid need ei laiene hästi vähese ressursiga keeltele, kus Wikipedia lehekülgi on vähe, kui üldse üldse. Hiljuti on näidatud, et siirdeõppe meetodid vähendavad nõudlust ressursside järele vähese ressursiga keeltes, kasutades ressursse tihedalt seotud keeltes, kuid tulemuslikkus jääb siiski palju maha nende suure ressursiga kolleegidest. Käesolevas dokumendis hindame esmalt probleeme, millega silmitsi seisavad praegused üksuste kandidaatide genereerimise meetodid madala ressursiga XEL-i jaoks, seejärel pakume välja kolm parandust, mis (1) vähendavad üksuste ja KB-kirjete vahelist lahtiühendust ning (2) parandavad mudeli tugevust vähese ressursiga stsenaariumide puhul. Meetodid on lihtsad, kuid tõhusad: me eksperimenteerime oma lähenemisviisi seitsme XEL andmekogumi põhjal ja leiame, et need annavad keskmise kasu 16,9% Top 30 kullakandidaadi tagasikutsumisel võrreldes kaasaegsete baasliinidega. Meie täiustatud mudel annab keskmise kasu 7,9% KB täpsuses otsast otsani XEL. 1', 'ha': "@ item: inmenu EL na farkon ƙõfõfi na (X)EL is an kiyaye mai kandida, wannan na sami wani jerin shaidar masu da za'a yi amfani da daga KB wa kowace kalmar. Dakata masu karata a kan wuri daga Wikimedia sun jarraba mafaniki a cikin mulkin harshen masu tsakain-resource, kuma amma, waɗannan bã su shimfiɗa alhẽri zuwa lugha na-resource da kaɗan, idan wani, pages Wikimedia. A yanzu, an nuna shiryoyin ayuka da aka sauƙaƙara umarni da ake tambayar su cikin lugha-wuri-resource, da kuma a yi amfani da ruwan mutane da ke samu'in lugha masu hususanya, kuma amma ajin yana bada ƙara bayan manyan-resource. Daga wannan takardan, Muke ƙayyade masu haɗi da matsayin kiyayen mazaɓa wa maɓalli na yanzu wa wuri-resource XEL, sa'an nan kuma Mu buɗa mafariko uku cewa (1) ya ƙarantar haɗi tsakanin da ma'anar gaskiyar da KB, kuma (2) za'a kyautata maɓallin motsi zuwa fassarar-resource. The methods are simple, but effective: We experiment with our approach on seven XEL datasets and find that they yield an average gain of 16.9% in Top-30 gold candidate recall, compared with state-of-the-art baselines.  Kifin da aka samar da shi yana ƙara kashi mai tsakanin kashi 7.9 cikin-KB na tsari na ƙari zuwa XEL. 1", 'he': 'Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts.  השלב הראשון של (X)EL הוא דור מועמדים, שמחזיר רשימה של יחידות מועמדים אמיתיות מהשפת המטרה KB לכל הזכרה. גישות מבוססות על משאבים בוויקיפדיה הוכחו מוצלחות בתחום שפות משאבים גבוהות יחסית, אך אלה לא מתרחשות היטב לשפות משאבים נמוכות עם כמה, אם יש, עמודים ויקיפדיה. לאחרונה, הוכחה שיטות לימוד העברה כדי להפחית את הדריש למשאבים בשפות המשאבים הנמוכות על ידי השימוש במשאבים בשפות הקשורות קרובות, אך ההופעה עדיין מאחורי שותפיהם המשאבים הגבוהים. בעיתון הזה, אנו קודם מעריכים את הבעיות שעומדות בפני שיטות יוצר מועמדים של יחידה הנוכחית עבור XEL משאבים נמוכים, ואז מציעים שלושה שיפורים השיטות פשוטות, אך יעילות: אנו מנסים עם הגישה שלנו על שבעה קבוצות מידע XEL ומצאים שהן מציגות רווח ממוצע של 16.9% בהזכרת מועמדים זהב ב-30 ביותר, בהשוואה עם קווי הבסיס המאוחרים ביותר. המודל השתפר שלנו נותן גם רווח ממוצע של 7.9% בדיוק ב-KB של XEL סוף-סוף. 1', 'sk': 'Večjezično povezovanje entitet (XEL) je naloga iskanja referentov v bazi znanja ciljnega jezika (KB) za omembe iz besedil izvornega jezika. Prvi korak (X)EL je generacija kandidatov, ki pridobi seznam verjetnih kandidatov iz KB ciljnega jezika za vsako omenitev. Pristopi, ki temeljijo na virih iz Wikipedije, so se izkazali za uspešni v področju razmeroma visokih virov jezikov, vendar se ti ne razširijo dobro na jezike z nizkimi viri z malo, če sploh sploh sploh, strani Wikipedije. V zadnjem času se je pokazalo, da metode prenosnega učenja zmanjšujejo povpraševanje po virih v jezikih z nizkimi viri z uporabo virov v tesno sorodnih jezikih, vendar učinkovitost še vedno precej zaostaja za njihovimi kolegi z visokimi viri. V tem prispevku najprej ocenimo probleme, s katerimi se soočajo sedanje metode generiranja kandidatov za subjekte za XEL z nizkimi viri, nato predlagamo tri izboljšave, ki (1) zmanjšujejo odklon med omenjanjem subjekta in vnosi KB ter (2) izboljšajo robustnost modela za scenarije z nizkimi viri. Metode so preproste, vendar učinkovite: z našim pristopom eksperimentiramo na sedmih naborih podatkov XEL in ugotovimo, da so povprečni dobiček 16,9% pri odpoklicu zlatih kandidatov Top 30 v primerjavi z najsodobnejšimi osnovnimi linijami. Naš izboljšani model prinaša tudi povprečno 7,9% povečanje natančnosti v KB XEL od konca do konca. 1', 'jv': 'structural navigation Pesetake tualke ning (X)él kang kandidête, awak dhéwé ngewehke listang karo kapangguna sing gak nggawe tarjamahan kanggo Kebok nggo sabên bahsa sapa nerimo. Rasané sing dipunangé karo perusahaan YouTube Kowé kudu-kudu, mbutuhake kuwi nggawe sistem kanggo ngerasai winih kanggo mberasan pangan kanggo kalaman pengguna kuwi tindan sing paling ngregan, njuk saiki sistem sing ngejaraké awak dhéwé, kepengin akeh njaluk sing wis ana sak bantuan liyane Nang paper iki, kita isih perusahaan kelas kuwi nggawe perusahaan kelas liyane karo iso nggawe sistem sing gawe nguasai nggawe sistem kang chang liyane wis kelas Xel, nambah dhéwé ngerasai telu nggawe gerakan kelas (1) supoyo perusahaan kelas kuwi tindakan bahsa sistem sing dadi nambah lan MB, lan (2) iso nggawe akeh perusahaan model nggawe cenari apa Awak dhéwé éntuk sistem sing klêb, nggonanêmên ngerasakno: awak dhéwé éntukno awak dhéwé nggalakno kayong sedhaya dataset Xél lan mbukaké diangkat dhéwé, padha bakal terusahaan langkung 16.9% sing apik dhéwé nêmên kandidé sing tata-30, ndéwé beraksi Kita nèng model sing paling dhéwé ngerasah sabané awak dhéwé, 7.9% sing kelas nèng kelas-Kib iki sabané galang-sampek Xél. 1', 'bo': 'Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. (X)EL ནང་གི་གྲལ་ཐོག་དང་པོ་དེ་ཆ་རྐྱེན་པ་ཞིག་ཡིན། Wikipedia ནང་གི་རྒྱུ་ནོར་དང་འབྲེལ་བ་དང་མཉམ་དུ་མཐུན་སྤྱོད་པའི་ཆ་རྐྱེན་ནང་གི་གྲངས་འབྲེལ་མཐུན་དཀའ ཉེ་ཆར་བརྟེན་ཁོང་ཚོའི་ཐབས་ལམ་སྤྲོད་ཀྱི་ཐབས་ལམ་དེ་མངོན་འཆར་ཡོད་པ་དེ་ནི་ཆ་རྐྱེན་འབྱུང་བའི་སྐད་རིགས་ནང་གི་རྒྱུ་ཆས་ཉུང་བའི་ནང་དུ་རྒྱུ་ཆས་པ་ཞ འོག་གི་ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོས་དང་པོ་ཞིག་གནས་ཚུལ་ཆེ་རྐྱེན་ཆེན་པོ་ལ་གནད་དོན་པའི་རྐྱེན་རིས་ལ། ཐབས་ལམ་འདི་སྟབས་བདེ་ཞིག་ཡོད། འོན་ཀྱང་ང་ཚོས་XEL གནད་སྡུད་གཞི་རྩིས་ཐོག ང་ཚོའི་མ་དབྱིབས་ཡར་རྒྱས་ཟིན་པའི་མ་དབྱིབས་ཀྱིས་རྒྱ་ཚད་ཀྱི་ཚད་ལྡན་ཡོད་པའི་%7.9་ཙམ་སྐྱེས་ཐུབ་ཀྱི་ཡོད། 1'}
{'en': 'Theoretical Limitations of Self-Attention in Neural Sequence Models', 'fr': "Limites théoriques de l'attention personnelle dans les modèles de séquence neur", 'es': 'Limitaciones teóricas de la autoatención en modelos de secuencia neuronal', 'ar': 'القيود النظرية للانتباه الذاتي في نماذج التسلسل العصبي', 'pt': 'Limitações Teóricas da Autoatenção em Modelos de Sequência Neural', 'ja': 'ニューラルシーケンスモデルにおける自己関心の理論的制限', 'zh': '神经序模自意之论局限性', 'ru': 'Теоретические ограничения самопритяжения в моделях нейропоследовательности', 'hi': 'तंत्रिका अनुक्रम मॉडल में आत्म-ध्यान की सैद्धांतिक सीमाएं', 'ga': 'Teorainneacha Teoiriciúla ar Fhéinaird i Múnlaí Seichimh Néaracha', 'hu': 'Az önfigyelem elméleti korlátai neurális szekvencia modellekben', 'ka': 'თავისუფალური მოდელში თავისუფალური დაახლოების ტეორეტიკური ლიმიტაციები', 'el': 'Θεωρητικοί περιορισμοί της αυτοπροσοχής σε μοντέλα νευρωνικής ακολουθίας', 'lt': 'Teoriniai savitarpio dėmesio apribojimai nervų sekos modeliuose', 'mk': 'Теоретските ограничувања на самовниманието во моделите на неврална секвенција', 'it': 'Limitazioni teoriche di auto-attenzione nei modelli di sequenza neurale', 'kk': 'Невралдық реттеу үлгілерінде өзіңіздің теоретикалық шектеулері', 'ms': 'Had teori perhatian-diri dalam Model Kelajuan Neural', 'mt': 'Limitazzjonijiet Teoretiċi tal-Attenzjoni Awto-Attenzjali fil-Mudelli tas-Sekwenza Newrali', 'ml': 'സ്വയം ശ്രദ്ധിക്കുന്നതിന്റെ തിയോറിക്കല്\u200d അതിരുകള്\u200d നെയുറല്\u200d സെക്കന്റ് മോഡലുകളില്\u200d', 'mn': 'Өөрийгөө анхаарлын теоретикийн хязгаарлалт', 'no': 'Teoretiske avgrensingar av selvmerksomhet i neiralsekvensmodeller', 'pl': 'Teoretyczne ograniczenia uwagi w modelach sekwencji neuronowej', 'ro': 'Limitările teoretice ale auto-atenţiei în modelele de secvenţă neurală', 'si': 'ස්වයංක්\u200dරියාත්මක සීමාන්\u200dතික සීමා', 'sr': 'Teoretičke ograničenje samopouzdanja u modelima neurološke sekvence', 'so': 'Theoretical Limitations of Self-Attention in Neural Sequence Models', 'sv': 'Teoretiska begränsningar av självuppmärksamhet i neurala sekvensmodeller', 'ta': 'நெருக்கல் வரிசை மாதிரிகளில் தானாகவே கவனம் வரம்புகள்', 'ur': 'نائرل سکوئنس موڈلز میں Self-Attention کی تئوریٹیکی محدودیت', 'vi': 'Giới hạn lý thuyết của tự chú ý trong chế độ Chuỗi thần kinh', 'uz': 'Name', 'bg': 'Теоретични ограничения на самовниманието в модели на неврални последователности', 'hr': 'Teoretičke ograničenje samopouzdanja u modelima neurološke sekvence', 'nl': 'Theoretische beperkingen van zelfaandacht in neuronale sequentiemodellen', 'da': 'Teoretiske begrænsninger af selvopmærksomhed i neurale sekvensmodeller', 'de': 'Theoretische Einschränkungen der Selbstaufmerksamkeit in neuronalen Sequenzmodellen', 'id': 'Limitasi teori perhatian diri dalam Model Sekuensi Neural', 'fa': 'محدودیت تئوریتیک توجه خودخواهی در مدل\u200cهای مختلف عصبی', 'ko': '신경 서열 모델에서의 자기주의 이론적 한계성', 'sw': 'Mipaka ya msingi ya Kujihisia Huru katika Mifumo ya Kifaransa', 'tr': 'Nöral sıralan Modellerinde özi-Attünsiň teoretikleri', 'af': 'Teoretiese beperking van Selfwaarskuwing in Neural Sequence Models', 'sq': 'Theoretical Limitations of Self-Attention in Neural Sequence Models', 'am': 'ምርጫዎች', 'hy': 'Նյարդային հաջորդականության մոդելներում ինքնաուշադրության տեսական սահմանափակումները', 'az': 'Neural Sequence Modell톛rind톛 칐z-칐z칲n칲n 칐z칲n칲 G칬zl톛m톛si', 'bn': 'নিউরেল সেকেন্স মোডেলে স্বয়ংক্রিয়ভাবে সীমাবদ্ধ', 'bs': 'Teoretičke ograničenje samopouzdanja u modelima neurološke sekvence', 'ca': "Limitacions teòriques d'auto-atenció en models de seqüències neuronals", 'cs': 'Teoretická omezení sebepozornosti v modelech neuronových sekvencí', 'et': 'Enesetähelepanu teoreetilised piirangud neurojärjestuse mudelites', 'fi': 'Itsehuomion teoreettiset rajoitukset hermosekvenssimalleissa', 'sk': 'Teoretične omejitve samopozornosti v modelih nevralnih sekvenc', 'ha': 'KCharselect unicode block name', 'he': 'הגבלות תיאורטיות של תשומת לב עצמית במודלים של רצף נוירולי', 'jv': 'Terotetik limitations of Self-Attention in Neral Sence model', 'bo': 'སྒེར་གྱི་དབྱེ་སྟངས་ལ་རང་ཉིད་ལྟར་བསམ་བློ་གཏོང་ཀྱི་སྐོར་ཚད་གཞི་སྒྲིག་དང་།'}
{'en': 'Transformers are emerging as the new workhorse of ', 'ar': 'تظهر المحولات باعتبارها العمود الفقري الجديد في البرمجة اللغوية العصبية ، مما يُظهر نجاحًا كبيرًا عبر المهام. على عكس LSTMs ، تعالج المحولات تسلسل المدخلات بالكامل من خلال الانتباه الذاتي. اقترح العمل السابق أن القدرات الحسابية للانتباه الذاتي لعملية الهياكل الهرمية محدودة. في هذا العمل ، نحقق رياضيًا في القوة الحسابية للانتباه الذاتي لنمذجة اللغات الرسمية. عبر كل من الاهتمام الناعم والصلب ، نظهر قيودًا نظرية قوية للقدرات الحسابية للانتباه الذاتي ، ووجدنا أنه لا يمكن نمذجة لغات الحالة المحدودة الدورية ، ولا الهيكل الهرمي ، ما لم يزداد عدد الطبقات أو الرؤوس مع طول الإدخال. تبدو هذه القيود مفاجئة بالنظر إلى النجاح العملي للانتباه الذاتي والدور البارز المخصص للبنية الهرمية في علم اللغة ، مما يشير إلى أنه يمكن تقريب اللغة الطبيعية جيدًا مع النماذج التي تكون ضعيفة جدًا بالنسبة للغات الرسمية المفترضة عادةً في علم اللغة النظري.', 'es': 'Los transformadores están emergiendo como el nuevo caballo de batalla de la PNL, mostrando un gran éxito en todas las tareas. A diferencia de los LSTM, los transformadores procesan las secuencias de entrada completamente mediante la autoatención Trabajos anteriores han sugerido que las capacidades computacionales de autoatención a las estructuras jerárquicas de procesos son limitadas. En este trabajo, investigamos matemáticamente el poder computacional de la autoatención para modelar lenguajes formales. A través de la atención suave y dura, mostramos fuertes limitaciones teóricas de las habilidades computacionales de la autoatención, encontrando que no puede modelar lenguajes periódicos de estados finitos, ni estructura jerárquica, a menos que el número de capas o cabezas aumente con la longitud de entrada. Estas limitaciones parecen sorprendentes dado el éxito práctico de la autoatención y el papel prominente asignado a la estructura jerárquica en la lingüística, lo que sugiere que el lenguaje natural se puede aproximar bien con modelos que son demasiado débiles para los lenguajes formales que normalmente se asumen en la lingüística teórica.', 'fr': "Les transformateurs sont en train de devenir le nouveau cheval de bataille de la PNL, avec un grand succès dans toutes les tâches. Contrairement aux LSTM, les transformateurs traitent les séquences d'entrée entièrement par auto-attention. Des travaux antérieurs ont suggéré que les capacités informatiques de l'auto-attention aux structures hiérarchiques de processus sont limitées. Dans ce travail, nous étudions mathématiquement la puissance informatique de l'attention personnelle pour modéliser des langages formels. À travers l'attention douce et dure, nous montrons de fortes limites théoriques des capacités informatiques de l'attention personnelle, constatant qu'elle ne peut modéliser des langages périodiques à états finis, ni une structure hiérarchique, à moins que le nombre de couches ou de têtes augmente avec la longueur d'entrée. Ces limites semblent surprenantes compte tenu du succès pratique de l'attention personnelle et du rôle prépondérant attribué à la structure hiérarchique en linguistique, ce qui suggère que le langage naturel peut être bien approché avec des modèles trop faibles pour les langues formelles généralement supposées en linguistique théorique.", 'pt': 'Os transformadores estão surgindo como o novo cavalo de batalha da PNL, mostrando grande sucesso em todas as tarefas. Ao contrário dos LSTMs, os transformadores processam as sequências de entrada inteiramente por meio de autoatenção. Trabalhos anteriores sugeriram que as capacidades computacionais de autoatenção para processar estruturas hierárquicas são limitadas. Neste trabalho, investigamos matematicamente o poder computacional da autoatenção para modelar linguagens formais. Tanto na atenção suave quanto na atenção dura, mostramos fortes limitações teóricas das habilidades computacionais da autoatenção, descobrindo que ela não pode modelar linguagens periódicas de estado finito, nem estrutura hierárquica, a menos que o número de camadas ou cabeças aumente com o comprimento da entrada. Essas limitações parecem surpreendentes, dado o sucesso prático da autoatenção e o papel proeminente atribuído à estrutura hierárquica na linguística, sugerindo que a linguagem natural pode ser bem aproximada com modelos que são muito fracos para as linguagens formais tipicamente assumidas na linguística teórica.', 'ja': 'トランスフォーマーは、NLPの新しいワークホースとして登場し、タスク全体で大きな成功を示しています。 LSTMとは異なり、変圧器は入力シーケンスを完全に自己注目によって処理する。 これまでの研究では、プロセス階層構造への自己注目の計算能力は限られていることが示唆されている。 本作では、形式言語をモデル化するための自己注目の計算力を数学的に調査している。 ソフトアテンションとハードアテンションの両方にわたって、私たちは自己注目の計算能力の強力な理論的制限を示しています。レイヤーまたはヘッドの数が入力の長さとともに増加しない限り、周期的な有限状態言語をモデル化することも、階層構造をモデル化することもできないことを発見しました。 これらの制限は、自己注目の実用的な成功と言語学における階層構造に割り当てられた顕著な役割を考えると驚くべきように思われ、理論言語学で典型的に想定される形式言語に対して弱すぎるモデルで自然言語をうまく近似できることを示唆している。', 'zh': '变形金刚方为NLP新主,百务咸有大功。 与 LSTM 不同,变压器悉以意处之。 前言自注层次结构计算能力有限也。 以数学自对模型形式语言计算能力。 夫软注意力,示计算能力强论局限性,见其不能拟周期性有限言语,亦不能拟层结构,非层头随输增长也。 鉴自注之实成,及语言学之赋予等构,此局限性似可讶,此自然语言可善近于理语言学中常假形式语言弱也。', 'hi': 'ट्रांसफॉर्मर एनएलपी के नए वर्कहॉर्स के रूप में उभर रहे हैं, जो कार्यों में बड़ी सफलता दिखा रहे हैं। एलएसटीएम के विपरीत, ट्रांसफॉर्मर पूरी तरह से आत्म-ध्यान के माध्यम से इनपुट अनुक्रमों को संसाधित करते हैं। पिछले काम ने सुझाव दिया है कि पदानुक्रमित संरचनाओं को संसाधित करने के लिए आत्म-ध्यान की कम्प्यूटेशनल क्षमताएं सीमित हैं। इस काम में, हम गणितीय रूप से मॉडल औपचारिक भाषाओं के लिए आत्म-ध्यान की कम्प्यूटेशनल शक्ति की जांच करते हैं। नरम और कठोर दोनों ध्यान में, हम आत्म-ध्यान की कम्प्यूटेशनल क्षमताओं की मजबूत सैद्धांतिक सीमाओं को दिखाते हैं, यह पाते हुए कि यह आवधिक परिमित-राज्य भाषाओं को मॉडल नहीं कर सकता है, न ही पदानुक्रमित संरचना, जब तक कि परतों या सिर की संख्या इनपुट लंबाई के साथ नहीं बढ़ जाती है। ये सीमाएं आत्म-ध्यान की व्यावहारिक सफलता और भाषाविज्ञान में पदानुक्रमित संरचना को सौंपी गई प्रमुख भूमिका को देखते हुए आश्चर्यजनक लगती हैं, यह सुझाव देते हुए कि प्राकृतिक भाषा को उन मॉडलों के साथ अच्छी तरह से अनुमानित किया जा सकता है जो आमतौर पर सैद्धांतिक भाषाविज्ञान में ग्रहण की जाने वाली औपचारिक भाषाओं के लिए बहुत कमजोर हैं।', 'ru': 'Трансформаторы становятся новой рабочей лошадкой NLP, демонстрируя большой успех в выполнении задач. В отличие от LSTM, трансформаторы обрабатывают входные последовательности полностью за счет самовнимания. Предыдущая работа предполагала, что вычислительные возможности самовнимания для обработки иерархических структур ограничены. В этой работе мы математически исследуем вычислительную силу самовнимания к модели формальных языков. Как через мягкое, так и через жесткое внимание, мы показываем сильные теоретические ограничения вычислительных способностей самовнимания, обнаруживая, что оно не может моделировать периодические конечные языки, ни иерархическую структуру, если количество слоев или головок не увеличивается с входной длиной. Эти ограничения кажутся удивительными, учитывая практический успех самовнимания и видную роль, отведенную иерархической структуре в лингвистике, предполагая, что естественный язык может быть хорошо аппроксимирован с моделями, которые слишком слабы для формальных языков, обычно принятых в теоретической лингвистике.', 'ga': 'Tá claochladáin ag teacht chun cinn mar chapall oibre nua NLP, rud a léiríonn rath iontach ar fud na dtascanna. Murab ionann agus LSTManna, déanann claochladáin seichimh ionchuir a phróiseáil go hiomlán trí fhéinaird. Tá sé tugtha le tuiscint in obair roimhe seo go bhfuil teorainn leis na hacmhainní ríomhaireachta a bhaineann le féinaird ar struchtúir ordlathacha a phróiseáil. Sa obair seo, déanaimid iniúchadh matamaitice ar chumhacht ríomhaireachtúil an fhéinaird ar mhúnlaí teangacha foirmiúla. Idir aird bhog agus chrua araon, léirímid teorainneacha teoiriciúla láidre ar chumais ríomhaireachtúla an fhéinaird, ag fáil amach nach féidir leis teangacha tréimhsiúla de chuid an stáit chríochnaigh a shamhaltú, ná struchtúr ordlathach, ach amháin má thagann méadú ar líon na sraitheanna nó na gcloigne le fad ionchuir. Is ábhar iontais na srianta seo i bhfianaise rath praiticiúil an fhéinaird agus an ról feiceálach a thugtar do struchtúr ordlathach sa teangeolaíocht, rud a thugann le tuiscint gur féidir teanga nádúrtha a chomhfhogasú go maith le samhlacha atá ró-lag do na teangacha foirmiúla a ghlactar go hiondúil sa teangeolaíocht theoiriciúil.', 'el': 'Οι μετασχηματιστές αναδύονται ως το νέο άλογο εργασίας του δείχνοντας μεγάλη επιτυχία σε όλες τις εργασίες. Σε αντίθεση με τα LSTMs, οι μετασχηματιστές επεξεργάζονται ακολουθίες εισόδου εξ ολοκλήρου μέσω της αυτοπροσοχής. Προηγούμενες εργασίες έχουν προτείνει ότι οι υπολογιστικές δυνατότητες της αυτοπροσοχής στις ιεραρχικές δομές διεργασιών είναι περιορισμένες. Σε αυτή την εργασία, ερευνούμε μαθηματικά την υπολογιστική δύναμη της αυτοπροσοχής σε μοντέλα τυπικών γλωσσών. Σε όλη τη μαλακή και σκληρή προσοχή, δείχνουμε ισχυρούς θεωρητικούς περιορισμούς των υπολογιστικών ικανοτήτων της αυτοπροσοχής, διαπιστώνοντας ότι δεν μπορεί να μοντελοποιήσει περιοδικές γλώσσες πεπερασμένων καταστάσεων, ούτε ιεραρχική δομή, εκτός αν ο αριθμός των στρωμάτων ή των κεφαλών αυξάνεται με το μήκος εισαγωγής. Αυτοί οι περιορισμοί φαίνονται έκπληκτοι δεδομένου της πρακτικής επιτυχίας της αυτοπροσοχής και του εξέχουσας ρόλου που αποδίδεται στην ιεραρχική δομή στη γλωσσολογία, υποδηλώνοντας ότι η φυσική γλώσσα μπορεί να προσεγγιστεί καλά με μοντέλα που είναι πολύ αδύναμα για τις τυπικές γλώσσες που συνήθως υιοθετούνται στη θεωρητική γλωσσολογία.', 'hu': 'A transzformátorok az NLP új munkalójaként jelennek meg, amelyek nagy sikert mutatnak a feladatok között. Az LSTM-ekkel ellentétben a transzformátorok teljes mértékben önfigyelemmel dolgozzák fel a bemeneti szekvenciákat. Korábbi munkák azt sugallják, hogy a folyamathierarchikus struktúrákra való önfigyelem számítástechnikai képességei korlátozottak. Ebben a munkában matematikailag vizsgáljuk az önfigyelem számítógépes erejét a formai nyelvek modellezésére. Az önfigyelem számítástechnikai képességeinek erős elméleti korlátait mutatjuk mind a lágy, mind a kemény figyelem mentén, megállapítva, hogy nem modellezhet periodikus véges állapotú nyelveket, sem hierarchikus struktúrát, hacsak a rétegek vagy fejek száma nem nő a bemeneti hosszúsággal. Ezek a korlátozások meglepőnek tűnnek, tekintettel az önfigyelem gyakorlati sikerére és a hierarchikus struktúra kiemelkedő szerepére a nyelvészetben, ami arra utal, hogy a természetes nyelvet jól lehet közelíteni olyan modellekkel, amelyek túl gyengék az elméleti nyelvészetben jellemzően feltételezett formai nyelvek számára.', 'ka': 'ტრანფორმაციები გახდება როგორც NLP-ის ახალი სამუშაო კონდი, რომელიც სამუშაო დამუშაობა. LSTMs-ის განსხვავებაში, ტრანფორმენტერები პროცესის შეცდომის შეცდომის სერექტის გარეშე სერექტიურად. წინა სამუშაო მუშაოდ იყო, რომ სამუშაო თავიდან დაახლოების კომპუტაციალური შესაძლებლობა იერაქტიკალური სტრუქტურების პროცესი ამ სამუშაოში, ჩვენ მათემატიკურად განსხვავებთ თავისუფლიო ენების მოდელზე კომპუტაციალური ძალიან. ჩვენ თავისუფლიოს კომპუტაციალური შესაძლებლობის ძალიან თეორეტიკური ზომილებები ჩვენ ჩვენ აჩვენებთ, რომ ის არ შეიძლება პერიოდიკური ბრძანებული ენების მოდელი, ან hiერაქტიკური სტრუქტურაციის მოდელი, თუ არ ეს ზომილებები იქნება საინტერესო, რომლებიც თავისუფლიოს მონაცემის პრაქტიკური წარმატებით და მნიშვნელოვანი პროლია, რომლებიც იერაქტიკური სტრუქტურაში იყო, რომლებიც იერაქტიკური ლენგლისტიკში იქნება, რომ ნახვ', 'it': "I trasformatori stanno emergendo come il nuovo cavallo di battaglia di NLP, mostrando grande successo in tutte le attività. A differenza degli LSTMs, i trasformatori elaborano le sequenze di input interamente attraverso l'auto-attenzione. Il lavoro precedente ha suggerito che le capacità computazionali di auto-attenzione alle strutture gerarchiche di processo sono limitate. In questo lavoro, analizziamo matematicamente il potere computazionale dell'auto-attenzione per modellare linguaggi formali. Attraverso l'attenzione morbida e dura, mostriamo forti limitazioni teoriche delle capacità computazionali dell'auto-attenzione, trovando che non può modellare linguaggi periodici a stato finito, né struttura gerarchica, a meno che il numero di strati o teste non aumenti con la lunghezza dell'input. Queste limitazioni sembrano sorprendenti, dato il successo pratico dell'auto-attenzione e il ruolo di primo piano assegnato alla struttura gerarchica nella linguistica, suggerendo che il linguaggio naturale può essere ben approssimato con modelli troppo deboli per i linguaggi formali tipicamente assunti nella linguistica teorica.", 'lt': 'Naujas NLP darbo arklys tampa transformatoriais, o uždaviniai labai sėkmingi. Priešingai nei LSTM, transformatoriai procesuoja įvedimo sekas visiškai per savarankišką dėmesį. Ankstesnis darbas parodė, kad savarankiško dėmesio procesų hierarchinėms struktūroms skaičiavimo pajėgumai yra riboti. In this work, we mathematically investigate the computational power of self-attention to model formal languages.  Kalbant apie minkštą ir griežtą dėmesį, mes parodome griežtus teorinius savarankiško dėmesio skaičiavimo gebėjimų apribojimus, nustatydami, kad jis negali modeliuoti periodinių terminuotos būsenos kalbų ar hierarchinės struktūros, nebent sluoksnių ar galvų skaičius didėja su įėjimo ilgiu. Šie apribojimai atrodo stebinantys, atsižvelgiant į praktinę savarankiško dėmesio sėkmę ir į svarbų vaidmenį, priskirtą kalbų hierarchinei struktūrai, ir tai rodo, kad natūralią kalbą galima gerai suderinti su modeliais, kurie yra per silpni formalioms kalboms, paprastai laikomoms teorinėje kalboje.', 'kk': 'Түрлендірушілер NLP жаңа жұмыс аты ретінде көрсетеді, тапсырмалардың арасында жақсы сәтті көрсетеді. LSTMs сияқты түрлендірушілер өзіңізге қатынас арқылы кіріс реттеулерін өзіңізге ауыстыру. Алдыңғы жұмыс иерархиялық құрылғыларды өзіңізге өзіңіздің есептеу мүмкіндіктері шектелген деп ойлады. Бұл жұмыста, біз математикалық түрлі тілдер үлгісіне өзіміздің есептеу қуатын зерттейміз. Біз қабаттар немесе басының ұзындығын өзіңізге өзіңіздің компьютерлік мүмкіндіктерінің теоретикалық шектеулерін көрсетедік. Ол периодикалық шектеу тілдерін, немесе иерархиялық құрылғыны үлгілей алмайды. Бұл шектеулері өзіңіздің өзіңіздің тәжірибесіне және лингвистикалық құрылымына таңдалған иерархиялық рөліне сәйкес келеді деп ойлайды. Табиғи тіл теориялық лингвистикалық тілдер үшін оқылмаған үлгілер үшін дұрыс жақсы жа', 'mk': 'Трансформирачите се појавуваат како новиот работен коњ на НЛП, кој покажува голем успех низ задачите. За разлика од ЛСТМ, трансформаторите процесираат вводни секвенци целосно преку самото внимание. Претходната работа покажа дека компјутерските способности на самото внимание на процесорските хиерархиски структури се ограничени. In this work, we mathematically investigate the computational power of self-attention to model formal languages.  Покрај меко и тешко внимание, покажуваме силни теоретски ограничувања на компјутерските способности на себеси внимание, откривајќи дека не може да се моделира периодични јазици со ограничена состојба, ниту хиерархична структура, освен ако бројот на слоеви или глави не се зголеми со должина Овие ограничувања се чинат изненадувачки со оглед на практичниот успех на самото внимание и истакнатата улога доделена на хиерархичната структура во јазикот, што укажува на тоа дека природниот јазик може да биде добро приближен со моделите кои се премногу слаби за формалните јазици кои обично се претпоставуваат во теор', 'ms': 'Penukar muncul sebagai kuda kerja baru NLP, menunjukkan sukses besar melalui tugas. Tidak seperti LSTM, pengubah proses urutan input secara keseluruhan melalui perhatian-diri. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited.  Dalam kerja ini, kita secara matematik menyelidiki kuasa pengiraan perhatian diri kepada bahasa formal model. Melalui perhatian lembut dan keras, kita menunjukkan keterangan teori yang kuat kemampuan pengiraan perhatian diri, mencari bahawa ia tidak boleh model bahasa-keadaan tertentu periodik, atau struktur hierarki, kecuali bilangan lapisan atau kepala meningkat dengan panjang input. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.', 'ml': 'ട്രാന്\u200dസ്ഫോര്\u200dമാര്\u200d NLP-ന്\u200dറെ പുതിയ പണികുതിരകളായി പുറപ്പെടുന്നുണ്ട്, ജോലികളില്\u200d വെച്ച് വലിയ വിജയ LSTMs വ്യത്യസ്തമായാല്\u200d, മാറ്റങ്ങളുടെ പ്രക്രിയയുടെ ഇന്\u200dപുട്ട് സെക്കന്\u200dസ് പൂര്\u200dണ്ണമായും സ്വയം ശ്രദ്ധിച് മുമ്പുള്ള ജോലി ആത്മശ്രദ്ധ പ്രവര്\u200dത്തിപ്പിക്കുന്ന സ്വയം ശ്രദ്ധിക്കുന്നതിന്റെ കണക്കിട്ടുള്ള കഴിവുകള്\u200d പര ഈ ജോലിയില്\u200d, നമ്മള്\u200d ഗണിക്കണക്കില്\u200d അന്വേഷിക്കുന്നു, ഫോള്\u200dമാലിക ഭാഷകളുടെ സ്വയം ശ്രദ്ധിക്കുന്ന ശക്തിയു മെളുപ്പത്തിലും കഠിനമായ ശ്രദ്ധിക്കുമ്പോള്\u200d സ്വയം ശ്രദ്ധിച്ചുകൊണ്ടിരിക്കുന്ന കണക്കിന്റെ സാധ്യതകളുടെ ശക്തിയുള്ള പരിധികള്\u200d നമ്മള്\u200d കാണിക്കുന്നു. അതിന് സ്വയം ശ്രദ്ധിക്കുന്നതിന്റെ പ്രാകൃതിക വിജയത്തിന്റെയും ഭാഷകങ്ങളില്\u200d ഹിയറാര്\u200dച്ചിക്കല്\u200d സ്ഥാപിക്കുന്ന സ്ഥാനത്തിന്റെയും പ്രധാനപ്പെട്ട വിജയത്തിന്റെയും പരിധികള്\u200d അത്', 'no': 'Transformerer oppstår som den nye arbeidhesten i NLP, som viser stor suksess over oppgåver. I motsetjing av LSTMs, transformerer inndata-sekvensar heilt gjennom selvmerking. Førre arbeid har foreslått at datamaskinasjonskapasiteten for selvmerking til prosessering av hierarkiske strukturar er begrenset. I denne arbeiden er vi matematisk undersøk datamaskina for selvmerksomhet til formelt språk. På både måkt og vanskelig oppmerksomhet viser vi sterke teoretiske grenser av datamaskinen for selvmerksomhet, og finn vi at det ikkje kan modellere periodiske begrensningsspråk, eller hierarkiske struktur, dersom antallet lag eller hovud aukar med inndata lengde. Desse grensene ser overraskende gjeven praktiske suksess av selvmerking og den prominente rolla tilordna hierarkiske strukturen i språk, som tyder på at naturspråk kan bli omtrent godt med modelane som er for svake for dei formlege språka som normalt antar i teoretiske språk.', 'mt': 'It-trasformaturi qegħdin jitfaċċaw bħala ż-żwiemel il-ġdid tal-NLP, li juru suċċess kbir fil-kompiti kollha. Unlike LSTMs, transformers process input sequences entirely through self-attention.  Xogħol preċedenti ssuġġerixxa li l-kapaċitajiet komputattivi ta’ awtonomija għall-istrutturi ġerarkiċi tal-proċess huma limitati. F’dan ix-xogħol, investigaw matematikament is-saħħa tal-komputazzjoni tal-awtonomija għal lingwi formali mudell. Minbarra kemm attenzjoni ratba kif ukoll iebsa, nuru limitazzjonijiet teoretiċi qawwija tal-kapaċitajiet komputattivi tal-awtonomija, u nsibu li ma tistax timmudella lingwi perjodiċi fi stat finit, u lanqas struttura ġerarkika, sakemm in-numru ta’ saffi jew irjus ma jiżdiedx bit-tul tal-input. Dawn il-limitazzjonijiet jidhru sorprendenti minħabba s-suċċess prattiku tal-awtonomija u r-rwol prominenti assenjat lill-istruttura ġerarkika fil-lingwistika, li jissuġġerixxu li l-lingwa naturali tista’ tiġi approssimatizzata tajjeb ma’ mudelli li huma dgħajfa wisq għall-lingwi formali li tipikament huma assunti fil-lingwistika teoretika.', 'pl': 'Transformatory stają się nowym koniem roboczym NLP, który odnosi ogromny sukces w różnych zadaniach. W przeciwieństwie do LSTMów transformatory przetwarzają sekwencje wejściowe całkowicie poprzez samą uwagę. Wcześniejsze prace sugerują, że możliwości obliczeniowe samodzielnej uwagi na struktury hierarchiczne procesów są ograniczone. W niniejszej pracy matematycznie badamy moc obliczeniową własnej uwagi na modelowe języki formalne. Wśród miękkiej i twardej uwagi pokazujemy silne teoretyczne ograniczenia obliczeniowe zdolności samoobserwacji, stwierdzając, że nie może ona modelować okresowych języków skończonych, ani struktury hierarchicznej, chyba że liczba warstw lub głów zwiększa się wraz z długością wejścia. Ograniczenia te wydają się zaskakujące biorąc pod uwagę praktyczny sukces uwagi na siebie oraz znaczącą rolę przypisywaną strukturze hierarchicznej w językoznawstwie, sugerując, że język naturalny można dobrze przybliżyć do modeli zbyt słabych dla języków formalnych typowo przyjmowanych w językoznawstwie teoretycznym.', 'sr': 'Transformeri se pojavljuju kao novi radni konj NLP-a, pokazujući veliki uspeh preko zadataka. Za razliku od LSTMs, transformatori procesiraju ulazne sekvence potpuno kroz samopouzdanje. Prethodni rad je predložio da su računalne sposobnosti samopouzdanja na proces hijerarhijske strukture ograničene. U ovom poslu, matematički istražujemo računalnu moć samopouzdanja na model formalnih jezika. Preko meke i teške pažnje pokazujemo jake teorijske ograničenje računalnih sposobnosti samopouzdanja, otkrivajući da ne može modelirati periodične krajnje države jezike, niti hijerarhijske strukture, osim ako broj slojeva ili glava ne povećava dužinu ulaza. Ove ograničenja izgledaju iznenađujuće s obzirom na praktični uspeh samopouzdanja i značajnu ulogu koja je dodijeljena hijerarhičkoj strukturi jezika, ukazujući na to da prirodni jezik može biti približen dobro sa modelima koji su previše slabi za formalne jezike koje se obično pretpostavljaju teoretičkom jeziku.', 'ro': 'Transformatorii apar ca noul cal de lucru al PNL, demonstrând un mare succes în toate sarcinile. Spre deosebire de LSTMs, transformatoarele procesează secvențele de intrare în întregime prin auto-atenție. Lucrările anterioare au sugerat că capacitățile computaționale de auto-atenție la structurile ierarhice ale proceselor sunt limitate. În această lucrare, investigăm matematic puterea computațională a auto-atenției la modelarea limbajelor formale. Atât prin atenție ușoară, cât și prin atenție grea, arătăm limitări teoretice puternice ale abilităților computaționale de auto-atenție, constatând că nu poate modela limbaje periodice cu stări finite, nici structura ierarhică, decât dacă numărul de straturi sau capete crește odată cu lungimea intrării. Aceste limitări par surprinzătoare având în vedere succesul practic al auto-atenției și rolul proeminent atribuit structurii ierarhice în lingvistică, sugerând că limbajul natural poate fi aproximat bine cu modele prea slabe pentru limbajele formale presupuse în mod obișnuit în lingvistica teoretică.', 'mn': 'Трансформаторууд NLP-ийн шинэ ажлын морь болж явж, ажил дахь гайхалтай амжилтыг харуулж байна. LSTMs шиг өөрчлөгчид өөртөө анхаарал хангалттай оролцоог шилжүүлдэг. Өмнөх ажлын ажил нь өөрийгөө анхаарлын үйл ажиллагаанд тооцоолох чадвар хязгаарлагддаг. Энэ ажлын хувьд бид математикийн хувьд өөрийгөө анхаарлын тооцооллын эрх мэдлийг официальн хэл загварын загварын загварын тулд судалж байна. Бид өөрсдийгөө анхаарлын тооцоолох чадварын теоретик хязгаарлалтыг харуулж байна. Энэ нь цаг хугацааны төгсгөл хэл болон төгсгөл бүтцийг загварчлах боломжгүй гэдгийг олж мэдсэн. Эдгээр хязгаар нь өөрийгөө анхаарлын амжилтын тулд, хэл хэлний хичээлийн бүтээгдэхүүнд зориулагдсан хамгийн чухал үүрэг нь гайхалтай мэт санагдаж байна. Байгалийн хэл нь теоретик хэлний хэлний хувьд ихэвчлэн хэлбэрт бага хэмжээний загваруудын тулд', 'si': 'NLP වල අලුත් වැඩ අශ්වයෙක් විදියට වෙනස් කරනවා, වැඩේ සැලසුම් වෙනුවෙන් ලොකු සාර්ථක පෙන්වනවා. LSTMs වගේ වෙනුවෙන්, ස්වයං අවධානයෙන් ප්\u200dරමාණය කරනවා ඇතුළත් අවධානයෙන්. මුලින් වැඩේ ප්\u200dරශ්නයක් තියෙනවා කියලා ස්වයංග්\u200dරහය අවධානය සඳහා පරීක්ෂණ ක්\u200dරියාත්මක ක්\u200dරියාත් මේ වැඩේ අපි ගණිතික ශක්තිය පරීක්ෂා කරන්නේ ස්වාමික අවධානයේ සාමාන්\u200dය භාෂාවක් නිර්මාණය කරන්න. සාමාන්\u200dය සහ අමාරු අවධානයක් දෙන්නම්, අපි ස්වාමාන්\u200dය අවධානයේ පරීක්ෂණ ක්\u200dරියාත්මක ක්\u200dරියාත්මක සීමාවක් පෙන්වන්න පුළුවන් කියලා, හොයාගන්නේ  මේ සීමාව පේන විශ්වාසයක් වගේ ස්වයංග්\u200dරහයේ අවස්ථාවක් සහ භාෂාත්මක වලින් සාමාන්\u200dය භාෂාත්මක වලින් ප්\u200dරභාවිත විශ්වාසයක් තියෙන්න පුළුවන් ව', 'so': 'Transformers are emerging as the new workhorse of NLP, showing great success across tasks.  Isku duwan LSTMs, iskutallaabta soo beddelashada jidhka input-soo-gelinta si kamid ah u soo jeeda iskuulka. Shaqo hore wuxuu soo jeeday in awoodda xisaabta ah ee iskuul-xisaabinta ay sameynayso dhismaha hierarkiisa. Markaas waxan, waxaynu xisaab ku baaraynaa xoogga xisaabta ee iskuulka ah oo u qoran luuqadaha rasmiga ah. Dhaqdhaqaaq iyo qallafsan, waxaynu muujinnaa xadiiqada cilmiga ah ee awoodda xisaabta naftiisa, waxaynu ogaanaynaa inuusan sameyn karin luuqadaha xilliga ah oo dowladda, ama dhismaha hierarkiisa, haddii aan tirada qasnada ama madaxu kordhiso dhererka input. Xuduudahan waxaa la yaabaa in la yaabo suurtagalka iskuulka isboorsashada iyo qaybta caadiga ah ee loo qaybiyey dhismaha afka hierarkiisa, taas oo ka jeeda in luqada dabiiciga ah lagu koobi karo samooyin aad u itaal yar ee luuqadaha rasmiga ah sida caadiga ah loogu sameeyo afka theoretical.', 'sv': 'Transformatorer växer fram som den nya arbetshästen för NLP, som visar stor framgång över olika uppgifter. Till skillnad från LSTM bearbetar transformatorer ingångssekvenser helt genom självuppmärksamhet. Tidigare arbete har föreslagit att beräkningsförmågan för självuppmärksamheten på processhierarkiska strukturer är begränsad. I detta arbete undersöker vi matematiskt självuppmärksamhetens beräkningskraft för modellering av formella språk. Genom både mjuk och hård uppmärksamhet visar vi starka teoretiska begränsningar av självuppmärksamhetens beräkningsförmåga och finner att det inte kan modellera periodiska ändliga tillståndsspråk, eller hierarkisk struktur, om inte antalet lager eller huvuden ökar med inmatningens längd. Dessa begränsningar förefaller förvånande med tanke på den praktiska framgången av självuppmärksamhet och den framträdande roll som tilldelats hierarkisk struktur i lingvistiken, vilket tyder på att naturligt språk kan närmas väl med modeller som är för svaga för de formella språk som vanligtvis antas inom teoretisk lingvistik.', 'ur': 'تبدیل کرنے والے NLP کی نوی کارگھوڑ کی طرح ظاہر ہوتے ہیں، کاموں میں بہت بڑی کامیابی دکھاتے ہیں. LSTMs کے بغیر، تغییر پھیلانے والے اپنے آپ کی توجه کے ذریعہ پوری طرح اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا سفارش کر رہے ہیں. اگلے کام نے سفارش کی ہے کہ اپنے آپ کو سمجھنے کے لئے کمپیوٹریشن قابلیت محدود ہیں. اس کام میں ہم ریاضیکی طور پر اپنے آپ کی توجه کی کمپیوٹی طاقت کی مدل رسمی زبانوں پر تحقیق کرتے ہیں. ہم نرم اور سخت توجه کے درمیان مضبوط نظریہ محدودیت کو دکھاتے ہیں جو اپنے آپ کی کمپیوٹریسی قابلیت کی محدودیت کرتی ہیں، اس کو معلوم ہوتا ہے کہ یہ موجود مقررہ وطن کی زبانوں کی مدل نہیں کر سکتا، اور نہ سطح یا سروں کی تعداد کے مطابق اضافہ ہوتی ہے. یہ محدودیت تعجب کرنے والی نظر آتی ہیں کہ اپنے آپ کی توجه کے قابل کامیابی اور زبان شناسی کی ساختاری کے لئے مقرر کیا گیا ہے، اور یہ معلوم ہے کہ طبیعی زبان کی مدلکوں کے ساتھ بہت زیادہ کمزور ہو سکتی ہے جو معمولاً نظریہ زبان شناسی میں معلوم ہوتے ہیں۔', 'ta': 'மாற்றுபவர்கள் NLP இன் புதிய வேலைகுதிரையாக வருகிறார்கள், பணிகள் முழுவதும் பெரிய வெற்றியை காட்டுகிறது. LSTMs மாற்றங்கள் முழுமையாக உள்ளீட்டு வரிசைகள் முழுமையாக தானே கவனத்தை மூலம் மாற்றும். முந்தைய வேலை பரிந்துரைக்கப்பட்டுள்ளது அந்த கணக்கீட்டு தன்னை கவனத்தை செயல்படுத்துவதற்கு கட்டுப்படுத்தல் தான். இந்த வேலையில், நாம் கணிதத்தில் வடிவமைப்பு மொழிகளுக்கு தன்னை கவனத்திற்கான கணிதமான சக்தியை கணிக்க வேண்டும். மென்னிருப்பு மற்றும் கடினமான கவனத்திற்கு மேலும், நாம் தன்னுடைய கணிப்பான திடீரியல் எல்லைகளைக் காட்டுகிறோம். இது குறிப்பிட்ட நிலையான மொழிகளை மாற்ற முடியா இந்த எல்லைகள் ஆச்சரியமாக இருக்கும் என்பதால் தானே கவனத்தின் செயல்பாடு வெற்றி மற்றும் மொழிகளில் உயர்ந்த மொழிகளுக்கு பங்கிடப்பட்ட பெரிய பங்கு, இயற்கையான மொழி சுருக்கமாக மா', 'uz': "Vazifalar vazifalar bilan juda ajoyib muvaffaqiyatli ko'rsatadi. @ info: whatsthis Oldingi vazifa esa, hierarchik strukturalarini jarayonligiga o'zimni o'zimga hisoblash imkoniyatini beradi. Bu ishda, matematika o'rganimiz, rasmlar tilga o'zining o'zimni o'zimni o'rganish maktabini o'rganamiz. Softa va qiyin murakkablik bilan biz o'zimni o'zimga hisoblash qobiliyatlarining katta teoretikal chegaralarini ko'rsamiz. Bu o'sha narsa davlat tillarini ko'paytirish mumkin, balки qatorlar yoki boshqalar soni ko'payishi mumkin. Bu chegaralar o'z o'zining o'zingizga muvaffaqiyatli va tillarda hierarchik tizimi yaratilgan muvaffaqiyatga qaramaydi, bu cheksiz tillar o'zgarishga juda yaxshi bo'lishi mumkin, o'ylaymaydi, asl tili teoretikal tillarda o'zgartirilgan modellar uchun juda yomon yo'q.", 'vi': 'Các robot biến hình đang phát triển thành ngựa làm việc mới của đài khôn ngoan. Không giống với LSTMs, máy biến đổi xử lý dãy nhập hoàn toàn nhờ chính mình. Những nghiên cứu trước cho thấy khả năng tính to án của bản thân chú ý đến cấu trúc cấp độ quy trình đã bị hạn chế. Trong công việc này, chúng tôi nghiên cứu to án học về sức mạnh tính của tự chú ý vào các ngôn ngữ chính thức. Bên cạnh sự chú ý mềm mại và mạnh mẽ, chúng tôi cho thấy những giới hạn lý thuyết mạnh mẽ của khả năng tính tập trung bản thân, tìm ra rằng nó không thể mô tả các ngôn ngữ xác định giới hạn, hay cấu trúc cấp dưới, trừ khi số lượng các lớp hoặc đầu tăng theo chiều dài nhập. Những giới hạn này có vẻ đáng ngạc nhiên vì khả năng tự trọng của mình và vai trò quan trọng gắn liền với hệ thống ngôn ngữ phân cấp, gợi ý rằng ngôn ngữ tự nhiên có thể được tương ứng tốt với các mô hình quá yếu cho ngôn ngữ văn học lý thuyết.', 'bg': 'Трансформаторите се появяват като нов работен кон на НЛО, показвайки голям успех в различните задачи. За разлика от ЛСТМ трансформаторите обработват входните последователности изцяло чрез самовнимание. Предишна работа предполага, че изчислителните възможности за самовнимание към процесните йерархични структури са ограничени. В тази работа математически изследваме изчислителната сила на самовниманието към моделните формални езици. При мекото и твърдото внимание показваме силни теоретични ограничения на изчислителните способности на самовниманието, като откриваме, че не може да моделира периодични езици с крайни състояния, нито йерархична структура, освен ако броят на слоевете или главите не се увеличава с дължината на входа. Тези ограничения изглеждат изненадващи предвид практическия успех на самовниманието и важната роля, възложена на йерархичната структура в лингвистиката, което предполага, че естественият език може да бъде прилаган добре с модели, които са твърде слаби за формалните езици, обикновено възприемани в теоретичната лингвистика.', 'hr': 'Transformeri se pojavljuju kao novi radni konj NLP-a, pokazujući veliki uspjeh u svim zadacima. Za razliku od LSTMs, transformatori procesiraju ulazne sekvence potpuno kroz samopouzdanje. Prethodni rad je predložio da su računalne sposobnosti samopouzdanja na proces hijerarhijske strukture ograničene. U ovom poslu, matematički istražujemo računalnu moć samopouzdanja na model formalnih jezika. Preko meke i teške pažnje pokazujemo jake teorijske ograničenje računalnih sposobnosti samopouzdanja, otkrivajući da ne može modelirati periodične ograničene jezike i hijerarhijske strukture, osim ako broj slojeva ili glava ne povećava s dužinom ulaza. Ove ograničenja izgledaju iznenađujuće s obzirom na praktični uspjeh samopouzdanja i značajnu ulogu koja je dodijeljena hijerarhičkoj strukturi jezika, što ukazuje na to da prirodni jezik može biti približen dobro sa modelima koji su previše slabi za formalne jezike obično pretpostavljene u teorijskom jeziku.', 'da': 'Transformere er ved at dukke op som den nye arbejdshest i NLP, der viser stor succes på tværs af opgaver. I modsætning til LSTMs behandler transformatorer input sekvenser udelukkende gennem selvopmærksomhed. Tidligere arbejde har antydet, at de beregningsmæssige muligheder for selvopmærksomhed på proces hierarkiske strukturer er begrænsede. I dette arbejde undersøger vi matematisk selvopmærksomhedens beregningskraft til modelsprog. På tværs af både blød og hård opmærksomhed viser vi stærke teoretiske begrænsninger af selvopmærksomhedens beregningsmæssige evner, idet vi konstaterer, at det ikke kan modellere periodiske finite-state sprog eller hierarkisk struktur, medmindre antallet af lag eller hoveder stiger med input længde. Disse begrænsninger synes overraskende i betragtning af selvopmærksomhedens praktiske succes og den fremtrædende rolle, der tildeles hierarkisk struktur i lingvistik, hvilket tyder på, at naturligt sprog kan tilnærmes godt med modeller, der er for svage til de formelle sprog, der typisk antages i teoretisk lingvistik.', 'nl': 'Transformers komen naar voren als het nieuwe werkpaard van NLP, die grote successen laten zien in alle taken. In tegenstelling tot LSTMs verwerken transformatoren invoersequenties volledig door zelfaandacht. Eerder onderzoek heeft gesuggereerd dat de rekenmogelijkheden van zelfaandacht voor proceshiërarchische structuren beperkt zijn. In dit werk onderzoeken we wiskundig de rekenkracht van zelfaandacht voor formele modeltalen. In zowel zachte als harde aandacht tonen we sterke theoretische beperkingen van de rekenmogelijkheden van zelfaandacht, waarbij we vaststellen dat het geen periodieke eindige-state talen kan modelleren, noch hiërarchische structuur, tenzij het aantal lagen of hoofden toeneemt met invoerlengte. Deze beperkingen lijken verrassend gezien het praktische succes van zelfaandacht en de prominente rol die wordt toegekend aan hiërarchische structuur in de linguïstiek, wat suggereert dat natuurlijke taal goed kan worden benaderd met modellen die te zwak zijn voor de formele talen die typisch worden aangenomen in de theoretische linguïstiek.', 'id': 'Transformers muncul sebagai kuda kerja baru NLP, menunjukkan sukses besar di seluruh tugas. Tidak seperti LSTM, transformer memproses urutan input sepenuhnya melalui perhatian diri. Pekerjaan sebelumnya menyarankan bahwa kemampuan komputasi perhatian diri untuk proses struktur hierarkis terbatas. Dalam pekerjaan ini, kami secara matematis menyelidiki kekuatan perhitungan perhatian diri kepada bahasa formal model. Melalui perhatian lembut dan keras, kita menunjukkan batasan teori yang kuat dari kemampuan komputasi perhatian diri, menemukan bahwa ia tidak dapat model bahasa periodik keadaan-batas, atau struktur hierarkis, kecuali jumlah lapisan atau kepala meningkat dengan panjang masukan. Pembatasan ini tampaknya mengejutkan karena sukses praktis perhatian diri dan peran terkemuka yang ditugaskan untuk struktur hierarkis dalam bahasa, yang menunjukkan bahasa alam dapat mendekati dengan baik dengan model yang terlalu lemah untuk bahasa formal biasanya dianggap dalam bahasa teori.', 'de': 'Transformatoren sind das neue Arbeitspferd von NLP und zeigen große Erfolge bei allen Aufgaben. Im Gegensatz zu LSTMs verarbeiten Transformatoren Eingangssequenzen ausschließlich durch Selbstachtung. Bisherige Arbeiten haben gezeigt, dass die Rechenleistungen der Selbstaufmerksamkeit auf hierarchische Prozessstrukturen begrenzt sind. In dieser Arbeit untersuchen wir mathematisch die Rechenleistung der Selbstaufmerksamkeit auf Modellformalsprachen. Sowohl bei weicher als auch bei harter Aufmerksamkeit zeigen wir starke theoretische Einschränkungen der Rechenfähigkeit der Selbstaufmerksamkeit, wobei festgestellt wird, dass sie weder periodische finite-state Sprachen noch hierarchische Strukturen modellieren kann, es sei denn, die Anzahl der Schichten oder Köpfe steigt mit der Eingabelänge. Diese Einschränkungen scheinen angesichts des praktischen Erfolgs der Selbstaufmerksamkeit und der herausragenden Rolle, die der hierarchischen Struktur in der Linguistik zugewiesen wird, überraschend, was darauf hindeutet, dass natürliche Sprache gut mit Modellen angenähert werden kann, die für die formalen Sprachen, die in der theoretischen Linguistik typischerweise angenommen werden, zu schwach sind.', 'fa': 'تغییر دهندگان به عنوان اسب کاری جدید NLP روشن می شوند، که موفقیت بزرگی را در سر کار نشان می دهند. برخلاف LSTMs، تغییر دهنده\u200cها از طریق توجه خودشان به طور کامل ردیابی ورودی را فرایند می\u200cکنند. کار قبلی پیشنهاد کرده است که توانایی محاسبات توجه به ساختارهای مختلف محدودیت شده است. در این کار، ما به ریاضی قدرت کامپیوتری خود را به مدل زبان رسمی تحقیق می کنیم. از طریق توجه نرم و سخت، محدودیت نظریه\u200cای قوی از توانایی محاسبات توجه خود را نشان می\u200cدهیم، و پیدا می\u200cکنیم که نمی\u200cتواند زبان\u200cهای محدودیت محدودیت محدودیت را مدل کند، و نه ساختار محدودیت، مگر اینکه تعداد لایه\u200cها یا سرها با طول ورودی افزایش کند. این محدودیت به نظر تعجب کننده می\u200cشود با توجه به موفقیت عملی توجه خود و نقش بزرگی که به ساختار شیراریکی در زبان\u200cشناسی وابسته می\u200cشود، پیشنهاد می\u200cدهد که زبان طبیعی می\u200cتواند با مدل\u200cهای زیادی ضعیف باشد که برای زبان\u200cهای رسمی معمولاً در زبان\u200cشناسی تئوری می', 'tr': 'Transformat챌ylar NLP t채ze i힊aty bolup g철r체n첵채r, i힊i흫 체st체nde 철r채n ba힊arnygy g철rke첵채rler. LSTMs 첵aly, terjime edip giri힊 dizirleri 철z체ne 체ns berip i힊le첵채r. 횜흫ki i힊i흫 hijerarhi첵a d체z체mlerni흫 철z-철z체ne 체ns bermegi 체챌in hesaplamak ukyplary 챌arpndyryldy. Bu i힊de, biz matematiksel olarak 철z체ni흫 체ns체ni formal dillere g철r채 hesaplamak g체c체ni barla첵arys. Hem yumu힊y hem kyn 체ns beril첵채ris, biz 철z체ni흫 체ns체ni흫 kalkulary흫 g체첵챌li teori첵aly 챌yky힊laryny g철rke첵채ris we munu흫 periodi첵a be첵ik durmu힊 dillerini, hem i첵erarhi첵a struktury흫 sany 첵a kell채흫 giri힊 uzunlygy bilen k철pr채k bolmasa di첵ip kabul edip bilme첵채ris. Bu 챌yky힊lar 철z-철z체ne 체ns bermegi we lingwistiklerde hiyerar힊ik strukturyna berilen t채sirli t채sirli bolup g철r체n첵채r, tebigy dil formal diller 체챌in 철r채n zay캇f di첵ip kabul edil첵채n 철r채n t채sirli bir nusga g철r첵채r.', 'sw': 'Watafsiri wanajitokeza kama farasi mpya ya NLP, wakionyesha mafanikio makubwa katika kazi. Tofauti na LSTMs, mchakato wa mabadiliko utaratibu wa input kwa ujumla kupitia kujitazama. Kazi zilizopita imependekeza kwamba uwezo wa kompyuta wa kujihisia katika kuchukua miundombinu ya ubunifu ni mdogo. Katika kazi hii, tunachunguza nguvu ya kompyuta ya kujitazama katika lugha rasmi. Katika hali ngumu na ngumu, tunaonyesha vizuizi vikali vya nadharia ya uwezo wa kompyuta wa kujihisabu, kwa kutambua kwamba haiwezi kuunda lugha za kiserikali za kawaida, wala muundo wa kiunde, ikiwa ni pamoja na idadi ya vipeperushi au vichwa vinaongezeka kwa kiwango cha input. Uzuizi huu unaonekana kushangaza kufuatia mafanikio ya msingi wa kujitegemea na jukumu maarufu lililowekwa kwenye muundo wa miundombi katika lugha za lugha, ikipendekeza kuwa lugha ya asili inaweza kupatikana vizuri na mifano ambayo ni dhaifu sana kwa lugha rasmi zilizochukuliwa kawaida katika lugha za kitaaluma.', 'sq': 'Transformuesit po shfaqen si kali i ri i punës i NLP, duke treguar sukses të madh nëpërmjet detyrave. Ndryshe nga LSTMs, transformuesit procesojnë sekuencat e hyrjes tërësisht nëpërmjet vetë-vëmendjes. Puna e mëparshme ka sugjeruar se aftësitë kompjuterike të vetëvëmendjes ndaj strukturave hierarkike të procesit janë të kufizuara. Në këtë punë, ne matematikisht hetojmë fuqinë kompjuterike të vetëvëmendjes ndaj gjuhëve zyrtare. Përmes vëmendjes së butë dhe të fortë, ne tregojmë kufizime të forta teorike të aftësive llogaritare të vetë-vëmendjes, duke gjetur se ajo nuk mund të modelojë gjuhë periodike të shtetit të kufizuar, as strukturë hierarkike, përveç nëse numri i shtresave apo kokave rritet me gjatësinë e hyrjes. Këto kufizime duken befasuese duke patur parasysh suksesin praktik të vetëvëmendjes dhe rolin e shquar të caktuar strukturës hierarkike në gjuhën gjuhësore, duke sugjeruar se gjuha natyrore mund të përafërtohet mirë me modele që janë shumë të dobëta për gjuhët zyrtare tipikisht të pranuara në gjuhën teorike.', 'am': 'ትርጓሚዎች በአዲስ አዲስ የNLP ፈረስ እያወጡ በሥራው ላይ ታላቅ ድል አግኝተዋል፡፡ በተለየ LSTMs፣ ለውጦች ፕሮጀክት የድምፅ ውጤቶች በሙሉ ራሳቸውን በመጠየቅ ይደረጋሉ፡፡ የቀድሞው ሥራ የራሳቸውን አካባቢነት የሥልጣን ሥርዓት ለመሥራት ግንኙነት ነው፡፡ በዚህ ሥራ፣ የራሳችንን የሥልጣን አካባቢ የፊደል ቋንቋዎች እናሳውቃለን፡፡ Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length.  እነዚህ ግንኙነት የራሳቸውን አካባቢ ስኬት እና ለቋንቋ ቋንቋዎች የተደረገው ትክክለኛ ክፍል፣ የፍጥረቱ ቋንቋ በአካባቢው ቋንቋዎች ላይ በጣም ደካማ ለመሆን ይችላል፡፡', 'af': 'Transformeerders word opgekom as die nuwe werksperd van NLP, wat groot sukses oor werke vertoon. Ongelyks van LSTMs, transformeerders proses invoer sekwensies heeltemal deur self-aandag. Vorige werk het voorgestel dat die rekenaarske kapasiteite van self-aandag na proses hierarkies strukture beperk is. In hierdie werk, ons wiskundig ondersoek die rekenaar krag van self-aandag na model formele tale. Binne sagte en moeilike aandag, wys ons sterke teorieese beperkings van die rekenaasjonale moontlikhede van self-aandag, en vind dat dit nie periodieke beperking-staat tale of hierarkies struktuur kan model nie, tensy die aantal lage of koppe met invoer lengte vergroot word nie. Hierdie beperkinge lyk verwonderbaar gegee het die praktiese sukses van self-aandag en die prominente rol wat aan hierarkiese struktuur in lingwistike toegewys is, wat voorstel dat natuurlike taal goed kan aangewys word met modele wat te swak is vir die formele tale tipies in teorieese lingwistike aangeneem word.', 'hy': 'Փոփոխակերպերը զարգանում են որպես ՆԼՊ-ի նոր աշխատաձի, որը մեծ հաջողություն է ցույց տալիս խնդիրների ընթացքում: Ի տարբերություն LSMT-ներին, վերափոխողները պրոցեսում են ներմուծման հաջորդականությունները ամբողջովին ինքնաուշադրության միջոցով: Նախորդ աշխատանքը առաջարկեց, որ ինքնաուշադրության հաշվարկների հնարավորությունները գործընթացի հիերարխիկ կառուցվածքների վրա սահմանափակված են: Այս աշխատանքի ընթացքում մենք մաթեմատիկապես ուսումնասիրում ենք ինքնաուշադրության հաշվարկների ուժը պաշտոնական լեզուների մոդելների վրա: Եթե ուշադրություն դարձնենք, ապա մենք ցույց ենք տալիս ինքնաուշադրության հաշվարկների հզոր տեսական սահմանափակումներ, որոնք ցույց են տալիս, որ այն չի կարող օրինակել պարբերական սահմանափակ վիճակում գտնվող լեզուներ, կամ հիերարխիկ կառուցվածք, եթե շերտերի կամ գլխավորների թիվը չբարձ Այս սահմանափակումները զարմանալի են թվում, հաշվի առնելով ինքնաուշադրության պրակտիկ հաջողությունը և լեզվաբանության հիերարխիկ կառուցվածքի նշանակալի դերը, որը առաջարկում է, որ բնական լեզուն կարող է լավ մոտենալ այնպիսի մոդելների հետ, որոնք չափազանց թույլ են պաշտոնական', 'az': "Transformers NLP'in yeni işatı kimi ortaya çıxarır, işlərdə böyük başarılı göstərir. LSTMs kimi, özünün dikkatini ilə transformatçılar giriş sıralarını tamamlayır. Əvvəlki işin hiyerarşik yapıları işlədirmək üçün özünün təsirlərinin hesablama qabiliyyətlərinin sınırlı olduğunu göstərdi. Bu işdə, biz matematiksel olaraq özümüzün ünsiyyətini formal dillərin modelinin hesablama gücünü incidirik. İkisində də yumuşaq və ağır dikkati, özünün dikkatini hesablayan təriqətli qabiliyyətlərin çox möhkəm təriqətli sınırlarını göstərdik, periodik təriqətli dillərin və hiyerarşik quruluşlarının modellərini göstərməyəcəyini öyrəndik. Lakin səviyyənin və başların giriş uzunluğu ilə artır Bu limitlərin özünün təsirlərinin praktik başarısından və dillərin hiyerarşik strukturlarına verilən ən böyük rolünün təəccüblənməsi təəccüblü görünür ki, təbiətli dil teoriqli dillərdə çox zəif olan modellərlə yaxınlaşdırılabilir.", 'bn': 'ট্রান্সফর্মাররা এনএলপির নতুন কার্যঘোড় হিসেবে উদ্ভাবন করছে, যারা কাজের বিভিন্ন সাফল্য দেখাচ্ছে। এলস্টিএমএস-এর অন্যান্য ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন ভিন্ন মনোযোগের মাধ্যমে বিনিময়ে প্রক্ পূর্ববর্তী কাজ পরামর্শ প্রদান করেছে যে হিয়েরার্কিকাল কাঠামো প্রক্রিয়ার জন্য আত্মমনোযোগের সংক্রান্ত ক্ষমত এই কাজে আমরা গণতান্ত্রিকভাবে গণতান্ত্রিকভাবে তদন্ত করি নিজেদের আত্মমনোযোগের ক্ষমতা গণনা করা হয়েছে ফর্মি কঠিন এবং কঠিন মনোযোগ দিয়ে আমরা স্বয়ংক্রিয়ভাবে সংক্রান্ত ততিত্ত্বিক সীমাবদ্ধতা দেখাচ্ছি যে এটি নিয়মিত সংখ্যা রাষ্ট্রীয় ভাষা মডেল করতে পারে না, আর হিরেরাক এই সীমাবদ্ধ মনোযোগের ব্যাপারে বিস্ময়কর মনোযোগ এবং ভাষায় হিয়েরার্কিক কাঠামোর জন্য বিশাল ভূমিকা দায়িত্ব দেয়া হয়েছে, তার পরামর্শ দেয়া হচ্ছে যে প্রাকৃতিক ভাষার প্রাকৃত', 'ca': "Els transformadors estan emergint com el nou cabell de treball de NLP, mostrant un gran èxit a través de les tasques. A diferència dels LSTMs, els transformadors processen seqüències d'entrada completament a través de l'autoatenció. La feina anterior ha suggerit que les capacitats computacionals de l'autoatenció a les estructures jeràrquiques de procés són limitades. En aquest treball, matemàticament investigam el poder computacional de l'autoatenció a les llengües formals modelades. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length.  These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.", 'ko': "'트랜스포머'는 NLP의 새로운 주축이 되고 있으며 각종 임무에서 큰 성공을 거두고 있다.LSTM과 달리 transformers는 입력 서열을 자기 관심으로 처리합니다.이전의 연구에 의하면 자기 관심 과정의 차원 구조의 계산 능력은 유한한 것으로 나타났다.이 작업에서 우리는 수학적 측면에서 모델 형식 언어에 대한 계산 능력을 연구했다.소프트 주의와 하드 주의에서 우리는 자기주의 계산 능력의 강력한 이론적 한계를 나타냈다. 이는 주기적인 유한한 상태 언어를 모의할 수 없고 차원 구조도 모의할 수 없다. 층이나 머리의 수량이 입력 길이가 증가함에 따라 증가하지 않는 한.자기 관심의 실제 성공과 언어학에서 등급 구조의 두드러진 작용을 감안하면 이러한 제한은 놀랄 만하다. 이는 자연언어가 이론언어학에서 일반적으로 가설하는 형식언어가 너무 약한 모델과 비슷하게 접근할 수 있음을 나타낸다.", 'cs': 'Transformátory se objevují jako nový pracovní kůň NLP, který ukazuje velký úspěch napříč úkoly. Na rozdíl od LSTMů transformátory zpracovávají vstupní sekvence výhradně prostřednictvím vlastní pozornosti. Předchozí práce naznačuje, že výpočetní schopnosti sebepozornosti na hierarchické struktury procesů jsou omezené. V této práci matematicky zkoumáme výpočetní sílu sebepozornosti k modelovým formálním jazykům. Napříč měkkou i tvrdou pozorností ukazujeme silná teoretická omezení výpočetních schopností sebepozornosti a zjišťujeme, že nemůže modelovat periodické jazyky konečných stavů, ani hierarchickou strukturu, pokud se počet vrstev nebo hlav nezvyšuje s délkou vstupu. Tato omezení se jeví překvapivě vzhledem k praktickému úspěchu sebepozornosti a významné roli hierarchické struktuře v lingvistice, což naznačuje, že přirozený jazyk lze dobře aproximovat s modely, které jsou příliš slabé pro formální jazyky typicky předpokládané v teoretické lingvistice.', 'bs': 'Transformeri se pojavljuju kao novi radni konj NLP-a, pokazujući veliki uspjeh preko zadataka. Za razliku od LSTMs, transformatori procesiraju ulazne sekvence potpuno kroz samopouzdanje. Prethodni rad je predložio da su računalne sposobnosti samopouzdanja na proces hijerarhijske strukture ograničene. U ovom poslu, matematički istražujemo računalnu moć samopouzdanja na model formalnih jezika. Preko meke i teške pažnje pokazujemo jake teorijske ograničenje računalnih sposobnosti samopouzdanja, otkrivajući da ne može modelirati periodične ograničene jezike i hijerarhijske strukture, osim ako broj slojeva ili glava ne povećava dužinu ulaza. Ove ograničenja izgledaju iznenađujuće s obzirom na praktični uspjeh samopouzdanja i značajnu ulogu koja je dodijeljena hijerarhičkoj strukturi na jeziku, ukazujući na to da prirodni jezik može biti približen dobro sa modelima koji su previše slabi za formalne jezike koje se obično pretpostavljaju teoretičkom jeziku.', 'et': 'Transformerid on kujunemas uue tööprogrammi uueks tööhobuseks, näidates ülesannete lõikes suurt edu. Erinevalt LSTMdest töötlevad trafod sisendjärjestusi täielikult enesetähelepanu kaudu. Varasemad tööd on näidanud, et eneseahelepanu arvutusvõimekus protsessi hierarhilistele struktuuridele on piiratud. Käesolevas töös uurime matemaatiliselt enesetähelepanu arvutusjõudu formaalsetele mudelkeeltele. Nii pehme kui ka kõva tähelepanu ulatuses näitame enesetähelepanu arvutusvõimete tugevaid teoreetilisi piiranguid, leides, et see ei saa modelleerida perioodilisi piiratud oleku keeli ega hierarhilist struktuuri, kui kihtide või peade arv sisendi pikkusega ei suurene. Need piirangud tunduvad üllatavad, arvestades isetähelepanu praktilist edu ja hierarhilise struktuuri silmapaistvat rolli lingvistikas, mis viitab sellele, et loomulikku keelt saab hästi ühtlustada mudelitega, mis on teoreetilises lingvistikas tavaliselt eeldatavate formaalsete keelte jaoks liiga nõrgad.', 'fi': 'Muuntajat ovat nousemassa NLP:n uudeksi työvuoroksi, mikä osoittaa suurta menestystä eri tehtävissä. Toisin kuin LSTMs, muuntajat prosessoivat syöttöjaksoja täysin itsetunnon kautta. Aiempi työ on ehdottanut, että prosessihierarkisten rakenteiden itsehuomion laskennalliset mahdollisuudet ovat rajalliset. Tässä työssä tutkimme matemaattisesti itsetunnon laskennallista voimaa mallinnuskieliin. Sekä pehmeän että kovan huomion kautta näytämme vahvoja teoreettisia rajoituksia itsetunnon laskennallisille kyvyille, havaiten, että se ei voi mallintaa määräajoin rajatilakieliä eikä hierarkkista rakennetta, ellei kerrosten tai päiden määrä kasva syötteen pituuden myötä. Nämä rajoitukset vaikuttavat yllättäviltä, kun otetaan huomioon itsetunnon käytännön menestys ja hierarkian rakenteen merkittävä rooli kielitieteessä, mikä viittaa siihen, että luonnollista kieltä voidaan hyvin lähentää malleilla, jotka ovat liian heikkoja muodollisille kielille, jotka tyypillisesti oletetaan teoreettisessa kielitieteessä.', 'jv': 'Transformer sampeyan mbut dumadhi nganggo cah operasi sing gawe NLP, iso nguasah barang pengguna sing gawe barang seneng operasi. UTC Workspace %1% Nang gunggo iki, kéné mataten sakjane nguasakno perusahaan anyar nggawe nguasakno tentang karo ingkang supaya. politenessoffpolite"), and when there is a change ("assertivepoliteness limiting', 'ha': "Transformers are emerging as new hesis of NLP, showing babban rabo mai girma a kan aikin. @ info: whatsthis Yin aikin da ya gabãni ya shauri cẽwa, abincin lissafi na bincike wa masu bincike wa aikin matsayin hierrchical ne wanda za'a ƙunsa. A cikin wannan aikin, za mu yi ƙidãya a lissafin zartar da ƙarfin kansa na zama masu bincike zuwa misalin ayuka masu rasmi. Ko cikin masu sauri da ƙwaƙasasshiya, Munã nũna kanana mai ƙarfin teoreoreki na abincin da ya lissafa kansa, kuma tuna cewa bã za ta iya motsar kamar lugha na-state na daidaici ko kuma da tsarin hiirarkiki ba, sai ƙidãyar ƙananan ko huɗu su ƙara da tsawo na inputi. Wannan ƙaddarar za ta yi mãmãki ko da babban rabo na masu fassarar kansa da rabon da aka sanar da shi zuwa matsayin hierrchical cikin linguistic, yana madaidaita cewa za'a karɓi lugha na natura da misãlai masu rauni ko da misãlai masu rauni wa lugha rasmi wanda aka ƙayyade a cikin linguistic na littafiki.", 'sk': 'Transformatorji se pojavljajo kot novi delovni konj NLP, ki kažejo velik uspeh pri vseh nalogah. Za razliko od LSTMs transformatorjev vhodne sekvence obdelujejo v celoti s samopozornostjo. Predhodno delo je predlagalo, da so računalniške zmogljivosti samopozornosti procesnim hierarhičnim strukturam omejene. V tem delu matematično raziskujemo računalniško moč samopozornosti modelnim formalnim jezikom. Skozi mehko in trdo pozornost kažemo močne teoretične omejitve računalniških sposobnosti samopozornosti, ugotovimo, da ne more modelirati periodičnih jezikov končnih stanj niti hierarhične strukture, razen če se število plasti ali glav z vhodno dolžino poveča. Te omejitve se zdijo presenetljive glede na praktični uspeh samopozornosti in pomembno vlogo hierarhične strukture v jezikoslovju, kar kaže, da je naravni jezik mogoče dobro približati modelom, ki so prešibki za formalne jezike, ki se običajno domnevajo v teoretičnem jezikoslovju.', 'he': 'המעברים מתגלים כסוס העבודה החדש של NLP, מראה הצלחה גדולה בכל משימות. בניגוד ל-LSTMs, משתנים מעבדים רצפי הכניסה לחלוטין דרך תשומת לב עצמית. העבודה הקודמת הציעה שיכולות החישוב של תשומת לב עצמית לתהליך מבנים היררכיים מוגבלות. בעבודה הזו, אנו חוקרים מתמטית את כוח החישוב של תשומת לב עצמית לשפה רשמית מודל. באמצעות תשומת לב רכה וקשה, אנו מראים מגבלות תיאורטיות חזקות של היכולות החישוביות של תשומת לב עצמית, למצוא שהוא לא יכול לדוגמא שפות תקופיות במצב מוגבל, או מבנה הייררכי, אלא אם מספר שכבות או ראשים מגדלים עם אורך הכניסה. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.', 'bo': 'རྒྱུན་བཅོས་པ་ཚོས་NLP ཡི་ལས་ཀ་གསར་བ་ཞིག་གིས་མཐོང་བཞིན་པས་ལས་ཀ་སྒྲུབ་མང་པོ་བྱེད་ཀྱི་ཡོད། Unlike LSTMs, transformers process input sequences entirely through self-attention. སྔོན་གྱི་ལས་ཀ་ནི་གྲངས་འབྱོར་གྱི་སྒེར་གྱི་རྩིས་འཁོར་གྱི་ཆ་ཁྱད་ཆོས་ཚོས་རང་ཉིད་ཀྱིས་ལས་སྦྱོར་བའི་དབང་ཆ་ ང་ཚོས་རང་ཉིད་ཀྱི་ལས་ཀ་འདིའི་ནང་གི་གྲངས་རིག་གིས་སྐོར་གྱི་གྲངས་རིག་གིས་རང་ཉིད་ཀྱི་ཆོས་ཉིད་ཀྱི་དཔེ་ Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. སྐད་ཆ་འདི་དག་རང་ཉིད་ཀྱི་རང་ཉིད་ཡུལ་གྱི་གྲུབ་སྐྱོང་དང་སྐད་རིགས་ནང་གི་དབྱིབས་མཐུན་གྱི་རྩ་བའི་གོ་སྣང་དག་བརྗོད་བསྐྱེད་ཡོད།'}
{'en': 'Acoustic-Prosodic and Lexical Cues to Deception and Trust : Deciphering How People Detect Lies', 'ar': 'الإشارات الصوتية والمفضلة والمعجمية للخداع والثقة: فك رموز كيف يكتشف الناس الأكاذيب', 'fr': 'Indices acoustiques-prosodiques et lexicaux de la tromperie et de la confiance\xa0: déchiffrer la façon dont les gens détectent les mensonges', 'pt': 'Dicas acústico-prosódicas e léxicas para enganar e confiar: decifrando como as pessoas detectam mentiras', 'es': 'Señales acústicas, prosódicas y léxicas del engaño y la confianza: descifrar cómo las personas detectan las mentiras', 'zh': '欺信声学韵词汇线索:破译人如何得妄言', 'ja': '偽りと信頼への音響的プロソディックおよび語彙的手がかり：人々が嘘を検出する方法の解読', 'hi': 'धोखे और विश्वास के लिए ध्वनिक-प्रोसोडिक और लेक्सिकल संकेत: यह समझना कि लोग झूठ का पता कैसे लगाते हैं', 'ru': 'Акустико-просодические и лексические сигналы к обману и доверию: расшифровка того, как люди обнаруживают ложь', 'ga': 'Leideanna Fuaimiúla-Sinsearach agus Léacsúla maidir le Meabhlaireacht agus Muinín: Ag Déanamh Conas a Bhraitheann Daoine Bréaga', 'ka': 'Name', 'hu': 'Akusztikus-prozódikus és lexikus jelek a megtévesztéshez és a bizalomhoz: Az emberek hazugságokat észlelnek', 'el': 'Ακουστικά-Προσodische και Λεξικά στοιχεία για την εξαπάτηση και την εμπιστοσύνη: Αποκωδικοποιώντας πώς οι άνθρωποι ανιχνεύουν ψέματα', 'it': 'Acustic-Prosodic e Lexical Cues to Deception and Trust: Deciphering How People Detective Lies', 'kk': 'Акустикалық- прозодикалық және лексикалық куыстар қабылдау және сенімдік: адамдар жалғасты қалай анықтау үшін шешу', 'lt': 'Akustinis, prosodinis ir leksinis sukčiavimo ir pasitikėjimo pagrindas: nustatymas, kaip žmonės aptinka melas', 'mk': 'Акустички-прозодични и лексикални причини за измама и доверба: дешифрирање како луѓето откриваат лаги', 'ml': 'ആകോസ്റ്റിക്- പ്രോസോഡിക്കും ലെക്സിക്കല്\u200d ക്യൂസ്', 'mn': 'Хүмүүс худлааныг хэрхэн олж мэдэхийг шийдвэрлэхийг хүсэж байна.', 'ms': 'Acoustic-Prosodic dan Lexical Cues to Deception and Trust: Deciphering How People Detect Lies', 'no': 'Akustisk- prosesskorg og leksisk kuber til dekeptering og tiltru: Deciphering korleis folk oppdagar løyve', 'mt': 'Il-Kawżi Akustiċi-Prosodiċi u Lessiċi għad-Deċettazzjoni u l-Fiduċja: Id-Deċifrazzjoni ta’ Kif in-nies jidentifikaw il-Liegi', 'pl': 'Akustyczno-prozodyczne i leksykalne wskazówki do oszustwa i zaufania: rozszyfrowanie, w jaki sposób ludzie wykrywają kłamstwa', 'ro': 'Acustic-Prosodic și Lexical Cues pentru decepție și încredere: Deciphering modul în care oamenii detectează minciunile', 'sr': 'Akustički prozodični i leksički kutovi do odlaganja i povjerenja: odlaganje kako ljudi otkrivaju laži', 'si': 'අකුස්ටික්-ප්\u200dරොසෝඩික් සහ ලෙක්සිකාල් කුස් විශ්වාස කරන්න සහ විශ්වාස කරන්න: මිනිස්සු කොහොමද බොරු හො', 'so': 'Acoustic-Prosodic and Lexical Cues to Deception and Trust: Decidering How People Detects Lies', 'sv': 'Akustisk-prosodisk och Lexical Cues to Bedrage and Trust: Deciphering Hur människor upptäcker lögner', 'ta': 'Acoustic-Prosodic and Lexical Cues to Deception and Trust: Deciphering How People Detect Lies', 'ur': 'آکوستیکی پردازی اور لکسیکی کائس کی تصدیق اور اعتماد کرنے کے لئے: لوگوں کو کس طرح جھوٹی باتیں پڑھنے کا تفصیل دینے والا ہے۔', 'uz': 'Name', 'vi': 'Âm đạo và ngôn ngữ học để lừa gạt và tin tưởng: Quyết định cách người khác phân tích dối trá', 'bg': 'Акустично-прозодични и лексикални подсказки за измама и доверие: измама как хората откриват лъжите', 'nl': 'Acoustic-Prosodic en Lexical Cues to Deception and Trust: Ontcijferen hoe mensen leugens detecteren', 'da': 'Akustisk-Prosodisk og Lexical Cues to Deception and Trust: Afkodning af, hvordan folk opdager løgne', 'hr': 'Akustički prozodični i leksički košići do odlaganja i povjerenja: odlaganje kako ljudi otkrivaju laži', 'de': 'Akustisch-prosodische und Lexikale Hinweise zu Täuschung und Vertrauen: Entschlüsseln, wie Menschen Lügen erkennen', 'id': 'Acoustic-Prosodic and Lexical Cues to Deception and Trust: Deciphering How People Detect Lies', 'ko': '사기와 신뢰의 성학적 운율과 어휘 단서: 사람들이 어떻게 거짓말을 발견하는지 해독', 'fa': 'آکوستیک-پروسودیک و لکسیکی برای تصمیم گرفتن و اعتماد: کشف کردن چگونه افراد دروغ شناسایی می کنند', 'sw': 'Masuala ya Kikosiki na Kilexico kwa Kuamuliwa na Kutegemea: Kuamua jinsi watu wanavyogundua uongo', 'tr': "Akoustik-Prosodik ve Leksik Kutuslar Açılmaya ve Güvenlik'e : İnsanların yalanları nasıl tanıdığını söylüyor", 'af': 'Akoustike- Prosodiese en Leksiese Kues na Besluit en Vertrou: Deciphering How People Detect Lies', 'sq': 'Akustik-Prosodik dhe Lexical Cues to Deception and Trust: Deciphering How People Detect Lies', 'hy': 'Ակոստիկ-պրոսոդիկ և լեքսիկական խաբեության և վստահության խոսքերը. որոշում, թե ինչպես են մարդիկ հայտնաբերում սուտ', 'am': 'የአኮስቲክ-ፕሮሮሮዶክኛ እና የሊክሲካዊ ኩላቶች ለDeception and Trust: How People Detects Lies', 'bn': 'বিশ্বাস এবং বিশ্বাসের জন্য একাসোস্টিক-প্রোসোডিক এবং লেক্সিক্যাল কিউস: মিথ্যা কিভাবে মিথ্যা সনাক্ত করা য', 'az': 'Acoustic-Prosodic and Lexical Cues to Deception and Trust: Decrypting People How to Detect Falsehood', 'bs': 'Akustički prozodični i leksički kosti do odlaganja i povjerenja: odlaganje kako ljudi otkrivaju laži', 'ca': 'Cues acústics-pròdics i lèxics a la decepció i la confiança: Descifrar com la gent detecta mentides', 'cs': 'Akusticko-prozodické a Lexické návody k klamu a důvěře: dešifrování, jak lidé detekují lži', 'et': 'Akustiline-prosoodiline ja leksiaalne viis pettuse ja usalduse kohta: petmine, kuidas inimesed valesid tuvastavad', 'fi': 'Akustiset, prosodiset ja leksikoiset vihjeet petokseen ja luottamukseen: Ihmisten valheiden havaitseminen', 'jv': 'Suwisik-Perusahaan lan Cues Leksik kanggo Kemerdekaan lan ngêngguna: Kemerdekaan Winih pejat usul cara ngêngguna', 'he': 'אקוסטי-פרוסודי וקסיקלי קווים להונאה ואמונה: להחליט איך אנשים מגלים שקרים', 'ha': 'KCharselect unicode block name', 'sk': 'Akustično-prosodični in leksični namigi za prevaro in zaupanje: prevara, kako ljudje zaznavajo laži', 'bo': 'Acoustic-Prosodic and Lexical Cues to Deception and Trust: Deciphering How People Detect Lies'}
{'en': 'Humans rarely perform better than chance at ', 'ar': 'نادرًا ما يكون أداء البشر أفضل من الصدفة في اكتشاف الكذب. لفهم تصور الإنسان للخداع بشكل أفضل ، أنشأنا إطارًا للعبة ، LieCatcher ، لجمع تقييمات الخداع المتصور باستخدام مجموعة كبيرة من المقابلات الخادعة والصادقة. قمنا بتحليل الخصائص الصوتية - الصوتية واللغوية للغة التي يثق بها المقيمون ولا يثقون بها ، وقارنناها بخصائص اللغة الحقيقية الصادقة والمضللة لفهم كيفية توافق الإدراك مع الواقع. باستخدام هذه البيانات ، قمنا ببناء المصنفات للتمييز تلقائيًا بين الكلام الموثوق به والكلام غير الموثوق به ، وحققنا F1 بنسبة 66.1٪. قمنا بعد ذلك بتقييم ما إذا كانت الاستراتيجيات قالت إنها استخدمتها للتمييز بين الردود الصادقة والمضللة كانت مفيدة في الواقع. تظهر نتائجنا أنه على الرغم من أن العديد من الميزات النمطيّة والمعجمية كان يُنظر إليها باستمرار على أنها جديرة بالثقة ، إلا أنها لم تكن إشارات موثوقة. أيضًا ، لم تكن الاستراتيجيات التي أبلغ القضاة عن استخدامها في اكتشاف الخداع مفيدة للمهمة. يلقي عملنا الضوء على طبيعة اللغة الموثوقة ويوفر نظرة ثاقبة للمشكلة الصعبة المتمثلة في اكتشاف الخداع البشري.', 'es': 'Los humanos rara vez rinden mejor que la detección de mentiras. Para comprender mejor la percepción humana del engaño, creamos un marco de juego, LieCatcher, para recopilar calificaciones de engaño percibido utilizando un gran corpus de entrevistas engañosas y veraces. Analizamos las características acústicas, prosódicas y lingüísticas del lenguaje en que confían y desconfían los evaluadores y las comparamos con las características del lenguaje verdadero, veraz y engañoso para comprender cómo la percepción se alinea con la realidad. Con estos datos creamos clasificadores para distinguir automáticamente la voz confiable de la desconfiada, logrando una F1 del 66,1%. A continuación, evaluamos si las estrategias que los evaluadores dijeron que utilizaban para discriminar entre respuestas veraces y engañosas eran de hecho útiles. Nuestros resultados muestran que, aunque varias características prosódicas y léxicas se percibían consistentemente como fiables, no eran señales fiables. Además, las estrategias que los jueces informaron usar en la detección de engaños no fueron útiles para la tarea. Nuestro trabajo arroja luz sobre la naturaleza del lenguaje confiable y proporciona información sobre el desafiante problema de la detección de engaños humanos.', 'pt': 'Os humanos raramente têm um desempenho melhor do que a chance na detecção de mentiras. Para entender melhor a percepção humana de engano, criamos uma estrutura de jogo, LieCatcher, para coletar classificações de engano percebido usando um grande corpus de entrevistas enganosas e verdadeiras. Analisamos as características acústico-prosódicas e linguísticas da linguagem confiável e desconfiada pelos avaliadores e as comparamos com as características da linguagem verdadeira e enganosa para entender como a percepção se alinha com a realidade. Com esses dados, construímos classificadores para distinguir automaticamente a fala confiável da não confiável, alcançando uma F1 de 66,1%. Em seguida, avaliamos se os avaliadores de estratégias que diziam usar para discriminar entre respostas verdadeiras e enganosas eram de fato úteis. Nossos resultados mostram que, embora várias características prosódicas e lexicais tenham sido consistentemente percebidas como confiáveis, elas não eram pistas confiáveis. Além disso, as estratégias que os juízes relataram usar na detecção de enganos não foram úteis para a tarefa. Nosso trabalho esclarece a natureza da linguagem confiável e fornece informações sobre o problema desafiador da detecção de enganos humanos.', 'fr': "Les humains sont rarement plus performants que le hasard pour détecter le mensonge. Pour mieux comprendre la perception humaine de la tromperie, nous avons créé un framework de jeu, LieCatcher, pour collecter des évaluations de la tromperie perçue à l'aide d'un vaste corpus d'interviews trompeuses et véridiques. Nous avons analysé les caractéristiques acoustiques-prosodiques et linguistiques d'un langage auquel les évaluateurs font confiance et méfiance et les avons comparées aux caractéristiques d'un langage réel véridique et trompeur afin de comprendre comment la perception s'aligne sur la réalité. Avec ces données, nous avons créé des classificateurs pour distinguer automatiquement les discours fiables des discours méfiants, atteignant un F1 de 66,1\xa0%. Nous avons ensuite évalué si les stratégies que les évaluateurs ont utilisées pour faire la distinction entre les réponses véridiques et trompeuses étaient effectivement utiles. Nos résultats montrent que, bien que plusieurs caractéristiques prosodiques et lexicales aient toujours été perçues comme fiables, elles n'étaient pas des indices fiables. De plus, les stratégies utilisées par les juges pour détecter les tromperies n'ont pas été utiles pour la tâche. Notre travail met en lumière la nature du langage de confiance et fournit un aperçu du problème difficile de la détection de la tromperie humaine.", 'ru': 'Люди редко демонстрируют лучшие результаты, чем вероятность обнаружения лжи. Чтобы лучше понять человеческое восприятие обмана, мы создали игровой фреймворк, LieCatcher, для сбора рейтингов воспринимаемого обмана с использованием большого корпуса обманчивых и правдивых интервью. Мы проанализировали акустико-просодические и лингвистические характеристики языка, которому доверяют и которому не доверяют эксперты, и сравнили их с характеристиками фактического правдивого и обманчивого языка, чтобы понять, как восприятие согласуется с реальностью. С помощью этих данных мы построили классификаторы, чтобы автоматически отличать доверенную речь от недоверенной, достигнув F1 66,1%. Затем мы оценили, были ли на самом деле полезны стратегии, которые, по словам экспертов, они использовали для проведения различия между правдивыми и обманчивыми ответами. Наши результаты показывают, что, хотя некоторые просодические и лексические признаки неизменно воспринимались как заслуживающие доверия, они не были надежными сигналами. Кроме того, стратегии, которые судьи использовали для выявления обмана, не были полезными для выполнения этой задачи. Наша работа проливает свет на природу доверенного языка и дает представление о сложной проблеме обнаружения человеческих обманов.', 'ja': 'ヒトは、嘘発見の機会よりも優れたパフォーマンスを発揮することはほとんどありません。 人間の欺瞞に対する認識をより深く理解するために、私たちはゲームフレームワークLieCatcherを作成し、欺瞞的で真実のインタビューの大規模なコーパスを使用して、欺瞞と認識された評価を収集しました。 評価者から信頼され、不信されている言語の音響学的特徴と言語学的特徴を分析し、これらを実際の真実で欺瞞的な言語の特徴と比較して、知覚が現実とどのように一致するかを理解した。 このデータを使用して、信頼できる音声と信頼できない音声を自動的に区別するための分類子を構築し、66.1 ％のF 1を達成しました。 次に、評価者が真実の回答と欺瞞的な回答を区別するために使用したと述べた戦略が実際に有用であるかどうかを評価しました。 我々の結果は、いくつかのプロソディックおよび語彙的特徴が一貫して信頼できると認識されていたが、信頼できる手がかりではなかったことを示している。 また、審査員が欺瞞検知で使用した戦略は、タスクに役立ちませんでした。 私たちの研究は、信頼できる言語の性質を明らかにし、人間の欺瞞検出という困難な問題についての洞察を提供します。', 'hi': 'मनुष्य शायद ही कभी झूठ का पता लगाने में मौके से बेहतर प्रदर्शन करते हैं। धोखे की मानवीय धारणा को बेहतर ढंग से समझने के लिए, हमने भ्रामक और सच्चे साक्षात्कार के एक बड़े कॉर्पस का उपयोग करके कथित धोखे की रेटिंग एकत्र करने के लिए एक गेम फ्रेमवर्क, लीकैचर बनाया। हमने रेटर्स द्वारा विश्वसनीय और अविश्वासित भाषा की ध्वनिक-प्रोसोडिक और भाषाई विशेषताओं का विश्लेषण किया और वास्तविक सच्ची और भ्रामक भाषा की विशेषताओं की तुलना में यह समझने के लिए कि धारणा वास्तविकता के साथ कैसे संरेखित होती है। इस डेटा के साथ हमने 66.1% के एफ 1 को प्राप्त करते हुए, अविश्वासपूर्ण भाषण से विश्वसनीय रूप से अलग करने के लिए क्लासिफायर का निर्माण किया। हमने अगले मूल्यांकन किया कि क्या रणनीतियों रेटर्स ने कहा कि वे सच्चे और भ्रामक प्रतिक्रियाओं के बीच भेदभाव करते थे, वास्तव में उपयोगी थे। हमारे परिणाम बताते हैं कि, हालांकि कई prosodic और lexical सुविधाओं को लगातार भरोसेमंद माना जाता था, वे विश्वसनीय संकेत नहीं थे। इसके अलावा, न्यायाधीशों ने धोखे का पता लगाने में जिन रणनीतियों का उपयोग करने की सूचना दी थी, वे कार्य के लिए सहायक नहीं थीं। हमारा काम विश्वसनीय भाषा की प्रकृति पर प्रकाश डालता है और मानव धोखे का पता लगाने की चुनौतीपूर्ण समस्या में अंतर्दृष्टि प्रदान करता है।', 'zh': '人之于测谎,鲜善于偶。 为善知欺,创一戏框架LieCatcher,多用欺骗性真采访以收其评级。 论评分者信与不信之声学韵与言,比其实与欺骗性言,以知所与今同。 因此数者,构分类以自别,信音不信,成66.1%之F1。 既而估评分员所谓区别真欺骗性之策,信有用乎? 吾之的结果表明,虽有韵词汇,固以为信,非可恃也。 此外,法官报在欺骗检测中用的方略对这件事没有帮助。 吾事发信言之质,而资人欺检之挑战性。', 'ga': 'Is annamh a n-éiríonn le daoine níos fearr ná an seans ag brath bréag. Chun tuiscint níos fearr a fháil ar dhearcadh an duine ar mheabhlaireacht, chruthaíomar creat cluiche, LieCatcher, chun rátálacha de mheabhlaireacht braite a bhailiú trí úsáid a bhaint as corpas mór d’agallamh mealltacha agus fírinneacha. Rinneamar anailís ar shaintréithe fuaimiúla-prosodacha agus teangeolaíocha teanga a bhfuil muinín ag lucht rátúcháin iontu agus nach bhfuil muinín acu astu agus chuireamar iad seo i gcomparáid le tréithe teanga atá fírinneach agus mealltach chun tuiscint a fháil ar an gcaoi a bhfuil an dearcadh ag teacht leis an réaltacht. Leis na sonraí seo, chuireamar aicmitheoirí le chéile chun idirdhealú a dhéanamh go huathoibríoch ar chaint iontaofa agus mímhuiníne, ag baint amach F1 de 66.1%. An chéad uair eile rinneamar measúnú ar cé acu an raibh na rátálacha straitéisí a dúirt siad a d’úsáid siad chun idirdhealú a dhéanamh idir freagraí atá fírinneach agus mealltach úsáideach i ndáiríre. Léiríonn ár dtorthaí, cé gur measadh go seasta go raibh roinnt gnéithe prosodic agus foclóireachta iontaofa, nach leideanna iontaofa a bhí iontu. Chomh maith leis sin, níor chuidigh na straitéisí a thuairiscigh na breithiúna gur úsáideadh iad chun an mheabhlaireacht a bhrath don tasc. Tugann ár gcuid oibre solas ar nádúr na teanga iontaofa agus tugann sé léargas ar an bhfadhb dhúshlánach a bhaineann le meabhlaireacht dhaonna a bhrath.', 'ka': 'ადამიანები წარმოდგენა უფრო მეტი, ვიდრე შესაძლებლობის განვიცნობაში. რომ უფრო უფრო გავიგეთ ადამიანის შეცდომის შეცდომა, ჩვენ შევქმნა თამაში სტრუმენტი, LieCatcher, რომელიც შევქმნით შეცდომის შეცდომის რეტენტი, რომელიც გამოყენება დიდი კორ ჩვენ აკსტიკური პრონოსტიკური და ლენგურისტიკური პროპრატიკური პროპრატიკური განსაზღვრებების და არასწორად გვერდილი რეტერტებით და ამ პროპრატიკური განსაზღვრებით აკსტიკური და მარტიკური ენის პრო ამ მონაცემებით ჩვენ კლასიფიკაცირებით ავტომატურად გადავწეროთ, რომლებიც არ გვერდილი სიტყვებით დავწეროთ, 66.1%-ის F1-ს მივიღეთ. ჩვენ შემდეგ გავამუშავოთ თუ არა სტრატიგიების რეტერისტები თქვა, რომ ისინი დისკრიმინაციას მართლად და მართლად განსხვავების შორის გამოყენება საკუთარ საჭირო ჩვენი შედეგი გამოჩნდა, რომ, თუმცა რამდენიმე პროსოდიული და ლექსიკალური განსხვავებები ყოველთვის იყენებულია როგორც გვერდილი, ისინი არ იყო დარწმუნებელი ასევე, სტრატიგიები, რომლებიც სექსიები შეუძლიათ გამოიყენება ტალიქტურის განახლებაში, არ იყო დასახმარებელი რაოდენობისთვის. ჩვენი სამუშაო მუშაობაში ჩვენი სამუშაო წარმოიდგინება გვერდილი ენაზე და დაახლოების შესაძლებელი პრობლემები ადამიანის განახლოების შესაძლებელი პრო', 'el': 'Οι άνθρωποι σπάνια αποδίδουν καλύτερα από την πιθανότητα ανίχνευσης ψεύδους. Για να κατανοήσουμε καλύτερα την ανθρώπινη αντίληψη για την εξαπάτηση, δημιουργήσαμε ένα πλαίσιο παιχνιδιού, το LieCatcher, για να συλλέξουμε αξιολογήσεις για την αντιληπτή εξαπάτηση χρησιμοποιώντας ένα μεγάλο σώμα παραπλανητικών και ειλικρινών συνεντεύξεων. Αναλύσαμε τα ακουστικά-προσωodische και γλωσσικά χαρακτηριστικά της γλώσσας που εμπιστεύονται και δεν εμπιστεύονται οι αξιολογητές και τα συγκρίναμε με τα χαρακτηριστικά της πραγματικής ειλικρινούς και παραπλανητικής γλώσσας για να κατανοήσουμε πώς ευθυγραμμίζεται η αντίληψη με την πραγματικότητα. Με αυτά τα δεδομένα δημιουργήσαμε ταξινομητές για να διακρίνουμε αυτόματα την αξιόπιστη από την δυσπιστία ομιλία, επιτυγχάνοντας ένα F1 των 66.1%. Στη συνέχεια αξιολογήσαμε αν οι στρατηγικές που αξιολογούν αναφέρουν ότι χρησιμοποιούσαν για να κάνουν διακρίσεις μεταξύ αληθών και παραπλανητικών απαντήσεων ήταν στην πραγματικότητα χρήσιμες. Τα αποτελέσματά μας δείχνουν ότι, αν και αρκετά προνodische και λεξικά χαρακτηριστικά θεωρήθηκαν σταθερά αξιόπιστα, δεν ήταν αξιόπιστα στοιχεία. Επίσης, οι στρατηγικές που ανέφεραν οι δικαστές να χρησιμοποιούν για την ανίχνευση εξαπάτησης δεν ήταν χρήσιμες για το έργο. Η εργασία μας ρίχνει φως στη φύση της αξιόπιστης γλώσσας και παρέχει διορατικότητα στο δύσκολο πρόβλημα της ανίχνευσης ανθρώπινης εξαπάτησης.', 'hu': 'Az emberek ritkán teljesítenek jobban, mint a hazugság felismerésére való esély. Annak érdekében, hogy jobban megértsük az emberi megtévesztésre vonatkozó érzékelést, létrehoztunk egy játék keretrendszert, a LieCatcher-t, amely az észlelt megtévesztésekről szóló értékeléseket gyűjti össze egy nagy halmaz megtévesztő és igaz interjú segítségével. Elemeztük a besorolók által megbízott és bizalmatlan nyelv akusztikus-prozódikus és nyelvi jellemzőit, és összehasonlítottuk ezeket a valódi igazságos és megtévesztő nyelv jellemzőivel, hogy megértsük, hogyan illeszkedik az érzékelés a valósághoz. Ezekkel az adatokkal osztályozókat építettünk, hogy automatikusan megkülönböztessük a megbízható és a bizalmatlan beszédet, elérve az F1 66,1%. Ezt követően értékeltük, hogy a stratégiák minősítői azt állították, hogy az igaz és megtévesztő válaszok közötti megkülönböztetésre hasznosak-e. Eredményeink azt mutatják, hogy bár számos prosódikus és lexikai jellemzőt következetesen megbízhatónak tartották, nem voltak megbízható utak. Továbbá azok a stratégiák, amelyeket a bírók a megtévesztés felismerésében használtak, nem voltak hasznosak a feladathoz. Munkánk rávilágít a megbízható nyelv természetére és betekintést nyújt az emberi megtévesztés felismerésének kihívást jelentő problémájába.', 'it': "Gli esseri umani raramente riescono meglio della possibilità di individuare le bugie. Per comprendere meglio la percezione umana dell'inganno, abbiamo creato un framework di gioco, LieCatcher, per raccogliere valutazioni dell'inganno percepito utilizzando un ampio corpus di interviste ingannevoli e veritiere. Abbiamo analizzato le caratteristiche acustiche-prosodiche e linguistiche del linguaggio fidato e diffidente dai valutatori e confrontate queste con le caratteristiche del linguaggio vero e ingannevole per capire come la percezione si allinea con la realtà. Con questi dati abbiamo costruito classificatori per distinguere automaticamente il parlato fidato da quello diffidente, raggiungendo una F1 del 66,1%. Abbiamo poi valutato se i valutatori delle strategie dicevano di discriminare tra risposte veritiere e ingannevoli fossero effettivamente utili. I nostri risultati mostrano che, sebbene diverse caratteristiche prosodiche e lessicali siano state costantemente percepite come affidabili, non erano indizi affidabili. Inoltre, le strategie che i giudici hanno riferito di utilizzare nel rilevamento degli inganni non sono state utili per il compito. Il nostro lavoro mette in luce la natura del linguaggio fidato e fornisce informazioni sul difficile problema della rilevazione dell'inganno umano.", 'lt': 'Žmonės retai veikia geriau nei tikimybė nustatyti melas. To better understand human perception of deception, we created a game framework, LieCatcher, to collect ratings of perceived deception using a large corpus of deceptive and truthful interviews.  Analizėme vertintojų patikimos ir nepasitikėtos kalbos akustines, prosodines ir kalbines savybes ir palyginome jas su tikros tiesos ir apgaulingos kalbos savybėmis, kad suprastume, kaip suvokimas suderinamas su tikrove. Su šiais duomenimis sukūrėme klasifikatorius, kad automatiškai atskirtume patikimą nuo nepasitikėtos kalbos ir pasiektume 66,1 % F1. Toliau įvertinome, ar strategijų vertintojai teigė, kad jie buvo naudojami diskriminuojant tikrus ir apgaulingus atsakymus, iš tikrųjų buvo naudingi. Mūsų rezultatai rodo, kad nors keli prosodiniai ir leksiniai požymiai buvo nuosekliai laikomi patikimais, jie nebuvo patikimi požymiai. Be to, strategijos, kurias teisėjai pranešė naudoti apgaulės aptikimui, nebuvo naudingos uždaviniui atlikti. Mūsų darbas atskleidžia patikimos kalbos pobūdį ir suteikia supratimą apie sudėtingą žmogaus apgaulės aptikimo problem ą.', 'ms': 'Manusia jarang melakukan lebih baik daripada peluang untuk mengesan kebohongan. Untuk memahami lebih baik perasaan manusia tentang penipuan, kami mencipta kerangka permainan, LieCatcher, untuk mengumpulkan nilai penipuan yang dirasakan menggunakan mayat besar dari temuduga penipuan dan jujur. Kami menganalisis ciri-ciri akustik-prosodik dan bahasa bahasa yang dipercayai dan tidak dipercayai oleh penentu dan membandingkannya dengan ciri-ciri bahasa sebenar yang benar dan penipu untuk memahami bagaimana persepsi menyesuaikan dengan realiti. Dengan data ini kami bina pengklasifikasi untuk membezakan secara automatik yang dipercayai dari ucapan yang tidak dipercayai, mencapai F1 66.1%. Seterusnya kami menilai sama ada penentu strategi mengatakan mereka digunakan untuk mendiskriminasi antara jawapan yang benar dan penipuan sebenarnya berguna. Hasil kami menunjukkan bahawa walaupun beberapa ciri-ciri prosodik dan leksikal secara konsisten dianggap sebagai dipercayai, mereka bukan tanda-tanda yang dipercayai. Juga, strategi yang hakim laporkan menggunakan dalam pengesan penipuan tidak membantu untuk tugas. Our work sheds light on the nature of trusted language and provides insight into the challenging problem of human deception detection.', 'mk': 'Луѓето ретко работат подобро од шансите за детективирање лаги. За подобро да ја разбереме човечката перцепција на измама, создадовме рамка за игра, LieCatcher, за да собереме оценки за перцепирана измама користејќи голем корпус на измамни и искрени интервјуа. Ги анализиравме акустичките-прозодични и лингвистичките карактеристики на јазикот кој им е доверлив и недоверлив на процентите и ги споредивме со карактеристиките на вистинскиот и измамен јазик за да разбереме како перцепцијата се согласува со реалноста. Со овие податоци ние изградивме класификатори за автоматски да се разликуваат доверливите од недоверливиот говор, достигнувајќи F1 од 66,1 отсто. Следниот пат проценивме дали стратегиските проценки велат дека користеле дискриминација помеѓу вистинските и измамните одговори всушност беа корисни. Our results show that, although several prosodic and lexical features were consistently perceived as trustworthy, they were not reliable cues.  Исто така, стратегиите кои судиите известија дека ги користат за детекција на измами не беа корисни за задачата. Нашата работа дава светлина на природата на доверливиот јазик и обезбедува сфаќање за предизвикувачкиот проблем на откривањето на човечките измами.', 'ml': 'കള്ള കണ്ടുപിടിക്കുന്നതിനെക്കാള്\u200d മനുഷ്യര്\u200d കുറച്ച് പ്രവര്\u200dത്തിക്കുന്നില്ല. വഞ്ചിക്കുന്ന മനുഷ്യന്റെ പ്രത്യേകം നന്നായി മനസ്സിലാക്കാന്\u200d വേണ്ടി, നമ്മള്\u200d ഒരു കളിയുടെ ഫ്രെയിമാര്\u200dക്ക് സൃഷ്ടിച്ചു, ലീക്കാച്ചര്\u200d, മനസ ഞങ്ങള്\u200d ഭാഷ വിശ്വസിക്കുന്നതും വിശ്വസിക്കുന്നതും തെറ്റും വിശ്വസിക്കുന്നതുമായ ഭാഷയുടെ അക്കോസ്റ്റിക്കും ഭാഷകങ്ങള്\u200d അന്വേഷിക്കുകയും ചെയ്തു. യഥാര്\u200dത്ഥ സത് ഈ വിവരങ്ങള്\u200d കൊണ്ട് നമ്മള്\u200d സ്വയം വിശ്വസിക്കാന്\u200d വേണ്ടി വിശ്വസിക്കുന്ന വാക്കുകള്\u200d നിര്\u200dമ്മിച്ചു. 66. We next evaluated whether the strategies raters said they used to discriminate between truthful and deceptive responses were in fact useful.  നമ്മുടെ ഫലങ്ങള്\u200d കാണിച്ചു കൊണ്ടിരിക്കുന്നു, കുറച്ചു പ്രൊസോഡിക്കക്കാരും വിശ്വസ്തന്മാരുമായ വ്യക്തിത്വങ്ങളും എന വിഡ്ഢിത്തം കണ്ടുപിടിക്കുന്നതില്\u200d ജഡ്ജുക്കള്\u200d റിപ്പോര്\u200dട്ട് ചെയ്തിട്ടുള്ള പദ്ധതികള്\u200d ജോലിക നമ്മുടെ ജോലി വിശ്വസ്തനായ ഭാഷയുടെ സ്വഭാവത്തെക്കുറിച്ച് വെളിച്ചം കാണിക്കുന്നു. മനുഷ്യന്\u200d വഞ്ചിക്കുന്', 'kk': 'Адамдар жалғасты анықтау мүмкіндігінен жақсы жұмыс істейді. Адамдың ақпараттың түсініктерін жақсы түсіндіру үшін, біз ойының фреймін жасадық, ЛиКатчер, оқылған ақпараттың үлкен және дұрыс сұрақтардың үлкен корпусын қолдану үшін түсін Біз сенімді тілдердің акустикалық-просодикалық және лингвистикалық қасиеттерін анализировадық. Бұларды дұрыс және дұрыс тілдің қасиеттеріне қалай түсінікті түсініп қалады. Бұл деректермен сенімді сөздерден автоматты түрде біз классификаторларды құрып, 66,1% F1 дегенді жеткіздік. Біз келесіден стратегиялардың бағалаушылары дұрыс және дұрыс жауаптардың арасындағы бөлексіздігін бағаладық. Біздің нәтижелеріміз бірнеше просодикалық және лексикалық мүмкіндіктері әрқашан сенімді деп ойлайтынын көрсетеді. Олар сенімді белгілер болмайды. Сонымен қатар, мұндай тапсырманы табуға көмектесмейді. Біздің жұмысамыз сенімді тілдің қасиетіне жарықтығын түсіндіреді және адамдардың оқиғаларын анықтау мәселесін түсіндіреді.', 'no': 'Mennesker ofte utfører bedre enn sjanse for å oppdaga låg. For å bedre forstå menneskelige oppfatningar av vilkårleg, oppretta vi eit spel-rammeverk, LieCatcher, for å samla oppfattingar av oppfatta vilkårleg med ein stor korpus av vilkårleg og sannsynleg intervjuar. Vi analysere akustiske-prosodiske og språkstiske karakteristikk av språk som vert tiltrudd og ikkje tiltrudd av raterar, og sammenligna desse med karakteristika av virkelige sannsynleg og falsifisert språk for å forstå korleis oppfatninga er tilsvarande med virkelighet. Med denne data bygge vi klassifikatorar for å automatisk skilja tiltrudd frå feiltiltrudd tale, og når det gjer ein F1 av 66,1%. Vi evaluerte om strategiraterane sa at dei brukte til å diskriminere mellom sannsynleg og dekraftig svar var faktisk nyttig. Resultatet våre viser at, selv om fleire prosodiske og leksiske funksjonar vart konsistentvis oppfatta som tiltrudige, var dei ikkje tiltrudige teikn. Strategiene som reglar rapportert ved å bruka i oppdaging av deltak var ikkje nyttig for oppgåva. Arbeidet vårt gjer lys på naturen av tiltrudde språk og gjer innsikt i den vanskelige problemen for oppdaging av menneskelige forbetringar.', 'pl': 'Ludzie rzadko radzą sobie lepiej niż szansa na wykrywanie kłamstw. Aby lepiej zrozumieć ludzkie postrzeganie oszustwa, stworzyliśmy ramy gry LieCatcher, aby zbierać oceny postrzeganych oszustw przy użyciu dużego zbioru oszustw zwodniczych i prawdziwych wywiadów. Przeanalizowaliśmy akustyczno-prozodyczne i językowe cechy języka zaufanego i nieufanego przez oceniaczy i porównaliśmy je z cechami rzeczywistego prawdziwego i zwodniczego języka, aby zrozumieć, w jaki sposób percepcja łączy się z rzeczywistością. Dzięki tym danym zbudowaliśmy klasyfikatory, które automatycznie odróżniają zaufaną od niezaufanej mowy, osiągając F1 w wysokości 66.1%. Następnie oceniliśmy, czy oceniający strategie, które używają do rozróżniania prawdziwych i zwodniczych odpowiedzi, są rzeczywiście użyteczne. Nasze wyniki pokazują, że chociaż kilka cech prozodycznych i leksykalnych było konsekwentnie postrzeganych jako godnych zaufania, nie były one wiarygodnymi wskazówkami. Ponadto strategie, które sędziowie zgłosili w wykrywaniu oszustw, nie były pomocne w tym zadaniu. Nasza praca rzuca światło na naturę zaufanego języka i daje wgląd w trudny problem wykrywania ludzkich oszustw.', 'mn': 'Хүмүүс худлаа олох боломжтоос илүү сайн ажилладаг. Хүн төрөлхтний худлааны ойлголтыг илүү ойлгохын тулд бид ЛиКачер тоглоомын хөтөлбөр бүтээсэн. Мэдээж бусдын хуурамч, үнэн ярилцлагын том корпус ашиглан ойлголтын үнэн цуглуулалтыг цуглуулахын тулд Бид үнэндээ итгэлтэй, буруу итгэлтэй хэлний акустик, хэл хэлний хувьцааны хувьцааны хувьцааны хувьцааны хувьцааны шинжилгээг судалж, эдгээрийг бодит байдлаар хэрхэн харьцуулдгийг ойлгохын тулд үнэн, хуурамч хэлн Энэ өгөгдлийн хувьд бид хувьд итгэлтэй яриагаас автоматаар хуваалцахын тулд хуваалцагчид бүтээсэн бөгөөд 66.1% F1 хүртэл итгэлтэй байдаг. Дараа нь бид стратегийн оюутнууд үнэн болон хуурамч хариултын хоорондоо ялгаатай эсэхийг үнэндээ үнэндээ хэрэгтэй гэж хэлсэн. Бидний үр дүнд хэд хэдэн сэтгэл хөдлөл, сэтгэл хөдлөл нь итгэл үнэ цэнэтэй гэж үздэг ч, тэд итгэл үнэ цэнэтэй байдал биш. Мөн шүүгчдийн хэрэглэх стратеги нь ажил дээр тусламжгүй байсан. Бидний ажил итгэлтэй хэлний байгалийг гэрэлтэй болгож, хүн төрөлхтний шийдвэрлэлтийн хэцүү асуудлыг олж мэддэг.', 'ro': 'Oamenii rareori se descurcă mai bine decât şansa de a detecta minciunile. Pentru a înțelege mai bine percepția umană a înșelăciunii, am creat un cadru de joc, LieCatcher, pentru a colecta evaluări ale înșelăciunii percepute folosind un corpus larg de interviuri înșelătoare și adevărate. Am analizat caracteristicile acustic-prosodice și lingvistice ale limbajului de încredere și neîncredere de către evaluatori și le-am comparat cu caracteristicile limbajului real adevărat și înșelător pentru a înțelege cum percepția se aliniază cu realitatea. Cu aceste date am construit clasificatori pentru a distinge automat vorbirea de încredere de cea neîncrezătoare, atingând un F1 de 66,1%. Apoi am evaluat dacă evaluatorii strategiilor au declarat că discriminau între răspunsurile adevărate și înșelătoare au fost de fapt utile. Rezultatele noastre arată că, deși mai multe caracteristici prosodice și lexicale au fost percepute în mod constant ca fiind de încredere, ele nu au fost indicii fiabile. De asemenea, strategiile pe care judecătorii au raportat că le-au folosit în detectarea înșelăciunii nu au fost utile pentru această sarcină. Munca noastră pune în lumină natura limbajului de încredere și oferă o perspectivă asupra problemei provocatoare a detectării înșelăciunii umane.', 'sr': 'Ljudi retko izvršavaju bolje od šanse da otkriju laži. Da bi bolje razumeli ljudsku percepciju prevare, stvorili smo okvir igre, LieCatcher, kako bi skupili ocjene percepcije prevare koristeći veliki korpus prevarantnih i iskrenih intervjua. Analizirali smo akustičke prosodske i jezičke karakteristike jezika koje su verovali i nepovjereni ocenicima, i usporedili smo to sa karakteristikama stvarnog istinskog i prevarantnog jezika kako bi razumeli kako percepcija odgovara realnosti. Sa ovim podacima smo izgradili klasifikatore da bi automatski odvojili povjerenje od nepovjerenog govora, ostvarili F1 od 66,1%. Sledeće smo procenili da li su ratenci strategija rekli da su diskriminirali između istinskih i lažnih odgovora bili koristni. Naši rezultati pokazuju da, iako su nekoliko prosodijskih i leksičkih karakteristika stalno smatrali povjerljivim, nisu bili pouzdani znakovi. Takođe, strategije koje su sudije prijavili kako koriste u otkrivanju prevare nisu bile od koristi za taj zadatak. Naš rad otkriva prirodu vernog jezika i pruža uvid u izazovni problem otkrivanja ljudskih prevara.', 'si': 'මිනිස්සුන්ට බොරු හොයාගන්න අවස්ථාවට වඩා හොඳ වැඩ කරන්න පුළුවන්. මිනිස්සුන්ගේ අදහසක් හොඳට තේරුම් ගන්න, අපි සෙල්ලම් සංකේතයක් නිර්මාණය කළා, ලිකැචර්, දැනගන්න පුළුවන්ගේ රේටින් සංක අපි භාෂාව විශ්වාස කරපු සහ විශ්වාස කරපු භාෂාව සහ භාෂාවික වර්ගයක් විශ්වාස කරලා තියෙන්නේ රේටර්ස් වලින් විශ්වාස කරපු සහ ම මේ තොරතුරු එක්ක අපි ස්වයංක්\u200dරියාවිතයෙන් විශ්වාස කරන්න විශ්වාස විශ්වාස කරන්න, 66.1% වල F1 එකක් ලබාගන්න. අපි ඊළඟ විශ්වාස කරලා තියෙන්නේ නැද්ද කියලා තියෙන්නේ නැද්ද කියලා කියලා එයාලා ඇත්තටම ප්\u200dරයෝජනය කරනවා කියල අපේ ප්\u200dරතිචාරය පෙන්වන්නේ, ප්\u200dරොසෝඩික් සහ ලෙක්සිකල් අවශ්\u200dය වලින් විශ්වාසයි කියලා, ඔවුන් විශ්වාසය ඒවගේම, විශ්වාසියෝ පරීක්ෂණයේ පාවිච්චි කරන්න පුළුවන් විදිහට පරීක්ෂා කරලා තියෙන්නේ නැ අපේ වැඩේ විශ්වාස කරපු භාෂාවේ ස්වභාවිතාවට ආලෝකය පිළිගන්නවා ඒ වගේම මිනිස්සුන්ගේ විශ්වාස කරප', 'so': 'Dadku waxey si yar u sameeyaan suurtagal ka wanaagsan in la ogaado beenta. Si aannu ugu fiican u garanayno aragtida khiyaanada dadka, waxaynu abuurnay firaaqad ciyaar ah, LieCatcher, si aannu u soo ururiyno qayb khiyaano lagu garto, si aannu ugu soo ururinno wareegayaal badan oo khiyaano badan oo runta ah. Waxaannu analyeelnay takhasuska afka ee hore iyo luqada ah oo ay isku aaminsan yihiin oo ay ku kalsoonaan jireen, waxaana isbarbarbardhignay takhasuska afka runta ah iyo khiyaanada si aan u garanayno sida muuqashada runta ah ugu siman karo. Macluumaadkan waxaan ku dhisnay fasaxyo si aan automatic uga kala soocno hadalka aan aamin ahayn, oo aan gaadhno F1 oo 66.1 boqolkiiba. Waxaannu qiimeynay waqtiga dambe inay si dhab ah u takoori jireen jawaabaha xaqa iyo khiyaanada. Abaalkayaga waxay muuqataa in kastoo xittaa qaar ka mid ah dhillooyin iyo waxyaabo la xiriiray ay si aamin ah u yihiin, ayan ahayn cudur aamin ah. Sidoo kale qalabka xaakinnadu ay ku soo sheegeen isticmaalka khiyaanada waxey faa’iido u lahayn shaqada. Shaqo-kayagu wuxuu iftiimiyaa dabiicadda luqada aaminka ah, waxna waxgarashada dhibaatada dhibaatada adag ee soo ogaashada khiyaanada dadka.', 'sv': 'Människor presterar sällan bättre än chansen att upptäcka lögn. För att bättre förstå människans uppfattning om bedrägeri skapade vi ett spelramverk, LieCatcher, för att samla in betyg av upplevd bedrägeri med hjälp av en stor samling vilseledande och sanningsenliga intervjuer. Vi analyserade akustisk-prosodiska och språkliga egenskaper hos språk som betroddes och misstrodda av bedömare och jämförde dessa med egenskaper hos verkliga sanningsenliga och vilseledande språk för att förstå hur perception anpassar sig till verkligheten. Med dessa data byggde vi klassificerare för att automatiskt skilja förtroende från misstroende tal, vilket uppnådde en F1 på 66,1%. Därefter utvärderade vi om strategierna bedömare sade att de brukade diskriminera mellan sanningsenliga och vilseledande svar faktiskt var användbara. Våra resultat visar att även om flera prosodiska och lexikala egenskaper konsekvent uppfattades som pålitliga, var de inte tillförlitliga ledtrådar. De strategier som domare rapporterade använda vid bedrägeridetektering var inte heller till hjälp för uppgiften. Vårt arbete belyser karaktären av betrodd språk och ger insikt i det utmanande problemet med upptäckt av mänskligt bedrägeri.', 'ta': 'பொய் கண்டுபிடிப்பதில் மனிதர்கள் சிறந்த வாய்ப்பை விட செயல்படுத்துவது குறைவாக. ஏமாற்றத்தின் மனிதன் புரியும் நல்ல புரிந்து கொள்ள, நாம் ஒரு விளையாட்டு சட்டத்தை உருவாக்கினோம், லீக்கேசர், உணர்ந்த புரிந்த வஞ்சகம நாம் மொழி நம்பிக்கை மற்றும் தவறான நம்பிக்கை மற்றும் நம்பிக்கை மொழியின் மொழியின் மொழியின் மொழிப்பெரும் மொழியை ஆராய்ந்தோம் மற்றும் உண்மையான ம இந்த தரவுடன் நாங்கள் வகுப்பாளர்களை உருவாக்கி தானாகவே பிரித்து நம்பிக்கப்பட்ட பேச்சிலிருந்து நம்பிக்கை கொள்ள, 66. 1%  We next evaluated whether the strategies raters said they used to discriminate between truthful and deceptive responses were in fact useful.  நம்முடைய முடிவுகள் காண்பிக்கப்படுகிறது, சில விபசாரணங்கள் மற்றும் வெறுக்கல் குணங்கள் முடிவிலும் நம்பிக்கை கொள் மேலும், வழக்கமான கண்டுபிடிப்பில் வழங்குபவர்கள் அறிவித்த திட்டங்கள் பணிக்கு உதவாது. நம்முடைய வேலை நம்பிக்கை மொழியின் இயற்கையை வெளிச்சமாக காட்டுகிறது மற்றும் மனித ஏமாற்றல் கண்டுபிடிப்பதி', 'ur': 'انسانوں کو جھوٹی بات کا اچھا کام نہیں کرتا۔ انسان کی خیانت کو بہتر سمجھنے کے لئے، ہم نے ایک کھیل فرمود بنایا، لیکچر، کہ ایک بڑے دروغگو اور سچا مصاحبات کے مطابق بہت بڑے دروغگو کی کورپوس کے ذریعہ ذخیره کرنے کے لئے نظر یافتہ خیانت کی راتینگ جمع کریں. اور ہم نے اس زبان کی مثالیں تحقیق کی ہیں جو امانت دار اور ناامید ہیں اور ان کی مثالیں سچی اور دھوکہ زبان کی مثالیں سمجھ لیں کہ عقل کس طرح سچی اور دھوکہ کی مثالیں ہیں ہم نے اس اطلاعات کے ذریعہ مخلوقات بنائے تاکہ اپنے ساتھ مطمئن ہونے والی باتوں سے اعتماد کریں، 66.1% کے F1 کو پہنچ سکیں۔ پھر ہم نے اس کا مطالبہ کیا کہ کیا ان کا مطالبہ اعتبار کرنے والوں نے کہا تھا کہ انہوں نے سچے اور فریب دینے والوں کے درمیان اختلاف کیا ہے ہمارے نتیجے دکھاتے ہیں کہ اگرچہ بہت سی قوانین اور زبان کی نشانیاں ثابت قدم رکھی گئی تھیں ان کو امانت دار نہ تھا اور یہ باتیں بھی ہیں جن کے فیصلہ کرنے والوں نے فریب کا اظہار کیا تھا کہ اس کام کے لئے کچھ فائدہ نہ تھا۔ ہمارا کام بھروسہ زبان کی طبیعت پر روشنی کرتا ہے اور انسان کی فریب کا مشکل معلوم کرتا ہے۔', 'mt': 'Rari l-bnedmin jagħmlu aħjar minn ċans li jidentifikaw il-bużijiet. Biex nifhmu a ħjar il-perċezzjoni tal-bniedem tal-inganna, ħolqu qafas tal-logħob, LieCatcher, biex niġbru klassifikazzjonijiet tal-inganna perċepita bl-użu ta’ korpus kbir ta’ intervisti qarrieqa u veri. Aħna analizzaw il-karatteristiċi akustiċi-prosodiċi u lingwistiċi tal-lingwa ta’ fiduċja u nuqqas ta’ fiduċja mill-klassifikaturi u qabblu dawn mal-karatteristiċi tal-lingwa vera u qarrieqa biex nifhmu kif il-perċezzjoni tallinja mar-realtà. B’din id-dejta bnijna klassifikaturi biex jiddistingwu awtomatikament il-fiduċja minn diskors mhux fiduċjat, u kisbu F1 ta’ 66.1%. Imbagħad ivvalutajna jekk ir-rati tal-istrateġiji qalux li użawx biex jiddiskriminaw bejn reazzjonijiet veri u qarrieqa kinux effettivament utli. Ir-riżultati tagħna juru li, għalkemm diversi karatteristiċi prosodiċi u lexiċi kienu konsistentement meqjusa bħala affidabbli, dawn ma kinux sinjali affidabbli. Barra minn hekk, l-istrateġiji li l-imħallfin irrappurtaw li jużaw fl-iskoperta tal-inganna ma kinux ta’ għajnuna għall-kompitu. Ix-xogħol tagħna joħloq dawl fuq in-natura tal-lingwa ta’ fiduċja u jipprovdi għarfien dwar il-problema ta’ sfida ta’ individwazzjoni ta’ ingannament uman.', 'uz': "Insonlar yolg'onni aniqlash imkoniyatindan yaxshi bajaradi. Biz odamning o'zini o'rganishni yaxshi o'rganish uchun, biz o'yincha o'yincha o'ylab, LieCatcher, bizning ko'plab o'ylab-yolg'onni o'rganish uchun o'ylab-yolg'on va haqiqiqiy interviewlar bilan bir katta qo'shiq Biz tilning acoustic prosodik va lingvistik xususiyatlarini o'rganish va oddiy ishlatuvchilar bilan ishlatadigan va ishonchingizni tasavvur qildik va bu haqiqiqiy va khiyaano tilning xususiyatlariga kamaytirish mumkin. Ushbu maʼlumot bilan biz bir darajalarni avtomatik ravishda ajratishga ishonchingiz mumkin, 66.1%'dan F1 ishga tushirish uchun. Biz keyingi qiymatimiz: strategiya ratektlari, ular haqiqiqiy va khitiliy javoblarning orasidagi ajratishga foydali deb aytadi. Bizning natijalarimizni ko'rsatadi, agar bir necha prosodik va leksikal xususiyatlar davomida ishonroq deb tushunishdi, ular ishonchini ishonch emas. Hujjatlar vazifani aniqlashda ruxsat beradigan strategiyalar vazifani yordam beradi. Bizning ishlarimiz ishonch ishonch bo'lgan tilning xususiyatini ko'rsatadi va odamning khiyaano'rganish muammolarining qiziqarli muammolarini anglatadi.", 'vi': 'Loài người hiếm khi thực hiện tốt hơn cơ hội để phát hiện nói dối. Để hiểu rõ nhận thức của con người về lừa dối, chúng tôi đã tạo ra một hệ thống trò chơi, LieCatco, để thu thập các đánh giá về ảo thuật được nhận thức bằng một tập đoàn lớn các cuộc phỏng vấn dối trá và chân thật. Chúng tôi phân tích các đặc tính âm-văn và ngôn ngữ của ngôn ngữ được huấn luyện và không tin tưởng bởi những người chuột, và so sánh chúng với những đặc điểm của ngôn ngữ thật sự và dối trá để hiểu cách nhận thức phù hợp với thực tại. Với dữ liệu này, chúng tôi đã tạo ra những phân loại được tin cậy từ ngôn ngữ sai, đạt được dạng F1 của Hey. Tiếp theo chúng ta sẽ đánh giá xem những chiến lược người ta nói họ đã sử dụng để phân biệt sự thật và phản ứng lừa dối là có ích hay không. Những kết quả của chúng tôi cho thấy, mặc dù có nhiều tính năng văn học và ngôn ngữ liên tục được xem là đáng tin cậy, nhưng chúng không phải là manh mối đáng tin cậy. Thêm vào đó, các chiến lược mà các thẩm phán báo cáo sử dụng trong việc phát hiện lừa dối không có ích cho nhiệm vụ. Công việc của chúng ta soi sáng bản chất của ngôn ngữ tin cậy và cung cấp tầm nhìn vào vấn đề thách thức của việc phát hiện lừa dối loài người.', 'bg': 'Хората рядко се представят по-добре от шанса за откриване на лъжа. За да разберем по-добре човешкото възприятие за измама, създадохме игрова рамка за събиране на рейтинги за възприеманата измама, използвайки голям корпус от измамни и истинни интервюта. Анализирахме акустично-прозодичните и лингвистичните характеристики на езика, на който се доверяват и не се доверяват оценителите, и ги сравнихме с характеристиките на действителния истинен и измамен език, за да разберем как възприятието съответства на реалността. С тези данни изградихме класификатори, които автоматично разграничават доверието от недоверието на речта, постигайки Ф1 от 66.1%. След това оценихме дали стратегиите, които оценителите казаха, че използват за разграничаване между истинни и измамни отговори, всъщност са полезни. Резултатите ни показват, че въпреки че няколко прозодични и лексикални особености са били възприемани последователно като надеждни, те не са били надеждни знаци. Също така стратегиите, които съдиите съобщават, че са използвали за откриване на измама, не са били полезни за задачата. Работата ни хвърля светлина върху естеството на доверения език и дава представа за предизвикателния проблем с откриването на човешка измама.', 'da': 'Mennesker præsterer sjældent bedre end chancen for at opdage løgne. For bedre at forstå menneskelig opfattelse af bedrageri har vi skabt et spilramme, LieCatcher, til at indsamle vurderinger af opfattet bedrageri ved hjælp af et stort korpus af vildledende og sandfærdige interviews. Vi analyserede de akustiske-prosodiske og sproglige karakteristika ved sprog, der betroes og mistroes af bedømmere, og sammenlignede disse med karakteristika ved faktisk sandfærdigt og vildledende sprog for at forstå, hvordan opfattelse stemmer overens med virkeligheden. Med disse data har vi bygget klassificerere til automatisk at skelne tillid fra mistillid tale og opnå en F1 på 66,1%. Vi vurderede derefter, om strategivurderingerne sagde, at de plejede at skelne mellem sandfærdige og vildledende svar faktisk var nyttige. Vores resultater viser, at selvom flere prosodiske og leksikske træk konsekvent blev opfattet som troværdige, var de ikke pålidelige signaler. Desuden var de strategier, som dommere rapporterede at bruge til detektering af bedrag, ikke nyttige til opgaven. Vores arbejde kaster lys over arten af betroet sprog og giver indsigt i det udfordrende problem med menneskelig bedrag detektering.', 'nl': 'Mensen presteren zelden beter dan kans op leugendetectie. Om de menselijke perceptie van misleiding beter te begrijpen, creëerden we een game framework, LieCatcher, om beoordelingen van waargenomen misleiding te verzamelen met behulp van een groot corpus van misleidende en waarheidsgetrouwe interviews. We analyseerden de akoestisch-prosodische en taalkundige kenmerken van taal die door beoordelaars vertrouwd en wantrouwen en vergeleken deze met kenmerken van werkelijke waarheidsgetrouwe en misleidende taal om te begrijpen hoe perceptie aansluit op de werkelijkheid. Met deze gegevens hebben we classificatoren gebouwd om automatisch vertrouwd en wantrouwig spraak te onderscheiden, waardoor een F1 van 66.1%. Vervolgens evalueerden we of de strategieën die beoordelaars zeiden dat ze gebruikten om onderscheid te maken tussen waarheidsgetrouwe en misleidende reacties in feite nuttig waren. Onze resultaten tonen aan dat, hoewel verschillende prosodische en lexicale kenmerken consequent als betrouwbaar werden ervaren, ze geen betrouwbare aanwijzingen waren. Ook waren de strategieën die rechters aangaven te gebruiken bij het detecteren van misleidingen niet nuttig voor de taak. Ons werk werpt licht op de aard van vertrouwde taal en geeft inzicht in het uitdagende probleem van detectie van menselijke misleiding.', 'hr': 'Ljudi rijetko izvode bolje od šanse za otkrivanje laži. Da bi bolje shvatili ljudsko percepciju prevare, stvorili smo okvir igre, LieCatcher, kako bi skupili ocjene percepcije prevare koristeći veliki korpus prevarantnih i iskrenih intervjua. Analizirali smo akustičke prosodične i jezičke karakteristike jezika koje su vjerovali i nepovjereni ocjennicima i usporedili ih s karakteristikama stvarnog istinskog i lažnog jezika kako bi razumjeli kako se percepcija uklapa s stvarnošću. S ovim podacima izgradili smo klasifikatore da bi se automatski odvojili povjereni od nepovjerenog govora, postigli F1 od 66,1%. Sljedeće smo procijenili da li su ratitelji strategija rekli da su diskriminirali između iskrenih i lažnih odgovora bili u stvari korisni. Naši rezultati pokazuju da, iako su nekoliko prosodijskih i leksičkih karakteristika konstantno smatrali povjerljivim, nisu bili pouzdani znakovi. Također, strategije koje su sudci prijavili kako se koriste u otkrivanju prevare nisu bile od koristi za taj zadatak. Naš rad prosvjetljuje prirodu vjernog jezika i pruža uvid u izazovni problem otkrivanja ljudskih prevara.', 'id': 'Manusia jarang melakukan lebih baik dari kesempatan untuk mendeteksi kebohongan. Untuk memahami lebih baik persepsi manusia tentang penipuan, kami menciptakan rangkaian permainan, LieCatcher, untuk mengumpulkan nilai dari penipuan persepsi menggunakan tubuh besar dari wawancara penipuan dan jujur. Kami menganalisis karakteristik akustik-prosodik dan bahasa bahasa yang dipercaya dan tidak dipercaya oleh peneliti dan membandingkan ini dengan karakteristik bahasa yang sebenarnya benar dan menipu untuk memahami bagaimana persepsi sesuai dengan kenyataan. Dengan data ini kami membangun klasifikasi untuk secara otomatis membedakan yang dipercaya dari pidato yang tidak dipercaya, mencapai F1 66,1%. Kami berikutnya mengevaluasi apakah strategi rater mengatakan mereka digunakan untuk diskriminasi antara respon yang benar dan penipuan sebenarnya berguna. Hasil kami menunjukkan bahwa, meskipun beberapa karakteristik prosodik dan lexik secara konsisten dianggap layak dipercaya, mereka bukan tanda-tanda yang dapat dipercaya. Juga, strategi yang dilaporkan oleh hakim untuk mendeteksi penipuan tidak membantu untuk tugas. Pekerjaan kami memberikan cahaya pada sifat bahasa yang dipercaya dan memberikan penglihatan ke dalam masalah menantang deteksi penipuan manusia.', 'de': 'Menschen sind selten besser als der Zufall bei der Lügendetektion. Um die menschliche Wahrnehmung von Täuschung besser zu verstehen, haben wir ein Spielframework entwickelt, LieCatcher, um Bewertungen wahrgenommener Täuschung anhand eines großen Korpus von täuschenden und wahrheitsgetreuen Interviews zu sammeln. Wir analysierten die akustisch-prosodischen und linguistischen Eigenschaften von Sprache, die von Bewertern vertraut und misstraut wird, und verglichen diese mit Eigenschaften tatsächlicher wahrheitsgetreuer und trügerischer Sprache, um zu verstehen, wie Wahrnehmung mit der Realität übereinstimmt. Mit diesen Daten haben wir Klassifikatoren entwickelt, um automatisch vertrauenswürdige von misstrauischer Sprache zu unterscheiden und ein F1 von 66.1% zu erreichen. Als nächstes bewerteten wir, ob die Strategien, die Bewerter sagten, sie verwendeten, um zwischen wahrheitsgemäßen und trügerischen Antworten zu unterscheiden, tatsächlich nützlich waren. Unsere Ergebnisse zeigen, dass einige prosodische und lexikalische Merkmale konsistent als vertrauenswürdig empfunden wurden, jedoch keine verlässlichen Hinweise waren. Auch die Strategien, die Richter bei der Betrugserkennung berichteten, waren für die Aufgabe nicht hilfreich. Unsere Arbeit beleuchtet die Natur vertrauenswürdiger Sprache und gibt Einblick in das herausfordernde Problem der Erkennung menschlicher Täuschungen.', 'sw': 'Raia nadra hufanya vizuri kuliko nafasi ya kutambua uongo. Ili kuelewa zaidi mtazamo wa binadamu wa udanganyifu, tulitengeneza mchezo wa mchezo, LieCatcher, ili kukusanya kiwango cha udanganyifu unaoonekana kwa kutumia mahojiano makubwa ya udanganyifu na ukweli. Tulichambua utaalamu wa lugha yenye uzalendo na lugha yenye imani na kutokuaminiwa na wachangiaji na kuwalinganisha na tabia hizi za lugha ya kweli na ya udanganyifu ili kuelewa jinsi mtazamo unavyofanana na ukweli. Kwa takwimu hizi tulijenga wataalamu ili kujitofautisha kwa kujitegemea kutokana na hotuba isiyo na imani, kupata F1 ya asilimia 66.1. Tulipofuata tathmini ikiwa kiwango cha mikakati walisema walikuwa na tofauti kati ya majibu ya kweli na ya udanganyifu yalikuwa na ufanisi. Our results show that, although several prosodic and lexical features were consistently perceived as trustworthy, they were not reliable cues.  Pia, mikakati ambayo majaji yaliripoti kutumia katika kutambua udanganyifu haikuwa ya faida kwa kazi hiyo. Kazi yetu inaonyesha mwangaza kuhusu asili ya lugha inayoaminika na inatoa uelewa wa tatizo la kutambua udanganyifu wa binadamu.', 'tr': 'Adamlar ýalan sözlerini tanyşdyrmak mümkinçilikden has gowydyr. Adamlaryň pikirini gowy düşünmek üçin, biz oýun çykyşynyň örän uly we dogruçyl çykyşynyň köpüsini ýygnamak üçin bir oýun çykyşyny bejerdik. Biz dili güýçli we ynamly hasaplanýan akustik-prosodik we lingwistiki karakterleriň hasaplanýarys we bunlary hakyky dogry we hili kelläpçi dilleriniň hasaplanýarys. Bu maglumatlar bilen biz klassifikatçi guruldyk we ynamly güýçli sözlerini öz-özüne aýlaşdyrmak üçin 66.1%-den F1 we ýetişdirip bardyk. Indiki çykyşlaryň dogry we aldatjyk jogabalaryň hakykatdanam tapawutlaryny aýtdyklaryny bardyk. Biziň netijelerimiz birnäçe prosodik we leksiýaly özellikler diňe ynamly möhüm bolup görünýändiklerini görkezýär. Ýöne, ahyrçylar aldatma işinde ulanmagy barada bildirilen strategiýalar işe yaramaz. Biziň işimiz ynamly diliň tebigatyna ýagtylygy çykarýar we adamlaryň aldatmyň çykyş meselesine düşündirir.', 'fa': 'انسان کم کم بهتر از شانس شناسايي دروغ انجام ميدن برای بهتر فهمیدن perception انسان از فریب، ما یک چهارچوب بازی ایجاد کردیم، لیکچر، برای جمع کردن امتیاز فریب شناخته شده با استفاده از یک جسد بزرگ از مصاحبه های فریب و راستگو. ما ویژگی\u200cهای آکوستیک و زبان\u200cشناسی را تحلیل کرده\u200cایم که از زبان\u200cهای مطمئن و غیر اعتماد شده\u200cاند و این\u200cها را با ویژگی\u200cهای زبان راست و فریب\u200cکننده\u200cای مقایسه کرده\u200cایم تا بفهمیم چگونه حس با واقعیت تطبیق می\u200cکند. با این اطلاعات ما راهنمایی ساختیم تا به طور خودکار اعتماد را از سخنرانی غیرقابل اعتماد جدا کنیم، تا F1 از 66.1 درصد برسیم. بعدش ارزیابی کردیم که آیا ارزیابان استراتژی ها گفتند که میان پاسخهای راستگویی و فریبکاری در واقع مفید بودند. نتیجه\u200cهای ما نشان می\u200cدهند که، اگرچه تعدادی از ویژه\u200cهای حرفه\u200cای و زبان\u200cشناسی همیشه به عنوان قابل اعتماد مشخص شدند، آنها نشانه\u200cهای قابل اعتماد نبودند. همچنین استراتژی\u200cهایی که قاضی\u200cها گزارش داده\u200cاند که در کشف فریب استفاده می\u200cکنند برای این کار کمک نمی\u200cکنند. کار ما به طبیعت زبان اعتماد روشن می کند و به مشکل سخت کشف فریب انسان مشاهده می کند.', 'af': "Mense het selfs beter uitvoer as kans by lê-opdekking. Om menslike verstaan van verleiding te beter verstaan, skep ons 'n speletjie raamwerk, LieCatcher, om ratings van verstaan verleiding te versamel deur 'n groot korpus van verleiding en waarheid intervjuis te gebruik. Ons het die akustiese-prosodiese en lingwisiese eienskappe van taal wat vertroud en misvertroud word deur raters en hierdie vergelyk met karakteristieke van werklike waarheid en verleiding taal om te verstaan hoe die verstaan vergelyk met realiteit. Met hierdie data het ons klassifiseerders gebou om outomaties vertroude spreek te verwyder, met 'n F1 van 66.1%. Ons het die volgende evalueer of die strategie-raters gesê het dat hulle gebruik het om te diskrimineer tussen waarheid en bedriefde antwoordes is in werklikheid nuttig. Ons resultate wys dat, alhoewel veelvuldige prosodiese en leksiese eienskappe konsistentlik as vertrouverdige was, hulle was nie vertroubare tekens nie. Ook, die strategies wat regters verkondig het deur te gebruik in verleiding opdekking was nie hulp vir die taak nie. Ons werk skep lig op die natuur van vertroude taal en verskaf insig in die vanskende probleem van menslike verleiding.", 'sq': 'Njerëzit rrallë bëjnë më mirë se shansi për zbulimin e gënjeshtrave. Për të kuptuar më mirë perceptimin njerëzor të mashtrimit, ne krijuam një kuadër loje, LieCatcher, për të mbledhur vlerësimet e mashtrimit të perceptuar duke përdorur një trup të madh intervistash mashtruese dhe të vërteta. Ne analizuam karakteristikat akustiko-prosodike dhe gjuhësore të gjuhës të besueshme dhe të mosbesueshme nga vlerësuesit dhe i krahasuam këto me karakteristikat e gjuhës së vërtetë të vërtetë dhe mashtruese për të kuptuar se si perceptimi përputhet me realitetin. Me këto të dhëna ne ndërtuam klasifikuesit për të dalluar automatikisht të besuar nga fjalimi i pasigurt, duke arritur një F1 prej 66.1%. Pastaj vlerësuam në se strategjitë që vlerësojnë thanë se përdornin për të diskriminuar mes përgjigjeve të vërteta dhe mashtruese ishin në fakt të dobishme. Our results show that, although several prosodic and lexical features were consistently perceived as trustworthy, they were not reliable cues.  Gjithashtu, strategjitë që gjyqtarët njoftuan se përdornin në zbulimin e mashtrimit nuk ishin të dobishme për detyrën. Puna jonë hedh dritë në natyrën e gjuhës së besuar dhe ofron kuptim në problemin e sfidueshëm të zbulimit të mashtrimit njerëzor.', 'am': 'ሰው በሐሰት ማግኘት የሚሻል ማድረግ በጣም ጥቂት ነው፡፡ ለሰው የሽንገላን ምሳሌ ለማስተዋል ይሻል፤ የሽንገላን ምሳሌ፣ ዋሊካቴር፣ የታወቀ ሽንገላን በመጠቀም የታላቁ እና እውነተኞች የውይይት ትልቅ ምዕራብ ለመቀበል ነው። የቋንቋው እና የቋንቋ ቋንቋዎች ስህተት አስተያይተናል፤ እውነተኛ እና እውነተኛ ቋንቋ እንዴት እንደተቃወመ እና እንዴት እንደሚያሳውቅ ነው ብለን እናስተያየዋለን፡፡ ከዚህም ዳታ የ66.1 በመቶ የF1 በመቶ ለማግኘት በአስታማሚ ንግግር እንዲለይ ክፍፍተቶችን ሠርተናል፡፡ የቀድሞው የስርዓት ተሟጋቾች እውነተኞች እና በሚያታዩት መልስ ላይ ጥቅም ሆኖ እንደሆነ እናውቀዋለን፡፡ Our results show that, although several prosodic and lexical features were consistently perceived as trustworthy, they were not reliable cues.  ደግሞም ዳኞች በሽንገላ ማግኘት የተዘጋጁት ስርዓት ለስራ አይጠቅሙም፡፡ ሥራችን የታመነ ቋንቋን ትክክል ላይ ያሳያል እና የሰው ሽንገላን ለማግኘት የጥላቻን ጉዳይ ያሳያል፡፡', 'hy': 'Մարդիկ հազվադեպ ավելի լավ են աշխատում, քան սուտ հայտնաբերելու հնարավորությունը: Որպեսզի ավելի լավ հասկանանք խաբեության մարդկային ընկալումը, մենք ստեղծեցինք խաղային շրջանակ, ԼեյԿատչեր, որպեսզի հավաքենք ընկալում խաբեության գնահատականներ՝ օգտագործելով խաբեի և ճշմարտության հարցազրույցների մեծ մարմին Մենք վերլուծեցինք լեզվի ձայնային-պրոսոդիկ և լեզվաբանական առանձնահատկությունները, որոնք վստահում են և չեն վստահում գնահատողների կողմից, և համեմատեցինք դրանք իրական ճշմարտության և խաբեության առանձնահատկությունների հետ, որպեսզի հասկանանք, թե ինչպես Այս տվյալների օգնությամբ մենք կառուցեցինք դասակարգեր, որպեսզի ինքնաբերաբար տարբերակենք վստահությունը անվստահելի խոսքից, հասնելով 66.1 տոկոսի F1-ին: Հաջորդը մենք գնահատեցինք, թե արդյոք ռազմավարիչները ասում են, որ նրանք օգտակար են ճշմարտության և խաբեության պատասխանների միջև տարբերակելու համար: Our results show that, although several prosodic and lexical features were consistently perceived as trustworthy, they were not reliable cues.  Նաև, այն ռազմավարությունները, որոնք դատավորները հայտարարել են օգտագործելով խաբեության հայտնաբերման մեջ, օգտակար չէին այս խնդրի համար: Մեր աշխատանքը լուսավորում է վստահելի լեզվի բնույթը և պարունակում է մարդկային խաբեության հայտնաբերման դժվար խնդիրը:', 'az': "İnsanlar yalan söylədiklərindən daha yaxşı işlər edirlər. İnsanın aldatmasını daha yaxşı anlamaq üçün, biz oyun framework ünü LieCatcher yaratdıq, böyük aldatmış və doğru danışmaqların korpusu vasitəsilə görünmüş aldatmanın rəjtini toplamaq üçün. Biz təvəkkül edilmiş və təvəkkül edilmiş dillərin akustik prosodik və dil əlamətlərini analiz etdik və bunları həqiqət və yalançı dillərin əlamətlərinə qarşılaşdırdıq ki, baxışların necə gerçeklik ilə bərabər olduğunu anlasın. Bu məlumatlar ilə, güvenilir sözlərdən fərqli olaraq, 66.1%-dən F1'i qəbul etmək üçün seçkilər inşa etdik. Sonra müəyyən etdik ki, strateji müəyyən edənlər doğru və yalançı cavablar arasında ayırdıqlarının faydalı olduğunu söylədilər. Bizim sonuçlarımız göstərir ki, bir neçə prosodik və leksi özellikləri mövcud olduğu halda, onlar güvenilir nişanələr deyildilər. Həmçin in həddi aşmalarda istifadə etdikləri stratejilər bu işə fayda vermədi. Bizim işimiz güvenilir dillərin təbiətinə işıq verir və insan aldatmasının çətin problemlərini təşkil edir.", 'bn': 'মানুষ মিথ্যা আবিষ্কারের সুযোগের চেয়েও ভালো কাজ করে। প্রতারণার ব্যাপারে মানুষের ধারণা বুঝতে পারার জন্য আমরা একটি খেলার ফ্রেম তৈরি করেছি, লিকক্যাচার, বিশাল প্রতারণা এবং সত্যিকারের সাক্ষাৎ আমরা ভাষার বিশ্বাসযোগ্য এবং ভাষার ভাষার বৈশ্বাসযোগ্য বিশ্লেষণ করেছি যে ভাষার বিশ্বাস এবং বিশ্বাস করেছি তারা বাস্তবতার ব্যাপারে সত্যি এবং প্রতারণা With this data we built classifiers to automatically distinguish trusted from mistrusted speech, achieving an F1 of 66.1%.  পরবর্তীতে আমরা মূল্যায়ন করেছি যে কৌশলের হেরেজারা বলেছিল যে তারা সত্যি এবং প্রতারণাকারীদের মধ্যে বৈষম্য করে দিতে পারেন কি আমাদের ফলাফল দেখা যাচ্ছে যদিও বেশ কিছু প্রোজোডিক এবং লেক্সিক্সিক বৈশিষ্ট্য বিশ্বাসী হিসেবে বিশ্বাস করা হয়েছে, তবে  এছাড়াও, বিচারকেরা প্রতারণা করেছে যে কৌশলগুলো বিভ্রান্ত আবিষ্কার করেছে তা কাজের জন্য সাহায্য করে নি। আমাদের কাজ বিশ্বস্ত ভাষার প্রকৃতির উপর আলোক প্রকাশ করে এবং মানুষের প্রতারণার চ্যালেঞ্জের ব্যাপারে দৃষ্টিভঙ', 'ca': "Rarament els humans aconsegueixen millor que la probabilitat de detectar mentides. Per entendre millor la percepció human a de l'engany, vam crear un marc de joc, LieCatcher, per recollir valoracions de l'engany perceptit fent servir un gran cos d'entrevistes enganyades i veritables. Vam analitzar les característiques acústiques-prosòdiques i lingüístices de la llengua confiada i desconfiada pels puntualitzadors i les vam comparar amb les característiques de la llengua veritable i enganyada per entendre com la percepció s'alinia amb la realitat. Amb aquestes dades vam construir classificadors per distingir automàticament la confiança dels discursos desconfiants, aconseguint un F1 del 66,1%. Després vam evaluar si els puntuadors d'estratègies deien que solien discriminar entre respostes veritables i enganyades eren de fet útils. Els nostres resultats mostren que, tot i que diverses característiques prosòdiques i lècsiques van ser considerades constantment fiables, no eren indicis fiables. També les estratègies que els jutges van dir utilitzar en la detecció de fraude no van ajudar a la tasca. La nostra feina revela la naturalesa del llenguatge de confiança i proporciona una visió del problema desafiant de la detecció de l'engany humà.", 'cs': 'Lidé zřídka vedou lépe než šance na detekci lží. Abychom lépe porozuměli lidskému vnímání klamu, vytvořili jsme herní rámec LieCatcher, který shromažďuje hodnocení vnímaného klamu pomocí velkého korpusu klamných a pravdivých rozhovorů. Analyzovali jsme akusticko-prozodické a jazykové charakteristiky jazyka, kterému hodnotící důvěřují a nedůvěřují, a porovnali je s charakteristikami skutečného pravdivého a klamavého jazyka, abychom pochopili, jak se vnímání ladí s realitou. Na základě těchto dat jsme vytvořili klasifikátory, které automaticky odlišují důvěryhodnou řeč od nedůvěryhodné řeči a dosahují F1 66,1%. Následně jsme zhodnotili, zda hodnotitelé strategií uvedli, že používali k rozlišování pravdivých a klamných reakcí, jsou ve skutečnosti užitečné. Naše výsledky ukazují, že ačkoli několik prozodických a lexikálních rysů bylo důsledně vnímáno jako důvěryhodné, nebyly spolehlivými návody. Také strategie, které soudci nahlásili při detekci klamů, nebyly pro tento úkol užitečné. Naše práce vrhá světlo na povahu důvěryhodného jazyka a poskytuje vhled do náročného problému detekce lidského klamu.', 'ko': '인류는 거짓말 탐지 방면에서 우연보다 더 좋은 모습을 보이는 경우가 드물다.사기에 대한 인류의 감지를 더욱 잘 이해하기 위해 우리는 Letchatcher라는 게임 프레임워크를 만들었고 대량의 사기성과 진실성에 대한 인터뷰를 통해 사기를 감지하는 평점을 수집했다.우리는 채점원이 믿고 믿지 않는 언어의 성학적 운율과 언어 특징을 분석하고 이를 진실과 사기성 언어의 특징과 비교하여 감지가 현실과 어떻게 일치하는지 이해했다.이러한 데이터로 우리는 신뢰받는 음성과 신뢰받지 못하는 음성을 자동으로 구분하는 분류기를 구축했는데 F1은 66.1%에 달했다.이어서 우리는 평점원들이 진실과 기만적인 대답을 구분하는 전략이 정말 유용한지 평가했다.우리의 연구 결과에 따르면, 비록 일부 운율과 어휘 특징은 줄곧 믿을 만하다고 여겨졌지만, 그것들은 결코 믿을 만한 단서가 아니다.이 밖에 법관은 사기 검측에 사용된 전략이 임무에 도움이 되지 않는다고 보고했다.우리의 작업은 신뢰할 수 있는 언어의 본질을 밝히고, 인류 사기 검측의 도전적인 문제에 대해 견해를 제공했다.', 'bs': 'Ljudi rijetko izvode bolje od šanse za otkrivanje laži. Da bi bolje shvatili ljudsku percepciju prevare, stvorili smo okvir igre, LieCatcher, kako bi skupili ocjene percepcije prevare koristeći veliki korpus prevarantnih i iskrenih intervjua. Analizirali smo akustičke prosodične i jezičke karakteristike jezika koje su vjerovali i nepovjereni ocjennicima i usporedili ih sa karakteristikama pravog i prevarantnog jezika kako bi shvatili kako percepcija odgovara realnosti. Sa ovim podacima smo izgradili klasifikatore da bi se automatski odvojili povjereni od nepovjerenog govora, ostvarili F1 od 66,1%. Sljedeće smo procijenili da li su ratitelji strategija rekli da su diskriminirali između iskrenih i lažnih odgovora bili u stvari korisni. Naši rezultati pokazuju da, iako su nekoliko prosodijskih i leksičkih karakteristika stalno smatrali povjerljivim, nisu bili pouzdani znakovi. Također, strategije koje su sudci prijavili kako koriste u otkrivanju prevare nisu bile od koristi za taj zadatak. Naš rad prosvjetljuje prirodu vjernog jezika i pruža uvid u izazovni problem otkrivanja ljudskih prevara.', 'et': 'Inimesed saavad valetuvastamisel harva paremini hakkama. Et paremini mõista inimlikku pettuse tajumist, lõime mänguraamistiku LieCatcher, et koguda tajutud pettuse hinnanguid, kasutades suurt korpust petlikke ja tõeseid intervjuusid. Analüüsisime hindajate usaldatud ja usaldamatu keele akustilisi-prosoodilisi ja keelelisi omadusi ning võrdlesime neid tegeliku tõese ja petliku keele omadustega, et mõista, kuidas tajumine reaalsusega vastab. Nende andmetega ehitasime klassifitseerijad, et automaatselt eristada usaldusväärset ja usaldamatut kõnet, saavutades F1 66,1%. Järgmisena hindasime, kas hindajad ütlesid, et nad kasutasid tõeliste ja petlike vastuste eristamiseks tegelikult kasulikke strateegiaid. Meie tulemused näitavad, et kuigi mitmeid prosoodilisi ja leksikaalseid tunnuseid peeti pidevalt usaldusväärseteks, ei olnud need usaldusväärsed vihjed. Samuti ei olnud strateegiad, mida kohtunikud teatasid pettuste tuvastamisel, ülesande täitmiseks kasulikud. Meie töö heidab valgust usaldusväärse keele olemusele ja annab ülevaate inimeste pettuste tuvastamise keerulisest probleemist.', 'fi': 'Ihmiset pystyvät harvoin valheenpaljastukseen paremmin. Ymmärtääksemme paremmin ihmisten käsitystä petoksesta loimme pelikehyksen, LieCatcher, kerätäksemme arvioita havaitusta petoksesta käyttäen suurta korpusta petollisia ja totuudenmukaisia haastatteluja. Analysoimme arvioijien luottaman ja epäluottamuksen kielen akustisia-prosodisia ja kielellisiä ominaisuuksia ja vertasimme niitä todellisen totuudenmukaisen ja harhaanjohtavan kielen ominaisuuksiin ymmärtääksemme, miten havainto vastaa todellisuutta. Näiden tietojen avulla rakensimme luokittelijoita erottamaan automaattisesti luotetun puheen epäluottamuslauseesta saavuttaen 66,1 prosentin F1-arvon. Seuraavaksi arvioimme, olivatko strategiat arvioijien mukaan hyödyllisiä totuudenmukaisten ja petollisten vastausten erottamiseen. Tuloksemme osoittavat, että vaikka useita prosodisia ja leksikaalisia piirteitä pidettiin johdonmukaisesti luotettavina, ne eivät olleet luotettavia vihjeitä. Myös strategioista, joita tuomarit raportoivat käyttäneensä petoksen havaitsemisessa, ei ollut apua tehtävässä. Työmme valaisee luotetun kielen luonnetta ja antaa tietoa ihmisen petoksen havaitsemisen haastavaan ongelmaan.', 'ha': "Mutane bã su kamfata mafiya alhẽri daga ganin ƙarya. To domin ka fahimta fikancin mutum na yaudara, Mun halitta firam mai wãsa, LieCatheri, dõmin mu sami rabon bakwai na yaudarar da aka fahimta, ko kuma misãlai mai yawa daga samun samurai na yaudara da masu gaskiya. Ba mu rarraba masu karatun karo-proodic da linguistic cikin harshen, wanda aka yi aminci da kuma ba mu amince da shi ba da haske da mutane kuma mun sammeni wannan da ke da sifatinin harshen gaskiya da bakarãriya dõmin ka fahimta jinsi gani na samu da gaskiya. Daga wannan danne, mun gina dangantaki dõmin ka rarraba masu farat ɗaya daga magana wanda ba a yi ĩmãni ba, da gaskata F1 daga 66.1%. Ga ta ƙaddara ko ma'anar musammani sun ce ma'anar da su gaura a tsakanin gaskiya da cewa ɗin da ba ta kasance na amfani ba. MatamayinMu na nũna cewa, kuma kõ da wasu mistakarda na zato ba su kasance amintacce ba, sai ba su kasance masu amintarwa ba. Kayya, kimar da mahakimar da aka bai amfani da shi ba ga aikin da za'a gane shi. Kayinmu yana nuna haske a kan halin harshen wanda ake amintar da shi kuma yana samar da gannai a cikin mataimaki na gane wa kudangan mutum.", 'he': 'בני אדם לעיתים נדירות מבצעים טוב יותר מהסיכוי לגלות שקרים. כדי להבין טוב יותר את התפיסה האנושית של הונאה, יצרנו מסגרת משחק, LieCatcher, כדי לאסוף ציונים של הונאה התפיסה באמצעות גוף גדול של ראיונות הונאים ואמיתיים. ניתחנו את האופיינים האקוסטיים-פרוסודיים והשפותיים של שפה שמאמינים ובטוחים בלתי מאמינים על ידי מערכים, ושוונו אותם עם אופיינים של שפה אמיתית ורמותית אמיתית כדי להבין איך התפיסה מתאימה למציאות. עם הנתונים האלה בנינו מסווגים כדי להבדיל באופן אוטומטי את האמון מנאום חסר אמון, להשיג F1 של 66.1%. הבא הערכנו אם המערכים האסטרטגיים אמרו שהם נהגו להפריע בין תגובות אמיתיות ורמות היו למעשה שימושיים. התוצאות שלנו מראות שלמרות שכמה תכונות פרוסודיות ולקסיקות נראו באופן קבוע כאמינים, הן לא היו סימנים אמינים. בנוסף, האסטרטגיות ששופטים דיווחו על השימוש בזיהוי הונאה לא עזרו למשימה. Our work sheds light on the nature of trusted language and provides insight into the challenging problem of human deception detection.', 'sk': 'Ljudje redko odkrivajo laži. Da bi bolje razumeli človeško dojemanje prevare, smo ustvarili igralni okvir, LieCatcher, za zbiranje ocen zaznane prevare z uporabo velikega korpusa zavajajočih in resničnih intervjujev. Analizirali smo akustično-prosodične in jezikovne značilnosti jezika, ki jim ocenjevalci zaupajo in jih primerjali z značilnostmi dejanskega resničnega in zavajajočega jezika, da bi razumeli, kako se percepcija usklajuje z realnostjo. S temi podatki smo zgradili klasifikatorje za samodejno ločevanje zaupanja vrednega govora od nezaupanja vrednega govora in dosegli F1 66,1%. Nato smo ocenili, ali so strategije ocenjevalcev, ki so jih uporabljali za razlikovanje med resničnimi in zavajajočimi odzivi, dejansko koristne. Naši rezultati kažejo, da čeprav so bile več prosodičnih in leksikalnih značilnosti dosledno dojemane kot zaupanja vredne, niso bili zanesljivi namigi. Prav tako strategije, ki so jih sodniki poročali o uporabi pri odkrivanju prevar, niso bile koristne za nalogo. Naše delo osvetljuje naravo zaupanja vrednega jezika in zagotavlja vpogled v zahteven problem odkrivanja človeških prevar.', 'jv': 'Awak dhèwèké sakjané luwih apik kanggo tinggal dipunangé. Ndheke kapan langkung rawuh dumadhi sing luwih dumadhi, awak dhéwé nggawe barang kelas, LiCatch, nggo ngerasakno kuwi tindakan kejahatan perbudhakan langkung wih apik dhéwé, lan akeh dumadhi sing beraksi sing apik dhéwé. Awak dhéwé énalusi sistem akustik-prosok karo akeh lan luwih nggawe gerarané karo nggawe barang nggawe gerarané karo nggawe barang dhéwé karo perusahaan karo ngangge langkung sing trus karo akeh lan kelangan kuwi mau Dheke awak dhéwé nggawe ing saklaser kanggo nggawe gerapakan karo perusahaan kanggo nganggo dilakoni sing ora nggawe barang, nanguwi F1 karo 6.1%. Awak dhéwé éntuk dhéwé ngerasah kaya perusahaan kelas kuwi wis dipulangan kelas karo hal-hal sing luwih apik lan kelas kuwi mau Rejalaké awak dhéwé ngomong nik, tho saben piye soko perusahaan lan kelas kang dipunasaben nggawe barang, dhewe ora iso ngregani aku. Nambah, hukum sing paling beraksi diputara winih ning diputara winih (sedhaya) kuwi ora nggawe barang nggawe gerakan. Awakdhéwé éntuk mbelaké aturan kanggo nglanggar luwih apik lan ijol-ijol kuwi nggawé kuwi nggawé kuwi kesempatan kanggo nguasai perbudhakan kanggo kebebasan pangan.', 'bo': 'ཆེས་ཉིད་བསམ་བཤེར་ལ་གོ་སྐབས་ཡོད་ཚད་ལས་ཀྱང་དཀའ་ངལ་ཡོད། མི་ཤེས་ཚོར་བ་དང་མི་ཤེས་པའི་སྐོར་ཚུལ་དེ་ལྟ་བུ་ཞིག་ཡིན་པའི་རྩེད་གཞུང་ཞིག་གིས་གསར་བསྐྲུན་བྱས། ང་ཚོས་སྐད་ཡིག་དང་ཡིད་རྟོན་རུང་བའི་གཟུགས་རིས་དང་སྐད་རིགས་ཀྱི་ཁྱད་ཆ་རྟོགས་དང་ཡིད་རྟོན་མེད་པའི་རིགས་ཉེས་དང་མཉམ་དུ་ཐད་རིང་དང་འགྱུར ང་ཚོས་རང་འགུལ་གྱིས་ཡིད་ཆ་ཡིད་པའི་སྐད་ཡིག འོན་ཀྱང་། གྲངས་འབོར་གྱི་ཐབས་ལམ་གཙོ་རིམ་པ་ཚོས་རང་ཉིད་ཀྱི་བདེན་བཤད་དང་འགྲེལ་བཤད་དབར་གྱི་ཕྱོགས་སྟོན ང་ཚོའི་འབྲས་བུ ད་དུང་། ཕན་ཚུན་རྟོན་བཤེར་སྐབས་སྤྱོད་བྱེད་པའི་བྱ་རིམ་གྱི་ཐབས་ལམ་ལ་ཕན་ཐོགས་མེད། ང་ཚོའི་ལས་ཀ་ལྟར་ཡིད་རྟོན་རུང་བའི་སྐད་རིགས་ཀྱི་རང་བཞིན་གྱིས་འོད་དྲག་སྟོན་པ་དང་། མི་རྣམས་ལྟ་ཞིབ་བཤེར་གྱི་ག'}
