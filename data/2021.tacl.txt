{'en': 'Revisiting Multi-Domain Machine Translation', 'pt': 'Revisitando a tradução automática de vários domínios', 'ar': 'مراجعة الترجمة الآلية متعددة المجالات', 'fr': 'Revisiter la traduction automatique multidomaine', 'es': 'Revisión de la traducción automática multidominio', 'hi': 'बहु डोमेन मशीन अनुवाद revisiting', 'ru': 'Пересмотр многодоменного машинного перевода', 'zh': '复审多域机器翻译', 'ja': 'マルチドメイン機械翻訳の再検討', 'ga': 'Athchuairt ar Aistriúchán Meaisín Ilfhearainn', 'el': 'Επανεξέταση μηχανικής μετάφρασης πολλαπλών τομέων', 'ka': 'მრავალ- დომენის მაქსინის გადატვირთვა', 'hu': 'Többtartományos gépi fordítás felülvizsgálata', 'lt': 'Peržiūrėti daugiadominių mašinų vertimą', 'it': 'Revisione della traduzione automatica multi-dominio', 'kk': 'Көп домен машинаның аудармасын қайталау', 'ms': 'Mengubah Terjemahan Mesin Berbilang Domain', 'mk': 'Ревизирање на превод на мултидомени машини', 'mt': 'Reviżjoni tat-Traduzzjoni ta’ Makkinarju Multidomestiku', 'ml': 'Multi- Domain Machine പരിഭാഷപ്പെടുത്തുന്നു', 'no': 'Omsetjing av fleire domenemaskiner', 'mn': 'Олон домойн машины хөрөнгө давтах', 'sr': 'Ревизирање многодоменских превода машина', 'si': 'ගොඩක් ඩොමේන් මේෂින් පරිවර්තනය', 'pl': 'Przegląd tłumaczenia maszynowego w wielu domenach', 'so': 'Revisiting Multi-Domain Machine Translation', 'ro': 'Revizuirea traducerii automate multidomenii', 'sv': 'Revidering av maskinöversättning för flera domäner', 'ta': 'பல- டொமைன் மொழிபெயர்ப்பை மீள்மாற்றுகிறது', 'ur': 'Multi-Domain Machine Translation', 'vi': 'Dịch lắp đa miền', 'uz': 'Name', 'nl': 'Machine Translation voor meerdere domeinen opnieuw bekijken', 'hr': 'Revizija multidomena prevoda stroja', 'da': 'Revidering af maskinoversættelse med flere domæner', 'bg': 'Преразглеждане на многодомейнен машинен превод', 'de': 'Überarbeitung der maschinellen Übersetzung mit mehreren Domänen', 'ko': '다역을 재론하다', 'id': 'Menyesuaikan Penerjemahan Mesin Multi-Domain', 'fa': 'تغییرات ماشین زیادی دامنه', 'sw': 'Tafsiri ya Mashine ya Ki-Domain', 'tr': 'Çoklu-domain Maşynyň terjimesini bejermek', 'sq': 'Duke rishikuar përkthimin e makinave me shumëdomeni', 'af': 'Hersiening van Multidomein Masjien Vertaling', 'am': "ዶሴ `%s'ን ማስፈጠር አልተቻለም፦ %s", 'hy': 'Բազմաբնական մեքենայի թարգմանման վերանայելը', 'az': 'Çoxlu Domain Makinesi Çeviri', 'bn': 'বহুডোমেইন মেশিন অনুবাদ পুনরায় সংশোধন করা হচ্ছে', 'cs': 'Opakování strojového překladu více domén', 'et': 'Mitme domeeni masintõlke läbivaatamine', 'bs': 'Revizija multidomena automatskog prevoda', 'fi': 'Useiden toimialueiden konekäännösten uudelleentarkastelu', 'ca': 'Revisió de la traducció de màquines multidominiosos', 'he': 'שינוי התרגום של מכונות מרובות שדות', 'ha': '@ action', 'sk': 'Pregled strojnega prevajanja z več domenami', 'jv': 'Ngubah Multi-domain Majin Terjamahan', 'bo': 'བསྐྱར་གསོ་བྱེད་མི་མང་ཆེ་བ་ཡིག་ཆས་འགྲེལ་བརྗོད་པ'}
{'en': 'When building  machine translation systems , one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing. This multi-domain scenario has attracted a lot of recent work that fall under the general umbrella of  transfer learning . In this study, we revisit multi-domain machine translation, with the aim to formulate the motivations for developing such systems and the associated expectations with respect to performance. Our experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behaviour of multi-domain systems and to make them fully hold their promises.', 'pt': 'Ao construir sistemas de tradução automática, muitas vezes é necessário tirar o melhor proveito de conjuntos heterogêneos de dados paralelos no treinamento e manipular de maneira robusta as entradas de domínios inesperados nos testes. Esse cenário de vários domínios atraiu muitos trabalhos recentes que se enquadram no guarda-chuva geral da aprendizagem por transferência. Neste estudo, revisitamos a tradução automática multidomínio, com o objetivo de formular as motivações para o desenvolvimento de tais sistemas e as expectativas associadas em relação ao desempenho. Nossos experimentos com uma grande amostra de sistemas multidomínio mostram que a maioria dessas expectativas dificilmente são atendidas e sugerem que mais trabalho é necessário para analisar melhor o comportamento atual dos sistemas multidomínio e fazê-los cumprir plenamente suas promessas.', 'es': 'Al crear sistemas de traducción automática, a menudo es necesario aprovechar al máximo los conjuntos heterogéneos de datos paralelos en el entrenamiento y gestionar de forma sólida las entradas de dominios inesperados en las pruebas. Este escenario multidominio ha atraído una gran cantidad de trabajos recientes que caen bajo el paraguas general del aprendizaje por transferencia. En este estudio, revisamos la traducción automática multidominio, con el objetivo de formular las motivaciones para desarrollar dichos sistemas y las expectativas asociadas con respecto al rendimiento. Nuestros experimentos con una gran muestra de sistemas multidominio muestran que la mayoría de estas expectativas apenas se cumplen y sugieren que es necesario seguir trabajando para analizar mejor el comportamiento actual de los sistemas multidominio y hacer que cumplan plenamente sus promesas.', 'ar': 'عند بناء أنظمة الترجمة الآلية ، غالبًا ما يحتاج المرء إلى الاستفادة القصوى من المجموعات غير المتجانسة من البيانات المتوازية في التدريب ، والتعامل بقوة مع المدخلات من المجالات غير المتوقعة في الاختبار. اجتذب هذا السيناريو متعدد المجالات الكثير من الأعمال الحديثة التي تندرج تحت المظلة العامة لنقل التعلم. في هذه الدراسة ، نعيد النظر في الترجمة الآلية متعددة المجالات ، بهدف صياغة الدوافع لتطوير مثل هذه الأنظمة والتوقعات المرتبطة بالأداء. تُظهر تجاربنا مع عينة كبيرة من الأنظمة متعددة المجالات أن معظم هذه التوقعات بالكاد يتم الوفاء بها وتشير إلى أن هناك حاجة إلى مزيد من العمل لتحليل السلوك الحالي للأنظمة متعددة المجالات بشكل أفضل وجعلها تفي بوعودها بشكل كامل.', 'fr': "Lors de la création de systèmes de traduction automatique, il est souvent nécessaire de tirer le meilleur parti d'ensembles hétérogènes de données parallèles lors de la formation et de gérer de manière robuste les entrées provenant de domaines inattendus lors des tests. Ce scénario multidomaine a suscité de nombreux travaux récents qui s'inscrivent dans le cadre général de l'apprentissage par transfert. Dans cette étude, nous revisitons la traduction automatique multidomaine, dans le but de formuler les motivations qui motivent le développement de tels systèmes et les attentes associées en matière de performances. Nos expériences avec un large échantillon de systèmes multidomaines montrent que la plupart de ces attentes ne sont guère satisfaites et suggèrent que des travaux supplémentaires sont nécessaires pour mieux analyser le comportement actuel des systèmes multidomaines et leur permettre de tenir pleinement leurs promesses.", 'zh': '机器翻译构之际,常须训练充用异构并行数集,并试中可任以意外。 此多领地多属移学保护伞近事。 于本研中,重审多领机器翻译,指定开发此机及性能之期。 多域统之实验,其望多不足,明更事以益论多域统之行,使尽然诺也。', 'ja': '機械翻訳システムを構築する場合、多くの場合、トレーニングで異種の並列データセットを最大限活用し、テストで予期しないドメインからの入力を堅牢に処理する必要があります。このマルチドメインシナリオは、転移学習の一般的な傘下に入る最近の多くの仕事を惹きつけてきた。この研究では、マルチドメイン機械翻訳を再検討し、そのようなシステムを開発する動機と、パフォーマンスに関する関連する期待を策定することを目的としています。マルチドメインシステムの大規模なサンプルを用いた実験では、これらの期待の大部分はほとんど満たされていないことが示されており、マルチドメインシステムの現在の挙動をよりよく分析し、約束を完全に守らせるためにさらなる作業が必要であることが示唆されています。', 'hi': 'मशीन अनुवाद प्रणालियों का निर्माण करते समय, किसी को अक्सर प्रशिक्षण में समानांतर डेटा के विषम सेटों से सर्वश्रेष्ठ बनाने की आवश्यकता होती है, और परीक्षण में अप्रत्याशित डोमेन से इनपुट को मजबूती से संभालने की आवश्यकता होती है। इस बहु-डोमेन परिदृश्य ने हाल के बहुत सारे काम को आकर्षित किया है जो स्थानांतरण सीखने की सामान्य छतरी के तहत आते हैं। इस अध्ययन में, हम बहु-डोमेन मशीन अनुवाद को फिर से देखते हैं, जिसका उद्देश्य इस तरह की प्रणालियों को विकसित करने के लिए प्रेरणाओं और प्रदर्शन के संबंध में संबंधित अपेक्षाओं को तैयार करना है। बहु-डोमेन प्रणालियों के एक बड़े नमूने के साथ हमारे प्रयोगों से पता चलता है कि इनमें से अधिकांश अपेक्षाएं शायद ही पूरी होती हैं और सुझाव देती हैं कि मल्टी-डोमेन सिस्टम के वर्तमान व्यवहार का बेहतर विश्लेषण करने और उन्हें अपने वादों को पूरी तरह से रखने के लिए आगे के काम की आवश्यकता है।', 'ru': 'При построении систем машинного перевода часто необходимо извлекать максимальную пользу из гетерогенных наборов параллельных данных при обучении и надежно обрабатывать входные данные из неожиданных областей при тестировании. Этот многодоменный сценарий привлек много недавней работы, которая относится к общему зонтику трансферного обучения. В этом исследовании мы пересматриваем многодоменный машинный перевод с целью формулирования мотивации для разработки таких систем и связанных с ними ожиданий в отношении производительности. Наши эксперименты с большой выборкой многодоменных систем показывают, что большинство из этих ожиданий едва ли оправдываются, и предполагают, что необходима дальнейшая работа, чтобы лучше проанализировать текущее поведение многодоменных систем и заставить их полностью сдержать свои обещания.', 'ga': 'Agus córais aistriúcháin mheaisín á dtógáil, is minic go gcaithfidh duine an leas is fearr a bhaint as tacair ilchineálacha de shonraí comhthreomhara san oiliúint, agus ionchuir ó réimsí tástála nach rabhthas ag súil leo a láimhseáil go láidir. Mheall an cás ilfhearainn seo go leor oibre le déanaí a thagann faoi scáth ginearálta na foghlama aistrithe. Sa staidéar seo, déanaimid athchuairt ar mheaisínaistriúchán ilfhearann, leis an aidhm na spreagthaí a cheapadh chun córais dá leithéid a fhorbairt agus na hionchais ghaolmhara maidir le feidhmíocht. Léiríonn ár dturgnaimh le sampla mór de chórais ilfhearainn gur ar éigean a chomhlíontar an chuid is mó de na hionchais seo agus tugann siad le fios go bhfuil gá le tuilleadh oibre chun anailís níos fearr a dhéanamh ar iompar reatha na gcóras ilfhearainn agus chun a ngealltanais a shealbhú go hiomlán.', 'ka': 'როდესაც მაქსინური გაგრძელების სისტემის შექმნა, ერთი უფრო უნდა გავაკეთოთ ყველაზე უკეთესი ჰეტეროგენური მონაცემების კონფიგურაციაში, და გავაკეთოთ გაგრძელების მონაცემების შესა ეს მრავალეთ დომინის სენარიო უფრო ბოლო სამუშაო სამუშაო, რომელიც სამუშაო სამუშაო სამუშაო სამუშაო გასწავლას. ამ კვლევაში ჩვენ მრავალეთ დიომინური მაქანის გაგრძნობის რედაქტირებით, მიზეზით, რომ მოტივიციები განვითარებისთვის და შესაბამისი განსაზღვრებისთვის გამოყენებას. ჩვენი ექსპერიმენტები, რომელიც მრავალეთ დომინის სისტემის დიდი ნაგულისხმებით, ჩვენი ექსპერიმენტები აჩვენებენ, რომ უფრო დიდი განმეხოვრებულებების უფრო დააკეთებულია და უკეთესი მრავალ', 'el': 'Κατά την κατασκευή συστημάτων μηχανικής μετάφρασης, συχνά πρέπει κανείς να αξιοποιήσει το καλύτερο από τα ετερογενή σύνολα παράλληλων δεδομένων στην εκπαίδευση και να χειριστεί αξιόπιστα τις εισόδους από απρόσμενους τομείς στις δοκιμές. Αυτό το σενάριο πολλαπλών τομέων προσέλκυσε πολλές πρόσφατες εργασίες που εμπίπτουν στη γενική ομπρέλα της μάθησης μεταφοράς. Στην παρούσα μελέτη, επανεξετάζουμε τη μηχανική μετάφραση πολλαπλών τομέων, με στόχο να διατυπώσουμε τα κίνητρα για την ανάπτυξη τέτοιων συστημάτων και τις σχετικές προσδοκίες όσον αφορά την απόδοση. Τα πειράματά μας με ένα μεγάλο δείγμα συστημάτων πολλαπλών τομέων δείχνουν ότι οι περισσότερες από αυτές τις προσδοκίες δύσκολα πληρούνται και δείχνουν ότι χρειάζεται περαιτέρω εργασία για να αναλύσουμε καλύτερα την τρέχουσα συμπεριφορά των συστημάτων πολλαπλών τομέων και να τα κάνουμε να κρατήσουν πλήρως τις υποσχέσεις τους.', 'it': "Quando si creano sistemi di traduzione automatica, è spesso necessario sfruttare al meglio insiemi eterogenei di dati paralleli durante la formazione e gestire in modo affidabile gli input provenienti da domini inaspettati durante i test. Questo scenario multi-dominio ha attirato molti lavori recenti che rientrano nell'ombrello generale del transfer learning. In questo studio, rivediamo la traduzione automatica multi-dominio, con l'obiettivo di formulare le motivazioni per lo sviluppo di tali sistemi e le relative aspettative rispetto alle prestazioni. I nostri esperimenti con un ampio campione di sistemi multi-dominio dimostrano che la maggior parte di queste aspettative è difficilmente soddisfatta e suggeriscono che è necessario lavorare ulteriormente per analizzare meglio il comportamento attuale dei sistemi multi-dominio e per farli mantenere pienamente le promesse.", 'hu': 'Gépi fordítási rendszerek építésekor gyakran a legjobbat kell kihozni a heterogén párhuzamos adatok készleteiből a képzés során, és robusztus kezelést kell végezni a váratlan területek bemeneteivel a tesztelés során. Ez a több domain forgatókönyv sok olyan közelmúltbeli munkát vonzott, amelyek a transzfer tanulás általános ernyőjébe tartoznak. Ebben a tanulmányban újra vizsgáljuk a több domain gépi fordítást, azzal a céllal, hogy megfogalmazzuk az ilyen rendszerek fejlesztésének motivációit és a teljesítményre vonatkozó kapcsolódó elvárásokat. A több domain rendszerek nagyméretű mintájával végzett kísérleteink azt mutatják, hogy ezen elvárások nagy része aligha teljesül, és arra utalnak, hogy további munkára van szükség a több domain rendszerek jelenlegi viselkedésének jobb elemzéséhez és teljes mértékben betartásához.', 'lt': 'Kuriant mašin ų vertimo sistemas dažnai reikia kuo geriau išnaudoti įvairiapusius lygiagrečių duomenų rinkinius rengiant mokymus ir patikimai tvarkyti netikėtų sričių įvedimus atliekant bandymus. Šis daugiadominis scenarijus pritraukė daug pastarojo meto darbo, kuris patenka į bendrą mokymosi perdavimu sistemą. Šiame tyrime persvarstome įvairių sričių mašinų vertimą, siekiant suformuluoti motyvacijas kurti tokias sistemas ir susijusius lūkesčius, susijusius su veiklos rezultatais. Mūsų eksperimentai su dideliu daugelio sričių sistemų pavyzdžiu rodo, kad dauguma šių lūkesčių sunkiai tenkinami ir rodo, kad reikia toliau dirbti siekiant geriau analizuoti dabartinį daugelio sričių sistemų elgesį ir užtikrinti, kad jos visiškai laikytųsi savo pažadų.', 'kk': 'Компьютердің аудару жүйелерін құру кезінде, біреуі тексеруде күтпеген домендерден ең жақсы параллель деректерді жасау керек, және тексеруде күтпеген домендерден енгізу керек. Бұл көптеген доменнің сценариясы жуырдың көптеген жұмысын аудару оқыту үшін жалпы мөлшерінің астында тұрады. Бұл зерттеулерде біз көп домен машинаның аудармасын қайта қарап, осы жүйелерді жұмыс істеу үшін жұмыс істеу үшін күтпеген күтпеулердің мотивацияларын формулау мақсатымыз. Біздің көп домен жүйелердің үлкен мәліметтеріміз бұл күтпеген көпшіліктердің көпшілігі келмейді және бірнеше домендық жүйелердің қазіргі әрекеттерін жақсы анализиялау үшін жұмыс істеу керек деп', 'mk': 'When building machine translation systems, one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing.  Овој мултидомен сценарио привлече многу неодамнешни работи кои паднаа под генералниот чадор на трансферентно учење. Во оваа студија, ние го повторуваме мултидоменскиот машински превод, со цел да ги формулираме мотивациите за развој на вакви системи и поврзаните очекувања во однос на перформансата. Нашите експерименти со голем примерок на мултидоменички системи покажуваат дека повеќето од овие очекувања едвај се исполнети и предлагаат дека е потребна понатамошна работа за подобра анализа на сегашното однесување на мултидоменичките системи и за нивното целосно исполнување на ветувањата.', 'ms': 'When building machine translation systems, one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing.  Skenario berbilang-domain ini telah menarik banyak kerja baru-baru ini yang jatuh di bawah payung umum pembelajaran pemindahan. Dalam kajian ini, kami mengulangi terjemahan mesin berbilang-domain, dengan tujuan untuk membentuk motivasi untuk mengembangkan sistem tersebut dan harapan yang berkaitan dengan prestasi. Eksperimen kami dengan sampel besar sistem-domain berbilang menunjukkan bahawa kebanyakan harapan-harapan ini hampir tidak dipenuhi dan menyarankan bahawa kerja lanjut diperlukan untuk menganalisis lebih baik perilaku semasa sistem-domain berbilang dan untuk membuat mereka memenuhi janji mereka sepenuhnya.', 'mt': 'Meta jinbnew sistemi ta’ traduzzjoni bil-magna, ħafna drabi wieħed irid jagħmel l-aħjar minn settijiet eteroġeniċi ta’ dejta parallela fit-taħriġ, u jimmaniġġja b’mod robust l-inputs minn dominji mhux mistennija fl-ittestjar. Dan ix-xenarju multidomestiku attira ħafna xogħol reċenti li jaqa’ taħt l-umbrella ġenerali tat-tagħlim tat-trasferiment. F’dan l-istudju, irrivedu t-traduzzjoni tal-magni b’diversi dominji, bil-għan li jiġu fformulati l-motivazzjonijiet għall-iżvilupp ta’ sistemi bħal dawn u l-aspettattivi assoċjati fir-rigward tal-prestazzjoni. L-esperimenti tagħna b’kampjun kbir ta’ sistemi multidomestiċi juru li ħafna minn dawn l-aspettattivi mhumiex sodisfatti u jissuġġerixxu li hemm bżonn ta’ aktar ħidma biex tiġi analizzata a ħjar l-imġiba attwali tas-sistemi multidomestiċi u biex ikunu jistgħu jirrispettaw bis-sħiħ il-wegħdiet tagħhom.', 'ml': 'മെഷിന്\u200d പരിശോധിക്കുന്നതില്\u200d നിന്നും പ്രതീക്ഷിക്കാത്ത ഡോമെന്\u200dസില്\u200d നിന്നും ഉത്തമമായ പരിശോധനങ്ങളില്\u200d നിന്നും മെച്ചപ്പെട്ട വിവരങ്ങള്\u200d  ഈ പല-ഡൊമൈന്\u200d സിനേരിയോ അടുത്തുള്ള ജോലിയില്\u200d ഒരുപാട് ആകര്\u200dഷിച്ചിരിക്കുന്നു. പൊതുവായ പഠനത്തിന്\u200dറെ കീഴില്\u200d വീ ഈ പഠനത്തില്\u200d നമ്മള്\u200d പല-ഡൊമൈന്\u200d മെഷീന്\u200d പരിഭാഷപ്പെടുത്തുന്നതിനെ വീണ്ടും പുനര്\u200dത്ഥിപ്പിക്കാന്\u200d ഉദ്ദേശിക്കുന്നു. പ്രവര്\u200dത്തനത് ഒരു വലിയ മാതൃകയുണ്ടായിരുന്നു നമ്മുടെ പരീക്ഷണങ്ങള്\u200d കാണിച്ചു കൊണ്ടിരിക്കുന്നത്, ഈ പ്രതീക്ഷ കുറച്ച് പ്രതീക്ഷ കാണാന്\u200d പോകുന്നില്ലെന്നും, കൂടുതല്\u200d ജോ', 'ro': 'Atunci când construiți sisteme de traducere automată, este adesea necesar să profitați la maximum de seturi eterogene de date paralele în cadrul instruirii și să gestionați în mod robust intrările din domenii neașteptate în cadrul testelor. Acest scenariu multidomeniu a atras o mulțime de lucrări recente care intră sub umbrela generală a învățării prin transfer. În acest studiu, revizuim traducerea automată multidomeniu, cu scopul de a formula motivațiile dezvoltării unor astfel de sisteme și așteptările asociate cu privire la performanță. Experimentele noastre cu un eșantion mare de sisteme multi-domeniu arată că majoritatea acestor așteptări sunt greu îndeplinite și sugerează că sunt necesare eforturi suplimentare pentru a analiza mai bine comportamentul actual al sistemelor multi-domeniu și pentru a le face să-și respecte pe deplin promisiunile.', 'pl': 'Budując systemy tłumaczenia maszynowego, często trzeba jak najlepiej wykorzystać heterogeniczne zbiory danych równoległych w szkoleniach i solidnie obsługiwać dane wejściowe z nieoczekiwanych domen w testowaniu. Ten scenariusz wielu domen przyciągnął wiele ostatnich prac, które wchodzą pod ogólny parasol uczenia się transferowego. W niniejszym opracowaniu przeglądamy wielodominienowe tłumaczenie maszynowe, mając na celu sformułowanie motywacji do rozwoju takich systemów i związanych z nimi oczekiwań w zakresie wydajności. Nasze eksperymenty z dużą próbką systemów multidomenowych pokazują, że większość z tych oczekiwań jest trudna i sugerują, że konieczne są dalsze prace, aby lepiej przeanalizować aktualne zachowanie systemów multidomenowych i sprawić, by w pełni dotrzymały swoich obietnic.', 'no': 'Når maskineoversettelsystemet bygger, må ein ofte gjera det beste ut av heterogeneske sett parallelle data i opplæring, og for å styra inndata frå uventa domene i testen. Dette fleire domene-scenarioen har tiltrekket mykje nyleg arbeid som fallar under den generelle umbrella av overføringsling. I denne studien reviserer vi fleire domenemaskinsomsetjingar, med målet å formere motivasjonane for å utvikla slike systemar og tilknytte forventingar med respekt til utvikling. Eksperimentane våre med eit stor prøve av fleire domenesystemer viser at dei fleste av desse forventingane er vanskeleg oppfylt og foreslår at meir arbeid er nødvendig for å betre analysera gjeldande oppførsel av fleire domenesystemer og gjera dei fullstendig å holde dei promissane sine.', 'mn': 'Машин хөрөнгө оруулах системийг бүтээхэд хүн ихэвчлэн багш сургуульд хамгийн бага зэрэг параллел өгөгдлийг хийх хэрэгтэй бөгөөд шалгахад хүлээн зөвшөөрөгдсөн орон нутгийнхээ зарцуулалтыг шалгах хэрэгтэй. Энэ олон төрлийн хувилбар нь шилжүүлэх сургалтын ерөнхий салбарын доор орж ирсэн олон саяхан ажлыг татсан. Энэ судалгаанд бид олон хэлбэрийн машины хөгжүүлэлтийг дахин шинэчлэх зорилготой. Ийм системүүд болон үйл ажиллагаанд хамааралтай хүлээн зөвшөөрөх урам зорилготой. Бидний олон холбоотой системийн жишээний туршилтууд эдгээр хүлээн зөвшөөрөгдсөн хүлээн зөвхөн олон холбоотой системийн орчин үеийн үйл явдлыг илүү шинжилгээр шинжилгээ хийх хэрэгтэй гэдгийг харуулж байна.', 'si': 'පරීක්ෂණයෙන් පද්ධතිය නිර්මාණය කරන්න පුළුවන් විතරයි, පරීක්ෂණයෙන් අනතුරු ඩෝමේන් වලින් ඇතුලක් ප්\u200dරශ්නය කරන්න පුළුවන් වෙන මේ ගොඩක් ඩෝමින් සිනාරියෝ ගොඩක් අලුත් වැඩක් අල්ලගත්තා. මේ පරීක්ෂණයේදී, අපි ගොඩක් ඩොමින් මැෂින් වාර්තාව පරීක්ෂණය කරනවා, අරක්ෂණයෙන් ඒ වගේ පද්ධති විකාශය සහ ප්\u200dරමාණය ස අපේ පරීක්ෂණය ගොඩක් ඩොමින් පද්ධතියේ ලොකු සාම්ප්ලේස් එක්ක පෙන්වන්නේ මේ බලාපොරොත්තු ගොඩක් හමුවෙන්නේ නැහැ කියලා ප්\u200dරශ්නයක් වෙන්න ඕන', 'sr': 'Kada građevinski sistem prevoda mašine, jedan često mora napraviti najbolje od heterogeneznih seta paralelnih podataka u treningu, i da se snažno obrađuje ulaganja iz neočekivanih domena u testiranju. Ovaj scenarij višedomenija privlačio je mnogo nedavnog rada koji pada pod općem kišobranom učenja prijenosa. U ovoj studiji, mi pregledamo prevod multidomenata mašine, sa ciljem da formulišemo motivacije za razvoj takvih sustava i povezane očekivanje u pogledu izvršnosti. Naši eksperimenti sa velikim uzorakom višedomeničkih sistema pokazuju da većina ovih očekivanja jedva ispunjava i sugerišu da je potreban daljnji posao da bi bolje analizirali trenutno ponašanje višedomeničkih sistema i da bi ih potpuno zadržali svoje obećanja.', 'sv': 'N채r man bygger maskin철vers채ttningssystem beh철ver man ofta f책 ut det b채sta av heterogena upps채ttningar parallella data i utbildningen och hantera input fr책n ov채ntade dom채ner i testningen p책 ett robust s채tt. Detta flerdom채nsscenario har lockat till sig mycket arbete p책 senare tid som faller under det allm채nna paraplyet f철r transferl채rande. I denna studie granskar vi maskin철vers채ttning med flera dom채ner i syfte att formulera motivationen f철r att utveckla s책dana system och d채rmed sammanh채ngande f철rv채ntningar p책 prestanda. V책ra experiment med ett stort urval av multi-domain system visar att de flesta av dessa f철rv채ntningar knappast uppfylls och tyder p책 att ytterligare arbete beh철vs f철r att b채ttre analysera det nuvarande beteendet hos multi-domain system och f책 dem att h책lla sina l철ften fullt ut.', 'so': 'Markii aad dhisataan nidaamka turjumidda machine, waxaa habboon in mid marar badan laga sameeyo koobab kala duduwan waxbarashada, iyo in lagu isticmaalo qalabka laga helo meelaha aan la filayn jirrabo. Muuqashadan kala duduwan ayaa soo jiiday shaqo badan oo la soo dhowaaday oo ku hoos dhacay barashada bedelka. Waxbarashadan ayaannu dib u soo celinaynaa tarjumaadda machine badan oo domain ah, kaas oo ku qoraya dhaqdhaqaalaha horumarinta nidaamka caynkaas ah iyo rajada la xiriira sameynta sameynta. Our experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behaviour of multi-domain systems and to make them fully hold their promises.', 'ta': 'இயந்திரம் மொழிபெயர்ப்பு அமைப்புகளை கட்டும் போது, ஒரு பெரும்பாலாவது பயிற்சியில் ஒத்திசைப்படுத்தும் தரவை வெளியேற்ற வேண்டும், மற்று இந்த பல-டோமைன் காட்சியோவு சமீபத்தில் நிறைய வேலையை ஆச்சரியமாக்கி விட்டது அது மாற்று கற்றத்தின் பொதுவான அளபுல இந்த ஆராய்ச்சியில், நாம் பல- domain இயந்திர மொழிபெயர்ப்பை மீண்டும் திரும்பச் செய்கிறோம். இது முறைமைகளை உருவாக்குவதற்கு தொடர்பு எத பல-domain அமைப்புகளில் பெரும்பாலான சோதனைகள் காண்பிக்கிறது இந்த எதிர்பார்ப்புகள் கடுமையாக சந்திக்கப்படுகிறது மற்றும் மேலும் வேலை செய்ய வேண்டும் பல- domain', 'ur': 'جب ماشین ترجمہ سیستم بنانے کی ضرورت ہوتی ہے، ایک کو بہت اچھی طرح تطالب میں پارالٹ ڈیٹوں کے بہترین سیٹ بنانے کی ضرورت ہوتی ہے، اور آزمائش میں غیر منتظر ڈومین سے اپنا اپنا اپنا اپنا اپنا اپنا اپنا اپنا یہ بہت سی ڈومین سناریو نے بہت سی اخیر کاروں کو آراستہ کیا ہے جو ترنسیٹ سیکھانے کی عمومی سطح کے نیچے پڑتے ہیں. اس مطالعہ میں ہم بہت سی ڈومین ماشین کی ترجمہ کو دوباره سنائیں گے، اس کے مطالعہ سے کہ ایسی سیستموں اور عملکرد کے معاملہ میں مرتبہ انتظار کرنے کے لئے موثر بنائیں۔ ہماری آزمائش بہت بڑی نمونہ ڈومین سیستم کے ساتھ دکھاتی ہے کہ ان کی اکثریت انتظار کم ہوتی ہے اور اس کی پیشنهاد کرتی ہے کہ بہت سی ڈومین سیستم کے موجود رفتار کو اچھی طرح تحلیل کرنے کے لئے اچھی طرح کی ضرورت ہے اور ان کا پورا وعدہ پورا کرنے کے لئے۔', 'uz': "Name Bu ko'plab-domen scenarini ko'pchilik o'rganish o'rganishning umumiy parametrlarida ko'pchiligi. Bu taʼminda, biz bir necha domen tarjima tarjima qilamiz, bu tizimni o'zgartirishning motivatorini va bajarish muhimligi bilan bog'liq kutilgan kutilgan ishni yaratish uchun. Bizning ko'pgina ko'plab-domen tizimlarimiz bilan bir misol ko'pchilik kutilgan narsalarning ko'pchiligi ham ko'proq kutilmaganda ishni bajarishimiz kerak va bir necha domen tizimning hodisasini bajarish va ularni butunlay ishga tushunish kerak.", 'vi': 'Khi chế tạo các hệ thống dịch chuyển máy, bạn thường phải tận dụng tối đa các dữ liệu song song trong huấn luyện, và sử dụng các nội dung từ những lĩnh vực bất ngờ để thử nghiệm. Trường hợp đa lĩnh vực này đã thu hút rất nhiều công việc gần đây nằm dưới tổng lãnh của việc học chuyển nhượng. Trong nghiên cứu này, chúng ta thăm lại dịch vụ máy đa miền, với mục đích xác định động cơ phát triển các hệ thống và những kỳ vọng liên quan đến khả năng. Những thí nghiệm với một mẫu lớn các hệ thống đa miền cho thấy hầu hết những kỳ vọng này hầu như không được đáp ứng đủ và cho thấy cần phải làm thêm để phân tích tốt hơn hành vi hiện tại của các hệ thống đa miền và khiến chúng giữ hoàn to àn lời hứa.', 'nl': 'Bij het bouwen van machine translation systemen moet men vaak het beste uit heterogene sets parallelle data in training halen en inputs uit onverwachte domeinen tijdens het testen robuust verwerken. Dit multi-domein scenario heeft veel recent werk aangetrokken dat onder de algemene paraplu van transfer learning valt. In deze studie bekijken we multi-domein machinevertaling, met als doel de motivaties voor de ontwikkeling van dergelijke systemen en de bijbehorende verwachtingen met betrekking tot prestaties te formuleren. Onze experimenten met een grote steekproef van multi-domein systemen laten zien dat aan de meeste van deze verwachtingen nauwelijks wordt voldaan en suggereren dat er verder werk nodig is om het huidige gedrag van multi-domein systemen beter te analyseren en ze volledig hun beloften te laten nakomen.', 'da': 'Når man bygger maskinoversættelsessystemer, skal man ofte få det bedste ud af heterogene sæt parallelle data under uddannelse og robust håndtere input fra uventede domæner under testning. Dette multi-domæne scenarie har tiltrukket en masse nyligt arbejde, der falder under den generelle paraply af transfer learning. I denne undersøgelse gennemgår vi maskinoversættelse med flere domæner med det formål at formulere motiverne til at udvikle sådanne systemer og de dermed forbundne forventninger til ydeevne. Vores eksperimenter med et stort udsnit af multi-domænesystemer viser, at de fleste af disse forventninger næppe opfyldes, og antyder, at der er behov for yderligere arbejde for bedre at analysere den nuværende adfærd af multi-domænesystemer og få dem til at holde deres løfter fuldt ud.', 'bg': 'Когато се изграждат системи за машинен превод, човек често трябва да извлече най-доброто от хетерогенните набори от паралелни данни при обучението и да се справя здраво с входовете от неочаквани области при тестването. Този многодомейн сценарий привлече много от скорошната работа, която попада под общия чадър на трансферното обучение. В настоящото проучване преразглеждаме многодомейнния машинен превод с цел формулиране на мотивите за разработване на такива системи и свързаните с тях очаквания по отношение на производителността. Нашите експерименти с голяма извадка от многодомейни системи показват, че повечето от тези очаквания едва ли са изпълнени и предполагат, че е необходима допълнителна работа, за да се анализира по-добре настоящото поведение на многодомейните системи и да се накара те да спазят напълно обещанията си.', 'hr': 'Kada građevinski sustav prevoda strojeva, jedan često mora napraviti najbolje od heterogeneznih seta paralelnih podataka u obuci, i snažno rješavati ulaze iz neočekivanih domena u testiranju. Ovaj višedomenički scenarij privlačio je mnogo nedavnog rada koji pada pod općim kišobranom učenja prijenosa. U ovom ispitivanju, pregledali smo prevod multidomenijskih strojeva, s ciljem formuliranja motivacija za razvoj takvih sustava i povezanih očekivanja u pogledu učinka. Naši eksperimenti s velikim uzorakom višedomeničkih sustava pokazuju da većina tih očekivanja jedva ispunjava i sugeriraju da je potreban daljnji posao kako bi bolje analizirali trenutno ponašanje višedomeničkih sustava i da bi ih potpuno zadržali svoje obećanja.', 'ko': '기계 번역 시스템을 구축할 때 보통 훈련에서 이구의 병행 데이터 집합을 충분히 활용하고 테스트에서 의외의 영역에서 온 입력을 안정적으로 처리해야 한다.이런 여러 분야의 장면은 최근의 업무를 많이 끌어들였는데 이런 업무는 모두 이동 학습의 범주에 속한다.이 연구에서 우리는 이런 시스템을 개발하는 동기와 성능과 관련된 기대를 밝히기 위해 여러 분야의 기계 번역을 재검토했다.우리가 다역 시스템의 대량의 견본을 실험한 결과 이러한 기대 중 대다수가 거의 만족을 얻지 못했고 다역 시스템의 현재 행위를 더욱 잘 분석하고 약속을 완전히 이행하도록 하는 작업이 필요하다는 것을 알 수 있다.', 'de': 'Beim Aufbau von maschinellen Übersetzungssystemen muss man oft das Beste aus heterogenen parallelen Datensätzen in Schulungen machen und Eingaben aus unerwarteten Bereichen beim Testen robust verarbeiten. Dieses Multi-Domain-Szenario hat viele neue Arbeiten angezogen, die unter den allgemeinen Schirm des Transfer Learning fallen. In dieser Studie untersuchen wir mehrdomänenübergreifende maschinelle Übersetzung, mit dem Ziel, die Motivationen für die Entwicklung solcher Systeme und die damit verbundenen Erwartungen an die Leistung zu formulieren. Unsere Experimente mit einer großen Stichprobe von Multi-Domain-Systemen zeigen, dass die meisten dieser Erwartungen kaum erfüllt werden und legen nahe, dass weitere Arbeiten erforderlich sind, um das aktuelle Verhalten von Multi-Domain-Systemen besser zu analysieren und zu erreichen, dass sie ihre Versprechen voll einhalten.', 'id': 'Ketika membangun sistem terjemahan mesin, seseorang sering perlu membuat yang terbaik dari set heterogene data paralel dalam latihan, dan untuk menangani dengan kuat input dari domain tidak terduga dalam tes. Skenario multi-domain ini telah menarik banyak pekerjaan baru-baru ini yang jatuh di bawah payung umum belajar transfer. Dalam studi ini, kami mengulangi terjemahan mesin multi-domain, dengan tujuan untuk menyiformulasi motivasi untuk mengembangkan sistem seperti itu dan harapan yang terkait dengan prestasi. Eksperimen kami dengan sampel besar sistem multi-domain menunjukkan bahwa kebanyakan harapan-harapan ini hampir tidak dipenuhi dan menyarankan bahwa pekerjaan lanjut diperlukan untuk menganalisis lebih baik perilaku saat ini sistem multi-domain dan untuk membuat mereka memenuhi janji mereka sepenuhnya.', 'af': "Wanneer masjien vertalingsstelsels bou, moet een dikwels die beste maak uit heterogenees stelle parallele data in onderwerp en om inputs van onverwagte domeine in te toets te hanteer. Hierdie multidomein-scenario het 'n baie onlangse werk aantrek wat onder die algemene umbrella van oordrag leer val. In hierdie studie het ons multidomein masjien vertaling hersien, met die doel om die motivasies vir sodanige stelsels te ontwikkel en die geassosieerde verwagtings met respek na prestasie te formeer. Ons eksperimente met 'n groot voorbeeld van multidomein stelsels wys dat die meeste van hierdie verwagtings moet hardlik ontmoet word en voorstel dat verdere werk benodig is om die huidige gedrag van multidomein stelsels te beter analyseer en om hulle volledig hul belofte te hou.", 'tr': 'Makina terjime sistemalary guranda köplenç bir adam öwrenmek üçin parallel hatlaryň gowyny çykarmaly we çalynmadyk alanlardan girişi çykarmaly bolar. Bu multi-domeny senaryo geçirmek öwrenmesiniň umumy saýawanyň astynda bolan köp işi çekdi. Bu araştyrmada, biz birnäçe-domeny maşynyň terjimesini, beýleki sistemlerden gelişmek üçin we başaryşlara golaýlaşyk gözlenmeleri döretmek üçin nusgasyny bejerdik. Biziň multi-domeny sistemalaryň örän uly nümüne görýän deneylerimiz bu beklenmeleriň köpüsi ýok bolmaýandygyny görkezýär we olaryň sözlerini doly çykarmak üçin häzirki zatlaryň üstüne bir şekilde çözmelidigini maslahat berýär.', 'sw': 'Wakati unajenga mfumo wa kutafsiri mashine, mara nyingi mtu anahitaji kutengeneza vizuri zaidi kutoka kwenye seti za takwimu za usalama katika mafunzo, na kuchukua vifaa vinavyotarajiwa katika majaribio. Hali hii ya maeneo mengi imevutia kazi nyingi za hivi karibuni ambazo zinaanguka chini ya umbrella la la kujifunza kwa ujumla. Katika utafiti huu, tunapitia tafsiri ya mashine ya ndani mbalimbali, kwa lengo la kutengeneza hatua za kutengeneza mifumo kama hizi na matumaini yanayohusiana na kuheshimu utendaji. Majaribio yetu yenye sampuli kubwa ya mifumo mingi ya ndani inaonyesha kuwa matumaini mengi haya hayakutana vigumu na yanapendekeza kwamba kazi zaidi inahitajika ili uchambuzi wa tabia za mifumo mingi ya ndani na kuwafanya wanavyoahidi kutimiza ahadi zao.', 'fa': 'وقتی سیستم\u200cهای ترجمه ماشین ساخته می\u200cشود، یکی اغلب نیاز به بهترین مجموعه\u200cهای داده\u200cهای متفاوتی در آموزش ساخته می\u200cشود، و به سختی از دامنه\u200cهای غیرمنتظر در آزمایش به کار می\u200cدهد. این سناریو بسیاری از کارهای اخیر را جذب کرده است که زیر سایه\u200cهای عمومی از یادگیری انتقال است. در این مطالعه، ما ترجمه\u200cهای ماشین\u200cهای مختلف دومین\u200cها را تغییر می\u200cدهیم، با هدف فرمول انگیزه\u200cهایی برای توسعه این سیستم\u200cها و انتظارات ارتباط با احترام عملکرد. آزمایش های ما با نمونه بزرگ از سیستم\u200cهای چندین دامنی نشان می\u200cدهند که بیشتر این انتظارها به سختی پیدا نمی\u200cشوند و پیشنهاد می\u200cدهند که کار بیشتر نیاز است تا بهتر تحلیل رفتار فعلی سیستم\u200cهای چندین دامنی\u200cها و تا آنها را کاملاً به قول\u200cهایشان نگه دارند.', 'am': 'መሣሪያን ትርጉም ሲገንጽ፣ ብዙ ጊዜ የሚጠቅመው የሥልጣናት ማህበረሰብ እና በተስፋት ከያልተጠባበቁት domains ውስጥ ያሉትን ጥያቄዎችን ለመቀነስ ያስፈልጋል፡፡ ይህ የብዙ ዶሜን ስዕይት በቅርብ ዘመን በመለወጥ ትምህርት በታች የሚወድቅ ብዙ ሥራ ያሳስታል፡፡ በዚህ ትምህርት ውስጥ፣ ብዙ ዶሜን መተርጓሚዎችን እና የዚህን ስርዓቶች ለመፍጠር እና በተለያዩ ተስፋዎችን ለመፍጠር እናደርጋለን፡፡ ብዙ ድምፅ ሲሞክራችንን በብዙ ድምፅ የሚያሳየው የእነዚህ ብዙዎቹ ተስፋዎች በጭንቅ እንደተገናኙ እና የብዙ ዶሜን ስርዓቶች የአሁኑን ሁኔታ አስተያየት እንዲያስፈልጋቸው እና የተስፋ ቃላትን ሙሉ እንዲያቆሙ ያስፈልጋል፡፡', 'az': 'Makinelərin çeviri sistemlərini in şa etdikdə, insanların sınaqlarında gözləməmiş domeinlərdən ən yaxşı paralel məlumatlardan istifadə etməsi lazımdır. Bu çoxlu domena senaryosu, hərəkət öyrənməsinin genel şəmsal altına düşən çoxlu yeni işləri tələb edir. Bu təhsil içində çoxlu domani maşına çevirilməsini yenidən dəyişdiririk, böyük sistemləri və təhsil ilə birlikdə təhsil etmək üçün motivasiyaları formülə vermək niyyətində. Bizim çoxlu domenin sistemlərin böyük nümunələri ilə təcrübələrimiz bu təcrübələrin çoxu çətin olmadığını göstərir və çoxlu domenin sistemlərinin həmişəlik davranışlarını daha yaxşı analiz etmək və onlara vədlərini tamamilə yerinə yetirmək üçün daha çox çalışma lazımdır.', 'bn': 'যখন মেশিন অনুবাদ সিস্টেম নির্মাণ করা হয়, তখন প্রায়শই প্রশিক্ষণের মধ্যে প্যারালেলেল ডাটা থেকে সর্বোচ্চ সেট বের করতে হবে এবং অপ্রত্যাশিত ডোমেন এই বহুডোমেইন দৃশ্য সাম্প্রতিক কাজের আকর্ষণ করেছে যা সাধারণ বিনিময়ের শিক্ষার অধীনে পড়েছে। এই গবেষণায় আমরা মাল্টিডোমেইন মেশিন অনুবাদ পুনরায় আবার পুনরায় বিবেচনা করি, যার উদ্দেশ্য হচ্ছে এই ধরনের সিস্টেম উন্নয়নের উদ্দেশ্য এবং প্রদর্শনে বহুডোমেইন সিস্টেমের বিশাল পরীক্ষা দিয়ে আমাদের পরীক্ষা দেখাচ্ছে যে এগুলোর বেশীরভাগ আশা কঠিন হয় না এবং পরামর্শ দেয় যে আরো কাজের প্রয়োজন যে বহুডোমেইন সিস্ট', 'bs': 'Kada građevinski sistem prevoda mašine, često je potrebno da napravi najbolje od heterogeneznih seta paralelnih podataka u obuci, i da se snažno obrađuje ulaganja iz neočekivanih domena u testiranju. Ovaj višedomenički scenarij privlačio je mnogo nedavnog rada koji pada pod općem kišobranom učenja prijenosa. U ovom istraživanju, mi pregledamo prevod multidomenijskih strojeva, sa ciljem formuliranja motivacija za razvoj takvih sustava i povezanih očekivanja u pogledu izvođenja. Naši eksperimenti s velikim uzorakom multidomenijskih sustava pokazuju da većina tih očekivanja jedva ispunjava i sugerišu da je potreban daljnji posao da bi bolje analizirali trenutno ponašanje multidomenijskih sustava i da bi ih potpuno zadržali svoje obećanja.', 'cs': 'Při stavbě strojového překladu systémů je často třeba využít to nejlepší z heterogenních souborů paralelních dat ve školení a robustně zpracovat vstupy z neočekávaných oblastí při testování. Tento scénář s více doménami přilákal mnoho nedávných prací, které spadají pod obecný deštník transferového učení. V této studii revidujeme multidoménový strojový překlad s cílem formulovat motivace pro vývoj těchto systémů a s tím související očekávání ohledně výkonu. Naše experimenty s velkým vzorkem multidoménových systémů ukazují, že většina těchto očekávání je stěží splněna a naznačují, že je zapotřebí další práce k lepší analýze současného chování multidoménových systémů a k tomu, aby plně splnily své sliby.', 'ca': "Quan construïm sistemes de traducció màquina, sovint cal aprofitar el millor dels conjunts heterogènes de dades paralleles en l'entrenament, i gestionar de manera robusta les entrades de dominys inesperats en les proves. Aquest escenari multidominiu ha atraït molta feina recent que cau sota l'umbrella general de l'aprenentatge de transfer ències. En aquest estudi, revisem la traducció de màquines multidominios, amb l'objectiu de formular les motivacions per desenvolupar aquests sistemes i les expectatives associades en relació al rendiment. Els nostres experiments amb una gran mostra de sistemes multidominiosos mostren que la majoria d'aquestes expectatives gairebé no es satisfen i suggereixen que es necessita més treball per analitzar millor el comportament actual dels sistemes multidominiosos i fer-los cumplir plenament les seves promeses.", 'fi': 'Konekäännösjärjestelmiä rakennettaessa on usein hyödynnettävä koulutuksen heterogeenisiä rinnakkaisaineistoja parhaalla mahdollisella tavalla ja käsiteltävä luotettavasti testauksen odottamattomien osa-alueiden syötteitä. Tämä moniulotteinen skenaario on houkutellut paljon viimeaikaista työtä, joka kuuluu siirtooppimisen yleiseen kattoon. Tässä tutkimuksessa tarkastelemme uudelleen monikielistä konekäännöstä, jonka tavoitteena on muotoilla motivaatiot tällaisten järjestelmien kehittämiseen ja niihin liittyvät odotukset suorituskyvystä. Kokeemme suurella määrällä monitahoisia järjestelmiä osoittavat, että suurin osa näistä odotuksista on tuskin täytetty, ja viittaavat siihen, että lisätyötä tarvitaan, jotta monitahoisten järjestelmien nykyinen käyttäytyminen voitaisiin analysoida paremmin ja jotta ne täyttäisivät lupauksensa.', 'et': 'Masintõlkesüsteemide ehitamisel tuleb sageli koolitusel ära kasutada paralleelsete andmete heterogeensed kogumid ning katsetamisel kasutada kindlalt ootamatutest valdkondadest pärit sisendeid. See mitmevaldkondlik stsenaarium on meelitanud palju hiljutist tööd, mis kuuluvad üldise siirdeõppe katusekambri alla. Käesolevas uuringus vaatame läbi mitmevaldkondliku masintõlke, eesmärgiga sõnastada selliste süsteemide arendamise motivatsioonid ja nendega seotud ootused seoses jõudlusega. Meie eksperimendid suure valimi mitmevaldkondlike süsteemidega näitavad, et enamik neist ootustest on vaevalt täidetud, ning näitavad, et on vaja edasist tööd, et analüüsida paremini mitmevaldkondlike süsteemide praegust käitumist ja panna need täielikult oma lubadusi täitma.', 'hy': 'Երբ ստեղծում ենք մեքենայի թարգմանման համակարգեր, հաճախ պետք է լավագույնը օգտագործենք զուգահեռ տվյալների հեթերգենյա համակարգերից ուսումնասիրության մեջ և ուժեղ վերահսկել անսպասելի բնագավառների ներմուծները թեստերի մե This multi-domain scenario has attracted a lot of recent work that fall under the general umbrella of transfer learning.  Այս ուսումնասիրության ընթացքում մենք վերադառնում ենք բազմաբնագավառների մեքենային թարգմանություն, որպեսզի ձևավորենք նման համակարգերի զարգացման մոտիվացիաները և դրանց հետ կապված ակնկալիքները արդյունքի հետ կապված: Մեր փորձարկումները բազմատիրային համակարգերի մեծ նմուշներով ցույց են տալիս, որ այս սպասելիքներից շատերը հազիվ են բավարարվում և առաջարկում են, որ ավելի շատ աշխատանք է անհրաժեշտ բազմատիրային համակարգերի ներկայիս վարքագծի ավելի լավ վերլուծելու և նրանց ամբողջովի', 'sq': 'When building machine translation systems, one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing.  Ky skenar shumëdomenik ka tërhequr shumë punë të fundit që bie nën parajashtimin e përgjithshëm të mësimit të transferimit. Në këtë studim, ne përsërisim përkthimin e makinave me shumë domene, me qëllim që të formulojmë motivet për zhvillimin e sistemeve të tilla dhe shpresat e lidhura me performancën. Our experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behaviour of multi-domain systems and to make them fully hold their promises.', 'he': 'כשנוצרים מערכות תרגום מכונות, לעתים קרובות צריך לעשות את הטוב ביותר מתוך קבוצות heterogeneous של נתונים מקבילים באימונים, ולטפל בצורה חזקה בתכניות מתחומים בלתי צפויים בבדיקות. התרחיש המחולק הזה נמשך הרבה עבודה לאחרונה שנפלה תחת המטריה הכללית של לימוד העברה. במחקר הזה, אנו חוזרים לתרגום מכונות רבות בתחומים, במטרה ליצור את המוטיבציות לפיתוח מערכות כאלה והציפיות הקשורות בנוגע לביצוע. הניסויים שלנו עם דגימה גדולה של מערכות multi-domain מראים שרוב ציפיות אלה בקושי נפגשות ומצייעים שהעבודה נוספת נדרשת כדי לנתח טוב יותר את התנהגות הנוכחית של מערכות multi-domain ולגרום להם להחזיק במלוא את הבטחותיהם.', 'ha': "Idan an ƙiƙira fassarar ayukan kwamfyutan ayuka, ko da yawa wani ya kamata ta mafi alhẽri daga matsayin hoterogenous data masu daidaita cikin shirin taƙaita, kuma ya yi riƙon abubuwa da inputi daga kayan aiki wanda ba'a ƙayyade ba, cikin jarraba. Wannan surori na multi-Domen ya shagaltar da aiki mai yawa wanda ke ƙaranci a ƙarƙashin jumla da aka samu karatun mafaka. A cikin wannan lõkaci, muna sake sarrafa fassarar masu cikin mutane, don an so zuwa a ƙayyade juyi wa masu buɗaɗwa na kasar wannan na'urar-tsari da misãlai masu husũma da masu husũma da mafarin aiki. Kayan jarrabõnmu da misali mai yawa na'urar-mutane na nuna cewa masu kamata a haɗa tsakanin waɗannan, kuma yana shauri cewa an buƙata aiki na ƙari dõmin a fi anaƙayyade aikin da ke kai yanzu na'urar-masu-mutane da kuma ya kamata su cika alkawarin da suka yi musu alkawari.", 'sk': 'Pri gradnji sistemov strojnega prevajanja je treba pri usposabljanju pogosto izkoristiti najboljše možnosti heterogenih naborov vzporednih podatkov in zanesljivo ravnati z vnosi iz nepričakovanih področij pri testiranju. Ta scenarij z več področji je pritegnil veliko nedavnega dela, ki spada v splošno krovnico transfernega učenja. V tej študiji ponovno obravnavamo večdomensko strojno prevajanje z namenom oblikovanja motivacij za razvoj takšnih sistemov in s tem povezanih pričakovanj glede uspešnosti. Naši poskusi z velikim vzorcem večdomenskih sistemov kažejo, da je večina teh pričakovanj komaj izpolnjenih, in kažejo, da je potrebno nadaljnje delo za boljšo analizo trenutnega vedenja večdomenskih sistemov in njihovo polno izpolnitev obljub.', 'jv': 'Laptop battery7 Multi-domain scenari iki attractive In this test, we resumed multi Awak dhéwé éntuk karo akeh sampeyan akeh sistem Multi-domain bukane ing nggawe barang nggawe barang nggawe winih iki dadi bisa metiné gawe ngubah bisa supayano karo akeh opera sing bisa butakon nggawe sistem multi-domain sistem dadi bisa dianggawe winih dhéwé.', 'bo': 'When building machine translation systems, one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing. This multi-domain scenario has attracted a lot of recent work that fall under the general umbrella of transfer learning. In this study, we revisit multi-domain machine translation, with the aim to formulate the motivations for developing such systems and the associated expectations with respect to performance. ང་ཚོའི་experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behavior of multi-domain systems and to make them fulfill their promises.'}
{'en': 'Conversation Graph : Data Augmentation, Training, and Evaluation for Non-Deterministic Dialogue Management', 'fr': 'Graphique de conversation\xa0: augmentation des données, formation et évaluation pour la gestion de dialogue non déterministe', 'ar': 'الرسم البياني للمحادثة: زيادة البيانات والتدريب والتقييم لإدارة الحوار غير الحتمي', 'pt': 'Gráfico de conversação: aumento de dados, treinamento e avaliação para gerenciamento de diálogo não determinístico', 'es': 'Gráfico de conversación: aumento de datos, capacitación y evaluación para la gestión del diálogo no determinista', 'ja': '会話グラフ：非決定論的対話管理のためのデータ拡張、トレーニング、および評価', 'zh': '对话图:非确定性对数增强、训练、评估', 'ru': 'Диалоговый график: расширение данных, обучение и оценка для управления недетерминированным диалогом', 'hi': 'वार्तालाप ग्राफ़: गैर-नियतात्मक संवाद प्रबंधन के लिए डेटा संवर्धन, प्रशिक्षण, और मूल्यांकन', 'ga': 'Graf Comhrá: Méadú ar Shonraí, Oiliúint agus Measúnú le haghaidh Bainistíocht Neamhchinntitheach Agallaimh', 'ka': 'პარაუზაცია გრაფიკა: მონაცემები აგგენტირება, განაწყვება და განაწყვება არ განაწყვეტირებულ დიალოგის მართვის', 'hu': 'Beszélgetési grafikon: Adatbővítés, képzés és értékelés a nem meghatározó párbeszédkezeléshez', 'it': 'Grafico di conversazione: Aumento dei dati, formazione e valutazione per la gestione del dialogo non deterministico', 'el': 'Γράφημα συνομιλίας: Επέκταση δεδομένων, εκπαίδευση και αξιολόγηση για τη διαχείριση μη-αποφασιστικού διαλόγου', 'kk': 'Сөйлесу график: Деректерді өзгерту, оқыту және бағалау диалогты басқару үшін деректерді өзгерту', 'mk': 'Граф за конверзација: Агментација на податоците, обука и евалуација за менаџмент на дијалогот без одредување', 'lt': 'Perskaičiavimo grafikas: duomenų didinimas, mokymas ir vertinimas, susijęs su dialogo valdymu ne determinacijomis', 'ms': 'Graf Perbualan: Pengembangan Data, Latihan, dan Evaluasi untuk Pengurusan Dialog Tidak Deterministik', 'ml': 'സംസാരം', 'mt': 'Grafika ta’ Konverżjoni: Żieda fid-Dejta, Taħriġ u Evalwazzjoni għall-Ġestjoni tad-Djalogu Mhux Deterministiku', 'mn': 'Conversation Graph: Data Augmentation, Training, and Evaluation for Non-Deterministic Dialog Management', 'pl': 'Wykres konwersacji: Rozszerzenie danych, szkolenie i ocena dla niedeterministycznego zarządzania dialogiem', 'ro': 'Graficul conversației: Augmentarea datelor, instruirea și evaluarea pentru managementul dialogului non-determinist', 'si': 'සංවාදය ග්\u200dරාෆ්: දත්ත විශාලනය, ප්\u200dරධානය, සහ අවශ්\u200dයාත්මක සංවාදය පාලනය සඳහා දත්ත විශාලනය', 'so': 'Graph la xiriirka: Aqoonsiga macluumaadka, Waxbarashada iyo kaartaynta diyaariga aan la aqoonin', 'sv': 'Konversationsdiagram: dataförstärkning, utbildning och utvärdering för icke-deterministisk dialog hantering', 'ta': 'பேச்சு வரைப்படம்:', 'ur': 'تبلیغ گراف: ڈاٹا اگنٹمنٹ، ٹرینینگ، اور مطالعہ غیر معلوم ڈیلوگو منظورت کے لئے', 'sr': 'Graf razgovora: Povećavanje podataka, obuka i procjena za upravljanje neodređenim dijalogima', 'no': 'Samtalegraf: Data Augmentation, Training and Evaluation for Non-Deterministic Dialog Management', 'uz': 'Dialog boshqaruvchisi uchun maò¥lumot bogò£lash, taò¥minlovchi va taò¥minlovchi darajasi', 'vi': 'Máy tính đối thoại: gia tăng dữ liệu, đào tạo và đánh giá đối phó', 'bg': 'Графика за разговори: Увеличаване на данни, обучение и оценка за управление на недетерминистичен диалог', 'nl': 'Gespreksgrafiek: Data Augmentation, Training en Evaluatie voor niet-deterministisch dialoogbeheer', 'hr': 'Graf razgovora: Povećanje podataka, obuka i procjena za upravljanje nedodređenim dijalogima', 'da': 'Conversation Graph: Dataudvidelse, uddannelse og evaluering til ikke-deterministisk dialog styring', 'ko': '대화도: 비확정적 대화 관리의 데이터 확충, 교육과 평가', 'fa': 'گراف گفتگو: افزایش داده\u200cها، آموزش و ارزیابی برای مدیریت گفتگوی غیر معلوم', 'de': 'Konversationsgrafik: Datenerweiterung, -training und -auswertung für nicht-deterministisches Dialogmanagement', 'sw': 'Conversation Graph: Data Augmentation, Training, and Evaluation for Non-Deterministic Dialogue Management', 'id': 'Graf konversasi: Data Augmentation, Training, and Evaluation for Non-Deterministic Dialogue Management', 'tr': 'New conversations Graph: Data Augmentation, Training, and Evaluation for Non-Deterministic Dialog Management', 'sq': 'Grafi i konversimit: rritja e të dhënave, stërvitja dhe vlerësimi për menazhimin e dialogut jo-determinativ', 'af': 'Gespraak Graf: Data Augmentation, Training, en Evaluering vir Non- Deterministic Dialog Management', 'hy': 'Խոսակցության գրաֆիկ. տվյալների աճը, պատրաստվածությունը և գնահատումը ոչ որոշող պատմությունների ղեկավարման համար', 'az': 'Qonuşma Grafiki: Data Augmentation, Training and Evaluation for Non-Deterministic Dialog Management', 'am': 'Conversation Graph: Data Augmentation, Training, and Evaluation for Non-Deterministic Dialogue Management', 'bn': 'আলোচনা', 'bs': 'Graf razgovora: povećanje podataka, obuka i procjena za upravljanje nedodređenim dijalogima', 'cs': 'Konverzační graf: Rozšíření dat, školení a hodnocení pro nederministické řízení dialogu', 'et': 'Vestlusgraafik: andmete täiendamine, koolitus ja hindamine mittedeterministliku dialoogi haldamiseks', 'fi': 'Keskustelukuvio: Tiedon lisääminen, koulutus ja arviointi ei-deterministisen vuoropuhelun hallintaa varten', 'ca': 'Gràfic de conversació: augment de dades, formació i evaluació de la gestió del diàleg no determinant', 'ha': 'KCharselect unicode block name', 'he': 'Graph Conversation: Data Augmentation, Training, and Evaluation for Non-Deterministic Dialogue Management', 'sk': 'Graf pogovora: Povečanje podatkov, usposabljanje in vrednotenje za upravljanje nedeterminističnega dialoga', 'bo': 'Conversation Graph: Data Augmentation, Training and Evaluation for Non-Deterministic Dialog Management', 'jv': 'New'}
{'en': 'Task-oriented dialogue systems typically rely on large amounts of high-quality training data or require complex handcrafted rules. However, existing datasets are often limited in size con- sidering the  complexity  of the  dialogues . Additionally, conventional training signal in- ference is not suitable for non-deterministic agent behavior, namely, considering multiple actions as valid in identical dialogue states. We propose the Conversation Graph (ConvGraph), a graph-based representation of dialogues that can be exploited for  data augmentation , multi- reference training and evaluation of non- deterministic agents. ConvGraph generates novel dialogue paths to augment data volume and diversity. Intrinsic and extrinsic evaluation across three datasets shows that data augmentation and/or multi-reference training with ConvGraph can improve dialogue success rates by up to 6.4 %.', 'ar': 'تعتمد أنظمة الحوار الموجه نحو المهام عادةً على كميات كبيرة من بيانات التدريب عالية الجودة أو تتطلب قواعد معقدة مصنوعة يدويًا. ومع ذلك ، غالبًا ما تكون مجموعات البيانات الحالية محدودة الحجم نظرًا لتعقيد الحوارات. بالإضافة إلى ذلك ، فإن إشارة التدريب التقليدية ليست مناسبة لسلوك الوكيل غير الحتمي ، أي اعتبار الإجراءات المتعددة صالحة في حالات الحوار المتطابقة. نقترح الرسم البياني للمحادثة (ConvGraph) ، وهو تمثيل قائم على الرسم البياني للحوارات يمكن استغلاله لزيادة البيانات والتدريب متعدد المراجع وتقييم العوامل غير الحتمية. ينشئ ConvGraph مسارات حوار جديدة لزيادة حجم البيانات والتنوع. يُظهر التقييم الداخلي والخارجي عبر ثلاث مجموعات بيانات أن زيادة البيانات و / أو التدريب متعدد المراجع باستخدام ConvGraph يمكن أن يحسن معدلات نجاح الحوار بنسبة تصل إلى 6.4٪.', 'pt': 'Os sistemas de diálogo orientados a tarefas geralmente dependem de grandes quantidades de dados de treinamento de alta qualidade ou exigem regras complexas feitas à mão. No entanto, os conjuntos de dados existentes são frequentemente limitados em tamanho, considerando a complexidade dos diálogos. Além disso, a inferência de sinal de treinamento convencional não é adequada para o comportamento não determinístico do agente, ou seja, considerando múltiplas ações como válidas em estados de diálogo idênticos. Propomos o Conversation Graph (ConvGraph), uma representação gráfica de diálogos que pode ser explorada para aumento de dados, treinamento multirreferência e avaliação de agentes não determinísticos. O ConvGraph gera novos caminhos de diálogo para aumentar o volume e a diversidade de dados. A avaliação intrínseca e extrínseca em três conjuntos de dados mostra que o aumento de dados e/ou treinamento multirreferência com o ConvGraph pode melhorar as taxas de sucesso do diálogo em até 6,4%.', 'es': 'Los sistemas de diálogo orientados a tareas suelen basarse en grandes cantidades de datos de formación de alta calidad o requieren reglas complejas elaboradas a mano. Sin embargo, los conjuntos de datos existentes a menudo tienen un tamaño limitado, teniendo en cuenta la complejidad de los diálogos. Además, la interferencia de señales de entrenamiento convencional no es adecuada para el comportamiento no determinista del agente, es decir, considerar múltiples acciones como válidas en estados de diálogo idénticos. Proponemos el Gráfico de conversación (ConvGraph), una representación gráfica de diálogos que se puede aprovechar para el aumento de datos, el entrenamiento de múltiples referencias y la evaluación de agentes no deterministas. ConvGraph genera nuevas vías de diálogo para aumentar el volumen y la diversidad de los datos. La evaluación intrínseca y extrínseca en tres conjuntos de datos muestra que el aumento de datos o la capacitación en múltiples referencias con ConvGraph pueden mejorar las tasas de éxito de los diálogos hasta en un 6,4%.', 'fr': "Les systèmes de dialogue axés sur les tâches reposent généralement sur de grandes quantités de données d'entraînement de haute qualité ou nécessitent des règles artisanales complexes. Cependant, les ensembles de données existants sont souvent de taille limitée compte tenu de la complexité des dialogues. De plus, l'interférence de signal d'entraînement conventionnelle ne convient pas au comportement non déterministe d'un agent, c'est-à-dire que l'on considère que plusieurs actions sont valides dans des états de dialogue identiques. Nous proposons le graphique de conversation (ConvGraph), une représentation graphique des dialogues qui peut être exploitée pour l'augmentation des données, la formation multi-références et l'évaluation d'agents non déterministes. ConvGraph génère de nouveaux chemins de dialogue pour augmenter le volume et la diversité des données. L'évaluation intrinsèque et extrinsèque de trois ensembles de données montre que l'augmentation des données et/ou la formation multi-références avec ConvGraph peuvent améliorer les taux de réussite des dialogues jusqu'à 6,4\xa0%.", 'ja': 'タスク指向の対話システムは、通常、大量の高品質のトレーニングデータに依存するか、複雑な手作りルールを必要とします。しかし、既存のデータセットは、ダイアログの複雑さを考慮してサイズが制限されていることが多い。さらに、従来のトレーニング信号は、非決定論的なエージェントの行動、すなわち、複数のアクションを同一の対話状態で有効であると見なすことには適していない。データ拡張、マルチリファレンスのトレーニング、非決定論的エージェントの評価に利用できるダイアログのグラフベースの表現である会話グラフ（ ConvGraph ）を提案します。ConvGraphは、新しいダイアログパスを生成して、データ量と多様性を増大させます。3つのデータセットにわたる内在的および外在的評価は、ConvGraphを使用したデータ拡張および/またはマルチレファレンストレーニングが、対話成功率を最大6.4%向上させることができることを示しています。', 'zh': '向任之对,常依大高质量训练,或须杂手工作法。 然见数集大小,往往限之,与对语之复杂性也。 又旧法训号不宜非确定性摄行,即以为多同对。 臣等发对图(ConvGraph),盖图之对,可施于数,多参练非确定性之评。 ConvGraph生新语路,以增数据量多样性。 三集之内,外之评明,ConvGraph之增/,参之训练,可以成功率6.4%。', 'hi': 'कार्य-उन्मुख संवाद प्रणाली आमतौर पर उच्च गुणवत्ता वाले प्रशिक्षण डेटा की बड़ी मात्रा पर भरोसा करती है या जटिल हस्तशिल्प नियमों की आवश्यकता होती है। हालांकि, मौजूदा डेटासेट अक्सर संवादों की जटिलता को दरकिनार करते हुए आकार में सीमित होते हैं। इसके अतिरिक्त, पारंपरिक प्रशिक्षण संकेत गैर-नियतात्मक एजेंट व्यवहार के लिए उपयुक्त नहीं है, अर्थात्, समान संवाद राज्यों में मान्य के रूप में कई कार्यों पर विचार करना। हम वार्तालाप ग्राफ (ConvGraph) का प्रस्ताव करते हैं, जो संवादों का एक ग्राफ-आधारित प्रतिनिधित्व है जिसका उपयोग डेटा वृद्धि, बहु-संदर्भ प्रशिक्षण और गैर-नियतात्मक एजेंटों के मूल्यांकन के लिए किया जा सकता है। ConvGraph डेटा की मात्रा और विविधता को बढ़ाने के लिए उपन्यास संवाद पथ उत्पन्न करता है। तीन डेटासेट में आंतरिक और बाह्य मूल्यांकन से पता चलता है कि ConvGraph के साथ डेटा वृद्धि और / या बहु-संदर्भ प्रशिक्षण 6.4% तक संवाद सफलता दर में सुधार कर सकता है।', 'ru': 'Системы диалога, ориентированные на решение конкретных задач, как правило, опираются на большой объем высококачественных учебных данных или требуют сложных правил, разработанных вручную. Тем не менее, существующие наборы данных часто ограничены по размеру, учитывая сложность диалогов. Кроме того, обычная последовательность тренировочных сигналов не подходит для недетерминированного поведения агента, а именно, считая несколько действий действительными в идентичных диалоговых состояниях. Мы предлагаем Диалоговый Граф (ConvGraph), графическое представление диалогов, которое может быть использовано для увеличения данных, многоцелевого обучения и оценки недетерминированных агентов. ConvGraph создает новые диалоговые пути для увеличения объема и разнообразия данных. Внутренняя и внешняя оценка трех наборов данных показывает, что расширение данных и/или мультиреференсное обучение с помощью ConvGraph может улучшить показатели успеха диалога до 6,4%.', 'ga': "Go hiondúil bíonn córais dialóige atá dírithe ar thascanna ag brath ar mhéideanna móra sonraí oiliúna ardchaighdeáin nó éilíonn siad rialacha casta lámhdhéanta. Mar sin féin, is minic a bhíonn na tacair sonraí atá ann faoi láthair teoranta ó thaobh méide de ag cur san áireamh castacht na gcomhphlé. Ina theannta sin, níl tátail chomhartha traenála traidisiúnta oiriúnach d'iompar oibreora neamhchinntitheach, eadhon, ilghníomhaíochtaí a mheas mar ghníomhartha bailí i stáit chomhphlé comhionann. Molaimid an Conversation Graph (ConvGraph), léiriú graf-bhunaithe ar chomhphlé ar féidir leas a bhaint as chun sonraí a mhéadú, oiliúint il-tagartha agus meastóireacht a dhéanamh ar ghníomhairí neamhchinntitheacha. Gineann ConvGraph conairí núíosacha idirphlé chun cur le méid sonraí agus éagsúlacht. Léiríonn meastóireacht intreach agus eistreach thar thrí thacar sonraí gur féidir le méadú sonraí agus/nó oiliúint il-tagartha le ConvGraph rátaí ratha idirphlé a fheabhsú suas le 6.4%.", 'ka': 'სამუშაო დიალოგის სისტემები, როგორც ტიპულად უფრო დიდი სამუშაო სამუშაო მონაცემების მონაცემებისთვის, ან უნდა დაჭირდება კომპლე მაგრამ არსებობს მონაცემების კომპლექსიტეტი დიალოგების კომპლექსიტეტის ზომაში უფრო დაფარდება. დამატებით, კონტუნციონალური განაცემის სიგნალის ინფერენცია არ შესაძლებელია განსაზღვრული ადვნენტის ქცევისთვის, ანუ, რაც მრავალური მოქმედება იდენტიკური დიალოგის ღ ჩვენ შეგიძლიათ განსაუბრება გრაფიკა (ConvGraph), დიალოგების გრაფიკური გამოსახულება, რომელიც შეიძლია გამოიყენება მონაცემების აგგენტაციისთვის, მრავალური განსახულება და არ განსაზღვრული ადვნ Name სამი მონაცემების განსაზღვრებაში ინტერრინსიკური და ექსტრინსიკური განსაზღვრება ჩვენებს, რომ მონაცემები აზექტირება და/ან მრავალური განსაზღვრების განსაზღვრება ConvGraph-ის შესაძ', 'el': 'Τα συστήματα διαλόγου προσανατολισμένα στις εργασίες βασίζονται συνήθως σε μεγάλες ποσότητες υψηλής ποιότητας δεδομένα κατάρτισης ή απαιτούν πολύπλοκους χειροποίητους κανόνες. Ωστόσο, τα υπάρχοντα σύνολα δεδομένων είναι συχνά περιορισμένα σε μέγεθος λαμβάνοντας υπόψη την πολυπλοκότητα των διαλόγων. Επιπλέον, η συμβατική μετάδοση σήματος εκπαίδευσης δεν είναι κατάλληλη για μη ντετερμινιστική συμπεριφορά παραγόντων, δηλαδή, θεωρώντας πολλαπλές ενέργειες έγκυρες σε πανομοιότυπες καταστάσεις διαλόγου. Προτείνουμε το γράφημα συνομιλίας (μια απεικόνιση διαλόγων βασισμένων σε γραφήματα που μπορούν να αξιοποιηθούν για την αύξηση δεδομένων, την εκπαίδευση πολλαπλών αναφορών και την αξιολόγηση μη ντετερμινιστικών παραγόντων. Το παράγει καινοτόμες διαδρομές διαλόγου για να αυξήσει τον όγκο και την ποικιλομορφία δεδομένων. Η εσωτερική και εξωτερική αξιολόγηση σε τρία σύνολα δεδομένων δείχνει ότι η αύξηση δεδομένων ή/και η εκπαίδευση πολλαπλών αναφορών με ConvGraph μπορεί να βελτιώσει τα ποσοστά επιτυχίας διαλόγου μέχρι 6.4%.', 'hu': 'A feladatorientált párbeszédrendszerek jellemzően nagy mennyiségű, kiváló minőségű képzési adatokra támaszkodnak, vagy bonyolult kézműves szabályokat igényelnek. A meglévő adatkészletek azonban gyakran korlátozott méretűek, tekintettel a párbeszédek összetettségére. Ezenkívül a hagyományos edzési jel in- ferencia nem alkalmas nem determinisztikus ügynök viselkedésére, nevezetesen több műveletet tekintve ugyanazon párbeszédállapotokban érvényesnek. Javasoljuk a Conversation Graph (ConvGraph) párbeszédek grafikon alapú ábrázolását, amely adatbővítésre, többreferencia képzésre és nem determinisztikus szereplők értékelésére használható. A ConvGraph új párbeszédutakat hoz létre az adatok mennyiségének és sokszínűségének növelésére. A három adatkészleten végzett belső és külső értékelés azt mutatja, hogy a ConvGraph segítségével végzett adatbővítés és/vagy többreferencia-képzés akár 6,4%-kal is javíthatja a párbeszédek sikerességét.', 'it': "I sistemi di dialogo orientati alle attività di solito si basano su grandi quantità di dati formativi di alta qualità o richiedono complesse regole artigianali. Tuttavia, i set di dati esistenti sono spesso limitati in termini di dimensioni, tenuto conto della complessità dei dialoghi. Inoltre, il segnale di addestramento convenzionale non è adatto al comportamento non deterministico dell'agente, vale a dire considerare azioni multiple come valide in stati di dialogo identici. Proponiamo il Conversation Graph (ConvGraph), una rappresentazione grafica di dialoghi utilizzabili per l'aumento dei dati, la formazione multi-riferimento e la valutazione di agenti non deterministici. ConvGraph genera nuovi percorsi di dialogo per aumentare il volume e la diversità dei dati. La valutazione intrinseca ed estrinseca su tre set di dati mostra che l'aumento dei dati e/o la formazione multi-riferimento con ConvGraph possono migliorare i tassi di successo dei dialoghi fino al 6,4%.", 'kk': 'Тапсырмалар бағытталған диалог жүйелері кәдімгі сапатты оқыту деректерінің үлкен сандарына тәуелді немесе комплексті қолдану ережелерін талап етеді. Бірақ бар деректер жиындары диалогтардың көпшілігін бағыттау үшін шектелген болады. Сонымен қатар, кәдімгі оқыту сигналы бір диалог күйінде бірнеше әрекеттерді дұрыс деп қарай алатын агенттердің қасиеттері үшін дефицистикалық емес. Біз мәліметтерді көбейту, көптеген сілтемелерді оқыту және дефицистикалық емес агенттер үшін қолданылатын диалогтардың графика негізінде негізделген сұлбатын график (ConvGraph) деп ұсынамыз. ConvGraph деректер үнділігін және әртүрлігін көтеру үшін жаңа диалог жолдарын құрады. Үш деректер жинағында ішкі және сыртқы оқиғалар ConvGraph деген деректерді көбейту және/немесе көбейтік сілтемелер оқиғаларының диалог сәттілігін 6,4% деп жақсартуға болады.', 'lt': 'Į užduotis orientuotos dialogo sistemos paprastai grindžiamos dideliu kokybiško mokymo duomenų kiekiu arba reikalauja sudėtingų rankinių taisyklių. Tačiau esami duomenų rinkiniai dažnai yra riboti atsižvelgiant į dialogų sudėtingumą. Be to, įprastinis mokymo signalo sutrikimas netinka nenustatytų veiksnių elgesiui, būtent, atsižvelgiant į tai, kad keli veiksmai galioja vienodomis dialogo sąlygomis. Siūlome konversacijos grafiką (ConvGraph), grafiškai pagrįstą dialogų, kurie gali būti naudojami duomenų didinimui, daugiamečiam mokymui ir nenustatyvių veikėjų vertinimui. ConvGraph sukuria naujus dialogo būdus, kuriais didinamas duomenų kiekis ir įvairovė. Intrinsic and extrinsic evaluation across three datasets shows that data augmentation and/or multi-reference training with ConvGraph can improve dialogue success rates by up to 6.4%.', 'mk': 'Системите на дијалог ориентирани на задачите обично зависат од големи количини на висококвалитетни податоци за обука или бараат комплексни рачно направени правила. Сепак, постојните датотеки честопати се ограничени со оглед на комплексноста на дијалозите. Покрај тоа, конвенционалниот сигнал на тренинг не е соодветен за однесувањето на недефинистички агент, имено, сметајќи дека повеќе акции се валидни во идентични дијалози. Предложуваме конверзациски граф (ConvGraph), графско засновано претставување на дијалози кои може да се искористат за зголемување на податоците, мултиреференциска обука и евалуација на недеterminatистичките агенти. ConvGraph генерира нови патишта за дијалог за зголемување на обемот на податоци и разновидноста. Интринска и екстринзичка проценка во трите податоци покажува дека зголемувањето на податоците и/или мултиреференциска обука со ConvGraph може да ги подобри стапките на успех на дијалогот за 6,4 отсто.', 'mn': 'Тапсыг дамжуулах диалог системүүд ихэвчлэн өндөр чанарын дасгал өгөгдлийн маш их хэмжээтэй байдаг эсвэл комплекс гар бүтээгдэхүүний хууль хэрэгтэй. Гэвч суурилсан өгөгдлийн сангууд диалогуудын цогцолтой хэмжээнд хязгаарлагддаг. Үүнээс гадна олон үйл ажиллагааг адилхан диалог хэлбэрээр зөв гэж үзэх аргагүй байдаг. Бид ярилцлагын график (ConvGraph), график дээр суурилсан диалогуудын үзүүлэлт, өгөгдлийн нэмэгдүүлэлт, олон тохиромжтой сургалтын сургалтыг, тогтоомжгүй агентуудын үнэлгээ хийх боломжтой. ConvGraph нь өгөгдлийн хэмжээ болон олон төрлийг нэмэгдүүлэхэд шинэ диалог зам үүсгэдэг. Гурван өгөгдлийн хэлбэрээр суурь болон гадна дүгнэлт нь ConvGraph-ын өгөгдлийн нэмэгдүүлэлт болон/эсвэл олон дасгал дасгал сургалт нь диалогын амжилтын хувьд 6.4%.', 'ms': 'Sistem dialog bertujuan tugas biasanya bergantung pada jumlah besar data latihan kualiti tinggi atau memerlukan peraturan yang kompleks. Namun, set data yang wujud sering terbatas dalam saiz mengingati kompleksiti dialog. Lagipun, isyarat latihan konvensional tidak sesuai untuk perilaku ejen bukan-menentukan, iaitu, mempertimbangkan tindakan berbilang sebagai sah dalam keadaan dialog yang sama. Kami cadangkan Graf Perbualan (ConvGraph), perwakilan berdasarkan graf bagi dialog yang boleh dieksploitasi untuk peningkatan data, latihan rujukan berbilang dan penilaian ejen bukan-determinasi. ConvGraph generates novel dialogue paths to augment data volume and diversity.  Evaluasi dalaman dan luar melalui tiga set data menunjukkan peningkatan data dan/atau latihan rujukan-berbilang dengan ConvGraph boleh meningkatkan kadar kesuksesan dialog sehingga 6.4%.', 'ml': 'ജോലി തിരിച്ചറിയപ്പെട്ട ഡയലോഗ് സിസ്റ്റം സാധാരണ വലിയ വിവരങ്ങളില്\u200d ആശ്രയിക്കുന്നു However, existing datasets are often limited in size con- sidering the complexity of the dialogues.  കൂടുതലായി, സാധാരണ ട്രെയിനിസ്റ്റന്\u200d സിഗ്നല്\u200d ഇന്\u200d ഫെയര്\u200dനെന്\u200dസില്\u200d പ്രവര്\u200dത്തിപ്പിക്കുന്നത് നിര്\u200dണ്ണയിക്കാത്ത ഏജന്\u200dറിന്റെ പ്രവര്\u200dത്തനങ നമ്മള്\u200d സംസാരിക്കുന്ന സംഭാഷണ ഗ്രാഫ് (കോണ്\u200dവ്ഗ്രാഫ്), ഡേറ്റാ കൂട്ടുന്നതിന് വേണ്ടിയും, പല-reference പരിശീലിപ്പിനും നിര്\u200dണ്ണയിക്കാത്ത ജെന്\u200dറുകളു കോണ്\u200dവ്ഗ്രാഫ് നോവല്\u200d ഡയലോഗ് വഴികള്\u200d ഉണ്ടാക്കുന്നു. ഡേറ്റാ വ്യത്യാസവും കൂടുതല്\u200d വ്യത്യാസവുമാണ്. മൂന്നു ഡാറ്റാസറ്റുകളില്\u200d മുഴുവന്\u200d ഇന്റ്രിനിസിക്കും വിലാസവും കാണിക്കുന്നുണ്ടെങ്കില്\u200d കോണ്\u200dവ്ഗ്രാഫിനോടൊപ്പം ഡേറ്റാവിന്റെ കൂടു', 'mt': 'Task-oriented dialogue systems typically rely on large amounts of high-quality training data or require complex handcrafted rules.  Madankollu, settijiet ta’ dejta eżistenti ta’ spiss huma limitati fid-daqs meta wieħed iqis il-kumplessità tad-djalogi. Barra minn hekk, is-sinjal konvenzjonali ta’ taħriġ in-ferenza mhuwiex adattat għall-imġiba ta’ aġent mhux determinanti, jiġifieri, meta jitqiesu diversi azzjonijiet bħala validi fi stati ta’ djalogu identiċi. Aħna nipproponu l-Grafika ta’ Konverżjoni (ConvGraph), rappreżentanza bbażata fuq il-grafika ta’ djalogi li jistgħu jiġu sfruttati għal żieda fid-dejta, taħriġ b’ħafna referenzi u evalwazzjoni ta’ a ġenti mhux determinanti. ConvGraph jiġġenera mogħdijiet ġodda ta’ djalogu biex iżid il-volum tad-dejta u d-diversità. Evalwazzjoni interna u esterna fi tliet settijiet ta’ dejta turi li ż-żieda fid-dejta u/jew taħriġ b’ħafna referenzi ma’ ConvGraph jistgħu jtejbu r-rati ta’ suċċess tad-djalogu b’sa 6.4%.', 'pl': 'Systemy dialogu zorientowanego na zadania zazwyczaj opierają się na dużych ilościach wysokiej jakości danych szkoleniowych lub wymagają złożonych ręcznie opracowanych reguł. Jednakże istniejące zbiory danych są często ograniczone pod względem złożoności dialogów. Dodatkowo konwencjonalny sygnał treningowy nie jest odpowiedni dla niedeterministycznych zachowań agentów, a mianowicie uznawania wielu działań za ważne w identycznych stanach dialogu. Proponujemy Konwersacyjny Graph (ConvGraph), reprezentację dialogów opartą na wykresie, która może być wykorzystywana do powiększania danych, szkolenia wielodniesienia i oceny czynników niedeterministycznych. ConvGraph generuje nowe ścieżki dialogowe w celu zwiększenia wolumenu i różnorodności danych. Ocena wewnętrzna i zewnętrzna w trzech zbiorach danych pokazuje, że powiększanie danych i/lub szkolenie wielu referencji za pomocą ConvGraph może poprawić wskaźnik sukcesu dialogu o nawet 6,4%.', 'ro': 'Sistemele de dialog orientate spre sarcini se bazează, de obicei, pe cantități mari de date de formare de înaltă calitate sau necesită reguli complexe realizate manual. Cu toate acestea, seturile de date existente sunt adesea limitate ca dimensiune, având în vedere complexitatea dialogurilor. În plus, semnalul convenţional de instruire nu este potrivit pentru comportamentul agentului non-determinist, şi anume, considerând acţiuni multiple ca valabile în stări identice de dialog. Vă propunem Conversation Graph (ConvGraph), o reprezentare bazată pe grafice a dialogurilor care pot fi exploatate pentru mărirea datelor, instruirea multi-referință și evaluarea agenților non-determiniști. ConvGraph generează noi căi de dialog pentru a mări volumul și diversitatea datelor. Evaluarea intrinsecă și extrinsă a trei seturi de date arată că mărirea datelor și/sau formarea cu mai multe referințe cu ConvGraph pot îmbunătăți ratele de succes ale dialogului cu până la 6,4%.', 'sr': 'Sistemi dijaloga orijentirani na zadatke obično se oslanjaju na velike količine podataka o kvalitetnoj obuci ili zahtevaju kompleksne pravila napravljene rukama. Međutim, postojeće kompleksnosti podataka često su ograničene u veličini koji se povezuje kompleksnost dijaloga. Osim toga, konvencionalni signal obuke u ferenciji nije odgovarajući za ponašanje bez određenih agenata, a to je, s obzirom na višestruke akcije koje su validne u identičnim državama dijaloga. Predlažemo grafiku razgovora (ConvGraph), zasnovanu na grafiku predstavu dijaloga koji se mogu iskoristiti za povećanje podataka, multireferentno obuku i procjenu nedefinističkih agenata. ConvGraph stvara nove puteve dijaloga do povećanja voluma podataka i raznolikosti. Unutrašnja i ekstrinska procjena u tri kompleta podataka pokazuje da povećanje podataka i/ili multireferenčna obuka sa ConvGraph može poboljšati stope uspeha dijaloga do 6,4%.', 'no': 'Oppgåveorienterte dialogsystemar brukar normalt på stor mengdar av høg kvalitetøvingsdata eller krev komplekse reglar med høg- kvalitet. Det eksisterande datasettet er ofte begrenset i storleiken ved å slå på kompleksiteten av dialogvindauga. I tillegg er det ikkje tilgjengeleg for ikkje-deterministisk agent-oppførsel, dvs. for å sjå på fleire handlingar som gyldig i identiske dialogtilstandar. Vi foreslår samtalegrafen (ConvGraph), eit grafikkbasert representasjon av dialogar som kan brukast for data augmentasjon, fleire referanser- trening og evaluering av ikkje- deterministiske agentar. ConvGraph lager nye dialogstigar til å auka datavolum og mangfold. Intrinsic and extrinsic evaluation across three datasets shows that data increase and/or multi reference training with ConvGraph can improve dialog success rates by up to 6.4%.', 'so': 'Sida caadiga ah nidaamka diyaarinta shaqada waxay ku xiran yihiin maadooyin badan oo waxbarashada tacliinta sare ama waxay u baahan yihiin qaynuunno adag oo gacmaha qabatay. However, existing datasets are often limited in size con- sidering the complexity of the dialogues.  Sidoo kale sawirka waxbarashada ee caadiga ah ee ku haboon habboon dabeecada shaqaalaha aan aqoonsiga aheyn, sida loo eego shaqooyin badan oo ay ku shaqeyn yihiin wadamada diyaarinta saxda ah. Waxaan soo jeedaynaa Graph conversations (ConvGraph), kaas oo ku qoran samooyin sawir ah, kaas oo loo isticmaali karo kordhiska macluumaadka, waxbarashada farsamada badan iyo qiimeynta shaqaalaha aan aqoonin. ConvGraph wuxuu sameeyaa waddooyin ku qoran diyaarinta saxda ah si loo kordhiyo tirada macluumaadka iyo kala duduwan. Qiimeynta caadiga iyo dibadda ah ee saddexda databaseeti ka mid ah waxay muuqataa in waxbarashada kordhinta data iyo/ama waxbarashada farsamada badan ee ConvGraf lagu baranayo uu beddeli karo heerka guulaysta bulshada badnaanta 6.4 boqolkiiba.', 'si': 'සාමාන්\u200dය විශේෂ ප්\u200dරධානය සංවාද පද්ධතිය සංවාද ප්\u200dරමාණය විශේෂ විශාල ප්\u200dරධානයක් තොරතුරු ගැන හෝ නමුත්, තියෙන්න තියෙන තොරතුරු සේට් සාමාන්\u200dය ප්\u200dරමාණය සඳහා ප්\u200dරමාණය සඳහා සංවාදයේ සංකේත සං ඉතින්, සාමාන්\u200dය ප්\u200dරශ්නය සංඥාවක් සංඥාවක් වෙනුවෙන් නිර්ධාරිත නියෝජිත ව්\u200dයාපෘතිකරණය සඳහා යුක්තිය නෑ, ම අපි කතා කරන්න ග්\u200dරාෆ් (ConvGraph) යෝජනා කරනවා, ග්\u200dරාෆ් අධාරිත සංවාදයේ සංවාදය සංවාදය සංවාදය සඳහා ප්\u200dරයෝජන වෙන්න පුළුවන් විදි Name ඉන්ධ්\u200dරින්සික් සහ ප්\u200dරශ්නයක් දත්ත සෙට් තුනක් විසින් ප්\u200dරශ්නයක් පෙන්වන්නේ ConvGraph එක්ක දත්ත විශාලනය සහ/සම්ප්\u200dරශ්නයක', 'sv': 'Uppgiftsinriktade dialogsystem är vanligtvis beroende av stora mängder högkvalitativ utbildningsdata eller kräver komplexa handgjorda regler. De befintliga datauppsättningarna är dock ofta begränsade till storlek med tanke på dialogernas komplexitet. Dessutom är konventionell träningssignal- ferens inte lämplig för icke- deterministiskt agentbeteende, nämligen att betrakta flera åtgärder som giltiga i identiska dialogtillstånd. Vi föreslår konversationsgrafen (ConvGraph), en grafbaserad representation av dialoger som kan utnyttjas för dataökning, flerreferensutbildning och utvärdering av icke-deterministiska agenter. ConvGraph genererar nya dialogvägar för att öka datavolym och mångfald. Intrinsk och extern utvärdering över tre datauppsättningar visar att dataökning och/eller flerreferensutbildning med ConvGraph kan förbättra dialogens framgång med upp till 6,4%.', 'ta': 'பணி திருத்தப்பட்ட உரையாடல் அமைப்புகள் வழக்கமாக உயர்தரமான பயிற்சி தரவை சார்ந்து கொள்ளும் அல்லது சிக்கலான கைமுறை விதிகள However, existing datasets are often limited in size con- sidering the complexity of the dialogues.  கூடுதலாக, வழக்கமான பயிற்சி குறியீடு தீர்மானிக்கப்படாத நடைமுறைக்கு பொருத்தமானது, அதாவது, பல செயல்கள் அடையாளம் உரையாடல் நாடுகளில் செல்லு நாம் உரையாடல் வரைப்படத்தை (கான்வ்கிராப்) பரிந்துரைக்கிறோம், தகவல் அதிகப்படுத்தல், பல- குறிப்பு பயிற்சி மற்றும் தீர்மானிக்காத மேலாளர்களை மதிப்பி ConvGraph புதிய உரையாடல் பாதைகளை உருவாக்குகிறது தரவு ஒலி மற்றும் பலவீதம். மூன்று தகவல் அமைப்புகளில் உள்ளுணர்வு மற்றும் வெளியீட்டு மதிப்பினை கான்வ் குறிப்பில் தரவு அதிகரிப்பு மற்றும்/அல்லது பல குறிப்பு பயிற்சி', 'ur': 'ٹاکس کی سمجھی دیالوگ سیسٹم معمولاً زیادہ گوارت ترینس ڈیٹا پر اعتماد رکھتے ہیں یا پیچیدہ ہاندکریٹ کے قانون کی ضرورت رکھتے ہیں. However, existing datasets are often limited in size con- sidering the complexity of the dialogues. اور اضافہ، منطقی ترینسی سیگنالہ، فرنس میں غیر منطقی اگنٹ رفتار کے لئے مناسب نہیں ہے، یعنی، بہت سی اقدامات کو ایک ہی سیگنالہ کی حالت میں مفید سمجھتے ہیں. ہم رابطہ گراف (ConvGraph) کو پیشنهاد کریں گے، ایک گراف بنیاد دار ڈالیلوگوں کی تصویر جو ڈاٹا اضافہ کرنے کے لئے استعمال کر سکتے ہیں، بہت سی رابطہ تطارین اور غیر منطقی اگنٹوں کا ارزش کرنا ہے. ConvGraph produces new dialog paths to increase data volume and diversity. تین ڈاٹ سٹ میں داخلی اور خارجی ارزیابی دکھاتی ہے کہ ConvGraph کے ساتھ ڈاٹ افزایش اور/یا بہت سی ارتباط تطالب کی سفارش 6.4%.', 'uz': "Name Faqat, mavjud maʼlumot tugmalari dialogning murakkablarini oʻlchamiga oʻzgartiriladi. @ info: whatsthis We propose the Conversation Graph (ConvGraph), a graph-based representation of dialogues that can be exploited for data augmentation, multi- reference training and evaluation of non- deterministic agents.  Name Uchta maʼlumotlar tarkibida qiymatni ko'rsatish mumkin, KonvGraph bilan muloqat taʼminlovchi maʼlumot soʻzlashtirish va/yoki muloqat bogʻlanish imkoniyatini 6.4 foizlik darajasini oshirish mumkin.", 'vi': 'Các hệ thống đối thoại chuyên nghiệp thường dựa trên một lượng lớn dữ liệu đào tạo chất lượng cao hoặc đòi hỏi quy định kỹ thuật phức tạp. Tuy nhiên, các tập tin tồn tại thường bị hạn chế về kích thước cân nhắc sự phức tạp của các ca- trình. Hơn nữa, tín hiệu huấn luyện thông thường không phù hợp với hành vi đặc vụ không phân biệt động, cụ thể là xem nhiều hành động hợp lệ trong các bang giống nhau. Chúng tôi đề xuất đồ thị cho cấu hình đối thoại (Convhraph), một mô tả biểu tượng biểu đồ của các ca được khai thác dữ liệu, huấn luyện đa tham khảo và đánh giá các đặc vụ không phân biệt. Biểu đồ hình sự tạo ra những đường dẫn mới để tăng lượng dữ liệu và đa dạng. Sự nghiên cứu kỹ lưỡng và dứt khoát trong ba tập tin cho thấy khả năng gia tăng dữ liệu và/hay đào tạo đa ngành tham khảo có thể cải tiến tỉ lệ thành công của cuộc đối thoại', 'nl': 'Taakgerichte dialoogsystemen zijn meestal afhankelijk van grote hoeveelheden trainingsgegevens van hoge kwaliteit of vereisen complexe handgemaakte regels. Bestaande datasets zijn echter vaak beperkt in omvang gezien de complexiteit van de dialogen. Bovendien is conventionele trainingssignaalinterferentie niet geschikt voor niet-deterministisch gedrag van agenten, namelijk het beschouwen van meerdere acties als geldig in identieke dialoogstaten. We stellen de Conversation Graph (ConvGraph) voor, een grafiekgebaseerde weergave van dialogen die kan worden gebruikt voor data augmentatie, multi-reference training en evaluatie van niet-deterministische agenten. ConvGraph genereert nieuwe dialoogpaden om datavolume en diversiteit te vergroten. Intrinsieke en extrinsieke evaluatie over drie datasets toont aan dat gegevensvergroting en/of multi-reference training met ConvGraph de succespercentages van dialoog met maximaal 6,4%.', 'hr': 'Sistemi dijaloga orijentirani na zadatke obično se oslanjaju na velike količine podataka o visokokvalitetnoj obuci ili zahtijevaju kompleksne pravila napravljene ruke. Međutim, postojeće sete podataka često su ograničene u veličini s kojima se pokazuje kompleksnost dijaloga. Osim toga, konvencionalni znak obuke u ferenciji nije odgovarajući za ponašanje bez određenih agenata, a to je, s obzirom na višestruke akcije koje su vrijedne u identičnim dijalogskim državama. Predlažemo grafiku razgovora (ConvGraph), zasnovanu na grafiku predstavu dijaloga koje se mogu iskoristiti za povećanje podataka, multireferentni obuku i procjenu nedodređujućih agenata. ConvGraph stvara nove puteve dijaloga do povećanja volumena podataka i raznolikosti. Intrinsicna i ekstrinsicna procjena u tri podataka pokazuje da povećanje podataka i/ili višereferentni trening s ConvGraph može poboljšati stope uspjeha dijaloga do 6,4%.', 'bg': 'Системите за диалог, ориентирани към задачите, обикновено разчитат на големи количества висококачествени данни за обучение или изискват сложни ръчно изработени правила. Съществуващите набори от данни обаче често са ограничени по размер предвид сложността на диалоговите прозорци. Освен това конвенционалният тренировъчен сигнал не е подходящ за недетерминистично поведение на агента, а именно, разглеждане на множество действия като валидни в идентични състояния на диалог. Предлагаме диалогова графика, базирана на графика, която може да бъде експлоатирана за увеличаване на данните, мултиреферентно обучение и оценка на недетерминистични агенти. Генерира нови диалогови пътища за увеличаване на обема и разнообразието на данните. Вътрешната и външната оценка на три набора данни показва, че увеличаването на данните и/или многореферентното обучение с могат да подобрят процента на успех на диалога с до 6,4%.', 'da': 'Opgaveorienterede dialogsystemer er typisk afhængige af store mængder træningsdata af høj kvalitet eller kræver komplekse håndlavede regler. De eksisterende datasæt er dog ofte begrænset i størrelse i betragtning af dialogernes kompleksitet. Derudover er konventionelle træningssignaler ikke egnede til ikke-deterministisk agentadfærd, nemlig at betragte flere handlinger som gyldige i identiske dialogtilstande. Vi foreslår Conversation Graph (ConvGraph), en grafbaseret repræsentation af dialoger, der kan udnyttes til dataforøgelse, multi-reference træning og evaluering af ikke-deterministiske agenter. ConvGraph genererer nye dialogstier til at øge datavolumen og mangfoldighed. Indgående og ekstern evaluering på tværs af tre datasæt viser, at dataforøgelse og/eller multi-reference træning med ConvGraph kan forbedre dialogens succesrater med op til 6,4%.', 'de': 'Aufgabenorientierte Dialogsysteme basieren typischerweise auf großen Mengen hochwertiger Trainingsdaten oder erfordern komplexe, handgefertigte Regeln. Allerdings sind vorhandene Datensätze aufgrund der Komplexität der Dialoge häufig in ihrer Größe begrenzt. Darüber hinaus eignet sich herkömmliche Trainingssignal-Intervention nicht für nicht-deterministisches Agentenverhalten, d.h. multiple Aktionen als gültig in identischen Dialogzuständen zu betrachten. Wir schlagen das Conversation Graph (ConvGraph) vor, eine graphenbasierte Darstellung von Dialogen, die für die Datenaufwandlung, das Training mit mehreren Referenzen und die Bewertung von nicht deterministischen Agenten genutzt werden kann. ConvGraph generiert neuartige Dialogpfade, um das Datenvolumen und die Vielfalt zu erhöhen. Intrinsische und extrinsische Auswertungen über drei Datensätze zeigen, dass die Datenerweiterung und/oder das Multireferenztraining mit ConvGraph die Dialogerfolgsraten um bis zu 6,4%.', 'fa': 'سیستم\u200cهای محاورۀ مشاوره\u200cای که به دنبال وظیفه\u200cها می\u200cباشند معمولاً بر مقدار زیادی از داده\u200cهای آموزش کیفیت بالا استفاده می\u200cکنند یا نیاز به قانون\u200cهای پیچیده\u200cای دارند. با این حال، مجموعه\u200cهای داده\u200cهای موجود اغلب در اندازه\u200cای محدود می\u200cشوند که پیچیدگی\u200cهای گفتگوها را محدود می\u200cکنند. علاوه بر این، سیگنال آموزشی معمولی در تفاوت برای رفتار مامور غیر تصمیم\u200cگیری مناسب نیست، یعنی با توجه به فعالیت\u200cهای تعدادی که در وضعیت\u200cهای محاورۀ یکسان استفاده می\u200cکنند. ما پیشنهاد می\u200cکنیم گراف گفتگوی (ConvGraph) یک نمایش بنیاد گراف از گفتگوهای که می\u200cتوانند برای افزایش داده\u200cها استفاده شود، آموزش متعدد مربوط و ارزیابی ماموران غیر تصمیم\u200cکننده استفاده شود. Name ارزیابی داخلی و خارجی در سه مجموعه اطلاعات نشان می دهد که افزایش داده ها و/یا آموزش متعدد مربوط با ConvGraph می تواند سرعت موفقیت دیalog را تا 6.4 درصد بهتر کند.', 'ko': '임무를 위한 대화 시스템은 대량의 고품질 교육 데이터에 의존하거나 복잡한 수동 규칙이 필요하다.그러나 대화의 복잡성을 감안하면 기존 데이터 집합의 크기는 한계가 있다.그 밖에 전통적인 훈련 신호 인용은 비확정적인 에이전트 행위에 적용되지 않는다. 즉, 여러 동작이 같은 대화 상태에서 효과적이라고 생각하는 것이다.우리는 대화도(ConvGraph)를 제시했는데 이것은 그림을 바탕으로 하는 대화로 데이터 확장, 다중 참고 훈련과 비확정적 대리에 대한 평가에 사용할 수 있음을 나타낸다.ConvGraph는 데이터 양과 다양성을 높이기 위해 새로운 대화 경로를 생성합니다.세 개의 데이터 집합에 대한 내재와 외재 평가에 따르면 콘브그라프를 이용하여 데이터 확충과/또는 다중 참고 훈련을 하면 대화 성공률을 6.4% 높일 수 있다.', 'sw': 'Mfumo wa mazungumzo yenye kazi hutegemea kwa kawaida kiasi kikubwa cha data za mafunzo yenye kiwango cha juu au unahitaji kanuni tata zilizo mikononi. Hata hivyo, seti za taarifa zinazopo mara nyingi zinazuiwa kwa kiwango kikubwa, kwa kuzingatia utata wa mazungumzo. Kwa kuongezea, alama ya mafunzo ya kawaida katika kipindi hiki si sahihi kwa tabia za wateja wasio na uthibitisho, yaani, ikichukua hatua mbalimbali kama ilivyo sahihi katika mazungumzo yanayofanana. Tunazipendekeza Graph ya Mazungumzo (ConvGraph), mwakilishi wa mazungumzo yenye picha ambayo yanaweza kutumiwa kwa ajili ya kuongeza data, mafunzo ya maoni mengi na tathmini ya maafisa wasio na uthibitisho. ConvGraph inatengeneza njia za mazungumzo ya kitaifa ili kuongeza ujazo wa data na utofauti. Utafiti wa msingi na wa nje katika seti tatu za takwimu zinaonyesha kuwa mafunzo ya kuongeza data na/au mafunzo ya maoni mengi na ConvGraph yanaweza kuongeza kiwango cha mafanikio ya mazungumzo kwa takribani asilimia 6.4.', 'af': 'Opdrag-orienteerde dialoog stelsels tipies vertrou op groot hoë-kwaliteit-onderwerking data of benodig kompleks handkryfde reëls. Maar bestaande datastelle word dikwels beperk in grootte con- sidering die kompleksiteit van die dialoog. Additionally, conventional training signal in- ference is not suitable for non- deterministic agent behavior, namely, considering multiple actions as valid in identical dialog states. Ons voorstel die Gespraak Graf (ConvGraph), â\x80\x99n graafgebaseerde verteenwoordigheid van dialoog wat kan uitgebruik word vir data augmentasie, multi- verwysing onderwerking en evaluering van nie- deterministiese agente. ConvGraph genereer nuwe dialoog pad na vergroot data volume en diversiteit. Intrinsic en extrinsic evaluasie oor drie datastelle vertoon dat data augmentasie en/of multi-verwysing onderwerking met ConvGraph die dialoog sukseshoeveelheid van tot 6.4%.', 'tr': "Görevlerden berilen dijalog sistemalary adatça ýokary kwalitet eğitim maglumatlaryna ynanýarlar ýa-da karmaşık elimden gelen kararlar gerek. Ýöne bolan veri düzümleri köplenç diýip çarpylýar. Beýleki, diňe-de, adil okuwçy sened- ferenziýa takyksyzlyk etmek üçin ýeterli däldir. Birnäçe hereketler birnäçe dijalog durumlarda geçerli bir hereket diýip pikir edýär. Biz Conversation Graph'u (ConvGraph) ConvGraf maglumaty we çeşitliklerini azaltmak üçin täze dynç yollary döredir. Çaltylyk we çäri çykyş 3 datasetlerde maglumat ýetişdirmegi we/ýa ConvGraph bilen çoklu referens okuwçylygy barlamak üçin 6.4% ýagdaýa çenli dijalogyň üstünlik häzirini gowurap biler.", 'sq': 'Sistemet e dialogut të orientuar ndaj detyrave zakonisht mbështeten në sasi të mëdha të dhënash të trajnimit të cilësisë së lartë apo kërkojnë rregulla komplekse të ndërtuar me dorë. Megjithatë, grupet e të dhënave ekzistuese janë shpesh të kufizuara në madhësinë duke marrë parasysh kompleksitetin e dialogeve. Përveç kësaj, sinjali konvencional i trajnimit në diferencë nuk është i përshtatshëm për sjelljen e agjentit jo-përcaktues, veçanërisht, duke konsideruar veprime të shumta si të vlefshme në shtetet e dialogut identik. Ne propozojmë grafikun e konversimit (ConvGraph), një përfaqësim i dialogut me bazë në grafik që mund të shfrytëzohet për rritjen e të dhënave, trajnimin e shumëreferencave dhe vlerësimin e agjentëve jo-vendosës. ConvGraph gjeneron rrugë të reja dialogu për të rritur volumin e të dhënave dhe diversitetin. Vlerësimi i brendshëm dhe i jashtëm nëpërmjet tre grupeve të dhënash tregon se rritja e të dhënave dhe/ose trajnimi me shumë referenca me ConvGraph mund të përmirësojë normat e suksesit të dialogut me deri në 6.4%.', 'id': 'Sistem dialog oriented tugas biasanya bergantung pada jumlah besar data pelatihan kualitas tinggi atau memerlukan peraturan yang kompleks. Namun, set data yang ada sering terbatas dalam ukuran mengingat kompleksitas dialog. Selain itu, sinyal pelatihan konvensional di-ference tidak cocok untuk perilaku agen non-deterministic, yaitu, mempertimbangkan beberapa tindakan sebagai valid dalam keadaan dialog yang sama. Kami mengusulkan Graf Conversation (ConvGraph), sebuah representation berdasarkan grafik dari dialog yang dapat dieksploitasi untuk peningkatan data, pelatihan multi-referensi dan evaluasi agen non-deterministic. ConvGraph menghasilkan laluan dialog baru untuk meningkatkan volum dan diversitas data. Intrinsic and extrinsic evaluation across three datasets shows that data augmentation and/or multi-reference training with ConvGraph can improve dialogue success rates by up to 6.4%.', 'hy': 'Պատասխաններով հիմնված հաղորդակցման համակարգերը սովորաբար հիմնված են բարձր որակի ուսումնասիրության տվյալների մեծ քանակությամբ կամ պահանջում են բարդ ձեռագործված կանոններ: Այնուամենայնիվ, գոյություն ունեցող տվյալների համակարգերը հաճախ սահմանափակված են, հաշվի առնելով պատկերացումների բարդությունը: Ավելին, ավանդական ուսումնասիրության ազդանշանները չեն համապատասխանում ոչ որոշող գործոնների վարքագիծին, հատկապես, հաշվի առնելով բազմաթիվ գործողությունները որպես ճիշտ նույն երկրներում։ Մենք առաջարկում ենք հաղորդակցման գրաֆը (հաղորդակցման գրաֆը), գրաֆի հիմնված պատկերացում, որը կարող է օգտագործվել տվյալների բարձրացման, բազմահամեմատական ուսումնասիրության և ոչ որոշող գործոնների գնահատման համար: Կանվագրաֆը ստեղծում է նոր միջոցներ, որոնք օգնում են աճել տվյալների ծավալը և բազմազանությունը: Ինտրինսիկ և արտաքին գնահատումը երեք տվյալների համակարգերում ցույց է տալիս, որ տվյալների բարձրացման և (կամ) բազմահամեմատական վարժեցման համակարգով Կանֆրաֆ-ի միջոցով կարող է բարելավել հաջողության ցուցանիշները մինչև 6.', 'am': 'ቦታ However, existing datasets are often limited in size con- sidering the complexity of the dialogues.  በተጨማሪም፣ የግንኙነት ማጠቃለያ በ-ፋርስ - ለጥያቄ አካባቢ ድርጊት አይጠቅምም፣ ብዙዎች ተግባር እንደ ጥያቄ በማስተካከል በአካባቢ አካባቢ አካሄድ ጥያቄ በመስጠት አይገባም፡፡ ለዳታ ማጠቃለያ፣ ለብዙ መልዕክት ማጠቃለያ እና የማይታወቀው ተሟጋቾች ማሰናከል የሚችሉትን አካባቢ አካባቢ አካሄል እና ማስተምር እና ማሳየት የሚችሉትን የግንኙነቶችን መክፈት እናሳስባታለን፡፡ ConvGraph አዲስ ዶሴ ፍጠር በሦስት ዳታ ማውጣት ላይ የኢንተርኔክና የውጭ ማስታወቂያ ከConvGraph ጋር የዳታ ማጨመር እና/ወይም በብዙ የሥልጣን ትምህርት ማድረግ 6.4 በመቶ ማድረግ ማሻሻል ይችላል፡፡', 'az': 'Gözmək tərəfindən təhsil edilən dijalog sistemləri genellikle yüksək kaliteli təhsil məlumatlarının çoxluğuna təvəkkül edir və ya kompleks əl yaratdığı qaydalara ehtiyac edir. Halbuki, mevcut verilən qurğular dialogların qarışıqlığını dəyişdirirlər. Üstəlik, fərenci müxtəlif təhsil sinyali təhsil etməyə uyğun deyil. Bu da, çoxlu eyni danışma durumlarında çoxlu eyni olaraq təhsil etmək üçün uyğun deyil. Biz məlumatları artırmaq üçün istifadə edilə biləcək məlumatların grafik-tabanlı rəsmini, çoxlu-referens təhsil və deterministik olmayan ajanların değerlendirməsini təklif edirik. ConvGraph məlumatların səviyyəsini və çeşitliyini artırmaq üçün yeni dialoq yollarını yaradır. Üç verilən qurğuların içindəki və ekstrinsic değerlendirmələri ConvGraph ilə verilən məlumatlar artırması və/və çox-referens təhsil təhsil etməsi 6,4%-ə qədər müvəffəqiyyəti düzəltə bilər.', 'bs': 'Sistemi dijaloga orijentirani na zadatke obično se oslanjaju na velike količine podataka o kvalitetnoj obuci ili zahtijevaju kompleksne pravila napravljene ruke. Međutim, postojeće kompleksnosti podataka često su ograničene u veličini koji se navodi na kompleksnost dijaloga. Osim toga, konvencionalni signal obuke u ferenciji nije odgovarajući za ponašanje bez određenih agenata, a to je, s obzirom na višestruke akcije koje su vrijedne u identičnim državama dijaloga. Predlažemo grafiku konverzacije (ConvGraph), grafičku zastupnicu dijaloga koji se mogu iskoristiti za povećanje podataka, multireferentni obuku i procjenu nedodredističkih agenata. ConvGraph stvara nove puteve dijaloga do povećanja voluma podataka i raznolikosti. Intrinsicna i ekstrinsicna procjena u tri podataka pokazuje da povećanje podataka i/ili multireferenčna obuka s ConvGraph može poboljšati stope uspjeha dijaloga do 6,4%.', 'et': 'Ülesannetele orienteeritud dialoogisüsteemid tuginevad tavaliselt suurele hulgale kvaliteetseid koolitusandmeid või nõuavad keerukaid käsitööeeskirju. Kuid olemasolevad andmekogumid on dialoogide keerukust arvestades sageli piiratud. Lisaks sellele ei sobi tavapärane treeningsignaal mittedeterministliku agendi käitumiseks, nimelt arvestades mitmeid toiminguid kehtivaks identsetes dialoogiseisundites. Pakume välja vestlusgraafiku (ConvGraph), mis on dialoogide graafiline esitus, mida saab kasutada andmete suurendamiseks, mitmeviiteliseks koolituseks ja mittedeterministlike tegurite hindamiseks. ConvGraph loob uudseid dialoogiviise andmete mahu ja mitmekesisuse suurendamiseks. Kolme andmekogumi sise- ja välishindamine näitab, et andmete suurendamine ja/või mitmevõrdluskoolitus ConvGraphiga võib parandada dialoogi edukust kuni 6,4%.', 'bn': 'Task-oriented dialogue systems typically rely on large amounts of high-quality training data or require complex handcrafted rules.  তবে বিদ্যমান ডাটাসেট প্রায়শই মাপ কন- পাশে সীমাবদ্ধ করা হয় ডায়ালগের জটিলতা। এছাড়াও, সাধারণ প্রশিক্ষণের সিগন্যাল ইন- ফেয়ারেন্স নির্ধারিত এজেন্টের আচরণের জন্য যথাযথ নয়, যেমন একই সাথে ডায়ালগের রাস্তায় বৈধ বিভি আমরা কথোপকথন গ্রাফ (কনভ্রাফ) প্রস্তাব করছি, একটি গ্রাফ ভিত্তিক আলোচনার প্রতিনিধিত্ব, যা ডাটা যোগাযোগের জন্য ব্যবহার করা যাবে, বহুল-referenced প্রশিক্ষণ এবং অন কনভ- গ্রাফ তথ্যের ভলিউম এবং বৈচিত্র্যের জন্য নভেল ডায়ালগের পাথ তৈরি করেছেন। তিনটি ডাটাসেটের মধ্যে ইন্ট্রিনিসিক এবং বিদেশী মূল্যায়ন দেখাচ্ছে যে কনভ্রাফের সাথে তথ্য বৃদ্ধি এবং/অথবা বহুগুরুত্বপূর্ণ প্রশিক্ষণের ম', 'fi': 'Tehtävälähtöiset vuoropuhelujärjestelmät perustuvat yleensä suuriin määriin korkealaatuista koulutustietoa tai vaativat monimutkaisia käsityönä tehtyjä sääntöjä. Nykyiset tietoaineistot ovat kuitenkin usein kooltaan rajallisia dialogien monimutkaisuuden vuoksi. Lisäksi perinteinen harjoitussignaali ei sovellu ei-deterministiseen agenttien käyttäytymiseen, nimittäin useiden toimien pitämiseen pätevinä identtisissä dialogitiloissa. Ehdotamme Conversation Graphia (ConvGraph), graafista dialogien esitystä, jota voidaan hyödyntää tiedon lisäämisessä, moniviitekoulutuksessa ja ei-determinististen tekijöiden arvioinnissa. ConvGraph luo uusia vuoropuhelupolkuja datan määrän ja monimuotoisuuden lisäämiseksi. Kolmen aineiston sisäinen ja ulkoinen arviointi osoittaa, että datan lisääminen ja/tai monireferenssikoulutus ConvGraphilla voivat parantaa dialogin onnistumisastetta jopa 6,4%.', 'ca': "Task-oriented dialogue systems typically rely on large amounts of high-quality training data or require complex handcrafted rules.  However, existing datasets are often limited in size con- sidering the complexity of the dialogues.  A més, la senyal d'entrenament convencional no és adequada per al comportament de l'agent no determinant, és a dir, considerant múltiples accions válides en estats de diàleg idèntics. We propose the Conversation Graph (ConvGraph), a graph-based representation of dialogues that can be exploited for data augmentation, multi- reference training and evaluation of non- deterministic agents.  ConvGraph genera noves víes de diàleg per augmentar el volum de dades i la diversitat. L'evaluació intrínsica i extrínsica en tres conjunts de dades mostra que l'augment de les dades i/o l'entrenament multireferència amb ConvGraph poden millorar els nivells d'èxit del diàleg fins al 6,4%.", 'cs': 'Dialogové systémy orientované na úkoly obvykle spoléhají na velké množství vysoce kvalitních tréninkových dat nebo vyžadují složitá ručně vytvořená pravidla. Stávající datové sady jsou však často omezeny velikostí s ohledem na složitost dialogů. Navíc konvenční tréninkový signál není vhodný pro nederministické chování agentů, konkrétně považovat více akcí za platné ve stejných dialogových stavech. Navrhujeme konverzační graf (ConvGraph), grafové zobrazení dialogů, které lze využít pro rozšíření dat, multireferenční školení a hodnocení nederministických agentů. ConvGraph generuje nové dialogové cesty ke zvýšení objemu dat a rozmanitosti. Intrinsické a extrinzické vyhodnocení napříč třemi datovými sadami ukazuje, že rozšíření dat a/nebo vícereferenční trénink s ConvGraphem může zlepšit míru úspěšnosti dialogu až o 6,4%.', 'he': 'מערכות דיאלוג ממוקדות למשימות תלויות בדרך כלל על כמויות גדולות של נתונים אימונים באיכות גבוהה או דורשות חוקים מורכבים. עם זאת, קבוצות נתונים קיימות לעתים קרובות מוגבלות בגודל בהתחשב במרכיבות הדיולוגים. בנוסף, סימן אימון קונבנציונלי בתנועה אינו מתאים להתנהגות של סוכנים לא קבועים, כלומר, בהתחשב בפעולות רבות בתנאים של דיאלוג זהים. אנו מציעים את גרף השיחה (ConvGraph), מייצג מבוסס על גרף של דיאלוגים שיכולים לנצל עבור גידול נתונים, אימונים רבים-התייחסות והעריכה של סוכנים לא קבועים. Name הערכה פנימית וחיצונית בשלושה קבוצות נתונים מראה שגידול נתונים ו/או אימון רבי-התייחסות עם ConvGraph יכול לשפר את שיעורי ההצלחה של דיאלוג עד 6.4%.', 'ha': "Ana ƙayyade tsarin zauren akwatin aiki da aka juyi wa zauren akwatin bayanin akwatin bayani na bayani na ɗabi'a, yana dõgara ko kuma yana daidai da kiman ƙidãya masu tsari na nau'in-nau'i ko kuma ana ƙayyade Rubuwan da hannayen da aka samu da Amma, ana ƙayyade daidaita maɓallin da ke jira a ƙayyade girma-bayanin zauren akwatin bayanin zauren akwatin bayani. Ƙaramiwa, Simbolin mafarinta na ɗabi'a cikin-fence bai da daidai wa aikin mai zartar da shi ba na-daki, misali, idan an yi tunãni ga masu yawa kamar aikin aiki masu inganci cikin zauren zauren akwatin bayanin zauren akwatin bayani. Munã buɗa karatun mazaɓa na ConvGraf (ConvGraf), wani masana na ƙayyade zauren zauren akwatin bayani wanda za'a iya amfani da wa ɗa ɗaɗarwa ga data, wa'adin multi-Reference da evaluation ga masu tsari na-daki. ConvGraf na ƙãga hanyõyi na zauren zauren akwatin bayani na yanzu don a ƙara saurin danne da baƙaƙƙe. Ana ƙari na Intrinci da bakin bayani na tsari uku, yana nuna cewa, shirin ƙaramako na data da/ko shirin multi-Reference na ConvGraf yana iya ƙara fassarar mafaniki na zauren zauren akwatin bayani da up to 6.4%.", 'bo': 'Task-oriented dialog systems typically rely on large amounts of high-quality training data or require complex handcrafted rules. ཡིན་ནའང་། གནས་ཡུལ་སྒྲིག་ཆ་འཕྲིན་གྱི་ཆེ་ཆུང་ལ་སྒྲིག་ཡིག་ཆའི་གླེང་སྒྲོམ་གྱི་དཀའ་ངལ་གཏོང་བ Additionally, conventional training signal in- ference is not suitable for non-deterministic agent behavior, namely, considering multiple actions as valid in identical dialog states. We propose the Conversation Graph (ConvGraph), a graph-based representation of dialogues that can be exploited for data augmentation, multi-reference training and evaluation of non-deterministic agents. ConvGraph་ཡིས་སྐྱེས་ཆ་དང་སྣེ་ཚོགས་ཆེ་ཆུང་བར་གཏོང་བའི་གསར་བ་གླེང་སྒྲོམ་བརྡ་གཏོང་བ Intrinsic and extrinsic evaluation across three datasets shows that data augmentation and/or multi-reference training with ConvGraph can improve dialog success rates by up to 6.4%.', 'sk': 'Sistemi dialoga, usmerjeni v naloge, običajno temeljijo na velikih količinah visokokakovostnih podatkov o usposabljanju ali zahtevajo zapletena ročno izdelana pravila. Vendar pa so obstoječi nabori podatkov pogosto omejeni glede na zapletenost pogovorov. Poleg tega običajni signal usposabljanja ni primeren za nedeterministično vedenje agentov, namreč glede večkratnih dejanj kot veljavnih v identičnih dialognih stanjih. Predlagamo Conversation Graph (ConvGraph), grafično predstavitev dialogov, ki ga je mogoče izkoristiti za povečanje podatkov, večreferenčno usposabljanje in vrednotenje nedeterminističnih dejavnikov. ConvGraph ustvarja nove poti dialoga za povečanje obsega in raznolikosti podatkov. Notranja in zunanja ocena v treh naborih podatkov kaže, da lahko povečanje podatkov in/ali večreferenčno usposabljanje s programom ConvGraph izboljša stopnjo uspešnosti dialoga za do 6,4%.', 'jv': 'dialog-direction slot politenessoffpolite"), and when there is a change ("assertivepoliteness Awak dhéwé nggunakake conversations Graph (conv Graph), gambar dipunangé diagram sing basa gambar ditambah sing bisa jewis nggawe nggawe data ogwektualangan, aku multi-reference curuwing karo cara-jewis antar uwong. (c) Language'}
{'en': 'Efficient Content-Based Sparse Attention with Routing Transformers', 'fr': 'Attention limitée basée sur le contenu efficace grâce aux transformateurs de routage', 'es': 'Atención dispersa basada en contenido eficiente con transformadores de enrutamiento', 'ja': 'ルーティング変圧器を使用した効率的なコンテンツベースのスパースアテンション', 'ar': 'الانتباه الفعال المستند إلى المحتوى مع محولات التوجيه', 'pt': 'Atenção esparsa baseada em conteúdo eficiente com transformadores de roteamento', 'zh': '路由转换器成,其高效疏也', 'hi': 'रूटिंग ट्रांसफॉर्मर के साथ कुशल सामग्री-आधारित विरल ध्यान', 'ru': 'Эффективное редкое внимание на основе контента при использовании маршрутизирующих трансформаторов', 'ga': 'Aird Éifeachtach Ábhar-Bhunaithe Gann le Claochladáin Ródú', 'hu': 'Hatékony tartalom alapú ritka figyelem az útválasztó transzformátorokkal', 'el': 'Αποτελεσματική βασισμένη στο περιεχόμενο Σπάνια προσοχή με μετασχηματιστές δρομολόγησης', 'ka': 'Name', 'mk': 'Ефикасен контент- базиран мал внимание со трансформери за патување', 'kk': 'Мазмұны түрлендірушілердің мазмұны негіздеген кеңістік назары', 'it': 'Attenzione sparsa basata sui contenuti con i trasformatori di routing', 'ml': 'റോട്ടിങ്ങ് ട്രാന്\u200dസ്ഫോര്\u200dമാരുമായി ഉപയോഗിക്കുന്ന ഉള്ളടക്കത്തിന്റെ അടിസ്ഥാനമായ സ്പെയിസ്', 'lt': 'Veiksmingas turiniu pagrįstas mažas dėmesys maršruto keitikliams', 'no': 'Effektiv innhaldsbasert mellomrommerking med ruteringstransformeringar', 'pl': 'Wydajne oparte na treści oszczędna uwaga dzięki transformatorom routing', 'mt': 'Attenzjoni baxxa bbażata fuq il-Kontenut Effiċjenti mat-Trasformaturi tar-Rotot', 'ro': 'Atenție redusă, bazată pe conținut, cu transformatoarele de rutare', 'ms': 'Perhatian Kurang Berasas Kandungan Efisien dengan Penukar Jalur', 'si': 'Name', 'sr': 'Efektivna pažnja na temelju sadržaja na prostoriji sa transformatorima putovanja', 'so': 'Effective Content-Based Sparse Attention with Routing Transformers', 'mn': 'Маршруулах шилжүүлэгчидтэй эмзэгтэй Content-Based Sparse Attention', 'ta': 'சுழற்று மாற்றுபவர்களுடன் தேவைப்படும் உள்ளடக்கம் அடிப்படையான வினாடிகள் கவனம்', 'ur': 'Name', 'sv': 'Effektiv innehållsbaserad sparsam uppmärksamhet med Routing Transformers', 'uz': 'Name', 'vi': 'Trình độ cong nhạy bén', 'bg': 'Ефективно базирано на съдържание оскъдно внимание с пренасочващи трансформатори', 'da': 'Effektiv indholdsbaseret sparsom opmærksomhed med Routing Transformers', 'nl': 'Efficiënte content-based beperkte aandacht met routing transformers', 'hr': 'Učinjena pozornost na temelju sadržaja prostora s transformatorima putovanja', 'id': 'Perhatian kecil berdasarkan isi efisien dengan Transformer Routing', 'de': 'Effiziente inhaltsbasierte Sparte Aufmerksamkeit mit Routing Transformers', 'sw': 'Kihispania kinachotumiwa na Utafiti', 'tr': 'Taýyn Mazmunlar', 'ko': '루트 변환기를 사용하는 효율적인 내용 기반 희소 주의', 'fa': 'توجه بین\u200cاندازه\u200cهای محتوای موثر با تغییردهندگان مسیر', 'af': 'Name', 'sq': 'Vëmendje e vogël efikase bazuar në përmbajtje me transformuesit e rrugës', 'am': 'ምርጫዎች', 'hy': 'Էֆեկտիվ պարունակության հիմնված փոքր ուշադրություն ճանապարհի վերափոխողների հետ', 'bn': 'রুটিং ট্রান্সফর্মার দ্বারা কার্যকর বিষয়বস্তু ভিত্তিক স্পেরের দৃষ্টিভঙ্গি', 'bs': 'Učinjena pozornost na temelju sadržaja prostora sa transformatorima putovanja', 'ca': 'Atenció reduïda basada en continguts eficients amb transformadors de ruta', 'cs': 'Efektivní obsahově založená řídká pozornost díky směrovacím transformátorům', 'az': 'İşkil Məzmun Üstündə Tərcümə Tərcümələri ilə Etkinlik Məzmun', 'fi': 'Tehokas sisältöpohjainen vähäinen huomio reititysmuuntajien avulla', 'et': 'Tõhus sisupõhine vähe tähelepanu marsruudi transformaatoritega', 'he': 'תשומת לב קטנה על תוכן יעילה עם מעצבי מסלול', 'jv': 'Attribute', 'bo': 'ནུས་པ་ཡོད་པའི་ནང་དོན་ལ་གཞི་བརྟེན་པའི་བར་སྟོང་རྣམ་པའི་གདོང་བསྒྱུར་བཟོ་བྱེད་པ', 'ha': 'KCharselect unicode block name', 'sk': 'Učinkovita vsebinska redka pozornost z usmerjevalnimi transformatorji'}
{'en': 'Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this  complexity  focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research : It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall  complexity  of  attention  to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on  language modeling  on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits / dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1', 'es': 'Recientemente se ha adoptado la autoatención para una amplia gama de problemas de modelado de secuencias. A pesar de su eficacia, la autoatención adolece de requisitos de computación cuadrática y memoria con respecto a la longitud de la secuencia. Los enfoques exitosos para reducir esta complejidad se centraron en atender las ventanas corredizas locales o un pequeño conjunto de ubicaciones independientemente del contenido. Nuestro trabajo propone aprender patrones de atención dispersos y dinámicos que eviten asignar computación y memoria para atender contenido no relacionado con la consulta de interés. Este trabajo se basa en dos líneas de investigación: combina la flexibilidad de modelado del trabajo anterior sobre la atención dispersa basada en el contenido con las ganancias de eficiencia de los enfoques basados en la atención dispersa local y temporal. Nuestro modelo, el transformador de enrutamiento, dota a la autoatención de un módulo de enrutamiento disperso basado en k-means en línea, al tiempo que reduce la complejidad general de la atención a O (n1.5d) desde O (n2d) para la longitud de secuencia n y la dimensión oculta d. Demostramos que nuestro modelo supera a los modelos de atención dispersa comparables en el lenguaje modelado en Wikitext-103 (perplejidad de 15,8 frente a 18,3), así como en la generación de imágenes en ImageNet-64 (3,43 frente a 3,44 bits/tenue) utilizando menos capas de autoatención. Además, establecimos un nuevo estado de la técnica en el conjunto de datos PG-19 recientemente lanzado, obteniendo una perplejidad de prueba de 33,2 con un modelo de transformador de enrutamiento de 22 capas entrenado en secuencias de longitud 8192. Abrimos el código para Routing Transformer en Tensorflow.1', 'ar': 'تم اعتماد الاهتمام الذاتي مؤخرًا لمجموعة واسعة من مشاكل نمذجة التسلسل. على الرغم من فعاليتها ، إلا أن الاهتمام الذاتي يعاني من الحساب التربيعي ومتطلبات الذاكرة فيما يتعلق بطول التسلسل. ركزت الأساليب الناجحة لتقليل هذا التعقيد على الاهتمام بالنوافذ المنزلقة المحلية أو مجموعة صغيرة من المواقع المستقلة عن المحتوى. يقترح عملنا تعلم أنماط الانتباه الديناميكية المتفرقة التي تتجنب تخصيص الحساب والذاكرة لحضور محتوى غير مرتبط باستعلام الاهتمام. يعتمد هذا العمل على سطرين من البحث: فهو يجمع بين مرونة النمذجة للعمل السابق على الاهتمام المتناثر المستند إلى المحتوى مع مكاسب الكفاءة من الأساليب القائمة على الاهتمام المحلي المتناثر الزمني. نموذجنا ، محول التوجيه ، يمنح الاهتمام الذاتي بوحدة توجيه متفرقة تعتمد على وسائل k عبر الإنترنت مع تقليل التعقيد الكلي للانتباه إلى O (n1.5d) من O (n2d) لطول التسلسل n والبعد المخفي d. نوضح أن نموذجنا يتفوق في الأداء على نماذج الانتباه المتفرقة المماثلة على نمذجة اللغة على Wikitext-103 (15.8 مقابل 18.3 الحيرة) ، وكذلك في إنشاء الصور على ImageNet-64 (3.43 مقابل 3.44 بت / قاتمة) أثناء استخدام طبقات أقل للانتباه الذاتي. بالإضافة إلى ذلك ، قمنا بتعيين أحدث ما توصلت إليه التكنولوجيا على مجموعة بيانات PG-19 التي تم إصدارها حديثًا ، وحصلنا على اختبار تعقيد قدره 33.2 مع نموذج محول توجيه 22 طبقة تم تدريبه على تسلسلات بطول 8192. نحن نفتح مصدر الشفرة لتوجيه محول في Tensorflow 1', 'pt': 'A autoatenção foi recentemente adotada para uma ampla gama de problemas de modelagem de sequência. Apesar de sua eficácia, a autoatenção sofre de computação quadrática e requisitos de memória em relação ao comprimento da sequência. As abordagens bem-sucedidas para reduzir essa complexidade se concentraram em atender a janelas deslizantes locais ou a um pequeno conjunto de locais independentes do conteúdo. Nosso trabalho propõe aprender padrões dinâmicos de atenção esparsa que evitam alocar computação e memória para atender a conteúdos não relacionados à consulta de interesse. Este trabalho se baseia em duas linhas de pesquisa: combina a flexibilidade de modelagem de trabalhos anteriores sobre atenção esparsa baseada em conteúdo com os ganhos de eficiência de abordagens baseadas em atenção esparsa local e temporal. Nosso modelo, o Routing Transformer, dota a autoatenção com um módulo de roteamento esparso baseado em k-means online enquanto reduz a complexidade geral da atenção para O(n1.5d) de O(n2d) para comprimento de sequência n e dimensão oculta d. Mostramos que nosso modelo supera modelos de atenção esparsa comparáveis na modelagem de linguagem no Wikitext-103 (15,8 vs 18,3 perplexidade), bem como na geração de imagem no ImageNet-64 (3,43 vs 3,44 bits/dim) usando menos camadas de autoatenção. Além disso, definimos um novo estado da arte no conjunto de dados PG-19 recém-lançado, obtendo uma perplexidade de teste de 33,2 com um modelo de transformador de roteamento de 22 camadas treinado em sequências de comprimento 8192. Abrimos o código-fonte para Transformador de Roteamento no Tensorflow.1', 'fr': "L'auto-attention a récemment été adoptée pour un large éventail de problèmes de modélisation de séquences. Malgré son efficacité, l'attention personnelle souffre des exigences de calcul quadratique et de mémoire en ce qui concerne la longueur de la séquence. Les approches efficaces pour réduire cette complexité se sont concentrées sur l'utilisation de fenêtres coulissantes locales ou d'un petit ensemble d'emplacements indépendants du contenu. Notre travail propose d'apprendre des modèles dynamiques d'attention clairsemée qui évitent d'allouer du calcul et de la mémoire pour traiter un contenu sans rapport avec la requête d'intérêt. Ce travail s'appuie sur deux axes de recherche\xa0: il combine la flexibilité de modélisation des travaux antérieurs sur l'attention clairsemée basée sur le contenu avec les gains d'efficacité des approches basées sur une attention clairsemée locale et temporelle. Notre modèle, le Transformateur de routage, dote l'attention de soi d'un module de routage clairsemé basé sur les k-moyennes en ligne tout en réduisant la complexité globale de l'attention à O (n1.5d) à partir de O (n2d) pour la longueur de séquence n et la dimension cachée d. Nous montrons que notre modèle surpasse les modèles d'attention clairsemée comparables sur le langage modélisation sur Wikitext-103 (perplexité 15,8 vs 18,3), ainsi que sur la génération d'images sur ImageNet-64 (3,43 contre 3,44 bits/dim) tout en utilisant moins de couches d'auto-attention. De plus, nous avons établi une nouvelle technologie sur le jeu de données PG-19 récemment publié, obtenant une perplexité de test de 33,2 avec un modèle de transformateur de routage à 22 couches entraîné sur des séquences de longueur 8192. Nous avons ouvert le code pour Routing Transformer dans Tensorflow.1", 'ja': '最近では、幅広いシーケンスモデリングの問題に自己注目が採用されています。 その有効性にもかかわらず、自己注目は、配列長に関して二次計算およびメモリ要件に苦しむ。 この複雑さを軽減するための成功したアプローチは、ローカルのスライディングウィンドウまたはコンテンツに依存しない小さなロケーションのセットへの対応に焦点を当てています。 私たちの研究は、関心のあるクエリに関連しないコンテンツに対応するために計算とメモリを割り当てることを避ける動的でまばらな注意パターンを学ぶことを提案しています。 この研究は、2つの研究ラインに基づいています。コンテンツベースのまばらな注目に関する以前の研究のモデリングの柔軟性と、ローカルで時間的にまばらな注目に基づくアプローチからの効率性の向上を組み合わせています。 当社のモデルであるルーティングトランスフォーマーは、オンラインｋ平均に基づいたまばらなルーティングモジュールで自己注目を与え、同時に、シーケンス長ｎおよび隠れた次元ｄについて、Ｏ （ ｎ ２ ｄ ）からＯ （ ｎ １ ． ５ ｄ ）への注目の全体的な複雑さを軽減する。 当社のモデルは、Wikitext -103上の言語モデリング（ 15.8対18.3の複雑さ）、ならびにImageNet -64上の画像生成（ 3.43対3.44ビット/薄暗さ）で、自己注目レイヤーを少なくしながら、同等の希薄な注目モデルを上回ることを示しています。 さらに、新たにリリースされたPG -19データセットに新たな最先端を設定し、長さ8192のシーケンスでトレーニングされた22層ルーティング変圧器モデルで33.2のテスト混乱を取得しました。 Tensorflow 1でルーティングトランスのオープンソースコードを作成しました。', 'zh': '近者,自注已施于广序建模。 虽其有效,而自注于序,二算记忆。 降复杂性之道侧重于动窗独立小组位。 臣等建议学动疏注意模式,免分计内存以处兴趣者。 立于两道之上,以前之疏者建模灵活性与局也,时之疏者效率益也。 吾道由变压器,赋之于在线k均值疏路模块自关注,降自O(n2d)至于序长n隐匿维数dO(n1.5d)之复杂性。 吾言Wikitext-103(15.8困于18.3)之言建模及ImageNet-64之图成(3.433于3.44位/dim)优于疏者,兼用少自注。 又于新发 PG-19 数集上置最先进之法,施于长 8192 之序 22 层路由变压器得 33.2 之试惑度。 开源 Tensorflow 中 Routing Transformer 代码。', 'ru': 'Самовнимание недавно было принято для широкого спектра задач моделирования последовательностей. Несмотря на свою эффективность, самовнимание страдает от квадратичных вычислений и требований к памяти в отношении длины последовательности. Успешные подходы к снижению этой сложности были сосредоточены на посещении местных раздвижных окон или небольшого набора мест, не зависящих от контента. Наша работа предлагает изучать динамические скудные паттерны внимания, которые избегают выделения вычислений и памяти на контент, не связанный с интересующим запросом. Эта работа основывается на двух направлениях исследований: она сочетает гибкость моделирования предыдущей работы по содержательному редкому вниманию с повышением эффективности от подходов, основанных на локальном, временном редком внимании. Наша модель, Трансформатор маршрутизации, наделяет себя вниманием редким модулем маршрутизации на основе онлайн k-средних, одновременно уменьшая общую сложность внимания к O(n1.5d) из O(n2d) для длины последовательности n и скрытого размера d. Мы показываем, что наша модель превосходит сопоставимые модели ограниченного внимания по языковому моделированию на Викитексте-103 (15,8 против 18,3 от недоумения), а также по генерации изображений на ImageNet-64 (3,43 против 3,44 бит/дим) при меньшем количестве слоев самовнимания. Кроме того, мы установили новый современный уровень на недавно выпущенном наборе данных PG-19, получая тестовое недоумение 33,2 с 22-уровневой моделью маршрутизирующего трансформатора, обученной на последовательностях длиной 8192. Мы открываем исходный код для маршрутизации трансформатора в Tensorflow.1', 'hi': 'स्व-ध्यान हाल ही में अनुक्रम मॉडलिंग समस्याओं की एक विस्तृत श्रृंखला के लिए अपनाया गया है। इसकी प्रभावशीलता के बावजूद, आत्म-ध्यान अनुक्रम लंबाई के संबंध में द्विघात गणना और स्मृति आवश्यकताओं से ग्रस्त है। इस जटिलता को कम करने के लिए सफल दृष्टिकोण स्थानीय स्लाइडिंग खिड़कियों या सामग्री से स्वतंत्र स्थानों के एक छोटे से सेट में भाग लेने पर केंद्रित है। हमारा काम गतिशील विरल ध्यान पैटर्न सीखने का प्रस्ताव करता है जो ब्याज की क्वेरी से असंबंधित सामग्री में भाग लेने के लिए गणना और स्मृति आवंटित करने से बचते हैं। यह काम अनुसंधान की दो लाइनों पर बनाता है: यह स्थानीय, अस्थायी विरल ध्यान के आधार पर दृष्टिकोण से दक्षता लाभ के साथ सामग्री-आधारित विरल ध्यान पर पूर्व कार्य के मॉडलिंग लचीलेपन को जोड़ता है। हमारे मॉडल, रूटिंग ट्रांसफॉर्मर, ऑनलाइन के-मतलब के आधार पर एक विरल रूटिंग मॉड्यूल के साथ आत्म-ध्यान संपन्न करता है, जबकि अनुक्रम लंबाई एन और छिपे हुए आयाम डी के लिए ओ (एन 2 डी) से ओ (एन 1.5 डी) पर ध्यान देने की समग्र जटिलता को कम करता है। हम दिखाते हैं कि हमारा मॉडल विकिटेक्स्ट -103 (15.8 बनाम 18.3 उलझन) पर भाषा मॉडलिंग पर तुलनीय विरल ध्यान मॉडल को मात देता है, साथ ही साथ इमेजनेट -64 (3.43 बनाम 3.44 बिट्स / मंद) पर छवि पीढ़ी पर, जबकि कम आत्म-ध्यान परतों का उपयोग करते हुए। इसके अतिरिक्त, हमने नए जारी किए गए पीजी -19 डेटा-सेट पर एक नया अत्याधुनिक सेट किया है, जो 8192 की लंबाई के अनुक्रमों पर प्रशिक्षित 22 परत रूटिंग ट्रांसफॉर्मर मॉडल के साथ 33.2 की परीक्षण उलझन प्राप्त करता है। हम Tensorflow.1 में रूटिंग ट्रांसफॉर्मर के लिए कोड खुला स्रोत', 'ga': "Glacadh féin-aird le déanaí do raon leathan fadhbanna samhaltú seichimh. In ainneoin a éifeachtúlachta, bíonn féinaird ag fulaingt ó riachtanais chearnach ríomh agus chuimhne maidir le fad seichimh. Dhírigh cuir chuige rathúla chun an chastacht seo a laghdú ar fhreastal ar fhuinneoga sleamhnáin áitiúla nó ar shraith bheag suíomhanna neamhspleách ar an ábhar. Tá sé i gceist lenár gcuid oibre patrúin dhinimiciúla gann airde a fhoghlaim a sheachnaíonn ríomh agus cuimhne a leithdháileadh chun freastal ar ábhar nach mbaineann leis an gceist spéise. Tógann an obair seo ar dhá líne taighde: Comhcheanglaíonn sé an tsolúbthacht samhaltaithe a bhaineann le réamhobair ar aird ghann bunaithe ar ábhar leis na gnóthachain éifeachtúlachta ó chur chuige atá bunaithe ar aird áitiúil, ama gann. Cuireann ár múnla, an Trasfhoirmeoir Ródúcháin, féin-aire le modúl ródaithe tanaí bunaithe ar k-acmhainní ar líne agus ag an am céanna laghdaítear castacht iomlán an aird ar O(n1.5d) ó O(n2d) d'fhad seichimh n agus toise folaithe d. Léirímid go sáraíonn ár múnla samhlacha inchomparáide aird thearc ar shamhaltú teanga ar Wikitext-103 (15.8 vs 18.3 perplexity), chomh maith le giniúint íomhá ar ImageNet-64 (3.43 vs 3.44 giotán/dim) agus úsáid á baint as níos lú sraitheanna féin-aird. Ina theannta sin, leagamar staid nua ar thacar sonraí PG-19 nua-eisithe, ag fáil perplexity tástála de 33.2 le múnla Trasfhoirmeoir Ródú 22 ciseal oilte ar sheichimh fad 8192. Oscailte againn an cód le haghaidh Trasfhoirmeoir Ródú i Tensorflow.1", 'hu': 'Az önfigyelmet a közelmúltban elfogadták a szekvencia modellezési problémák széles skálájára. Hatékonysága ellenére az önfigyelem a szekvencia hosszát illetően kvadratikus számítási és memóriaigényektől szenved. Ennek a bonyolultságnak a csökkentésére irányuló sikeres megközelítések a helyi tolóablakok vagy a tartalomtól független kis helyszínek ellátására összpontosítottak. Munkánk olyan dinamikus, ritka figyelemminták tanulását javasolja, amelyek elkerülik a számítási és memóriák allokálását az érdeklődésre vonatkozó lekérdezéshez nem kapcsolódó tartalmakhoz. Ez a munka két kutatási területre épül: ötvözi a tartalom alapú ritka figyelemmel kapcsolatos korábbi munkák modellezési rugalmasságát a helyi, időbeli ritka figyelemmel kapcsolatos megközelítések hatékonyságnövelésével. Modellünk, az Routing Transformer, az online k-eszközökön alapuló ritka útválasztási modullal ruházza fel az önfigyelmet, miközben csökkenti az O(n1.5d) figyelem összetettségét az n sorozathossz és a d rejtett dimenzió tekintetében. Megmutatjuk, hogy modellünk túllép a hasonló ritka figyelem modelleket a Wikitext-103 nyelvmodellezésében (15.8 vs 18.3 zavaróság), Az ImageNet-64 (3,43 vs 3,44 bit/dim) képgenerálásával kevesebb önfigyelemre méltó réteg használata mellett. Továbbá egy új korszerű PG-19 adatkészletet állítottunk be az újonnan kiadott PG-19 adatkészletre, amely 33.2-es tesztzavaróságot ért el egy 22 rétegű Routing Transformer modellel, amely 8192 hosszúságú szekvenciákra képzett. Nyílt forráskódú útvonalas transzformátor kódját Tensorflow-ban. 1', 'ka': 'საბოლოო მონიშვნელობა მხოლოდ განვითარებული პრობლემებისთვის განსაზღვრებულია. მისი ეფექტიურობის მარცხოვრებით, თავიდან დაახლოებით კვადრატური კომპუტიკაციის და მეხსიერების განსაზღვრებისგან. შემდგომარებელი დახმარება ამ კომპლექსიტეტის შემდეგ, რომელიც ლოკალური გადატვირთვა ფანჯრებში დაწყებულია, ან ცოტა ადგილების ნაწილად შემდგომარე ჩვენი სამუშაო მუშაობა დავისწავლა დინამიკური მონაცემები, რომლებიც არ გახსენებენ კომპუტაცია და მეხსიერება, რომ დავწყებენ ინტერესტის კითხვაზე,  ეს სამუშაო იქნება ორი სტრიქონის სტრიქონის შესახებ: ის კომბიზება წინა მუშაოდ სამუშაო მუშაო მუშაო მუშაო მუშაო მუშაო მუშაო მუშაო მუშ ჩვენი მოდელ, პროტინტრუქტურატორი, ჩვენი მოდელ, ჩვენი მოდელ, რომელიც კონტროქტურაციის მოდულის დამატებული k-means-ზე დაახლოებით, როდესაც O(n1.5d) სწორედ სიგრძნობის კომპლექტურება O(n2d) სწორედ სიგრძნობის სიგრძნობის n და დახლოების განზომილების d) სწორედ ImageNet-64 (3.43 vs 3.44 ბიტი/დამუშავებული) სურათების შესახებ უფრო მცირე თავიდან დაახლოებით. დამატებით, ჩვენ ახალი მონაცემული PG-19 მონაცემების შესახებ დავყენეთ ახალი მონაცემული მონაცემების შესახებ, 33.2-ის შესახებ 22-layer Routing Transformer მოდელს, რომელიც 8192 სიგრძე სიგრძე შესახებ განაკეთებული ჩვენ კოდის გახსნა ტენსორტროლექტის გარეშე. 1', 'it': "L'auto-attenzione è stata recentemente adottata per una vasta gamma di problemi di modellazione sequenziale. Nonostante la sua efficacia, l'auto-attenzione soffre di calcoli quadratici e requisiti di memoria rispetto alla lunghezza della sequenza. Approcci di successo per ridurre questa complessità si sono concentrati sulla cura di finestre scorrevoli locali o di una piccola serie di luoghi indipendenti dal contenuto. Il nostro lavoro propone di apprendere schemi dinamici di attenzione sparsa che evitino di allocare calcolo e memoria per occuparsi di contenuti estranei alla query di interesse. Questo lavoro si basa su due linee di ricerca: combina la flessibilità di modellazione del lavoro precedente sulla scarsa attenzione basata sui contenuti con i guadagni di efficienza derivanti da approcci basati su scarsa attenzione locale e temporale. Il nostro modello, il Routing Transformer, dota l'auto-attenzione di un modulo di routing sparso basato su k-means online riducendo al contempo la complessità complessiva dell'attenzione a O(n1.5d) da O(n2d) per la lunghezza di sequenza n e la dimensione nascosta d. Mostriamo che il nostro modello supera i modelli di attenzione sparsa comparabili sulla modellazione del linguaggio su Wikitext-103 (perplessità 15.8 vs 18.3), Oltre alla generazione di immagini su ImageNet-64 (3,43 vs 3,44 bit/dim) utilizzando meno livelli di auto-attenzione. Inoltre, abbiamo impostato un nuovo stato dell'arte sul set di dati PG-19 appena rilasciato, ottenendo una perplessità di test di 33.2 con un modello Routing Transformer a 22 strati addestrato su sequenze di lunghezza 8192. Abbiamo aperto il codice per Routing Transformer a Tensorflow. 1", 'el': 'Η αυτοπροσοχή έχει υιοθετηθεί πρόσφατα για ένα ευρύ φάσμα προβλημάτων μοντελοποίησης ακολουθίας. Παρά την αποτελεσματικότητά της, η αυτοπροσοχή πάσχει από τετραγωνικούς υπολογισμούς και απαιτήσεις μνήμης όσον αφορά το μήκος ακολουθίας. Οι επιτυχημένες προσεγγίσεις για τη μείωση αυτής της πολυπλοκότητας επικεντρώθηκαν στην παρακολούθηση τοπικών συρόμενων παραθύρων ή σε ένα μικρό σύνολο τοποθεσιών ανεξάρτητα από το περιεχόμενο. Η εργασία μας προτείνει να μάθουμε δυναμικά αραιά πρότυπα προσοχής που αποφεύγουν την κατανομή υπολογισμού και μνήμης για να ασχοληθούμε με περιεχόμενο που δεν σχετίζεται με το ερώτημα ενδιαφέροντος. Η εργασία αυτή βασίζεται σε δύο ερευνητικές γραμμές: Συνδυάζει την ευελιξία μοντελοποίησης προηγούμενης εργασίας σχετικά με την περιορισμένη προσοχή βασισμένη στο περιεχόμενο με τα κέρδη αποδοτικότητας από προσεγγίσεις που βασίζονται στην τοπική, χρονική περιορισμένη προσοχή. Το μοντέλο μας, ο μετασχηματιστής δρομολόγησης, παρέχει στην αυτοπροσοχή μια αραιή ενότητα δρομολόγησης βασισμένη σε διαδικτυακά Κ-μέσα ενώ μειώνει τη συνολική πολυπλοκότητα της προσοχής στο Ο(1.5δ) από το Ο(για μήκος ακολουθίας και κρυφή διάσταση Δείχνουμε ότι το μοντέλο μας ξεπερνά συγκρίσιμα μοντέλα αραιής προσοχής στη γλωσσική μοντελοποίηση στο Βικιτext-103 (15.8 vs 18.3 σύγχυση), καθώς και για την παραγωγή εικόνας στο ImageNet-64 (3.43 vs 3.44 bits/dim) ενώ χρησιμοποιείτε λιγότερα επίπεδα αυτοπροσοχής. Επιπλέον, θέσαμε μια νέα κατάσταση τεχνολογίας στο πρόσφατα κυκλοφορημένο σύνολο δεδομένων αποκτώντας μια δοκιμαστική σύγχυση 33.2 με ένα 22 στρώμα μετασχηματιστή δρομολόγησης εκπαιδευμένο σε ακολουθίες μήκους 8192. Ανοίγουμε τον κώδικα για τον μετασχηματιστή δρομολόγησης στο Tensorflow. 1', 'ms': 'Perhatian-diri baru-baru ini telah diterima untuk julat luas masalah pemodelan urutan. Walaupun keefektivitasnya, perhatian diri menderita dari perkiraan kuadratik dan keperluan ingatan berkaitan dengan panjang urutan. Pendekatan berjaya untuk mengurangi kompleksiti ini fokus pada menghadiri tetingkap slaid setempat atau set kecil lokasi yang tidak tergantung pada kandungan. Kerja kami mencadangkan untuk belajar corak dinamik perhatian yang sedikit yang mengelakkan mengalokasi komputer dan memori untuk menghadiri kandungan yang tidak berkaitan dengan pertanyaan kepentingan. Kerja ini dibangun pada dua garis penyelidikan: Ia menggabungkan fleksibiliti model kerja sebelumnya pada perhatian yang sedikit berdasarkan kandungan dengan keuntungan efisiensi dari pendekatan berdasarkan perhatian yang sedikit setempat, sementara. Model kami, Penukar Jalur, memberikan perhatian diri dengan modul Jalur ringan berdasarkan makna-k-online sambil mengurangkan kompleksiti keseluruhan perhatian kepada O(n1.5d) dari O(n2d) untuk panjang urutan n dan dimensi tersembunyi d. Kami menunjukkan bahawa model kami melampaui model perhatian ringan yang boleh dibandingkan pada pemodelan bahasa di Wikitext-103 (15.8 vs 18.3 keseluruhan), ) serta pada generasi imej pada ImageNet-64 (3.43 vs 3.44 bit/dim) semasa menggunakan lebih sedikit lapisan perhatian diri. Selain itu, kami menetapkan keadaan baru pada set data PG-19 yang baru dilepaskan, memperoleh perpleksi ujian 33.2 dengan model Transformer Routing 22 lapisan dilatih pada urutan panjang 8192. Kami buka-sumber kod untuk Routing Transformer di Tensorflow. 1', 'kk': 'Бірнеше реттеу модель мәселелері үшін өзіңіздің өзіңіздің өзіңіздің қатынасы қолданылды. Өзіңіздің эффективнілігіне қарамастан, квадратикалық есептеу мен жады қажеттерінің ұзындығына қатынасыз болады. Жергілікті жүгірту терезелеріне не мазмұның тәуелсіз кішкентай жергіліктерге қатынау үшін осы тәуелсіздікті көшірмелеу арқылы сәтті болды. Біздің жұмысамыз, есептеу мен жады бөлігіне қатынауға болмайтын динамикалық қатынау үлгілерін үйрену үшін ұсынады. Бұл жұмыс екі зерттеу жолдарына құрылады: ол мазмұның негізінде алдыңғы жұмыс істеу гибкиттігін біріктіреді. Жергілікті, уақытша қызмет тәртіпсіздігіне негізінде тәртіпсіздік жетк Біздің үлгіміз, маршруттау түрлендірушіміз, онлайн k- мерзімдеріне негізделген көңіл маршруттау модулімен, O( n1. 5d) ретінде N және жасырын өлшемі d) ретінде O( n1. 5d) қатынасын төмендету үшін, өзіміздің өзіміздің өзімізді көрсетеді. Біздің үлгіміз Wikitext-103 (15. 8 жә Өзіңізге қарау қабаттарын қолдану кезінде ImageNet- 64 (3, 43 және 3, 44 бит/ тұмандық) кескінді құру кезінде де болады. Қосымша, біз жаңа шығарылған PG- 19 деректер жиынына жаңа құрылғының күйін орнатып, 33. 2 қабаттың 22 қабатты маршруттау түрлендіруші үлгісін 8192 ұзындығында бақылады. Біз тенсорфлаузердегі маршруттау түрлендірушісінің кодын ашық. 1', 'lt': 'Neseniai dėmesys buvo skiriamas įvairioms sekos modeliavimo problemoms spręsti. Nepaisant jo veiksmingumo, savarankiškas dėmesys susiduria su kvadratiniais skaičiavimo ir atminties reikalavimais, susijusiais su sekos ilgiu. Sėkmingi metodai, kuriais siekiama sumažinti šį sudėtingumą, daugiausia dėmesio skiria vietiniams skleidžiamiesiems langams arba mažoms vietoms, nepriklausančioms nuo turinio. Mūsų darbe siūloma išmokti dinamiškų nedidelio dėmesio modelių, kurie vengtų paskirstyti skaičiavimus ir atmintį, kad būtų atsižvelgta į turinį, nesusijusį su suinteresuotu klausimu. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention.  Mūsų modelis, maršruto transformatorius, suteikia savarankišk ą d ėmesį spartaus maršruto moduliui, pagrįstu internetinėmis k priemonėmis, kartu mažinant bendrą dėmesio į O(n1.5d) kompleksiškumą iš O(n2d) sekos ilgio n ir paslėpto matmens d atžvilgiu. Mes rodome, kad mūsų modelis pasiekia panašius spartaus dėmesio modelius kalbų modeliavimui Wikitext-103 (15.8 vs 18.3 perplexity), ) taip pat ImageNet-64 (3,43 ir 3,44 bitų/dim) vaizdo kūrimo metu, naudojant mažiau savarankiško dėmesio sluoksnių. Be to, mes nustatėme naują naujausią PG-19 duomenų rinkinį, gaunantį 33,2 lygio bandymų perpleksiją su 22 sluoksnių maršruto transformatoriaus modeliu, apmokytu 8192 ilgio sekomis. Mes atviras kodas, kodas, kodas, skirtas transformuotojui Tensorflow. 1', 'mk': 'Самовниманието неодамна е усвоено за голем број проблеми со моделирањето на секвенциите. И покрај нејзината ефикасност, самото внимание страда од квадратни обчистувања и потреби од меморија во однос на должината на секвенцијата. Успешни пристапи за намалување на оваа комплексност се фокусираа на присуството на локалните прозорци со лизгање или мал набор локации независни од содржината. Нашата работа предлага да се научат динамични ниски шеми на внимание кои избегнуваат да се додели компјутација и меморија за да се погрижат за содржината која не е поврзана со прашањето за интерес. Оваа работа се базира на две линии истражувања: ја комбинира моделирачката флексибилност на претходната работа на содржината базирана на мало внимание со ефикасноста од пристапите базирани на локално, температурно мало внимание. Нашиот модел, Routing Transformer, дава себеси внимание со мал модул за рутирање базиран на онлајн k-средства, а истовремено ја намалува целокупната комплексност на вниманието на O(n1.5d) од O(n2d) за должина на секвенца n и скриена димензија d. Ние покажуваме дека нашиот модел ги надминува споредливите мали модели за внимание на јазичното моделирање на Wikitext-103 (15 - како и на генерацијата на слики на ImageNet- 64 (3, 43 против 3, 44 бити/ темнина) додека се користат помалку слоеви за самовнимание. Покрај тоа, поставуваме нова најнова технологија на новиот пуштен набор на податоци од PG-19, добивајќи тестова перфексност од 33,2 со 22 слоеви модел на рутинг трансформ трениран на секвенции од должина 8192. Го отвораме кодот за трансформирање на патот во Тенсорструм. 1', 'mt': 'Dan l-a ħħar ġiet adottata l-awtonomija għal firxa wiesgħa ta’ problemi ta’ mudellar tas-sekwenzi. Minkejja l-effettività tagħha, l-awtonomija tbati minn rekwiżiti kwadratiċi ta’ komputazzjoni u memorja fir-rigward tat-tul tas-sekwenza. Approċċi ta’ suċċess biex titnaqqas din il-kumplessità ffukaw fuq l-attendenza għal twieqi lokali li jiżżerżqu jew sett żgħir ta’ postijiet indipendenti mill-kontenut. Ix-xogħol tagħna jipproponi li jitgħallmu xejriet dinamiċi ta’ attenzjoni baxxa li jevitaw l-allokazzjoni tal-komputazzjoni u l-memorja biex jattendu għal kontenut mhux relatat mal-mistoqsija ta’ interess. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention.  Il-mudell tagħn a, it-Trasformatur tar-Rotot, jagħti l-attenzjoni lilu nnifsu b’modulu rari tar-rotot ibbażat fuq mezzi k onlajn filwaqt li jnaqqas il-kumplessità globali tal-attenzjoni għal O(n1.5d) minn O(n2d) għat-tul tas-sekwenza n u d-dimensjoni moħbija d. Aħna nuru li l-mudell tagħna jwettaq mudelli komparabbli ta’ attenzjoni rari dwar l-immudellar tal-lingwi fuq Wikitext-103 (15.8 vs ) kif ukoll fuq il-ġenerazzjoni ta’ immaġni fuq ImageNet-64 (3.43 vs 3.44 bit/dim) filwaqt li jintużaw inqas saffi ta’ awtonomija. Barra minn hekk, iffissajna a ġġornament ġdid fuq is-sett ta’ dejta PG-19 li għadu kif ġie rilaxxat, li kisbu perplessità tat-test ta’ 33.2 b’mudell ta’ Trasformer ta’ Routing 22 saff imħarreġ fuq sekwenzi ta’ tul 8192. Aħna nagħmlu sors miftuħ il-kodiċi għat-Trasformer tar-Rotot fit-Tensorflow. 1', 'ml': 'അടുത്തുതന്നെ സ്വയം ശ്രദ്ധിക്കുന്നത് ഒരു വിശാലമായ പ്രശ്നങ്ങള്\u200dക്ക് വേണ്ടിയാണ്. അതിന്റെ പ്രവർത്ഥത്തിന് ശേഷം, സ്വയം ശ്രദ്ധിക്കുന്നത് ക്വാഡ്രാറ്റിക്ക് കണക്ട് ചെയ്യുന്നതിനും മെമ്മറിയുട ലോക്കല്\u200d സ്ലൈഡിങ്ങ് ജാലകങ്ങളിലേക്കോ സ്വാതന്ത്ര്യതയില്ലാത്ത സ്ഥലങ്ങളിലേക്കോ ചെറിയ സ്ഥാനങ്ങളിലേക്കോ ഈ സംബന്ധ Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest.  ഈ ജോലി രണ്ടു രീതിയില്\u200d നിര്\u200dമ്മിക്കുന്നു: അതിന്റെ മുമ്പിലുള്ള ജോലിയുടെ മോഡലിങ് ഫ്ലാക്സിറ്റിയില്\u200d മാത്രം ജോലിയുടെ ശ്രദ്ധ കൂട്ട ഞങ്ങളുടെ മോഡല്\u200d, റോട്ടിംഗ് ട്രാന്\u200dസ്ഫോര്\u200dമാര്\u200d, ഓ( n1. 5d) അടിസ്ഥാനത്ത് അടിസ്ഥാനമായി സ്വയം ശ്രദ്ധിക്കുന്ന ഒരു സ്പോര്\u200dട്ട് റൂട്ടിംഗ് മോഡിള്\u200d ഓ( n1. 5d) ല്\u200d നിന്ന് മൊത്തം ശ്രദ്ധ കുറച്ച് കൂടുതല്\u200d കൂടുതല്\u200d കൂടുതല്\u200d കൂടുതല്\u200d  ഇമേജ്നെറ്റ്-64 (3. 43 vs 3. 44 ബിറ്റ്/dim) ലെ ഇമേജ് തലമുറയിലും ചെറുതായി സ്വയം ശ്രദ്ധിക്കുന്ന layers ഉപയോഗിക്കുമ്പോള്\u200d. കൂടാതെ, പുതിയ പിജി-19 ഡേറ്റാ സെറ്റില്\u200d പുതിയ സ്റ്റേറ്റ് സ്റ്റേറ്റ് സെറ്റ് ചെയ്യുന്നു. ഒരു പരീക്ഷണത്തിന്റെ പരീക്ഷണത്തില്\u200d 33. 2 ടെസ്റ്റ് ട്രാന്\u200dസ്ഫോ ടെന്\u200dസോര്\u200dഫ്ലൂട്ടില്\u200d റോട്ടിംഗ് ട്രാന്\u200dസ്ഫോര്\u200dമാന്\u200dസിന്\u200dറെ കോഡ് തുറക്കുന്നു. 1', 'no': 'Sjølvoppmerksomhet er nyleg godtatt for eit stor rekke av rekkemodelleringsproblemer. Til tross effektiviteten blir selvmerksomheten gått frå kvadratiske rekningar og minnebruk med hensyn til sekvenslengd. Vellukka tilnærmingar for å redusera denne kompleksiteten fokusert på å delta på lokale glidebrytarvindauge eller ei lite sett stader uavhengig av innhaldet. Arbeidet vårt foreslår å lære dynamiske sparse oppmerksmønster som unngår å tildela datamaskina og minne til innhaldet som ikkje er tilknytta til spørjinga om interesse. Dette arbeidet bygger på to forskningslinjer: Det kombinerer fleksibilitet for modellering av førre arbeid på innhaldsbasert spark oppmerksomhet med effektiviteten får frå tilnærmingar basert på lokale, temporale spark oppmerksomhet. Dette modellet vårt, ruteringstransformeren, gjev selvmerksomheten med ein sparse ruteringsmodul basert på online k-betyr medan du reduserer overalt kompleksitet av oppmerksomheten til O(n1.5d) frå O(n2d) for sekvenslengd n og skjulte dimensjon d. Vi viser at modellen vårt utfører sammenlignbare sparse oppmerksomhetsmedular på språk-modellen på Wikitext-103 (15.8 vs 18.3 kompleksitet), Og ved laging av bilete på ImageNet-64 (3.43 vs 3.44 bits/dim) mens du brukar mindre selvmerkingslag. I tillegg bestemmer vi eit nytt kunsttilstand på den nye lagte datasettet PG-19, som får ein test-kompleksitet av 33,2 med ein 22 lagruteringstransformeringsmodell trent på rekkjefølgja med lengde 8192. Vi opnar kjeldekoden for ruteringstransformeringa i Tensorflow. 1', 'mn': 'Саяхан өөрийгөө анхаарлын төвлөрөмж олон дарааллын загварын асуудлуудын тулд хүлээн зөвшөөрсөн. Үнэндээ үр дүнтэй ч, өөртөө анхаарлыг квадратикийн тооцооллоос, санамжийн хэрэгцээнд дарааллын урттай холбоотой. Энэ төвөгтэй байдлыг багасгах амжилттай арга зам нь орон нутгийн сүлжээний цонхны тухай анхаарлаа төвлөрүүлсэн байдаг. Бидний ажлын хувьд тооцоололт болон санамж санаагаа хуваалцахын тулд динамикийн анхаарлын загваруудыг сурах боломжтой. Энэ ажил хоёр шинжлэх ухааны шугам дээр бий болгодог. Энэ нь эхний ажил дээр байрлал дээр суурь санаа зориулан анхаарал хангалттай, орон нутгийн, хугацааны анхаарал дээр суурь хангалттай хүмүүсийн үр ашигтай хүмүүсийг нийлүүлдэг. Бидний загвар, маршруулах Трансформатор, интернет k-means дээр үндсэн жижиг хэлбэрээр анхаарлаа хандуулдаг. O(n1.5d) тооны N(n2d) загварын урт болон нуугдмал хэмжээст d-ээс O(n1.5d) анхаарлыг багасгаж байна. Бид загварын загвар Wikitext-103 дээр харьцуулагдах жижиг анхаарлын загварыг хийдэг. Өөрийгөө төвлөрүүлэхэд бага анхаарлаа ашиглаж байгаа ImageNet-64 (3.43 vs 3.44 бит/хэмжээний) зураг үеийнх нь бага. Мөн бид шинэ нээлттэй PG-19 өгөгдлийн комплекс дээр шинэ урлагийн байр суурилуулсан. 33.2 давхар руу шилжүүлэх загвартай 22 давхар руу шилжүүлэх загвар 8192 урт хугацаанд сургалтыг авсан. Бид Tensorflow-д Routing Transformer-ын кодыг нээлттэй эх үүсвэртэй. 1', 'sr': 'Nedavno je prihvaćena sama pažnja za širok niz problema modeliranja sekvencija. Uprkos njenoj efikasnosti, samopouzdanje pati od kvadratičnog računala i zahteva pamćenja u vezi dužine sekvencije. Uspješni pristupi smanjivanju ove kompleksnosti fokusirane na prisustvu lokalnih prozora klizanja ili malih set a lokacija nezavisnih od sadržaja. Naš rad predlaže da naučimo dinamične rezervne pažnje koje izbjegavaju dodavanje računala i pamćenja da prisustvuju sadržaju bez veze sa pitanjem interesa. Ovaj rad se osnova na dve linije istraživanja: Kombinuje fleksibilnost modeliranja prethodnog rada na osnovu sadržaja rezervne pažnje sa učinkovitostima od pristupa na temelju lokalne, privremene rezervne pažnje. Naš model, putujući transformator, podržava samopouzdanje rezervnim modulom rutiranja baziranim n a internetskim k-sredstvima, dok smanjuje ukupnu kompleksnost pažnje na O(n1.5d) od O(n2d) za dužinu sekvence n i skrivenu dimenziju d. Pokazujemo da n a š model iznosi usporedbene rezervne pažnje modele jezika na Wikitext-103 (15,8 protiv 18,3 kompleksnost), Kao i na generaciji slika na ImageNet-64 (3,43 protiv 3,44 bita/tamno) dok koristi manje slojeva samopouzdanja. Osim toga, postavili smo novo stanje umjetnosti na novog oslobođenog set a podataka PG-19, dobivajući test perpleksitet od 33,2 sa model 22 slojeva rutiranog transformera koji je obučen na sekvencijama dužine 8192. Otvoreni izvor koda za putovanje transformera u Tensorflow. 1', 'pl': 'Ostatnio uwaga na siebie została przyjęta dla szerokiego zakresu problemów modelowania sekwencji. Pomimo swojej skuteczności, uwaga na siebie cierpi z powodu obliczeń kwadratowych i wymagań pamięci w odniesieniu do długości sekwencji. Skuteczne podejście do zmniejszenia tej złożoności skupiało się na obsłudze lokalnych okien przesuwnych lub małego zestawu lokalizacji niezależnych od treści. Nasza praca proponuje naukę dynamicznych, rzadkich wzorców uwagi, które unikają przydzielania obliczeń i pamięci do obsługi treści niezwiązanych z zainteresowaniem. Praca ta opiera się na dwóch liniach badań: łączy elastyczność modelowania wcześniejszych prac nad rzadką uwagą opartą na treści z zyskami efektywności wynikającymi z podejść opartych na lokalnej, czasowej rzadkiej uwadze. Nasz model, transformator routingowy, wyposaża samą uwagę w rzadki moduł routingowy oparty na środkach k online, przy jednoczesnym zmniejszeniu ogólnej złożoności uwagi na O(n1.5d) z O(n2d) dla długości sekwencji n i ukrytego wymiaru d. Pokazujemy, że nasz model przewyższa porównywalne modele rzadkiej uwagi w modelowaniu językowym na Wikitext-103 (15.8 vs 18.3 zdezorientowanie), Jak również generowanie obrazów na ImageNet-64 (3.43 vs 3.44 bity/dim) przy użyciu mniejszej ilości warstw uwagi. Dodatkowo ustawiliśmy nowy state-of-the-art na nowo wydanym zbiorze danych PG-19, uzyskując zakłopotanie testowe 33.2 z 22-warstwowym modelem transformatora routing przeszkolonym na sekwencjach długości 8192. Otworzyliśmy kod do Routing Transformer w Tensorflow. 1', 'sv': 'Självuppmärksamhet har nyligen antagits för ett brett spektrum av sekvensmodelleringsproblem. Trots sin effektivitet lider självuppmärksamheten av kvadratiska beräkningar och minneskrav med avseende på sekvenslängd. Framgångsrika metoder för att minska denna komplexitet fokuserade på att ta hand om lokala skjutfönster eller en liten uppsättning platser oberoende av innehåll. Vårt arbete föreslår att lära sig dynamiska sparsamma uppmärksamhetsmönster som undviker att allokera beräkning och minne för att ta hand om innehåll som inte är relaterat till intressefrågan. Detta arbete bygger på två forskningslinjer: Det kombinerar modelleringsflexibiliteten i tidigare arbete med innehållsbaserad sparsam uppmärksamhet med effektivitetsvinster från tillvägagångssätt baserade på lokal, tidsbegränsad uppmärksamhet. Vår modell, Routing Transformer, förser självuppmärksamheten med en gles routingmodul baserad på online k-medel samtidigt som den totala komplexiteten i uppmärksamheten på O(n1.5d) från O(n2d) för sekvenslängd n och dold dimension d. Vi visar att vår modell överträffar jämförbara glesa uppmärksamhetsmodeller på språkmodellering på Wikitext-103 (15,8 vs 18,3 perplexitet), samt bildgenerering på ImageNet-64 (3,43 vs 3,44 bitar/dim) med färre självuppmärksamhetslager. Dessutom satte vi en ny state-of-the-art på den nyligen släppta PG-19 datauppsättningen, vilket fick en testperplexitet på 33.2 med en 22-lagers Routing Transformer modell utbildad på sekvenser av längd 8192. Vi öppnar koden för Routing Transformer i Tensorflow. 1', 'so': "Dhab ahaanshaha iskaa waxaa loo qaaday dhibaatooyin kala duduwan tusaalayaal kala duduwan. Inta kastoo ay tayada leedahay, iskuul-dhigista waxay ka dhibaataysaa xisaabinta qiyaasta iyo baahida xusuusta ee ku saabsan dhererka dabarka. Dhaqdhaqaaqyo liibaansan si ay u hoosaystiro qalabkan adag oo uu ku kalsoonaado daaqadaha sawirida ama meelo yar oo aan wax ku filnayn. Shuqulkayagu wuxuu soo jeedaa in uu barto qaabab aad u dhaqdhaqaaqi karto, kaas oo ka leexda qaybinta xisaabta iyo xasuusta si ay uga qeybiso waxyaabaha aan la xiriirin su'aalka danaha. Shaqadaasu wuxuu ku qoran yahay laba saf oo waxbarasho ah: waxay isku daryeelataa dhaqdhaqaalaha hore ee shaqada oo ku saabsan qiimo yar ee waxyaabaha ku jirta, faa’iidada aad ka helayso meelaha laga soo galo ee ku saleynayo goobaha waqtiga ah. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), Sidoo kale sawir qarnigiisa ku qoran ImageNet-64 (3.43 vs 3.44 bits/dim) iyadoo isticmaalaya qasnadaha iskuulka. Sidoo kale waxaynu qornay xaalad cusub oo farshaxan ah oo cusub la soo daayay PG-19, oo aan sameynnay imtixaan 33.2 oo ay sameynaysaa tilmaamo 22 darajad oo lagu tababaray durdurada 8192. Waxaynu furan nahay sumadda turjubaanka ee Tensorflow. 1", 'ro': 'Auto-atenția a fost adoptată recent pentru o gamă largă de probleme de modelare a secvențelor. În ciuda eficacității sale, auto-atenția suferă de cerințele de calcul pătratic și memorie în ceea ce privește lungimea secvenței. Abordările de succes pentru reducerea acestei complexități s-au concentrat pe participarea la ferestre glisante locale sau un set mic de locații independente de conținut. Lucrarea noastră propune să învățăm modele dinamice de atenție slabă care evită alocarea calculelor și a memoriei pentru a participa la conținut care nu are legătură cu interogarea de interes. Această lucrare se bazează pe două linii de cercetare: combină flexibilitatea modelării lucrărilor anterioare privind atenția slabă bazată pe conținut cu câștigurile de eficiență ale abordărilor bazate pe atenția slabă locală, temporală. Modelul nostru, Routing Transformer, dotează auto-atenția cu un modul de rutare rar bazat pe mijloace k online, reducând în același timp complexitatea generală a atenției la O(n1.5d) de la O(n2d) pentru lungimea secvenței n și dimensiunea ascunsă d. Aratăm că modelul nostru depășește modelele de atenție rare comparabile pe modelarea limbajului pe Wikitext-103 (perplexitate 15.8 vs 18.3), precum și generarea de imagini pe ImageNet-64 (3,43 vs 3,44 biți/dim), utilizând mai puține straturi de auto-atenție. În plus, am stabilit un nou set de date PG-19 nou lansat, obținând o perplexitate de testare de 33.2 cu un model de Routing Transformer de 22 straturi instruit pe secvențe de lungime 8192. Deschidem codul pentru Routing Transformer în Tensorflow. 1', 'ta': 'அண்மையில் நிறைய தொடர் மாதிரி பிரச்சனைகளுக்காக தானே கவனத்தை எடுத்துக் கொள்ளப்பட்டது. அதன் விளைவுக்காக இருந்தாலும், தொடர்ந்து நீளத்தை பற்றி தன்னியக்கவனம் குவாட்டிக் கணக்கிலிருந்தும் நினைவு தேவைகள @ info: whatsthis எங்கள் வேலை ஆர்வத்தின் கேள்வி இல்லாத உள்ளடக்கத்திற்கு ஒதுங்குவதற்கு மற்றும் நினைவகத்தை தவிர்க்கும் தேவைப்படும் தேவையி இந்த வேலை இரண்டு ஆராய்ச்சியின் வரிகளில் கட்டப்படுகிறது: இது முன் வேலையின் மாதிரியான புள்ளிவெளிச்சத்தை இணைக்கிறது உள்ளடக்கத்தில் உள்ளார்ந எங்கள் மாதிரி, சுழற்சி மாற்றும் மாதிரி ஒலின் k- மாற்றியில் அடிப்படையில் உள்ள ஒரு சிறிய சுற்றும் வழி கூற்றுடன் தன்னை கவனம் அளிக்கிறது. O( n1. 5d) லிருந்து மொத்த சிக்கல் கவனத்தை குறைக்கும் போது தொடர் நீளம் n மற்றும் மறைக்கப் பிம்பத்தை உருவாக்கும் போதும் பிம்பத்தின் உருவத்திலும் (3. 43 vs 3. 44 பிட்கள்/dim) சிறிய தன்னை கவனம் அடுக்கை பயன்படுத்தும் மேலும், நாம் புதிய வெளிப்படுத்தப்பட்ட PG-19 தரவு அமைப்பில் புதிய நிலையை அமைத்தோம், சோதனையின் பிரச்சனையை 33. 2 க்கு கிடைக்கும், ஒரு 22 அடுக்கு சுற்றி மாற்ற நாங்கள் சுழற்சி மாற்றும் குறியீட்டை திறந்து விட்டோம். 1', 'si': 'ස්වයංග අවධානයක් අලුත් වෙලාවට ප්\u200dරශ්නයක් වෙනුවෙන් ප්\u200dරශ්නයක් ලැබුනා. ස්වාමික අවස්ථාවක් නැත්තම්, ස්වාමික අවස්ථාවක් පරීක්ෂණය සහ මතක අවස්ථාවක් පරීක්ෂණය සඳහා පරීක සාර්ථක ස්ලායින් කවුළුවෙන් ස්ථානික ස්ථානයක් නැති ස්ථානයක් නිර්මාණය කරලා මේ සංකෘතිය අඩංගුවක්  අපේ වැඩේ ඉගෙන ගන්න පුළුවන් විදිහට ජාතික අවධානයක් ඉගෙන ගන්න පුළුවන් විදිහට පරිගණනය සහ මතකය සම්බන්ධතාව මේ වැඩේ පරීක්ෂණාගාර දෙකක් පරීක්ෂණය සඳහා නිර්මාණය කරනවා: මුලින් වැඩේ ප්\u200dරමාණය සඳහා ප්\u200dරමාණය සඳහා පරීක්ෂණය සඳහා පරීක්ෂණය සඳහා  අපේ මොඩල්, රුටින් ප්\u200dරවර්තනයක්, ස්වයංගයක් ස්වයංගයක් ගැන ස්වයංගයක් අන්තර්ජාත k-means අධිරූපයෙන් ස්වයංගයක් ප්\u200dරවර්තනය කරනවා වගේම O(n1.5d) වෙනුවෙන් ස්වයංගයක් නිර්මාණය සහ හ හංගයක්  පින්තූර පින්තූර පරීක්ෂණය සහ පින්තූර පරීක්ෂණය (3.43 විරුද්ධ 3.44 බිට්/ඩිම්) ස්වයංග්\u200dරහයක් ප්\u200dරයෝජනය තවත්, අපි අළුත් ප්\u200dරතික්\u200dරියාත්මක ප්\u200dරතික්\u200dරියාත්මක ප්\u200dරතික්\u200dරියාත්මක ප්\u200dරතික්\u200dරියාත්මක කරනවා PG-19 දත්ත සැට, 33.2 ගැන පරීක්ෂණයක් ප්\u200dරතික් අපි ටෙන්සෝර්ප්ලෝව් වල රුටින්ස්ප්\u200dරාන්ස්ටර් එක්ක කෝඩ් විවෘත කරන්න පුළුවන්. 1', 'ur': 'اچھا ہی دھوپ سے اپنا اظہار اٹھایا گیا ہے بہت سی سیم نمڈلینگ مشکلات کے لئے۔ اس کے اثرات کے بغیر، اپنے آپ کی توجه کو چوٹراٹیک کمپیوٹر اور یادگاری کی ضرورت سے پہنچتی ہے. اس پیچیدگی کو کمزور کرنے کے لئے موفق طریقے ہیں جو محلی سلیڈنگ ویندوز کے حضور حاضر ہوتے ہیں یا منزلت کے بغیر کسی چھوٹی جگہ کا سٹ۔ ہمارے کام کی پیشنهاد کرتا ہے کہ ڈینمانیک توجه پٹرنے کو سیکھ سکیں جو کمپیوٹر اور یاد کا تقسیم کرنے سے پرہیزگاری کریں کہ علاقہ کے سوال کے بغیر منطقی موجود موجود موجود موجود موجود ہوں۔ یہ کام دو خطوط تحقیقات پر بناتا ہے: اس نے پہلے کام کی مدل مہربانی کو متصل کر رکھا ہے جو منزل کی بنیاد رکھی ہوئی کم توجه کے ذریعہ سے پہلے کام کی مدل مہربانی کو متصل کر رہا ہے، جو محلی، موقت کی کم توجه پر بنیاد رکھی ہوئی تقریب ہمارا موڈل، روٹینگ ٹرنفسر، اپنی توجه کو اولین ک مہینٹ پر بنیاد رکھتا ہے، اولین ک مہینٹ کے اولین اول(n1.5d) سے اول(n2d) کی تمام پیچیدگی کو کم کرتا ہے اول(n1.5d) کی اول(n2d) کی مدت n اور پوشیدہ اندازہ d کے لئے. ہم دکھاتے ہیں کہ ہماری موڈل ویکیٹکس-103 (15.8 vs 18.3 پیچیدگی) پر مثالفی توجه کی اور تصویر کی نسل پر بھی ImageNet-64 (3.43 vs 3.44 bits/dim) کم سمجھنے لہروں کو استعمال کرتے ہیں. اور اضافہ، ہم نے نئی اگلی اگلی اگلی اگلی اگلی اگلی اگلی اگلی اگلی اگلی ڈیٹ سٹ پر ایک نئی ایٹ-آرت کی حالت مقرر کی، 33.2 کی آزمائش پر ایک دوسری لائر رائٹ ٹرنفسر موڈل کے ساتھ آزمائش کی گئی ہے، 8192 کی کثیر ہم ٹینسورفول میں روٹنگ ترنسفور کے لئے کوڈ کھولتے ہیں. 1" (msgctxt: "panel:showusername") to "1', 'uz': "Yaqinda bir necha xil model muammolari uchun o'zimni boshqarish. Ikkinchi darajada, o'zimni o'zimga qidratik hisoblash va xotira keraklaridan foydalanadi. Name Bizning ishlarimiz qiziqarish usullarini o'rganish imkoniyatini o'rganish talab qiladi. Bu hisoblash va xotira qo'shish uchun qiziqarishga murojaat qilmagan mavzuga ega bo'lishni xohlanadi. Bu ishni ikkita tarjima qoidaga yaratadi: Bu birinchi ishni o'zgartiradi, o'sha tarkib asosida qiymati bilan birlashtirish mumkin, lokal, vaqt cheksiz paydo bo'lgan murakkablaridan foydalanadi. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), Name Ko'pchilik, biz yangi released PG-19 maʼlumotlar tarkibida yangi sanat holatini boshlab, 33.2 ta'minlovchi tizimni boshlab, 8192 sonlari sonlarida o'rgangan 22 darajada uchrashuv modelini olish mumkin. Biz tensorflow tarjima qilish qoidasini ochamiz. 1", 'vi': 'Tự chăm sóc bản thân đã được chấp nhận cho một loạt các vấn đề về chế tạo gien. Bất chấp hiệu quả của nó, sự chú ý của chính mình bị chi phối tính to án phần tư và các nhu cầu trí nhớ về độ dài. Cách tiếp cận thành công để giảm sự phức tạp này tập trung vào việc tham gia vào các cửa sổ trượt hay một nhóm nhỏ không phụ thuộc vào nội dung. Công việc của chúng tôi đề nghị học các mô hình chú ý thoáng động động, tránh phân bổ tính to án và bộ nhớ để tham gia vào các nội dung không liên quan đến câu hỏi của sự quan tâm. Công việc này dựa trên hai phương pháp nghiên cứu: Nó kết hợp sự linh hoạt của việc nghiên cứu sơ sài dựa trên nội dung và sự chú ý hiệu quả từ các phương pháp dựa trên sự chú ý gần gũi địa phương. Máy biến đổi tuyến sẽ tạo ra sự chú ý thoáng qua của chúng ta, d ựa trên phương tiện k trên mạng, và giảm dần sự phức tạp của sự chú ý quan tâm đến O(n.1.5d) từ O(n.2d) cho chiều dài chuỗi n và chiều không bị che dấu. Chúng ta cho thấy rằng mô- đun của chúng ta hoàn thiện các mô hình chú ý thoáng đối nhau về mô hình ngôn ngữ trên k ênh 103 (15.8/3) cũng như trên hình ảnh sản xuất trên ImageNet-chợt (3.43 v 3.44 bits/dim) trong khi sử dụng ít lớp chăm sóc bản thân. Thêm vào đó, chúng tôi đặt một bộ phim mới tinh trên bộ dữ liệu PG-19 vừa được khởi động, để lấy thêm được thử nghiệm trong phạm vi 33.2 với mô hình Biến đổi đường dài 22 được huấn luyện theo dãy số liên kết 8192. Chúng ta mở nguồn cho bộ dịch chuyển lộ trình ở Tenorf. L', 'hr': 'Nedavno je prihvaćena sama pažnja za širok niz problema modeliranja sekvencija. Unatoč njenoj učinkovitosti, samopouzdanje pati od kvadratnih zahtjeva računala i sjećanja u vezi dužine sekvencije. Uspješni pristupi smanjivanju ove kompleksnosti usredotočene na prisustvo lokalnim prozorima klizanja ili malim skupom lokacija nezavisnih od sadržaja. Naš rad predlaže naučiti dinamične rezervne obrasce pažnje koje izbjegavaju prilagođenje računala i pamćenja da prisustvuju sadržaju bez veze sa pitanjem interesa. Ovaj rad se osnova na dvije linije istraživanja: Kombinuje fleksibilnost modeliranja prethodnog rada na temelju sadržaja rezervne pažnje s učinkovitostima od pristupa na temelju lokalne, privremene rezervne pažnje. Naš model, putujući transformator, pruža samopouzdanje rezervnim modulom rutiranja temeljnim na internetskim k-sredstvima dok smanjuje ukupnu kompleksnost pažnje na O(n1.5d) iz O(n2d) za dužinu sekvencije n i skrivenu dimenziju d. Pokazujemo da n a š model iznosi usporedbene rezervne pažnje modele jezika na Wikitext-103 (15,8 protiv 18,3 kompleksnost), Kao i na generaciji slika na ImageNet-64 (3,43 protiv 3,44 bita/tamno) dok koristi manje slojeva samopouzdanja. Osim toga, postavili smo novo stanje umjetnosti na novom oslobođenom setu podataka PG-19, dobivajući kompleksnost ispitivanja od 33,2 s model 22 slojeva putovanja transformera obučen na sekvencijama dužine 8192. Otvorimo šifru za rutiranje transformera u Tensorflow. 1', 'da': 'Selvopmærksomhed er for nylig blevet vedtaget for en bred vifte af sekvensmodelleringsproblemer. På trods af sin effektivitet lider selvopmærksomhed af kvadratisk beregning og hukommelse krav med hensyn til sekvenslængde. Succesfulde tilgange til at reducere denne kompleksitet fokuserede på at tage sig af lokale skydevinduer eller et lille sæt placeringer uafhængigt af indhold. Vores arbejde foreslår at lære dynamiske sparsomme opmærksomhedsmønstre, der undgår at allokere beregning og hukommelse til at beskæftige sig med indhold, der ikke er relateret til spørgsmålet om interesse. Dette arbejde bygger på to forskningslinjer: Det kombinerer modelleringsflejligheden af tidligere arbejde med indholdsbaseret sparsom opmærksomhed med effektivitetsgevinster fra tilgange baseret på lokal, tidsmæssig sparsom opmærksomhed. Vores model, Routing Transformer, udstyrer selvopmærksomhed med et sparsomt routingmodul baseret på online k-midler, samtidig med at den samlede kompleksitet af opmærksomhed på O(n1.5d) fra O(n2d) for sekvenslængde n og skjult dimension d. Vi viser, at vores model overgår sammenlignelige sparsomme opmærksomhedsmodeller på sprogmodellering på Wikitext-103 (15,8 vs 18,3 forvirring), samt på billedgenerering på ImageNet-64 (3,43 vs 3,44 bits/dim), mens der bruges færre selvopmærksomhedslag. Derudover har vi sat et nyt state-of-the-art på det nyligt udgivet PG-19 datasæt, der opnår en test forvirring på 33.2 med en 22 lags Routing Transformer model trænet på sekvenser af længde 8192. Vi åbner koden til Routing Transformer i Tensorflow. 1', 'nl': 'Zelfaandacht is recentelijk overgenomen voor een breed scala aan sequentiemodelingsproblemen. Ondanks zijn effectiviteit heeft zelfaandacht te lijden onder kwadratische berekening en geheugenvereisten met betrekking tot sequentielengte. Succesvolle benaderingen om deze complexiteit te verminderen concentreerden zich op lokale schuiframen of een kleine set locaties onafhankelijk van de inhoud. Ons werk stelt voor om dynamische schaarse aandachtspatronen te leren die voorkomen dat berekeningen en geheugen worden toegewezen aan inhoud die niet gerelateerd is aan de vraag van belang. Dit werk bouwt voort op twee onderzoekslijnen: het combineert de modelleringsflexibiliteit van eerder werk aan content-based sparse attentie met de efficiëntiewinsten van benaderingen gebaseerd op lokale, temporele sparse attentie. Ons model, de Routing Transformer, geeft zelfaandacht met een schaarse routingmodule gebaseerd op online k-middelen, terwijl de algehele complexiteit van aandacht voor O(n1.5d) van O(n2d) voor sequentielengte n en verborgen dimensie d wordt verminderd. We tonen aan dat ons model beter presteert dan vergelijkbare schaarse aandachtsmodellen op taalmodellering op Wikitext-103 (15.8 vs 18.3 verwardheid), Zowel op het genereren van afbeeldingen op ImageNet-64 (3.43 vs 3.44 bits/dim) met minder zelfaandacht lagen. Daarnaast hebben we een nieuwe state-of-the-art ingesteld op de onlangs vrijgegeven PG-19 dataset, waardoor een testverwarring van 33.2 werd verkregen met een 22-laags Routing Transformer model getraind op sequenties van lengte 8192. We openen de code voor Routing Transformer in Tensorflow. 1', 'bg': 'Наскоро самовниманието е прието за широк спектър от задачи за моделиране на последователността. Въпреки ефективността си, самовниманието страда от квадратни изчисления и изисквания за памет по отношение на дължината на последователността. Успешните подходи за намаляване на тази сложност се фокусират върху обслужването на местни плъзгащи се прозорци или малък набор от места, независими от съдържанието. Нашата работа предлага да научим динамични модели на рядко внимание, които избягват разпределянето на изчисления и памет, за да се обърне внимание на съдържание, което не е свързано с заявката за интерес. Тази работа се основава на две направления на изследване: Тя съчетава гъвкавостта на моделирането на предишната работа по съдържание на оскъдно внимание с повишаването на ефективността от подходи, базирани на местно, времево оскъдно внимание. Нашият модел, Трансформаторът на маршрутизацията, дарява самовниманието с рядък модул за маршрутизиране, базиран на онлайн к-средствата, като същевременно намалява цялостната сложност на вниманието до О(n1.5г) от О(n2d) за дължина на последователността n и скрито измерение Показваме, че нашият модел превъзхожда сравними модели на рядко внимание при езиковото моделиране на Уикитекст-103 (15.8 срещу 18.3 объркване), както и при генериране на изображения на ImageNet-64 (3.43 срещу 3.44 бита/дим), като се използват по-малко слоеве на самовнимание. Освен това, ние поставихме ново състояние на изкуството върху новоиздадения набор от данни, получавайки тестова перфлексия 33.2 с 22 слоен модел на маршрутизационен трансформатор, обучен на последователности с дължина 8192. Открихме кода за пренасочване на трансформатора в Тенсорфлоу. 1', 'de': 'Selbstaufmerksamkeit wurde vor kurzem für eine Vielzahl von Sequenzmodellierungsproblemen übernommen. Trotz ihrer Effektivität leidet die Selbstaufmerksamkeit unter quadratischen Berechnungen und Speicheranforderungen hinsichtlich der Sequenzlänge. Erfolgreiche Ansätze zur Reduzierung dieser Komplexität konzentrierten sich auf lokale Schiebefenster oder einen kleinen Satz von Orten unabhängig von Inhalten. Unsere Arbeit schlägt vor, dynamische, spärliche Aufmerksamkeitsmuster zu erlernen, die es vermeiden, Berechnungen und Speicher zuzuweisen, um Inhalte zu bearbeiten, die nicht mit der Frage des Interesses zusammenhängen. Diese Arbeit baut auf zwei Forschungslinien auf: Sie kombiniert die Modellierungsflexibilität früherer Arbeiten an inhaltsbasierter spärlicher Aufmerksamkeit mit den Effizienzgewinnen von Ansätzen, die auf lokaler, zeitlicher spärlicher Aufmerksamkeit basieren. Unser Modell, der Routing Transformer, stattet Selbstaufmerksamkeit mit einem spärlichen Routing-Modul aus, das auf Online k-Mittelwerten basiert und gleichzeitig die Gesamtkomplexität der Aufmerksamkeit auf O(n1.5d) von O(n2d) für Sequenzlänge n und versteckte Dimension d reduziert. Wir zeigen, dass unser Modell vergleichbare spärliche Aufmerksamkeitsmodelle bei der Sprachmodellierung auf Wikitext-103 übertrifft (15.8 vs 18.3 Verwirrung), sowie bei der Bildgenerierung auf ImageNet-64 (3.43 vs 3.44 Bits/Dim) mit weniger Selbstaufmerksamkeitsebenen. Zusätzlich haben wir einen neuen Stand der Technik auf den neu veröffentlichten PG-19 Datensatz gesetzt und eine Testverwirrung von 33.2 mit einem 22-Schicht Routing Transformer Modell erhalten, das auf Sequenzen der Länge 8192 trainiert wurde. Wir öffnen den Code für Routing Transformer in Tensorflow. 1', 'id': 'Self-attention has recently been adopted for a wide range of sequence modeling problems.  Meskipun efektifnya, perhatian diri menderita dari komputasi kuadratik dan keperluan ingatan dalam hal panjang urutan. Pendekatan sukses untuk mengurangi kompleksitas ini fokus pada menghadiri jendela sliding lokal atau set kecil lokasi independen dari isi. Pekerjaan kami mengusulkan untuk belajar dinamik pola perhatian yang sedikit yang menghindari mengalokasi komputasi dan ingatan untuk menghadiri konten yang tidak terkait dengan pertanyaan yang menarik. Pekerjaan ini dibangun pada dua garis penelitian: Ini menggabungkan fleksibilitas modeling dari pekerjaan sebelumnya pada konten berdasarkan perhatian yang sedikit dengan keuntungan efisiensi dari pendekatan berdasarkan perhatian lokal, sementara yang sedikit. Model kami, Routing Transformer, memberikan perhatian diri dengan modul routing yang sedikit berdasarkan online k-means sementara mengurangi kompleksitas keseluruhan perhatian ke O(n1.5d) dari O(n2d) untuk panjang urutan n dan dimensi tersembunyi d. Kami menunjukkan bahwa model kami melebihi model perhatian yang sedikit yang dapat dibandingkan pada model bahasa di Wikitext-103 (15.8 vs 18.3 perplexity), ) serta pada generasi gambar di ImageNet-64 (3,43 vs 3,44 bit/dim) sementara menggunakan lebih sedikit lapisan perhatian diri. Selain itu, kami menetapkan state-of-the-art baru pada set data PG-19 yang baru dibebaskan, mendapatkan perplexi tes 33.2 dengan model Routing Transformer 22 lapisan dilatih pada urutan panjang 8192. Kami membuka sumber kode untuk Routing Transformer di Tensorflow. 1', 'ko': '최근 서열 모델링 문제에 자아 관심이 널리 응용되고 있다.비록 자기주의는 효과가 있지만, 서열의 길이는 2차 계산과 기억 수요의 영향을 받는다.이런 복잡성을 낮추는 성공 방법은 로컬 슬라이딩 창이나 내용에 독립된 일부분에 집중된다.우리의 업무는 흥미 조회와 무관한 내용에 관심을 가지지 않도록 동적 희소 주의 모델을 배우는 것을 권장한다.이 작업은 두 가지 연구 노선을 바탕으로 한다. 그것은 내용의 희소한 주의를 바탕으로 하는 이전 작업의 모델링 유연성과 국부적, 시간적 희소한 주의를 바탕으로 하는 방법의 효율 수익을 결합시킨다.우리의 모델, 루트 변환기는 온라인 k-균일치를 기반으로 하는 희소한 루트 모듈에 주의를 부여하고 서열 길이 n과 숨겨진 차원 d에 대한 주의를 O(n2d)에서 O(n1.5d)까지의 전체적인 복잡도를 낮춘다. 우리의 모델은 위키텍스-103 언어 모델링에서 유사한 희소한 주의 모델(15.8 vs 18.3)보다 우수하다는 것을 보여준다.그리고 ImageNet-64에 이미지(3.43대 3.44비트/어두움)를 생성하고 자기중심층을 적게 사용한다.또한 새로 발표된 PG-19 데이터 세트에 8192 길이의 시퀀스에서 훈련된 22층 라우팅 변압기 모델을 통해 33.2의 테스트 복잡도를 얻었다.Tensorflow에서 루트 변환기의 원본 코드를 열었습니다.1', 'fa': 'اخیرا توجه خودخواهی برای مشکلات مدل\u200cسازی گسترده شده است. با وجود موثرتش، توجه خودش از محاسبات چهارداریک و نیازهای حافظه با ارتباط به طول ردیابی درد می\u200cیابد. دسترسی موفقیت برای کاهش این پیچیدگی روی حضور به پنجره\u200cهای تخفیف محلی یا مجموعه کوچک محل\u200cهای مستقل از محتویات تمرکز شده است. کار ما پیشنهاد می\u200cدهد که الگوهای توجه دینامیک را یاد بگیریم که از تقسیم محاسبات و حافظه جلوگیری کنند تا به محتویات متصل به پرسش علاقه\u200cای ندارند. این کار بر روی دو خط تحقیقات ساخته می\u200cشود: این مدل\u200cسازی flexibility of prior work on content-based spare attention را ترکیب می\u200cکند با کمک\u200cهای موثرتی از نزدیک\u200cهای بر روی توجه محلی، فعالیت\u200cهای فعالیت. مدل ما، ترجمه\u200cکننده مسیر، توجه خود را با یک مدل مسیر کمی بر اساس k-means آنلاین می\u200cدهد، در حالی که کل پیچیدگی توجه به O(n1.5d) از O(n2d) برای طول مدت n و اندازه مخفی d را کاهش می\u200cدهد. نشان می\u200cدهیم که مدل ما مدل توجه کمی بر روی مدل\u200cهای نسبت به زبان\u200cسازی در ویکیمتک-103 (15.8 vs 18.3 پیچیدگی) بیشتر انجام می\u200cده همچنین در نسل تصویر روی ImageNet-64 (3.43 vs 3.44 bits/dim) در حال استفاده از لایه\u200cهای کمتر توجه به خود. به اضافه، ما یک وضعیت جدید هنر را روی مجموعه داده های PG-19 جدید آزاد کرده ایم، که با یک مدل ۲۲ طبقه مسیر تغییر دهنده آموزش داده شده در مجموعه طولانی 8192 دریافت می\u200cکنیم. ما کد روت تغییرگر در تنسورفلوژ را باز می کنیم. ۱', 'sw': 'Hivi karibuni kujitegemea imechukuliwa kwa matatizo mengi ya mifano. Pamoja na ufanisi wake, kujitegemea huguswa na mahitaji ya ukarabati na kumbukumbu kwa kuheshimu kiwango kikubwa. Matokeo ya mafanikio ya kupunguza utata huu wenye lengo la kuhudhuria kwenye viwanja vya usafiri nchini humo au seti ndogo ya maeneo yenye maudhui huru. Kazi yetu inapendekeza kujifunza mifano ya kuchochea hisia za msimamo ambao huepusha kusambaza hisabati na kumbukumbu ili kuhudhuria maudhui yasiyokubalika na utafiti wa maslahi. Kazi hii inajenga kwenye mistari miwili ya utafiti: Inaunganisha ubunifu wa kazi ya kabla kwa ajili ya uchunguzi wa vifaa vya maudhui kwa ufanisi unaopokea kutoka kwenye mitazamo inayohusiana na mwangalizi wa muda wa muda. Mfano wetu, The Routing Transfer, unajishughulikia n a kifaa cheche cha kuongoza kwa njia ya mtandaoni wakati ukipunguza mfululizo wa ujumla wa kusikiliza O(n1.5d) kutoka O(n2d) kwa lengo lenye urefu na kujificha. Tunaonyesha kuwa modeli yetu inaonyesha mifano inayofanana na mifano ya uchunguzi unaofanana kwenye mifano ya kuonyesha kwa lugha ya Wikitext-103 (15.8 vs 18.3 na tatizo), Vilevile katika kizazi cha picha kwenye mtandao wa ImageNet-64 (3.43 vs 3.44 biti/dim) wakati wakitumia vipande wachache vya kujitegemea. Zaidi ya hayo, tuliweka hali mpya ya sanaa juu ya seti ya data mpya iliyotolewa na PG-19, kupata mtihani wa kutia wasiwasi wa daraja 33.2 na mtindo wa Sauti ya Kupitia Mitandao 22 ulioelekezwa kwa mfululizo wa urefu wa miaka 8192. Tunafunguza kanuni ya Kupitia Uchunguzi katika Tensorflow. 1', 'tr': 'Soňra özüne üns berilýär birnäçe dizim modleme meseleleri üçin. Iýetleşmeligine rağmen, özüniň ünsüni kvadratik hasaplamada we hatyra gerekliklerinden çykar. Bu çyglaýyşy ýerleşdirip geçirmek üçin gadyry ýeterlik äpişgelerini azaltmak üçin başarıylar. Çalışmalarymyz dinamik wajyp öwrenmegi teklip edip, hatyramyz geňleýän soragy üçin gatnaşmakdan uzaklaşanoklar. Bu işe iki çyzgyň araştyrmasynda inşa edýär: bu işiň öňki işiň fleksibilitasyny daşarylýan maksadyň üstünde etkinlik gazanlyklaryny yerel we zamaly gaýd etmek üçin birleşýär. Biziň modelimiz, Marşrut Transformer, online k-meýd a n çasyna d a ýanýa n özüne özüni üns berýär we O(n1.5d) O(n2d) d öwletlerden n we gizlenen bölümden üçin üns gelmegi üçin azaltýar. Biziň modelimiz Wikitext-103 (15.8 we 18.3 görkezilişinde) ýakynlaşykly üns modelslerimizi çykarýar, ImageNet-64 (3.43 we 3.44 bits/dim) surat döwletlerine ýaly Beýleki, täze sereden PG-19 data düzeninde täze bir sanat taýýarlapdyk. 33.2 katmanyň bir 22 katmanyň ýörünji terjime modelini 8192 uzlyk düzenlerde okuwýardy. Biz Tensoraklyda ýöretmäge terjime etmek üçin çeşme açýarys. 1', 'sq': 'Vetëvëmendja është miratuar kohët e fundit për një gamë të gjerë problemesh të modelimit të sekuencës. Pavarësisht nga efektshmëria e saj, vetë-vëmendja vuan nga llogaritjet kuadratike dhe kërkesat e kujtesës lidhur me gjatësinë e sekuencës. Përqasjet e suksesshme për të reduktuar këtë kompleksitet u përqëndruan në ndjekjen e dritareve lokale të rrëshqitjes ose një grup të vogël vendesh të pavarura nga përmbajtja. Puna jonë propozon të mësojmë modele dinamike të pakta vëmendje që shmangen përcaktimin e llogaritjeve dhe kujtesës për të ndjekur përmbajtjen e jo të lidhur me pyetjen e interesit. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention.  Modeli ynë, Routing Transformer, jep vetë vëmendjen me një modul të shkurtër routing bazuar n ë mjetet online k-n d ërkohë që redukton kompleksitetin e përgjithshëm të vëmendjes në O(n1.5d) nga O(n2d) për gjatësinë e sekuencës n dhe dimensionin e fshehur d. Ne tregojmë se modeli ynë ekziston modele të shkurtër të përballueshme të vëmendjes në modelimin gjuhësor në Wikitext-103 (15.8 vs 18.3 perplexity), Si dhe në gjenerimin e imazheve në ImageNet-64 (3.43 vs 3.44 bit/dim) duke përdorur më pak shtresa vetë-vëmendjeje. Përveç kësaj, ne vendosëm një gjendje të re të teknologjisë në grupin e të dhënave PG-19 të sapo lëshuar, duke marrë një test perplexiteti 33.2 me një model 22 shtresa Routing Transformer të stërvitur në sekuencat e gjatësisë 8192. Ne hapim kodin për Transformuesin e Rrugës në Tensorflow. 1', 'am': 'በቅርብ ጊዜ ለብዙው ተቃውሞ የመተላለፊያ ጉዳዮች የራሳቸውን ትኩረት ተወሰደ፡፡ ምንም እንኳን ፍጥረቱ ቢሆንም የራሳቸው ትኩረት በቁጥጥር እና ማስታሰቢያ ግንኙነት በቁጥጥር ላይ ነው፡፡ ቦታ ሥራችን ለጥያቄ ጥያቄ እና ማስታወቂያውን ለመማር የሚችል የውስጥነት ጥያቄ ለመጠየቅ ማቀናቀል ነው፡፡ ይህ ሥራ በሁለት የመረጃ መስመር ላይ ይሠራል፡፡ ሞዴሌያችን፣ የRouting Transfer፣ የበይነመረብ ክፍል ላይ የተመሳሳይ የኮምፕዩተር አካባቢ እና የO(n1.5d) የሙሉ ጥንካሬነትን ከO(n2.5d) ለቁጥር ርዝመት እና የተሰወረውን መጠቀሚያ እንዲያሳያል፡፡ Net-64 (3.43 vs 3.44 bits/dim) በተጨማሪው የምስል ትውልድ ላይ በተደረገ ጊዜ ራሳቸውን በመጠቀም ላይ ያነሳል፡፡ በተጨማሪም፣ አዲስ የ-የ-አርእስት ሥርዓት አዲስ አዲስ አርእስት ላይ አቆምተናል፤ በቁጥር 8192 ላይ የተማረከውን የ33.2 ደረጃን መፈትነቱን አግኝተናል፡፡ በቴንስሮፕሎን ውስጥ የRouting Transformer ኮድ ክፈት እናደርጋለን፡፡ 1', 'hy': 'Ինքնաուշադրությունը վերջերս ընդունվել է մի շարք հաջորդականության մոդելավորման խնդիրների համար: Չնայած դրա արդյունավետությանը, ինքնաուշադրությունը տառապում է քառակուսի հաշվարկումների և հիշողության պահանջներից հաջորդականության երկարության հարաբերությամբ: Այս բարդությունը նվազեցնելու հաջողակ մոտեցումները, որոնք կենտրոնացված են տեղական սլայդի պատուհանների կամ պարունակությունից անկախ մի փոքրիկ տեղափոխությունների մասնակցության վրա: Մեր աշխատանքը առաջարկում է սովորել դինամիկ հազվադեպ ուշադրության կաղապարներ, որոնք խուսափում են հաշվարկների և հիշողության բաժանելու, որպեսզի զբաղվեն պարունակությամբ, որը կապված չէ հետաքրքրության հարցով: Այս աշխատանքը հիմնված է երկու հետազոտությունների վրա. այն համադրում է նախորդ աշխատանքի մոդելավորման ճկունությունը պարունակության վրա հիմնված քիչ ուշադրություն և տեղական, ժամանակական քիչ ուշադրության վրա հիմնված մոտեցումների արդյունավետությունը: Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), - ինչպես նաև պատկերների ստեղծման վրա (3.43 համեմատած 3.44 բիթ: Ավելին, մենք նոր տեխնոլոգիաներ ենք տեղադրել նորից հրատարակված ԳԳ-19 տվյալների համակարգի վրա, ստանալով 33.2-ի փորձարկման խառնաշփոթ 22 շերտի ճանապարհի վերափոխման մոդելի, որը ուսուցանվում է 8192 երկարության հաջորդականությամբ: Մենք բաց կոդի ենք տալիս տենսորհումսի մեջ տեղափոխվող ճանապարհի կոդին: 1', 'af': "Selfaandag is onlangs aangeneem vir 'n wye reek van sekwensiemodeling probleme. Terwyl sy effektiviteit, lyk self-aandag van kwadratik rekenaar en geheue benodigte met respek na sekwensielengte. Suksesfol toegang om hierdie kompleksiteit te verminder wat gefokus is op by by by plaaslike sliding vensters of 'n klein stel liggings onafhanklik van inhoud. Ons werk stel voorstel om dinamiese sparse aandag patrone te leer wat voorkom om rekenaar en geheue te alloseer om by inhoud wat onverwante is aan die navraag van belang te bied. Hierdie werk bou op twee lyn van ondersoek: Dit kombinieer die modellering fleksibiliteit van vooraf werk op inhoud-gebaseerde sparse aandag met die effektiviteit verskaf van toegange gebaseer op plaaslike, tydelike sparse aandag. Ons model, die Routing Transformer, gee self-aandag met 'n sparse routing module gebaseer op online k-beteken terwyl die hele kompleksiteit van aandag n a O(n1.5d) af van O(n2d) vir sekwensielengte n en verborge dimensie d. Ons wys dat ons model uitvaar vergelykbare sparse aandag model op taal modellering op Wikitext-103 (15.8 teen 18.3 perpleksie), Maar ook op beeldgenerasie op ImageNet-64 (3.43 teen 3.44 bits/dim) terwyl minder self-aandag laag gebruik word. In addition, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. Ons open-bron die kode vir Routing Transformer in Tensorflow. 1", 'bs': 'Nedavno je prihvaćena sama pažnja za širok niz problema modeliranja sekvencija. Unatoč njenoj učinkovitosti, samopouzdanje pati od kvadratičnog računala i zahtjeva pamćenja u vezi dužine sekvencije. Uspješni pristupi smanjivanju ove kompleksnosti fokusirani na prisustvu lokalnih prozora klizanja ili malih set a lokacija nezavisnih od sadržaja. Naš rad predlaže naučiti dinamične rezervne obrasce pažnje koje izbjegavaju allociranje računala i pamćenja da prisustvuju sadržaju bez veze sa pitanjem interesa. Ovaj rad se osnova na dvije linije istraživanja: Kombinuje fleksibilnost modeliranja prethodnog rada na temelju sadržaja rezervne pažnje sa učinkovitostima od pristupa na temelju lokalne, privremene rezervne pažnje. Naš model, putujući transformator, podržava samopouzdanje rezervnim modulom rutiranja baziranim n a internetskim k-medijima dok smanjuje ukupnu kompleksnost pažnje na O(n1.5d) iz O(n2d) za dužinu sekvence n i skrivenu dimenziju d. Pokazujemo da n a š model iznosi usporedbene rezervne pažnje modele jezika na Wikitext-103 (15,8 protiv 18,3 kompleksnost), Kao i na generaciji slika na ImageNet-64 (3.43 protiv 3.44 bita/tamno) dok koristi manje slojeva samopouzdanja. Osim toga, postavili smo novo stanje umjetnosti na novom oslobođenom setu podataka PG-19, dobivajući kompleksnost test a od 33,2 sa 22 slojnim modelom transformera koji je obučen na sekvencijama dužine 8192. Otvorimo šifru za rutiranje transformera u Tensorflow. 1', 'cs': 'Sebeobecná pozornost byla v poslední době přijata pro širokou škálu problémů modelování sekvencí. Navzdory své účinnosti trpí sebepozornost kvadratickým výpočtem a požadavky na paměť s ohledem na délku sekvence. Úspěšné přístupy ke snížení této složitosti se zaměřily na péči o lokální posuvná okna nebo malou sadu míst nezávislých na obsahu. Naše práce navrhuje naučit se dynamické řídké vzorce pozornosti, které se vyhnou přidělování výpočtů a paměti k obsahu, který nesouvisí s dotazem zájmu. Tato práce staví na dvou liniích výzkumu: kombinuje modelování flexibility předchozí práce na obsahově založené řídké pozornosti s efektivními přístupy založenými na lokální, časově řídké pozornosti. Náš model, směrovací transformátor, vybavuje sebe-pozornost řídkým směrovacím modulem založeným na on-line k-středních prostředcích a současně snižuje celkovou složitost pozornosti O(n1.5d) z O(n2d) pro délku sekvence n a skrytý rozměr d. Ukazujeme, že náš model překoná srovnatelné řídké pozornosti modelů na jazykovém modelování na Wikitextu-103 (15.8 vs 18.3 zmatenost), stejně jako na generování obrazu na ImageNet-64 (3.43 vs 3.44 bity/dim) při použití menšího množství vrstev sebe-pozornosti. Kromě toho jsme na nově vydané sadě dat PG-19 nastavili nový state-of-the-art a získali jsme zkušební zmatenost 33.2 s 22 vrstvým modelem směrovacího transformátoru trénovaným na sekvencích délky 8192. Otevřeli jsme kód pro Routing Transformer v Tensorflowu. 1', 'az': 'Öz dikkati yenidən çox sıralama modellik problemləri üçün qəbul edildi. Növbəti olmasına rağmen, özünün təhlükəsi kvadratik hesaplama və yaddaş ehtiyaçlarına gəlir. Bu qarışıqlığı yerli slim pəncərələrinə qovuşdurmaq üçün müvəffəqiyyətini azaltmaq üçün müvəffəqiyyəti olaraq yaxınlaşır. Bizim işimiz dinamik məlumatları öyrənməyə təklif edir ki, hesablama və yada salmaqdan uzaqlaşan məlumatlara bağlı olmayan məlumatlara istifadə etməkdən uzaqlaşarlar. Bu iş iki sətir araştırma üstündə inşa edir: Bu, əvvəlki işlərin modelləşdirilməsini, içərisində mövcud olan təsirli təsirli təsiri ilə yerli, müddətli təsirli təsirlərin təsirlərindən təsirli qazanmaqla birləşdirir. Bizim modelimiz, Routing Transformer, o(n2d) seçmə uzunluğu n və gizli ölçü d üçün O(n1.5d) və o(n2d) seçmə müd d ətini O(n1.5d) ilə dəyişdirir. Bizim modelimiz Wikitext-103 (15.8 vs 18.3 müddəti) dil modellərinə qarşılaşdırılabilir müddətli dikkati modellərini göstərir. ImageNet-64 (3.43 vs. 3.44 bits/dim) surat nəsilində də daha az self-attention layers istifadə edərkən. Üstəlik, yeni yayınlanmış PG-19 veri qutusunda yeni bir sanatın durumu təyin etdik, 33.2 ilə 22 katlı yol transformer modeli 8192 uzunluğunda təhsil edilmişdir. Biz Tensorflow\'da Routing Transformer üçün kodu açarıq. 1" (msgctxt: "panel:showusername") to "1', 'bn': 'সম্প্রতি স্বেচ্ছায় আত্মমনোযোগ গ্রহণ করা হয়েছে বিশাল বিভিন্ন ধরনের মডেলিং সমস্যার জন্য। তার কার্যকলাপ সত্ত্বেও কোয়াড্রাটিক গণনা এবং স্মৃতির প্রয়োজন থেকে আত্মমনোযোগ পেয়েছে যেখানে সংক্রান্ত দীর্ স্থানীয় স্লাইডিং উইন্ডোতে অংশগ্রহণ করার জন্য এই জটিলতা কমানোর জন্য সফল উপায় বা স্থানীয় বিষয়বস্তুর স্বাধীন স্থানের একটি ছো আমাদের কাজের প্রস্তাব প্রস্তাব করেছে যে তারা স্বার্থের অনুসন্ধানের প্রশ্নে বিচ্ছিন্ন বিষয়বস্তুতে অংশগ্রহণ করার বিষয়বস্ত এই কাজ দুটি গবেষণার মাধ্যমে তৈরি করে: এটি পূর্ববর্তী কাজের মডেলিং ফ্লিক্সিটিভিস্ট ভিত্তিক কাজের মাধ্যমে স্থানীয়, সাময়িক স্প্যান্সে আমাদের মডেল, রুটিং ট্রান্সফার, অনলাইন কি মানে ভিত্তিক একটি স্মার্চ রুটিং মডিউলের সাথে নিজেকে মনোযোগ প্রদান করেছে এবং O( n1. 5d) থেকে সামান্য দীর্ঘদিন এবং লুকিয়ে রাখা মাত্রার জন্য অনলাইনের মাধ্যমে। আমরা দেখাচ্ছি যে আমাদ ছবির প্রজন্ম ইমেজেন্ট-64 (৩. 43 vs ৩. ৪৪ বিট/ডিম) এবং সাথে ছবির প্রজন্মে কম আত্মমনোযোগ স্তর ব্যবহার করে। এছাড়াও, আমরা নতুন পিজি-১৯ তথ্য সেটে নতুন একটি রাষ্ট্র-অফ-শিল্প নির্ধারণ করেছি, যা ৩৩. টেনসারফ্লুয়ারে রুটিং ট্রান্সফ্রান্সফ্রান্সের কোড খোলা যায়। 1', 'fi': "Itsehuomio on hiljattain hyväksytty monenlaisiin sekvenssimallintaongelmiin. Tehokkuudestaan huolimatta itsehuomio kärsii kvadraattisesta laskennasta ja muistivaatimuksista sekvenssin pituuden suhteen. Onnistuneet lähestymistavat tämän monimutkaisuuden vähentämiseksi keskittyivät paikallisiin liukuikkunoihin tai pieniin sisältöön riippumattomiin paikkoihin. Työssämme on tarkoitus oppia dynaamisia harvoja huomiomalleja, jotka välttävät laskennan ja muistin kohdentamista kiinnostavaan kyselyyn liittymättömään sisältöön. Tämä työ perustuu kahteen tutkimuslinjaan: Siinä yhdistyvät sisältöön perustuvan harvaan huomioon perustuvan aikaisemman työn mallinnusjoustavuus ja paikalliseen, ajalliseen harvaan huomioon perustuvien lähestymistapojen tehokkuusedut. Mallimme, reititysmuuntaja, antaa itsehuomion niukalla reititysmoduulilla, joka perustuu online k-keskiarvoihin ja vähentää huomion yleistä monimutkaisuutta O(n1.5d) O(n2d) sekvenssipituuden n ja piilotetun ulottuvuuden d osalta. Osoitamme, että mallimme suoriutuu paremmin kuin vastaavat harvat huomiomallit kielimallinnuksessa Wikitext-103 (15.8 vs 18.3 perplexity). Kuvan tuottamiseen ImageNet-64:llä (3,43 vs 3,44 bittiä/himmeä) samalla kun käytetään vähemmän itsetuntokerroksia. Lisäksi asetimme uuden huippuluokan PG-19-aineistoon, jolloin testihämmennys oli 33,2 22-kerroksinen reititysmuunnin malli, joka on koulutettu pituudeltaan 8192. Avaamme koodin reititysmuuntajalle Tensorflow'ssa. 1", 'ca': "Recentment s'ha adoptat l'autoatenció per a una gran varietat de problemes de modelació de seqüències. Malgrat la seva eficiència, l'autoatenció pateix els requisits quadràtics de computació i memòria en relació a la llargada de la seqüència. Els enfocaments exitosos per reduir aquesta complexitat van centrar-se en atendre a finestres deslizants locals o a un petit conjunt de llocs independents del contingut. La nostra feina propon aprendre patrons dinàmics d'atenció escassos que eviten asignar computació i memòria per atendre al contingut no relacionat amb la pregunta d'interès. Aquesta feina es basa en dues línies d'investigació: Combina la flexibilitat de modelació de la feina anterior en continguts poca atenció amb els guanys d'eficiència d'enfocaments basats en atenció local i temporal poca. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), Com també en la generació d'imatges a ImageNet-64 (3,43 vs 3,44 bits/dim) mentre utilitzen menys capes d'auto-atenció. A més, vam posar una nova tecnologia en el nou conjunt de dades PG-19, obtenint una perplexitat de prova de 33,2 amb un model de transformador de ruta de 22 capes entrenat en seqüències de longitud 8192. El codi de codi obert de Routing Transformer a Tensorflow. 1", 'et': "Enesetähelepanu on hiljuti vastu võetud paljude jadade modelleerimise probleemide jaoks. Hoolimata oma efektiivsusest kannatab enesetähelepanu ruutkarvutuse ja mälu nõuded seoses jada pikkusega. Edukad lähenemisviisid selle keerukuse vähendamiseks keskendusid kohalike lükandeaknade või väikeste sisust sõltumatute kohtade hooldamisele. Meie töö teeb ettepaneku õppida dünaamilisi hõredaid tähelepanu mustreid, mis vältivad arvutuste ja mälu eraldamist huvi pakkuva päringuga mitteseotud sisule. See töö põhineb kahel uurimisliinil: see ühendab sisupõhise hõreda tähelepanu varasema töö modelleerimise paindlikkuse ja kohalikul ajalisel hõredal tähelepanul põhinevate lähenemisviiside tõhususe suurenemise. Meie mudel, marsruutimismuundur, annab enesele tähelepanu hõreda marsruutimismooduli, mis põhineb online k-keskmistel, vähendades samas tähelepanu üldist keerukust O(n1.5d) jaoks O(n2d) järjestuse pikkuse n ja varjatud dimensiooni d jaoks. Me näitame, et meie mudel ületab võrreldavaid hõreda tähelepanu mudeleid keele modelleerimisel Wikitext-103 (15.8 vs 18.3 segadus). Samuti piltide genereerimisel ImageNet-64-l (3,43 vs 3,44 bitti/tumedus), kasutades samal ajal vähem enesetähelepanu kihte. Lisaks seadsime uue tehnika taseme äsja väljastatud PG-19 andmekogumile, saavutades testisegaduse 33,2 22 kihilise marsruuditeformeri mudeliga, mis on treenitud pikkusega 8192 järjestustel. Avasime Tensorflow'i suunamistransformaatori koodi. 1", 'sk': 'Pred kratkim je bila sprejeta samopozornost za širok spekter problemov modeliranja zaporedja. Kljub svoji učinkovitosti samopozornost trpi zaradi kvadratnega računalništva in pomnilniških zahtev glede dolžine zaporedja. Uspešni pristopi za zmanjšanje te kompleksnosti so se osredotočili na obisk lokalnih drsnih oken ali majhnega nabora lokacij, neodvisnih od vsebine. Naše delo predlaga učenje dinamičnih redkih vzorcev pozornosti, ki se izogibajo dodeljevanju računalništva in pomnilnika za vsebine, ki niso povezane z poizvedbo zanimanja. Delo temelji na dveh raziskovalnih linijah: združuje fleksibilnost modeliranja predhodnega dela na vsebinski osnovi redke pozornosti s povečanjem učinkovitosti pristopov, ki temeljijo na lokalni, časovni redki pozornosti. Naš model, transformator usmerjanja, daje samopozornost redkim modulom usmerjanja, ki temelji na spletnih k-sredstvih, hkrati pa zmanjšuje celotno kompleksnost pozornosti na O(n1.5d) iz O(n2d) za dolžino zaporedja n in skrito dimenzijo d. Pokažemo, da naš model presega primerljive modele redke pozornosti pri modeliranju jezikov na Wikibesedilu-103 (15,8 vs 18,3 zmedenost), kot tudi pri ustvarjanju slik na ImageNet-64 (3,43 v primerjavi s 3,44 biti/zatemnitev), pri čemer uporabljate manj plasti samopozornosti. Poleg tega smo postavili nov najsodobnejši nabor podatkov PG-19, pri čemer smo pridobili preskusno zmedenost 33,2 z 22-slojnim modelom usmerjanja transformatorja, usposobljenim na zaporedjih dolžine 8192. Odprli bomo kodo za usmerjanje transformatorja v Tensorflow. 1', 'ha': "A yanzu an zãɓi kanayen farat ɗaya wa masu matsalar masu motsi masu tsawo. Inã rantsuwa da fassararsa, masu jiɓintar rãyukansu yana jin ƙidãyar hisãbi na akan lokaci da kuma ma'anar tunãtarwa masu husu da tsawo. @ info: whatsthis Kayan aikinMu na ƙari da za'a sanar da misãlai na saurari masu saurãre masu yin sauri da kuma don su raba lissafar da tunãtarwa dõmin su shiga cikin takardar da ba'a sami zuwa tambayin riba ba. Wannan aikin yana samar da misalin biyu na fassarar research: Yana haɗa motsar fleksibinci wa zaman aikin a kan muhimmi-muhimmi da fassarar-muhimmanci daga matsayi, a kan muhimmi mai lokaci. Misalinmu, shirin Routin Transformer, yana yarda d a kansa da wata shirin shiryarwa mai ƙaranci a k a n ƙananan k-ma'anar kwamfyuta da zai ƙara muhimmin muhimmada ga O(n1.5d) daga O(n2d) ga tsawo na yi tsawo da kuma a ɓõye. Tuna n ũna cewa misalinmu yana samar da misãlai masu sami da misalin ƙaranci a kan harshen Wikittext-103 (15.8 vers 18.3 masu shagala), Net-64 (3.43 vers 3.44 bits/dim) sami da ke amfani da zane-zane masu ƙaranci. Ina ƙaranci, za'a daidaita wani halin-sanar da aka saka a yanzu-da-yanzu PG-19, kana sami wata jarraba masu haɗi da matsayin 33.2 da wani motel mai 22 na Routin Transformer wanda aka yi wa wa duran tsawo 8192. Tuna buɗe kodi na Shirin Rutsurin na Tafiyar da. 1", 'he': 'תשומת לב עצמית אומץ לאחרונה לטווח רחב של בעיות דוגמניות רצף. למרות יעילותו, תשומת לב עצמית סובלת מחשב רביעי ודרישות זיכרון בנוגע לאורך רצף. גישות מצליחות כדי להפחית את המורכבות הזו ממוקדות על להשתתף בחלונות מחליקים מקומיים או קבוצה קטנה של מקומות ללא תוכן. העבודה שלנו מציעה ללמוד דפוסי תשומת לב דמיוניות שממנעות ממחלקת מחשב וזיכרון כדי לטפל בתוכן שלא קשור לבקשה של עניין. העבודה הזאת בונה על שני קווים של מחקר: היא משלבת את הגמילות הדוגמנית של עבודה קודמת על תשומת לב מובססת בתוכן נמוכה המודל שלנו, הטרנספורטר המסלול, נותן תשומת לב לעצמנו מודול טיפול נדיר מבוסס על אמצעי k באינטרנט בזמן שמפחית את המורכבות הכללית של תשומת לב O( n1. 5d) מאורך N( n2d) לאורך רצף ומימד מוסתר d. אנו מראים שהמודל שלנו מוביל מודלים תשומת לב נדיר שווים על מודל שפה בוויקיטקסט- 103 (15. 8 vs 18. 3 בלגשות), ) כמו גם על יוצר תמונות על ImageNet-64 (3.43 נגד 3.44 ביטים/דים) בזמן ששימוש פחות שכבות תשומת לב עצמית. בנוסף, קבענו מצב חדש של האמנות על קבוצת נתונים PG-19 שחררה לאחרונה, להשיג מבחן מעורבות של 33.2 עם דוגמא 22 שכבות של מעבר דרכים מאומנת על רצפים של אורך 8192. אנחנו פותחים את הקוד לטרנספורטר בטנסורפלום. 1', 'jv': 'Self-Attribute Ngayon nguasai efek, sampeyan nganggo sampeyan kuwi kaluwargane soko kaluwargane gambar lan ingkang dipunangke bataran Sugeng success Workspace %d Awakdhéwé iki nggawe nguasakno perusahaan iki: iso ngubah akeh perusahaan winih sing apik perusahaan ning aturan sing paling nggawe barang nggawe barang apik perusahaan winih dhéwé suku winih sing dibutuhke sakjane nguasakno dianggap Awakdhéwé model, the routing Transformer, idéwé nggawe Selah-atensi karo partis routing module basa online k-means nggawe nguasai nggawe komplikasi nggawe atensi nggo O(n1.5 d) kanggo O(n2d) kanggo nganggo langgar n lan hidden dimensi d. images-action Mungkin daftar, kita mulai perpliksi barêng-sampek sing gawe ngubah Algorithm 19 Awakdhéwé Open-source nggo kode kanggo Ketokaké Transformer nang TensFlow 1', 'bo': 'རང་ཉིད་ཀྱིས་རང་ཉིད་དུས་ཡོད་ཚད་གྲལ་རིམ་དབྱེ་བའི་དཀའ་ངལ་ཞིག་བྱེད་ཚར་བ Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds on two lines of research: It combines the modeling flexibility of prior work on content-based attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), The image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using less self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. This is the first part of the journey. ང་ཚོས་Tensorflow ནང་དུ་འགྲུལ་སྐྱོད་མེད་པའི་འགྲུལ་ལམ་ལུགས་ཀྱི་ཨང་ཨང་ཁ་འབྱེད་པ 1'}
{'en': 'Deciphering Undersegmented Ancient Scripts Using Phonetic Prior', 'ar': 'فك رموز النصوص القديمة غير المجزأة باستخدام صوتي مسبق', 'fr': "Déchiffrer des scripts anciens sous-segmentés à l'aide de Phonetic Prior", 'pt': 'Decifrando Scripts Antigos Subsegmentados Usando Prioridade Fonética', 'es': 'Descifrar escrituras antiguas subsegmentadas mediante el prior fonético', 'hi': 'ध्वन्यात्मक पूर्व का उपयोग कर Undersegmented प्राचीन लिपियों को समझना', 'zh': '用语音先验破译分段不足古文', 'ja': '未分節の古代スクリプトを音声学的に解読する', 'ru': 'Расшифровка недосегментированных древних скриптов с использованием фонетического предшественника', 'ga': 'Scripteanna Sean-Roinne a Dhí-dheighilt ag Úsáid Tosaí Foghraíochta', 'ka': 'Name', 'it': 'Decriperazione di script antichi sottosegmentati utilizzando Priore fonetico', 'el': 'Αποκωδικοποίηση υποσημειωμένων αρχαίων σεναρίων χρησιμοποιώντας Φωνητικό Πρώην', 'hu': 'Alászegmentált ősi szkriptek dekódolása Phonetic Prior használatával', 'kk': 'Фонетикалық алдыңғысын қолданатын артық сегменттік артық скрипттерді шешіру', 'ms': 'Deciphering Undersegmented Ancient Scripts Using Phonetic Prior', 'mk': 'Name', 'lt': 'Deciphering Undersegmented Ancient Scripts Using Phonetic Prior', 'ml': 'ഫോണെറ്റിക് മുന്\u200dഗണന ഉപയോഗിക്കുന്ന പഴയ സ്ക്രിപ്റ്റുകള്\u200d താഴെയിടുന്നു', 'mt': 'Diżifrar ta’ Skrifti Anzjani Sottosegmentati bl-Użu ta’ Qabel Phonetiku', 'pl': 'Odszyfrowanie niepodsegmentowanych skryptów starożytnych za pomocą fonetycznego przedrzenia', 'ro': 'Decriperea scripturilor antice subsegmentate utilizând anteriorul fonetic', 'mn': 'Фонетикийн өмнө ашиглаж буй эртний скриптүүдийг шийдвэрлэх', 'no': 'Desifrerer undersegmenterte gamle skript som brukar fonetisk prioritet', 'so': 'Qoraayinka waayeelka ah ee la xiriiray ee la isticmaalayo Fonetic Prior', 'sr': 'Dešifrovanje podsegmentiranih drevnih skripta koristeći fonetički prioritet', 'si': 'Phonetic Early Uses', 'ta': 'போன்டெடிக் முன்னிருப்பை பயன்படுத்தி குறைந்த பழைய சிறுநிரல்களை குறைக்கவும்', 'sv': 'Avkoda underordnade antika skript med hjälp av fonetic Prior', 'ur': 'فونیٹیکی پہلے استعمال کرتے ہیں', 'vi': 'Giải mã các văn lệnh cũ đã được gài', 'uz': 'Name', 'bg': 'Дешифриране на недостатъчно сегментирани древни скриптове с фонетичен приор', 'hr': 'Dešifriranje podsegmentiranih drevnih skripta koristeći fonetički prioritet', 'da': 'Afkodning af undersegmenterede gamle scripts ved hjælp af fonetic Prior', 'nl': 'Ondersegmenteerde oude scripts ontcijferen met behulp van fonetische prior', 'de': 'Entschlüsseln untersegmentierter alter Skripte mit Phonetic Prior', 'id': 'Menghapus Skrip Lama Terbawah Segmen Menggunakan Sebelumnya Phonetic', 'fa': 'Ш§ШіШҒЩЃШ§ШҮЩ‡ Ш§ШІ ЩңЫЊШөЫЊЩ†Щ‡ ШҒЩ„ЩЃЩ†ЫЊвЂЊ', 'ko': '음성 우선법으로 결단 고문자를 해독하다', 'sw': 'Kupindua Vitabu vya Kizamani vilivyotengwa kwa kutumia Makala ya Picha', 'af': 'Decyptering Ondersegmenteerde Oue Skripte gebruik Phonetic Prior', 'tr': 'Fonetik Öňkini ullanýan Asty Skriptler', 'sq': 'Shkrimi i Skripeve të lashta të nënsegmentuara duke përdorur paraardhësin fonetik', 'hy': 'Comment', 'am': 'ምስሉን በሌላ ስም አስቀምጥ', 'az': 'Fonetik 톛vv톛lkil톛ri istifad톛 ed톛n Eski Skriptl톛r', 'bn': 'ফোনেটিক পূর্ববর্তী ব্যবহার করে অন্তর্ভুক্ত বৈশিষ্ট্য স্ক্রিপ্ট অবতরণ করা হচ্ছে', 'bs': 'Dešifriranje podsegmentiranih drevnih skripta koristeći Phonetic Prior', 'cs': 'Dešifrování podsegmentovaných antických skriptů pomocí fonetického převoru', 'fi': 'Alisegmentoitujen muinaisten komentosarjojen purkaminen Phonetic Priorin avulla', 'ca': 'Deciphering Undersegmented Ancient Scripts Using Phonetic Prior', 'et': 'Alasegmenteeritud iidsete skriptide dekifreerimine foneetilise eelpooli abil', 'jv': 'politenessoffpolite"), and when there is a change ("assertivepoliteness', 'he': 'לפענח תסריטים עתיקים מתמוטטים באמצעות קודם פונטי', 'sk': 'Dešifriranje premalo segmentiranih starodavnih skriptov z uporabo fonetic Prior', 'ha': 'KCharselect unicode block name', 'bo': 'འོད་སྐུད་བྱས་པའི་རྩིས་གཞུང་གི་སྔོན་མའི་སྒྲིག་ཡིག་ཚོགས་སྤྱོད་བཞིན་པ'}
{'en': 'Most undeciphered lost languages exhibit two characteristics that pose significant decipherment challenges : (1) the scripts are not fully segmented into words ; (2) the closest known language is not determined. We propose a decipherment model that handles both of these challenges by building on rich linguistic constraints reflecting consistent patterns in historical sound change. We capture the natural phonological geometry by learning  character embeddings  based on the  International Phonetic Alphabet (IPA) . The resulting  generative framework  jointly models  word segmentation  and cognate alignment, informed by  phonological constraints . We evaluate the model on both deciphered languages (Gothic, Ugaritic) and an undeciphered one (Iberian). The experiments show that incorporating phonetic geometry leads to clear and consistent gains. Additionally, we propose a measure for language closeness which correctly identifies related languages for  Gothic  and  Ugaritic . For  Iberian , the method does not show strong evidence supporting  Basque  as a related language, concurring with the favored position by the current scholarship.1', 'ar': 'تظهر معظم اللغات المفقودة غير المفككة سمتين تطرحان تحديات كبيرة في فك التشفير: (1) النصوص غير مجزأة بالكامل إلى كلمات ؛ (2) لم يتم تحديد أقرب لغة معروفة. نقترح نموذجًا لفك الشفرات يتعامل مع هذين التحديين من خلال البناء على قيود لغوية غنية تعكس أنماطًا متسقة في تغيير الصوت التاريخي. نحن نلتقط الهندسة الصوتية الطبيعية من خلال تعلم حفلات الزفاف على أساس الأبجدية الصوتية الدولية (IPA). يقوم الإطار التوليدي الناتج بشكل مشترك بنماذج تجزئة الكلمات والمحاذاة المماثلة ، المستنيرة بالقيود الصوتية. نقوم بتقييم النموذج على كل من اللغات التي تم فك شفرتها (القوطية والأوغاريتية) وأخرى غير مفككة (الأيبيرية). تظهر التجارب أن دمج الهندسة الصوتية يؤدي إلى مكاسب واضحة ومتسقة. بالإضافة إلى ذلك ، نقترح مقياسًا لتقارب اللغة يحدد بشكل صحيح اللغات ذات الصلة بالقوطية والأوغاريتية. بالنسبة للأيبيرية ، لا تُظهر الطريقة دليلًا قويًا يدعم لغة الباسك كلغة ذات صلة ، بما يتوافق مع الموقف المفضل للمنحة الحالية.', 'es': 'La mayoría de los idiomas perdidos no descifrados exhiben dos características que plantean importantes desafíos de descifrado: (1) los guiones no están completamente segmentados en palabras; (2) no se determina el idioma conocido más cercano. Proponemos un modelo de desciframiento que maneja ambos desafíos basándose en abundantes restricciones lingüísticas que reflejan patrones consistentes en el cambio de sonido histórico. Capturamos la geometría fonológica natural mediante el aprendizaje de incrustaciones de caracteres basadas en el Alfabeto Fonético Internacional (AFI). El marco generativo resultante modela conjuntamente la segmentación de palabras y la alineación afín, informada por restricciones fonológicas. Evaluamos el modelo tanto en lenguas descifradas (gótico, ugarítico) como en lenguas no descifradas (ibérico). Los experimentos muestran que la incorporación de la geometría fonética conduce a ganancias claras y consistentes. Además, proponemos una medida de cercanía lingüística que identifique correctamente los idiomas relacionados con el gótico y el ugarítico. Para el ibérico, el método no muestra pruebas sólidas que respalden el euskera como lengua relacionada, coincidiendo con la posición favorecida por la beca actual.1', 'fr': "La plupart des langues perdues non déchiffrées présentent deux caractéristiques qui posent d'importants défis de déchiffrement\xa0: (1) les scripts ne sont pas entièrement segmentés en mots\xa0; (2) la langue connue la plus proche n'est pas déterminée. Nous proposons un modèle de déchiffrement qui répond à ces deux défis en s'appuyant sur de riches contraintes linguistiques reflétant des modèles cohérents dans les changements sonores historiques. Nous capturons la géométrie phonologique naturelle en apprenant l'intégration des caractères sur la base de l'alphabet phonétique international (API). Le cadre génératif qui en résulte modélise conjointement la segmentation des mots et l'alignement apparenté, en fonction des contraintes phonologiques. Nous évaluons le modèle à la fois sur des langues déchiffrées (gothique, ougaritique) et une langue non déchiffrée (ibérique). Les expériences montrent que l'intégration de la géométrie phonétique conduit à des gains clairs et cohérents. De plus, nous proposons une mesure de proximité linguistique qui identifie correctement les langues apparentées pour le gothique et l'ougaritique. Pour la langue ibérique, la méthode ne montre pas de preuves solides soutenant le basque en tant que langue apparentée, ce qui concordait avec la position privilégiée par la bourse actuelle.", 'pt': 'A maioria das línguas perdidas não decifradas exibe duas características que apresentam desafios significativos de decifração: (1) as escritas não são totalmente segmentadas em palavras; (2) o idioma conhecido mais próximo não é determinado. Propomos um modelo de decifração que lida com esses dois desafios, construindo ricas restrições linguísticas que refletem padrões consistentes na mudança histórica do som. Capturamos a geometria fonológica natural aprendendo a incorporação de caracteres com base no Alfabeto Fonético Internacional (IPA). A estrutura generativa resultante modela conjuntamente a segmentação de palavras e o alinhamento cognato, informado por restrições fonológicas. Avaliamos o modelo em ambas as línguas decifradas (gótica, ugarítica) e uma não decifrada (ibérica). Os experimentos mostram que a incorporação da geometria fonética leva a ganhos claros e consistentes. Além disso, propomos uma medida de proximidade linguística que identifica corretamente idiomas relacionados para gótico e ugarítico. Para o ibérico, o método não apresenta fortes evidências que sustentem o basco como língua afim, concordando com a posição privilegiada pela corrente acadêmica.1', 'ja': 'ほとんどの未解読の失われた言語は、重大な解読課題をもたらす2つの特徴を示す： （ 1 ）スクリプトが単語に完全にセグメント化されていない； （ 2 ）最も近い既知の言語が決定されていない。 歴史的な音変化における一貫したパターンを反映した豊富な言語的制約に基づいて、これらの両方の課題に対処する解読モデルを提案する。 私たちは、国際音韻アルファベット（ IPA ）に基づいて文字埋め込みを学ぶことで、自然な音韻論的幾何学を捉えています。 結果として生じる生成的枠組みは、音声学的制約から情報を得て、単語のセグメンテーションとコグネイトアライメントを共同でモデル化する。 私たちは、解読された言語（ゴシック語、ウガリット語）と解読されていない言語（イベリア語）の両方でモデルを評価します。 実験は、音声幾何学を組み込むことが明確で一貫した利得につながることを示している。 さらに、ゴシック語とウガリット語に関連する言語を正しく識別する、言語の近さの尺度を提案します。 イベリア語については、この方法は関連する言語としてバスク語を支持する強力な証拠を示しておらず、現在の奨学金によって支持されている立場に同意している。1', 'hi': 'अधिकांश अस्पष्ट खोई हुई भाषाएं दो विशेषताओं को प्रदर्शित करती हैं जो महत्वपूर्ण समझ की चुनौतियों का सामना करती हैं: (1) लिपियों को पूरी तरह से शब्दों में विभाजित नहीं किया जाता है; (2) निकटतम ज्ञात भाषा निर्धारित नहीं की जाती है। हम एक समझने के मॉडल का प्रस्ताव करते हैं जो ऐतिहासिक ध्वनि परिवर्तन में सुसंगत पैटर्न को प्रतिबिंबित करने वाले समृद्ध भाषाई बाधाओं पर निर्माण करके इन दोनों चुनौतियों को संभालता है। हम अंतर्राष्ट्रीय ध्वन्यात्मक वर्णमाला (आईपीए) के आधार पर चरित्र एम्बेडिंग सीखकर प्राकृतिक ध्वन्यात्मक ज्यामिति पर कब्जा करते हैं। परिणामी उत्पादक ढांचा संयुक्त रूप से शब्द विभाजन और संज्ञेय संरेखण को मॉडल करता है, जो ध्वन्यात्मक बाधाओं द्वारा सूचित किया जाता है। हम दोनों को समझने वाली भाषाओं (गोथिक, Ugaritic) और एक undeciphered एक (इबेरियन) पर मॉडल का मूल्यांकन करते हैं। प्रयोगों से पता चलता है कि ध्वन्यात्मक ज्यामिति को शामिल करने से स्पष्ट और लगातार लाभ होता है। इसके अतिरिक्त, हम भाषा निकटता के लिए एक उपाय का प्रस्ताव करते हैं जो गोथिक और युगारिटिक के लिए संबंधित भाषाओं की सही पहचान करता है। इबेरियन के लिए, विधि एक संबंधित भाषा के रूप में बास्क का समर्थन करने वाले मजबूत सबूत नहीं दिखाती है, जो वर्तमान छात्रवृत्ति द्वारा इष्ट स्थिति के साथ सहमति देती है।', 'zh': '多未破译失言两征,为破译大破译挑战:(1)脚本未尽分单词; (2)近者知言未定。 吾为一破译模形,以立声变之中,约言约束以处二者。 学基于国际音标(IPA)字符,自然语音几何。 由是生成框架共拟分词与同源对齐,语音约束。 两破译语(哥特语,乌加里特语)与未破译语(伊比利亚语)上评模形。 实验明,合音几何体可以致清一之益也。 又言亲密度度量,可正识哥特语与乌加里特语相关语。 其于伊比利亚人也,未见强证巴斯克语以关言语,与时学者同目。', 'ru': 'Большинство недешифрованных потерянных языков имеют две характеристики, которые создают значительные проблемы с расшифровкой: (1) скрипты не полностью сегментированы на слова; (2) ближайший известный язык не определен. Мы предлагаем модель расшифровки, которая решает обе эти проблемы, опираясь на богатые лингвистические ограничения, отражающие последовательные закономерности в исторических звуковых изменениях. Мы фиксируем естественную фонологическую геометрию, изучая вложения символов на основе международного фонетического алфавита (IPA). Полученная генеративная структура совместно моделирует сегментацию слов и когнатное выравнивание, основанное на фонологических ограничениях. Мы оцениваем модель на обоих расшифрованных языках (готическом, угаритском) и на некодифицированном (иберийском). Эксперименты показывают, что включение фонетической геометрии приводит к четким и последовательным выигрышам. Кроме того, мы предлагаем меру близости языков, которая правильно определяет родственные языки для готики и угарита. Для иберийского языка метод не показывает убедительных доказательств, подтверждающих баскский как родственный язык, что согласуется с предпочтительной позицией нынешней стипендии .1', 'ga': 'Léiríonn formhór na dteangacha caillte neamhshonraithe dhá thréith a chruthaíonn dúshláin shuntasacha dí-aitheanta: (1) níl na scripteanna deighilte go hiomlán i bhfocail; (2) nach bhfuil an teanga is gaire ar eolas a chinneadh. Molaimid múnla dí-aitheanta a láimhseálann an dá dhúshlán seo trí thógáil ar shrianta saibhre teangeolaíochta a léiríonn patrúin comhsheasmhacha in athrú fuaime stairiúil. Gabhaimid an chéimseata nádúrtha fóineolaíochta trí leabú carachtar a fhoghlaim bunaithe ar an Aibítir Idirnáisiúnta Foghraíochta (IPA). Déanann an creat giniúna a eascraíonn as seo comhshamhaltú ar dheighilt focal agus ailíniú gaolmhar, bunaithe ar shrianta fóineolaíochta. Déanaimid meastóireacht ar an múnla ar an dá theanga dhíscríofa (Gotach, Ugaritic) agus an teanga neamhchinntithe (Ibéiris). Léiríonn na turgnaimh go dtagann gnóthachain shoiléire agus chomhsheasmhacha trí chéimseata foghraíochta a ionchorprú. Ina theannta sin, molaimid beart do ghaireacht teanga a shainaithníonn i gceart teangacha gaolmhara don Ghotach agus don Ugaritic. I gcás na hIbéire, ní léiríonn an modh fianaise láidir ag tacú leis an mBascais mar theanga ghaolmhar, ag teacht leis an seasamh is fearr leis an scoláireacht reatha.1', 'ka': 'უფრო მეტი განსხვავებული გამოიყენებული ენები გამოიყენებს ორი პროგრამეტრები, რომელიც განსხვავებს მნიშვნელოვანი განსხვავება: (1) სკრიპტები სიტყვებში არ უფრო ს (2) უფრო მხოლოდ ცნობილი ენა არ განსაზღვრულია. ჩვენ გვეძლევთ დესფერმაციის მოდელს, რომელიც ამ ორივე განცემების გამოყენება, რომელიც ბედნიერი ლენგურისტიკური განცემებების შესახებ, რომელიც ისტორიული ცვლილების შე ჩვენ მივიღებთ ნაირთი ფონოლოგიური გეომეტრია ინტერაციონალური ფონოლოგიური ალფებეტის (IPA) დაბაზეულებით. წარმოიდგინეთ გენერაციური ფრამეტრი ერთად მოდელი სიტყვების სექმენტირება და კონონიტური სექმენტირება, რომელიც ფონოლოგიური სექმენტირე ჩვენ მოდელის გავამუშავებთ ორივე განაკუთფირებული ენების (Gothic, Ugaritic) და განაკუთფირებული (Iberian). ექსპერიმენტები ჩვენებს, რომ ფონეტიკური ჯემეტრიის შეყვარება წარმოადგილად და შემდგომარებელი წარმოდგენება. დამატებით, ჩვენ მივიღებთ ენის წინასწორედ განსაზღვრება, რომელიც დოტიკური და სგარიტიკური ენებისთვის დაკავშირებულია. იბერიანეთისთვის მეტი არ აჩვენებს ძალიან კოდენობა, რომელიც ბასკის მხარდაჭირებული ენაზე, რომელიც მხარდაჭირებული სტუდენციის მხარდაჭირებული პოზიცია. 1', 'hu': 'A legtöbb titkosítatlan elveszett nyelv két jellemzővel rendelkezik, amelyek jelentős megfejtési kihívást jelentenek: (1) a szkriptek nem teljesen szavakra szegmentálódnak; (2) a legközelebbi ismert nyelvet nem határozzák meg. Olyan megfejtési modellt javasolunk, amely mindkét kihívást kezeli, gazdag nyelvi korlátokra építve, amelyek a történelmi hangváltozások következetes mintáit tükrözik. A természetes fonológiai geometriát a Nemzetközi fonetikus ábécé (IPA) alapján tanuljuk meg. Az ebből eredő generációs keretrendszer közösen modellezi a szószegmentációt és a kognált összehangolást fonológiai korlátozások alapján. A modellt mindkét megfejtett nyelven értékeljük (gótikus, ugaritikus) és egy megfejtett nyelven (ibériai). A kísérletek azt mutatják, hogy a fonetikai geometria beépítése világos és következetes nyereséghez vezet. Emellett javasoljuk a nyelvközelség intézkedését, amely helyesen azonosítja a kapcsolódó nyelveket a gótikus és ugaritikus nyelvekhez. Az ibériai nyelv esetében ez a módszer nem mutat erős bizonyítékot arra, hogy a baszk mint rokon nyelv legyen, ami egyetért a jelenlegi ösztöndíj által kedvelt pozícióval. 1', 'el': 'Οι περισσότερες μη αποκρυπτογραφημένες χαμένες γλώσσες παρουσιάζουν δύο χαρακτηριστικά που θέτουν σημαντικές προκλήσεις στην αποκρυπτογράφηση: (1) τα σενάρια δεν είναι πλήρως διαιρούμενα σε λέξεις. (2) δεν προσδιορίζεται η πλησιέστερη γνωστή γλώσσα. Προτείνουμε ένα μοντέλο αποκρυπτογράφησης που διαχειρίζεται και τις δύο αυτές προκλήσεις, οικοδομώντας σε πλούσιους γλωσσικούς περιορισμούς που αντικατοπτρίζουν συνεπή πρότυπα στην ιστορική αλλαγή ήχου. Καταγράφουμε τη φυσική φωνολογική γεωμετρία μαθαίνοντας ενσωμάτωση χαρακτήρων με βάση το Διεθνές Φωνητικό Αλφάβητο (IPA). Το προκύπτον παραγωγικό πλαίσιο μοντελοποιεί από κοινού την κατάτμηση λέξεων και την ευθυγράμμιση των συγγενών, με γνώμονα τους φωνολογικούς περιορισμούς. Αξιολογούμε το μοντέλο τόσο σε αποκρυπτογραφημένες γλώσσες (γοτθική, ουγγαριτική) όσο και σε μη αποκρυπτογραφημένες (ιβερική). Τα πειράματα δείχνουν ότι η ενσωμάτωση φωνητικής γεωμετρίας οδηγεί σε σαφή και συνεπή κέρδη. Επιπλέον, προτείνουμε ένα μέτρο γλωσσικής εγγύτητας που προσδιορίζει σωστά τις σχετικές γλώσσες για τα γοτθικά και τα ουγγαριτικά. Για τα Ιβηρικά, η μέθοδος δεν παρουσιάζει ισχυρά στοιχεία που υποστηρίζουν τα Βασκικά ως συγγενική γλώσσα, συμφωνώντας με την ευνοϊκή θέση της τρέχουσας υποτροφίας. 1', 'it': "La maggior parte dei linguaggi perduti indecifrati presentano due caratteristiche che pongono notevoli sfide di decifrazione: (1) gli script non sono completamente segmentati in parole; (2) la lingua più vicina conosciuta non è determinata. Proponiamo un modello di decifrazione che affronti entrambe queste sfide costruendo su ricchi vincoli linguistici che riflettono modelli coerenti nel cambiamento sonoro storico. Catturiamo la geometria fonologica naturale imparando le incorporazioni dei caratteri basate sull'alfabeto fonetico internazionale (IPA). Il framework generativo risultante modella congiuntamente la segmentazione delle parole e l'allineamento cognitivo, informato da vincoli fonologici. Valutiamo il modello sia sulle lingue decifrate (gotico, ugaritico) che su quelle indecifrate (iberico). Gli esperimenti dimostrano che incorporare la geometria fonetica porta a guadagni chiari e coerenti. Inoltre, proponiamo una misura per la vicinanza linguistica che identifica correttamente le lingue correlate per il gotico e l'ugaritico. Per l'iberico, il metodo non mostra prove forti a sostegno del basco come lingua correlata, in accordo con la posizione favorita dall'attuale borsa di studio. 1", 'mk': 'Повеќето нешифрирани изгубени јазици покажуваат две карактеристики кои претставуваат значителни предизвици за дешифрирање: (1) сценаријата не се целосно сегментирани во зборови; (2) најблискиот познат јазик не е одреден. Предложуваме модел за дешифрирање кој ги решава двата предизвици со изградба на богати јазички ограничувања кои одразуваат константни шеми во историските звучни промени. Ја снимаме природната фонолошка геометрија со учење на вградувања на карактери базирани на Меѓународниот фонетски алфабет (ИПА). Резултатот на генерационата рамка заеднички моделира сегментација на зборовите и когнативно пристапување, информирана со фонолошки ограничувања. Го проценуваме моделот на двете дешифрирани јазици (готски, угаритски) и нешифрирани (иберски). Експериментите покажуваат дека вклучувањето на фонетската геометрија води до јасни и константни профили. Additionally, we propose a measure for language closeness which correctly identifies related languages for Gothic and Ugaritic.  За Иберијанскиот метод не покажува силни докази за поддршката на Баски како поврзан јазик, во согласност со омилената позиција на сегашната стипендија. 1', 'ml': 'പിന്നീട് നഷ്ടപ്പെടാത്ത ഭാഷകളില്\u200d ഏറ്റവും പ്രധാനപ്പെട്ട ഡെസിഫര്\u200dമെന്\u200dറ് വിലാസങ്ങള്\u200d ഉണ്ടാക്കുന്ന രണ്ടു വിവരങ്ങള്\u200d കാണിക്കു (2) അടുത്ത് അറിയപ്പെട്ട ഭാഷ തിരഞ്ഞെടുക്കപ്പെട്ടിട്ടില്ല. ചരിത്രത്തിലെ മാറ്റങ്ങളില്\u200d സമ്പന്നഭാഷയുടെ നിയന്ത്രണങ്ങളില്\u200d നിര്\u200dമ്മിക്കുന്ന സമ്പത്തുള്ള ഭാഷക്കാരുടെ നിയന്ത്രണങ്ങളില്\u200d ന നമ്മള്\u200d സ്വാഭാവികമായ ഫോളോളജിക്കല്\u200d ജോമിത്രിയെ പിടികൂടുന്നു. ഇന്റര്\u200dനാഷണല്\u200d ഫോണെറ്റിക് ആല്\u200dഫാബ്ലാറ്റ് അടിസ്ഥാനത അതിന്റെ ഫലമായ ജെനററിവ് ഫ്രെയിമോഡലുകള്\u200d ഒരുമിച്ച് മോഡലുകളില്\u200d വാക്ക് സംഘടിപ്പിക്കുന്നതും കോഗ്നേറ്റ് ചേര്\u200dക് നമ്മള്\u200d രണ്ട് ഡിസിഫര്\u200d ഭാഷകളിലും ഗോട്ടിക്ക്, ഉഗാരിറ്റിക്കും, ഒരു അപരിചിതമായ ഭാഷയിലും മാതൃകയെ വിലാസപ്പെടുത്തുന്നു. പരീക്ഷണങ്ങള്\u200d കാണിച്ചു കൊണ്ടിരിക്കുന്നത് ഫോണെറ്റിക് ജോമിത്രിയില്\u200d ചേര്\u200dക്കുന്നത് വ്യക്തമാക്കുകയ കൂടുതല്\u200d ഗോട്ടിക്കും ഉഗാരിറ്റിക്കും സംബന്ധിച്ച ഭാഷകളെ ശരിയായി പരിചയപ്പെടുത്തുന്ന ഭാഷയ്ക്ക് വേണ്ടി നാം ഒരു  ഇബേരിയയില്\u200d നിന്ന്, ബാസ്കിനെ സംബന്ധിച്ച ഭാഷയായി പിന്തുണക്കുന്ന ശക്തിയുള്ള തെളിവുകള്\u200d കാണിക്കുന്നില്ല, ഇപ്പോ 1', 'kk': 'Шифрланбаған жоғалтылған тілдердің көпшілігі үлкен дешифрлау мәселелерін көрсетеді: (1) скрипттер сөздерге толық бөліктелмейді; (2) Ең жақын белгіленген тіл анықталмайды. Біз бұл әдістердің екеуін басқару үлгісін қолданатын дешифрлау үлгісін таңдаймыз. Бұл әдістерді тарихлық дыбыс өзгерістерінде тұрақты үлгілерді құрып тұр Біз халықаралық фонетикалық альфабет (IPA) негізінде таңбаларды үйрену арқылы табиғи фонологикалық геометрияны түсіндіреміз. Сондағы жасалған жасалған фрейм сөздерді біріктіру үлгілері және фонологиялық шектері бойынша мәліметті белгілеу үлгілері. Біз үлгісін шешілмеген тілдер (Gothic, Ugaritic) мен шешілмеген (Иберия) үлгісін бағалаймыз. Тәжірибелер фонетикалық геометрияны ендіру керектігін көрсетеді. Қосымша, біз гетик және угартик тілдерді дұрыс анықтайтын тілдер үшін жақын түрлендіру үшін өлшемін ұсынамыз. Иберия үшін бұл әдіс Баск тіліне қолдау үшін күшті құқықтар көрсетілмейді. Бұл әдіс қазіргі стипендіктердің жақсы орнына сәйкес келеді. 1', 'mt': "Il-biċċa l-kbira tal-lingwi mitlufa mhux iddikjarati juru żewġ karatteristiċi li joħolqu sfidi sinifikanti ta’ deċifrazzjoni: (1) l-iskripti mhumiex segmentati kompletament fi kliem; (2) l-eqreb lingwa magħrufa mhijiex determinata. Aħna nipproponu mudell ta' deċifrazzjoni li jindirizza dawn iż-żewġ sfidi billi nibnu fuq restrizzjonijiet lingwistiċi rikki li jirriflettu xejriet konsistenti fil-bidla storika tajba. Aħna nqabdu l-ġeometrija fonoloġika naturali billi nitgħallmu l-inkorporazzjonijiet tal-karattri bbażati fuq l-Alfabet Fonetiku Internazzjonali (IPA). Il-qafas ġenerattiv li jirriżulta jimmudella b’mod konġunt is-segmentazzjoni tal-kliem u l-allinjament konġunt, infurmat minn restrizzjonijiet fonoloġiċi. Aħna jevalwaw il-mudell kemm fil-lingwi deċifrati (Gotiċi, Ugaritiċi) kif ukoll fil-lingwa mhux diċifrata (Iberiana). L-esperimenti juru li l-inkorporazzjoni tal-ġeometrija fonetika twassal għal kisbiet ċari u konsistenti. Additionally, we propose a measure for language closeness which correctly identifies related languages for Gothic and Ugaritic.  For Iberian, the method does not show strong evidence supporting Basque as a related language, concurring with the favored position by the current scholarship. 1", 'lt': 'Dauguma nedešifruotų prarastų kalbų turi dvi charakteristikas, keliančias reikšmingus iššūkius: 1) scenarijai nėra visiškai suskirstyti į žodžius; (2) the closest known language is not determined.  Siūlome išskyrimo model į, kuris sprendžia abu šiuos uždavinius remdamasis dideliais kalbiniais apribojimais, atspindinčiais nuoseklius istorinių garso pokyčių modelius. Mes gauname natūralią fonologinę geometriją mokydami simbolių įdėjimus remiantis Tarptautiniu fonetiniu alfabetu (IPA). Atsižvelgiant į tai gautą kartinę sistemą, bendrai modeliuojama žodžių segmentacija ir pažįstamųjų medžiagų suderinimas, informuojamas fonologiniais apribojimais. Mes vertiname model į tiek dešifruotomis kalbomis (gotiškomis, ugaritinėmis), tiek nedešifruotomis (iberinėmis). Eksperimentai rodo, kad fonetinės geometrijos įtraukimas lemia aiškią ir nuoseklią naudą. Be to, siūlome kalbų artimumo priemonę, kurioje teisingai nustatomos susijusios gotiškos ir ugaritiškos kalbos. Iberijos kalbos metodas neparodo tvirtų įrodymų, patvirtinančių baskų kalbą kaip susijusią kalbą, kartu su dabartinės stipendijos palankia padėtimi. 1', 'ms': 'Kebanyakan bahasa yang hilang tidak disifrasi menunjukkan dua ciri-ciri yang mengakibatkan cabaran penyecahan yang signifikan: (1) skrip tidak disegmen sepenuhnya ke dalam perkataan; (2) bahasa terdekat yang diketahui tidak ditentukan. Kami cadangkan model penyahsifat yang mengatasi kedua-dua cabaran ini dengan membina pada kekangan bahasa yang kaya yang mencerminkan corak konsisten dalam perubahan bunyi sejarah. Kami menangkap geometri fonologi alami dengan mempelajari bentuk aksara berdasarkan Alfabet fonetik antarabangsa (IPA). kerangka generatif yang menghasilkan secara bersama-sama model segmen perkataan dan segenasi pengetahuan, diberitahu oleh kewajiban fonologi. Kami menilai model pada kedua-dua bahasa yang dikosongkan (Gothic, Ugaritic) dan bahasa yang tidak dikosongkan (Iberian). Eksperimen menunjukkan bahawa menggabungkan geometri fonetik membawa kepada keuntungan yang jelas dan konsisten. Selain itu, kami mengusulkan ukuran untuk kedekatan bahasa yang betul mengenalpasti bahasa berkaitan untuk Gothic dan Ugaritic. Bagi bahasa Iberia, kaedah ini tidak menunjukkan bukti yang kuat menyokong Bahasa Bask sebagai bahasa berkaitan, bersetuju dengan kedudukan yang disukai oleh beasiswa semasa. 1', 'mn': 'Ихэнх шинжилгээгүй алдагдсан хэл нь тодорхой шинжилгээний сорилтуудыг бий болгодог хоёр чанарыг харуулдаг: (1) шинжилгээ бүрэн үг хэлбэрээр хуваагдсан биш; (2) Хамгийн ойрхон мэддэг хэл тодорхойгүй. Бид эдгээр сорилтуудыг баян хэл хэлний хязгаарлалуудын тулд түүхийн дуу өөрчлөлттэй тогтмол загваруудыг дүрслэх загварын загварыг санал болгож байна. Бид байгалийн фонологикийн геометрийг олон улсын Фонетик Альфабет (IPA) дээр суралцаж суралцаж авдаг. Үүний үр дүнд бий болгон бүтээгдэхүүний хэмжээсүүдийг нэгтгэхэд загвар хэлбэрийг загварлаж, фонологийн хязгаарлалтад мэдээлэл өгдөг. Бид шийдвэрлэгдсэн хэлний загварыг (Готик, Угаритик) болон шийдвэрлэгдсэн (Ибериан) хэлний талаар үнэлдэг. Эдгээр туршилтууд фонетик геометрийг нэгтгэх нь тодорхой, тогтмол ашигтай болдог. Мөн бид хэл ойролцоогоо зөв тодорхойлж чадах хэлний тухай хэлбэрийг санал болгож байна. Иберийн хувьд энэ арга нь Баскыг хамааралтай хэл гэж дэмжиж байгаа хүчтэй баталгаа харуулж чадахгүй. 1', 'ro': 'Cele mai multe limbi pierdute nediscriptate prezintă două caracteristici care reprezintă provocări semnificative de descifrare: (1) scripturile nu sunt complet segmentate în cuvinte; (2) limba cea mai apropiată cunoscută nu este determinată. Propunem un model de descifrare care abordează ambele provocări, bazându-se pe constrângeri lingvistice bogate care reflectă modele coerente în schimbarea sonoră istorică. Capturăm geometria fonologică naturală prin învățarea încorporărilor caractere bazate pe alfabetul fonetic internațional (IPA). Cadrul generativ rezultat modelează împreună segmentarea cuvintelor și alinierea cognitivă, informată de constrângeri fonologice. Evaluăm modelul atât pe limbile descifrate (gotică, ugaritică) cât și pe cea nedecificată (iberică). Experimentele arată că încorporarea geometriei fonetice duce la câștiguri clare și consistente. În plus, propunem o măsură pentru apropierea limbilor care identifică corect limbile conexe pentru gotic și ugarit. Pentru iberică, metoda nu prezintă dovezi puternice care să susțină basca ca limbă conexă, în concordanță cu poziția favorizată de bursa actuală. 1', 'sr': 'Većina neošifrovanih izgubljenih jezika pokazuje dve karakteristike koje predstavljaju značajne izazove za dezšifrovanje: (1) skriptovi nisu u potpunosti podeljeni u reči; (2) najbliži jezik nije određen. Predlažemo model za deksificiranje koji se bavi obje ove izazove izgradnjem bogatih jezičkih ograničenja koji odražavaju konsekventne obrasce u istorijskoj promjeni zvuka. Uhvatili smo prirodnu fonološku geometriju učeći uloženje karaktera na osnovu Međunarodnog fonetičkog alfabeta (IPA). Rezultativni generativni okvir zajedno modelira segmentaciju riječi i kognitivnu prilagodbu, obavijesti fonološki ograničenja. Procjenjujemo model na obje dezšifrovane jezike (Gotički, Ugaritički) i neošifrovane (Iberijanski). Eksperimenti pokazuju da uključujući fonetičku geometriju vodi do jasnih i konsekventnih dobića. Osim toga, predlažemo mjeru za bliskost jezika koja ispravno identifikuje povezane jezike za gotičke i ugaritske. Za Iberijance, metod ne pokazuje jake dokaze koji podržavaju Basku kao povezan jezik, u skladu sa omiljenom pozicijom trenutne stipendije. 1', 'pl': 'Większość nieszyfrowanych utraconych języków wykazuje dwie cechy, które stanowią istotne wyzwania w zakresie rozszyfrowania: (1) skrypty nie są w pełni podzielone na słowa; (2) nie ustalono najbliższego znanego języka. Proponujemy model rozszyfrowania, który radzi sobie z obydwoma wyzwaniami, opierając się na bogatych ograniczeniach językowych odzwierciedlających spójne wzorce historycznej zmiany dźwięku. Uchwytujemy naturalną geometrię fonologiczną, ucząc się osadzeń znaków opartych na Międzynarodowym Alfabetie Phonetycznym (IPA). Powstałe ramy generacyjne wspólnie modelują segmentację słów i wyrównanie poznawcze, oparte na ograniczeniach fonologicznych. Oceniamy model zarówno na językach odszyfrowanych (gotycki, ugarycki) jak i nieszyfrowanych (iberyjski). Eksperymenty pokazują, że włączenie geometrii fonetycznej prowadzi do wyraźnych i spójnych zysków. Dodatkowo proponujemy środek zbliżenia językowego, który poprawnie identyfikuje języki pokrewne dla gotyku i ugaryckiego. W przypadku Iberyjskiego metoda ta nie wykazuje silnych dowodów potwierdzających baskijski jako język pokrewny, zgadzających się z preferowaną pozycją obecnego stypendium. 1', 'si': 'ගොඩක් අවස්ථාවක් නැති භාෂාවල් ප්\u200dරදේශ කරනවා විශේෂතාවක් දෙකක් ප්\u200dරදේශ කරනවා: (1) ස්ක්\u200dරිප්ට් වචන වලට සංප (2) දැනගෙන ඉන්න ලොකු භාෂාව තීරණය කරන්නේ නැහැ. අපි ප්\u200dරශ්නයක් කරනවා මේ අභ්\u200dයානයක් දෙන්නම් ප්\u200dරශ්නයක් වෙනුවෙන් ප්\u200dරශ්නයක් වෙනුවෙන් ප්\u200dරශ්නයක් වෙනුවෙන්  අපි ස්වභාවික ශෝන්තික භාවිමිතිය අල්ලගන්නවා ජාතික ශෝන්තික භාවිමිතිය (IPA) සඳහා අන්තිමාත්\u200dර ප්\u200dරතිචාර ප්\u200dරවෘත්තිය සාමාන්\u200dය වාර්තාවක් සමාන්\u200dය විශේෂණය සහ ප්\u200dරවෘත්තිය සමාන්\u200dය විශේෂණය සඳහා  අපි ප්\u200dරමාණය විශ්වාස කරලා තියෙන දෙන්නම් විශ්වාස කරනවා (ගොතික්, වුගාරිටික්) සහ ප්\u200dරමාණය විශ්වාස කරලා න පරීක්ෂණය පෙන්වන්නේ ෆෝනෙටික් ජාමිතිය සම්පූර්ණයෙන් පැහැදිලි හා සාමාන්\u200dය විශ්වාස කරනවා. තවත්, අපි භාෂාව සම්බන්ධතාවක් ගොතික් සහ උගාරිටික් වලට සම්බන්ධතාවක් තේරුම් කරනවා. අයිබෙරියාන් වලින්, මේ විධානය බාස්ක්ව සම්බන්ධ භාෂාවක් විදියට සම්බන්ධ සාක්ෂියක් පෙන්වන්නේ නැහැ, ම 1', 'sv': 'De flesta odekodade förlorade språk uppvisar två egenskaper som innebär betydande dechiffreringsutmaningar: (1) skripten är inte helt segmenterade i ord; (2) Det närmaste kända språket är inte fastställt. Vi föreslår en dechiffreringsmodell som hanterar båda dessa utmaningar genom att bygga på rika språkliga begränsningar som återspeglar konsekventa mönster i historiska ljudförändringar. Vi fångar den naturliga fonologiska geometrin genom att lära oss karaktärsinbäddningar baserade på International Phonetic Alphabet (IPA). Den resulterande generativa ramen modellerar gemensamt ordsegmentering och kognitiv justering, informerad av fonologiska begränsningar. Vi utvärderar modellen på både dekrypterade språk (gotiska, ugaritiska) och ett odekodat språk (iberiska). Experimenten visar att införlivandet av fonetisk geometri leder till tydliga och konsekventa vinster. Dessutom föreslår vi en åtgärd för språknärhet som korrekt identifierar besläktade språk för gotiska och ugaritiska. För iberiska visar metoden inte starka bevis för baskiska som besläktat språk, vilket överensstämmer med den gynnade positionen av det nuvarande stipendiet. 1', 'no': 'Dei fleste ukrypterte tapte språk viser to karakteristikk som poserer signifikante desiferansfordringar: (1) skriptane er ikkje fullstendig segmentert i ord; og (2) den næraste kjende språket er ikkje bestemt. Vi foreslår ein desiferingsmodell som handterer begge desse utfordringane ved å bygge på rike lingviske begrensningar som reflekserer konsistent mønsterelement i historiske lydsendringar. Vi får den naturlege fonologiske geometrien ved å lære teikn-innbygging basert på den internasjonale fonetiske alfabeten (IPA). Det resulterte genererande rammeverktøyet samtidig modeller ordsegmentasjon og kjenner justering, informert av fonologiske begrensningar. Vi evaluerer modellen på begge desifererte språk (Gotisk, Ugaritisk) og ein ukryptert (Iberisk). Eksperimentane viser at inkludering av fonetiske geometri fører til klar og konsistent forsøk. I tillegg foreslår vi eit mål for språk nærleiken som rett identifiserer tilhøyrande språk for Gotisk og Ugaritisk. For Iberian viser metoden ikkje sterke beviser som støttar Bask som eit relatert språk, samtidig med den favoritte posisjonen av den gjeldande stipende. 1', 'so': "Inta badan luqada aan la baahnayn waxay leedahay laba takhash oo ay leedahay dhibaatooyin muhiim ah ee go'aanka:(1) Qoraaniyadu kuma qeybeeyaan hadal buuxda; (2) Luqada la yaqaan ugu dhaw lama yaqaan. Tusaale go'aan ah oo xafiiska labadaas dhibaatooyin ah ku saabsan dhisidda dhibaatooyinka luuqadda taajirka ah oo ku sawiraya qaabab la mid ah oo ku beddelaya codka taariikhda. Waxaynu qabsannaa geometry dabiicadda ah oo ku qoran xarafka barashada ee ku saleysan Internetic Alphabet (IPA). Shaqooyinka geneeral ee wadajirka ah ayaa sameynaya isbedelka hadalka, isbedelka iyo isbedelka, taasoo loo soo sheegay qasabka foonolojiga. Tusaalada waxaan ku qiimeynaynaa labada luqadood oo lagu qoray (Gothic, Ugaritic) iyo mid aan la aqoon karin (Iberian). Imtixaanka waxaa muuqda in ku biirista joomeetiyada telefonetka ah uu ku hoggaamiyaa faa'iido cad oo la mid ah. Additionally, we propose a measure for language closeness which correctly identifies related languages for Gothic and Ugaritic.  Waayo, Iberian qaababkiisu ma muujiyo caddeyn xoog leh oo taageeraya Basque sida luqad la xiriira, kaas oo la xiriira booska la jecel yahay ee waxbarashada joogta ah. 1", 'ur': 'اکثر ناپذیر ہوئی زبانیں دو خصوصی نشانیاں دکھاتے ہیں جن میں اہم دففر چالیاں موجود ہیں: (1) لکھائیں کلمات میں پوری طرح ٹکڑے نہیں ہیں۔ (2) سب سے قریب جانے والی زبان مقرر نہیں کی گئی۔ ہم ایک ڈیفر موڈل کو پیشنهاد کرتے ہیں جو ان دونوں چالیوں کو مسلط کرتا ہے تاریخی صدا بدلنے کے مطابق ثروت زبان کی محدودیت پر بناتے ہیں۔ ہم طبیعی فانولوژیکی جسمٹری کو پکڑ رہے ہیں کہ ان کی بین المللی فونوٹیکی الفابت (IPA) پر بنیاد رکھتے ہیں. نتیجۂ پیدا ہونے والی فراموشہ کے ساتھ کلمات سیگنٹ سیگنٹ اور پہچان ہونے والی سیگنٹ، جسے فانولوژیکی محدودیت کے ذریعہ بتایا جاتا ہے. ہم نے مدل کو دونوں دفعہ زبانوں میں (گوتیک، اوگریٹیک) اور ایک غیر دفعہ کیا گیا ہے۔ آزمائش دکھاتے ہیں کہ فونیٹیکی جسمٹریت کے ساتھ شامل ہونے کی وجہ صریح اور ثابت قدم پہنچاتی ہے. اور ہم زبان کی نزدیکتی کے لئے ایک اندازہ پیشنهاد کرتے ہیں جو گوتیک اور اوگاریٹیک کے لئے ٹھیک سمجھتا ہے۔ ابرین کے لئے یہ طریقہ باسک کی مدد کرنے کی طاقت کی نشانیاں نہیں دکھاتی ہیں، جو موجود علم کے مطابق مہربان موقعیت کے ساتھ ہے. 1" (msgctxt: "panel:showusername") to "1', 'ta': 'பெரும்பாலான அழிக்கப்படாத மொழிகள் இரண்டு விருப்பங்களை காட்டுகிறது அது முக்கியமான தள்ளீட்டு சவால்கள் இருக்கிறது: (1) சிறுநிர (2) தெரியப்பட்ட மொழி நிர்ணயிக்கப்படவில்லை. நாம் இந்த இரண்டு சவால்களையும் கட்டுப்படுத்தும் பதிப்பு மாதிரி பரிந்துரைக்கிறோம். இது வரலாற்று ஒலி மாற்றத்தில் சொந்த மாதி நாம் இயற்கையான போலோஜியோஜியோமிதியை பிடித்துக் கொள்கிறோம் இந்த இணையானநெடிக் ஆல்பாப்ட் அடிப்படையில் கற்ற எழுத @ info: whatsthis நாம் இரண்டு பதிநகல் மொழிகள் (கோடிக், உகாரிடிக்) மற்றும் ஒரு தவிர்க்கப்படாத மாதிரியை மதிப்பிடுகிறோம். The experiments show that incorporating phonetic geometry leads to clear and consistent gains.  கூடுதலாக, நாம் மொழி நெருக்கமாக ஒரு அளவை பரிந்துரைக்கிறோம். அது சரியாக தொடர்புடைய மொழிகளை கோடிக் மற்றும் உகார இப்பேரியாவின் முறையில் பாஸ்கை ஒரு தொடர்புடைய மொழியாக ஆதரிக்கும் உறுதியான ஆதரவை காட்டுவதில்லை, தற்போதைய கல்வியியல்  1', 'uz': "Ko\xa0Ľp noto\xa0Ľg\xa0Ľri tillar yo\xa0Ľqolmagan ikki xususiyatlarni ko'rsatadi, bu juda muhim cheksiz qiymatlar bor: (1) skriptlar to\xa0Ľliq so\xa0Ľzlarga ajratilmaydi; (2) Keyingi noma\xa0ľlum til aniqlanmagan. Biz bu ikkita muammolarni boshqarish modelini o'ylaymiz, tarixi tovush o'zgarishda davomida o'xshash shakllarni o'zgartirish bilan taxminan tillar tartiblarini yaratish bilan o'zgartiradi. Biz tabiiy fonologiya geometriyni ko'rib turamiz, International Phonetic Alphabet (IPA) asosida o'rganish belgini o'rganish mumkin. Name Biz bir xil tilga (Gotik, Ugaritik) va noto\xa0Ľg\xa0Ľri bir (Iberiya) modelini qiymatmiz. Imtizolar fonetik geometri ichida o'rganishni ko'rsatadi, to \xa0Ľg\xa0Ľri va davomida muvaffaqiyatlarni ko'rsatadi. Ko'pchilik, biz tilning eng yaxshi tilni tahrirlash tilini anglatamiz. Bu tilning bog'liq tillarini o'xshash o'zgartiradi. Bu Iberiya uchun, bu usuli Basque bilan bog'liq tillarini qo'llashni qo'llashmaydi. Joriy talabatlarning eng yaxshi darajada birlashtirish mumkin. 1", 'vi': 'Hầu hết các ngôn ngữ đã mất tín hiệu chưa xác định có hai đặc trưng tạo ra thử thách giải mã quan trọng: 1) các tập lệnh chưa được chia thành từ hoàn toàn. 2) không xác định ngôn ngữ gần nhất được biết. Chúng tôi đề xuất một mô hình giải mã nắm giữ cả hai thử thách bằng cách dựa trên những giới hạn ngôn ngữ giàu có phản ánh những mô hình liên quan trong biến đổi âm thanh lịch sử. Chúng tôi thu được hình học gốc nhờ sự nhúng vào nhân vật dựa trên Bảng chữ I.P.A. Kết quả là cấu trúc mang lại cấu trúc hai từ mô tả phân chia và mã hoá, được thông báo bởi giới hạn âm thanh. Chúng tôi đánh giá mô hình của cả hai ngôn ngữ đã được giải (Gothic, Ugritic) và một ngôn ngữ chưa xác định (Iberian). Các thí nghiệm cho thấy rằng gắn kết hình học điện thoại dẫn tới lợi nhuận rõ ràng và chắc chắn. Thêm nữa, chúng tôi đề xuất một biện pháp cho sự gần gũi ngôn ngữ để xác định chính xác ngôn ngữ tương tự với Gothic và Ugritic. Với người Iberian, phương pháp không cho thấy chứng cứ mạnh mẽ ủng hộ người Basque như một ngôn ngữ liên quan, nằm đồng thuận với vị trí được ưa thích bởi học bổng hiện tại. L', 'bg': 'Повечето нешифрирани изгубени езици имат две характеристики, които поставят значителни предизвикателства при дешифрирането: (1) скриптовете не са напълно сегментирани на думи; (2) най-близкият познат език не е определен. Предлагаме модел на дешифриране, който се справя и с двете предизвикателства, като се основава на богати езикови ограничения, отразяващи последователни модели в историческата промяна на звука. Ние улавяме естествената фонологична геометрия чрез изучаване на вграждането на знаци въз основа на Международната фонетична азбука (ИПП). Получената генеративна рамка съвместно моделира сегментацията на думите и съответстващото подравняване, информирано от фонологични ограничения. Оценяваме модела както на дешифрирани езици (готически, угаритски), така и на нешифрирани (иберийски). Експериментите показват, че включването на фонетична геометрия води до ясни и последователни печалби. Освен това предлагаме мярка за езикова близост, която правилно идентифицира сродните езици за готически и угаритски. За иберийския методът не показва силни доказателства в подкрепа на баския като сроден език, което съответства на предпочитаната позиция от настоящата стипендия. 1', 'hr': 'Većina neošifrovanih izgubljenih jezika pokazuju dvije karakteristike koje predstavljaju značajne izazove za dezšifrovanje: (1) skriptovi nisu potpuno dijeljeni u riječi; (2) najbliži jezik nije određen. Predlažemo model dezsificiranja koji rješava obe ove izazove izgradnjem bogatih jezičkih ograničenja koji odražavaju konsekvente obrasce u povijesnim promjenama zvuka. Uhvaćamo prirodnu fonološku geometriju učeći uloženje karaktera na temelju Međunarodnog fonetičkog alfabeta (IPA). Rezultativni generativni okvir zajednički modeli segmentacije riječi i poznatog poravnanja, obavijesti fonološki ograničenja. Procjenjujemo model na obje dezšifrovane jezike (Gotički, Ugaritički) i neošifrovane (Iberijanski). Eksperimenti pokazuju da uključujući fonetičku geometriju vodi do jasnih i konsekventnih dobića. Osim toga, predlažemo mjeru za bliskost jezika koja ispravno identificira povezane jezike za gotičke i Ugaritičke. Za Iberijance, metod ne pokazuje jake dokaze koji podržavaju Basku kao povezan jezik, u skladu s omiljenim pozicijom trenutne stipendije. 1', 'da': 'De fleste ubekodede tabte sprog udviser to karakteristika, der udgør betydelige dechiffreringsudfordringer: (1) scripts er ikke helt segmenteret i ord; (2) det nærmeste kendte sprog er ikke fastlagt. Vi foreslår en dechiffreringsmodel, der håndterer begge disse udfordringer ved at bygge på rige sproglige begrænsninger, der afspejler konsekvente mønstre i historiske lydforandringer. Vi indfanger den naturlige fonologiske geometri ved at lære tegn indlejringer baseret på det internationale fonetiske alfabet (IPA). Den resulterende generative ramme modellerer i fællesskab ordsegmentering og kognitiv justering, der er baseret på fonologiske begrænsninger. Vi evaluerer modellen på både dekrypterede sprog (gotisk, ugaritisk) og et ukrypteret sprog (iberisk). Eksperimenterne viser, at indarbejdelse af fonetisk geometri fører til klare og konsekvente gevinster. Derudover foreslår vi en foranstaltning til sprognærhed, der korrekt identificerer beslægtede sprog for gotisk og ugaritisk. For iberisk viser metoden ikke stærke beviser, der understøtter baskisk som beslægtet sprog, hvilket stemmer overens med den foretrukne position af det nuværende stipendium. 1', 'nl': 'De meeste niet-gecodeerde verloren talen vertonen twee kenmerken die aanzienlijke ontcijferingsuitdagingen opleveren: (1) de scripts zijn niet volledig gesegmenteerd in woorden; (2) de dichtstbijzijnde bekende taal wordt niet bepaald. We stellen een ontcijferingsmodel voor dat beide uitdagingen aanpakt door voort te bouwen op rijke taalkundige beperkingen die consistente patronen in historische geluidsveranderingen weerspiegelen. We leggen de natuurlijke fonologische geometrie vast door karakterinbeddingen te leren gebaseerd op het International Phonetic Alphabet (IPA). Het resulterende generatieve raamwerk modelleert samen woordsegmentatie en cognaat alignment, gebaseerd op fonologische beperkingen. We evalueren het model op zowel ontcijferde talen (Gotisch, Oegarisch) als een niet-ontcijferde taal (Iberisch). De experimenten tonen aan dat het opnemen van fonetische geometrie leidt tot duidelijke en consistente winsten. Daarnaast stellen we een maatregel voor taaldichtheid voor waarmee verwante talen voor het Gotisch en Oegarisch correct worden geïdentificeerd. Voor Iberisch toont de methode geen sterk bewijs dat Baskisch als verwante taal ondersteunt, in overeenstemming met de gunstige positie van de huidige studiebeurs. 1', 'de': 'Die meisten unverschlüsselten verlorenen Sprachen weisen zwei Merkmale auf, die erhebliche Herausforderungen bei der Entschlüsselung aufwerfen: (1) die Skripte sind nicht vollständig in Wörter unterteilt; (2) die nächstgelegene bekannte Sprache wird nicht bestimmt. Wir schlagen ein Entschlüsselungsmodell vor, das beide Herausforderungen bewältigt, indem es auf umfangreichen sprachlichen Zwängen aufbaut, die konsistente Muster im historischen Klangwandel widerspiegeln. Wir erfassen die natürliche phonologische Geometrie, indem wir Zeicheneinbettungen auf Basis des Internationalen Phonetischen Alphabets (IPA) erlernen. Das daraus resultierende generative Framework modelliert gemeinsam Wortsegmentierung und kognitive Ausrichtung, basierend auf phonologischen Einschränkungen. Wir evaluieren das Modell sowohl auf entschlüsselten Sprachen (Gotisch, Ugarisch) als auch auf einer nicht entschlüsselten (Iberisch). Die Experimente zeigen, dass die Einbeziehung der phonetischen Geometrie zu klaren und konsistenten Gewinnen führt. Zusätzlich schlagen wir eine Maßnahme für Sprachnähe vor, die verwandte Sprachen für Gotisch und Ugarisch korrekt identifiziert. Für Iberisch zeigt die Methode keine starken Beweise für Baskisch als verwandte Sprache, was mit der bevorzugten Position des aktuellen Stipendiums übereinstimmt. 1', 'fa': 'بیشترین زبان\u200cهای ناپدید نشده\u200cاند، دو ویژگی را نشان می\u200cدهند که چالش\u200cهای جدایی بزرگی را نشان می\u200cدهند: (۱) نوشته\u200cها کاملا به کلمات جدایی نشده\u200cاند. (۲) نزدیکترین زبان شناخته نشده است. ما پیشنهاد می\u200cکنیم یک مدل رمزبندی که هر دو از این چالش\u200cها را با ساختن بر محدودیت زبان\u200cشناسی ثروت می\u200cکند که الگوهای موجود در تغییر صدای تاریخی نشان می\u200cدهد. ما geometry natural phonological را با یادگیری شخصیت\u200cها بر اساس الفابت بین المللی فونیک (IPA) گرفتیم. نتیجه\u200cی چهارچوب\u200cهای ژنترافی با همدیگر مدل\u200cهای جدایی کلمه\u200cها و تنظیم\u200cهای شناخته شده، که توسط محدودیت\u200cهای تلفنی اطلاع داده می\u200cشود. ما مدل را در هر دو زبان\u200cهای کشف\u200cشده (گوتیک، اوگاریتیک) و یک غیر رمز\u200cشده (ابریه) ارزیابی می\u200cکنیم. آزمایش ها نشان می دهند که شامل گیاهометریی تلفنی به پیروزی روشن و پایدار می شود. به اضافه، ما یک اندازه برای نزدیکترین زبان پیشنهاد می کنیم که به طور درست زبانهای ارتباطی برای گوتیک و اوگارتیک را مشخص می کند. برای ابریایی، این روش مدارک قوی را به عنوان زبان مربوط به باسک نشان نمی دهد که با موقعیت مورد علاقه\u200cمند توسط دانش آموزش فعلی است. ۱', 'sw': 'Lugha nyingi zisizo na uharibifu huonyesha takwimu mbili ambazo zina changamoto kubwa za uamuzi: (1) manuscripta haziganganishwa kabisa kwa maneno; (2) Lugha inayofahamika kwa karibu haina uthibitisho. Tunazipendekeza muundo wa uamuzi ambao unakabiliana na changamoto hizi mbili kwa kujenga vikwazo vya lugha tajiri ambavyo vinaonyesha mitindo inayofanana katika mabadiliko ya kihistoria ya sauti. Tunakamata geometri ya asili kwa kujifunza tabia za kujifunza zinazotumika kwa kutumia Alpha ya Kimataifa ya Picha (IPA). Miundombi ya kizalendo ya pamoja ni mifano ya kutengwa kwa neno na kupangwa kwa ushirikiano, inayoelezwa na vikwazo vya kifolojia. Tutathmini mtindo wa lugha zote zilizotengenezwa (Gothic, Ugaritic) na moja isiyoeleweka (Iberia). Majaribio yanaonyesha kuwa kuunganisha geometri ya simu yanapelekea mafanikio ya wazi na yanayoendelea. Kwa nyongeza, tunapendekeza hatua ya ukaribisho wa lugha ambayo inaonyesha lugha zinazohusiana sahihi kwa ajili ya Gothic na Ugariki. Kwa Iberia, mbinu hizo hazionyesha ushahidi wenye nguvu kuunga mkono Basque kama lugha inayohusiana, ukiunganisha nafasi inayopendelewa na elimu ya sasa. 1', 'id': 'Most undeciphered lost languages exhibit two characteristics that pose significant decipherment challenges: (1) the scripts are not fully segmented into words;  (2) bahasa terdekat yang dikenal tidak ditentukan. Kami mengusulkan model penyesuaian yang menangani kedua tantangan ini dengan membangun batas bahasa yang kaya yang merefleksikan pola konsisten dalam perubahan suara sejarah. Kami menangkap geometri fonologi alami dengan mempelajari bentuk karakter berdasarkan Alfabet Phonetik Internasional (IPA). Rubrik generatif yang menghasilkan secara bersama-sama model segregasi kata dan penyesuaian kognasi, diberitahu oleh keterangan fonologi. Kami mengevaluasi model pada kedua bahasa yang ditentukan (Gothic, Ugaritic) dan bahasa yang tidak ditentukan (Iberian). Eksperimen menunjukkan bahwa memasukkan geometri fonetik mengarah ke keuntungan yang jelas dan konsisten. Selain itu, kami mengusulkan ukuran untuk kedekatan bahasa yang benar mengidentifikasi bahasa terkait untuk Gothic dan Ugaritic. Bagi Iberian, metode ini tidak menunjukkan bukti yang kuat mendukung bahasa Basque sebagai bahasa yang berhubungan, bersamaan dengan posisi favorit dari beasiswa saat ini. 1', 'ko': '대부분의 해독되지 않은 언어는 두 가지 특징을 나타내는데 이것은 해독에 커다란 도전을 초래했다. (1) 스크립트가 단어로 완전히 분할되지 않았다.(2) 가장 가까운 알려진 언어는 아직 확정되지 않았다.우리는 역사의 소리 변화를 반영하는 일치된 모델의 풍부한 언어 제약을 구축함으로써 이 두 가지 도전에 대응할 수 있는 해독 모델을 제시했다.우리는 국제음표(IPA) 기반의 문자 삽입을 배워서 자연스러운 음성 기하학을 포착한다.이로 인해 발생하는 생성 프레임워크는 분사와 동원 정렬을 공동으로 모의하고 음성 제약의 영향을 받는다.우리는 이미 해독된 두 가지 언어 (고딕, 우가리트어) 와 미해독된 언어 (에페리아어) 에서 이 모델을 평가했다.실험에 따르면 음성 기하학을 결합하면 뚜렷하고 일치하는 결과를 얻을 수 있다.그 밖에 우리는 언어의 친밀도를 측정하는 방법을 제시하여 고딕어와 우가리트어의 관련 언어를 정확하게 식별할 수 있다.에페리아인들에게 이 방법은 바스크어를 관련 언어로 지지하는 유력한 증거를 보여주지 못했는데 이것은 현재 학계의 선호와 일치한다.1', 'tr': 'Köp küfrelenmedik ýüklenen diller, sözleriň çözgütlerini aňladýan iki karakterlere görkezýär: (1) skriptler doly sözlere bölmedi; (2) iň ýakyn bilinen dil takyklanmaýar. Biz bu kynçylyklaryň ikisini de baý lingwistiki çözgütleri ýüze çykaryp, taryhy ses üýtgewlerinde bir şekilde täsirli nusgalary görkeýän bir nusga teklip edýäris. Biz tebigy fonolojik geometriýany Halkara Fonetik Alfabet (IPA) tarapyndan daýanýar. netijä gelen döredijili çapda söz segmentasyny bilen nusgala we tanyş gabdalygy, fonolojik eserlerden tarapyndan bildirilýär. Biz muny hem çözüldir dillerde (Gotik, Ugaritik) we muny küfrelmeýän bir nusga çykýarys. Deneyler fonetik geometriýany daşary we daşary gazanýandygyny görkezýärler. Beýleki, biz geşik we Ugaritik dilleriniň ýakynlygy dogry tanap diller üçin bir ölçü teklip edip görýäris. Iberiýa üçin, bu yöntem baskyny golaý dilinde golaýlaýan güýçli kanunlary görkezmez. 1', 'af': "Die meeste ongesifreerde verlore tale vertoon twee karakteristieke wat betekende deksiferingsuitdrukkings stel: (1) die skripte word nie heeltemal in woorde segmenteer nie; en (2) die naaste bekende taal is nie bepaal nie. Ons voorstel 'n deksiferingsmodel wat beide hierdie uitdagings hanteer deur op ryk lingwisiese beheinings te bou wat die konsistente patrone in historiese klankverander reflekteer. Ons vang die natuurlike fonologiese geometrie deur te leer karakter inbettings gebaseer op die internasionale fonetiese alfabet (IPA). Die resulteerde genereerbare raamwerk saamstig modele woord segmentasie en bekende aanpassing, inligtig deur fonologiese beheinings. Ons evalueer die model op beide desifereerde tale (Gotiese, Ugaritiese) en 'n ongecifereerde een (Iberië). Die eksperimente wys dat die inkorporering van fonetiese geometrie lei na duidelik en konsistente verkrywings. In addition, we propose a measure for language closeness which correctly identifies related languages for Gothic and Ugaritic. Vir Iberian, vertoon die metode nie sterk getuienis wat Baske as 'n verwante taal ondersteun nie, wat saam met die favorite posisie deur die huidige stipendskap. 1", 'sq': 'Shumica e gjuhëve të papërshkruar të humbura ekspozojnë dy karakteristika që përbëjnë sfida të rëndësishme të shpërndarjes: (1) skriptet nuk janë segmentuar plotësisht në fjalë; (2) nuk është përcaktuar gjuha më e afërt e njohur. Ne propozojmë një model çifrimi që trajton të dy këto sfida duke ndërtuar mbi kufizime të pasura gjuhësore që pasqyrojnë modele konsistente në ndryshimin historik të tingullit. Ne kapim gjeometrinë fonologjike natyrore duke mësuar përfshirje karakteresh bazuar në Alfabetin Fonetik Ndërkombëtar (IPA). Korniza gjenerative që rezulton modelon së bashku segmentimin e fjalëve dhe rregullimin e njohur, të informuar nga kufizimet fonologjike. Ne e vlerësojmë modelin në të dy gjuhët e shkriptuara (gotike, ugaritike) dhe në gjuhën e pa shkriptuar (iberike). Eksperimentet tregojnë se përfshirja e gjeometrisë fonetike shpie në fitime të qarta dhe konsistente. Përveç kësaj, ne propozojmë një masë për afërsinë e gjuhës që identifikon saktësisht gjuhët e lidhura për gotik dhe ugaritik. Për iberinë, metoda nuk tregon prova të forta që mbështesin baskun si një gjuhë të lidhur, në përputhje me pozitën e favorizuar nga bursa aktuale. 1', 'am': 'ብዙዎቹ ያልተሳሳቱ ቋንቋዎች ሁለት የውጤት ግንኙነትን የሚያስፈልጉ ናቸው:(1) ጽሑፎቹ በንግግር የተለየ አይደሉም፤ (2) የታወቀ ቋንቋ አልተረጋገጠም። የዚህን ሁለታዊ ድምፅ ለታሪክ ድምፅ ማሳየት ባለጠጋ ቋንቋዎች ግንኙነትን በመሠረቱ የዲስ መተላለፊያ ሞዴል እናስባለን፡፡ የአለምአቀፍ ፎንቲክ አበባ (IPA) በተገኘ የፍጥረት ፎሎጂ ጂዮሜትሪን በመማር እናስጨንቃለን፡፡ የፍጥረት አቀማመጥ የፍሬም ክፍል በፎሎጂ ግንኙነት የተማረከ ቃላት ክፍተት እና ክፍል ማቀናጃ ነው፡፡ በሁለቱ ቋንቋዎች (ጎቲክ፣ ኦጋሪቲክ) እና ያልታወቀ አንዱን (ቢብሪያ) ምሳሌ እናስተውልታለን፡፡ ፈተናዎቹ የፎንnetic geometry መግባት የሚያሳየው ትክክለኛውን እና ትክክል ማግኘት ነው፡፡ በተጨማሪም፣ የቋንቋ አቅራቢያ ቋንቋዎች ለጎቲ እና ኦጋሪቲክ የተለየ ቋንቋዎችን አስተያየት እናስፈልጋለን፡፡ ቢብሬያም፣ የባስኩን በአካባቢው ቋንቋ ለመደጋገም ኃይለኛ ማስረጃ አያሳየውም፡፡ 1', 'hy': 'Most undeciphered lost languages exhibit two characteristics that pose significant decipherment challenges: (1) the scripts are not fully segmented into words;  (2) ամենամոտ հայտնի լեզուն չի որոշվում: Մենք առաջարկում ենք բացահայտելու մոդել, որը կլուծի այս երկու մարտահրավերները՝ հիմնվելով հարուստ լեզվաբանական սահմանափակումների վրա, որոնք արտացոլում են պատմական ձայնային փոփոխության համապատասխան կաղապարները: Մենք նկարում ենք բնական ֆոնոլոգիական երկրաչափությունը՝ սովորելով հիմնված միջազգային ֆոնոլոգիական ալբաբեթի (IPA) հիմքում: Արդյունքում ստեղծված սերունդային շրջանակը միասին մոդելներ է տալիս բառերի սեգմետրացիային և ճանաչողական հարմարեցման, տեղեկացված ֆոնոլոգիական սահմանափակումների միջոցով: Մենք գնահատում ենք մոդելը բացահայտված երկու լեզուների (գոտիկ, ուգարիտիկ) և անբացահայտված լեզուների (իբերական) վրա: Փորձարկումները ցույց են տալիս, որ ֆոնետիկ երկրաչափության ներառումը հանգեցնում է պարզ և կայուն շահույթների: Ավելին, մենք առաջարկում ենք մի չափություն լեզվի մոտիկության համար, որը ճիշտ բացահայտում է գոտիկ և ուգարիտիկ լեզուները: Իբերական դեպքում մեթոդը հզոր ապացույցներ չի ցույց տալիս, որոնք աջակցում են Բասկին որպես կապված լեզու, համաձայն ներկայիս գիտնականների սիրած դիրքի հետ: 1', 'az': 'Ən çox çəkilməmiş zəifli dillər mövcud çəkişmə çətinliklərini göstərən iki xüsusiyyət göstərir: (1) Skriptlər tamamilə sözlərə bölüklənmir; (2) ən yaxın bilinmiş dil təyin edilməz. Biz bu çətinliklərdən hər ikisini təşkil edib, zənginli dil çətinlikləri üzərində in şa edərək, tarixəli səs dəyişikliklərinə uyğun nöqtələri göstərir. Biz təbiətli fonolojik geometri, International Phonetic Alphabet (IPA) vasitəsilə qarşılıqlarını öyrəndik. Sonuçları, fonolojik müəyyənləşdirilmələr ilə bildirilən generiktif frameworklər birlikdə söz segmentasiyasını modelləşdirir və tanıyırlar. Biz bu modeli çözülmüş dillərin (Gotik, Ugaritik) və müəyyən edilməmiş bir dili ilə değerləşdiririk. Həyatlar fonetik geometriya daxil olmaq açıq və sürəkli qazanmağa yol göstərir. Üstəlik, Gotik və Ugaritik dillərini doğrudan tanıyan dillər üçün bir ölçü təklif edirik. İbrayanlara görə, bu metod baskı ilə istifadə edən möhkəm dəlillər göstərməz. Bu metod, həmin təhsil təhsil təhsil təhsil təhsil təhsil təhsil təhsil təhsil tərəfindən istifadə edilən təhsil kimi. 1" (msgctxt: "panel:showusername") to "1', 'bs': 'Većina neošifrovanih izgubljenih jezika pokazuju dvije karakteristike koje predstavljaju značajne izazove za dezšifrovanje: (1) skriptovi nisu potpuno segmentirani u riječi; (2) najbliži jezik nije određen. Predlažemo model dezsificiranja koji rješava obe ove izazove izgradnjem bogatih jezičkih ograničenja koji odražavaju konsekventne obrasce u istorijskoj promjeni zvuka. Uhvaćamo prirodnu fonološku geometriju učeći uloženje karaktera na temelju Međunarodnog fonetičkog alfabeta (IPA). Rezultativni generativni okvir zajedno modelira segmentaciju riječi i kognitivnu prilagodbu, obaviještenu fonološkim ograničenjima. Procjenjujemo model na obje dezšifrovane jezike (Gotički, Ugaritički) i neošifrovane (Iberijanski). Eksperimenti pokazuju da uključenje fonetičke geometrije vodi do jasnih i konsekventnih dobića. Osim toga, predlažemo mjeru za bliskost jezika koja ispravno identificira povezane jezike za gotiće i Ugaritičke. Za Iberijance, taj metod ne pokazuje jake dokaze koji podržavaju Basku kao povezan jezik, u skladu s omiljenim pozicijom trenutne stipendije. 1', 'bn': 'বেশীরভাগ ভাষা হারিয়ে যাওয়া ভাষা দুটি বৈশিষ্ট্য প্রদর্শন করে যা গুরুত্বপূর্ণ ডিসেফারেন্টমেন্ট চ্যালেঞ্জের মাধ্যমে: (1) স (2) নিকটতম পরিচিত ভাষা নির্ধারিত নয়। আমরা একটি ডিসেফারেন্টমেন্ট মডেল প্রস্তাব করছি যা এই দুটির চ্যালেঞ্জের মাধ্যমে সমৃদ্ধ ভাষার নিয়ম নির্মাণ করে ঐতিহাসিক শব্দ পরি আন্তর্জাতিক ফোনেটিক আলফ্যাবেট (আইপি) ভিত্তিক অক্ষর শিক্ষা দিয়ে আমরা প্রাকৃতিক ফোলোজিক্যাল জিওমেটিকে ধরেছি। এর ফলে জেনারেটিভ ফ্রেম মোডেল যুক্তিভাবে শব্দ বিভক্ত এবং কোগ্নেট স্থাপন করা হয়েছে, ফোনোলজিক্যাল নিয়ন্ত্রণের তথ্য আমরা দুই ভাষায় এই মডেলের মূল্য মূল্য দিচ্ছি (গোটিক, উগারিটিক) এবং একটি অবৈধ (ইবেরিয়ান)। পরীক্ষাগুলো দেখা যাচ্ছে যে ফোনেটিক ভূমিতির মধ্যে যোগাযোগ করা হচ্ছে তা পরিষ্কার এবং সাধারণ অর্জনের কারণে  Additionally, we propose a measure for language closeness which correctly identifies related languages for Gothic and Ugaritic.  ইবেরিয়ানের জন্য, এই পদ্ধতি বাস্কেকে সংশ্লিষ্ট ভাষা হিসেবে সমর্থন করার শক্তিশালী প্রমাণ দেখায় না, বর্তমান শিক্ষার্থীর 1', 'ca': "Most undeciphered lost languages exhibit two characteristics that pose significant decipherment challenges: (1) the scripts are not fully segmented into words;  (2) no es determina el llenguatge més proper conegut. Proposem un model de desxifració que aborda ambdós aquests reptes construint-se en restriccions lingüístices riques que reflexionin patrons consistents en el canvi històric del so. Captem la geometria fonològica natural aprenent incorporacions de caràcters basades en l'Alfabet Fonètic Internacional (IPA). El marc generador resultant modela conjuntament la segmentació de paraules i l'alliniament cognat, informat per restriccions fonològices. Evaluam el model en les llengües desxifrades (gòtica, ugarítica) i en una indexifrada (ibèrica). Els experiments demostren que incorporar geometria fonètica porta a guanys clars i consistents. A més, proposem una mesura per a la aproximació del llenguatge que identifica correctament les llengües relacionades per a la gòtica i la ugarítica. Per a l'iberi, el mètode no mostra fortes evidències que suporten el vasc com a llenguatge relacionat, concordant amb la posició favorita de la beca actual. 1", 'cs': 'Většina nešifrovaných ztracených jazyků vykazuje dvě charakteristiky, které představují významné výzvy pro dešifrování: (1) skripty nejsou plně segmentovány do slov; (2) nejbližší známý jazyk není určen. Navrhujeme dešifrovací model, který zvládne obě tyto výzvy stavěním na bohatých jazykových omezeních odrážejících konzistentní vzorce historické změny zvuku. Zachycujeme přirozenou fonologickou geometrii učením se vložení znaků založených na Mezinárodní fonetické abecedě (IPA). Výsledný generativní rámec společně modeluje segmentaci slov a kognitivní zarovnání, informované fonologickými omezeními. Model hodnotíme jak na dešifrovaných jazycích (gotický, ugaritický) tak na nešifrovaných jazycích (iberština). Experimenty ukazují, že začlenění fonetické geometrie vede k jasným a konzistentním ziskům. Navíc navrhujeme opatření pro jazykovou blízkost, které správně identifikuje příbuzné jazyky pro gotiku a ugaritiku. Pro iberštinu tato metoda neprokazuje silné důkazy podporující baskičtinu jako příbuzný jazyk, což souhlasí s oblíbenou pozicí současného stipendia. 1', 'et': 'Enamikul šifreerimata kadunud keeltel on kaks omadust, mis põhjustavad märkimisväärseid dešifreerimisprobleeme: (1) skriptid ei ole täielikult sõnadeks segmenteeritud; (2) lähimat teadaolevat keelt ei ole kindlaks määratud. Pakume välja dešifreerimismudeli, mis käsitleb mõlemat probleemi, tuginedes rikkalikele keelelistele piirangutele, mis peegeldavad ajaloolise helimuutuse järjepidevaid mustreid. Me jäädvustame loomuliku fonoloogilise geomeetria, õppides rahvusvahelisel foneetilisel tähestikul (IPA) põhinevaid tunnuseid. Sellest tulenev generatiivne raamistik modelleerib ühiselt sõna segmenteerimist ja kognitiivset joondust, lähtudes fonoloogilistest piirangutest. Me hindame mudelit mõlemal detšifreeritud keelel (gooti, ugariitiline) ja ühel tšifreerimata keelel (pürenee keel). Katsed näitavad, et foneetilise geomeetria kaasamine toob kaasa selge ja järjepideva kasu. Lisaks pakume välja keelelise läheduse meetme, mis õigesti identifitseerib gooti ja ugariiti sarnaseid keeli. Pürenee keele puhul ei näita meetod tugevaid tõendeid baski keele kui seotud keele toetamiseks, mis on nõus praeguse stipendiumi eelistatud positsiooniga. 1', 'fi': 'Useimmilla salaamattomilla kadonneilla kielillä on kaksi ominaisuutta, jotka aiheuttavat merkittäviä tulkintahaasteita: (1) skriptejä ei ole täysin segmentoitu sanoiksi; (2) lähintä tunnettua kieltä ei ole määritetty. Ehdotamme purkumallia, joka käsittelee molempia haasteita hyödyntämällä rikkaita kielellisiä rajoitteita, jotka heijastavat johdonmukaisia malleja historiallisessa äänimuutoksessa. Kuvaamme luonnollisen fonologisen geometrian oppimalla kansainvälisen foneettisen aakkosen (IPA) mukaisia merkkiupotuksia. Tuloksena oleva generatiivinen viitekehys mallintaa yhdessä sanasegmentointia ja kognitiivista linjausta fonologisten rajoitusten perusteella. Mallia arvioidaan sekä puretuilla kielillä (goottilainen, ugariittinen) että puremattomilla kielillä (iberialainen). Kokeet osoittavat, että foneettisen geometrian yhdistäminen johtaa selkeisiin ja johdonmukaisiin voittoihin. Lisäksi ehdotamme kielellisen läheisyyden toimenpidettä, joka tunnistaa goottilaisen ja ugarilaisen sukulaiskielen oikein. Iberian kielessä menetelmä ei näytä vahvaa näyttöä baskin tukemisesta sukulaisena kielenä, mikä tukee nykyisen stipendin suosimaa asemaa. 1', 'ha': "Babu mafi yawan harshen da ba'a sãɓa ba, yana nuna wasu sifati biyu wanda ke ƙuncin tsumarni mai girma: (1) umarnin scripta ba su zama cikakken su cikin magana ba; (2) Ba'a ƙayyade harshen da aka sani mafi kusantar. Tuna goyya da wata shekara ta sami biyu masu hanyor waɗannan, da za'a samar da tsari na lugha tajiri, kuma yana yin musamman sami'a cikin musanyawa na historical. Tuna kãma naturan fological geometry da Muke iya karatun karatun idan an sanar da littãfin da ake shigar a kan International Phonet alphat (IPA). @ info: whatsthis Ana ƙaddara misalin a kan lugha guda (Gotic, Agaritic) da wata ba'a karɓi ba (Iberian). Tajarakin na nuna cewa, da za'a shigar da jometiri na Phonet, yana ƙara zuwa kashi mai tsari da mai daidai. Ina ƙaranci, za'a buƙata kafin rufaffiyar lugha da ke gane lugha masu da inganci wa Gotic da Agaritic. Ga Iberian, metoden bã ya nuna bayan bayani masu ƙaranci ga Basque kamar wata harshe na masu husũma, kuma yana sami da mafiya kyauta a yanzu. 1", 'sk': 'Večina nešifriranih izgubljenih jezikov ima dve značilnosti, ki predstavljata pomembne izzive pri dešifriranju: (1) skripti niso popolnoma segmentirani v besede; (2) najbližji znani jezik ni določen. Predlagamo model dešifriranja, ki obravnava oba izziva z gradnjo na bogatih jezikovnih omejitvah, ki odražajo dosledne vzorce zgodovinskih sprememb zvoka. Naravno fonološko geometrijo zajemamo z učenjem vgradnje znakov, ki temeljijo na mednarodni fonetični abecedi (IPA). Izhajajoči generativni okvir skupaj modelira segmentacijo besed in poravnavo kognitov, ob upoštevanju fonoloških omejitev. Model ocenjujemo na obeh dešifriranih jezikih (gotski, ugaritski) in nešifriranem (iberski). Poskusi kažejo, da vključevanje fonetične geometrije vodi do jasnih in doslednih dobičkov. Poleg tega predlagamo ukrep za jezikovno bližino, ki pravilno identificira sorodne jezike za gotsko in ugaritsko. Pri iberskem jeziku metoda ne kaže trdnih dokazov, ki podpirajo baskovščino kot sorodni jezik, kar se strinja s priljubljenim položajem sedanje štipendije. 1', 'jv': 'politenessoffpolite"), and when there is a change ("assertive (2) Perintah pakan kelas nang langgar sing ora bisa diunting. Awak dhéwé ngerasah model sing dadi nggawe ngubah winih iki dadi nggawe ngubah tindakan Awak dhéwé wis ngerasai jeogras pribadi, dipunangé katêpakan karcis sing basa ning basa Gambar Urawa Fonetik (ipA). Laptop" and "Desktop Awak dhéwé kuwi nggunakake model ning awak dhéwé nggambar luwih (Gotik, Ugaritik) lan unggade seneng pisan (Iber). Wang aparan wong liyane Mungkin, kéné supoyo tresnaning kanggo langkung kapan kanggo langgar sapa-kapan ingkang dipatengan kanggo nggago nggo butek karo Ugaritik. kanggo Iberah, kuwi lho kuwi kalungkat sabanjuré nggawe Basque kayané luwih apik, sampeyan ngono kuwi kalungkat awak dhéwé. 1', 'he': 'רוב שפות אבדות לא מוצפנות מציגות שני אופיינים שמעמידים אתגרים משמעותיים לפיתוח: (1) התסריטים לא מוחלקים לחלוטין למילים; (2) השפה הידועה הקרובה ביותר אינה קבועה. אנו מציעים מודל פיתוח שמטפל בשני האתגרים האלה על ידי בניית על מחסומים שפתיים עשירים שמשתקפים דפוסים קבועים בשינוי קול היסטורי. אנחנו לוקחים את הגיאומטריה הפונולוגית הטבעית על ידי לימוד תוספות אופיים מבוססים על אלפאבט הפונטי הבינלאומי (IPA). המסגרת הדורתית המתוצאת בדוגמנים משותפים של סגמנציה מילים וההתאמה קוגנטית, מודיעה על ידי מחסומים פונולוגיים. אנו מעריכים את הדוגמא בשתי שפות מוצפנות (גוטיות, אוגריטיות) ושל שפות לא מוצפנות (איברית). The experiments show that incorporating phonetic geometry leads to clear and consistent gains.  בנוסף, אנו מציעים אמצע לקרוב לשפה שמזהה נכון שפות קשורות לגוטית ואוגריטית. For Iberian, the method does not show strong evidence supporting Basque as a related language, concurring with the favored position by the current scholarship. 1', 'bo': 'གསང་བ་མེད་པའི་སྐད་རིགས་མང་ཆེ་ཤོས་ཀྱིས་གསལ་བཀོད་པའི་ཁྱད་ཆ་གཉིས་ཀྱིས་མཚོན་པའི་ཁྱད་ཆོས་རྟགས་དགོས: (1) སྒྲིག་ཡིག (2) སྐད་རིགས་ཤིག་གི་ཉེ་ཤོས་བའི་སྐད་ཡིག་གཏན་ཁེལ་མི་བྱེད། We propose a decipherment model that handles both of these challenges by building on rich linguistic constraints reflecting consistent patterns in historical sound change. We capture the natural phonological geometry by learning character embeddings based on the International Phonetic Alphabet (IPA). The resulting generative framework jointly models word segmentation and cognate alignment, informed by phonological constraints. ང་ཚོས་རྩིས་མེད་པའི་སྐད་ཡིག་གཉིས་ཀྱིས་མིང་གི་རྣམ་པ་གཉིས་ལས་དབྱེ་བ་བཟོས་བྱས། The experiments show that incorporating phonetic geometry leads to clear and consistent gains. ད་དུང་། ང་ཚོས་སྐད་ཡིག་གདོང་ཚད་གཅིག་ཁར་བྱེད་པར་བསམ་འཆར་བྱེད་ཀྱི་ཡོད། དེ་ནི་ཨི་བེ་རི་ཡཱན་གྱི་ཐབས་ལམ་དེ་ལྟར་བྱ་རིམ་དང་། 1'}
{'en': 'Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement', 'ar': 'محول رسم بياني إلى رسم تكراري غير ذاتي الانحدار لتحليل التبعية مع الصقل التكراري', 'es': 'Transformador de gráfico a gráfico recursivo no autorregresivo para el análisis de dependencias con refinamiento iterativo', 'pt': 'Transformador gráfico a gráfico recursivo não autorregressivo para análise de dependência com refinamento iterativo', 'fr': "Transformateur graphique à graphe récursif non autorégressif pour l'analyse des dépendances avec raffinement itératif", 'zh': '递归非自归图到图转换器,用于迭代细化赖解析', 'ja': '依存関係解析のための再帰的非自動回帰グラフからグラフへの変換', 'hi': 'पुनरावर्ती परिशोधन के साथ निर्भरता पार्सिंग के लिए पुनरावर्ती गैर-Autoregressive ग्राफ-से-ग्राफ ट्रांसफॉर्मर', 'ru': 'Рекурсивный неавторегрессивный графо-графический преобразователь для анализа зависимостей с итеративным уточнением', 'ga': 'Trasfhoirmeoir Graf-go-Graf Atarlaithe Neamh-Uath-chéimnitheach le haghaidh Parsála Spleáchais le Míniúchán atriallach', 'hu': 'Rekurzív, nem önregresszív grafikon-grafikon transzformátor a függőség értelmezéséhez Iteratív finomítással', 'ka': 'რეკურსიური არ- ავტორეგრესიური გრაფიკით- დან გრაფიკით ტრანფორმაციაName', 'lt': 'Recursyvus neautoregresinis grafiko į grafiko transformatorius priklausomybės analizei su pakartotiniu rafinavimu', 'kk': 'Тәуелсіздік талдау үшін авторегрессивні емес графикалық түрлендіруші', 'ms': 'Penukar Graf-ke-Graf Tidak-Autoregresif Berrekursif untuk Menghurai Dependensi dengan Penyulitan Iteratif', 'ml': 'ആത്മാര്\u200dത്ഥികമായ പാര്\u200dസിങ്ങിനുള്ള പാര്\u200dസിങ്ങിന് വേണ്ടി ആത്മാര്\u200dജ്ജികമില്ലാത്ത ഗ്രാഫിലേക്ക്- ലേക്ക്', 'it': "Trasformatore grafico-grafico non autoregressivo ricorsivo per l'analisi della dipendenza con raffinazione iterativa", 'el': 'Ανακλαστικός μετασχηματιστής γραφήματος-σε-γραφήματος για ανάλυση εξάρτησης με επαναλαμβανόμενη βελτίωση', 'mk': 'Рекурсивен неавторегресивен графски трансформер за анализирање зависности со итеративно рефинерирање', 'pl': 'Rekursywny, nieautoresywny transformator graf-to-graf dla parowania zależności z iteracyjnym dopracowaniem', 'mt': 'Trasformatur Rikorsiv Mhux Awtoregressiv tal-Graff għall-Graff għall-Analiżi tad-Dipendenza bir-Riffinazzjoni Iterattiva', 'mn': 'График-с график шилжүүлэгч дахин автоматжуулалт биш-автоматжуулалт', 'no': 'Rekursiv ikkje- autoregressiv grafikk- til- grafikk- transformering for avhengighetstolking med iterativ refinering', 'ro': 'Transformator recursiv grafic-la-grafic non-autoregresiv pentru analizarea dependenței cu rafinare iterativă', 'si': 'ආයාත්මක නොස්වයංක්\u200dරියාත්මක ග්\u200dරාෆ් ටි- ග්\u200dරාෆ් වෙනස් කරණාකරුවා', 'so': 'Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement', 'sr': 'Rekursivni ne-autoregresivni transformator grafika do grafika za analizu zavisnosti sa Iterativnim refinencijom', 'sv': 'Rekursiv icke-autoregressiv graf-till-graftransformator för beroendetolkning med iterativ förfining', 'ta': 'சார்ந்த சார்ந்த பாடலுடன் தானியங்கி கட்டுப்படுத்தல் இல்லாத வரைகலை மாற்றுதல்', 'ur': 'دوبارہ غیر اٹوگریسٹی گراف سے گراف ترفنرسٹر کے لئے اعتمادی پارچینگ کے ساتھ دوبارہ غیر اٹوگریسٹی گراف ترفنرسٹر', 'uz': 'Comment', 'vi': 'Biến đổi đồ họa/ đồ họa cho độ quan hệ với giá trị cảm xúc:', 'bg': 'Рекурсивен неавторегресивен графичен трансформатор за анализ на зависимостта с итеративно усъвършенстване', 'da': 'Rekursiv ikke-autoregressiv graf-til-graf transformator til afhængighed fortolkning med iterativ raffinering', 'hr': 'Rekursivni ne-autoregresivni transformator grafika do grafika za analizu zavisnosti s iteratnim refinencijom', 'nl': 'Recursieve niet-autoregressieve grafiek-naar-grafiek transformator voor afhankelijkheidsparsing met iteratieve verfijning', 'id': 'Transformer Graf-ke-Graf Non-Autoregressiv Recursif untuk Parsing Dependency dengan Refinement Iteratif', 'de': 'Rekursiver, nicht autoregressiver Graph-to-Graph Transformator für Abhängigkeitsparsing mit iterativer Verfeinerung', 'fa': 'تغییردهنده گراف به گراف غیر خودگریسی برای تولید بستگی با تغییر دوباره', 'sw': 'Tafsiri isiyo na kujidhibiti-Graph-hadi-Graph kwa ajili ya Kuchapisha Utegemea na Uchambuzi', 'tr': 'Taýiksyzlyk Taýiksyzlyk üçin otoregressiv gaýd edilmez grafik we grafik terjimeçisi', 'ko': '비자귀환도에서 그림 변환기로 귀속, 반복적으로 정밀도를 구하는 의존 해석에 사용', 'sq': 'Transformuesi rekursiv jo-autoregresiv i grafikut në grafik për analizimin e varësive me rafinimin iterativ', 'az': 'D칬vl톛tlik S톛fl톛m톛si 칲칞칲n Avtomatik-Avtomatik Grafik-to-Grafik Transformat칞캼s캼', 'af': 'Rekursief Nie- Autoregressief Graaf- na- Graaf Transformeerder vir Afhanklikheid Ontlegging met Iteratief Verwydering', 'hy': 'Ռեկուրսիվ ոչ ինքնառեգրեսիվ գրաֆի-գրաֆի փոխարինողը կախվածության վերլուծության համար իտերատիվ վերլուծությամբ', 'bs': 'Rekursivni ne-autoregresivni transformator grafika do grafika za analizu zavisnosti s iteratnim refinencijom', 'ca': "Transformer de gràfic a gràfic no autoregressiv recursiv per a l'analització de dependencies amb raffinement iteratiu", 'am': 'ምርጫዎች', 'cs': 'Rekurzivní neregresivní transformátor grafu na graf pro analýzu závislosti s iterativním vylepšením', 'bn': 'নির্ভরিত পার্সিং এর জন্য পুনরায় স্বয়ংক্রিয়ভাবে স্বয়ংক্রিয়ভাবে গ্রাফ- থেকে গ্রাফ- ট্রান্সফার্নার', 'fi': 'Rekursiivinen ei-autoregressiivinen grafiikkamuuntaja riippuvuuden käsittelyyn iteratiivisella viimeistelyllä', 'et': 'Rekursiivne mitte-autoregressiivne graafiline trafo sõltuvuse parsimiseks iteratiivse rafineerimisega', 'jv': 'undo-type', 'he': 'מעבר גרף לגרף לגרף לא אוטורגרסיבי', 'ha': 'Transformation for Debugger and Debugger', 'sk': 'Rekurzivni nesamoregresivni grafični transformator za razčlenitev odvisnosti z iterativno rafiniranjem', 'bo': 'Rekursive non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement'}
{'en': 'We propose the Recursive Non-autoregressive Graph-to-Graph Transformer architecture (RNGTr) for the iterative refinement of arbitrary graphs through the recursive application of a non-autoregressive Graph-to-Graph Transformer and apply it to syntactic dependency parsing. We demonstrate the power and effectiveness of RNGTr on several dependency corpora, using a refinement model pre-trained with BERT. We also introduce Syntactic Transformer (SynTr), a non-recursive parser similar to our refinement model. RNGTr can improve the accuracy of a variety of initial parsers on 13 languages from the Universal Dependencies Treebanks, English and Chinese Penn Treebanks, and the German CoNLL2009 corpus, even improving over the new state-of-the-art results achieved by SynTr, significantly improving the state-of-the-art for all corpora tested.', 'ar': 'نقترح بنية محول الرسم البياني إلى الرسم البياني التكراري غير التلقائي (RNGTr) من أجل التحسين التكراري للرسوم البيانية التعسفية من خلال التطبيق التكراري لمحول الرسم البياني إلى الرسم البياني غير الانحدار التلقائي وتطبيقه على تحليل التبعية النحوية. نبرهن على قوة وفعالية RNGTr على العديد من مؤسسات التبعية ، باستخدام نموذج صقل تم تدريبه مسبقًا باستخدام BERT. نقدم أيضًا المحول النحوي (SynTr) ، وهو محلل غير تكراري مشابه لنموذج التحسين الخاص بنا. يمكن لـ RNGTr تحسين دقة مجموعة متنوعة من المحللين الأوليين في 13 لغة من تريبانكس يونيفرسال تريبانكس والإنجليزية والصينية بين تريبانكس ، ومجموعة CoNLL2009 الألمانية ، وحتى تحسين النتائج الجديدة التي حققتها SynTr بشكل ملحوظ تحسين الدولة من بين الفن لجميع المؤسسات التي تم اختبارها.', 'fr': "Nous proposons l'architecture Récursive Non-autorégressive Graph-to-Graph Transformer (RNGTR) pour le raffinement itératif de graphes arbitraires par l'application récursive d'un Transformateur Graph-to-Graph non autorégressif et l'appliquons à l'analyse syntaxique des dépendances. Nous démontrons la puissance et l'efficacité du rNGTr sur plusieurs corpus de dépendance, à l'aide d'un modèle de raffinement pré-entraîné avec BERT. Nous introduisons également Syntactic Transformer (SyntR), un analyseur non récursif similaire à notre modèle de raffinement. RnGTr peut améliorer la précision d'une variété d'analyseurs initiaux sur 13 langues, des Banques d'arbres des dépendances universelles, des Penn Treebanks anglais et chinois et du corpus allemand ConLL2009, améliorant même les nouveaux résultats de pointe obtenus par SynTR, améliorant considérablement la pointe de la technologie pour Tous les corpus ont été testés.", 'pt': 'Propomos a arquitetura Recursive Non-autoregressive Graph-to-Graph Transformer (RNGTr) para o refinamento iterativo de grafos arbitrários através da aplicação recursiva de um Transformador Graph-to-Graph não autorregressivo e aplicá-lo à análise sintática de dependência. Demonstramos o poder e a eficácia do RNGTr em vários corpora de dependência, usando um modelo de refinamento pré-treinado com BERT. Também apresentamos o Syntactic Transformer (SynTr), um analisador não recursivo semelhante ao nosso modelo de refinamento. O RNGTr pode melhorar a precisão de uma variedade de analisadores iniciais em 13 idiomas do Universal Dependency Treebanks, do Penn Treebanks em inglês e chinês e do corpus alemão CoNLL2009, melhorando até mesmo os novos resultados de última geração alcançados pelo SynTr, significativamente melhorando o estado da arte para todos os corpora testados.', 'es': 'Proponemos la arquitectura de transformador gráfico a gráfico no autorregresivo recursivo (rNGTR) para el refinamiento iterativo de gráficos arbitrarios mediante la aplicación recursiva de un transformador gráfico a gráfico no autorregresivo y lo aplicamos al análisis sintáctico de dependencias. Demostramos el poder y la eficacia de rNGTR en varios cuerpos de dependencia, utilizando un modelo de refinamiento previamente entrenado con BERT. También presentamos Syntactic Transformer (SynTR), un analizador no recursivo similar a nuestro modelo de refinamiento. RNgTR puede mejorar la precisión de una variedad de analizadores iniciales en 13 idiomas, desde los bancos de árboles de dependencias universales, los bancos de árboles Penn en inglés y chino y el corpus alemán ConLL2009, incluso mejorando los nuevos resultados de vanguardia logrados por SynTR, mejorando significativamente el estado de todos los cuerpos han sido probados.', 'ja': '再帰的非自己回帰グラフツーグラフ変換アーキテクチャ（ RNGTr ）を提案し、非自己回帰グラフツーグラフ変換の再帰的適用を通じて任意のグラフを反復的に微細化し、構文依存性解析に適用します。私たちは、BERTで事前にトレーニングされた洗練モデルを使用して、いくつかの依存性コーラに対するRNGTrのパワーと有効性を実証します。また、絞り込みモデルに似た非再帰的パーサーであるSynTr （ Syntactic Transformer ）も紹介します。RNGTrは、Universal Dependencies Treebanks、English and Chinese Penn Treebanks、およびGerman CoNLL 2009コーパスの13の言語のさまざまな初期パーサーの精度を向上させることができ、SynTrによって達成された最新の結果よりもさらに向上し、テストされたすべてのコーパの最先端を大幅に向上させます。', 'hi': 'हम पुनरावर्ती गैर-autoregressive ग्राफ-टू-ग्राफ ट्रांसफॉर्मर आर्किटेक्चर (RNGTr) एक गैर-autoregressive Graph-to-Graph Transformer के पुनरावर्ती अनुप्रयोग के माध्यम से मनमाने ढंग से रेखांकन के पुनरावर्ती परिशोधन के लिए प्रस्ताव करते हैं और इसे वाक्यात्मक निर्भरता पार्सिंग पर लागू करते हैं। हम कई निर्भरता निगम पर RNGTr की शक्ति और प्रभावशीलता का प्रदर्शन करते हैं, BERT के साथ पूर्व-प्रशिक्षित एक शोधन मॉडल का उपयोग करते हैं। हम सिंटैक्टिक ट्रांसफॉर्मर (SynTr) भी पेश करते हैं, जो हमारे शोधन मॉडल के समान एक गैर-पुनरावर्ती पार्सर है। RNGTr यूनिवर्सल निर्भरता Treebanks, अंग्रेजी और चीनी पेन Treebanks, और जर्मन CoNLL2009 कॉर्पस से 13 भाषाओं पर प्रारंभिक पार्सर की एक किस्म की सटीकता में सुधार कर सकते हैं, यहां तक कि SynTr द्वारा प्राप्त नए अत्याधुनिक परिणामों पर भी सुधार, काफी सुधार सभी corpora परीक्षण के लिए राज्य के कला में सुधार.', 'ru': 'Предложена рекурсивная неавторегрессивная архитектура граф-граф-трансформатора (RNGTr) для итерационного уточнения произвольных графов посредством рекурсивного применения неавторегрессивного граф-граф-трансформатора и применена к синтаксическому синтаксическому разбору зависимостей. Мы демонстрируем силу и эффективность RNGTr на нескольких зависимых телах, используя модель уточнения, предварительно обученную с BERT. Мы также представляем синтаксический трансформатор (SynTr), нерекурсивный парсер, аналогичный нашей модели уточнения. RNGTr может улучшить точность различных начальных парсеров на 13 языках от Universal Dependencies Treebanks, английского и китайского Penn Treebanks, и немецкого CoNLL2009 корпуса, даже улучшая по сравнению с новыми современными результатами, достигнутыми SynTr, значительно улучшая современное для всех протестированных корпусов.', 'zh': '臣等建递归非自归图到图转换器架构(RNGTr),因递归宜用非自归图到图转换器以迭代细化任情,而施于句法赖解析。 吾用BERT豫教之优化,示RNGTr数恃语料库之强有效性。 引入语法转换器(SynTr),类吾细化之非递归解析器也。 RNGTr 可以崇通赖树库、英文及中文 Penn 树库及德语 CoNLL2009 语料库中 13 言语诸初解析器之准确性,甚于 SynTr 先进之新有所提高,显加诸试语料库之最新技术水平。', 'ga': 'Molaimid an ailtireacht Athchúrsach Neamh-uathchéimneach Graf-go-Graph (RNGTr) chun graif treallach a mhionchoigeartú go atriallach trí Trasfhoirmeoir Graf-go-Graph neamh-uathchéimneach a chur i bhfeidhm go hathchúrsach agus cuirimid i bhfeidhm é ar pharsáil spleáchais chomhréire. Léirímid cumhacht agus éifeachtúlacht RNGTr ar roinnt corparáidí spleáchais, ag baint úsáide as múnla mionchoigeartaithe atá réamhoilte le BERT. Tugaimid isteach freisin Syntactic Transformer (SynTr), parsálaí neamh-athfhillteach atá cosúil lenár múnla mionchoigeartaithe. Is féidir le RNGTr feabhas a chur ar chruinneas parsálaithe tosaigh éagsúla ar 13 theanga ó na Treebanks Uilíoch-spleáchais, Banc na gCrann Penn Béarla agus na Síne, agus corpas Gearmáinis CoNLL2009, fiú ag feabhsú thar na torthaí úrscothacha a bhain SynTr amach, go suntasach. feabhas a chur ar an úrscothacht do gach corpra a tástáladh.', 'el': 'Προτείνουμε την αρχιτεκτονική αναδρομικού μη αυτοανακλαστικού μετασχηματιστή γραφήματος-σε-γραφήματος (για τον επαναληπτικό καθαρισμό αυθαίρετων γραφημάτων μέσω της αναδρομικής εφαρμογής ενός μη αυτοανακλαστικού μετασχηματιστή γραφήματος-σε-γραφήματος και την εφαρμόζουμε στην ανάλυση συντακτικής εξάρτησης. Επιδεικνύουμε τη δύναμη και την αποτελεσματικότητα του σε διάφορα σώματα εξάρτησης, χρησιμοποιώντας ένα μοντέλο τελειοποίησης προενταξιακό με BERT. Παρουσιάζουμε επίσης τον Συντακτικό Μετασχηματιστή (SynTr), έναν μη αναδρομικό αναλυτή παρόμοιο με το μοντέλο βελτίωσης μας. Η RNGTr μπορεί να βελτιώσει την ακρίβεια μιας ποικιλίας αρχικών αναλύσεων σε 13 γλώσσες από τις παγκόσμιες τράπεζες δέντρων εξαρτήσεων, τις αγγλικές και κινεζικές τράπεζες δέντρων και το γερμανικό σώμα βελτιώνοντας ακόμη και τα νέα αποτελέσματα τελευταίας τεχνολογίας που επιτυγχάνονται από το SynTr, βελτιώνοντας σημαντικά την κατάσταση της τεχνολογίας για όλα τα σώματα που εξετάζονται.', 'ka': 'ჩვენ შეგიძლიათ რეკურსიური არ- ავტორეგრესიური გრაფი- დან- გრაფის ტრანფორმეტრის აქტიქტიქტურაციას (RNGTr) არბრიტური გრაფიკის რეფინქციური რეფინქციოს რეკურსიური პროგრამის გამოყენება, რომელიც არ- ავტორეგრესიური გრაფიკის რეფ ჩვენ გამოჩვენებთ RNGTr-ის ძალიან და ეფექტიურობას რამდენიმე დასამხოლოდ კოპორაზე, გამოყენებული რეფინექციის მოდელს BERT-თან პრებინექტირებული. ჩვენ ასევე სინტექტიკური ტრანფორმაციელი (SynTr) ჩვენი რეფინქციის მოდელისთვის მსგავსი არა რეკურსიგური პასუტერი დავიყენებთ. RNGTr შეუძლია უფრო უფრო მნიშვნელოვანი პარაზერების მართლაობას 13 ენაში, რომლებიც სამყარო განსაკუთრებულებების Treebanks, ანგლისური და ჩინგლისური Penn Treebanks, და გერმანული CoNLL2009 კორპუსი, რომლებიც უფრო უფრო უფრო უფრო მნიშვნელოვანია სინტერის ახალი წარმოდგ', 'hu': 'Javasoljuk a Recursive Non-autoregressive Graph-to-Graph Transzformátor architektúrát (RNGTr) a tetszőleges grafikonok iteratív finomítására egy nem autoregressive Graph-to-Graph Transzformátor rekurzív alkalmazásával, és alkalmazzuk azt a szintaktikus függőség elemzésére. Az RNGTr hatékonyságát és hatékonyságát több függőségi társaságon is bemutatjuk, egy BERT által előre kiképzett finomítási modell segítségével. Bemutatjuk a Syntactic Transformert (SynTr), egy nem rekurzív elemzőt, amely hasonló a finomítási modellünkhöz. Az RNGTr 13 nyelven javíthatja a különböző kezdeti elemzők pontosságát az Universal Dependencies Treebanks, az angol és a kínai Penn Treebanks, valamint a német CoNL2009 korpusz 13 nyelvén, még a SynTr által elért új, korszerű eredményekhez képest is, jelentősen javítva az összes tesztelt corpora állapotát.', 'kk': 'Біз авторегрессивно емес графикалық түрлендіруші архитектурасын (RNGTr) қайталау үшін арбитралық графиктерді қайталау үшін авторегрессивно емес графикалық түрлендіруші қайталанатын қолданбаны және оны синтактикалық тәуелсіздік талдау үшін қолданамыз. Біз бірнеше тәуелсіздік корпораға RNGTr қуатты және эффектілігін көрсетедік, BERT- мен алдын- ала оқылған рефининг үлгісін қолдануға арналған. Сонымен қатар синтактикалық түрлендіруші (SynTr) дегенді таңдаймыз. Қайталану үлгісімізге ұқсас емес түрлендіруші болады. RNGTr Барлық Тәуелсіздіктерінің 13 тілдегі бастапқы талдаушылардың дұрыстығын жасай алады, ағылшын және Қытай Пен Брибанктарының ағылшын және Қытай Пен Брибанктарының 13 тілдегі бастапқы талдаушылардың дұрыстығын жасай алады, сондай-ақ неміс КоNLL2009 корпус SynTr-ның жаңа жа', 'it': "Proponiamo l'architettura Recursive Non-autoregressive Graph-to-Graph Transformer (RNGTr) per il perfezionamento iterativo di grafici arbitrari attraverso l'applicazione ricorsiva di un Graph-to-Graph Transformer non autoregressivo e la applichiamo all'analisi sintattica delle dipendenze. Dimostriamo la potenza e l'efficacia di RNGTr su diversi corpi di dipendenza, utilizzando un modello di perfezionamento pre-addestrato con BERT. Presentiamo anche Syntactic Transformer (SynTr), un parser non ricorsivo simile al nostro modello di raffinazione. RNGTr può migliorare l'accuratezza di una varietà di parser iniziali su 13 lingue dagli Universal Dependences Treebanks, inglese e cinese Penn Treebanks e il corpus tedesco CoNL2009, migliorando anche rispetto ai nuovi risultati all'avanguardia raggiunti da SynTr, migliorando significativamente lo stato dell'arte per tutti i corpora testati.", 'lt': 'Mes pasiūlome rekursyvią neegoregresinę grafiko į grafiką transformatoriaus architektūrą (RNGTr), skirtą pakartotiniam savavališkų grafikų tobulinimui taikant rekursyvią neegoregresinį grafiko į grafiką transformatorių taikymą ir ją taikyti sintaktiniam priklausomybės analizavimui. Mes įrodome RNGTr galią ir veiksmingumą kelioms priklausomybės korporams, naudojant iš anksto parengtą tobulinimo model į su BERT. Taip pat pristatome Sintaktinį transformatorių (SynTr), nerekursyvų analizatorių, panašų į mūsų rafinavimo model į. RNGTr gali pagerinti įvairių pradinių analizatorių tikslumą 13 kalbų iš Universal Dependencies Treebanks, English and Chinese Penn Treebanks ir Vokietijos CoNLL2009 korpus, net pagerindamas naujausius SynTr pasiektus rezultatus ir gerokai pagerindamas visų bandomų korpų pažangą.', 'mt': 'Aħna nipproponu l-arkitettura tat-Trasferiment tal-Grafika għall-Grafika mhux awtoregressiva rikorrenti (RNGTr) għar-raffinar iterattiv tal-graffi arbitrarji permezz tal-applikazzjoni rikorrenti ta’ Trasferiment tal-Grafika għall-Grafika mhux awtoregressiva u nagħmluha għall-analiżi tad-dipendenza sintattika. We demonstrate the power and effectiveness of RNGTr on several dependency corpora, using a refinement model pre-trained with BERT.  We also introduce Syntactic Transformer (SynTr), a non-recursive parser similar to our refinement model.  RNGTr can improve the accuracy of a variety of initial parsers on 13 languages from the Universal Dependencies Treebanks, English and Chinese Penn Treebanks, and the German CoNLL2009 corpus, even improving over the new state-of-the-art results achieved by SynTr, significantly improving the state-of-the-art for all corpora tested.', 'ms': 'Kami cadangkan arkitektur Graf-ke-Graf Transformer (RNGTr) bukan-autoregresif untuk penarafan iteratif graf arbitrari melalui aplikasi rekursif bagi Graf-ke-Graf Transformer bukan-autoregresif dan aplikasikannya pada penghuraian dependensi sintaktik. Kami menunjukkan kekuatan dan efektiviti RNGTr pada beberapa korpra dependensi, menggunakan model penarafan terlatih-terlatih dengan BERT. Kami juga memperkenalkan Penukar Sintaktik (SynTr), penghurai tidak rekursif yang sama dengan model penarafan kami. RNGTr boleh meningkatkan ketepatan pelbagai parser awal pada 13 bahasa dari Universal Dependencies Treebanks, English and Chinese Penn Treebanks, dan korpus CoNLL2009 Jerman, walaupun meningkatkan keputusan baru-state-of-the-art yang dicapai oleh SynTr, meningkatkan keadaan-state-of-the-art untuk semua korpra yang diuji.', 'mk': 'Предложуваме рекурсивна неавторегресивна графска трансформирана архитектура (RNGTr) за итеративно рафинирање на арбитралните графики преку рекурсивна апликација на неавторегресивна графска трансформирана и апликација на синтактичката анализа на зависноста. Ја демонстрираме моќта и ефикасноста на РНГТр на неколку зависни корпора, користејќи модел за рафинансирање предобучен со БЕРТ. Исто така, го претставуваме Синтактичкиот трансформирач (SynTr), нерекурсивен анализатор сличен на нашиот модел за рафинансирање. РНГТр може да ја подобри прецизноста на различните првични анализатори на 13 јазици од Universal Dependencies Treebanks, English and Chinese Penn Treebanks, и германскиот корпус CoNLL2009, дури и подобрувајќи се во врска со новите најсовремени резултати постигнати од Синтр, значително подобрувајќи ја современоста на сите тестирани корпора.', 'no': 'Vi foreslår rekursivt ikkje- autoregressivt graf- til- graf- transformeringsarkitektur (RNGTr) for den gjentekravne refinering av tilfeldige grafen gjennom rekursivt program av eit ikkje- autoregressivt graf- til- graf- transformerer og bruk den til syntaksisk tolking av avhengighet. Vi demonstrerer kraft og effektiviteten av RNGTr på fleire avhengighetskorporat, ved å bruka ein refineringsmodell før-trent med BERT. Vi introduserer også syntaksisk transformer (SynTr), eit ikkje- rekursivt tolkar som liknar refinermodellen vår. RNGTr kan forbedra nøyaktigheten av ein del første analyserar på 13 språk frå Universell avhengighet Treebanks, engelsk og kinesisk Penn Treebanks, og den tyske CoNLL2009-korpusen, selv forbedring over den nye kunsttilstanden-resultatene som er oppretta av SynTr, og betydelig forbedring av kunsttilstanden for alle testerte korporane.', 'ml': 'We propose the Recursive Non-autoregressive Graph-to-Graph Transformer architecture (RNGTr) for the iterative refinement of arbitrary graphs through the recursive application of a non-autoregressive Graph-to-Graph Transformer and apply it to syntactic dependency parsing.  ബെര്\u200dട്ടിയോടൊപ്പം പരിശീലനത്തിനു മുന്\u200dപ് പരിശീലനം കൊണ്ട് ഞങ്ങള്\u200d RNGTr-ന്\u200dറെ ശക്തിയും പ്രവർത്തനവും കാണിച്ചു തരുന്നു. ഞങ്ങള്\u200d സിന്റാക്റ്റിക് ട്രാന്\u200dസ്ഫാര്\u200dമാറ്റര്\u200d (സിന്\u200dടിരി) പരിചയപ്പെടുത്തുന്നു. നമ്മുടെ പ്രവര്\u200dത്തനങ്ങളുടെ മ യൂണിവല്\u200d ഡിപ്പെന്\u200dസ് ട്രെബാങ്കുകള്\u200d, ഇംഗ്ലീഷ് പെന്\u200d ട്രീബാങ്കുകളില്\u200d നിന്നും ചൈനീസ് പെന്\u200d ട്രീബാങ്കുകളില്\u200d നിന്നും ആര്\u200dഎന്\u200dജിടിരില്\u200d നിന്നും പ്രധാനപ്പെടുത്തുന്ന ഒരു വ്യത്യസ്തമായ പാര്\u200dസര്\u200dസിന്\u200dറെ പുതിയ', 'ro': 'Propunem arhitectura Transformer Grafic-Graph non-autoregressiv (RNGTr) pentru rafinarea iterativă a graficelor arbitrare prin aplicarea recursivă a unui Transformer Grafic-Graph non-autoregressiv și aplicarea acestuia la analizarea dependențelor sintactice. Demonstrăm puterea și eficacitatea RNGTr pe mai multe corpore de dependență, folosind un model de rafinare pre-instruit cu BERT. De asemenea, introducem Syntactic Transformer (SynTr), un parser non-recursiv similar modelului nostru de rafinare. RNGTr poate îmbunătăți acuratețea unei varietăți de analize inițiale pe 13 limbi de la Universal Dependences Treebanks, engleză și chineză Penn Treebanks și corpul german CoNL2009, chiar îmbunătățind față de noile rezultate de ultimă oră obținute de SynTr, îmbunătățind semnificativ starea de ultimă oră pentru toate corporele testate.', 'pl': 'Proponujemy architekturę Rekursywnego Nieautoresywnego Transformera Graph-to-Graph (RNGTr) do iteracyjnego udoskonalania dowolnych wykresów poprzez rekursywne zastosowanie nieautoresywnego Transformera Graph-to-Graph i zastosowanie go do parsowania zależności składni. Wykazujemy moc i skuteczność RNGTr na kilku korpusach zależności, wykorzystując model udoskonalenia wstępnie przeszkolony z BERT. Wprowadzamy również Syntaktic Transformer (SynTr), nierekursywny parser podobny do naszego modelu udoskonalenia. RNGTr może poprawić dokładność różnych wstępnych parserów na 13-językach z Universal Dependencies Treebanks, angielskich i chińskich Penn Treebanks oraz niemieckiego korpusu CoNLL2009, nawet poprawiając nowe najnowocześniejsze wyniki uzyskane przez SynTr, znacząco poprawiając stan techniki dla wszystkich testowanych korpusów.', 'sr': 'Predlažemo rekursivnu ne-autoregresivnu arhitekturu transformatora grafika do grafa (RNGTr) za iterativnu refinenciju arbitrarnih grafa kroz rekursivnu primjenu ne-autoregresivnog transformatora grafika do grafa i primjenjujemo ga na analizu sintaktične zavisnosti. Pokazujemo snagu i učinkovitost RNGTr na nekoliko korpora zavisnosti, koristeći model refinencije predobučen BERT-om. Takoðe predstavljamo sintaktièki transformator (SynTr), neprekursivni analizator slièan našoj refinerskoj modeli. RNGTr može poboljšati tačnost različitih početnih parsera na 13 jezika od Univerzalnih nezavisnih Treebanka, engleskih i kineskih Penn Treebanka i njemačkog korpusa CoNLL2009, čak i poboljšati nove rezultate umjetnosti koje je ostvario Syntr, značajno poboljšavajući stanje umjetnosti za sve testovane korpore.', 'si': 'අපි ප්\u200dරශ්නයක් කරනවා නොස්වයංක්\u200dරීය ග්\u200dරාෆ් ට්-ග්\u200dරාෆ් වෙනස්ථාපකය (RNGTr) සඳහා සාමාන්\u200dය ග්\u200dරාෆ් වලට ප්\u200dරශ්නයක් නොස්වයංක්\u200dරීය ග්\u200dරාෆ් ට්-ග්\u200dරාෆ් වෙනස්ථ අපි RNGTr ගේ ශක්තිය හා අවශ්\u200dයතාවක් පෙන්වන්න පුළුවන් වෙනවා, BERT එක්ක ප්\u200dරධානයක් ප්\u200dරයෝජනය කරනවා. අපි සම්පූර්ණ වාර්ථාපකය (SynTr), අපේ ප්\u200dරවර්ණ වාර්ථාපකය සමග නොප්\u200dරවර්ණ වාර්ථාපකයෙක්. RNGTr පුළුවන් පුළුවන් විශේෂ භාෂාවක් 13 වලින් ප්\u200dරධාන පරීක්ෂකයේ විශේෂ විශේෂ විශේෂ විශේෂ බැන්ක්ස්, ඉංග්\u200dරීසි සහ චීන් පෙන් ට්\u200dරී බැන්ක්ස්, සහ ජර්මාන් CoNLL2009 කොර්', 'ta': 'நாம் மீண்டும் திரும்பச் செயல்படுத்தும் தானியங்கி கட்டுப்பாட்டு மாற்றும் உருவாக்கும் வரைபடங்களை சரிபார்ப்பதற்கான திரும்பச் செய்து ஒரு தானாக்கியமைக்காத வரைகலை மாற்றுதல் மூலம நாம் பல சார்ந்த சார்பு நிறுவனத்தில் RNGTr மின் சக்தி மற்றும் விளைவை காட்டுகிறோம், BERT முன் பயிற்சி முன்பு பயிற்சி ம நாம் ஒத்திசைவு மாற்றுதலை குறிப்பிடுகிறோம், திரும்ப திரும்ப மாதிரியில் போன்ற ஒரு திரும்பப் பரிசுத்தம RNGTr பல்வேறு முதல் பார்சர்களின் சரித்திரத்தை மேம்படுத்த முடியும் உலகளாவிய சார்ந்த சேர்ப்புகள், ஆங்கிலம் மற்றும் சீனா பென் ட்ரீபாங்க்ஸ், மற்றும் ஜெர்மன் கோன்எல் 2009 நிறுவனத்தில் இருந்து சின்டிர் மூலம் செ', 'sv': 'Vi föreslår den rekursiva icke-autoregressiva graf-till-graftransformatorarkitekturen (RNGTr) för iterativ förfining av godtyckliga grafer genom rekursiv tillämpning av en icke-autoregressiv graf-till-graftransformator och tillämpar den för syntaktisk beroendetolkning. Vi demonstrerar kraften och effektiviteten hos RNGTr på flera beroendecorpora, med hjälp av en förfiningsmodell som är färdigutbildad med BERT. Vi introducerar också Syntactic Transformer (SynTr), en icke-rekursiv parser som liknar vår förfiningsmodell. RNGTr kan förbättra noggrannheten hos en mängd olika initiala tolkare på 13 språk från Universal Dependences Treebanks, engelska och kinesiska Penn Treebanks och den tyska CoNL2009 corpus, även förbättra jämfört med de nya state-of-the-art resultat som uppnåtts av SynTr, vilket avsevärt förbättrar state-of-the-art för alla testade corpora.', 'so': 'Waxaynu soo jeedaynaa dhismaha ku wareegista ee aan auto-regressive-to-Graph-graph (RNGTr) in loo sameeyo sawir rasmi ah oo la soo celiyo dalbashada dib u soo celinta Graf-to-Graph-Graf-aan-auto-regressive, waxaana lagu codsanaynaa baarlamaanka syntactic-dependency. Waxaynu muujinnaa xoogga iyo shaqeynta RNGTr oo ku saabsan shirkado badan oo ay ku xiran tahay, waxaynu isticmaalnaa model refinement horay loo tababaray BERT. Sidoo kale waxaynu soo bandhignaa tartamaha Syntactic (SynTr), kaas oo u eg sameynta sameynta hagitaanka. RNGTr wuxuu hagaajin karaa saxda baarlamayaasha bilowga ah oo ku qoran 13 luuqadood oo kala duduwan oo ay ka hagaajin karaan Deebanks, Ingiriis iyo Shiino Penn Treebanks, iyo kooxda Jarmalka CoNLL2009, xataa horumarinta resultinta sanadka ee SinTr ku soo gaadhay, si weyn ayuu u hagaajin karaa xaaladda-sanadda ee loo tijaabiyey shirkadaha oo dhan.', 'mn': 'Бид өөрийн график болон график түрүүгч архитектур (RNGTr) нь авторегрессийн график болон график түрүүгч болохгүй график түрүүгч болон синтактикийн хамааралтай хуваалцааны дахин дахин дахин сэргээгдэхийг санал болгож байна. Бид RNGTr-ын хүч болон үр дүнг олон хамааралтай корпора дээр харуулж, BERT-тэй өмнө сургалтын загварыг ашиглаж байна. Бид мөн Синтактик Трансформатор (Syntr), бидний рефинингийн загвартай адилхан дахин дахин дахин дахин дахин дахин дахин дахин дахин дахин хуваалцагч гэдгийг танилцуулдаг RNGTr нь Universal Dependencies Treebanks, English, Chinese Penn Treebanks болон Германы CoNLL2009 корпус болон Syntr-ын шинэ урлагийн үр дүнг улам сайжруулж чадна.', 'ur': 'ہم ایک غیر-autoregressive Graph-to-Graph Transformer architecture (RNGTr) کے ذریعہ ایک غیر-autoregressive Graph-to-Graph Transformer کے دوبارہ کاروبار کے ذریعہ دوبارہ کریں اور اسے سینٹاکتیک اعتماد پارسینٹ پر لازم کریں. ہم نے RNGTr کی قدرت اور فعالیت کو چند اعتمادی کورپورا پر دکھایا ہے، BERT کے ساتھ پہلے تطابق کی ایک رافینٹ موڈل کے استعمال سے۔ ہم نے بھی سینٹکتیک ٹرنفسر (Syntr) کو معرفی کیا ہے، ایک غیر دوبارہ پارچر جو ہمارے پارکینٹ موڈل کے برابر ہے۔ RNGTr نے 13 زبانوں کے بارے میں ایک مختلف شروع پارس کی صحیح عمدہ کر سکتا ہے کہ انگلیسی اور چینی پن تری بانک کے درختوں سے 13 زبانوں پر، اور جرمانی کونLL2009 کورپوس کو بھی بہترین کر سکتا ہے، جو سینٹر کے ذریعہ پہنچ چکی ہے، اور سب کورپورا کے لئے بہترین شروع کی حالت عمدہ کر سکتا ہے.', 'uz': "Biz raqamli raqamlarni avtomatik boshqarish nazoratuvchi Graph-to-Graph tarjima etish uchun arbitratik grammatikal refektlarni o'zgartirish va qaytariv graf-to-Graph tarjima qilish uchun qoʻllanmagan va buni sintaktikk ishlatadigan tashqi ravishda qoʻllash mumkin. Biz bir necha tashkilotga ishlatadigan RNGTr kuchi va effektiyasini ko'rsatdik, BERT bilan birinchi o'rganish modeldan foydalanish mumkin. Biz buni rejalashtirish modelimizga oʻxshash qo'llanmagan sintaktik tarjima (SynTr) bilan ishlatamiz. Name", 'vi': 'Chúng tôi đề nghị tái tạo các đồ thị ngẫu nhiên qua một ứng dụng khác thường của một bộ chế biến đồ họa siêu việt, và áp dụng nó vào chế độ phân tách phụ thuộc pháp thuật. Chúng tôi chứng minh năng lượng và hiệu quả của RNGTr dựa trên nhiều phụ thuộc hạ, sử dụng một mô hình lọc được tiền luyện với BERT. Chúng tôi cũng giới thiệu robot biến hình (SynTr), một phân tích không đệ nhất tương tự với mẫu lọc của chúng tôi. RNGTr có thể cải thiện độ chính xác của một số phân tích ban đầu trên ngôn ngữ 13 từ Universal Depot Treebanks, English and China Penn Treebanks, and the German CoNL2009 Corpus, even improving over the new state-of-art results Achieved bởi SynTr, considerable improving the status-of-the-art for all corporus tested.', 'bg': 'Предлагаме Рекурсивната Неавторегресивна Граф-Граф трансформаторна архитектура (РНГТр) за итеративно усъвършенстване на произволни графики чрез рекурсивно приложение на неавторегресивен Граф-Граф трансформатор и го прилагаме при синтактичен анализ на зависимостта. Ние демонстрираме силата и ефективността на РНГТр върху няколко корпуса на зависимост, използвайки модел на усъвършенстване, предварително обучен с BERT. Също така въвеждаме Синтактичен трансформатор (SynTr), нерекурсивен анализатор, подобен на нашия модел за усъвършенстване. РНГТр може да подобри точността на различни първоначални анализатори на 13 езика от Универсалните зависимости, Английските и Китайските Пен Трейбанки и германския корпус, дори да се подобри над новите най-съвременни резултати, постигнати от СинTr, значително подобрявайки състоянието на всички тествани корпуси.', 'nl': 'We stellen de recursieve niet-autoregressieve Graph-to-Graph Transformer architectuur (RNGTr) voor voor de iteratieve verfijning van willekeurige grafieken door de recursieve toepassing van een niet-autoregressieve Graph-to-Graph Transformer en passen deze toe op syntactische afhankelijkheidsparsing. We demonstreren de kracht en effectiviteit van RNGTr op verschillende afhankelijkheidscorpora, met behulp van een verfijningsmodel voorgetraind met BERT. We introduceren ook Syntactic Transformer (SynTr), een niet-recursieve parser vergelijkbaar met ons verfijningsmodel. RNGTr kan de nauwkeurigheid van een verscheidenheid van eerste parsers op 13-talen van de Universal Dependencies Treebanks, Engelse en Chinese Penn Treebanks en het Duitse CoNLL2009 corpus verbeteren, zelfs verbeteren ten opzichte van de nieuwe state-of-the-art resultaten verkregen door SynTr, aanzienlijk verbeteren van de state-of-the-art voor alle geteste corpora.', 'de': 'Wir schlagen die Recursive Non-autoregressive Graph-to-Graph Transformer Architektur (RNGTr) für die iterative Verfeinerung beliebiger Graphen durch die rekursive Anwendung eines nicht-autoregressiven Graph-to-Graph Transformers vor und wenden sie auf syntaktische Abhängigkeitsparsing an. Wir demonstrieren die Leistungsfähigkeit und Effektivität von RNGTr auf mehreren Abhängigkeitskorpora mit Hilfe eines mit BERT trainierten Verfeinerungsmodells. Wir stellen auch Syntactic Transformer (SynTr) vor, einen nicht rekursiven Parser ähnlich unserem Verfeinerungsmodell. RNGTr kann die Genauigkeit einer Vielzahl von ersten Parsern auf 13-Sprachen aus den Universal Dependencies Treebanks, englischen und chinesischen Penn Treebanks und dem deutschen CoNLL2009-Korpus verbessern, sogar gegenüber den neuen State-of-the-Art-Ergebnissen von SynTr verbessern und den Stand der Technik für alle getesteten Korpora signifikant verbessern.', 'da': "Vi foreslår den rekursive ikke-autoregressive Graph-to-Graph Transformer arkitektur (RNGTr) til iterativ forfinelse af vilkårlige grafer gennem rekursiv anvendelse af en ikke-autoregressiv Graph-to-Graph Transformer og anvende den til syntaktisk afhængighedsparsing. Vi demonstrerer RNGTr's styrke og effektivitet på flere afhængighedskorpora ved hjælp af en forududdannet forbedringsmodel med BERT. Vi introducerer også Syntactic Transformer (SynTr), en ikke-rekursiv fortolker svarende til vores raffinement model. RNGTr kan forbedre nøjagtigheden af en række forskellige indledende fortolkere på 13 sprog fra Universal Dependences Treebanks, engelske og kinesiske Penn Treebanks og den tyske CoNL2009 korpus, selv forbedre i forhold til de nye state-of-the-art resultater opnået af SynTr, betydeligt forbedre state-of-the-art for alle testede corpora.", 'hr': 'Predlažemo rekursivnu ne-autoregresivnu arhitekturu transformatora grafa do grafa (RNGTr) za iterativnu refineraciju arbitrarnih grafa kroz rekursivnu primjenu ne-autoregresivnog transformatora grafa do grafa i primjenjivati ga na analizu sintaktične zavisnosti. Pokazujemo snagu i učinkovitost RNGTr na nekoliko tijela ovisnosti, koristeći model refinerije predobučen BERT-om. Također predstavljamo sintaktički transformator (SynTr), neprekursivni analizač sličan našoj refinerskoj modeli. RNGTr može poboljšati preciznost različitih početnih parsera na 13 jezika od Univerzalnih zavisnosti Treebanka, engleskih i kineskih Penn Treebanka i njemačkog korpusa CoNLL2009, čak i poboljšati nove rezultate umjetnosti koje je ostvario Syntr, značajno poboljšavajući stanje umjetnosti za sve testovane korpore.', 'ko': '우리는 역귀환 비자귀환 도면에서 도면 변환기 구조(RNGTr)를 제시했고 역귀환 응용 비자귀환 도면에서 도면 변환기를 통해 임의의 그림을 반복적으로 세분화하고 문법 의존 분석에 응용했다.우리는 BERT 사전 훈련을 거친 정화 모델을 사용하여 몇 개의 의존 어료 라이브러리에서 RNGTr의 위력과 유효성을 보여 주었다.우리는 또한 문법 변환기(Syntro Transformer, SynTr)를 소개했는데 우리의 세분화 모델과 유사한 비귀속 해석기이다.RNGTr는 유니버설 의존 트리 라이브러리, 영어와 중국어 Penn 트리 라이브러리, 독일어 CoNLL 2009 어료 라이브러리 중 13개 언어의 각종 초기 해석기의 정확성을 높일 수 있으며, 심지어 SynTr가 얻은 최신 성과보다 높아 모든 테스트 어료 라이브러리의 최신 수준을 현저히 높일 수 있다.', 'id': 'Kami mengusulkan arsitektur Graph-to-Graph Transformer (RNGTr) Non-autoregressive Recursive untuk refinement iterative of arbitrary graphs through the recursive application of a non-autoregressive Graph-to-Graph Transformer and apply it to syntactic dependency parsing. Kami menunjukkan kekuatan dan efektivitas RNGTr pada beberapa korpra dependensi, menggunakan model refinement pra-dilatih dengan BERT. Kami juga memperkenalkan Syntactic Transformer (SynTr), parser tidak rekursif yang mirip dengan model refinement kami. RNGTr dapat meningkatkan akurasi dari berbagai pembicara awal pada 13 bahasa dari Universal Dependencies Treebanks, English and Chinese Penn Treebanks, dan korpus CoNLL2009 Jerman, bahkan meningkatkan atas hasil terbaru yang dilakukan oleh SynTr, meningkatkan secara signifikan state-of-the-art untuk semua corpora yang diuji.', 'fa': 'ما پیشنهاد می\u200cکنیم که معماری تغییر\u200cپذیر گراف به گراف\u200cها (RNGTr) را برای تغییر\u200cپذیر تکرار گراف\u200cهای تصادفی از طریق کاربرد تکرار\u200cپذیر یک تغییر\u200cپذیر گراف به گراف غیر autoregressive و تغییر\u200cپذیر گراف- به گراف- غیر autoregressive و آن را ما قدرت و فعالیت RNGTr را بر بستگی\u200cهای چندین شرکت نشان می\u200cدهیم، با استفاده از یک مدل پیش آموزش پیش آموزش شده با BERT. ما همچنین تغییرگر سنتاکتیک (Syntr) را معرفی می\u200cکنیم، یک متفرق غیر تکرار مانند مدل تغییر ما. RNGTr می\u200cتواند دقیقات متفاوتی از متفاوت آغاز در ۱۳ زبان از درخت بستگی جهانی، انگلیسی و پن درخت چینی و کورپوس آلمانی CoNLL2009 را بهتر کند، حتی در نتیجه\u200cهای جدید هنری که توسط Syntr به دست آورده است، به معنی بهتر کردن وضعیت هنری برای تمام آزمایش\u200cهای شرکت\u200cها.', 'sw': 'Tunawapendekeza ujenzi wa asiye na uhuru wa kujitegemea Graph-hadi Graph (RNGTr) kwa ajili ya kuboresha picha za arbitrary kwa kutumia matumizi ya kurejea kwa ajili ya tafsiri ya Graph-hadi Graph isiyo ya kudhibiti kujitegemea na kuitumia ili kutegemea uungwaji wa kiteknolojia. Tunaonyesha nguvu na ufanisi wa RNGTr kwa makampuni kadhaa yanayotegemea, kwa kutumia muundo wa mageuzi uliofanyika kabla na BERT. We also introduce Syntactic Transformer (SynTr), a non-recursive parser similar to our refinement model.  RNGTr anaweza kuboresha uhakika wa wabunge kadhaa wa mwanzo katika lugha 13 kutoka Uingereza na Penn Treebanks, Uingereza na China, na makampuni ya CoNLL2009 ya Ujerumani, hata kuboresha matokeo ya sanaa yaliyofanikiwa na SynTr, kwa kiasi kikubwa kuboresha hali ya sanaa kwa ajili ya makampuni yote yaliyojaribiwa.', 'tr': "Bir otoregressiv graf-we-graf terjime arhitektegi (RNGTr) bir arhitektik gaýşartmak üçin rekursiv bir grafik üçin arhitektegi teklif edip görkezip Biz RNGTr'yň güýji we etkinliýetini birnäçe baglanylyk korpoýasyna görkezip, BERT bilen öňünden öňünden öňünden öňünden eğlenen refinensiň modelini ullanýarys. Biz de Sintaktik Çevirmeli (SynTr), refineleme modelimize benzeri bir tekrarlı çözümler tanıtıyoruz. RNGTr Universal Dependencies Treebanks, Iňlisçe we Çinçe Penn Treebanks we Almança CoNLL2009-njy corpus tarapyndan gelen täze sanat netijesini gowurap biler.", 'af': "Ons voorstel die rekursiewe nie- autoregressiewe graf- na- graf- transformer arkitektuur (RNGTr) vir die iteratiewe vervulling van arbitrêre graaf deur die rekursiewe aansoek van 'n nie- autoregressiewe graf- na- graf- transformeerder en toewend dit na sintaktiewe afhanklikheid verwerking. Ons wys die krag en effektiviteit van RNGTr op verskeie afhanklikheid korpora, met gebruik van 'n refineeringsmodel voor-opgelei met BERT. Ons introduseer ook Sintaktieke Transformeerder (SynTr),  'n nie-rekursief ontvanger soos ons refineeringsmodel. RNGTr kan die presies van 'n verskillende aanvanklike verwerkers op 13 tale verbeter van die Universele afhanklikheidsbanks, Engels en Sjinese Penn Treebanks, en die Duitse CoNLL2009 corpus, selfs verbeter oor die nuwe staat-van-kuns-resultate wat deur SynTr bereik is, betekenlik die staat-van-kuns vir al die korpore toets.", 'hy': 'Մենք առաջարկում ենք ռեկուրսիվ ոչ ինքնագրավիչ Գրաֆ-Գրաֆ-Գրաֆ-Գրաֆ տրանսֆերմերի ճարտարապետություն (ՌՆԹԹՌ) կամավոր գրաֆիկների կրկնօրինակ բարելավման համար ոչ ինքնագրավիչ Գրաֆ-Գրաֆ տրանսֆերմերի կրկնօրինակ կիրառման միջոցով և կիրառել այն սինտակտի Մենք ցույց ենք տալիս ՌՆԳՏR-ի ուժը և արդյունավետությունը կախվածության մի քանի մարմնի վրա, օգտագործելով BER-ով նախապատրաստված բարելավման մոդել: We also introduce Syntactic Transformer (SynTr), a non-recursive parser similar to our refinement model.  ՌՆԳՏr-ը կարող է բարելավել սկզբնական վերլուծումների ճշգրիտությունը 13 լեզուների վրա, որը գտնվում է Անգլերենի և Չինաստանի ծառերի վրա, Անգլերենի և Չինաստանի ծառերի վրա, ինչպես նաև Գերմանական ԿոՆԼԼ2009-ի կորպոսը, նույնիսկ բարելավելով ՍինԹրի կողմից ստացված նոր ամենակարևոր արդյունքների վերաբերյալ, և նշանակալ', 'am': 'የራሱ-ሥልጣን የሌለባትን መዝገብ-ወደ-ግራፍ መክፈቻን (RNGTr) ለመፍጠር አካባቢ ቀረፋዎችን ለማንቀሳቀስ እናሳልጋለን፡፡ የRNGTr ኃይል እና ድጋሜን በብዙ ጥሩ ኮርፖርት ላይ እናሳየዋለን፣ BERT በተማረ በፊት የተሰናከረውን ንድፍ ሞዴል እናሳየዋለን፡፡ We also introduce Syntactic Transformer (SynTr), a non-recursive parser similar to our refinement model.  RNGTr የዩንቨርስቲ ደጋፊዎች፣ እንግሊዘኛ እና ቻይናዊ ፕኒን ቴርebanks እና የጀርመን ኮንጆርቲ 2009 ኮንፕስ፣ በሲንቲር የተደረገውን አዲስ የart-ፍጻሜ ውጤት አበጅቷል፡፡', 'az': "Biz özündə olmayan Grafik-to-Grafik Transformer arhitektüsü (RNGTr) ilə arhitektiv grafiklərin yenidən rafinməsini, otoregressiv Grafik-to-Grafik Transformer çisinin rekursiv uyğulamasını təklif edirik və onu sintaktik bağlılıq ayırmasına uyğun edirik. Biz RNGTr'in gücünü və etkinliğini bir çox bağımlılıq korporasına göstərdik, BERT ilə əvvəlcə təhsil edilmiş refinement modeli kullanarak. Biz də Sintaktik Transformer (Syntr) ilə birləşdiririk, refinement modelimizə bənzər bir neçə-neçə ayırıcı. RNGTr Universal Dependencies Treebanks, English and Chinese Penn Treebanks, and the German CoNLL2009 corpus, even improved over the new state of the art results achieved by Syntr, significantly improving the state of the art for all corpora tested.", 'sq': 'Ne propozojmë arkitekturën Recursive Non-autoregressive Graph-to-Graph Transformer (RNGTr) për përmirësimin iterativ të grafikave arbitrarë nëpërmjet aplikimit rekursiv të një Graph-to-Graph Transformer jo-autoregressive dhe e aplikojmë në analizimin sintaktik të varësisë. Ne demonstrojmë fuqinë dhe efektshmërinë e RNGTr në disa korpra varësie, duke përdorur një model rafinimi të paratrajnuar me BERT. Ne prezantojmë gjithashtu Syntactic Transformer (SynTr), një analizues jo-rekursiv i ngjashëm me modelin tonë të rafinuar. RNGTr can improve the accuracy of a variety of initial parsers on 13 languages from the Universal Dependencies Treebanks, English and Chinese Penn Treebanks, and the German CoNLL2009 corpus, even improving over the new state-of-the-art results achieved by SynTr, significantly improving the state-of-the-art for all corpora tested.', 'bn': 'আমরা পুনঃস্বয়ংক্রিয়ভাবে নিয়ন্ত্রিত গ্রাফ-থেকে গ্রাফ-ট্রান্সফ্রান্সফ্রান্সভার কাঠামোর প্রস্তাব করছি যেখানে নির্দিষ্ট গ্রাফ-ট্রান্সফার্নারের মাধ্যমে পুনরায় সংস্ক আমরা আরএনজিটির ক্ষমতা এবং কার্যক্রম বিভিন্ন নির্ভরশীল কোর্পোরার উপর প্রদর্শন করি, বিবেরেটের সাথে প্রশিক্ষণের পূর্বে প্রশিক্ষণের ম আমরা সিন্ট্রাক্টিক ট্রান্সফ্রান্সফার (সিনিটির), আমাদের সুসংস্কারের মডেলের মতো একটি অন্যান্য প্যারাসার্ আরএনজিটির বিশ্ববিদ্যালয়ের নির্ভরিত ট্রেব্যাংক, ইংরেজি এবং চীনা পেন ট্রিবাঙ্ক এবং জার্মান কনএল ২০০৯ কোর্পাস থেকে ১৩ ভাষায় প্রাথমিক পার্সারের সঠিকভাবে উন্নত করতে পারে, এমনকি সিন্টিরের অর্জনের নতুন শিল', 'bs': 'Predlažemo rekursivnu ne-autoregresivnu arhitekturu transformatora grafa do grafa (RNGTr) za iterativnu refinenciju arbitrarnih grafa kroz rekursivnu primjenu ne-autoregresivnog transformatora grafa do grafa i primjenjujemo ga na analizu sintaktične zavisnosti. Mi pokazujemo moć i učinkovitost RNGTr na nekoliko korpora zavisnosti, koristeći model refiniranja predobučen BERT-om. Također predstavljamo sintaktički transformator (SynTr), neprekursivni analizač sličan našoj refinerskoj modeli. RNGTr može poboljšati preciznost različitih početnih parsera na 13 jezika od Univerzalnih zavisnosti Treebanka, engleskih i kineskih Penn Treebanka i njemačkog korpusa CoNLL2009, čak i poboljšati nove rezultate umjetnosti koje je ostvario Syntr, značajno poboljšavajući stanje umjetnosti za sve testovane korpore.', 'cs': 'Navrhujeme architekturu rekurzivního non-autoregresivního grafu na graf transformátoru (RNGTr) pro iterativní zdokonalování libovolných grafů rekurzivní aplikací non-autoregresivního grafu na graf transformátoru a aplikujeme ji na syntaktickou analýzu závislostí. Demonstrujeme sílu a efektivitu RNGTr na několika závislostních korpusech, pomocí rafinovacího modelu předem trénovaného s BERT. Představujeme také Syntaktický transformátor (SynTr), nekrekurzivní parser podobný našemu modelu rafinace. RNGTr může zlepšit přesnost různých počátečních parserů na 13-jazycích z univerzálních závislostí stromů, anglických a čínských Penn Treebanks a německého korpusu CoNLL2009, dokonce zlepšit oproti novým nejmodernějším výsledkům dosaženým SynTr, což výrazně zlepšuje stav techniky pro všechny testované korpusy.', 'et': 'Pakume välja rekursiivse mittearregressiivse graafiku teisendaja arhitektuuri (RNGTr) suvaliste graafikute iteratiivseks rafineerimiseks mittearregressiivse graafiku teisendaja rekursiivse rakenduse kaudu ning rakendame seda süntaktilise sõltuvuse parsimisel. Näitame RNGTr võimsust ja efektiivsust mitmete sõltuvuskorporate puhul, kasutades BERT-iga eelnevalt koolitatud rafineerimismudelit. Tutvustame ka süntaktilist transformaatorit (SynTr), mitterekursiivset parserit, mis sarnaneb meie rafineerimismudeliga. RNGTr võib parandada mitmesuguste esialgsete parserite täpsust 13 keeles universaalsetest sõltuvustest puupankidest, inglise ja hiina Penn puupankidest ja saksa CoNLL2009 korpusest, isegi parandades SynTr saavutatud uute kaasaegsete tulemustega, parandades oluliselt kõigi testitud korpuste kaasaegset taset.', 'fi': 'Ehdotamme rekursiivista ei-autoregressiivista graafista grafiikkaan muuntajan arkkitehtuuria (RNGTr) mielivaltaisten graafien iteratiiviseen tarkentamiseen rekursiivisella sovelluksella ei-autoregressiivisella grafiikkaan muuntajaan ja sovellamme sitä syntaktiseen riippuvuuden jäsentämiseen. Demonstroimme RNGTr:n tehoa ja tehokkuutta useilla riippuvuuskorporadeilla käyttäen BERT:n esikoulutettua jalostusmallia. Esittelemme myös Syntactic Transformer (SynTr), ei-rekursiivinen jäsentäjä, joka on samanlainen kuin jalostusmallimme. RNGTr voi parantaa useiden alkuperäisten jäsennysten tarkkuutta 13 kielellä universaaleista riippuvuuksista Treebanks, englanti ja kiina Penn Treebanks ja saksan CoNLL2009 korpus, jopa parantaa SynTr:n saavuttamat uudet huipputekniset tulokset, mikä parantaa merkittävästi kaikkien testattujen korpusten huipputasoa.', 'ca': "Proposem l'arquitectura Recursiva de Transformer de Gràfic a Gràfic (RNGTr) no autoregressiva per a refinar iterativament els gràfics arbitraris mitjançant l'aplicació recursiva d'un Transformer de Gràfic a Gràfic no autoregressiu i aplicar-lo a l'analització sinàctica de dependencies. Demonstrem el poder i l'eficacia del RNGTr en diversos corpores de dependencia, utilitzant un model de refinament pré-entrenat amb BERT. We also introduce Syntactic Transformer (SynTr), a non-recursive parser similar to our refinement model.  RNGTr pot millorar la precisió d'una varietat d'analitzadors inicials en 13 llengües de Universal Dependencies Treebanks, English and Chinese Penn Treebanks i el corpus alemany CoNLL2009, fins i tot millorant amb els nous resultats d'última generació de SynTr, millorant significativament l'última generació de totes les corpores testades.", 'jv': 'We proposal the recurrecurrecurve Not-autoRegresve Graph-to-Graph Transformer architecture Awak dhéwé éntukno nggambar nggawe lan effek ngéwangi DNGTR kanggo ngerasakno perusahaan winih, gambar model nggawe ngubah suarané karo BERT. Awak dhéwé nglebok Senaksi Transformer (sinaTR), akeh gak-recursif maneh karo model nggawe refinénsi. RT GTR', 'ha': "Tuna goyyar da matsayin tarakirin na-raye-zuwa-Graf-Graf (RNGTr) wa mai daidaita kafaffiyar grafyutan arbitatiki, a bayan shiryoyin ayuka na komasar da shiryoyin-graf-zuwa-Graf-farat-farat, kuma za'a amfani da shi zuwa parge da inganci na syntactic. Tuna nuna ƙarfin RNGTr kan wasu firma masu inganci, kuma tuna amfani da misãlai mai gyarawa ta gabã ɗaya da BERT. Tuna ƙara bayan Transformer na Sintact (synTr), wani parse na-resursive mai daidaita misãlai na refinement. RNGTr can improve tsarin wasu parparser masu farko cikin harshen 13 daga Universal Debties Treebanks, Ingiriya da China Penn Treebanks, da naun Jarman CoNLL2009, ko kuboren mafiya ƙaranci ga fassarar-sanar da SinTr ta sami, kuma yana ƙaranci halin-kunst wa dukkan korana wanda aka jarraba.", 'he': 'אנו מציעים את הארכיטקטורה המעברת גרף-לגרף לא אוטורגרסיבית (RNGTr) לטיפול האיטרטיבי של גרפים רציניים דרך שימוש מחדש של טרנספורט גרף-לגרף לא אוטורגרסיבי ולהשתמש בו בחיפוש תלויות סינטאקטית. אנחנו מראים את הכוח והיעילות של RNGTr על מספר גופות תלויות, בשימוש מודל שיפור מאומן מראש עם BERT. אנחנו גם מכירים את המעבר הסינטקטי (SynTr), מעבד לא חוזר דומה לדוגמא שלנו. RNGTr יכול לשפר את מדויקתו של מגוון של חוקרים ראשונים ב-13 שפות מ-Universal Dependencies Treebanks, English and Chinese Penn Treebanks, וקורפוס CoNLL2009 הגרמני, אפילו לשפר על התוצאות החדשות של המצב האמנותי שנשגשו ע"י SynTr, משפר באופן משמעותי את המצב האמנותי לכל הקורפורה שנבחנה.', 'sk': 'Za iterativno rafiniranje poljubnih grafov s pomočjo rekurzivne aplikacije nenavtoregresivnega grafskega transformatorja (RNGTr) predlagamo arhitekturo rekurzivnega nenavtoregresivnega grafskega transformatorja in jo uporabimo pri sintaktičnem razčlenitvi odvisnosti. Prikazujemo moč in učinkovitost RNGTr na več odvisnih korpusih z uporabo modela prefinjenosti, ki je bil predhodno usposobljen z BERT. Predstavljamo tudi Syntactic Transformer (SynTr), nerekurzivni razčlenjevalnik, podoben našemu modelu prefinjevanja. RNGTr lahko izboljša natančnost različnih začetnih razčlenjevalnikov na 13 jezikih iz univerzalnih odvisnosti Treebanks, angleških in kitajskih Penn Treebanks ter nemškega korpusa CoNLL2009, celo izboljša nad novimi najsodobnejšimi rezultati, ki jih doseže SynTr, in bistveno izboljša stanje tehnologije za vse testirane korpuse.', 'bo': 'We propose the Recursive Non-autoregressive Graph-to-Graph Transformer architecture (RNGTr) for the iterative refinement of arbitrary graphs through the recursive application of a non-autoregressive Graph-to-Graph Transformer and apply it to syntactic dependency parsing. ང་ཚོས་རྟེན་འབྲེལ་བའི་མཐུན་སྣེ་མང་པོ་ཞིག་གིས་RNGTr འི་ནུས་མཐུན་དང་ནུས་ཡོད་ཚད་མངོན་འཆར་བྱེད་ཀྱི་ཡོད། ང་ཚོས་རང་ཉིད་ཀྱིས་དབྱིབས་བཟོས་མིན་པའི་དབྱིབས་སྒྱུར་བྱེད་ཆས་ལ་ངོས་འཛིན་བྱེད་དགོས་པ། RNGTr ཡིས་སྤྱི་ཚོགས་ཀྱི་འཛུལ་སྤྱོད་མཁན་གྱི་ནང་དུ་འཇིག་རྟེན་འདིའི་གོ་སྐབས་13ལས་འགོ་ཐོག་ཏེ།'}
