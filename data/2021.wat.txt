{'en': 'NICT’s Neural Machine Translation Systems for the WAT21 Restricted Translation Task NICT ’s Neural Machine Translation Systems for the  WAT 21 Restricted Translation Task', 'ar': 'أنظمة الترجمة الآلية العصبية الخاصة بـ NICT لمهمة الترجمة المقيدة لـ WAT21', 'pt': 'Sistemas de tradução automática neural da NCT para a tarefa de tradução restrita WAT21', 'fr': 'Systèmes de traduction automatique neuronale des NTIC pour la tâche de traduction restreinte WAT21', 'es': 'Sistemas de traducción automática neuronal de NICT para la tarea de traducción restringida de WAT21', 'ja': 'WAT 21制限翻訳タスクのためのNICTの神経機械翻訳システム', 'zh': 'NICT以WAT21限译者神经机器翻译统', 'hi': 'WAT21 प्रतिबंधित अनुवाद कार्य के लिए NICT की तंत्रिका मशीन अनुवाद प्रणाली', 'ru': "NICT 's Neural Machine Translation Systems for the WAT21 Restricted Translation Task (Системы нейронного машинного перевода NICT для задачи ограниченного перевода WAT21)", 'ga': 'Córais Néar-Aistriúcháin Inneallta NICT do Thasc Srianta Aistriúcháin WAT21', 'el': 'Νευρικά Μηχανολογικά Μεταφραστικά Συστήματα του για την Περιορισμένη Μεταφραστική Εργασία του WAT21', 'ka': 'Name', 'hu': 'A NICT idegi gépi fordító rendszerei a WAT21 korlátozott fordítási feladathoz', 'it': 'I sistemi di traduzione automatica neurale NICT per il compito di traduzione limitato WAT21', 'kk': 'WAT21 шектелген аудармалар тапсырмасының NICT нейрондық машинаны аудару жүйесі', 'ml': 'WAT21 പരിശോധിക്കപ്പെട്ട പരിഭാഷകുട്ടികള്\u200d', 'lt': 'NICT nervinių mašinų vertimo sistemos, skirtos WAT21 ribotam vertimui atlikti', 'mk': 'Неурални машински транслативни системи на NICT за задачата WAT21', 'mt': 'Sistemi ta’ Traduzzjoni ta’ Magni Newrali tan-NICT għall-Kompitu ta’ Traduzzjoni Restretta tal-WAT21', 'mn': "NICT's Neural Machine Translation Systems for the WAT21 Restricted Translation Task", 'no': 'NICT sin neiral maskinsomsetjingssystemet for WAT21- avgrensa omsetjingssystemet', 'ms': 'Sistem Terjemahan Mesin Neural NICT untuk Tugas Terlarangan WAT21', 'pl': 'Neuralne systemy tłumaczenia maszynowego NICT dla zadania tłumaczenia ograniczonego WAT21', 'ro': 'Sistemele neurale de traducere automată NICT pentru sarcina de traducere restricționată WAT21', 'sr': "NICT's Neural Machine Translation Systems for the WAT21 Restricted Translation Task", 'so': "NICT's Neural machine Translation Systems for the WAT21 Restricted Translation Task", 'si': 'Name', 'sv': 'NICT:s neurala maskin철vers채ttningssystem f철r WAT21 Begr채nsade 철vers채ttningsuppgifter', 'ta': "NICT's Neural Machine Translation Systems for the WAT21 Restricted Translation Task", 'ur': 'Name', 'uz': 'Tarjima qilish tizimi', 'vi': 'Hệ thống dịch máy thần kinh của NIST cho WATN21 Hạn chế dịch vụ', 'nl': "NICT's Neural Machine Translation Systemen voor de WAT21 Beperkte Vertalingstak", 'bg': 'Системите за неврален машинен превод на НИКТ за задачата за ограничен превод', 'da': "NICT's Neurale maskinoversættelsessystemer til WAT21 Begrænset oversættelsesopgave", 'de': 'Neuronale maschinelle Übersetzungssysteme von NICT für die eingeschränkte Übersetzungsaufgabe WAT21', 'hr': 'NICT-ov Neuralni sustav prevoda stroja za WAT21 ograničeni prevod zadatak', 'id': "NICT's Neural Machine Translation Systems for the WAT21 Restricted Translation Task", 'ko': 'WAT21 제한된 번역 작업을 위한 NICT 신경 기계 번역 시스템', 'fa': 'سیستم ترجمه ماشین عصبی NICT برای تابع ترجمه محدود WAT21', 'tr': "NICT'iň neiral Maşynyň WAT21 Terjime Görevi üçin terjime sistemleri", 'af': 'Name', 'am': 'የNICT የኔural machine translation systems for the WAT21 Restricted Translation Task', 'hy': 'Comment', 'sw': 'Mfumo wa Tafsiri wa Mashine ya Kifaransa ya NICT kwa ajili ya Tamko la Tafsiri la WAT21 lililozuiwa', 'az': 'WAT21 Q캼s 캼rl캼 T톛rc칲m톛 Qullan캼 칲칞칲n NICT N칬ral Makinel톛rin T톛rc칲m톛 Sisteml톛ri', 'bn': "WAT21 বিরোধিত অনুবাদের কাজের জন্য NICT's নিউরাল মেশিন অনুবাদ সিস্টেম", 'bs': "NICT's Neural Machine Translation Systems for the WAT21 Restricted Translation Task", 'cs': 'Neurální strojové překladové systémy NICT pro omezený překlad WAT21', 'sq': "NICT's Neural Machine Translation Systems for the WAT21 Restricted Translation Task", 'ca': 'Sistemes de traducció de màquines neuronals del NICT per a la tasca de traducció restringida WAT21', 'fi': 'NICT:n neuraaliset konekäännösjärjestelmät WAT21 rajoitetun käännöksen tehtävään', 'et': 'NICT neuraalsed masintõlkesüsteemid WAT21 piiratud tõlketöö jaoks', 'jv': "NITT's Neral Masukan Terjamahan Sistem kanggo WAT 22 Terjamahan", 'sk': 'NICT-jevi sistemi nevralnega strojnega prevajanja za nalogo omejenega prevajanja WAT21', 'ha': '@ item license', 'he': 'מערכות התרגום של מכונות נוירויות של NICT למשימת התרגום המוגבלת WAT21', 'bo': "NICT's Neural Machine Translation System for the WAT21 Restricted Translation Task"}
{'en': 'This paper describes our  system  (Team ID : nictrb) for participating in the WAT’21 restricted machine translation task. In our submitted  system , we designed a new  training approach  for restricted machine translation. By sampling from the translation target, we can solve the problem that ordinary training data does not have a restricted vocabulary. With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution. In addition, we also tried the vanilla and sparse Transformer as the backbone network of the  model , as well as  model ensembling , which further improved the final translation performance.', 'ar': "تصف هذه الورقة نظامنا (معرف الفريق: nictrb) للمشاركة في مهمة الترجمة الآلية المقيدة لـ WAT'21. في نظامنا المقدم ، قمنا بتصميم نهج تدريبي جديد للترجمة الآلية المقيدة. من خلال أخذ العينات من هدف الترجمة ، يمكننا حل المشكلة المتمثلة في أن بيانات التدريب العادية لا تحتوي على مفردات محدودة. مع مزيد من المساعدة لفك التشفير المقيد في مرحلة الاستدلال ، حققنا نتائج أفضل من خط الأساس ، مما يؤكد فعالية حلنا. بالإضافة إلى ذلك ، جربنا أيضًا الفانيليا والمحول المتناثر كشبكة العمود الفقري للنموذج ، بالإضافة إلى تجميع النموذج ، مما أدى إلى زيادة تحسين أداء الترجمة النهائية.", 'pt': "Este documento descreve nosso sistema (ID da equipe: nictrb) para participar da tarefa de tradução automática restrita do WAT'21. Em nosso sistema enviado, projetamos uma nova abordagem de treinamento para tradução automática restrita. Ao fazer uma amostragem do destino de tradução, podemos resolver o problema de que os dados de treinamento comuns não possuem um vocabulário restrito. Com a ajuda adicional de decodificação restrita na fase de inferência, obtivemos resultados melhores do que a linha de base, confirmando a eficácia de nossa solução. Além disso, também testamos o vanilla e o sparse Transformer como a rede principal do modelo, bem como o model ensembling, o que melhorou ainda mais o desempenho final da tradução.", 'es': "Este documento describe nuestro sistema (ID de equipo: nictrb) para participar en la tarea de traducción automática restringida de WAT'21. En nuestro sistema presentado, diseñamos un nuevo enfoque de formación para la traducción automática restringida. Al tomar muestras del objetivo de traducción, podemos resolver el problema de que los datos de entrenamiento ordinarios no tienen un vocabulario restringido. Con la ayuda adicional de la decodificación restringida en la fase de inferencia, logramos mejores resultados que la línea base, lo que confirma la eficacia de nuestra solución. Además, también probamos el Transformer básico y disperso como red troncal del modelo, así como el ensamblaje del modelo, lo que mejoró aún más el rendimiento de la traducción final.", 'fr': "Cet article décrit notre système (ID d'équipe\xa0: nictrb) pour participer à la tâche de traduction automatique restreinte WAT'21. Dans notre système soumis, nous avons conçu une nouvelle approche de formation pour la traduction automatique restreinte. En échantillonnant à partir de la cible de traduction, nous pouvons résoudre le problème selon lequel les données d'entraînement ordinaires n'ont pas un vocabulaire restreint. Grâce au décodage contraint dans la phase d'inférence, nous avons obtenu de meilleurs résultats que la base, ce qui confirme l'efficacité de notre solution. En outre, nous avons également essayé le Transformer Vanilla et Sparse comme réseau de base du modèle, ainsi que l'assemblage du modèle, ce qui a encore amélioré les performances de traduction finale.", 'zh': '本文引臣等参WAT21限制性机器翻译之统(团队ID:nictrb)。 于我系统,为受限机器翻译设新培训法。 以译者抽样,可以决常数无限词汇。 理解码益助,取善于基线,以验解决方案之有效性。 又试普通变形金刚疏变形金刚为骨干网络,及模形融合,更进终译。', 'ja': "本稿では、WAT '21制限機械翻訳タスクに参加するためのシステム（チームID ： nictrb ）について説明します。提出されたシステムでは、制限された機械翻訳のための新しいトレーニングアプローチを設計しました。翻訳ターゲットからサンプリングすることで、通常のトレーニングデータに制限された語彙がないという問題を解決できます。推論フェーズで制約されたデコードのさらなる助けを借りて、ベースラインよりも良い結果を達成し、ソリューションの有効性を確認しました。さらに、モデルのバックボーンネットワークとしてバニラとまばらなトランスフォーマー、そして最終的な翻訳パフォーマンスをさらに向上させたモデルアンサンブルも試しました。", 'hi': "यह पेपर WAT'21 प्रतिबंधित मशीन अनुवाद कार्य में भाग लेने के लिए हमारे सिस्टम (टीम आईडी: nictrb) का वर्णन करता है। हमारे प्रस्तुत प्रणाली में, हमने प्रतिबंधित मशीन अनुवाद के लिए एक नया प्रशिक्षण दृष्टिकोण तैयार किया है। अनुवाद लक्ष्य से नमूना लेकर, हम इस समस्या को हल कर सकते हैं कि साधारण प्रशिक्षण डेटा में प्रतिबंधित शब्दावली नहीं है। अनुमान चरण में विवश डिकोडिंग की आगे की मदद से, हमने बेसलाइन की तुलना में बेहतर परिणाम प्राप्त किए, हमारे समाधान की प्रभावशीलता की पुष्टि की। इसके अलावा, हमने मॉडल के बैकबोन नेटवर्क के रूप में वेनिला और विरल ट्रांसफॉर्मर की भी कोशिश की, साथ ही साथ मॉडल ensembling, जिसने अंतिम अनुवाद प्रदर्शन में और सुधार किया।", 'ru': "В этой статье описывается наша система (Team ID: nictrb) для участия в задаче ограниченного машинного перевода WAT'21. В нашей поданной системе мы разработали новый подход к обучению ограниченному машинному переводу. Выбирая из цели перевода, мы можем решить проблему, заключающуюся в том, что обычные обучающие данные не имеют ограниченного словарного запаса. С дальнейшей помощью ограниченного декодирования на этапе вывода мы достигли лучших результатов, чем базовые, что подтверждает эффективность нашего решения. Кроме того, мы также попробовали ванильный и редкий трансформатор в качестве магистральной сети модели, а также сборку модели, что дополнительно улучшило конечные характеристики перевода.", 'ga': "Déanann an páipéar seo cur síos ar ár gcóras (Team ID: nictrb) chun páirt a ghlacadh sa tasc aistriúcháin meaisín srianta WAT'21. Inár gcóras a cuireadh isteach, dhearamar cur chuige oiliúna nua maidir le haistriúchán meaisín srianta. Trí shampláil a dhéanamh ón sprioc aistriúcháin, is féidir linn an fhadhb a réiteach nach bhfuil stór focal srianta ag gnáthshonraí oiliúna. Le cabhair bhreise ó dhíchódú srianta sa chéim tátail, bhaineamar torthaí níos fearr amach ná an bhunlíne, ag deimhniú éifeachtacht ár réitigh. Ina theannta sin, rinneamar iarracht freisin an Claochladán fanaile agus tanaí mar líonra cnámh droma na samhla, chomh maith le cumadóireacht mhúnla, rud a chuir feabhas breise ar fheidhmíocht an aistriúcháin deiridh.", 'hu': "Ez a tanulmány bemutatja a WAT'21 korlátozott gépi fordítási feladatban való részvételre szolgáló rendszerünket (Team ID: nicktrb). Beküldött rendszerünkben új képzési megközelítést terveztünk a korlátozott gépi fordításhoz. A fordítási célból való mintavétellel megoldhatjuk azt a problémát, hogy a hagyományos képzési adatok nem rendelkeznek korlátozott szókincsekkel. A következtetési fázisban a korlátozott dekódolás további segítségével jobb eredményeket értünk el, mint az alap, megerősítve megoldásunk hatékonyságát. Ezenkívül kipróbáltuk a modell gerinchálózatát a vanília és ritka Transformert, valamint a modell együttesét, ami tovább javította a végső fordítási teljesítményt.", 'ka': "ჩვენი სისტემა (Team ID: nictrb) გადაწყვეტილებისთვის WAT'21 მაქსინის გადაწყვეტილების რაოდენობაში დაწყვეტილება. ჩვენი გადატანა სისტემაში, ჩვენ ახალი განვიყენება მაქინის გაგრძელებისთვის განვიყენება. ჩვენ შეგვიძლია გავაკეთოთ პრობლემა, რომელიც საერთო განაკეთებული მონაცემები არ აქვს დაფართებული სიტყვებულია. უფრო მეტი დახმარებული რეკოდირეციის ფეზაში, ჩვენ გავაკეთეთეთ უფრო მეტი წარმოდგენები, რომლებიც ჩვენი წარმოდგენის ეფექტიურობას დარწმუნეთ. დამატებით, ჩვენ შევცდილოთ განილია და გამოცდილობული ტრანფორმეტრისტრისტრისტრისტრისტრისტრისტრისტრისტრისტრისტრისტრისტრისტრისტრისტრი როგორც მოდელის და", 'el': "Η παρούσα εργασία περιγράφει το σύστημά μας (αναγνωριστικό ομάδας: nickrb) για τη συμμετοχή στην περιορισμένη εργασία μηχανικής μετάφρασης του WAT'21. Στο σύστημά μας σχεδιάσαμε μια νέα προσέγγιση εκπαίδευσης για περιορισμένη μηχανική μετάφραση. Με δειγματοληψία από τον μεταφραστικό στόχο, μπορούμε να λύσουμε το πρόβλημα ότι τα συνηθισμένα δεδομένα εκπαίδευσης δεν έχουν περιορισμένο λεξιλόγιο. Με την περαιτέρω βοήθεια της περιορισμένης αποκωδικοποίησης στη φάση συμπερασμάτων, επιτύχαμε καλύτερα αποτελέσματα από τη βάση, επιβεβαιώνοντας την αποτελεσματικότητα της λύσης μας. Επιπλέον, δοκιμάσαμε επίσης τον βανίλια και τον αραιό μετασχηματιστή ως το δίκτυο σπονδυλικής στήλης του μοντέλου, καθώς και τη σύνθεση μοντέλων, η οποία βελτίωσε περαιτέρω την τελική απόδοση της μετάφρασης.", 'it': "Questo articolo descrive il nostro sistema (Team ID: nicktrb) per partecipare al compito limitato di traduzione automatica WAT'21. Nel nostro sistema presentato, abbiamo progettato un nuovo approccio formativo per la traduzione automatica limitata. Campionando dal target di traduzione, possiamo risolvere il problema che i dati di formazione ordinaria non hanno un vocabolario ristretto. Con l'ulteriore aiuto della decodifica vincolata nella fase di inferenza, abbiamo ottenuto risultati migliori rispetto alla base, confermando l'efficacia della nostra soluzione. Inoltre, abbiamo provato anche il Transformer vanilla e sparso come rete portante del modello, così come l'ensembling del modello, che ha ulteriormente migliorato le prestazioni di traduzione finale.", 'kk': "Бұл қағаз біздің жүйемізді (Топ ID: nictrb) WAT' 21 шектелген машинаны аудару тапсырмасына қатынау үшін түсіндіреді. Біздің жүйемізде шектелген машинаны аудару үшін жаңа оқыту тәсілі жасадық. Аудару мақсатынан мәліметті алып тастап, кәдімгі оқыту деректерінің шектелген сөздігін шектеуге болады. Біз шектелген деңгейінде шектелген деңгейінің қосымша көмегімен негізгі жолдан жақсы нәтижелерді жеткіздік. Біздің шешіміміздің эффектілігін тексеру үшін. Қосымша, бұл үлгінің қапшық желі ретінде vanilla және sparse Transformer- тізбегін қолданып көрдік. Соңғы аудармалардың жақсартылығын жақсартып жатыр.", 'lt': "Šiame dokumente apibūdinama mūsų sistema (komandos identifikavimo kodas: nictrb), skirta dalyvauti WAT'21 ribotoje mašin ų vertimo užduotyje. Mūsų pateiktoje sistemoje sukūrėme naują mokymo metodą ribotam mašinų vertimui. Imdami mėginius iš vertimo tikslo, galime išspręsti problem ą, kad paprastai rengiami duomenys neturi riboto žodyno. Toliau padedant ribotam dekodiavimui išvados etape, mes pasiekėme geresnius rezultatus nei pradinis rezultatas, patvirtinantį mūsų sprendimo veiksmingumą. Be to, mes taip pat bandėme vanilą ir nedidelį transformatorių kaip modelio nugaros tinklą, taip pat modelių rinkimą, kuris dar labiau pagerino galutinį vertimo rezultatą.", 'mk': "Овој весник го опишува нашиот систем (ИД на тимот: nictrb) за учество во задачата на рестриктираниот машински превод WAT' 21. Во нашиот поднесен систем, дизајниравме нов пристап за обука за ограничен машински превод. Со примерок од метата на превод, можеме да го решиме проблемот дека обичните податоци за обука немаат ограничен речник. Со понатамошна помош од ограничена декодирање во фазата на инференцијата, постигнавме подобри резултати од основната, потврдувајќи ја ефикасноста на нашето решение. Покрај тоа, ние, исто така, се обидовме со ванила и ослободивме Трансформер како грб-коска мрежа на моделот, како и модел ансемблирање, што понатаму ја подобри финалната преводна перформанса.", 'ml': "ഈ പത്രത്തില്\u200d ഞങ്ങളുടെ സിസ്റ്റത്തെ വിവരിക്കുന്നു (ടീം ഐഡി: നിക്ട്രിബ്) WAT'21 നിര്\u200dബന്ധിതമായ മെഷീന്\u200d പരിഭാഷണ ജ ഞങ്ങളുടെ സമ്മതിച്ച സിസ്റ്റത്തില്\u200d, നിര്\u200dബന്ധിതമായ മെഷിന്\u200d പരിശീലനത്തിനുള്ള ഒരു പുതിയ പരിശീലന നടപടി പരിഭാഷയുടെ ലക്ഷ്യത്തില്\u200d നിന്ന് ടാമാമ്പിള്\u200d ചെയ്താല്\u200d സാധാരണ പരിശീലിപ്പിക്കുന്ന വിവരങ്ങള്\u200d നിര്\u200dബന്ധിതമായ പദവി With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution.  അതുകൂടാതെ, ഞങ്ങള്\u200d വാനില്ലായ്ക്ക് ശ്രമിച്ചു ട്രാന്\u200dസ്ഫോര്\u200dഫിനെ മോഡലിന്\u200dറെ ബാക്ക് ബോണ്\u200d നെറ്റ്\u200cവര്\u200dക്കില്\u200d നിര്\u200dത്തുകയും ചെയ്ത", 'ms': "Kertas ini menggambarkan sistem kami (ID Pasukan: nictrb) untuk berpartisipasi dalam tugas terjemahan mesin WAT'21 yang diharamkan. Dalam sistem kami, kami merancang pendekatan latihan baru untuk terjemahan mesin yang terbatas. Dengan mengambil sampel dari sasaran terjemahan, kita boleh selesaikan masalah bahawa data latihan biasa tidak mempunyai kamus terbatas. Dengan bantuan lebih lanjut dekoding terhalang dalam fase kesimpulan, kami mencapai keputusan yang lebih baik daripada asas, mengesahkan kegunaan penyelesaian kami. Selain itu, kami juga cuba vanilla dan sparse Transformer sebagai rangkaian tulang belakang model, serta kumpulan model, yang lebih meningkatkan prestasi terjemahan akhir.", 'mn': "Энэ цаас бидний системийг (Баг ID: nictrb) WAT'21 хязгаарлагдсан машин хөрөнгө оруулах ажилд оролцохын тулд тайлбарладаг. Бидний дамжуулагдсан системд машин хөрөнгө оруулахын тулд шинэ сургалтын арга зам зохион байгуулсан. Хувьсалын зорилготой жишээ нь бид энгийн дасгал өгөгдлийн мэдээллийг хязгаарлагдмал үг байхгүй асуудлыг шийдэж болно. Халдвар халдвар дахь хязгаарлагдсан шийдвэрлэлийн тусламжтайгаар бид үндсэн шугамнаас илүү сайн үр дүнг гаргаж, шийдвэрлэлийн үр дүнг баталж байна. Мөн бид загварын хөндлөн шугам болон загварын загварын хөндлөн шугам болон загварын загварын хөндлөн шилжүүлэгчдийг дахин сайжруулсан.", 'mt': "Dan id-dokument jiddeskrivi s-sistema tagħna (ID tat-Tim: nictrb) għall-parteċipazzjoni fil-kompitu ristrett tat-traduzzjoni tal-magni tal-WAT'21. Fis-sistema ppreżentata tagħna, iddisinjna approċċ ġdid ta’ taħriġ għat-traduzzjoni ristretta tal-magni. By sampling from the translation target, we can solve the problem that ordinary training data does not have a restricted vocabulary.  Bl-għajnuna ulterjuri tad-dekodifikazzjoni ristretta fil-fażi ta’ inferenza, kisbet riżultati aħjar mil-linja bażi, li kkonfermaw l-effettività tas-soluzzjoni tagħna. Barra minn hekk, ipprovajna wkoll il-vanilla u l-sparse Transformer bħala n-netwerk tad-dahar tal-mudell, kif ukoll l-assemblaġġ tal-mudell, li tejjeb aktar il-prestazzjoni finali tat-traduzzjoni.", 'pl': "Niniejszy artykuł opisuje nasz system (Team ID: nictrb) do udziału w ograniczonym zadaniu tłumaczenia maszynowego WAT'21. W naszym przesłanym systemie zaprojektowaliśmy nowe podejście szkoleniowe dla ograniczonych tłumaczeń maszynowych. Pobierając próbki z celu tłumaczenia, możemy rozwiązać problem, że zwykłe dane treningowe nie mają ograniczonego słownictwa. Dzięki dalszej pomocy ograniczonego dekodowania w fazie wnioskowania osiągnęliśmy lepsze wyniki niż baza, potwierdzając skuteczność naszego rozwiązania. Dodatkowo wypróbowaliśmy również waniliowy i rzadki Transformer jako sieć kręgosłupa modelu, a także zestaw modeli, co jeszcze bardziej poprawiło ostateczną wydajność tłumaczenia.", 'ro': "Această lucrare descrie sistemul nostru (Team ID: nicktrb) pentru participarea la sarcina de traducere automată restricționată WAT'21. În sistemul nostru transmis, am proiectat o nouă abordare de instruire pentru traducerea automată restricționată. Prin eșantionarea din ținta de traducere, putem rezolva problema că datele obișnuite de formare nu au un vocabular restricționat. Cu ajutorul ulterior al decodării constrânse în faza de inferență, am obținut rezultate mai bune decât baza de referință, confirmând eficiența soluției noastre. În plus, am încercat și transformatorul vanilat și rar ca rețea coloanei vertebrale a modelului, precum și ansamblul modelului, care a îmbunătățit și mai mult performanța traducerii finale.", 'sr': "Ovaj papir opisuje naš sistem (ID tima: nictrb) za sudjelovanje u zadatku WAT'21 ograničenom prevodom mašine. U našem podnošenom sistemu, dizajnirali smo novi pristup obuke za ograničeni prevod mašine. Uzimajući uzorak od cilja prevođenja, možemo rešiti problem da obični podaci obuke nemaju ograničeni rečnik. Uz daljnju pomoć ograničenog dekodiranja u fazi infekcije, postigli smo bolji rezultat od početne linije, potvrđujući učinkovitost našeg rješenja. Osim toga, pokušali smo i vanilu i rezervni transformer kao rezervni mreža modela, kao i model ensembliranja, što je dalje poboljšalo konačnu izvedbu prevoda.", 'no': "Denne papiret beskriver systemet vårt (gruppe ID: nictrb) for å delta i oppgåva for WAT'21 avgrensa maskinsomsetjing. I vår tilsendte systemet designerte vi ein ny treningstilgang for avgrensa maskinsomsetjing. Ved å prøve ut frå omsetjingsmålet kan vi løse problemet som vanlege treningsdata ikkje har ei begrenset ordbok. Med det fleire hjelpet om begrenset dekoding i infeksjonsfase, har vi oppnådd bedre resultat enn baselinja, som stadfestar effektiviteten av løysinga vårt. I tillegg prøvde vi også vanilla og sparse transformeringa som bakgrunnsbanenettverket i modellen, og modellen for ensembering, som framleis forbedra den siste omsetjingsfunksjonen.", 'si': "මේ පැත්තේ අපේ පද්ධතිය (කණ්ඩායම ID: nictrib) විවෘත කරනවා WAT' 21 සීමාවිත පද්ධතිය භාවිතය වැඩේ සම්බන්ධ කරන්න. අපේ පිළිගන්න පද්ධතියේ අපි අළුත් ප්\u200dරශ්නයක් සැකසුම් කරලා තියෙන්නේ පද්ධතිය අවවාදය සඳහ පරිවර්තනය ඉලක්කයෙන් සැම්පල් කරන්න, අපිට පුළුවන් සාමාන්\u200dය ප්\u200dරශ්නයක් තියෙන ප්\u200dරශ්නයක් විසඳන්න පුළු අපිට ප්\u200dරතිශාල විදිහට වඩා හොඳ ප්\u200dරතිශාල ප්\u200dරතිශාල වෙනුවෙන් ප්\u200dරතිශාල වෙනුවෙන් ප්\u200dරතිශාල වෙනුවෙන් ප්\u200dරත ඒ වගේම, අපි වැනිල්ලා වලින් ප්\u200dරමාණය කරලා තියෙන්නේ මොඩේල්ගේ පස්සේ බැක්කෝන් ජාලය වලින්, මොඩේල් එක්ක අන්තිම වාර්", 'so': "Warqaddan waxaa ku qoran nidaamka (kooxda ID: nictrb) si aad uga qayb gasho shaqada turjumista machine ee WAT'21 xadhig ah. nidaamka la soo dhiibay, waxaynu sameynay qaabab cusub oo waxbarasho ah si loo turjumo mashiinka xadiijiyey. Tusaale ahaan kaalmada turjumaadda ayaannu ku xalli karnaa dhibaatada arimaha waxbarashada caadiga ah aysan heysan hadal xadiig ah. Markaan caawimaad dheeraad ah oo la qasbay in la kordhiyo fasaxa jirada, waxaynu helay resulti ka wanaagsan tan hoose, waxaan ku xaqiijinnay waxyaabaha ay ku yeelan karto xafiiska. Sidoo kale waxaynu isku daynay baabuurta iyo baabuurta sida shabakadda bakhtiga ee muusikada, sidoo kale sameynta muusikada, kaasoo sii kordhisay sameynta tarjumaadka ugu dambeeya.", 'sv': "Denna uppsats beskriver v책rt system (Team ID: nicktrb) f철r deltagande i WAT'21 begr채nsade maskin철vers채ttningsuppgifter. I v책rt inl채mnade system utformade vi en ny utbildningsmetod f철r begr채nsad maskin철vers채ttning. Genom att ta prover fr책n 철vers채ttningsm책let kan vi l철sa problemet med att vanliga tr채ningsdata inte har ett begr채nsat ordf철rr책d. Med ytterligare hj채lp av begr채nsad avkodning i inferensfasen uppn책dde vi b채ttre resultat 채n baslinjen, vilket bekr채ftar effektiviteten i v책r l철sning. Dessutom testade vi 채ven vanilj och glesa Transformer som modellens ryggradsn채tverk, samt modellensemblering, vilket ytterligare f철rb채ttrade den slutliga 철vers채ttningen.", 'ta': '@ info எங்கள் வழங்கப்பட்ட அமைப்பில், நாங்கள் ஒரு புதிய பயிற்சி முறைமையை வடிவமைத்தோம் கடுமையான இயந்திர மொழிபெயர்ப்பு சேர்க்கையிலிருந்து மாதிரிக்கும் பொழுது, சாதாரண பயிற்சி தரவு சொல்வளத்தின் வரம்பு இல்லாதது என பாதிப்புக்குறியீட்டை மேலும் குறைக்கப்பட்ட உதவியுடன், நாம் அடிப்படையில் விட சிறந்த முடிவு மேலும், நாம் வானில்லா மற்றும் மாற்றி மாற்றி மாற்றி மாதிரியை மாதிரியின் பின்னணி வலைப்பின்னலாக சேர்க்க முயற்சி செய்தோம', 'ur': "This paper describes our system (Team ID: nictrb) for participating in the WAT'21 restricted machine translation task. ہم نے اپنے مستقیم سیسٹم میں ایک نئی تدریس طریقہ طراحی کی محدودہ ماشین ترجمہ کے لئے۔ ترجمہ موقع سے نمونہ بنانے کے ذریعہ، ہم اس مسئلہ کو حل کر سکتے ہیں جس کی معمول ترکین ڈیٹے کے پاس کوئی محدودہ واژوکلی نہیں ہے. کمزور دکھانے کی مدد کے ساتھ ہم نے بنیادی لین سے بہتر نتیجے پہنچ گئے، ہمارے حل کی تائید کی۔ اس کے علاوہ ہم نے وینلیا اور اسپارس ٹرنفسر کو مدل کے پشت ہڈی نیٹورک بنا کر آزمایا تھا، اور مدل انٹنسمبلینگ کے ساتھ، جس نے آخری ترجمہ کی عمدہ کو اضافہ کیا۔", 'uz': "Name Joʻnatilgan tizimimizda, biz qanchalik mashina tarjima qilish uchun yangi ta'lim usulini yaratdik. Tarjima maqsaddan misol qilib, oddiy taʼminlovchi maʼlumot qoʻllanmagan lugʻatning cheksiz maxfiy soʻzda emas. Ko'rib chiqarish darajadagi qanday yordam bilan biz asosiy qoidadan yaxshi natijalarni bajaramiz, bu tuzuvchimizning effektini ishlatish imkoniyatini bajaramiz. Ko'pchilik, biz bu modelning backbon tarmoqda vanillarni ko'rib chiqarishni istadik va modelni foydalanishimiz mumkin. Bu oxirgi tarjima bajarishni bajaradi.", 'vi': "Tờ giấy này mô tả hệ thống của chúng ta (Team ID: nictrb) để tham gia vào việc dịch chuyển máy hạn chế WAT'21. Chúng tôi đã thiết kế một phương pháp huấn luyện mới cho việc dịch chuyển máy hạn chế. Bằng cách lấy mẫu từ mục tiêu dịch, chúng ta có thể giải quyết vấn đề rằng dữ liệu tập luyện thông thường không có từ ngữ hạn. Với sự giúp đỡ tiếp theo của việc giải mã giới hạn trong giai đoạn nhận biết, chúng tôi đạt được kết quả tốt hơn so với cơ sở cơ bản, xác nhận hiệu quả của giải pháp. Thêm vào đó, chúng tôi cũng thử một chương trình biến hình vani và rò rỉ như hệ thống xương sống của mô hình, cũng như dàn hợp mô hình, nâng cao độ dịch cuối cùng.", 'bg': 'Тази статия описва нашата система за участие в задачата за ограничен машинен превод. В подадената от нас система разработихме нов подход за обучение за ограничен машинен превод. Чрез вземане на проби от целта за превод можем да решим проблема, че обикновените данни за обучение нямат ограничен речник. С по-нататъшната помощ на ограниченото декодиране във фазата на заключение постигнахме по-добри резултати от базовата база, потвърждавайки ефективността на нашето решение. В допълнение, опитахме ваниловия и рядък трансформатор като гръбначна мрежа на модела, както и ансамблирането на модела, което допълнително подобри ефективността на окончателния превод.', 'hr': "Ovaj papir opisuje naš sustav (ID tima: nictrb) za sudjelovanje u zadatku WAT'21 ograničenom prevodom stroja. U našem podnošenom sustavu, dizajnirali smo novi pristup obuke za ograničeni prevod stroja. Uzorak iz cilja prevođenja možemo riješiti problem u kojem obični podaci obuke nemaju ograničeni rječnik. Uz daljnju pomoć ograničenog dekodiranja u fazi infekcije postigli smo bolji rezultat od početne linije, potvrđujući učinkovitost našeg rješenja. Osim toga, pokušali smo i vanilu i rezervni transformer kao rezervni mreža modela, kao i model ensembliranja, što je dalje poboljšalo konačnu izvodnju prevoda.", 'da': "Denne artikel beskriver vores system (Team ID: nictrb) til deltagelse i WAT'21 begrænset maskinoversættelsesopgave. I vores indsendte system har vi designet en ny træningstilgang til begrænset maskinoversættelse. Ved at prøve fra oversættelsesmålet kan vi løse problemet, at almindelige træningsdata ikke har et begrænset ordforråd. Med yderligere hjælp fra begrænset afkodning i inferencefasen opnåede vi bedre resultater end baseline, hvilket bekræftede effektiviteten af vores løsning. Derudover prøvede vi også vanilje og sparsomme Transformer som modellens rygradenetværk, samt model ensembling, som yderligere forbedrede den endelige oversættelses ydeevne.", 'de': "Dieses Papier beschreibt unser System (Team ID: nictrb) für die Teilnahme an der WAT'21 eingeschränkten maschinellen Übersetzungsaufgabe. In unserem eingereichten System haben wir einen neuen Schulungsansatz für eingeschränkte maschinelle Übersetzungen entwickelt. Durch Stichproben aus dem Übersetzungsziel können wir das Problem lösen, dass gewöhnliche Trainingsdaten keinen eingeschränkten Wortschatz haben. Mit der weiteren Hilfe der eingeschränkten Dekodierung in der Inferenzphase erzielten wir bessere Ergebnisse als die Baseline und bestätigten die Wirksamkeit unserer Lösung. Darüber hinaus haben wir auch den vanille und sparse Transformer als Rückgratnetzwerk des Modells ausprobiert, sowie das Modellensembling, das die endgültige Übersetzungsleistung weiter verbessert hat.", 'id': "This paper describes our system (Team ID: nictrb) for participating in the WAT'21 restricted machine translation task.  Dalam sistem kami, kami merancang pendekatan latihan baru untuk terjemahan mesin terbatas. Dengan mengambil sampel dari sasaran terjemahan, kita dapat memecahkan masalah bahwa data pelatihan biasa tidak memiliki vocabulari terbatas. With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution.  Selain itu, kami juga mencoba vanilla dan sparse Transformer sebagai jaringan tulang belakang model, serta model ensembling, yang lebih memperbaiki prestasi terjemahan akhir.", 'ko': "본고는 우리가 WAT'21 제한된 기계 번역 임무에 참여하는 시스템(팀 ID:nictrb)을 묘사한다.우리가 제출한 시스템에서, 우리는 제한된 기계 번역을 위해 새로운 훈련 방법을 설계했다.번역 목표에 대한 표본 추출을 통해 우리는 일반 훈련 데이터에 어휘량이 한정되지 않은 문제를 해결할 수 있다.추리 단계에서 디코딩을 제약하는 진일보한 도움 아래 우리는 기선보다 더 좋은 결과를 얻었고 우리의 해결 방안의 유효성을 증명했다.그 밖에 우리는 바닐라와 희소 변환기를 모델의 메인 네트워크로 하고 모델 암호화를 시도했기 때문에 최종 번역 성능을 더욱 향상시켰다.", 'fa': "این کاغذ سیستم ما را توصیف می\u200cکند (کارت شناسایی تیم: nictrb) برای شرکت در کار ترجمه ماشین محدود شده WAT'21. در سیستم تحویل داده شده\u200cایم، ما یک روش آموزش جدید برای ترجمه ماشین محدود طراحی کردیم. با نمونه\u200cبندی از هدف ترجمه، می\u200cتوانیم مشکلی را حل کنیم که داده\u200cهای آموزش معمولی یک کلمه محدود ندارد. با کمک بیشتری از دکوندن محدودیت در مرحله آلودگی، نتیجه\u200cهای بهتر از مرحله\u200cی پایین به دست آوردیم، تایید کردن فعالیت راه حل ما. علاوه بر این، ما وانیلا را هم امتحان کردیم و تغییر دهنده را به عنوان شبکه پشت استخوان مدل، همچنین مدل تغییر دهنده را هم امتحان کردیم، که انجام انجام نهایی ترجمه را بهتر کرد.", 'sw': "Gazeti hili linaelezea mfumo wetu (Idara ya Timu: nictrb) kwa kushiriki kazi ya kutafsiri mashine yenye vizuizi vya WAT'21. Katika mfumo wetu ulioandaliwa, tuliunda mbinu mpya ya mafunzo kwa ajili ya kutafsiri mashine ya vizuizi. Kwa kupitia sampuli kutoka kwenye lengo la kutafsiri, tunaweza kutatua tatizo ambalo taarifa za mafunzo ya kawaida hazina lugha isiyo ya vizuizi. Kwa msaada zaidi wa kupungua kwa kiwango cha maambukizi, tulifanikiwa matokeo bora kuliko msingi wa msingi, tunathibitisha ufanisi wa ufumbuzi wetu. Zaidi ya hayo, tulijaribu pia kusababisha Transfer kama Mtandao wa mpira wa mifano, pamoja na muonekano wa mifano, ambao ulibadilisha ufafanuzi wa mwisho wa tafsiri.", 'tr': "Bu kagyz biziň sistemimizi (topar ID: nictrb) WAT'iň 21-nji muglany maşynyň terjime täblisasynda goşulýar. Biziň gönderilen sistemamyzda maşynyň terjime edilmesi üçin täze bir eğitim metody tasarladyk. Terjime maksadynyň öräni örän çözerek, adatça okuwçy maglumatyň sözleriň ýok diýip çözebiliriz. Çykyşyrlyk phasesynda azalýan azalýan kömegimiz bilen, basit çykyşymyzyň etkinliğini tassyklap başladyk. Munuň üçin biz de vanilla we küýtgeden transformatöri modeliniň arka çykyş aňladygyny we nusgasyny çykardyk. Soňky terjime edeniň netijesini geliştirdik.", 'nl': "Dit artikel beschrijft ons systeem (Team ID: nictrb) voor deelname aan de WAT'21 beperkte machinevertaaltaak. In ons ingediende systeem hebben we een nieuwe trainingsaanpak ontworpen voor beperkte machinevertaling. Door te bemonsteren van het vertaaldoel kunnen we het probleem oplossen dat gewone trainingsgegevens geen beperkte woordenschat hebben. Met verdere hulp van beperkte decodering in de inferentiefase bereikten we betere resultaten dan de baseline, wat de effectiviteit van onze oplossing bevestigde. Daarnaast probeerden we ook de vanille en sparse Transformer als backbone netwerk van het model, evenals model ensembling, wat de uiteindelijke vertaalprestaties verder verbeterde.", 'sq': "Ky dokument përshkruan sistemin tonë (ID ekipi: nictrb) për pjesëmarrjen në detyrën e përkthimit të kufizuar të makinave WAT'21. Në sistemin tonë të paraqitur, kemi projektuar një metodë të re trajnimi për përkthimin e kufizuar të makinave. By sampling from the translation target, we can solve the problem that ordinary training data does not have a restricted vocabulary.  Me ndihmën e mëtejshme të dekodimit të kufizuar në fazën e përfundimit, arritëm rezultate më të mira se baza, duke konfirmuar efektshmërinë e zgjidhjes sonë. Përveç kësaj, ne provuam gjithashtu vanilën dhe shkurtuam Transformuesin si rrjetin e shpinës së modelit, si dhe modelin e mbledhjes, që përmirësoi më tej performancën përfundimtare të përkthimit.", 'am': 'ይህ ፕሮግራም የጦማር መሣሪያን ትርጓሜ ስራትን ለመጋቢት ስርዓታችንን (የتیም ID: nictrb) ይናገራል፡፡ በተገኘን ስርዓታችን አዲስ የሞክራዊ ትርጓሜን ለመግለጽ አዲስ አስተማርነት አግኝተናል፡፡ ትርጉም አካላቢ በመምሳሌ፣ የዳታ ማጠቃለያ አካባቢ ቃላት የሌላትን መክፈት እንችላለን፡፡ With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution.  በተጨማሪም፣ የፎናይላን እና የሞዴል ጀርባ መረብ እና የሞዴል መረብ እና የፍጻሜውን ትርጉም አሻራው፡፡', 'hy': 'Այս հոդվածը նկարագրում է մեր համակարգը (թիմի ID: Nick trb) մասնակցելու սարքավորված մեքենայի թարգմանման աշխատանքի համար: Մեր ներկայացված համակարգում մենք ստեղծեցինք նոր վարժման մոտեցում սահմանափակ մեքենայի թարգմանման համար: Մենք կարող ենք լուծել այն խնդիրը, որ սովորական ուսումնասիրության տվյալները սահմանափակ բառարան չունեն: Առաջին օգնությամբ սահմանափակ կոդավորման դեպքում, մենք ավելի լավ արդյունքներ ստացանք, քան հիմնական արդյունքները, հաստատելով մեր լուծության արդյունավետությունը: In addition, we also tried the vanilla and sparse Transformer as the backbone network of the model, as well as model ensembling, which further improved the final translation performance.', 'bs': "Ovaj papir opisuje naš sistem (ID tima: nictrb) za sudjelovanje u zadatku WAT'21 ograničenom prevodom mašine. U našem podnošenom sistemu, dizajnirali smo novi pristup obuke za ograničeni prevod stroja. Uzorak iz cilja prevođenja, možemo riješiti problem koji obični podaci obuke nemaju ograničeni rečnik. Uz daljnju pomoć ograničenog dekodiranja u fazi infekcije, postigli smo bolji rezultat od početne linije, potvrđujući učinkovitost našeg rješenja. Osim toga, pokušali smo i vanilu i rezervni transformer kao rezervni mreža modela, kao i model ensembliranja, što je dalje poboljšalo konačnu izvodnju prevoda.", 'af': "Hierdie papier beskryf ons stelsel (Tim ID: nictrb) vir deelnadering in die WAT' 21 beperkte masjien vertaling taak. In ons ingestuurde stelsel het ons 'n nuwe onderwerp toegang ontwerp vir beperkte masjien vertaling. Deur voorbeeld van die vertaling doel, kan ons die probleem oplos wat gewone onderwerking data nie 'n beperkte woordeboek het nie. Met die verdere hulp van beperkte dekoding in die inferensie fase het ons beter resultate as die basisline bereik en die effektiviteit van ons oplossing bevestig. In addition, we also tried the vanilla and sparse Transformer as the backbone network of the model, as well as model ensembling, which further improved the final translation performance.", 'bn': "এই পত্রিকা আমাদের সিস্টেমের (টিম আইডি: নিক্ট্রিব) ব্যাখ্যা করেছে WAT'21 সীমিত মেশিন অনুবাদ কর্মসূচিতে অংশগ্ আমাদের জবাব দেয়া সিস্টেমে আমরা সীমিত মেশিন অনুবাদের জন্য একটি নতুন প্রশিক্ষণের পদ্ধতি গঠন করেছি। অনুবাদের লক্ষ্য থেকে উদাহরণের মাধ্যমে আমরা সমস্যা সমাধান করতে পারি যে সাধারণ প্রশিক্ষণের তথ্য সীমিত শব্দভাণ্ডার নেই। এই অসুস্থ পর্যায়ে নিষেধাজ্ঞা প্রদানের আরো সাহায্যের সাথে আমরা বেসেলাইনের চেয়ে ভাল ফলাফল অর্জন করেছি, আমাদের সমাধানের কার্যকর ক এছাড়াও আমরা ভ্যানিলাকে চেষ্টা করেছি এবং মডেলের পিছনের নেটওয়ার্ক হিসেবে ট্রান্সফারকে স্মার্স করে দিয়েছি, আর মডেলের সাথে মোডেলের", 'ca': "Aquest article descriu el nostre sistema (ID de l'equip: nictrb) per participar en la tasca restringida de traducció màquina WAT'21. En el nostre sistema submetit, vam dissenyar un nou enfocament d'entrenament per a traducció restringida de màquines. A través de la mostra de l'objectiu de traducció, podem resoldre el problem a que les dades d'entrenament normal no tenen un vocabulari restringit. Amb més ajuda de decodificació restringida en la fase de inferència, vam aconseguir millors resultats que el basal, confirmant l'eficacia de la nostra solució. A més, també vam provar la vanília i l'escassa transformadora com la xarxa vertebral del model, així com la combinació de models, que millorava més el rendiment final de la traducció.", 'et': "Käesolevas artiklis kirjeldatakse meie süsteemi (Team ID: nictrb) osalemiseks WAT'21 piiratud masintõlke ülesandes. Meie esitatud süsteemis töötasime välja uue koolitusviisi piiratud masintõlke jaoks. Tõlke sihtmärgist võttes saame lahendada probleemi, et tavalistel koolitusandmetel ei ole piiratud sõnavara. Piiratud dekodeerimise abil järeldusetapis saavutasime algtasemest paremad tulemused, mis kinnitasid meie lahenduse efektiivsust. Lisaks proovisime mudeli selgroogvõrguks vaniljet ja hõredat Transformerit, samuti mudeli ansamblit, mis parandas lõplikku tõlkimisjõudlust veelgi.", 'az': "Bu kağıt bizim sistemimizi (Team ID: nictrb) Bizim müəyyən edilmiş sistemimizdə, maşına qurğulamaq üçün yeni təhsil metodlarını tasarladıq. Tərcümə məqsədindən nümunə çəkməklə, sıradan təhsil məlumatlarının sınırlı sözləri olmadığı problemi çəkə bilərik. infeksiya fəzisində çox çətin kodlaması üçün daha yaxşı sonuçları başladıq, çətinliklərimizin etkinliğini təsdiqləyici olaraq. Daha sonra, biz də vanilla və küçük Transformer'i modelin arka sümük ağı olaraq sınağa çəkdik, həmçinin modeli ensembling olaraq, sonuncu çeviri performansını daha yaxşılaşdırdı.", 'cs': "Tento článek popisuje náš systém (Team ID: nictrb) pro účast na omezeném strojovém překladu WAT'21. V našem předloženém systému jsme navrhli nový přístup školení pro omezený strojový překlad. Vzorkováním z překladového cíle můžeme vyřešit problém, že běžná tréninková data nemají omezenou slovní zásobu. S další pomocí omezeného dekódování ve fázi inference jsme dosáhli lepších výsledků než základní hodnota, což potvrdilo efektivitu našeho řešení. Kromě toho jsme vyzkoušeli také vanilkový a řídký Transformer jako páteřní síť modelu, stejně jako model ensembling, který dále zlepšil konečný překlad výkonu.", 'fi': "Tässä artikkelissa kuvataan järjestelmäämme (Team ID: nictrb) WAT'21:n rajoitetun konekäännöksen suorittamiseksi. Lähetetyssä järjestelmässä suunnittelimme uuden koulutusmenetelmän rajoitetulle konekäännökselle. Kääntämistavoitteesta otantamalla voimme ratkaista ongelman, että tavallisilla koulutustiedoilla ei ole rajoitettua sanastoa. Rajoitetun dekoodauksen avulla päättelyvaiheessa saavutimme parempia tuloksia kuin lähtötilanne, mikä vahvisti ratkaisumme tehokkuuden. Lisäksi kokeilimme mallin selkärankana vaniljaa ja harvaa Transformeria sekä mallikokoonpanoa, mikä paransi entisestään lopullisen käännöksen suorituskykyä.", 'jv': "Ngetong iki oleh nggambar sistem dhéwé (Group ID: NITRb) nggawe ngubah weruh sistem sing berarti nang WAT'1, nari babagan Nang sistèm-sistem sing nyimpen, awak dhéwé nggawe sistem sing dibutuhke tarjamahan anyar kanggo tarjamahan dino. YB Ganggang langkung wigatining langkung di luwih dumadhi kanggo kalagayaan pangan ning lah, awak dhéwé bakal nggawe barang langkung wigat sing luwih apik dhéwé, ngêpakan layakno sing bakal nggawe bakal terus tambah apik dhéwé. Nambah, awak dhéwé wis ngubah perusahaan winih lan babagan capan pangan transformer podho nek mbukak tanggal maneh dumadhi, lak ngono model model model sing apik perusahaan, padha apik dhéwé ngrebut kanggo tukang tarjamahan.", 'ha': "Wannan takardan na bayyana tsarin na'urarmu (Team ID: nictrb) dõmin ya yi shirin da aikin fassarar mashine na ƙunsa da WAT'21. A cikin shirin da muka ƙarfafa, sai muka design wata hanyoyi na aikin mafarinta zuwa fassarar mashine da aka ƙunsa. Ga ka sami daga shirin birane na fassarar, za mu iya solar wa matsalar da data na amfani da ɗabi'a ba da wani tsari ba. Da taimakon a ƙari da aka lazimta kodi cikin fasani na kasa, sai muka sami mafiya alhẽri ga matsayi ko bakin basalin ayuka, kuma mun gaskata mafiya amfani da fasalinmu. Da haka, mun jarraba Transformer kamar jerin-bakin misalin misalin, da kuma misalin misalin misalin, wanda ya ƙaranci tsarin fassarar ta ƙarami.", 'sk': "Ta prispevek opisuje naš sistem (ID ekipe: nictrb) za sodelovanje pri omejenem strojnem prevajanju WAT'21. V našem predloženem sistemu smo oblikovali nov pristop usposabljanja za omejeno strojno prevajanje. Z vzorčenjem iz prevajalskega cilja lahko rešimo problem, da običajni podatki o usposabljanju nimajo omejenega besedišča. Z nadaljnjo pomočjo omejenega dekodiranja v fazi sklepanja smo dosegli boljše rezultate od izhodišča, kar potrjuje učinkovitost naše rešitve. Poleg tega smo preizkusili tudi vanilijev in redki transformator kot hrbtenično mrežo modela, kot tudi model ansambling, ki je dodatno izboljšal končno prevajanje.", 'bo': 'ཤོག་བྱང་འདིས་ང་ཚོའི་མ་ལག་ཅིག་འགྲེལ་བཤད་པ(Team ID:nictrb) ང་ཚོའི་མ་ལག་ལ་འཛུགས་བྱས་པའི་ནང་དུ་ང་ཚོས་ལག་ལེན་འཐབ་ལམ་གསརཔ་ཞིག་བཟོ་བྱས་པ་རེད། དཔེ་དབྱིབས་ཡིག་གི་དམིགས་ཡུལ་ལས་དཔེ་དབྱིབས་བྱས་ན་ང་ཚོས་རྒྱུན་ལྡན་གྱི་གནད་དོན་མིན་འདུག འོད་ཀྱང་། ཨ་ནི། ད་དུང་། ང་ཚོས་བྱ་ཚིག་དང་། བརྗེད་སྒྲུང་གི་རྒྱབ་མཐའ་དྲ་ཡིས་ལྟར་འབད་བརྩོན་བྱས་པ་ཡིན།', 'he': "העיתון הזה מתאר את המערכת שלנו (ID הצוות: nictrb) לשתתף במשימת התרגום המכונית מוגבלת WAT' 21. In our submitted system, we designed a new training approach for restricted machine translation.  על ידי דגימה ממטרת התרגום, אנחנו יכולים לפתור את הבעיה כי נתוני אימון רגילים אין מילון מוגבל. עם עזרה נוספת של פיקוד מוגבלת בשלב המסקנה, השגנו תוצאות טובות יותר מהמראש, מאשר את היעילות של הפתרון שלנו. In addition, we also tried the vanilla and sparse Transformer as the backbone network of the model, as well as model ensembling, which further improved the final translation performance."}
{'en': 'NECTEC’s Participation in WAT-2021 NECTEC ’s Participation in  WAT -2021', 'es': 'Participación de NECTEC en WAT-2021', 'ar': 'مشاركة NECTEC في WAT-2021', 'fr': 'Participation de NECTEC à WAT-2021', 'pt': 'Participação da NECTEC no WAT-2021', 'ja': 'NECTECのWAT -2021への参加', 'zh': 'NECTECé¢„WAT-2021', 'hi': 'WAT-2021 में NECTEC की भागीदारी', 'ru': 'Участие NECTEC в WAT-2021', 'ga': 'Rannpháirtíocht NECTEC in WAT-2021', 'el': 'Συμμετοχή της NECTEC στο WAT-2021', 'hu': 'A NECTEC részvétele a WAT-2021-ben', 'ka': 'NECTEC-ის დაწყვეტილება WAT-2021-ში', 'kk': 'NECTEC- ның WAT- 2021 қатысуы', 'it': 'Partecipazione del NECTEC al WAT-2021', 'lt': 'NECTEC dalyvavimas WAT-2021 m.', 'mk': "NECTEC's Participation in WAT-2021", 'ms': 'Pesertaan NECTEC dalam WAT-2021', 'mt': 'Il-Parteċipazzjoni tan-NECTEC fl-WAT-2021', 'mn': 'NECTEC-ын WAT-2021-д оролцоо', 'ml': 'വാട്ട്- 2021 ല്\u200d നെക്റ്റെസിസിയുടെ പങ്കാളി', 'pl': 'Udział NECTEC w WAT-2021', 'ro': 'Participarea NECTEC la WAT-2021', 'no': 'NECTEC- deltakaren i WAT- 2021', 'sr': 'NECTEC učestvovanje u WAT-2021', 'si': "NECTCE's partition in WAT-2021", 'so': 'NECTEC ka qayb-qaadashada WAT-2021', 'sv': 'NECTEC:s deltagande i WAT-2021', 'ur': 'NECTEC کا حصہ WAT-2021 میں', 'ta': 'WAT- 2021 ல் நெக்டிசியின் பகிர்ந்தது', 'uz': 'Comment', 'vi': 's ự tham gia của NEnổ trong WAT-2021', 'bg': 'Участие на NECTEC в WAT-2021', 'da': "NECTEC's deltagelse i WAT-2021", 'nl': 'Deelname van NECTEC aan WAT-2021', 'id': "NECTEC's Participation in WAT-2021", 'ko': 'NECTEC 참여 WAT-2021', 'fa': 'شرکت NECTEC در WAT-2021', 'sw': 'Ushiriki wa NECTEC katika WAT-2021', 'hr': 'NECTEC učestvovanje u WAT-2021', 'af': 'NECTEC se Deelnadering in WAT-2021', 'sq': 'Pjesëmarrja e NECTEC në WAT-2021', 'de': 'Teilnahme von NECTEC an WAT-2021', 'tr': "NECTEC's Participation in WAT-2021", 'am': 'Šč®NECTEC ŠČįŠĆćŠČ£Šą≠ ŠČ†WAT-2021', 'az': "NECTEC's Participation in WAT-2021", 'bn': 'ওয়াট-২০১২ এ নেটসিসির অংশগ্রহণ', 'bs': 'Učestvo NECTEC-a u WAT-2021', 'ca': 'La participació del NECTEC en WAT-2021', 'cs': 'Účast NECTEC na WAT-2021', 'hy': 'Նեկտիկի մասնակցությունը', 'fi': 'NECTECin osallistuminen WAT-2021 -ohjelmaan', 'et': 'NECTECi osalemine WAT-2021-s', 'he': 'השתתפות של NECTEC ב-WAT-2021', 'sk': 'Sodelovanje NECTEC v WAT-2021', 'ha': '@ info: whatsthis', 'jv': 'ni', 'bo': "NECTEC's Participation in WAT-2021"}
{'en': 'In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team for the translation tasks of WAT-2021. Basically, our models are based on  neural methods  for both directions of English-Myanmar and Myanmar-English language pairs. Most of the existing Neural Machine Translation (NMT) models mainly focus on the conversion of sequential data and do not directly use syntactic information. However, we conduct multi-source neural machine translation (NMT) models using the multilingual corpora such as string data corpus, tree data corpus, or POS-tagged data corpus. The multi-source translation is an approach to exploit multiple inputs (e.g. in two different formats) to increase translation accuracy. The RNN-based encoder-decoder model with attention mechanism and transformer architectures have been carried out for our experiment. The experimental results showed that the proposed models of RNN-based architecture outperform the  baseline model  for English-to-Myanmar translation task, and the multi-source and shared-multi-source transformer models yield better translation results than the  baseline .', 'ar': 'في هذه الورقة ، نقدم تقريرًا عن النتائج التجريبية لنماذج الترجمة الآلية التي أجراها فريق NECTEC لمهام ترجمة WAT-2021. تعتمد نماذجنا بشكل أساسي على الأساليب العصبية لكلا الاتجاهين من أزواج اللغتين الإنجليزية - ميانمار ، واللغة الإنجليزية الميانمارية. تركز معظم نماذج الترجمة الآلية العصبية (NMT) الحالية بشكل أساسي على تحويل البيانات المتسلسلة ولا تستخدم المعلومات النحوية بشكل مباشر. ومع ذلك ، فإننا نجري نماذج ترجمة آلية عصبية متعددة المصادر (NMT) باستخدام مجموعات متعددة اللغات مثل مجموعة بيانات السلسلة ، أو مجموعة بيانات الشجرة ، أو مجموعة بيانات POS الموسومة. الترجمة متعددة المصادر هي طريقة لاستغلال المدخلات المتعددة (على سبيل المثال في نسقين مختلفين) لزيادة دقة الترجمة. تم تنفيذ نموذج وحدة فك التشفير المستند إلى RNN مع آلية الانتباه وبنيات المحولات لتجربتنا. أظهرت النتائج التجريبية أن النماذج المقترحة للهندسة المعمارية القائمة على RNN تتفوق على النموذج الأساسي لمهمة الترجمة من الإنجليزية إلى ميانمار ، وأن نماذج المحولات متعددة المصادر والمشتركة متعددة المصادر تسفر عن نتائج ترجمة أفضل من خط الأساس.', 'es': 'En este artículo, presentamos los resultados experimentales de modelos de traducción automática realizados por un equipo de NECTEC para las tareas de traducción de WAT-2021. Básicamente, nuestros modelos se basan en métodos neuronales para ambas direcciones de los pares lingüísticos inglés-Myanmar y Myanmar-inglés. La mayoría de los modelos de traducción automática neuronal (NMT) existentes se centran principalmente en la conversión de datos secuenciales y no utilizan directamente información sintáctica. Sin embargo, realizamos modelos de traducción automática neuronal (NMT) de múltiples fuentes utilizando los corpus multilingües, como el corpus de datos de cadenas, el corpus de datos de árbol o el corpus de datos con etiquetas POS. La traducción de múltiples fuentes es un enfoque para aprovechar múltiples entradas (por ejemplo, en dos formatos diferentes) para aumentar la precisión de la traducción. Para nuestro experimento se ha llevado a cabo el modelo de codificador-decodificador basado en RNN con mecanismos de atención y arquitecturas de transformadores. Los resultados experimentales mostraron que los modelos propuestos de arquitectura basada en RNN superan al modelo de referencia para la tarea de traducción del inglés al birmano, y que los modelos de transformadores de múltiples fuentes y de múltiples fuentes compartidas producen mejores resultados de traducción que la línea de base.', 'fr': "Dans cet article, nous présentons les résultats expérimentaux de modèles de traduction automatique menés par une équipe NECTEC pour les tâches de traduction de WAT-2021. Fondamentalement, nos modèles sont basés sur des méthodes neuronales pour les paires de langues anglais-Myanmar et Myanmar-Anglais dans les deux directions. La plupart des modèles de traduction automatique neuronale (NMT) existants se concentrent principalement sur la conversion de données séquentielles et n'utilisent pas directement d'informations syntaxiques. Cependant, nous réalisons des modèles de traduction automatique neuronale (NMT) multi-sources à l'aide de corpus multilingues tels que des corpus de données de chaînes, des corpus de données arborescentes ou des corpus de données étiquetés POS. La traduction multi-sources est une approche qui exploite plusieurs entrées (par exemple dans deux formats différents) pour augmenter la précision de la traduction. Le modèle encodeur-décodeur basé sur RNN avec mécanisme d'attention et architectures de transformateurs a été réalisé pour notre expérience. Les résultats expérimentaux ont montré que les modèles proposés d'architecture basée sur RNN surpassent le modèle de base pour les tâches de traduction de l'anglais vers le Myanmar, et que les modèles de transformateurs multi-sources et multi-sources partagées donnent de meilleurs résultats de traduction que le modèle de référence.", 'pt': 'Neste artigo, relatamos os resultados experimentais de modelos de tradução automática conduzidos por uma equipe da NECTEC para as tarefas de tradução do WAT-2021. Basicamente, nossos modelos são baseados em métodos neurais para ambas as direções dos pares de idiomas Inglês-Mianmar e Mianmar-Inglês. A maioria dos modelos de Neural Machine Translation (NMT) existentes concentram-se principalmente na conversão de dados sequenciais e não usam diretamente informações sintáticas. No entanto, conduzimos modelos de tradução automática neural de várias fontes (NMT) usando os corpora multilíngues, como corpus de dados de string, corpus de dados de árvore ou corpus de dados com tags POS. A tradução de várias fontes é uma abordagem para explorar várias entradas (por exemplo, em dois formatos diferentes) para aumentar a precisão da tradução. O modelo de codificador-decodificador baseado em RNN com mecanismo de atenção e arquiteturas de transformador foi realizado para nosso experimento. Os resultados experimentais mostraram que os modelos propostos de arquitetura baseada em RNN superam o modelo de linha de base para a tarefa de tradução de inglês para Mianmar, e os modelos de transformador de várias fontes e fontes compartilhadas produzem melhores resultados de tradução do que a linha de base.', 'ja': '本稿では、NECTECチームがWAT -2021の翻訳タスクのために実施した機械翻訳モデルの実験結果を報告する。 基本的に私たちのモデルは英語とミャンマー語とミャンマー語のペアの両方向のニューラルメソッドに基づいています 既存の神経機械翻訳（ NMT ）モデルのほとんどは、主にシーケンシャルデータの変換に焦点を当てており、構文情報を直接使用していません。 しかし、文字列データコーパス、ツリーデータコーパス、POSタグ付きデータコーパスなどの多言語コーパスを使用して、多源ニューラル機械翻訳（ NMT ）モデルを実施しています。 マルチソース翻訳は、翻訳の精度を高めるために複数の入力を活用するアプローチです（たとえば、2つの異なる形式）。 注意メカニズムと変圧器アーキテクチャを備えたRNNベースのエンコーダデコーダモデルを実験のために実施しました。 実験結果は、提案されたRNNベースのアーキテクチャのモデルが、英語からミャンマー語への翻訳タスクのベースラインモデルを上回り、マルチソースモデルと共有マルチソーストランスモデルは、ベースラインよりも良い翻訳結果をもたらすことを示した。', 'zh': '于本文,告NECTEC团队为WAT-2021译者机器翻译实验之。 盖英语 - 缅甸语与缅语 - 英语语言两方神经法也。 今世大众神经机器翻译(NMT)要在次第转换,不用句法息。 然用多语言语料库(如字符串数语料库、树数语料库、 POS 标数语料库)多源神经机器翻译 (NMT) 模。 多源译者,一用多输(例,两格不同)以重准确性法。 吾实验用其RNN编码器-解码器,宜有机变压器架构。 实验结果表明者,RNN之架构优于英缅译之基线,多源与共多源变压器译之效优于基线。', 'ru': 'В этой работе мы сообщаем экспериментальные результаты моделей машинного перевода, проведенных командой NECTEC для задач перевода WAT-2021. В основном, наши модели основаны на нейронных методах для обоих направлений английско-мианмарской и мьянманско-английской языковых пар. Большинство существующих моделей нейронного машинного перевода (НМП) в основном ориентированы на преобразование последовательных данных и непосредственно не используют синтаксическую информацию. Тем не менее, мы проводим модели многоисточника нейронного машинного перевода (NMT) с использованием многоязычных корпусов, таких как корпус строковых данных, корпус данных дерева или корпус данных с POS-тегами. Мультиисточниковый перевод - это подход к использованию нескольких входных данных (например, в двух различных форматах) для повышения точности перевода. Для нашего эксперимента была проведена модель кодировщик-декодер на основе RNN с механизмом внимания и трансформаторными архитектурами. Экспериментальные результаты показали, что предлагаемые модели архитектуры на основе RNN превосходят базовую модель для задачи перевода с английского на мианмарский язык, а многоисточники и совместно используемые многоисточники трансформаторных моделей дают лучшие результаты перевода, чем базовые.', 'hi': 'इस पेपर में, हम WAT-2021 के अनुवाद कार्यों के लिए NECTEC टीम द्वारा आयोजित मशीन अनुवाद मॉडल के प्रयोगात्मक परिणामों की रिपोर्ट करते हैं। मूल रूप से, हमारे मॉडल अंग्रेजी-म्यांमार और म्यांमार-अंग्रेजी भाषा जोड़े की दोनों दिशाओं के लिए तंत्रिका विधियों पर आधारित हैं। मौजूदा न्यूरल मशीन ट्रांसलेशन (एनएमटी) मॉडल के अधिकांश मुख्य रूप से अनुक्रमिक डेटा के रूपांतरण पर ध्यान केंद्रित करते हैं और सीधे वाक्यात्मक जानकारी का उपयोग नहीं करते हैं। हालांकि, हम बहुभाषी कॉर्पोरेट जैसे स्ट्रिंग डेटा कॉर्पस, ट्री डेटा कॉर्पस, या पीओएस-टैग किए गए डेटा कॉर्पस का उपयोग करके बहु-स्रोत तंत्रिका मशीन अनुवाद (एनएमटी) मॉडल का संचालन करते हैं। बहु-स्रोत अनुवाद अनुवाद सटीकता बढ़ाने के लिए कई इनपुट (जैसे दो अलग-अलग प्रारूपों में) का शोषण करने के लिए एक दृष्टिकोण है। ध्यान तंत्र और ट्रांसफॉर्मर आर्किटेक्चर के साथ आरएनएन-आधारित एन्कोडर-डिकोडर मॉडल हमारे प्रयोग के लिए किया गया है। प्रयोगात्मक परिणामों से पता चला है कि आरएनएन-आधारित आर्किटेक्चर के प्रस्तावित मॉडल अंग्रेजी-से-म्यांमार अनुवाद कार्य के लिए बेसलाइन मॉडल से आगे निकलते हैं, और बहु-स्रोत और साझा-बहु-स्रोत ट्रांसफार्मर मॉडल बेसलाइन की तुलना में बेहतर अनुवाद परिणाम देते हैं।', 'ga': 'Sa pháipéar seo, tuairiscímid torthaí turgnamhacha na múnlaí Aistriúcháin Meaisín a rinne foireann NECTEC do thascanna aistriúcháin WAT-2021. Go bunúsach, tá ár samhlacha bunaithe ar mhodhanna néaracha don dá threo de phéirí Béarla-Maenmar agus Maenmar-Béarla. Díríonn an chuid is mó de na samhlacha Neural Machine Translation (NMT) go príomha ar thiontú sonraí seicheamhach agus ní úsáideann siad faisnéis chomhréire go díreach. Mar sin féin, déanaimid samhlacha ilfhoinse néar-aistriúcháin meaisín (NMT) ag baint úsáide as an gcorpas ilteangach cosúil le corpas sonraí téad, corpas sonraí crann, nó corpas sonraí clibeáilte POS. Is cur chuige é an t-aistriúchán ilfhoinse chun ionchuir iolracha a shaothrú (m.sh. in dhá fhormáid dhifriúla) chun cruinneas aistriúcháin a mhéadú. Rinneadh an tsamhail ionchódóra-díchódóra atá bunaithe ar RNN le meicníocht aird agus ailtireachtaí claochladán dár dturgnamh. Léiríodh sna torthaí turgnamhacha go sáraíonn na múnlaí ailtireachta atá bunaithe ar RNN an tsamhail bhunlíne don tasc aistriúcháin Béarla-go-Maenmar, agus go mbíonn torthaí aistriúcháin níos fearr ag na samhlacha claochladán ilfhoinse agus ilfhoinse roinnte ná an bonnlíne.', 'hu': 'Ebben a tanulmányban beszámolunk a WAT-2021 fordítási feladataira a NECTEC csapat által végzett gépi fordítási modellek kísérleti eredményeiről. Alapvetően az angol-mianmari és a mianmari-angol nyelvpárok idegi módszerein alapulnak. A legtöbb meglévő Neural Machine Translation (NMT) modell elsősorban a szekvenciális adatok konvertálására összpontosít, és nem használ közvetlenül szintaktikus információkat. Mindazonáltal többforrásból álló neurális gépi fordítási modelleket (NMT) készítünk a többnyelvű korpuszok felhasználásával, mint például string data corpus, fa data corpus vagy POS-címkézett adatkorpus. A többforrásból származó fordítás több bemenet (pl. két különböző formátumban) kihasználására szolgál a fordítási pontosság növelése érdekében. Kísérletünk céljából az RNN alapú kódoló-dekóder modellt figyelemmechanizmussal és transzformátor architektúrákkal végeztük. A kísérleti eredmények azt mutatták, hogy az RNN-alapú architektúra javasolt modelljei felülmúlják az angol-mianmari fordítási feladat alapmodelljét, a többforrásos és megosztott-többforrásos transzformátor modellek pedig jobb fordítási eredményeket eredményeznek, mint az alapérték.', 'el': 'Στην παρούσα εργασία, αναφέρουμε τα πειραματικά αποτελέσματα των μοντέλων μηχανικής μετάφρασης που διεξήχθησαν από μια ομάδα για τα μεταφραστικά καθήκοντα του WAT-2021. Βασικά, τα μοντέλα μας βασίζονται σε νευρωνικές μεθόδους και για τις δύο κατευθύνσεις των γλωσσικών ζευγαριών Αγγλικά-Μιανμάρ και Μιανμάρ-Αγγλικά. Τα περισσότερα από τα υπάρχοντα μοντέλα Νευρικής Μηχανικής Μετάφρασης (NMT) επικεντρώνονται κυρίως στη μετατροπή διαδοχικών δεδομένων και δεν χρησιμοποιούν άμεσα συντακτικές πληροφορίες. Ωστόσο, διεξάγουμε μοντέλα νευρωνικής μηχανικής μετάφρασης πολλαπλών πηγών χρησιμοποιώντας τα πολύγλωσσα σώματα, όπως το σώμα δεδομένων συμβολοσειράς, το σώμα δεδομένων δέντρου ή το σώμα δεδομένων με ετικέτα. Η μετάφραση πολλαπλών πηγών είναι μια προσέγγιση για την αξιοποίηση πολλαπλών εισροών (π.χ. σε δύο διαφορετικές μορφές) για την αύξηση της ακρίβειας της μετάφρασης. Το μοντέλο κωδικοποιητή-αποκωδικοποιητή με μηχανισμό προσοχής και αρχιτεκτονικές μετασχηματιστών έχει πραγματοποιηθεί για το πείραμα μας. Τα πειραματικά αποτελέσματα έδειξαν ότι τα προτεινόμενα μοντέλα αρχιτεκτονικής βασισμένης σε RNN ξεπερνούν το μοντέλο βάσης για την εργασία μετάφρασης από Αγγλικά προς Μιανμάρ, ενώ τα μοντέλα μετασχηματιστών πολλαπλών πηγών και κοινής πηγής προσφέρουν καλύτερα αποτελέσματα μετάφρασης από τη βάση βάσης.', 'ka': 'ამ დომენტში, ჩვენ შევცდომებთ მაქსინის გასაგულისხმების მოდელების ექსპერიმენტიური შედეგები, რომლებიც NECTEC ჯგუფი გავაკეთებული WAT-2021-ის გასაგულისხმებ ჩვენი მოდელები ინგლისური-მიანმარი და მიანმარი-ანგლისური ზოგების ორივე მიზეზებისთვის ნეიროლური მეტისთვის დაბაზიან. მსგავსი ნეიროლური მაქსინის გასაგრძელება (NMT) მოდელების უფრო მეტად ფონსკენტიური მონაცემების გასაგრძელებაზე და არ გამოყენებს სინტაქტიური ინფორმაციის მაგრამ, ჩვენ მრავალური წიგნის ნეიროლური მანქანის გაგრძელება (NMT) მოდელების გამოყენება მრავალენგური კორპორაზე, როგორც სტრიქონის მონაცემები კორპუსი, ხერხის მონაცემე მრავალური წიგნალის გაგრძელება არის მრავალური წიგნალის გაგრძელება (მაგალითად, ორი განსხვავებული ფორმატებში) გაგრძელება წესიერება. RNN-დაბათი კოდერის რეკოდერების მოდელი ჩვენი ექსპერიმენტისთვისთვისთვის გააკეთებულია მონაცემული მექანიზმი და ტრანფორმების აქტიქტიქტურის ექსპერიმენტიური წარმოდგენები გამოჩვენეთ, რომ RNN-დაბათული არქტიქტურის მოდელები ინგლისურ-მიანმარის გადარგენის მოდელზე უფრო უფრო მეტი წარმოდგენების მოდელს, და მრავალური წარმოდგენების და მრავალური', 'it': "In questo articolo riportiamo i risultati sperimentali dei modelli di traduzione automatica condotti da un team NECTEC per le attività di traduzione di WAT-2021. Fondamentalmente, i nostri modelli si basano su metodi neurali per entrambe le direzioni di coppie di lingue inglese-Myanmar e Myanmar-inglese. La maggior parte dei modelli esistenti di traduzione automatica neurale (NMT) si concentra principalmente sulla conversione di dati sequenziali e non utilizzano direttamente informazioni sintattiche. Tuttavia, eseguiamo modelli di traduzione automatica neurale multi-source (NMT) utilizzando corpora multilingue come corpus di dati stringa, corpus di dati albero o corpus di dati con tag POS. La traduzione multi-source è un approccio per sfruttare più input (ad esempio in due formati diversi) per aumentare la precisione della traduzione. Il modello encoder-decoder basato su RNN con meccanismo di attenzione e architetture di trasformatori sono stati eseguiti per il nostro esperimento. I risultati sperimentali hanno mostrato che i modelli proposti di architettura basata su RNN superano il modello di base per le attività di traduzione dall'inglese al Myanmar, e i modelli di trasformatori multi-sorgente e multi-sorgente condivisi forniscono risultati di traduzione migliori rispetto alla base.", 'kk': 'Бұл қағазда, WAT-2021 жұмысының аудармаларына NECTEC командасы жасалған машинаны аудару үлгілерінің тәжірибелі нәтижелерін хабарладық. Біздің үлгілеріміз ағылшын-Мьянмар және Мьянмар-ағылшын тілінің екі жағындағы невралдық әдістеріне негізделген. Бар нейралық машинаны аудару (NMT) үлгілерінің көпшілігі негізінде келесі деректерді аударып, синтактикалық мәліметті тікелей қолданбады. Бірақ біз көп көзі невралдық компьютердің аударуын (NMT) үлгілерді көп тілді корпораларды, мәтін деректер корпус, ағаш деректер корпус, немесе POS- тегілеген деректер корпус сияқты қолданып,  Көп көзі аудармасы - бірнеше кіріс (мысалы, екі түрлі пішімде) аудармасының дұрыстығын өзгерту үшін арқылы. RNN-негіздеген кодер-декодер үлгісі біздің тәжірибемізге архитектуралар мен түрлендіруші архитектураларымен байланысты. Тәжірибелі нәтижелер RNN негізіндегі архитектураның негізгі үлгілері ағылшын тілінен Мьянма аудару тапсырмасының негізгі үлгісін және көп көзі және ортақ көзі түрлендіру үлгілері негізгі жолдан артық ау', 'ms': 'In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team for the translation tasks of WAT-2021.  Basically, our models are based on neural methods for both directions of English-Myanmar and Myanmar-English language pairs.  Kebanyakan model Terjemahan Mesin Neural (NMT) yang wujud terutamanya fokus pada penukaran data sekuensial dan tidak menggunakan maklumat sintaktik secara langsung. Namun, kita melaksanakan model terjemahan mesin saraf berbilang sumber (NMT) menggunakan korpra berbilang bahasa seperti korpus data rentetan, korpus data pokok, atau korpus data POS-tagged. Terjemahan-sumber berbilang adalah pendekatan untuk mengeksploitasi input berbilang (cth. dalam dua format berbeza) untuk meningkatkan ketepatan terjemahan. Model pengekod-dekoder berasaskan RNN dengan mekanisme perhatian dan arkitektur pengubah telah dilakukan untuk eksperimen kami. Hasil percubaan menunjukkan bahawa model yang diusulkan untuk arkitektur berasaskan RNN melebihi model asas untuk tugas terjemahan bahasa Inggeris ke Myanmar, dan model pengubah berbilang sumber dan berkongsi-berbilang sumber memberikan keputusan terjemahan yang lebih baik daripada asas.', 'lt': 'In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team for the translation tasks of WAT-2021.  Basically, our models are based on neural methods for both directions of English-Myanmar and Myanmar-English language pairs.  Daugumoje esamų neurologinių mašinų vertimo (NMT) modelių daugiausia dėmesio skiriama sekacinių duomenų keitimui ir tiesiogiai nenaudojama sintaksinės informacijos. Vis dėlto mes atliekame daugialypius nervinių mašinų vertimo (NMT) modelius naudojant daugiakalbį korprą, pvz., string ų duomenų korpusą, medžio duomenų korpusą arba POS pažymėtą duomenų korpusą. Daugiašaltinis vertimas yra metodas, pagal kurį naudojami keli įvediniai (pvz., dviem skirtingomis formomis), siekiant padidinti vertimo tikslumą. Mūsų eksperimentui buvo atliktas RNN pagrįstas kodavimo kodavimo modelis su dėmesio mechanizmu ir transformatorių architektūra. Eksperimentiniai rezultatai parodė, kad siūlomi RNN grindžiamos architektūros modeliai viršija pradinį anglų ir Mianmaro vertimo užduoties model į, o daugialypiai ir bendrai naudojami daugialypiai transformatoriai duoda geresnių vertimo rezultatų nei pradiniai.', 'mk': 'Во овој весник, ги известуваме експерименталните резултати на моделите за машинска транслекција спроведени од страна на тимот НЕКТЕК за преводните задачи на WAT-2021. Во основа, нашите модели се базирани на нервни методи за двете насоки на англиско-мианмарски и мианмарски-англиски парови. Повеќето од постоечките модели на неурална машина (НМТ) се фокусираат главно на конверзијата на секвенцијалните податоци и не употребуваат директно синтактички информации. Сепак, спроведуваме модели од мултиизворен нервен машински превод (НМТ) користејќи мултијазичен корпора како што се string data corpus, tree data corpus, или POS-означен податок корпора. Мултиизворниот превод е пристап за искористување на многуте внесувања (на пример во два различни формати) за зголемување на преводната точност. The RNN-based encoder-decoder model with attention mechanism and transformer architectures have been carried out for our experiment.  Експерименталните резултати покажаа дека предложените модели на архитектура базирана на РНН го надминуваат основниот модел за преведувачката задача од англиски до Мјанмар, а моделите од мултиизвори и споделени мултиизвори трансформатори даваат подобри резултати од основ', 'ml': 'ഈ പത്രത്തില്\u200d, നെക്സിസി ടീമില്\u200d നടത്തിയ മെഷീന്\u200d പരീക്ഷണത്തിന്റെ ഫലങ്ങള്\u200d ഞങ്ങള്\u200d റിപ്പോര്\u200dട്ട് ചെയ്യുന്നു. വാട്ട്-2021-ന്റെ വി അടിസ്ഥാനത്തില്\u200d, ഞങ്ങളുടെ മോഡലുകള്\u200d ഇംഗ്ലീഷ്-മിയാന്\u200dമാരിന്റെയും ഇംഗ്ലീഷ് ഇംഗ്ലീഷ് ഭാഷ രണ്ടുപേര്\u200dക്കും അടിസ നിലവിലുള്ള നെയുറല്\u200d മെഷീന്\u200d പരിഭാഷ (NMT) മോഡലുകളില്\u200d മിക്കവാറും പ്രധാനപ്പെട്ട വിവരങ്ങളുടെ മാറ്റം ശ്രദ്ധിക്കുകയും നേരിട്ട എങ്കിലും സ്ട്രിങ്ങ് ഡാറ്റാ കോര്\u200dപ്പുസ്, മരത്തിലെ ഡേറ്റാ കോര്\u200dപ്പുസ്, അല്ലെങ്കില്\u200d പോഎസ് ടാഗ്ഗേജ് ഡേറ്റാ കോര്\u200dപ്പുസ് എന്നിവയ്ക്ക് ഉപയോഗ @ info: whatsthis നമ്മുടെ പരീക്ഷണത്തിനുവേണ്ടി ശ്രദ്ധിക്കുന്നതും ശ്രദ്ധിക്കുന്നതും കൊണ്ട് RNN-അടിസ്ഥാനമായ കോഡെര്\u200d ഡെക്കോഡെര്\u200d മോഡ പരീക്ഷണ ഫലങ്ങള്\u200d കാണിച്ചുകൊണ്ടിരിക്കുന്നത് RNN-അടിസ്ഥാനമായ ആര്\u200dക്കെക്ട്രെക്ടറിന്റെ പ്രൊദ്ദേശിക്കപ്പെട്ട മോഡലുകള്\u200d ഇംഗ്ലീഷ്-മൈയാന്\u200dമാര്\u200d പരിഭാഷക്ക', 'mt': 'F’dan id-dokument, nirrappurtaw ir-riżultati sperimentali tal-mudelli tat-Traduzzjoni tal-Magni mwettqa minn tim tan-NECTEC għall-kompiti tat-traduzzjoni tal-WAT-2021. Bażikament, il-mudelli tagħna huma bbażati fuq metodi newrali għaż-żewġ direzzjonijiet ta’ pari lingwistiċi Ingliż-Mjanmar u Mjanmar-Ingliż. Il-biċċa l-kbira tal-mudelli eżistenti tat-Traduzzjoni tal-Magni Newrali (NMT) jiffokaw prinċipalment fuq il-konverżjoni ta’ dejta sekwenzjali u ma jużawx direttament informazzjoni sintattika. Madankollu, nagħmlu mudelli ta’ traduzzjoni ta’ magni newrali b’diversi sorsi (NMT) bl-użu tal-korpra multilingwi bħal string data corpus, tree data corpus, jew POS-tagged data corpus. The multi-source translation is an approach to exploit multiple inputs (e.g. in two different formats) to increase translation accuracy.  Għall-esperiment tagħna twettqu l-mudell tad-dekoder ibbażat fuq l-RNN b’mekkaniżmu ta’ attenzjoni u arkitetturi tat-trasformaturi. Ir-riżultati sperimentali wrew li l-mudelli proposti ta’ arkitettura bbażata fuq RNN jaqbżu l-mudell ta’ referenza għall-kompitu ta’ traduzzjoni Ingliż-Mjanmar, u l-mudelli ta’ trasformaturi b’diversi sorsi u kondiviżi b’diversi sorsi jagħtu riżultati aħjar ta’ traduzzjoni mil-linja bażi.', 'mn': 'Энэ цаасан дээр бид WAT-2021 оны хөрөнгө оруулалтын ажлын туршилтын үр дүнг НЕКТЕК багийн хийсэн машины хөрөнгө оруулалтын загварын туршилтын үр дүнг тайлбарлаж байна. Үнэндээ бидний загвар нь Англи-Мьянмар, Мьянмар-Англи хэл хоёр талын мэдрэлийн арга зам дээр суурилсан. Ихэнх мэдрэлийн мэдрэлийн хөгжүүлэлт (NMT) загварууд ихэнх нь дараагийн өгөгдлийн шилжүүлэлт дээр анхаарлаа хандуулдаг ба шууд синтактик мэдээллийг ашиглахгүй. Гэхдээ бид олон эх үүсвэрийн мэдрэлийн механикийн хөрөнгө оруулалт (NMT) загваруудыг стринг өгөгдлийн корпус, модны өгөгдлийн корпус, эсвэл POS-тэй өгөгдлийн корпус ашигладаг. Олон эх үүсвэрийн орчуулалт нь олон орнуудыг ашиглах арга юм. ДНХ-ын үндсэн коддогч загвар нь анхаарлын механизм болон шилжүүлэгч архитектурууд бидний туршилтад хийгдсэн. Ингээд туршилтын үр дүнд RNN-д суурилсан архитектурын загвар Англи-т Мьянма руу орчуулах ажлын суурилсан загварыг дамжуулдаг. Мөн олон эх үүсвэрийн, олон эх үүсвэрийн шилжүүлэгчийн загвар суурилсан шугамнаас илүү сайн орчуулагч үр дүн', 'pl': 'W niniejszym artykule przedstawiamy wyniki eksperymentalne modeli tłumaczenia maszynowego przeprowadzonych przez zespół NECTEC dla zadań tłumaczeniowych WAT-2021. Zasadniczo nasze modele opierają się na metodach neuronowych dla obu kierunków par językowych angielsko-myanmar oraz myanmar-angielski. Większość istniejących modeli neuronowego tłumaczenia maszynowego (NMT) koncentruje się głównie na konwersji danych sekwencyjnych i nie wykorzystuje bezpośrednio informacji składniowych. Jednak prowadzimy wielojęzyczne modele neuronowego tłumaczenia maszynowego (NMT) z wykorzystaniem wielojęzycznych korpusów, takich jak korpus danych ciągów, korpus danych drzewa czy korpus danych oznaczony POS. Tłumaczenie wielu źródeł to podejście do wykorzystania wielu wejść (np. w dwóch różnych formatach) w celu zwiększenia dokładności tłumaczenia. Do naszego eksperymentu przeprowadzono model kodera-dekodera oparty na RNN z mechanizmem uwagi i architekturą transformatorów. Wyniki eksperymentalne wykazały, że proponowane modele architektury opartej na RNN przewyższają model bazowy dla zadania tłumaczeniowego z języka angielskiego na Myanmar, a modele transformatorów wielu źródeł i współdzielonych wielu źródeł dają lepsze wyniki tłumaczenia niż modele bazowe.', 'no': 'I denne papiret rapporterer vi eksperimentelle resultatene av maskineoversettelsmodeller gjennomført av ein NECTEC-gruppe for oversettelsesoppgåva av WAT-2021. Grunnleggjande er våre modeller basert på neuralmetodar for både retningar av engelsk-Myanmar og Myanmar-engelsk språk par. Dei fleste av dei eksisterande neuralmaskineoversettelse (NMT) modelane fokuserer hovudsakelig på konverteringa av sekvensielle data og ikkje bruk direkte syntaksiske informasjon. Men vi gjer fleire kjelde neuralmaskinsomsetjingar (NMT) med fleirspråk korpora som strengdatakorpus, tredatakorpus eller POS-merket datakorpus. Fleikkjeldeomsetjinga er e in tilnærming til å eksplodera fleire inndata (f.eks. i to ulike formatar) for å økja omsetjingsakritet. Den RNN-baserte koderingsmodellen med oppmerksmekanismen og transformeringsarkitekturar er utført for eksperimentet vår. Eksperimentale resultatet viste at den foreslåde modellen av RNN-baserte arkitektur utfører baserte modellen for omsetjing av engelsk til Myanmar, og fleire kildekode og delte multikildekode-transformeringsmodelane gjer betre omsetjingsformat enn baseline.', 'ro': 'În această lucrare, raportăm rezultatele experimentale ale modelelor de traducere automată realizate de o echipă NECTEC pentru sarcinile de traducere ale WAT-2021. Practic, modelele noastre se bazează pe metode neurale pentru ambele direcții ale perechilor de limbi engleză-Myanmar și Myanmar-engleză. Majoritatea modelelor existente de traducere automată neurală (NMT) se concentrează în principal pe conversia datelor secvențiale și nu utilizează direct informații sintactice. Cu toate acestea, efectuăm modele de traducere automată neurală multi-sursă (NMT) utilizând corpore multilingve, cum ar fi corpul de date șiruri, corpul de date arbore sau corpul de date etichetat POS. Traducerea multi-sursă este o abordare pentru a exploata mai multe intrări (de exemplu, în două formate diferite) pentru a crește acuratețea traducerii. Pentru experimentul nostru au fost realizate modelul codificator-decodor bazat pe RNN cu mecanism de atenție și arhitecturi transformatoare. Rezultatele experimentale au arătat că modelele propuse de arhitectură bazată pe RNN depășesc modelul de referință pentru sarcina de traducere din limba engleză în Myanmar, iar modelele transformatoare multi-sursă și partajate-multi-sursă oferă rezultate de traducere mai bune decât cele de referință.', 'so': 'Qoraalkan waxaan wargelinaynaa arimaha imtixaanka ah ee modellada turjumidda Mashine ee lagu sameeyay koox NECTEC ee turjumaadda WAT-2021. Sida caadiga ah modelalkayagu waxay ku saleysan yihiin hababka neurada ee labada hagitaan afka Ingiriis-Myanmar iyo labada noocyo ee Ingiriis-Ingiriis. Inta badan qaababka tarjumaadda ee qoraalka Neural (NMT) waxay ugu horeyn focus on beddelinta macluumaadka soo socda islamarkaasna aan toos u isticmaalin macluumaadka syntactic. Si kastaba ha ahaatee, waxaynu sameynaa tarjumaadka maskaxda neurada badan (NMT) modellada isticmaalka shirkadda luuqadaha kala duduwan sida string data corpus, tree data corpus, ama POS-tagged data corpus. Turjumista badan waa qaab ay ku isticmaalaan input badan (tusaale ahaan laba noocyo oo kala duduwan) si uu u kordhiyo saxda turjumaadda. Tusaalada codcoder-ku saleysan RNN oo lagu haysto qalcadaha digtoonaanta iyo beddelinta, waxaa loo sameeyay baaritaankeeni. Midhaha imtixaanka ayaa tusay in modellada la soo jeeday dhismaha ku saleysan RNN waxay muujiyaan modelka hoose-hoose ee u qoran maamulka turjumista Ingiriis-to-Myanmar, iyo modellada badan iyo qayb-badan-sourceed-transformer-sources-badan waxay soo bixisaa resulto ka fiican turjumista hoosta.', 'sr': 'U ovom papiru izvještavamo eksperimentalne rezultate modela prevoda mašine koje je vodio tim NECTEC za prevod zadataka WAT-2021. U osnovi, naši modeli su zasnovani na neuralnim metodama za obje upute engleskog-Myanmara i Myanmar-engleskog parova. Većina postojećih modela neurološkog prevoda (NMT) uglavnom se fokusirala na konvertaciju sekvencijalnih podataka i ne koriste direktno sintaktične informacije. Međutim, mi vodimo modele multiizvornog neuralnog prevoda (NMT) sa multijezičkim korporama kao što su korpus podataka, korpus podataka drveta ili korpus podataka sa POS-om. Višeizvorski prevod je pristup iskoristiti višestruke ulaznice (npr. u dva različita formata) kako bi povećala preciznost prevoda. Model kodera na RNN-u sa mehanizam pažnje i arhitekturom transformera proveden je za naš eksperiment. Eksperimentalni rezultati pokazali su da predloženi modeli arhitekture bazirane na RNN iznosi početni model za prevodni zadatak engleskog na Myanmar, i multiizvorski i zajednički multiizvorski transformatorski modeli donose bolji rezultat prevoda od početne linije.', 'ta': 'இந்த காகிதத்தில், நாம் ஒரு NECTEC குழு மொழிபெயர்ப்பு பணிகளுக்கு செயல்படுத்தப்பட்ட இயந்திர மொழிமாற்றும் மாதிரிகளின் சோதனையின அடிப்படையில், எங்கள் மாதிரிகள் ஆங்கில- மையான்மார் மற்றும் மையான்மார்- ஆங்கில மொழி ஜோடிகளுக்கு புதிய முறைகளை  இருக்கும் பெரும்பாலான நெருக்கி இயந்திரம் மொழிபெயர்ப்பு (NMT) மாதிரிகள் முக்கியமாக பின்வரும் தகவல் மாற்றுதலை மாற்றி கவனம் செல ஆனால், நாம் பல மூலங்களின் மொழிமாற்ற மாதிரிமாற்றம் (NMT) மாதிரிகளை செயல்படுத்துகிறோம் சரம் தரவு கோர்ப்ஸ், மரத்து தரவு குறிப்பு, அல்லது POS- ஒட்ட பல- மூலம் மொழிபெயர்ப்பு மொழிபெயர்ப்பு பல உள்ளீடுகளை பயன்படுத்த ஒரு வழியாகும் (உ. ம். இரண்டு வேறு வடிவங்களில்) மொழிபெயர்ப RNN-அடிப்படையிலான குறியீட்டாளர் மாதிரி கவனம் முறைமையுடன் மற்றும் மாற்றி அமைப்புகள் எங்கள் சோதனைக்கு செய்யப்பட்டது. பரிசோதனை முடிவு', 'si': 'මේ පත්තරේ අපි පරීක්ෂණ ප්\u200dරතිචාරයක් වාර්තා කරනවා මැෂින් වාර්තාවන් මොඩේල්ස් එකේ පරීක්ෂණ ප්\u200dරතිචාරයක් වි මූලිකයෙන්, අපේ මොඩේල් ඉන්ග්\u200dරීසිය-මයින්මාර් සහ මයින්මාර්-ඉංග්\u200dරීසිය භාෂාවක් දෙන්නම් ප්\u200dරතිකා අවස්ථාවත් න්\u200dයූරාල් මැෂින් වාර්ථාව (NMT) මොඩල් වලින් ප්\u200dරධානයෙන් ප්\u200dරමාණය පරිවර්තනයේ තොරතුරු වාර්ථා නමුත්, අපි ගොඩක් ප්\u200dරභාවිත න්\u200dයූරල් යන්ත්\u200dරය භාවිතය (NMT) මොඩල් කරනවා වගේම ස්ට්\u200dරින්ට් දත්ත කොර්පස්, ගස් දත්ත කොර්පස්, නැත් ගොඩක් ප්\u200dරමාණ පරිවර්තනය තමයි වෙනස් පරිවර්තනයක් විශේෂ කරන්න (වගේම වෙනස් ප්\u200dරමාණ දෙකක් වලින්) භාවිතාව RNN-ආධාරිත සංකේතකය-ඩිකෝඩර් මොඩල් අපේ පරීක්ෂණය සඳහා අවධාන පද්ධතිය සහ වෙනස් කරන්න විදිහට පරීක්ෂණය පරීක්ෂණ ප්\u200dරතිචාර ප්\u200dරතිචාර විදිහට පෙන්වන්නේ RNN-ආධාරිත ස්ථාපනයේ ප්\u200dරතිචාරිත විදිහට ඉංග්\u200dරීසි වලින් මයින්මාර් වාර්ථාපනය වැඩ', 'sv': 'I denna uppsats redovisar vi de experimentella resultaten av maskinﾃｶversﾃ､ttningsmodeller som genomfﾃｶrts av ett NECTEC-team fﾃｶr ﾃｶversﾃ､ttningsuppgifterna fﾃｶr WAT-2021. I grund och botten ﾃ､r vﾃ･ra modeller baserade pﾃ･ neurala metoder fﾃｶr bﾃ･da riktningarna av engelsk-Myanmar och Myanmar-engelska sprﾃ･kpar. De flesta av de befintliga NMT-modellerna fokuserar frﾃ､mst pﾃ･ konvertering av sekventiella data och anvﾃ､nder inte direkt syntaktisk information. Vi utfﾃｶr dock NMT-modeller (neural machine translation) med hjﾃ､lp av flersprﾃ･kiga korpuser som strﾃ､ngdatakorpuser, trﾃ､ddatakorpuser eller POS-mﾃ､rkta datakorpuser. ﾃ没ersﾃ､ttningen med flera kﾃ､llor ﾃ､r ett tillvﾃ､gagﾃ･ngssﾃ､tt fﾃｶr att utnyttja flera inmatningar (t.ex. i tvﾃ･ olika format) fﾃｶr att ﾃｶka ﾃｶversﾃ､ttningens noggrannhet. Den RNN-baserade encoder-dekoder modellen med uppmﾃ､rksamhetsmekanism och transformatorarkitekturer har utfﾃｶrts fﾃｶr vﾃ･rt experiment. De experimentella resultaten visade att de fﾃｶreslagna modellerna av RNN-baserad arkitektur ﾃｶvertrﾃ､ffar baslinjen fﾃｶr engelsk-till-Myanmar ﾃｶversﾃ､ttningsuppgift, och multi-source och delade-multi-source transformatormodeller ger bﾃ､ttre ﾃｶversﾃ､ttningsresultat ﾃ､n baslinjen.', 'ur': 'اس کاغذ میں ہم نے WAT-2021 کے ترجمہ کاموں کے لئے ایک NECTEC تیم کے ذریعے مشین ترجمہ موڈل کے آزمائش نتیجے کی گزارش دی۔ بنیادی طور پر، ہمارے مدل انگلیسی-میانمار اور میانمار-انگلیسی زبان جوڑوں کے دونوں طریقوں پر بنیاد ہیں. اکثر موجود نیورال ماشین ترجمہ (NMT) موڈل میں سے اکثر سیکوئنسیل ڈاٹے کی تبدیل پر تمرکز کریں اور سینکٹیک معلومات کو مستقیماً استعمال نہ کریں۔ لیکن ہم multi-source neural machine translation (NMT) موڈلوں کو multilingual corpora کے استعمال کرتے ہیں جیسے string data corpus, tree data corpus, یا POS-tagged data corpus. Multisource ترجمہ ایک طریقہ ہے کہ تعداد کی دقیق بڑھنے کے لئے بہت سے اینپوٹ (جیسے دو مختلف فرموٹوں میں) استعمال کرے۔ RNN-based encoder-decoder model with attention mechanism and transformer architectures are carried out for our experiment. آزمائش نتیجے دکھائے گئے کہ RNN بنیادی معماری موڈل انگلیسی سے Myanmar ترجمہ کا بنیاسی لین موڈل سے زیادہ اچھا ترجمہ کرتا ہے اور بہت سی سورج اور shared-multi-source ترجمہ موڈل بنیاسی لین سے زیادہ اچھا ترجمہ نتیجہ حاصل کرتا ہے.', 'vi': 'Trong tờ giấy này, chúng tôi báo cáo kết quả thử nghiệm của mô- đun cấu trúc máy được thực hiện bởi một nhóm NETEC cho các nhiệm vụ dịch chuyển của WAT-2021. Về cơ bản, các mẫu của chúng tôi dựa trên phương pháp thần kinh cho cả hai hướng của cặp ngôn ngữ Anh-Myanmar và Myanmar-Anh. Hầu hết các mô- đun cấu trúc thần kinh (NMB) tồn tại chủ yếu tập trung vào việc chuyển đổi dữ liệu liên tục và không trực tiếp sử dụng thông tin cú pháp. Tuy nhiên, chúng tôi thực hiện các mô hình dịch thiết bị thần kinh đa nguồn (NMB) sử dụng cơ thể đa dạng như tập đoàn dữ liệu dây, tập đoàn dữ liệu cây, hay tập đoàn dữ liệu có gắn thẻ. Dịch đa nguồn là một phương pháp khai thác nhiều nội dung (v.d. hai dạng khác nhau) để tăng độ chính xác dịch. Mô hình mã hóa dựa trên RNN với cơ cấu trúc chú ý và cấu trúc chuyển hóa được thực hiện cho thí nghiệm của chúng tôi. Các kết quả thử nghiệm cho thấy các mô hình kiến trúc dựa trên RNN vượt trội so với cơ sở cơ bản dịch dịch của Anh-Myanmar, và các mô hình biến đổi đa nguồn và nhiều nguồn chia sẻ tạo ra kết quả dịch tốt hơn so với cơ sở cơ bản.', 'uz': "In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team for the translation tasks of WAT-2021.  Asosida, bizning modellarimiz ingliz- Myanmar va Myanmar- Ingliz tili tili ikkita yordamlarga neyron usullar asosida. Name Lekin, biz bir necha-manba neyural tarjima modellari (NMT) bilan bir necha tilda streng data corpus, daraxt maʼlumot corpus yoki POS yorlangan maʼlumot korpus kabi bir necha tilda ishlatamiz. @ info: whatsthis Name Tasavvur natijalari esa RNN asosida yaratilgan arxituvchi modellari ingliz- to-Myanmar tarjima vazifasi asosiy modelini bajaradi, va ko'plab- manba va bir nechta ko'plab- ko'plab- ko'plab- manba transformer modellari asosiy tarjima natijalaridan yaxshi tarjima natijalarini bajaradi.", 'bg': 'В настоящата статия са отчетени експерименталните резултати на моделите за машинен превод, проведени от екип на НЕКТЕК за преводачески задачи на ВАТ-2021. По принцип нашите модели са базирани на невронни методи за двете посоки на английски-Мианмар и Мианмар-английски езикови двойки. Повечето от съществуващите модели на невронен машинен превод (НМТ) се фокусират главно върху преобразуването на последователни данни и не използват директно синтактична информация. Въпреки това, ние провеждаме модели на невронен машинен превод с множество източници, използвайки многоезични корпуси като струнен корпус от данни, дървесен корпус от данни или маркиран с ПОС корпус от данни. Преводът с множество източници е подход за използване на множество входове (например в два различни формата), за да се увеличи точността на превода. За нашия експеримент е направен моделът на кодер-декодер с механизъм за внимание и трансформаторна архитектура. Експерименталните резултати показаха, че предложените модели на архитектура, базирана на RNN, превъзхождат базовия модел за задачата за превод от английски към Мианмар, а моделите на трансформатори с множество източници и споделени източници дават по-добри резултати от базовия.', 'da': 'I denne artikel rapporterer vi eksperimentelle resultater af maskinoversættelsesmodeller udført af et NECTEC-team til oversættelsesopgaverne i WAT-2021. Grundlæggende er vores modeller baseret på neurale metoder for begge retninger af engelsk-Myanmar og Myanmar-engelsk sprogpar. De fleste af de eksisterende NMT-modeller fokuserer hovedsageligt på konvertering af sekventielle data og bruger ikke direkte syntaktiske oplysninger. Vi udfører imidlertid NMT-modeller (neural machine translation) med flere kilder ved hjælp af flersprogede korpuser såsom strengdatakorpus, trædatakorpus eller POS-mærket datakorpus. Oversættelsen med flere kilder er en tilgang til at udnytte flere input (f.eks. i to forskellige formater) for at øge oversættelsesnøjagtigheden. Den RNN-baserede encoder-dekoder model med opmærksomhedsmekanisme og transformer arkitekturer er blevet udført til vores eksperiment. De eksperimentelle resultater viste, at de foreslåede modeller af RNN-baseret arkitektur overstiger baseline modellen for engelsk-til-Myanmar oversættelsesopgave, og multi-source og delt-multi-source transformer modeller giver bedre oversættelsesresultater end baseline.', 'nl': "In dit artikel rapporteren we de experimentele resultaten van Machine Translation modellen uitgevoerd door een NECTEC team voor de vertaaltaken van WAT-2021. In principe zijn onze modellen gebaseerd op neurale methoden voor beide richtingen van Engels-Myanmar en Myanmar-Engels taalparen. De meeste bestaande NMT-modellen (Neural Machine Translation) richten zich voornamelijk op het omzetten van sequentiële gegevens en maken niet direct gebruik van syntactische informatie. We voeren echter multi-source neural machine translation (NMT) modellen uit met behulp van meertalige corpora's, zoals string data corpus, tree data corpus of POS-tagged data corpus. De multi-source vertaling is een aanpak om meerdere inputs (bijvoorbeeld in twee verschillende formaten) te exploiteren om de vertaalnauwkeurigheid te verhogen. Het RNN-gebaseerde encoder-decoder model met aandachtsmechanisme en transformatorarchitecturen is uitgevoerd voor ons experiment. De experimentele resultaten toonden aan dat de voorgestelde modellen van RNN-gebaseerde architectuur beter presteren dan het basismodel voor Engels-naar-Myanmar vertaaltaak, en de multi-source en shared-multi-source transformatormodellen betere vertaalresultaten opleveren dan de baseline.", 'hr': 'U ovom papiru izvještavamo eksperimentalne rezultate modela prevoda stroja koje je vodio tim NECTEC za prevod zadataka WAT-2021. U osnovi, naši modeli su temeljeni na neuralnim metodama za obje smjere paira engleskog-Myanmara i Myanmar-engleskog jezika. Većina postojećih modela neurološkog prevoda (NMT) uglavnom se fokusirala na konvertaciju sekvencijskih podataka i ne koriste direktno sintaktične informacije. Međutim, mi vodimo modele multiizvornog prevoda neuralnih strojeva (NMT) koristeći multijezičku korporaciju poput korpusa podataka podataka, korpusa podataka drveta ili korpusa podataka s POS-om. Višeizvorski prevod je pristup iskoristiti višestruke ulaznice (npr. u dvije različite formate) kako bi povećala preciznost prevoda. Model kodera na RNN-u s mehanizam pažnje i arhitekturom transformera proveden je za naš eksperiment. Eksperimentalni rezultati pokazali su da predloženi modeli arhitekture na temelju RNN-a iznosi početni model prijevoznog zadatka za prevod engleskog na Myanmar, te modeli multiizvora i dijeljenih multiizvorskih transformera donose bolji rezultat prevoda nego početni.', 'ko': '본고에서 우리는 NECTEC팀이 WAT-2021 번역 임무에 대해 진행한 기계 번역 모델의 실험 결과를 보고하였다.기본적으로 우리의 모델은 영어-미얀마어와 영어-미얀마어의 쌍방향 신경 방법을 바탕으로 한다.기존의 대다수 신경기계번역(NMT) 모델은 주로 서열 데이터의 전환에 주목하고 문법 정보를 직접 사용하지 않는다.그러나 우리는 다국어 자료 라이브러리(예를 들어 문자열 데이터 자료 라이브러리, 트리 데이터 자료 라이브러리 또는 POS 표기 데이터 자료 라이브러리)를 사용하여 다원신경기계번역(NMT) 모델을 진행한다.다원적 번역은 여러 가지 입력(예를 들어 두 가지 다른 격식)을 이용하여 번역의 정확성을 높이는 방법이다.RNN 기반의 인코더 - 디코더 모형은 주의 메커니즘과 변환기 구조를 가지고 있어 이미 우리의 실험에서 실현되었다.실험 결과에 따르면 제시된 RNN 기반의 구조 모델은 영국 갈기 번역 임무에서 기선 모델보다 우수하고 다원과 공유 다원 변환기 모델의 번역 효과가 기선 모델보다 우수하다는 것을 알 수 있다.', 'sw': 'Katika karatasi hii, tunaripoti matokeo ya michoro ya Tafsiri ya Mashine yaliyofanywa na timu ya NECTEC kwa ajili ya kazi za tafsiri za WAT-2021. Kimsingi, miundo yetu inategemea mbinu za neura kwa mbinu zote za lugha za Kiingereza-Myanmar na Kiingereza. Wengi wa mifano ya Tafsiri ya Mashine ya Neural (NMT) yanayopo sasa yanajikita zaidi kwenye mabadiliko ya data zinazofuata na wala hawatumii moja kwa moja taarifa za ushirikiano. Hata hivyo, tunafanya utafsiri wa mashine ya taratibu mbalimbali (NMT) kwa kutumia makampuni ya lugha mbalimbali kama vile viungo vya taarifa za mfumo, makampuni ya takwimu za miti, au makampuni ya taarifa za POS. Tafsiri ya vyanzo vingi ni njia ya kutumia vifaa vingi (kwa mfano katika namna mbili tofauti) kuongeza usahihi wa tafsiri. Mfano wa kodi ya RNN anayeishi RNN wenye miundo mbinu na mabadiliko yametokea kwa ajili ya majaribio yetu. Matokeo ya majaribio yalionyesha kuwa mifano ya ujenzi wa msingi wa RNN yaliyopendekezwa yanatoa mifano ya tafsiri ya Kiingereza-hadi Myanmar, na mifano ya mabadiliko ya vyanzo vingi na vyanzo vinavyoshirikishwa vinatoa matokeo bora ya tafsiri kuliko mitandao ya msingi.', 'fa': 'در این کاغذ، ما نتایج آزمایش مدل ترجمه ماشین را گزارش می دهیم که توسط یک تیم NECTEC برای کار ترجمه WAT-2021 انجام شده است. در اصل، مدل\u200cهای ما بر روش\u200cهای عصبی برای جفت\u200cهای زبان انگلیسی-میانمار و میانمار-انگلیسی بنیاد دارند. بیشتر از مدل\u200cهای ترجمه\u200cهای ماشین عصبی موجود (NMT) اکثر روی تغییر داده\u200cهای بعدی تمرکز می\u200cکند و مستقیماً از اطلاعات سنتاکتیک استفاده نمی\u200cکنند. با این حال، ما مدل\u200cهای ترجمه\u200cهای ماشین\u200cهای عصبی چندین منبع (NMT) را با استفاده از شرکت\u200cهای زیادی زبان\u200cها انجام می\u200cدهیم، مانند کورپوس داده\u200cهای سیستم\u200cها، کورپوس داده\u200cهای درخت، یا کورپوس داده\u200cهای POS-tagged. ترجمه\u200cهای متعدد منبع یک روش برای استفاده از ورودهای متعدد (مثلا در دو فرم متفاوت) برای افزایش دقیق ترجمه است. مدل رمز\u200cدهنده\u200cی رمز\u200cدهنده\u200cهای RNN با مکانیسم توجه و معماری تغییردهنده\u200cها برای آزمایش ما انجام شده است. نتیجه آزمایشی نشان داد که مدل پیشنهاد معماری بنیاد RNN از مدل اصلی برای ترجمه\u200cهای انگلیسی به میانمار و مدل\u200cهای ترجمه\u200cکننده\u200cهای منبع\u200cهای زیادی و منبع\u200cهای زیادی از منبع\u200cها نتیجه\u200cهای ترجمه بهتر از خط اصلی به وجود می\u200cآورند.', 'de': 'In diesem Beitrag berichten wir über die experimentellen Ergebnisse von Machine Translation Modellen, die von einem NECTEC Team für die Übersetzungsaufgaben von WAT-2021 durchgeführt wurden. Grundsätzlich basieren unsere Modelle auf neuronalen Methoden für beide Richtungen von Englisch-Myanmar und Myanmar-Englisch Sprachpaaren. Die meisten existierenden NMT-Modelle konzentrieren sich hauptsächlich auf die Konvertierung sequenzieller Daten und verwenden keine syntaktischen Informationen direkt. Wir führen jedoch Multi-Source Neuronal Machine Translation (NMT)-Modelle durch, die mehrsprachige Korpora wie String-Datenkorpus, Baumdatenkorpus oder POS-tagged Datenkorpus verwenden. Die Multi-Source-Übersetzung ist ein Ansatz, um mehrere Eingaben (z.B. in zwei verschiedenen Formaten) zu nutzen, um die Übersetzungsgenauigkeit zu erhöhen. Für unser Experiment wurde das RNN-basierte Encoder-Decoder-Modell mit Aufmerksamkeitsmechanismus und Transformatorarchitekturen durchgeführt. Die experimentellen Ergebnisse zeigten, dass die vorgeschlagenen Modelle der RNN-basierten Architektur das Basismodell für Übersetzungsaufgaben von Englisch nach Myanmar übertreffen und die Transformatormodelle aus mehreren Quellen und geteilten Quellen bessere Übersetzungsergebnisse liefern als die Baseline.', 'id': 'Dalam kertas ini, kami melaporkan hasil percobaan dari model Translation Mesin yang dilakukan oleh tim NECTEC untuk tugas terjemahan WAT-2021. Pada dasarnya, model kita berdasarkan metode saraf untuk kedua arah sepasang bahasa Inggris-Myanmar dan bahasa Inggris-Myanmar. Kebanyakan model Translation Machine Neural (NMT) yang ada terutama fokus pada konversi data sekwensial dan tidak menggunakan informasi sintaksi secara langsung. Namun, kita melaksanakan model multi-sumber mesin saraf terjemahan (NMT) menggunakan korpra multibahasa seperti string data corpus, tree data corpus, atau POS-tagged data corpus. The multi-source translation is an approach to exploit multiple inputs (e.g. in two different formats) to increase translation accuracy.  Model koder-dekoder berasaskan RNN dengan mekanisme perhatian dan arsitektur transformer telah dilakukan untuk eksperimen kami. Hasil percobaan menunjukkan bahwa model yang diusulkan arsitektur berdasarkan RNN melebihi model dasar untuk tugas terjemahan bahasa Inggris-Myanmar, dan model transformer multisumber dan berbagi-berbagi-sumber memberikan hasil terjemahan yang lebih baik dari dasar.', 'tr': 'Bu kagyzda, biz maşynyň terjime nusgalarynyň netijelerini NECTEC toparynyň WAT-2021-iň terjime görenleri üçin berilýäris. Adatça, biziň modellerimiz iňlisçe-Myanmar we Myanmar-iňlisçe dil çiftleri üçin nural yönlere daýanýar. Öň bar näural maşynyň köpüsi (NMT) nusgalarynyň esasy ýagdaýy maglumatyň üýtgetmegine üns berýär we sintaktik maglumatyny direkt ullamaýarlar. Ýöne biz birnäçe çeşmeli näyral maşynyň terjimesini (NMT) köp dilli korpora ýaly string data korpus, agaç maglumaty korpus ýa-da POS-etilgeli maglumaty korpus ullanýarys. Birnäçe-çe şme terjime bolmak üçin köp gabdaly girişden ullanmak üçin bir goldadyr (meselâ, iki dürli biçimde) terjime edilen dogrylygyny artmak üçin. RNN tabanly kodçy nusgasy biziň deneyimiz üçin edildi. Deneymeli netijeler RNN tabanly arhitekturyň nusgalarynyň inglis-we-Myanmar terjime täbliginiň basit nusgasyny çykarýandygyny görkezilýär we multi-çeşme we paylaşyk-multi-çeşme terjime täbliginiň basit hatdan gowy terjime netijesini çykarýandygyny görkezilýär', 'af': "In hierdie papier, ons rapporteer die eksperimentale resultate van masjien vertaling modele wat deur 'n NECTEC-span gedoen is vir die vertaling opdragte van WAT-2021. Basies is ons modele gebaseer op neurale metodes vir beide rigtings van Engels-Myanmar en Myanmar-Engels taal pare. Die meeste van die bestaande Neurale Masjien Vertaling (NMT) modele fokus hoofsaaklik op die omskakeling van sekwensiele data en doen nie direk sintaksiese inligting gebruik nie. Maar ons doen multibronne neurale masjien vertaling (NMT) modele wat gebruik die multilinglike korpora soos string data corpus, boom data corpus of POS- tagged data corpus. Die veelvuldige bronne vertaling is 'n toegang om veelvuldige inputs te gebruik (bv. in twee verskillende formaat) om vertaling presies te vermeerder. Die RNN-gebaseerde enkoder-dekoder model met aandag mekanisme en transformeerder arkitektuur is uitgevoer vir ons eksperiment. Die eksperimentale resultate het vertoon dat die voorgestelde modele van RNN-gebaseerde arkitektuur die basisline model vir Engels-na-Myanmar vertaling taak uitvoer, en die multibron en gedeelde-multibron-transformeermodele beter vertaling resultate gee as die basisline.", 'sq': 'Në këtë letër, raportojmë rezultatet eksperimentale të modeleve të përkthimit të makinave kryer nga një ekip NECTEC për detyrat e përkthimit të WAT-2021. Në thelb, modelet tona janë bazuar në metodat nervore për të dy drejtimet e çifteve të gjuhës angleze-mianmare dhe të gjuhës angleze-mianmare. Most of the existing Neural Machine Translation (NMT) models mainly focus on the conversion of sequential data and do not directly use syntactic information.  Megjithatë, ne kryejmë modele me shumë burime të përkthimit të makinave nervore (NMT) duke përdorur korpra shumëgjuhëse të tilla si string data corpus, tree data corpus, or POS-tagged data corpus. Përkthimi me shumë burime është një qasje për të shfrytëzuar inputs të shumta (për shembull në dy formate të ndryshme) për të rritur saktësinë e përkthimit. Modeli kodues-dekoder bazuar në RNN me mekanizëm vëmendje dhe arkitektura transformues janë kryer për eksperimentin tonë. Rezultatet eksperimentale treguan se modelet e propozuara të arkitekturës me bazë në RNN-të kalojnë modelin bazë për detyrën e përkthimit anglez-në-Mianmar dhe modelet e transformuesve me shumë burime dhe me shumë burime të përbashkëta japin rezultate më të mira përkthimi se baza.', 'hy': 'Այս թղթի մեջ մենք զեկուցում ենք մեքենայի թարգմանման մոդելների փորձարկման արդյունքները, որոնք կատարվում են ՆԵԿԹԵԿ թիմի կողմից, և որոնք կատարվում են ՎԱԹ-2021 թարգմանման խնդիրների համար: Հիմնականում մեր մոդելները հիմնված են անգլերեն-միանմար և միանմար-անգլերեն զույգերի երկու ուղղությունների նեյրոնային մեթոդների վրա: Գոյություն ունի նյարդային մեքենայի թարգմանման (NMT) մոդելների մեծամասնությունը հիմնականում կենտրոնանում է հաջորդականության տվյալների փոխակերպման վրա և անմիջապես չի օգտագործում սինտակտիկ տեղեկատվություն: Այնուամենայնիվ, մենք կատարում ենք բազմա աղբյուրների նյարդային մեքենայի թարգմանման (NMT) մոդելներ՝ օգտագործելով բազլեզու կորպորա, ինչպիսիք են լարերի տվյալների կորպոսը, ծառերի տվյալների կորպոսը կամ POS-ի Բազմաաղբյուրների թարգմանությունը բազմաթիվ ներմուծների (օրինակ երկու տարբեր ձևերով) օգտագործման մոտեցում է թարգմանության ճշգրտությունը բարձրացնելու համար: Մեր փորձի համար կատարվել է ՌՆՆ-ի հիմնված կոդերի-կոդերի մոդելը, որը ունի ուշադրության մեխանիզմ և վերափոխող ճարտարապետություն: Փորձարկվող արդյունքները ցույց տվեցին, որ ՌՆՆ-ով հիմնված ճարտարապետության առաջարկած մոդելները գերազանցում են Անգլերեն-Միանմարի թարգմանման առաջադրանքի հիմնական մոդելը, և բազմաաղբյուրների և բազմաաղբյուրների փոխակերպման մոդելները ավելի լավ են', 'az': "Bu kağızda, WAT-2021'in çeviriş işləri üçün NECTEC ekibi təyin etdiyi maşın çeviri modellərin eksperimentli sonuçlarını bildiririk. Əslində, modellərimiz İngilizce-Myanmar və Mjanma-İngilizce dil çiftləri üçün nöral metodlarına dayanılır. Mevcut nöral mašin tərcümünün çoxu modellərin çox səviyyətli məlumatların dönüşünü və sintaktik məlumatların do ğrudan istifadə etməyin. Lakin biz çoxlu mənbə nöral maşına çevirməsi (NMT) modellərini çoxlu dil korporasını istifadə edirik: string data corpus, tree data corpus, or POS-tagged data corpus kimi. Çoxlu mənbə çeviri çoxlu inputləri istifadə etmək üçün bir tərzidir. RNN tabanlı kodlayıcı modeli gözləmə mehanizmi və transformer arhitektarı bizim eksperimentimiz üçün təşkil edildi. Müxtəlif sonuçlar, RNN-tabanlı arhitektura layihəsi modellərin İngilizə-Myanmara çevirilmiş işlərin baseline modelini və çoxlu-kaynak və paylaşılmış-çoxlu-kaynak transformer modellərin baseline nəticəsindən daha yaxşı çevirilməsi nəticəsini verir.", 'bn': 'এই কাগজটিতে আমরা একটি নেস্টেসিসি দলের অনুবাদের কাজের জন্য মেশিন অনুবাদ মডেলের পরীক্ষার ফলাফল প্রতিবেদন প্রদান করি। মৌলিক ভাবে আমাদের মডেল ইংরেজি-মায়ানমার এবং মায়ানমার-ইংরেজি ভাষার দুই দিকের দিকে নির্যাতন পদ্ধতির উপর ভিত্তি  বিদ্যমান নিউরাল মেশিন অনুবাদ (এনএমটি) মডেলের বেশীরভাগ মনোযোগ প্রদান করে যাচ্ছে পরবর্তী তথ্য পরিবর্তনের উপর এবং সরাসরি সিন্ট্যাকটিক তথ However, we conduct multi-source neural machine translation (NMT) models using the multilingual corpora such as string data corpus, tree data corpus, or POS-tagged data corpus.  অনুবাদের সঠিকভাবে বৃদ্ধি করার জন্য অনেক ইনপুট ব্যবহারের (উদাহরণস্বরূপ দুই বিভিন্ন ফর্মের মধ্যে) অনুবাদ বৃদ্ধি করা এনএন-ভিত্তিক এনকোডার-ডেকোডার মডেল আমাদের পরীক্ষার জন্য মনোযোগ প্রদান করা হয়েছে। পরীক্ষার ফলাফল দেখা যাচ্ছে যে প্রস্তাবিত আরএনএন-ভিত্তিক আর্কিটেক্টারের মডেল ইংরেজি থেকে মায়ানমারের অনুবাদ কাজের জন্য বেসেলাইন মডেল প্রদর্শন করে, আর বহুসূত্র এবং শ', 'am': 'በዚህ ካላት፣ የመኪን ትርጓሜ-2021 ለመትርጉም ትርጓሜዎች የናቴቴክሲ ቡድን የተደረገውን የመኪን ትርጉም ሞዴላዎችን እናስታውሳለን፡፡ በመጀመሪያ፣ ሞዴላዎቻችን እንግሊዝኛ-ሚያማር እና የሜያማር-እንግሊዘኛ ቋንቋ-ቋንቋ ሁለቱን መንገዶች በመሠረት ነው፡፡ የአሁኑ የነዌብ መኪን ትርጉም (NMT) ሞዴላዎች በተለየ በኋላ የዳታ መክፈቻን በመጠቀም እና በአቅራቢያ መረጃዎችን በመጠቀም አይጠቅሙም። ምንም እንኳን፣ የብዙsource የናውሬል መኪና ማተርጓም (NMT) ሞዴላዎችን እንደርታተር ዳታ ካርፓስ፣ ዛፍ ዳታ ካርፓስ ወይም የPOS-tagged ዳታ ኮፖርፓስ በመጠቀም እናደርጋለን፡፡ የብዙ-source ትርጉም ተርጓሚውን ትርጉም እርግጠኛ እንዲያበዛ የብዙ ጥያቄዎችን ለመጠቀም ማቀናጃ ነው። የRNN-መሠረት የሆኑት የኮድዶር-አካዳር ሞዴል ለፈተናችን ነው፡፡ የፈተናው ውጤቶች የRNN-based መሠረተ ሥርዓት ሞላዎችን የኢንጂልኛ-ወደ Myanmar ትርጉም አድራሻ መደበቂያ ሞዴል እና ብዙ-source እና የተለየ-multi-source transformer models ከbaseline ይልቅ የበለጠ ትርጉም ፍሬዎችን ይሰጣል፡፡', 'ca': "En aquest paper, informem dels resultats experimentals dels models de traducció màquina conduïts per un equip NECTEC per les tasques de traducció del WAT-2021. Bàsicament, els nostres models estan basats en mètodes neurals per ambdues direccions dels parells anglo-mianmar i anglo-mianmar. La majoria dels models de traducció neural de màquines (NMT) es centren principalment en la conversió de dades seqüencials i no utilitzen directament informació sinàctica. Tot i així, fem models de traducció neural de múltiples fonts (NMT) utilitzant el corpore multillenguatge com el corpus de cadenes de dades, el corpus d'arbres de dades o el corpus de dades etiquetat POS. La traducció de múltiples fonts és un enfocament per explotar múltiples entrades (per exemple en dos formats diferents) per augmentar la precisió de la traducció. El model de codificador-decodificador basat en RNN amb mecanisme d'atenció i arquitectures transformadores s'ha fet per al nostre experiment. Els resultats experimentals van demostrar que els models proposats d'arquitectura basada en RNN superen el model de base de la tasca de traducció anglès-a-mianmar, i que els models de transformadors multifonts i compartits multifonts donen millors resultats de traducció que el basal.", 'cs': 'V tomto článku jsou uvedeny experimentální výsledky modelů strojového překladu provedených týmem NECTEC pro překladatelské úkoly WAT-2021. V zásadě jsou naše modely založeny na neuronových metodách pro obě směry jazykových párů anglicko-myanmar a myanmar-anglicky. Většina stávajících NMT modelů se zaměřuje především na konverzi sekvenčních dat a nepoužívá přímo syntaktické informace. Nicméně provádíme multizdrojové modely neuronového strojového překladu (NMT) s využitím vícejazyčných korpusů, jako jsou například datový korpus řetězců, stromový datový korpus nebo datový korpus označený POS. Překlad z více zdrojů je přístup k využití více vstupů (např. ve dvou různých formátech) pro zvýšení přesnosti překladu. Pro náš experiment byl proveden RNN kodérový model s pozornostním mechanismem a transformátorovou architekturou. Experimentální výsledky ukázaly, že navržené modely architektury založené na RNN překonávají základní model pro překladatelskou úlohu z angličtiny do Myanmaru a transformátorové modely s více zdroji a sdílenými více zdroji přinášejí lepší výsledky překladu než základní.', 'et': 'Käesolevas töös anname ülevaate NECTECi meeskonna poolt WAT-2021 tõlketöödeks läbi viidud masintõlke mudelite eksperimentaalsetest tulemustest. Põhimõtteliselt põhinevad meie mudelid neuraalsetel meetoditel inglise-Myanmari ja Myanmari-inglise keele paaride mõlemas suunas. Enamik olemasolevatest neuromasintõlke mudelitest keskendub peamiselt järjestikuste andmete teisendamisele ega kasuta otseselt süntaktilist teavet. Siiski teostame mitme allika neuro masintõlke mudeleid, kasutades mitmekeelseid korpuseid, nagu stringiandmekorpus, puuandmekorpus või POS-märgistusega andmekorpus. Mitmealliline tõlge on lähenemisviis mitme sisendi (nt kahes erinevas vormingus) kasutamiseks tõlke täpsuse suurendamiseks. Meie eksperimendi jaoks on läbi viidud RNN-põhine tähelepanumehhanismi ja trafoarhitektuuriga kodeerija-dekooder mudel. Eksperimentaalsed tulemused näitasid, et kavandatud RNN-põhise arhitektuuri mudelid ületavad algmudelit inglise-Myanmari tõlketöö puhul ning mitme allika ja jagatud-mitme allika trafo mudelid annavad algtasemest paremaid tõlketulemusi.', 'fi': 'Tässä työssä raportoimme NECTECin tiimin WAT-2021 käännöstehtäviin tekemien konekäännösmallien kokeellisia tuloksia. Mallimme perustuvat pohjimmiltaan englannin-Myanmarin ja Myanmarin kieliparin molempiin suuntiin liittyviin neuromenetelmiin. Useimmat olemassa olevista neurokonekäännösmalleista keskittyvät pääasiassa sekvenssiaalisten tietojen muuntamiseen eivätkä käytä suoraan syntaktista tietoa. Teemme kuitenkin monikielisiä neurokonekäännösmalleja (NMT) käyttäen monikielisiä korpusia, kuten string data corpus, tree data corpus tai POS-tagged data corpus. Monilähdekäännös on tapa hyödyntää useita syötteitä (esim. kahdessa eri muodossa) käännöksen tarkkuuden parantamiseksi. Kokeeseen on tehty RNN-pohjainen kooderi-dekooderimalli huomiomekanismilla ja muuntaja-arkkitehtuurilla. Kokeelliset tulokset osoittivat, että ehdotetut RNN-pohjaisen arkkitehtuurin mallit ylittävät englannin-Myanmariin-käännöstehtävän perusmallin ja monilähteen ja jaetun monilähteen muuntajamallit tuottavat parempia käännöstuloksia kuin lähtötilanne.', 'bs': 'U ovom papiru izvještavamo eksperimentalne rezultate modela prevoda mašine koje je vodio tim NECTEC za prevod zadataka WAT-2021. U osnovi, naši modeli su bazirani na neuralnim metodama za obje smjere paira engleskog-Myanmara i Myanmar-engleskog jezika. Većina postojećih modela neurološkog prevoda (NMT) uglavnom se fokusira na konvertaciju sekvencijskih podataka i ne koriste direktno sintaktične informacije. Međutim, mi vodimo modele multiizvornog prevoda neuralnih strojeva (NMT) koristeći multijezičku korporaciju poput korpusa podataka podataka, korpusa podataka drveta ili korpusa podataka s POS-om. Višeizvorski prevod je pristup iskoristiti višestruke ulaznice (npr. u dva različita formata) kako bi povećala preciznost prevoda. Model kodera na RNN-u s mehanizam pažnje i arhitekturom transformera proveden je za naš eksperiment. Eksperimentalni rezultati pokazali su da predloženi modeli arhitekture bazirane na RNN-u iznosi početni model za prevod zadataka engleskog na Myanmar, te multiizvorski i zajednički multiizvorski transformatorski modeli donose bolji rezultat prevoda od početne linije.', 'he': 'בעיתון הזה, אנחנו מדווחים על התוצאות הניסיונייות של דוגמני תרגום מכונות שהובילו על ידי צוות NECTEC למשימות התרגום של WAT-2021. בעיקרון, הדוגמנים שלנו מבוססים על שיטות עצביות לשני הכיוונים של זוגות שפת אנגלית-מיאנמר ומיאנמר-אנגלית. רוב הדוגמנים הנוכחים למכונה נוירולית (NMT) מתמקדים בעיקר על השינוי של נתונים רצפיים ולא משתמשים ישירות במידע סינטקטי. בכל אופן, אנו מנהלים מודלים של מכונות עצביות ממקורות רבות (NMT) באמצעות גופות רבות שפות כמו קופוס מידע קושרות, קופוס מידע עץ, או קופוס מידע עם תווים POS. התרגום ממקורים רבים הוא גישה לנצל נכנסים רבים (למשל בשני צורות שונות) כדי להעלות את מדויקת התרגום. דוגמא הקודר-מפענח המבוסס על RNN עם מנגנון תשומת לב וארכיטקטורות המעבר הופעו לניסוי שלנו. התוצאות הניסיוניים הראו שהמודלים המוצעים של ארכיטקטורה מבוססת על ארנ"ן יוצאים מעל המודל הבסיסי עבור משימה התרגום אנגלית-למיאנמר, והמודלים המפורסמים המפורסמים המפורסמים ומפורסמים במפורסמים המפורסמים יוצאים תוצאות התרגום טובות', 'sk': 'V prispevku poročamo o poskusnih rezultatih modelov strojnega prevajanja, ki jih je izvedla ekipa NECTEC za prevajalske naloge WAT-2021. Naši modeli temeljijo na nevronskih metodah za obe smeri angleško-mjanmarskih in mjanmarskih jezikovnih parov. Večina obstoječih modelov nevralnega strojnega prevajanja (NMT) se osredotoča predvsem na pretvorbo zaporednih podatkov in ne uporablja neposredno sintaktičnih informacij. Vendar pa izvajamo modele več virov nevronskega strojnega prevajanja (NMT) z uporabo večjezičnih korpusov, kot so strunski podatkovni korpus, drevesni podatkovni korpus ali POS-označeni podatkovni korpus. Večvirni prevod je pristop k izkoriščanju več vhodov (npr. v dveh različnih formatih) za povečanje natančnosti prevajanja. Za naš eksperiment smo izvedli model kodirnika-dekoderja na osnovi RNN z mehanizmom pozornosti in transformatorsko arhitekturo. Rezultati eksperimentalnih rezultatov so pokazali, da predlagani modeli arhitekture, ki temelji na RNN, presegajo osnovni model za nalogo prevajanja angleščine v Mjanmar, modeli transformatorjev z več viri in deljenimi viri pa prinašajo boljše rezultate prevajanja kot osnovni model.', 'ha': "Ga wannan takardan, Munã bãyar da lãbãri ga matsalar da misãlai masu Translate na Mashine da aka samar da na koma na NEATEC wa fassarar aikin WAT-2021. Kiɓangan, misalinmu ne a kan hanyõyin neura zuwa duk shiryoyin Ingiriya-Myanmar da zimanmar-Ingiriya. Babu mafi yawan motsi na Fassari na Kikakkar Naural (NMT) masu gaba yana fokus a kan juzgar da data masu saka kuma bã su yi amfani da bayani koda. Kayya, ko kuma, muna aiki misãlai masu tarjiwa na masu ƙarƙashin neural (NMT) da ke amfani da shirin komaniya masu mulki-lingui kamar nau'in danne da za'a yi nau'in, ƙarami ga itãce data, ko kuma da bass-basketar data. translation An sami misãlin kodkoder-a-based RNN da muhalli masu aikin muhimmanci da masu shige matsayin ayuka. Matarin da aka jarraba shi ya nuna cewa misãlai da aka buƙata a kan RNN-based matsayin ya ƙarfafa motel na bincike wa aikin fassarar Ingiriya-zuwa-Myanmar, da masu multi-sourcen kuma da aka raba-multi-source transformer masu motsi masu ƙarai ga fassarar turjuya mafi alhẽri daga basalin.", 'jv': 'Nang pepulan iki, kita balik perbudhakan bakal ning model Ingurungkang Manus Terjamahan sing gunakake karo nggawe geraraning tasks WAT-2020. Rasane, model sing basa dadi sabanjure-sabanjuré nunggo jarang ingkang Mynmr karo Mynmr-ingkang sampeyan ingkang. Banyak kudu model sing dibutuhi Nyural Majin Terjamahan (NMT) sing ngomong dipundukno karo nggambar dadi sing dikantakno karo akeh informasi sing dikantakno. politenessoffpolite"), and when there is a change ("assertive politenessoffpolite"), and when there is a change ("assertivepoliteness Den model sing dibenalke ngkoder-decoder dumaten karo mekanistik lan architecture transformer Rejalaké sing paling-paling mbukakipun ngono nggawe model sing bisalui nggawe architecture sing bisalui nggawe barang nggawe gerangkat inggiles-karo Mynmr terjamahan, lan model multi-source lan share-multi-source transformer model sing bisalui nyebute mbutuhak dhéwé kayata tanggal sisalu sing luwih dumateng.', 'bo': 'འོག རྩ་བའི་གཞུང་ནས་ང་ཚོའི་མིག་གཞུང་གིས་དབྱིན་ཡིག་གཟུགས་དང་། མི་ཡུན་མཁན་གྱི་སྐད་རིགས་གཉིས་ཀྱི་གཟུགས་ལམ་ལ་གཞི་རྟེན གནས་ཡོད་པའི་Neural Machine Translation (NMT)མ་དབྱིབས་མང་ཆེ་ཤོས་ཀྱི་དཔེ་དབྱིབས་ཡང་རྒྱས་ལྗོངས་ཀྱི་གནས་ཚུལ However, we conduct multi-source neural machine translation (NMT) models using the multilingual corpora such as string data corpus, tree data corpus, or POS-tagged data corpus. འདིའི་ནང་དུ་འགྱུར་བའི་སྣ་མང་པོ་ཞིག་ནི་ཡིག་གཟུགས་འགྲུལ་བཙུགས་ཀྱི་ཐབས་ལམ་ཞིག་ཡིན། RNN-based encoder-decoder model with attention mechanism and transformer architectures have been carried out for our experiment. The experimental results showed that the proposed models of RNN-based architecture outperform the baseline model for English-to-Myanmar translation task, and the multi-source and shared-multi-source transformer models yield better translation results than the baseline.'}
{'en': 'Zero-pronoun Data Augmentation for Japanese-to-English Translation J apanese-to- E nglish Translation', 'es': 'Aumento de datos con pronombres cero para la traducción del japonés al inglés', 'ar': 'زيادة بيانات الضمائر الصفرية للترجمة من اليابانية إلى الإنجليزية', 'pt': 'Aumento de dados de pronome zero para tradução de japonês para inglês', 'fr': "Augmentation des données à pronom zéro pour la traduction du japonais vers l'anglais", 'ja': '日本語から英語への翻訳のためのゼロ代名詞データ拡張', 'zh': '用日语至英语翻译零代词数增强', 'hi': 'जापानी-से-अंग्रेजी अनुवाद के लिए शून्य-सर्वनाम डेटा वृद्धि', 'ru': 'Расширение данных с нулевым местоимением для перевода с японского на английский', 'ga': 'Méadú Sonraí náid-forainm le haghaidh Aistriú Seapáinis go Béarla', 'ka': 'Name', 'el': 'Αύξηση δεδομένων μηδενικής αντωνυμίας για ιαπωνική-αγγλική μετάφραση', 'it': 'Aumento dati del pronome zero per la traduzione giapponese-inglese', 'kk': 'Жапон- және ағылшын аудармасының Zero- pronoun деректерін авгументация', 'mk': 'Зголемување на податоците за јапонски на англиски превод', 'lt': 'Zero-pronoun Data Augmentation for Japanese-to-English translation', 'ml': 'ജാപ്പാനീസ്- ലേക്ക്- ഇംഗ്ലീഷ് പരിഭാഷക്കുള്ള പൂര്\u200dണ്ണമായ ഡേറ്റാ ഓഗ്മെന്റേഷന്\u200d', 'ms': 'Pembesaran Data Zero-pronoun untuk Terjemahan Jepun-ke-Inggeris', 'hu': 'Zéró névmással Adatbővítés japán-angol fordításhoz', 'mn': 'Япон-ээс Англи хэлний орчуулалтын Zero-pronoun өгөгдлийн нэмэгдүүлэлт', 'no': 'Comment', 'ro': 'Augmentația datelor cu pronume zero pentru traducerea japoneză în engleză', 'sr': 'Povećanje podataka za prevod japanskog na engleskom', 'mt': 'Żero-pronoun Data Augmentation for Japanese-to-English Translation', 'si': 'Name', 'ta': 'Name', 'ur': 'جاپانی سے انگلیسی ترجمہ کے لئے Zero- pronoun Data Augmentation', 'pl': 'Rozszerzenie danych o zaimku zerowym dla tłumaczenia japońskiego na angielski', 'so': 'Tilmaamaha macluumaadka Zero-pronoun-data for translation of Japanese-to-Ingiriis', 'sv': 'Zero-pronomen Data Augmentation för japansk-till-engelsk översättning', 'vi': 'Sơ suất dữ liệu từ Tây Ban Nha', 'uz': 'Zero-pronoun Data Augmentation for Japanese-to-English Translation', 'hr': 'Povećanje podataka za prevod japanskog na engleskom', 'nl': 'Data Augmentatie met nul voornaamwoorden voor vertaling van Japans-naar-Engels', 'id': '0-pronoun Data Augmentation for Japanese-to-English Translation', 'bg': 'Увеличаване на данни с нулево местоимение за превод от японски на английски', 'da': 'Nul-stedord Data Augmentation for japansk-til-engelsk oversættelse', 'fa': 'افزایش داده\u200cهای zero- pronoun برای ترجمه ژاپن- به انگلیسی', 'sw': 'Utafiti wa Taarifa zisizo na sifa kwa Tafsiri ya Kijapani-hadi Kiingereza', 'tr': 'Ýaponiýadan-iňlisçe terjime üçin nul-terjime Maglumaty Agymlandyrma', 'af': 'Comment', 'de': 'Datenerweiterung ohne Pronomen für Übersetzungen aus dem Japanischen ins Englische', 'sq': 'Rritja e të dhënave zero-pronoun për përkthimin japonez-në-anglez', 'am': 'መግለጫ', 'ko': '일영 번역 중의 제로 대명사 데이터 확장', 'az': 'Yaponca-İngilizə çevirilməsi üçün Zero-pronoun Data Augmentation', 'bn': 'জাপানি থেকে ইংরেজি অনুবাদের জন্য পূর্ণ তথ্য অংশগ্রহণ', 'ca': 'Aumentació de dades de zero per a la traducció japonès a anglès', 'cs': 'Rozšíření dat s nulovým zájmem pro japonsko-anglický překlad', 'et': 'Zero- pronoun andmete suurendamine jaapani- inglise tõlke jaoks', 'fi': 'Zero-pronomin Data Augmentation for japani-englanti käännös', 'hy': 'Զերոպրոնոնի տվյալների աճը ճապոներեն անգլերեն թարգմանության համար', 'bs': 'Povećanje podataka za prevod japanskog na engleskom', 'ha': 'Zero-pronoun Data Augmentation for Japanese-to-English Translation', 'sk': 'Povečanje podatkov z ničelnim zaimkom za prevod japonščine v angleščino', 'jv': 'AllProgressBar', 'he': 'תוספת מידע אפס לתרגום יפני לאנגלית', 'bo': 'ཇ་པོའིས་ལས་དབྱིན་ཡིག་ཆ་ལ་སྡོམ་བརྗོད་ཀྱི་ཆོག་ཐམ།'}
{'en': 'For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding  pronoun  in the target side of the English sentence. However, although fully resolving zero pronouns often needs  discourse context , in some cases, the local context within a sentence gives clues to the inference of the zero pronoun. In this study, we propose a data augmentation method that provides additional training signals for the translation model to learn correlations between  local context  and zero pronouns. We show that the proposed method significantly improves the  accuracy  of zero pronoun translation with  machine translation  experiments in the conversational domain.', 'fr': "Pour la traduction du japonais vers l'anglais, aucun pronom en japonais pose un défi, car le modèle doit déduire et produire le pronom correspondant dans le côté cible de la phrase anglaise. Cependant, bien que la résolution complète des pronoms zéro nécessite souvent un contexte de discours, dans certains cas, le contexte local dans une phrase donne des indices sur l'inférence du pronom zéro. Dans cette étude, nous proposons une méthode d'augmentation des données qui fournit des signaux d'entraînement supplémentaires pour le modèle de traduction afin d'apprendre les corrélations entre le contexte local et les pronoms zéro. Nous montrons que la méthode proposée améliore considérablement la précision de la traduction du pronom zéro grâce à des expériences de traduction automatique dans le domaine conversationnel.", 'ar': 'بالنسبة للترجمة من اليابانية إلى الإنجليزية ، فإن الضمائر الصفرية في اليابانية تشكل تحديًا ، لأن النموذج يحتاج إلى استنتاج وإنتاج الضمير المقابل في الجانب الهدف من الجملة الإنجليزية. ومع ذلك ، على الرغم من أن حل الضمائر الصفرية بشكل كامل يحتاج غالبًا إلى سياق خطاب ، إلا أنه في بعض الحالات ، يعطي السياق المحلي داخل الجملة أدلة على استدلال الضمير الصفري. في هذه الدراسة ، نقترح طريقة زيادة البيانات التي توفر إشارات تدريب إضافية لنموذج الترجمة لتعلم الارتباطات بين السياق المحلي والضمائر الصفرية. نوضح أن الطريقة المقترحة تحسن بشكل كبير من دقة الترجمة الصفرية باستخدام تجارب الترجمة الآلية في مجال المحادثة.', 'es': 'Para la traducción del japonés al inglés, los pronombres cero en japonés representan un desafío, ya que el modelo necesita inferir y producir el pronombre correspondiente en el lado objetivo de la oración en inglés. Sin embargo, aunque la resolución completa de pronombres cero a menudo necesita contexto discursivo, en algunos casos, el contexto local dentro de una oración da pistas sobre la inferencia del pronombre cero. En este estudio, proponemos un método de aumento de datos que proporciona señales de entrenamiento adicionales para que el modelo de traducción aprenda las correlaciones entre el contexto local y los pronombres cero. Demostramos que el método propuesto mejora significativamente la precisión de la traducción de pronombres cero con experimentos de traducción automática en el dominio conversacional.', 'ja': '日本語から英語への翻訳では、モデルが英文のターゲット側で対応する代名詞を推論して生成する必要があるため、日本語の代名詞ゼロが課題となります。しかし、ゼロ代名詞を完全に解決するには、しばしば話の文脈が必要であるが、場合によっては、文内のローカル文脈は、ゼロ代名詞の推論に手がかりを与える。この研究では、ローカルコンテキストとゼロ代名詞の間の相関を学習するために、翻訳モデルに追加のトレーニング信号を提供するデータ拡張法を提案します。提案された方法は、会話領域での機械翻訳実験で、ゼロ代名詞翻訳の精度を大幅に向上させることを示しています。', 'pt': 'Para a tradução de japonês para inglês, zero pronomes em japonês representam um desafio, pois o modelo precisa inferir e produzir o pronome correspondente no lado alvo da frase em inglês. No entanto, embora a resolução completa de pronomes zero muitas vezes precise de contexto de discurso, em alguns casos, o contexto local dentro de uma frase dá pistas para a inferência do pronome zero. Neste estudo, propomos um método de aumento de dados que fornece sinais de treinamento adicionais para o modelo de tradução aprender correlações entre contexto local e pronomes zero. Mostramos que o método proposto melhora significativamente a precisão da tradução de pronome zero com experimentos de tradução automática no domínio conversacional.', 'hi': 'जापानी-से-अंग्रेजी अनुवाद के लिए, जापानी में शून्य सर्वनाम एक चुनौती पैदा करते हैं, क्योंकि मॉडल को अंग्रेजी वाक्य के लक्ष्य पक्ष में संबंधित सर्वनाम का अनुमान लगाने और उत्पादन करने की आवश्यकता होती है। हालांकि, हालांकि पूरी तरह से शून्य सर्वनाम को हल करने के लिए अक्सर प्रवचन संदर्भ की आवश्यकता होती है, कुछ मामलों में, एक वाक्य के भीतर स्थानीय संदर्भ शून्य सर्वनाम के अनुमान के लिए सुराग देता है। इस अध्ययन में, हम एक डेटा संवर्धन विधि का प्रस्ताव करते हैं जो स्थानीय संदर्भ और शून्य सर्वनाम के बीच सहसंबंध सीखने के लिए अनुवाद मॉडल के लिए अतिरिक्त प्रशिक्षण संकेत प्रदान करता है। हम दिखाते हैं कि प्रस्तावित विधि संवादी डोमेन में मशीन अनुवाद प्रयोगों के साथ शून्य सर्वनाम अनुवाद की सटीकता में काफी सुधार करती है।', 'ru': 'Для перевода с японского на английский нулевые местоимения в японском языке представляют собой проблему, поскольку модель должна выводить и производить соответствующее местоимение в целевой части английского предложения. Однако, хотя полное разрешение нулевых местоимений часто требует контекста дискурса, в некоторых случаях локальный контекст в предложении дает подсказки к выводу нулевого местоимения. В этом исследовании мы предлагаем метод дополнения данных, который обеспечивает дополнительные обучающие сигналы для модели перевода, чтобы узнать корреляции между локальным контекстом и нулевыми местоимениями. Показано, что предлагаемый метод значительно повышает точность перевода нулевого местоимения с экспериментами машинного перевода в разговорной области.', 'zh': '日语之英语译,日语之零代词会致师者,以其英语句之侧推而生代词也。 然虽全解析零代词常须语上下文,句中局上下文为零代词推给线索。 于此论之,立数增强之法,为译模给额外训练信号,以习局部上下文零代词之间相关性。 结果表明,所立法于会话域中机器翻译实验著为零代词译者准确性.', 'ga': 'Maidir le haistriúchán Seapáinis go Béarla, is dúshlán é forainmneacha nialasacha sa tSeapáinis, mar ní mór don mhúnla an forainm comhfhreagrach a bhaint amach agus a tháirgeadh ar thaobh sprice na habairte Béarla. Mar sin féin, cé gur minic go mbíonn comhthéacs dioscúrsa ag teastáil chun forainmneacha a réiteach go hiomlán, i gcásanna áirithe, tugann an comhthéacs áitiúil laistigh d’abairt leideanna do thátal an fhorainm nialais. Sa staidéar seo, molaimid modh méadaithe sonraí a sholáthraíonn comharthaí oiliúna breise do mhúnla an aistriúcháin chun comhghaolta a fhoghlaim idir an comhthéacs áitiúil agus forainmneacha nialasacha. Léirímid go gcuireann an modh molta feabhas suntasach ar chruinneas an aistriúcháin náid forainm le trialacha aistriúcháin mheaisín sa réimse comhrá.', 'el': 'Για την ιαπωνική-αγγλική μετάφραση, οι μηδενικές αντωνυμίες στα ιαπωνικά αποτελούν πρόκληση, καθώς το μοντέλο πρέπει να συναγάγει και να παράγει την αντίστοιχη αντωνυμία στην πλευρά στόχου της αγγλικής πρότασης. Ωστόσο, αν και η πλήρης επίλυση μηδενικών αντωνυμίων συχνά χρειάζεται πλαίσιο λόγου, σε ορισμένες περιπτώσεις, το τοπικό πλαίσιο μέσα σε μια πρόταση δίνει ενδείξεις για το συμπέρασμα της μηδενικής αντωνυμίας. Στην παρούσα μελέτη, προτείνουμε μια μέθοδο αύξησης δεδομένων που παρέχει πρόσθετα εκπαιδευτικά σήματα για το μεταφραστικό μοντέλο για να μάθει συσχετισμούς μεταξύ τοπικού πλαισίου και μηδενικών αντωνυμίων. Δείχνουμε ότι η προτεινόμενη μέθοδος βελτιώνει σημαντικά την ακρίβεια της μετάφρασης μηδενικής αντωνυμίας με πειράματα μηχανικής μετάφρασης στον τομέα της συζήτησης.', 'ka': 'იაპონიდან-ანგლისურად გაგრძელებისთვის ნული წაპონიდან გამოსახულება იყოს გამოსახულება, რადგან მოდელმა უნდა გაგრძელოთ და გავამუშავოთ შესაძლებელი გამოსახულება ანგლის მაგრამ, თუმცა ნულ გამოსახულებების უფრო უნდა გადაწყვება, რამდენიმე შემთხვევაში, ლოკალური კონტექსტური კონტექსტური კონტექსტური კონტექსტური უნდა გადა ამ კვლევაში ჩვენ მოვიწყებთ მონაცემების აგგენტირების მეტი, რომელიც დამატებით აგენტირების სიგნალების შემდეგ გადაწყვეტილების მოდელეში, რომელიც ლოკალური კონტექ ჩვენ ჩვენ ჩვენ ჩვენ აჩვენებთ, რომ პროგრამის მეტი მნიშვნელოვანად უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო', 'kk': 'Жапон- ағылшын тілінен аудару үшін, жапон тіліндегі нөл сөйлемелер жауап береді, себебі үлгі ағылшын тілінің мақсатты жағынан сәйкесті сөйлемелерді жасау және жауап беру керек. Бірақ нөл сөздерді толық шешу керек, кейбір жағдайда сөздердің жергілікті контексті қажет етеді. Жергілікті контексті сөздердің нөл сөздерінің шешуіне белгілейді. Бұл зерттеулерде, жергілікті контекст мен нөл белгілер арасындағы корреляцияларды оқыту үшін қосымша оқыту сигналдарын қолданатын деректерді қосу әдісін таңдаймыз. Біз таңдалған тәсілінің нөл аудару тәжірибесімен машиналық аудару тәжірибесінің тәжірибесінің дұрыстығын өзгертеді.', 'hu': 'A japán-angol fordítás esetén a nulla névmást japán nyelven kihívást jelent, mivel a modellnek következtetnie kell és meg kell állítania a megfelelő névmást az angol mondat céloldalán. Habár a nullás névmások teljes megoldása gyakran diskurzus kontextusra van szükség, bizonyos esetekben a helyi kontextus egy mondaton belül nyomokat ad a nullás névmások következtetésére. Ebben a tanulmányban olyan adatbővítési módszert javasolunk, amely további képzési jeleket biztosít a fordítási modell számára, hogy megismerje a helyi kontextus és a nulla névmások közötti korrelációkat. Megmutatjuk, hogy a javasolt módszer jelentősen javítja a nulla névmásos fordítás pontosságát a beszélgetési területen végzett gépi fordítási kísérletekkel.', 'it': "Per la traduzione giapponese-inglese, zero pronomi in giapponese rappresentano una sfida, poiché il modello deve dedurre e produrre il pronome corrispondente nel lato target della frase inglese. Tuttavia, sebbene la risoluzione completa dei pronomi zero abbia spesso bisogno di un contesto di discorso, in alcuni casi, il contesto locale all'interno di una frase dà indizi sull'inferenza del pronome zero. In questo studio, proponiamo un metodo di aumento dei dati che fornisce ulteriori segnali di formazione per il modello di traduzione per imparare le correlazioni tra contesto locale e pronomi zero. Mostriamo che il metodo proposto migliora significativamente l'accuratezza della traduzione di pronomi zero con esperimenti di traduzione automatica nel dominio conversazionale.", 'ml': 'ജപ്പാനീസില്\u200d നിന്നും ഇംഗ്ലീഷില്\u200d നിന്നും പരിഭാഷകള്\u200dക്ക്, ജാപ്പാനിലെ പൂര്\u200dണ്ണമാര്\u200dക്കും ഒരു വ്യാല്\u200dക്കാലം ഉണ്ടാക്കിയിരിക്കുന്നു. അത എന്നാലും പൂര്\u200dണ്ണമായി പ്രഖ്യാപിക്കുന്ന പൂര്\u200dണ്ണമായി തീരുമാനിക്കുന്നത് പോലും പലപ്പോഴും സംസാരിക്കേണ്ടതുണ്ടെങ്കിലും, ചില ക ഈ പഠനത്തില്\u200d നമ്മള്\u200d ഒരു ഡേറ്റാ കൂടുതല്\u200d പരിശീലിക്കുന്ന രീതിയില്\u200d പ്രാദേശിപ്പിക്കുന്നു. പരിശീലന മോഡലിന് കൂടുതല്\u200d പരിശീലന സ പ്രൊദ്ദേശിക്കപ്പെട്ട രീതിയില്\u200d സംസാരിക്കുന്ന ഡോമെയിനിലെ മെഷീന്\u200d പരിശോധന പരീക്ഷണങ്ങളുമായി പൂര്\u200dണ്ണമായ പരിശോധനങ്', 'ms': 'For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding pronoun in the target side of the English sentence.  Namun, walaupun menyelesaikan sepenuhnya pronun sifar sering memerlukan konteks diskors, dalam beberapa kes, konteks setempat dalam kalimat memberikan petunjuk kepada kesimpulan pronun sifar. Dalam kajian ini, kami cadangkan kaedah peningkatan data yang menyediakan isyarat latihan tambahan untuk model terjemahan untuk belajar korelasi antara konteks setempat dan pronun sifar. Kami menunjukkan bahawa kaedah yang diusulkan meningkatkan secara signifikan ketepatan terjemahan bernama sifar dengan eksperimen terjemahan mesin dalam domain perbualan.', 'mt': 'Għat-traduzzjoni Ġappuniża għall-Ingliż, żero pronouns fil-Ġappuniż joħolqu sfida, peress li l-mudell jeħtieġ li jikkonkludi u jipproduċi l-pronoun korrispondenti fin-naħa mmirata tas-sentenza Ingliża. Madankollu, għalkemm is-soluzzjoni sħiħa ta’ żero pronouns spiss teħtieġ kuntest ta’ diskors, f’xi każijiet, il-kuntest lokali fi ħdan sentenza jagħti indikazzjonijiet għall-inferenza tal-pronoun żero. F’dan l-istudju, qed nipproponu metodu ta’ żieda tad-dejta li jipprovdi sinjali ta’ taħriġ addizzjonali għall-mudell ta’ traduzzjoni biex jitgħallmu korrelazzjonijiet bejn il-kuntest lokali u l-pronomi żero. Aħna nuru li l-metodu propost itejjeb b’mod sinifikanti l-preċiżjoni tat-traduzzjoni b’mod ċar żero bl-esperimenti tat-traduzzjoni bil-magna fid-dominju tal-konverżjoni.', 'lt': 'Kalbant apie vertimą japonų kalba anglų kalba, neišspręsta, nes modelis turi daryti išvadą ir parengti atitinkamą vardą anglų kalbos sakinio tikslinėje pusėje. Vis dėlto, nors visiškai išspręsti nulinius žodžius dažnai reikia diskurso konteksto, kai kuriais atvejais vietinis žodžio kontekstas rodo nulinio žodžio išvadą. Šiame tyrime siūlome duomenų didinimo metodą, kuris suteikia papildomų mokymo signalų vertimo modeliui, siekiant išsiaiškinti vietos konteksto ir nulinio išraiško koreliacijas. Mes parodome, kad pasiūlytas metodas gerokai pagerina nulinio aiškio vertimo tikslumą su mašininio vertimo eksperimentais konversacijos srityje.', 'mk': 'За јапонски на англиски превод, нула изрази на јапонски претставуваат предизвик, бидејќи моделот мора да го заклучи и произведува соодветниот израз на целната страна на англиската реченица. Сепак, иако целосно решавање на нула изрази честопати му треба дискурсниот контекст, во некои случаи, локалниот контекст во рамките на реченицата дава траги за конференцијата на нула израз. Во оваа студија, предложуваме метод за зголемување на податоците кој обезбедува дополнителни сигнали за обука за моделот на превод за да се научат корелации помеѓу локалниот контекст и нула изрази. Покажуваме дека предложениот метод значително ја подобрува точноста на нуларниот превод со експерименти за машински превод во конверзационалниот домен.', 'no': 'For å omsetja japansk til engelsk, er nulle utfordringar i japansk, sidan modellen må inferer og produsere den tilsvarande utfordringen i målside av engelsk setninga. Men selv om det fullstendig løysinga av null-uttrykk ofte treng diskurskontekst, i nokre tilfelle vil det lokale konteksten inne eit setning gje klokka til nedgangen av null-uttrykk. I denne studien foreslår vi ein metode for å auka data som gjev fleire opplæringssignaler for omsetjingsmodulen for å lære korrelasjonar mellom lokale kontekst og null uttrykk. Vi viser at den foreslåde metoden betydelig forbedrar nøyaktigheten til null-pronoun-omsetjing med maskinsomsetjingsperimentar i konvertasjonskområdet.', 'mn': 'Япон-аас англи хэл хэлэхэд 0 хэлбэрүүд япон хэлний хэлбэрээр сорилт бий болдог. Учир нь загвар Англи хэлний зорилготой талд хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо хооро Гэхдээ тэгш утгыг бүрэн шийдвэрлэхэд хэд хэдэн тохиолдолд ярианы тухай хэрэгтэй. Зарим тохиолдолд хэдэн тохиолдолд орчин тохиолдол нь тэгш утгын дарамтыг харуулдаг. Энэ судалгаанд бид өөрсдийгөө илүү дасгал хөгжүүлэх арга загвар өгөгдлийн нэмэгдүүлэх арга загвар өгдөг. Энэ нь орчин орчин тохиолдол болон 0 тохиолдол хоорондын хоорондын холбоотой бай Бид санал өгсөн арга нь харилцааны холбоотой машин орчуулах туршилттай тэр хэлбэрийн тодорхойлолтын тодорхойлолтыг нэмэгдүүлдэг гэдгийг харуулж байна.', 'pl': 'W przypadku tłumaczenia japońsko-angielskiego zero zaimków w języku japońskim stanowi wyzwanie, ponieważ model musi wnioskować i wyprodukować odpowiedni zaimk w stronie docelowej zdania angielskiego. Jednak choć pełne rozwiązanie zaimków zerowych często wymaga kontekstu dyskursu, w niektórych przypadkach kontekst lokalny w zdaniu daje wskazówki do wniosku zaimka zerowego. W niniejszym opracowaniu proponujemy metodę powiększania danych, która dostarcza dodatkowych sygnałów treningowych dla modelu tłumaczenia, aby poznać korelacje między kontekstem lokalnym a zaimkami zerowymi. Pokazujemy, że proponowana metoda znacznie poprawia dokładność tłumaczenia zaimków zerowych dzięki eksperymentom z tłumaczeniem maszynowym w domenie konwersacji.', 'sr': 'Za prevod japanski na engleski, nula proglašavanja na japanskom jeziku predstavlja izazov, jer model mora da potvrdi i proizvodi odgovarajući izgovor na ciljnoj strani engleske rečenice. Međutim, iako u potpunosti rješavanje nule progovore često treba kontekst diskursa, u nekim slučajevima, lokalni kontekst unutar rečenice daje tragove infekcije nule progovore. U ovoj studiji predlažemo metodu povećanja podataka koja pruža dodatne signale obuke za model prevođenja da naučimo korelacije između lokalnog konteksta i nule izraze. Pokazujemo da je predložena metoda značajno poboljšala preciznost nulog prevoda sa eksperimentima za prevod mašine u razgovornom domenu.', 'ro': 'Pentru traducerea din japoneză în engleză, zero pronume în japoneză reprezintă o provocare, deoarece modelul trebuie să deducă și să producă pronumele corespunzător în partea țintă a propoziției în engleză. Cu toate acestea, deși rezolvarea completă a pronumelor zero necesită adesea contextul discursului, în unele cazuri, contextul local dintr-o propoziție oferă indicii pentru inferența pronumelor zero. În acest studiu, propunem o metodă de augmentare a datelor care oferă semnale suplimentare de formare pentru modelul de traducere pentru a afla corelațiile dintre contextul local și pronumele zero. Arătăm că metoda propusă îmbunătățește semnificativ acuratețea traducerii pronumelor zero cu experimente de traducere automată în domeniul conversațional.', 'si': 'ජාපානිස් වල ඉංග්\u200dරීසි භාෂාව සඳහා, ජාපානිස් වල ශූන්ය ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් තියෙනවා, මොඩේල් එක අවශ්නයක් ති නමුත්, සුන්ධ ප්\u200dරශ්නයක් සම්පූර්ණයෙන් විශ්වාස කරනවා නමුත්, සුන්ධ ප්\u200dරශ්නයක් සම්පූර්ණයෙන්, සමහර විසින්ධ මේ පරීක්ෂණයේ අපි දත්ත විශාලනය විධානයක් ප්\u200dරයෝජනය කරනවා, ඒකෙන් වාර්ථාන මදුල්යය සහ ශූන්ය ප්\u200dරයෝජනය සඳහ අපි පෙන්වන්නේ ප්\u200dරතිචාර විධානය විශේෂයෙන් ප්\u200dරතිචාර විශේෂයෙන් සුන්ධ පරිචාර පරීක්ෂණය සමග පරීක්', 'sv': 'För översättning från japansk till engelska utgör noll pronomen på japanska en utmaning, eftersom modellen måste härleda och producera motsvarande pronomen i målsidan av den engelska meningen. Även om en fullständig lösning av nollpronomen ofta behöver diskurskontext, ger det lokala sammanhanget i en mening ledtrådar till slutsatsen av nollpronomen. I denna studie föreslår vi en dataförstärkningsmetod som ger ytterligare träningssignaler för översättningsmodellen för att lära sig korrelationer mellan lokalt sammanhang och nollpronomen. Vi visar att den föreslagna metoden avsevärt förbättrar noggrannheten hos nollpronomsöversättning med maskinöversättningsexperiment inom konversationsområdet.', 'so': 'Turjumista Jabanees-to-Ingiriis waxaa loola jeedaa wax dhibaato ah, sababtoo ah modelku waa in uu u baahan yahay inuu dhibaato iyo uu soo saaro mid ku habboon dhinaca afka Ingiriiska. Si kastaba ha ahaatee, in kastoo ay si buuxsanto u go’aanka nuurka ah u baahan yihiin in marka qaarkood ay xaaladaha qaarkood ka baahan yihiin, xaaladaha degmada ee ku jirta xaaladaha, waxay sababto ku leedahay cudurka nuurka. Waxbarashadan waxaynu soo jeedaynaa qaab kordhiska macluumaadka oo u fidinaya calaamado dheeraad ah oo waxbarashada, si ay u barto xiriirka ku dhexeeya xaaladaha deegaanka iyo nuqulka. Waxaynu muujinnaa in qaababka la soo jeeday uu si muhiim ah u kordhiyo saxda tarjumaadda nuurka lagu kordhiyey oo lagu tijaabiyo baaritaanka tarjumaadda machine ee gudaha sameynta.', 'ta': 'ஜப்பானிஸ்- மொழிபெயர்ப்பு மொழிபெயர்ப்பிற்கு, ஜாப்பானியில் பூஜ்ஜியமானவர்கள் ஒரு சவாலாக இருக்கிறது, ஏனெனில் மாதிரியில் குறைந்து மற பூஜ்ஜியத்தை முழுமையாக தீர்வு செய்ய வேண்டுமானாலும் பெரும்பாலும் பேச்சு சூழல் தேவைப்படுகிறது, சில நிகழ்வில், உள்ளூர் ச இந்த ஆராய்ச்சியில், நாம் ஒரு தரவு கூட்டுதல் முறைமையை பரிந்துரைக்கிறோம். அது மொழிபெயர்ப்பு மாதிரிக்கு கூடுதல் பயிற்சிக நாம் முன்நிர்ணயிக்கப்பட்ட முறைமையில் பூஜ்ஜியமான மொழிபெயர்ப்பின் சரியை மேம்படுத்துகிறது என்பதை காட்டுகிறோம் என', 'ur': 'جاپانی سے انگلیسی ترجمہ کے لئے، جاپانی میں صفر کی تعبیر ایک چال ہے، کیونکہ موڈل کے لئے انگلیسی جماعت کے موافق سمجھنے کی ضرورت ہے۔ لیکن، اگرچہ صفر تعلیم کو پورا پورا حل کرنے کی ضرورت ہے، بعض مواقع میں، ایک جماعت کے اندر موقعیت صفر تعلیم کے ذریعہ اضافہ کرتا ہے. اس مطالعہ میں ہم ایک ڈیٹا اگنٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ڈیٹ ہم نشان دیتے ہیں کہ پیشنهاد کی طریقہ صفر کی تعبیر کی دقیقیت کو مشین ترجمہ آزمائش کے مطابق مصاحبہ ڈومین میں بہتر کر دیتی ہے.', 'vi': 'Đối với dịch tiếng Nhật-Anh, không phát từ tiếng Nhật là một thử thách, vì theo mẫu cần phải ghi nhận và sản xuất đại từ tương ứng trong phần mục tiêu của câu tiếng Anh. Tuy nhiên, mặc dù từ không giải quyết hoàn to àn thường cần ngữ cảnh ngôn ngữ, trong một số trường hợp, ngữ cảnh địa phương trong một câu cho thấy sự thừa nhận của đại từ không. Trong nghiên cứu này, chúng tôi đề xuất một phương pháp gia tăng dữ liệu cung cấp các tín hiệu huấn luyện bổ sung cho mô hình dịch để học sự tương quan giữa ngữ cảnh địa phương và từ không. Chúng tôi cho thấy phương pháp đề nghị cải thiện đáng kể độ chính xác của dịch từ không bằng các thí nghiệm dịch biến máy trong khu vực đối thoại.', 'uz': "Yaponchadan Inglizchaga tarjima qilish uchun, Yaponchadagi nuqta taʼminlovchi tarjima bo'ladi, chunki model ingliz tilining so'zlarining eng maqsadi chegarasini ko'paytirish kerak. Lekin hech qachon narsalarni butunlay o'zgartirish kerak bo'lganda, ba'zi holatda, lokal muktadha qo'shishga hech narsa yoʻq emas. Bu taʼminotda, biz taʼminlovchi maʼlumot qoʻshish usulini talab qilamiz. Tarjima modeli uchun qoʻshimcha taʼminlovchi imkoniyatlarni o'rganish uchun qoʻshimcha imkoniyatlar imkoniyatini beradi. Ko'rib chiqaradigan usulni ko'rsatishimiz mumkin, bu muloqat domenadagi Mashine tarjima tahrirlarini tahrirlash mumkin.", 'bg': 'За превода от японски на английски език нулевите местоимения на японски представляват предизвикателство, тъй като моделът трябва да направи заключение и да произведе съответното местоимение в целевата страна на английското изречение. Въпреки че пълното решаване на нулеви местоимения често се нуждае от дискурсен контекст, в някои случаи локалният контекст в изречението дава улики за заключението на нулевото местоимение. В това проучване предлагаме метод за увеличаване на данните, който осигурява допълнителни тренировъчни сигнали за модела на превод, за да се научат корелации между локален контекст и нулеви местоимения. Показваме, че предложеният метод значително подобрява точността на нулевия превод на местоимения с експерименти с машинен превод в областта на разговора.', 'hr': 'Za prevod japanskog na engleskom jeziku nula proglašavanja na japanskom jeziku predstavlja izazov, jer model mora uvući i proizvesti odgovarajući izraz na ciljnoj strani engleske rečenice. Međutim, iako u potpunosti rješavanje nula izračuna često treba kontekst diskursa, u nekim slučajevima, lokalni kontekst unutar rečenice daje tragove infekcije nule izračuna. U ovom ispitivanju predlažemo metodu povećanja podataka koja pruža dodatne signale obuke za model prevoda kako bi naučili korelacije između lokalnog konteksta i nule izraze. Pokazujemo da je predložena metoda značajno poboljšala preciznost nulog prevoda s eksperimentima prevoda strojeva u razgovornom domenu.', 'nl': 'Voor Japans-naar-Engels vertaling vormen nul voornaamwoorden in het Japans een uitdaging, omdat het model het overeenkomstige voornaamwoord moet afleiden en produceren in de doelkant van de Engelse zin. Hoewel het volledig oplossen van nulvoornaamwoorden vaak discourscontext vereist, geeft de lokale context binnen een zin aanwijzingen voor de conclusie van het nulvoornaamwoord. In deze studie stellen we een data augmentatie methode voor die extra trainingssignalen biedt voor het vertaalmodel om correlaties tussen lokale context en nulvoornaamwoorden te leren. We laten zien dat de voorgestelde methode de nauwkeurigheid van nul voornaamwoorden vertaling aanzienlijk verbetert met behulp van machine translation experimenten in het conversationele domein.', 'da': 'For japansk-til-engelsk oversættelse udgør nul stedord på japansk en udfordring, da modellen skal udlede og producere det tilsvarende stedord i målsiden af den engelske sætning. Men selvom fuldstændig løsning af nul pronomen ofte kræver diskurskontekst, giver den lokale kontekst i en sætning spor til slutningen af nul pronomen. I denne undersøgelse foreslår vi en dataaugmentationsmetode, der giver yderligere træningssignaler til oversættelsesmodellen for at lære korrelationer mellem lokal kontekst og nul pronomen. Vi viser, at den foreslåede metode betydeligt forbedrer nøjagtigheden af nul pronomen oversættelse med maskinoversættelseseksperimenter i samtaldomænet.', 'de': 'Für die Übersetzung von Japanisch nach Englisch stellen Nullpronomen im Japanischen eine Herausforderung dar, da das Modell das entsprechende Pronomen auf der Zielseite des englischen Satzes ableiten und produzieren muss. Obwohl die vollständige Auflösung von Nullpronomen oft Diskurskontext erfordert, gibt der lokale Kontext innerhalb eines Satzes Hinweise auf die Schlussfolgerung des Nullpronomen. In dieser Studie schlagen wir eine Methode zur Datenauswertung vor, die zusätzliche Trainingssignale für das Übersetzungsmodell bereitstellt, um Korrelationen zwischen lokalem Kontext und Nullpronomen zu lernen. Wir zeigen, dass die vorgeschlagene Methode die Genauigkeit der Nullpronomen-Übersetzung durch maschinelle Übersetzungsexperimente im Konversationsbereich signifikant verbessert.', 'fa': 'برای ترجمه ژاپن به انگلیسی صفر در ژاپن یک چالش قرار می دهد، زیرا مدل نیاز دارد که آزاد و تولید کند کلمه مربوط به سمت هدف جمله انگلیسی. ولی اگرچه کاملا حل کردن کلمه\u200cهای صفر اغلب به محیط گفتگو نیاز دارد، در بعضی مواقع، محیط محلی در یک جمله نشانه\u200cهای تفاوت صفر را می\u200cدهد. در این مطالعه، ما یک روش افزایش داده پیشنهاد می کنیم که سیگنال های آموزش اضافه برای مدل ترجمه را برای یاد گرفتن ارتباطات بین محلی محلی و تعریف صفر می دهد. ما نشان می دهیم که این روش پیشنهاد دقیق ترجمه صفر را با آزمایشات ترجمه ماشین در دامنه مکالمانی بهتر می کند.', 'id': 'For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding pronoun in the target side of the English sentence.  Namun, meskipun menyelesaikan sepenuhnya nol pronouns sering membutuhkan konteks diskors, dalam beberapa kasus, konteks lokal dalam kalimat memberikan petunjuk untuk kesimpulan dari nol pronoun. Dalam studi ini, kami mengusulkan metode peningkatan data yang menyediakan sinyal pelatihan tambahan untuk model terjemahan untuk mempelajari korelasi antara konteks lokal dan nol pronon. Kami menunjukkan bahwa metode yang diusulkan meningkatkan secara signifikan akurasi dari terjemahan kosong dengan eksperimen terjemahan mesin dalam domain konversasi.', 'ko': '일본어 번역에 있어 일본어의 0대명사는 도전이다. 왜냐하면 이 모델은 영어 문장의 목표단에서 해당하는 대명사를 추정하고 만들어야 하기 때문이다.그러나 비록 영대사를 완전히 해석하려면 통상적으로 언어의 언어 환경이 필요하지만 어떤 상황에서 문장 중의 국부적인 언어 환경은 영대사의 추리에 단서를 제공한다.본 연구에서 우리는 데이터 강화 방법을 제시하여 번역 모델에 추가 훈련 신호를 제공하여 현지 상하문과 0대어 간의 관련성을 학습하도록 했다.회화 분야의 기계 번역 실험을 통해 이 방법은 0대어 번역의 정확성을 현저히 높였다.', 'sw': 'Kwa tafsiri ya Kijapani-hadi Kiingereza, watu wa Kijapani hawana changamoto, kwa sababu modeli inahitaji kupunguza na kutengeneza matukio yanayofanana katika upande wa hukumu ya Kiingereza. Hata hivyo, ingawa kutatua kauli sifa mara nyingi huwa inahitaji mazungumzo ya mazungumzo, kwa baadhi ya matukio, muktadha wa ndani ya hukumu unaonyesha kutokuwepo kwa sifa. Katika utafiti huu, tunapendekeza njia ya kuongeza takwimu inayotoa ishara za mafunzo ya ziada kwa mfano wa tafsiri ili kujifunza mahusiano kati ya mazingira ya ndani na watu wasio na sifa. We show that the proposed method significantly improves the accuracy of zero pronoun translation with machine translation experiments in the conversational domain.', 'sq': 'Për përkthimin japonez-në-anglez, zero pronone në japonez përbëjnë një sfidë, pasi modeli duhet të përfundojë dhe prodhojë prononin e përkatshëm në anën objektive të fjalimit anglez. Megjithatë, megjithëse zgjidhja plotësisht e shprehjeve zero shpesh ka nevojë për kontekst diskursor, në disa raste, konteksti lokal brenda një fjalie jep shenja për përfundimin e shprehjes zero. Në këtë studim, ne propozojmë një metodë shtimi të të dhënave që ofron sinjale shtesë trainimi për modelin e përkthimit për të mësuar korrelacione midis kontekstit lokal dhe zero pronumeve. We show that the proposed method significantly improves the accuracy of zero pronoun translation with machine translation experiments in the conversational domain.', 'af': "Vir japanse-na-Engelske vertaling, nul voorskrifte in japanse poseer 'n uitdrukking, omdat die model nodig om die ooreenstemmende voorskrifte in die doelskant van die Engelse seting te bring en te produseer. Maar, alhoewel die volledige oplossing van nul uitdrukkings dikwels nodig diskursie konteks, in sommige gevalle, die plaaslike konteks binne 'n seting gee kloue aan die uitdrukking van die nul uitdrukking. In hierdie studie voorstel ons 'n data augmentasie metode wat addisionele onderwerp signale verskaf vir die vertaling model om korrelasies tussen plaaslike konteks en nul voorwerp te leer. Ons wys dat die voorgestelde metode betekeurig verbeter die presisiteit van zero pronoun vertaling met masjien vertaling eksperimente in die omskakelingsdomein.", 'tr': 'Japonça-we-iňlisçe terjime etmek üçin japonça terjime etmek üçin 0 sany kynçylyk bar, sebäbi nusga iňlisçe sözläniň maksady tarapynda täsirli sözläni çykarmak we çykarmak gerek. Ýöne, nähili sözleriň çözmesine köplenç sözleşme konteksti gerek bolsa, käbir wagtlarda ýerleşki konteksti bir sözleriň içinde nähili sözleriň azalyşyny çaklaýar. Bu öwrenmede, biz daýlar ýerli kontekst we 0 a ýrym arasynda correlikler öwrenmek üçin daýlar ýetişirmek üçin golaýlaşma işaretlerini teklip edip bilýäris. Biz teklip eden yöntemiň soňra söhbet domaýyndaky maşynyň terjime deneyleri bilen dogrylygyny gowurap ýöreýändigini görkeýäris.', 'hy': 'For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding pronoun in the target side of the English sentence.  Այնուամենայնիվ, չնայած, որ զրո արտահայտությունների ամբողջ լուծումը հաճախ անհրաժեշտ է խոսակցական կոնտեքստ, որոշ դեպքերում նախադասության մեջ տեղական կոնտեքստը ցույց է տալիս զրո արտահայտության եզրակացու Այս ուսումնասիրության ընթացքում մենք առաջարկում ենք տվյալների բարձրացման մեթոդ, որը պարունակում է ավելին վարժեցման ազդանշաններ թարգմանման մոդելի համար, որպեսզի հասկանանք տեղական կոնտեքստի և զրո արտահայտությունների միջ Մենք ցույց ենք տալիս, որ առաջարկված մեթոդը նշանակալիորեն բարելավում է զրոյի արտահայտված թարգմանման ճշմարտությունը զրոյի փորձարկումներով խոսակցության ոլորտում:', 'az': 'Japonca-İngilizə çevirilməsi üçün, Japonca sıfır sözlər çətinliklərə məcburiyyət edir, çünki modeli İngilizə sözlərinin məqsədilə uyğun sözləri təşkil etmək və təşkil etmək lazımdır. Halbuki, sıfır tərzlərini tamamlamaq üçün çox sıfırla müzakirə məlumatı lazımdır, bəzi vaxtlarda, sözlərin içində yerli məlumatı sıfır tərzlərinin azaltmasına nəticələr verir. Bu təhsil içində, yerli məlumat və sıfır təhsil arasındakı bağlantıları öyrənmək üçün çox təhsil göstərən məlumat yükləmə metodu təklif edirik. Biz göstəririk ki, təbliğ edilmiş metodun, söhbət domenindəki maşın çeviri experimentləri ilə sıfır pronoun çevirisinin doğruluğunu çox yaxşılaşdırır.', 'am': 'ለጃፓንኛ-ወደ እንግሊዘኛ ትርጓሜ፣ በጃፓንኛ ውስጥ አካባቢ የኢንዝላዊ ግንኙነት መግለጫ ነው፡፡ ሞዴል በንግግሊዝኛ ግንኙነት መቆጣጠር የሚያስፈልገውን እና መግለጫ ያስፈልጋል፡፡ ምንም እንኳን፣ ምንም እንኳን በቁጥር በሙሉ ማስታወቂያውን ቢፈቅድ ብዙ ጊዜ የንግግር ግንኙነት ያስፈልጋል፣ በአንዳንድ ጉዳይ፣ የግንኙነቱ ግንኙነት በክፍል ውስጥ የ0 ውጤት ማድረግ ያስፈልጋል፡፡ በዚህ ትምህርት ውስጥ የአካባቢው ግንኙነት እና በክፍለ ክፍል መካከል ግንኙነትን ለመማር የሚጨመር ማኅበረሰብ የሚጨመር የዳታ አካባቢ ማድረግ ማድረግ እና ለትርጉም ሞዴል እና ለክፍለ ክፍል ማግኘትን እናስጀራለን፡፡ በተዘጋጀው ሥርዓት በተቃዋሚው አካባቢ ትርጓሜዎችን በሀሳዊ ትርጉም ፈተና እንዲያበዛ እናሳየዋለን፡፡', 'bs': 'Za prevod japanskog na engleskom jeziku nula proglašavanja na japanskom jeziku predstavlja izazov, jer model mora zaraditi i proizvesti odgovarajući izraz na ciljnoj strani engleske rečenice. Međutim, iako u potpunosti rješavanje nule progovore često treba kontekst diskusija, u nekim slučajevima, lokalni kontekst unutar rečenice daje tragove infekcije nule progovore. U ovom ispitivanju predlažemo metodu povećanja podataka koja pruža dodatne signale obuke za model prevoda kako bi naučili korelacije između lokalnog konteksta i nule izraze. Pokazujemo da je predložena metoda značajno poboljšala preciznost nulog prevoda sa eksperimentima za prevod mašine u razgovornom domenu.', 'bn': 'জাপানি থেকে ইংরেজি অনুবাদের জন্য জাপানীয় বাসিন্দাদের একটি চ্যালেঞ্জ প্রদান করেছে, যেহেতু মডেলটি ইঙ্গিলের শাস্তির লক্ষ্যের পাশে সং তবে যদিও পুরোপুরি শূন্য প্রচারণার সমাধান প্রয়োজন, কিছু ক্ষেত্রে স্থানীয় প্রতিক্রিয়া বিষয়টিকে আলোচনা প্রয়োজন, কিছু ক্ষেত্রে  এই গবেষণায় আমরা একটি তথ্য যোগাযোগ মাধ্যম প্রস্তাব করি যা অনুবাদ মডেলের জন্য আরো প্রশিক্ষণের সিগন্যাল প্রস্তাব করে স্থানীয় পরিসংখ্যা  আমরা দেখাচ্ছি যে প্রস্তাবিত পদ্ধতি এই আলোচনায় ডোমেইনে মেশিন অনুবাদের পরীক্ষার মাধ্যমে শুধুমাত্র পরিমাণের সঠিকভাবে উন্নত', 'cs': 'Pro japonsko-anglický překlad představují nulová zájmena v japonštině výzvu, protože model musí odvodit a vytvořit odpovídající zájméno v cílové straně anglické věty. Nicméně, ačkoli plné řešení nulových zájmen často vyžaduje diskurzní kontext, v některých případech lokální kontext ve větě dává vodítko k odvodu nulového zájmena. V této studii navrhujeme metodu rozšíření dat, která poskytuje další tréninkové signály pro překladový model, aby se naučila korelace mezi lokálním kontextem a nulovými zájmeny. Ukazujeme, že navržená metoda výrazně zlepšuje přesnost nulového překladu zájmen pomocí experimentů strojového překladu v konverzační doméně.', 'fi': 'Japani-englanti-käännöksessä nollapronominit japaniksi ovat haaste, koska mallin on johdettava ja tuotettava vastaava pronomini englanninkielisen lauseen kohdepuolella. Vaikka nollapronominien täydellinen ratkaiseminen vaatii usein diskurssikontekstia, joissakin tapauksissa lauseen paikallinen konteksti antaa vihjeitä nollapronominin päättelyyn. Tässä tutkimuksessa ehdotamme datan lisäämismenetelmää, joka tarjoaa lisäkoulutussignaaleja käännösmallille paikallisen kontekstin ja nollapronominien välisten korrelaatioiden oppimiseksi. Osoitamme, että ehdotettu menetelmä parantaa merkittävästi nollapronominkäännöksen tarkkuutta konekäännöskokeilla keskustelualueella.', 'ca': "Per a la traducció japonès a anglès, zero pronòms en japonès posen un repte, ja que el model ha de inferir i produir el pronòm correspondent a la banda d'objectiu de la frase anglesa. However, although fully resolving zero pronouns often needs discourse context, in some cases, the local context within a sentence gives clues to the inference of the zero pronoun.  En aquest estudi, proposem un mètode d'augmentació de dades que proporciona senyals adicionals de capacitació al model de traducció per aprendre correlacions entre el context local i zero pronons. Ens demostram que el mètode proposat millora significativament la precisió de la traducció pronunciada zero amb experiments de traducció màquina en el domini de conversació.", 'et': 'Jaapani-inglise tõlke puhul on jaapani keeles null-pronounid väljakutseks, kuna mudel peab tulema ja tootma vastava pronouni inglise lause sihtküljel. Kuigi nullpäsuna täielik lahendamine vajab sageli diskursuse konteksti, annab mõnel juhul lauses kohalik kontekst vihjeid nullpäsuna järeldusele. Selles uuringus pakume välja andmete suurendamise meetodi, mis annab tõlkemudelile täiendavaid koolitussignaale, et õppida korrelatsioone kohaliku konteksti ja nullpronoomide vahel. Näitame, et väljapakutud meetod parandab märkimisväärselt nullpooltõlke täpsust masintõlke eksperimentidega vestlusvaldkonnas.', 'ha': "@ item Spelling dictionary A lokacin da za'a iya yin rabo da sifri, ko da yawa, ma'abũcin mazaɓa da mazaɓa, da kuma a cikin wani abu, muhallin lokacin da ke cikin maganar, yana ƙara wa'ura da sifri. Daga wannan lõkaci, Munã buɗa wata hanyor ƙãri ga data, da za'a bãyar da alama masu ƙaranci wa shirin fassarar da za'a sanar da haɗi tsakanin mazaɓa na lokacin da sifire. Tuna nũna cewa metoden da aka buƙata, yana ƙara muhimmin tsarin taƙaitaccen sifiri da taƙaitori masu cikin taƙaitori masu fassarwa na mashine cikin mazaɓa.", 'sk': 'Za prevod japonščine v angleščino predstavlja izziv nič zaimkov v japonščini, saj mora model sklepati in ustvariti ustrezen zaimk na ciljni strani angleškega stavka. Čeprav je za popolno reševanje ničelnih zaimkov pogosto potreben diskurzni kontekst, v nekaterih primerih lokalni kontekst znotraj stavka namiguje na sklep ničelnega zaimka. V tej študiji predlagamo metodo povečanja podatkov, ki zagotavlja dodatne signale usposabljanja za prevajalski model za učenje korelacij med lokalnim kontekstom in ničelnimi zaimki. Pokazali smo, da predlagana metoda bistveno izboljša natančnost ničelnega zaimkovnega prevajanja s poskusi strojnega prevajanja v pogovorni domeni.', 'he': 'עבור התרגום יפני לאנגלית, אפס פרסומות ביפנית יוצרים אתגר, מאחר שהמודל צריך להוציא ומוציא את פרסומת המתאימה בצד המטרה של המשפט האנגלית. בכל אופן, למרות שהפתרון מלא של אפס מילים זקוק לעתים קרובות לקשר דיבורי, במקרים מסוימים, הקשר המקומי בתוך משפט נותן רמזים למסקנה של אפס מילה. במחקר הזה, אנו מציעים שיטת גידול נתונים שמספקת אותות אימונים נוספים למודל התרגום כדי ללמוד קשרים בין הקשר המקומי לאפס פרונומים. אנו מראים שהשיטה המוצעת משפר באופן משמעותי את מדויקת התרגום של אפס מילים עם ניסויים התרגום מכונות בתחום השיחה.', 'bo': 'སྐད་ཡིག་གི་སྐད་ཡིག་ལས་དབྱིན་ཡིག་ཆ་ལ་བཤད་ཀྱི་མིང་ཚིག་ཉེ་ཧ་གོ་ཡིག་ནང་གི་གནད་དོན་ཅིག་བཤད་ཀྱི་ཡོད་པ་རྒྱུ་དང་། ཡིན་ནའང་། རྩིས་འབྲས་ཀྱི་མིང་ཚིག་འདི་ལྟར་སྐབས་ཀྱི་ཁ་ཤས་ཀྱི་དགོས་པ་ཡིན་ནའང་། སྐབས་ལ་ཁ་ཤས་ཀྱི་གནས་སྟངས་ཅིག་ཡིག་ཚིག་ནང་ལ ལྟ་བ་འདིའི་ནང་དུ་ང་ཚོས་གནས་ཚུལ་རྒྱ་བསྐྱེད་ཀྱི་ཐབས་ལམ་ལུགས་གསལ ང་ཚོས་གྲོས་འཆར་བྱས་པའི་ཐབས་ལམ་དེ་ཚོས་མིང་ཚིག་གི་འགྲེལ་བཤད་ཀྱི་བདེན་པར', 'jv': 'Terjamahan neng Japang-kanggo Inggris, Nulah Ngucap ning Japang kuwi ngubah ujak-ujak Nanging, sabên ono sampeyan luwih-luwih nggawe balêr, sapar kang dipunangé disinteksi awak dhéwé, ning piye cah-cah, sampeyan loché nang daftar sapa nyimpen kawit mengkar ngregani uwong. Nang barêng-barêng iki, kéné supoyo tresnaning data ampungan method sing nyenggawe supoyo tarjamahan karo model terjamahan kanggo nggambar nggambar barang sampek tarjamahan karo lokal kontèks lan 0 nggambar. Awak dhéwé éntuk ngéwé ngerasakno nggawe Perintah sing beraksi kanggo nyenggawe perintah kanggo nik nggambar tarjamahan karo perintah panelusuran ning sak conversations.'}
{'en': 'TMU NMT System with Japanese BART for the Patent task of WAT 2021 TMU   NMT  System with  J apanese  BART  for the Patent task of  WAT  2021', 'ar': 'نظام TMU NMT مع BART الياباني لمهمة براءة اختراع WAT 2021', 'pt': 'Sistema TMU NMT com BART japonês para a tarefa de patente do WAT 2021', 'fr': 'Système TMU NMT avec BART japonais pour la tâche de brevet du WAT 2021', 'es': 'Sistema TMU NMT con BART japonés para la tarea de patentes de WAT 2021', 'ja': 'WAT 2021の特許業務のための日本のBARTを備えたTMU NMTシステム', 'zh': 'TMU NMT系统与日本BART同成WAT 2021专利', 'hi': 'WAT 2021 के पेटेंट कार्य के लिए जापानी BART के साथ TMU NMT सिस्टम', 'ru': 'Система TMU NMT с японским BART для патентной задачи WAT 2021', 'ga': 'Córas TMU NMT le BART na Seapáine le haghaidh tasc Paitinne WAT 2021', 'el': 'Σύστημα TMU NMT με ιαπωνικό BART για το έργο ευρεσιτεχνίας του WAT 2021', 'ka': 'TMU NMT სისტემა იაპონური BART-ის პოტენტის დავალებისთვის WAT 2021', 'hu': 'TMU NMT rendszer japán BART-vel a WAT 2021 szabadalmi feladatához', 'kk': 'WAT 2021 патенттік тапсырманың жапон BART жүйесі TMU NMT жүйесі', 'lt': 'TMU NMT sistema su Japonijos BART, skirta 2021 m. WAT patentų uždaviniui atlikti', 'it': 'TMU NMT System con BART giapponese per il compito di brevetto di WAT 2021', 'mt': 'Sistema TMU NMT b’BART Ġappuniż għall-kompitu tal-Privattivi tal-WAT 2021', 'mn': 'TMU NMT System with Japanese BART for the Patent task of WAT 2021', 'mk': 'TMU NMT систем со јапонски BART за задачата на патентот на WAT 2021', 'ms': 'Sistem NMT TMU dengan BART Jepun untuk tugas Paten WAT 2021', 'no': 'TMU NMT- systemet med japansk BART for patentoppgåva av WAT 2021', 'ml': 'ജാപ്പനീസ് ബാര്\u200dട്ടിനുള്ള ടിഎംഎംടി സിസ്റ്റം വാട്ട് 2021-ന്റെ പാതിന്റ് ജോലി', 'ro': 'Sistemul TMU NMT cu BART japonez pentru sarcina de brevetare a WAT 2021', 'sr': 'TMU NMT sistem sa japanskim BART za patentni zadatak WAT 2021.', 'pl': 'System TMU NMT z japońskim BART do zadania patentowego WAT 2021', 'si': 'TMU NMT පද්ධතිය ජාපානි BART එක්ක WAT 2021 ගේ පැටෙන්ට් වැඩේ වෙනුවෙන්.', 'so': 'TMU NMT System with Japanese BART for the Patent task of WAT 2021', 'sv': 'TMU NMT System med japansk BART för patentuppgiften för WAT 2021', 'ta': 'ஜாப்பானிய BART உடன் TMU NMT அமைப்பு', 'ur': 'WAT 2021 کے پیٹینٹ کام کے لئے جاپانی BART کے ساتھ TMU NMT سیسٹم', 'uz': 'Name', 'vi': 'Giao thông TMU NMT với BART Nhật Bản cho công việc sáng chế của WAT 2021', 'da': 'TMU NMT System med japansk BART til patentopgaven af WAT 2021', 'bg': 'ТМУ НМТ Система с японски БАРТ за патентната задача на ВАТ 2021', 'de': 'TMU NMT System mit japanischem BART für die Patentaufgabe von WAT 2021', 'id': 'Sistem TMU NMT dengan BART Jepang untuk tugas Paten dari WAT 2021', 'ko': 'TMU NMT 시스템은 일본 BART와 협력하여 WAT 2021 특허 작업 수행', 'fa': 'سیستم TMU NMT با BART ژاپنی برای کار patent از WAT 2021', 'nl': 'TMU NMT Systeem met Japanse BART voor de Octrooitaak van WAT 2021', 'hr': 'TMU NMT sustav sa japanskim BART za patentni zadatak WAT 2021.', 'tr': "Japonça BART bilen TMU NMT sistemi WAT 2021'iň Patent işi üçin", 'sq': 'TMU NMT System with Japanese BART for the Patent task of WAT 2021', 'af': 'TMU NMT Stelsel met Japaanse BART vir die Patent taak van WAT 2021', 'am': 'ከጃፓንኛ BART ጋር TMU NMT ስርዓት ለWAT 2021 ለPatent ስራ', 'hy': 'ԹՄՄ ՆՄԹ համակարգը ճապոնական Բարթ-ով, որն օգտագործում է պայթենտի աշխատանքի համար', 'az': 'TMU NMT System with Japanese BART for the Patent task of WAT 2021', 'bn': 'ওয়াট ২০১২ সালের প্যাটেন্ট কাজের জন্য জাপানী বার্টের সাথে TMU NMT সিস্টেম', 'bs': 'TMU NMT sustav sa japanskim BART za patentni zadatak WAT 2021.', 'sw': 'Mfumo wa TMU NMT na BART wa Japani kwa ajili ya kazi ya Patients ya WAT 2021', 'ca': 'TMU NMT System with Japanese BART for the Patent task of WAT 2021', 'et': 'TMU NMT süsteem Jaapani BARTiga WAT 2021 patendiülesandeks', 'cs': 'TMU NMT systém s japonským BART pro patentový úkol WAT 2021', 'fi': 'TMU NMT -järjestelmä japanilaisen BART:n kanssa WAT 2021 -patenttitehtävään', 'jv': 'MU NMT System karo BaRT japongan kanggo nggawe patent kanggo WAT 2020 1', 'sk': 'TMU NMT System z japonskim BART za patentno nalogo WAT 2021', 'ha': 'KCharselect unicode block name', 'he': 'מערכת TMU NMT עם BART יפני למשימת הפטנטים של WAT 2021', 'bo': 'TMU NMT མ་ལག་གི་རྒྱ་ནག་གི་BART སྤྱད་ནས་ WAT 2021 ཡི་patent task of the WAT 2021'}
{'en': 'In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. One of the pre-trained models, BART (Lewis et al., 2020), was shown to improve translation accuracy via fine-tuning with bilingual data. However, they experimented only Romanian!English translation using  English BART . In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.', 'ar': 'في هذه الورقة ، نقدم نظام TMU Neural Machine Translation (NMT) المقدم لمهمة براءات الاختراع (الكورية اليابانية والإنجليزية اليابانية) في ورشة العمل الثامنة حول الترجمة الآسيوية (ناكازاوا وآخرون ، 2021). في الآونة الأخيرة ، اقترحت العديد من الدراسات نماذج مدربة مسبقًا لوحدات فك التشفير باستخدام بيانات أحادية اللغة. تم عرض أحد النماذج المدربة مسبقًا ، BART (Lewis et al. ، 2020) ، لتحسين دقة الترجمة من خلال الضبط الدقيق للبيانات ثنائية اللغة. ومع ذلك ، فقد جربوا الترجمة الرومانية فقط الإنجليزية باستخدام BART الإنجليزية. في هذه الورقة ، قمنا بفحص فعالية BART اليابانية باستخدام مجموعة مكتب براءات الاختراع اليابانية 2.0. تشير تجاربنا إلى أن تقنية BART اليابانية يمكنها أيضًا تحسين دقة الترجمة في الترجمات اليابانية الكورية واليابانية والإنجليزية.', 'fr': "Dans cet article, nous présentons notre système de traduction automatique neuronale TMU (NMT) soumis pour la tâche de brevet (coréen japonais et anglais japonais) du 8e atelier sur la traduction asiatique (Nakazawa et al., 2021). Récemment, plusieurs études ont proposé des modèles encodeur-décodeur pré-entraînés utilisant des données monolingues. Il a été démontré que l'un des modèles pré-entraînés, BART (Lewis et al., 2020), améliorait la précision de la traduction grâce à un réglage précis avec des données bilingues. Cependant, ils n'ont expérimenté que le roumain\xa0! Traduction en anglais en utilisant le BART anglais. Dans cet article, nous examinons l'efficacité du BART japonais en utilisant le Corpus 2.0 de l'Office des brevets du Japon. Nos expériences indiquent que le BART japonais peut également améliorer la précision des traductions en coréen japonais et en anglais japonais.", 'es': 'En este artículo, presentamos nuestro sistema de traducción automática neuronal (NMT) TMU presentado para la tarea de patentes (coreano, japonés e inglés japonés) del octavo taller de traducción asiática (Nakazawa et al., 2021). Recientemente, varios estudios propusieron modelos de codificador-decodificador previamente entrenados que utilizan datos monolingües. Se demostró que uno de los modelos previamente entrenados, BART (Lewis et al., 2020), mejora la precisión de la traducción mediante el ajuste fino con datos bilingües. Sin embargo, ¡solo experimentaron rumano! Traducción al inglés con BART en inglés. En este artículo, examinamos la eficacia del BART japonés utilizando el Corpus 2.0 de la Oficina de Patentes de Japón. Nuestros experimentos indican que el BART japonés también puede mejorar la precisión de la traducción en las traducciones de coreano, japonés e inglés japonés.', 'pt': 'Neste artigo, apresentamos nosso sistema TMU Neural Machine Translation (NMT) enviado para a tarefa de Patentes (Japonês Coreano e Japonês Inglês) do 8º Workshop sobre Tradução Asiática (Nakazawa et al., 2021). Recentemente, vários estudos propuseram modelos codificadores-decodificadores pré-treinados usando dados monolíngues. Um dos modelos pré-treinados, o BART (Lewis et al., 2020), demonstrou melhorar a precisão da tradução por meio do ajuste fino com dados bilíngues. No entanto, eles experimentaram apenas a tradução romeno!inglês usando o inglês BART. Neste artigo, examinamos a eficácia do BART japonês usando o Japan Patent Office Corpus 2.0. Nossos experimentos indicam que o BART japonês também pode melhorar a precisão das traduções em coreano japonês e inglês japonês.', 'hi': 'इस पेपर में, हम एशियाई अनुवाद पर 8 वीं कार्यशाला के पेटेंट कार्य (कोरियाई जापानी और अंग्रेजी जापानी) के लिए प्रस्तुत हमारे टीएमयू न्यूरल मशीन ट्रांसलेशन (एनएमटी) सिस्टम को पेश करते हैं (नाकाज़ावा एट अल। हाल ही में, कई अध्ययनों ने मोनोलिंगुअल डेटा का उपयोग करके पूर्व-प्रशिक्षित एन्कोडर-डिकोडर मॉडल का प्रस्ताव दिया। पूर्व-प्रशिक्षित मॉडलों में से एक, BART (लुईस एट अल, 2020), द्विभाषी डेटा के साथ ठीक-ट्यूनिंग के माध्यम से अनुवाद सटीकता में सुधार करने के लिए दिखाया गया था। हालांकि, उन्होंने केवल रोमानियाई प्रयोग किया! अंग्रेजी BART का उपयोग करके अंग्रेजी अनुवाद। इस पेपर में, हम जापान पेटेंट ऑफिस कॉर्पस 2.0 का उपयोग करके जापानी BART की प्रभावशीलता की जांच करते हैं। हमारे प्रयोगों से संकेत मिलता है कि जापानी BART भी कोरियाई जापानी और अंग्रेजी जापानी अनुवाद दोनों में अनुवाद सटीकता में सुधार कर सकता है।', 'ja': '本稿では、第8回アジア翻訳ワークショップ（ Nakazawa et al., 2021 ）の特許業務（韓国語、日本語、英語、日本語）のために提出されたTMU神経機械翻訳（ NMT ）システムについて紹介する。最近、いくつかの研究では、モノリンガルデータを使用した事前に訓練されたエンコーダデコーダモデルが提案されています。事前に訓練されたモデルの一つであるBART （ Lewis et al., 2020 ）は、バイリンガルデータを用いた微調整を介して翻訳精度を向上させることが示された。しかし、彼らはルーマニア語のみを実験した！英語BARTを用いた英訳。本稿では、日本特許庁コーパス2.0を用いた日本語BARTの有効性について検討する。私たちの実験では、日本語のBARTは韓国語の日本語訳と英語の日本語訳の両方で翻訳の精度を向上させることもできることが示されています。', 'zh': '本文引为第8届亚洲译研讨会专利(韩语日语与英语日语)交TMU神经机器翻译(NMT)系统(Nakazawa等,2021)。 近者,几项讲求用单语数预训练编码器 - 解码器模形。 其一先训者 BART(Lewis 等,2020 年)证可因双语数以重译准确性。 但试之罗马尼亚语! 用英语 BART 英语翻译。 本文,究日本专利局用日本专利局语料库2.0日本BART有效性。 吾实验之明,日语BART可以崇韩语日语英语日语翻译之准确性。', 'ru': 'В этой статье мы представляем нашу систему нейронного машинного перевода (НМП) TMU, представленную для патентной задачи (корейский японский и английский японский) 8-го семинара по азиатскому переводу (Nakazawa et al., 2021). Недавно в нескольких исследованиях были предложены предварительно обученные модели кодировщик-декодер с использованием одноязычных данных. Было показано, что одна из предварительно обученных моделей, BART (Lewis et al., 2020), повышает точность перевода за счет точной настройки с двуязычными данными. Однако они экспериментировали только на румынском языке!Английский перевод с использованием английского языка BART. В этой статье мы исследуем эффективность японского языка BART с использованием японского патентного ведомства Corpus 2.0. Наши эксперименты показывают, что японский БАРТ также может улучшить точность перевода как в корейском японском, так и в английском японском переводе.', 'ga': 'Sa pháipéar seo, tugaimid isteach ár gcóras TMU Neural Machine Translation (NMT) a cuireadh isteach don tasc Paitinne (Seapáinis na Cóiré agus Béarla na Seapáine) den 8ú Ceardlann ar Aistriúchán na hÁise (Nakazawa et al., 2021). Le déanaí, mhol roinnt staidéar samhlacha réamh-oilte ionchódóra-díchódóra ag baint úsáide as sonraí aonteangacha. Léiríodh go bhfuil ceann de na samhlacha réamhoilte, BART (Lewis et al., 2020), chun cruinneas an aistriúcháin a fheabhsú trí mhionchoigeartú a dhéanamh ar shonraí dátheangacha. Mar sin féin, rinne siad tástáil ar aistriúchán Béarla Rómáinis amháin ag baint úsáide as Béarla BART. Sa pháipéar seo, scrúdaímid éifeachtacht BART na Seapáine ag baint úsáide as Japan Paitinn Office Corpus 2.0. Léiríonn ár dturgnaimh gur féidir le BART na Seapáine feabhas a chur ar chruinneas aistriúcháin in aistriúcháin Cóiréis Seapánacha agus Béarla araon.', 'el': 'Στην παρούσα εργασία, παρουσιάζουμε το σύστημα Νευρικής Μηχανικής Μετάφρασης (NMT) που υποβλήθηκε για το έργο Διπλώματος ευρεσιτεχνίας (Κορεατικά Ιαπωνικά και Αγγλικά Ιαπωνικά) του 8ου Εργαστηρίου Ασιατικής Μετάφρασης (Νακαζάβα κ.α., 2021). Πρόσφατα, αρκετές μελέτες πρότειναν προ-εκπαιδευμένα μοντέλα κωδικοποιητών-αποκωδικοποιητών χρησιμοποιώντας μονογλωσσικά δεδομένα. Ένα από τα προ-εκπαιδευμένα μοντέλα, το BART (Lewis et al., 2020), αποδείχθηκε ότι βελτιώνει την ακρίβεια της μετάφρασης μέσω της τελειοποίησης με δίγλωσσα δεδομένα. Ωστόσο, πειραματίστηκαν μόνο ρουμανικά! Αγγλική μετάφραση χρησιμοποιώντας την αγγλική BART. Σε αυτή την εργασία, εξετάζουμε την αποτελεσματικότητα του ιαπωνικού BART χρησιμοποιώντας το Ιαπωνικό Γραφείο Διπλωμάτων Ευρεσιτεχνίας Corpus 2.0. Τα πειράματά μας δείχνουν ότι τα ιαπωνικά μπορούν επίσης να βελτιώσουν την ακρίβεια της μετάφρασης τόσο σε κορεατικές ιαπωνικές όσο και σε αγγλικές ιαπωνικές μεταφράσεις.', 'it': "In questo articolo presentiamo il nostro sistema di traduzione automatica neurale TMU (NMT) presentato per il compito di brevetto (coreano giapponese e inglese giapponese) dell'8° Workshop sulla traduzione asiatica (Nakazawa et al., 2021). Recentemente, diversi studi hanno proposto modelli di encoder-decoder pre-addestrati utilizzando dati monolingue. Uno dei modelli pre-addestrati, BART (Lewis et al., 2020), ha dimostrato di migliorare l'accuratezza della traduzione tramite la messa a punto con i dati bilingui. Tuttavia, hanno sperimentato solo rumeno! Traduzione inglese usando l'inglese BART. In questo articolo esaminiamo l'efficacia del BART giapponese utilizzando Japan Patent Office Corpus 2.0. I nostri esperimenti indicano che il giapponese BART può anche migliorare l'accuratezza della traduzione sia in giapponese coreano che in giapponese inglese.", 'hu': 'Ebben a tanulmányban bemutatjuk a TMU Neural Machine Translation (NMT) rendszerünket, amelyet az Ázsiai Fordításról szóló 8. Workshop on Asian Translation (Nakazawa et al., 2021) szabadalmi feladatára benyújtottak (koreai japán és angol japán). A közelmúltban több tanulmány előre képzett útmérő-dekódoló modelleket javasolt egynyelvű adatok felhasználásával. Az egyik előre képzett modell, a BART (Lewis et al., 2020) kimutatták, hogy a kétnyelvű adatok finomhangolásával javítja a fordítási pontosságot. De csak románul kísérleteztek! Angol fordítás angol BART használatával. Ebben a tanulmányban a japán BART hatékonyságát vizsgáljuk a Japan Patent Office Corpus 2.0 alkalmazással. Kísérleteink azt mutatják, hogy a japán BART a koreai japán és angol japán fordítások pontosságát is javíthatja.', 'lt': 'Šiame dokumente pristatome mūsų TMU neurologinių mašinų vertimo (NMT) sistemą, pateiktą 8-ojo seminaro „Azijos vertimas“ (Nakazawa ir kt., 2021 m.). Neseniai keliuose tyrimuose siūlomi iš anksto parengti kodavimo kodavimo modeliai, naudojantys vienkalbius duomenis. Įrodyta, kad vienas iš iš iš anksto apmokytų modelių, BART (Lewis et al., 2020), gerina vertimo tikslumą tiksliau derinant su dvikalbiais duomenimis. Tačiau jie eksperimentavo tik rumunų! English translation using English BART.  Šiame dokumente nagrinėjame Japonijos BART veiksmingumą naudojant Japonijos patentų biurą Corpus 2.0. Mūsų eksperimentai rodo, kad Japonijos BART taip pat gali pagerinti vertimo tikslumą Korėjos japonų ir anglų kalbomis.', 'kk': 'Бұл қағазда, Азия аудармасының 8- ші жұмысының патенттік тапсырмасына (Корея жапон және жапон тілінде) жүйеңізді таңдап береміз. Жуырда бірнеше зерттеулерді бірнеше тілдік деректерді қолдану үшін алдын- оқылған кодер- декодер үлгілерін қолданылады. БаRT (Lewis et al., 2020) бағдарламасының бірінші оқылған үлгілері, екі тілі деректерді түзету арқылы аудармалардың дұрыстығын жақсарту үшін көрсетілді. Бірақ олар тек Румынша тәжірибеледі! Ағылшын BART қолданып ағылшын тілінің аудармасы. Бұл қағазда, Япония патенттер офисы Корпус 2.0 қолданатын Япония BART әрекетін тексереміз. Біздің тәжірибелеріміз жапон BART және Корея жапон және ағылшын жапон аудармаларында аудармалардың дұрыстығын жасай алады деп белгіледі.', 'ms': 'Dalam kertas ini, kami memperkenalkan sistem TMU Neural Machine Translation (NMT) kami dihantar untuk tugas Patent (Korea Jepun dan Inggeris Jepun) dari Workshop ke-8 tentang Translation Asia (Nakazawa et al., 2021). Baru-baru ini, beberapa kajian mencadangkan model pengekod-dekoder terlatih-terlatih menggunakan data monobahasa. Salah satu model pra-dilatih, BART (Lewis et al., 2020), dipaparkan untuk meningkatkan ketepatan terjemahan melalui penyesuaian dengan data dua bahasa. However, they experimented only Romanian! Terjemahan Bahasa Inggeris menggunakan BART Bahasa Inggeris. Dalam kertas ini, kami memeriksa kegunaan BART Jepun menggunakan Pejabat Paten Jepun Corpus 2.0. Eksperimen kami menunjukkan bahawa BART Jepun juga boleh meningkatkan ketepatan terjemahan dalam kedua-dua terjemahan Jepun Korea dan bahasa Inggeris Jepun.', 'ka': 'ამ დოკუნში ჩვენ ჩვენი TMU ნეიროლური მაქსინური განაცვლის (NMT) სისტემა, რომელიც აზიანეთის განაცვლის 8-ი სამუშაო სამუშაო სამუშაო (კორეული იაპონეთი და იაპონეთი წაპონეთ მიმდინარე, რამდენიმე კვლევები მოძლევა მონოლენგური მონაცემების გამოყენებით წინასწარმოვიდგინეთ კოდერების რეკოდერების მოდელები. ერთი მოდელეში, BART (Lewis et al., 2020) გამოჩვენებულია, რომ უფრო უფრო მეტად გადაწყვეტა წესიერება ორიენგური მონაცემებით. მაგრამ, ისინი ექსპერიმენტირებენ მხოლოდ პრომინული! English translation using English BART. ჩვენ ამ დოკუნში წაპონეთის BART-ის ეფექტიურობას გამოყენებთ წაპონეთის პეტენტის კოპუს 2.0. ჩვენი ექსპერიმენტები აჩვენებს, რომ იაპონური BART შეუძლია ასევე უფრო უფრო უფრო მეტადება თავისწორეული იაპონური და ანგლიური წონური', 'mk': 'In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021).  Неодамна, неколку студии предложија предобучени модели за декодирање на кодери користејќи монојазични податоци. Еден од предобучените модели, БАРТ (Lewis и други, 2020), покажа дека ја подобрува преведувачката точност преку финетизирање со двојјазични податоци. Сепак, тие експериментираа само романски! Англиски превод користејќи англиски BART. Во овој весник ја испитуваме ефикасноста на Јапонската Барт користејќи Јапонска патентска канцеларија Корпус 2.0. Нашите експерименти покажуваат дека јапонскиот Барт, исто така, може да ја подобри преведувачката точност на корејски јапонски и англиски јапонски преведувања.', 'mt': 'F’dan id-dokument, aħna nintroduċu s-sistema tagħna tat-Traduzzjoni tal-Magni Newrali tat-TMU (NMT) ippreżentata għall-kompitu tal-Privattivi (Ġappuniż Korean u Ġappuniż Ingliż) tat-8 Workshop dwar it-Traduzzjoni Asjatika (Nakazawa et al., 2021). Dan l-aħħar, diversi studji pproponu mudelli ta’ kodifikatur-dekoder imħarrġa minn qabel bl-użu ta’ dejta monolingwi. Wieħed mill-mudelli mħarrġa minn qabel, BART (Lewis et al., 2020), intwera li jtejjeb il-pre ċiżjoni tat-traduzzjoni permezz ta’ aġġustament fin b’dejta bilingwi. Madankollu, esperimentaw biss ir-Rumen! Traduzzjoni bl-Ingliż bl-użu tal-BART bl-Ingliż. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0.  Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.', 'ml': 'ഈ പത്രത്തില്\u200d ഞങ്ങള്\u200d നമ്മുടെ ടിഎം യു ന്യൂറല്\u200d മെഷീന്\u200d പരിഭാഷ (NMT) സിസ്റ്റമിനെ പരിചയപ്പെടുത്തുന്നു. ഏഷ്യയിലെ പരിഭാഷണത്തിന്റെ എട്ടാം വര്\u200dക്കാര്\u200dക്ക് വേണ അടുത്തുതന്നെ, പല പഠനങ്ങളും മുമ്പ് പരിശീലിക്കപ്പെട്ട കോഡെര്\u200d ഡെക്കോഡെര്\u200d മോഡലുകള്\u200d പരിശോധിച്ചു മുമ്പ് പരിശീലിക്കപ്പെട്ട മോഡലുകളില്\u200d ഒരാള്\u200d ബാര്\u200dട്ടി (ലെവിസ് et al., 2020), രണ്ടു ഭാഷ വിവരങ്ങള്\u200d മുഖേന പരിഭാഷപ്രകാരം മെച്ചപ്പെടു എന്നാലും, അവര്\u200d റൊമാനിയന്\u200d മാത്രം പരീക്ഷിച്ചു! ഇംഗ്ലീഷ് BART ഉപയോഗിച്ച് ഇംഗ്ലീഷ് പരിഭാഷ ഈ പത്രത്തില്\u200d, ജപ്പാന്\u200d പാപ്റ്റന്റ് ഓഫീസ് 2.0 ഉപയോഗിച്ച് ജപ്പാന്\u200d ഭാര്\u200dട്ടിന്റെ പ്രഭാവം പരിശോധിക്കുന് Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.', 'mn': 'Энэ цаасан дээр бид TMU мэдрэлийн машин хөгжүүлэх (NMT) системийг Азийн хөгжүүлэх (Nakazawa et al., 2021) 8-р ажлын Патент даалгаврын ажлын төлөө хийсэн. Саяхан олон судалгаанууд нэг хэлний өгөгдлийг ашиглаж сургалтын өмнө сургалтын кодлогч-декодлогч загварыг санал болгосон. БАРТ (Lewis et al., 2020) дасгал сургалтын нэг загвар нь хоёр хэлний өгөгдлийг сайжруулахын аргаар хөгжүүлсэн орнуудын тодорхойлолтыг сайжруулах боломжтой болсон. Гэхдээ тэд зөвхөн Румын зөвхөн туршилт хийсэн. Англи хэлний БАРТ ашиглаж Англи хэлний орчуулалт. Энэ цаасанд бид Японы Патент Оффис Корпус 2.0 ашиглан Япон Бартын үр дүнг шалгаж байна. Бидний туршилтууд Япон БАРТ нь Корейн Япон болон Англи Япон хэлбэрээр орчуулах зөв байдлыг илүү сайжруулж чадна.', 'ro': 'În această lucrare, prezentăm sistemul nostru TMU Neural Machine Translation (NMT) depus pentru sarcina de brevetare (coreeană japoneză și engleză japoneză) al celui de-al 8-lea atelier de traducere asiatică (Nakazawa et al., 2021). Recent, mai multe studii au propus modele pre-instruite de encoder-decoder utilizând date monolingve. Unul dintre modelele pre-instruite, BART (Lewis et al., 2020), a demonstrat că îmbunătățește acuratețea traducerii prin ajustarea fină cu date bilingve. Cu toate acestea, au experimentat doar români! Traducere în engleză folosind limba engleză BART. În această lucrare, examinăm eficacitatea BART japoneză folosind Japan Patent Office Corpus 2.0. Experimentele noastre indică faptul că BART japoneză poate îmbunătăți precizia traducerii atât în traducerile japoneze coreene, cât și în engleză japoneză.', 'sr': 'U ovom papiru predstavljamo naš sistem Neuralnog prevoda (NMT) TMU koji je predan za patentni zadatak (Korejski japanski i engleski japanski) 8. radionice o azijskom prevodu (Nakazawa et al., 2021). Nedavno, nekoliko studija predložilo je predobučene modele kodera-dekodera koristeći monojezičke podatke. Jedan od predobučenih modela, BART (Lewis et al., 2020), pokazuje se da je poboljšao preciznost prevoda putem fino-tuniranja sa dvojezičkim podacima. Međutim, eksperimentirali su samo rumunski! Engleski prevod koristeći engleski BART. U ovom papiru pregledamo učinkovitost japanskog BART korištenja Japanske patentne kancelarije korpusa 2.0. Naši eksperimenti ukazuju na to da japanski BART takođe može poboljšati preciznost prevoda na korejskim japanskim i engleskim japanskim prevodima.', 'pl': 'W niniejszym artykule przedstawiamy nasz system neuronowego tłumaczenia maszynowego TMU (NMT) zgłoszony do zadania patentowego (koreański japoński i angielski japoński) VIII Warsztatu Tłumaczenia Azjatyckiego (Nakazawa et al., 2021). Ostatnio kilka badań zaproponowało wstępnie przeszkolone modele kodera-dekodera z wykorzystaniem danych jednojęzycznych. Wykazano, że jeden z wstępnie przeszkolonych modeli, BART (Lewis et al., 2020), poprawia dokładność tłumaczenia poprzez dostosowanie danych dwujęzycznych. Jednak eksperymentowali tylko po rumuńsku! Tłumaczenie angielskie przy użyciu angielskiego BART. W niniejszym artykule badamy skuteczność japońskiego BART przy użyciu Japońskiego Urzędu Patentowego Corpus 2.0. Nasze eksperymenty wskazują, że japoński BART może również poprawić dokładność tłumaczenia zarówno w koreańskim, jak i angielskim tłumaczeniu japońskim.', 'no': 'I denne papiret introduserer vi vårt TMU Neural Machine Translation (NMT) system som er sendt til patentoppgåva (Koreansk og engelsk japansk) av 8. arbeidsområdet på Asian Translation (Nakazawa et al., 2021). Nyleg har fleire studier foreslått først trengte koderingsmodeller med monospråk- data. Ein av dei først trengte modelane, BART (Lewis et al., 2020), vert vist til å forbedra omsetjingskokretasjonen ved å finne opp med bilinguelt data. Men dei eksperimenterte berre romnisk! Engelsk omsetjing med engelsk BART. I denne papiret undersøker vi effektiviteten av japansk BART med Japan Patent Office Corpus 2.0. Våre eksperimenter tyder på at Japansk BART kan også forbedra omsetjingsakratitet i både Koreanske og engelske japanske omsetjingar.', 'so': 'Qoraalkan waxaan ku soo bandhignaa nidaamka tarjumaadda ee TMU Neural Machine (NMT) ee loo soo dhiibay shaqada bukaanka (Koreaniya Japanese iyo Ingiriis) oo ah 8aad Workshop on Translation Aasiya (Nakazawa et al., 2021). Muddii u dhowaad, waxbarasho badan ayaa lagu soo jeeday modelal koordiyuhu uu ku isticmaalayo macluumaad luuqad ah. Mid ka mid ah modellada horay loo tababaray, BART (Lewis et al., 2020), waxaa looga muujiyey inuu kordhiso saxda turjumista via fine-tuning with labada luuqadood. Si kastaba ha ahaatee, waxay jirrabeen Romanian oo keliya! Turjumista Ingiriis ee isticmaalaya Ingiriis BART. Warqadan waxaynu baaritaan waxyaabaha ay japaniya BART ku leeyihiin isticmaalka xafiiska bukaanka Japan Korpus 2.0. Imtixaanadayada waxay muujinayaan in Jabanees BART sidoo kale uu kordhin karo saxda turjumaadda ee Koreaniya iyo turjumaadda Ingiriis ee Jabanees.', 'ta': 'இந்த காக்கியத்தில், நாம் எங்கள் டிஎம்யு நெருக்கர் இயந்திரம் மொழிபெயர்ப்பு (NMT) அமைப்பை குறிப்பிடுகிறோம் எட்டாவது ஆசிய மொழிபெயர்ப்பின் மொழிபெயர்ப்ப சமீபத்தில், பல ஆராய்ச்சிகள் மோனோலிங்கல் தரவை பயன்படுத்தி முன் பயிற்சி குறியீட்டு மாதிரிகளை பரிந்துரைக்கப முன்பயிற்சிக்கப்பட்ட மாதிரிகளில் ஒன்று, BART (லீவி et al., 2020), இரு மொழிகள் தரவுடன் மொழிபெயர்ப்பு சரியான தெளிவை மேம்படுத்துவதற்கு  ஆனாலும், அவர்கள் ரோமானியன் மட்டும் சோதனைப்படுத்தினார்கள்! English translation using English BART.  இந்த காகிதத்தில், நாம் ஜப்பான் பாதுகாப்பு அலுவலகம் 2.0 பயன்படுத்தி ஜாப்பான் பார்ட் விளைவுகளை பரிசோதி எங்கள் சோதனைகள் குறிப்பிடுகிறது ஜப்பானிய பார்ட் மொழிபெயர்ப்பு சரியை மேலும் மொழிபெயர்ப்பு தெளிவாக்க முடி', 'si': 'මේ පැත්තට, අපි අපේ TMU න්\u200dයුරල් මැෂින් අවවාදය (NMT) පද්ධතිය ප්\u200dරදේශය (කෝරියාන් ජාපානි සහ ඉංග්\u200dරීසි ජාපානි වලින්) 8ම වැඩසටහන් අ අවසානයෙන්, විශේෂ අධ්\u200dයානය ප්\u200dරීක්ෂණා කරලා තියෙන්නේ ප්\u200dරීක්ෂණා කරපු කෝඩෝර් ඩිකොඩර් ම ප්\u200dරධානය කරපු මොඩේල් එකක්, BART (Luis et al., 2020යි), පෙන්වන්න පුළුවන් විදිහට පරිවර්තන ක්\u200dරියාත්මක විශේෂය කරන්න පුළුවන ඒත් ඔවුන් රෝමානියාන් විතරයි පරීක්ෂා කරලා! ඉංග්\u200dරීසි BART භාවිතානය කරන්න ඉංග්\u200dරීසි භාවිතානය. මේ පත්තරේ අපි ජාපාන් පැටෙන්ට් කාර්පුස් 2.0 භාවිතා කරන්න ජාපාන් බාර්ට් එකේ ප්\u200dරශ්ණතාවක් පරීක් අපේ පරීක්ෂණය පෙන්වන්නේ ජාපාන් බාර්ට් වලින් කෝරියාන් ජාපානි වලින් ඉංග්\u200dරීසි ජාපානි වලි', 'sv': 'I denna uppsats presenterar vi vårt TMU Neural Machine Translation (NMT) system som lämnats in för patentuppgiften (koreansk japansk och engelsk japansk) i 8:e Workshop on Asian Translation (Nakazawa et al., 2021). Nyligen har flera studier föreslagit färdigutbildade encoder-avkodarmodeller med enspråkiga data. En av de förberedda modellerna, BART (Lewis et al., 2020), visade sig förbättra översättningens noggrannhet genom finjustering med tvåspråkiga data. Men de experimenterade bara rumänska! Engelska översättning med engelska BART. I denna uppsats undersöker vi effektiviteten av japansk BART med hjälp av Japan Patent Office Corpus 2.0. Våra experiment visar att japanska BART också kan förbättra översättningens noggrannhet i både koreanska japanska och engelska japanska översättningar.', 'ur': 'اس کاغذ میں ہم نے اپنی TMU نیورال ماشین ترجمہ (NMT) سیستم کو آسیا ترجمہ کے 8م کارشاپ کے لئے پیش کیا ہے۔ اچھا، بہت سی تحقیقات پیش آموزش کی پیش آموزش دی گئی ایک زبان دکھانے کے مطابق ایک کوڈر-ڈیکوڈر موڈل پیش کیے گئے ہیں. پہلے آموزش کی مدل میں سے ایک BART (Lewis et al., 2020) کو دکھایا گیا تھا کہ دو زبان اولاد کے ذریعہ مطابق ترجمہ کی دقیقیقیت کو بہتر کرنے کے لئے۔ لیکن وہ صرف رومانی آزمائش کرتے ہیں۔ انگلیسی BART کے مطابق انگلیسی ترجمہ. اس کاغذ میں، ہم جاپانی پٹینٹ ऑفیس کورپوس 2.0 کے مطابق جاپانی برٹ کی فعالیت کی تحقیق کرتے ہیں. ہماری آزمائش نشان دیتی ہے کہ جاپانی BART بھی کوریا ژاپنی اور انگلیسی ژاپنی ترجمہ میں ترجمہ دقیق بھی بہتر کر سکتا ہے.', 'uz': "Bu hujjatda biz Asiy tarjima daftarining 8- toʻplami (Nakazawa et al, 2021) uchun Patent vazifasi (Koriya Yaponiya va Ingliz Yaponcha) uchun tarjima qiladigan TMU Neural Mashine tarjima (NMT) tizimini ko'rsamiz. Yaqinda ko'pchilik o'rganishlar monolingual maʼlumot yordamida bir necha ta'minlovchi kodekoder modellarini talab qiladi. Birinchi taʼminlovchi modellardan biri BART (Lewis et al., 2020), ikkita tillar yordamida tarjima tayyorligini oshirish mumkin. Lekin, улар фақат Румий синовчи бўлган. Inglizcha tarjima qilindiName In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0.  Bizning imtiyozlarimiz, Yaponcha BART xitoycha Japoniya va Ingliz Japoniya tarjimalarining ikkita tarjimalarini oshirish imkoniyatini oshirish mumkin.", 'vi': 'Trong tờ giấy này, chúng tôi xin giới thiệu hệ thống dịch máy thần kinh TMU (NMB) được giao cho công việc sáng chế (Nhật Bản Hàn Quốc và Anh Quốc) của 8th Workshop in Asian Translation (Nakazawa et al., 2021). Gần đây, nhiều nghiên cứu đề xuất mô hình mã hóa đã được đào tạo. Một trong những mô hình được huấn luyện trước, BART (Lewis et al., 2020) đã được cho thấy cải thiện độ chính xác của dịch qua việc tinh chỉnh hai thứ bằng độ chính xác. Tuy nhiên, họ chỉ thử nghiệm Romani! Dịch bằng tiếng Anh BART. Trong tờ giấy này, chúng tôi kiểm tra hiệu quả của Nhật BART sử dụng Nhật bản sáng chế Corpus 2.0. Những thí nghiệm của chúng tôi cho thấy rằng bên Nhật BART cũng có thể cải thiện độ chính xác dịch trong cả tiếng Nhật Triều Tiên và Anh.', 'bg': 'В настоящата статия представяме нашата система за невронен машинен превод (НМТ), подадена за патентна задача (корейски японски и английски японски) на 8-ма работилница по азиатски превод (Наказава и др., 2021). Наскоро няколко проучвания предлагат предварително обучени модели кодер-декодер, използващи едноезични данни. Доказано е, че един от предварително обучените модели подобрява точността на превода чрез фина настройка с двуезични данни. Но те експериментираха само румънски! Английски превод с помощта на английски BART. В настоящата статия изследваме ефективността на японския БАРТ, използвайки Японското патентно ведомство Корпус 2.0. Нашите експерименти показват, че японският може да подобри точността на превода както в корейския, така и в английския японски превод.', 'nl': 'In dit artikel introduceren we ons TMU Neural Machine Translation (NMT) systeem dat is ingediend voor de Patenttaak (Koreaans Japans en Engels Japans) van 8e Workshop on Asian Translation (Nakazawa et al., 2021). Onlangs hebben verschillende studies voorgetrainde encoder-decoder modellen voorgesteld met eentalige gegevens. Een van de voorgetrainde modellen, BART (Lewis et al., 2020), bleek de vertaalnauwkeurigheid te verbeteren door finetuning met tweetalige gegevens. Ze experimenteerden echter alleen met Roemeens! Engelse vertaling met behulp van het Engels BART. In dit artikel onderzoeken we de effectiviteit van Japanse BART met behulp van Japan Patent Office Corpus 2.0. Uit onze experimenten blijkt dat Japans BART ook vertaalnauwkeurigheid kan verbeteren in zowel Koreaans Japans als Engels Japans vertalingen.', 'hr': 'U ovom papiru predstavljamo naš sistem za neurološki prevod (NMT) TMU koji je podignut za zadatak patenta (korejski japanski i engleski japanski) 8. radionice o azijskom prevodu (Nakazawa et al., 2021). Nedavno je nekoliko ispitivanja predložilo predobučene modele kodera-dekodera koristeći monojezičke podatke. Jedan od predobučenih modela, BART (Lewis et al., 2020), pokazalo je kako bi poboljšao preciznost prevođenja putem ispravnog korištenja s dvojezičkim podacima. Međutim, oni su eksperimentirali samo rumunski! English translation using English BART. U ovom papiru pregledamo učinkovitost japanskog BART korištenja Japanskog patentnog ureda korpusa 2.0. Naši eksperimenti ukazuju na to da japanski BART također može poboljšati preciznost prevoda na korejskim japanskim i engleskim japanskim prevodima.', 'da': 'I denne artikel introducerer vi vores TMU Neural Machine Translation (NMT) system indsendt til patentopgaven (koreansk japansk og engelsk japansk) på 8. workshop om asiatisk oversættelse (Nakazawa et al., 2021). For nylig foreslog flere undersøgelser præ-trænede encoder-dekoder modeller ved hjælp af ensprogede data. En af de forududdannede modeller, BART (Lewis et al., 2020), viste sig at forbedre oversættelsesnøjagtigheden ved finjustering med tosprogede data. Men de eksperimenterede kun rumænsk! Engelsk oversættelse ved hjælp af engelsk BART. I denne artikel undersøger vi effektiviteten af japansk BART ved hjælp af Japan Patent Office Corpus 2.0. Vores eksperimenter viser, at japansk BART også kan forbedre oversættelsesnøjagtigheden i både koreansk japansk og engelsk japansk oversættelse.', 'de': 'In diesem Beitrag stellen wir unser TMU Neural Machine Translation (NMT) System vor, das für die Patentaufgabe (Koreanisch Japanisch und Englisch Japanisch) des achten Workshops zur asiatischen Übersetzung (Nakazawa et al., 2021) eingereicht wurde. Kürzlich schlugen mehrere Studien vortrainierte Encoder-Decoder-Modelle vor, die monolinguale Daten verwenden. Eines der vortrainierten Modelle, BART (Lewis et al., 2020), verbesserte die Übersetzungsgenauigkeit durch Feinabstimmung mit zweisprachigen Daten. Allerdings experimentierten sie nur rumänisch! Englische Übersetzung mit Hilfe von BART. In diesem Beitrag untersuchen wir die Wirksamkeit des japanischen BART unter Verwendung des japanischen Patentamts Corpus 2.0. Unsere Experimente zeigen, dass Japanisch BART auch die Übersetzungsgenauigkeit sowohl in Koreanisch-Japanisch- als auch in Englisch-Japanisch-Übersetzungen verbessern kann.', 'ko': '본고에서 제8회 아시아번역세미나(Nakazawa 등, 2021년)의 특허 임무(아사히와 영일)를 위해 제출한 TMU 신경기계번역(NMT) 시스템을 소개한다.최근 일부 연구에서는 단어 데이터를 사용하는 예비 트레이닝 인코더인 디코더 모델을 제시했다.BART(Lewis et al., 2020)는 사전 훈련을 거친 모델로 이중 언어 데이터를 미세하게 조정하여 번역 정밀도를 높일 수 있다.그러나 그들은 루마니아어만 시험했다!영어 BART를 사용하여 영어로 번역합니다.본고에서 우리는 일본 특허국 어료고 2.0을 이용하여 일본 BART의 유효성을 검증한다.우리의 실험은 일본어 BART도 한일과 영일 번역의 정확성을 높일 수 있음을 나타냈다.', 'id': 'In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021).  Baru-baru ini, beberapa studi mengusulkan model koder-dekoder yang dilatih-dilatih menggunakan data monobahasa. Salah satu model prapelatih, BART (Lewis et al., 2020), ditunjukkan untuk meningkatkan akurasi terjemahan melalui fine-tuning dengan data dua bahasa. Namun, mereka hanya eksperimen Rumania! Terjemahan Inggris menggunakan BART Inggris. Dalam kertas ini, kami memeriksa efektivitas dari BART Jepang menggunakan Korpus Paten Jepang 2.0. Eksperimen kami menunjukkan bahwa BART Jepang juga dapat meningkatkan akurasi terjemahan dalam terjemahan Jepang Korea dan bahasa Inggris Jepang.', 'fa': 'در این کاغذ، سیستم ترجمه ماشین عصبی TMU (NMT) ما را معرفی می\u200cکنیم که برای کار patent (ژاپنی ژاپنی و ژاپنی ژاپنی کوریه) هشتم کارشناسی در ترجمه آسیا (Nakazawa et al., 2021) فرستاده شده است. اخیرا، چند مطالعه پیش از آموزش مدل\u200cهای رمزبندی پیش آموزش داده شده با استفاده از داده\u200cهای یک زبان. یکی از مدلهای پیش آموزش داده شده، BART (Lewis et al., 2020) نشان داده شد که دقیق ترجمه را با اطلاعات دو زبان بهتر کند. ولی اونا فقط رومانی آزمایش کردند! ترجمه انگلیسی با استفاده از BART انگلیسی. در این کاغذ، ما موثیت BART ژاپنی را با استفاده از اداره پتانس ژاپن Corpus 2.0 تحقیق می کنیم. آزمایشات ما نشان می دهند که BART ژاپنی هم می تواند دقیق ترجمه را در ترجمه ژاپنی ژاپنی و ژاپنی ژاپنی بهتر کند.', 'tr': "Bu kagyzda, biz TMU NMT-iň näral Maşynyň terjimelerini (NMT) sistemamyzy Aziýa terjimelerinde 8-nji Iýpet bellenilýär (Nakazawa et al., 2021). Soňky wagtlar, birnäçe öňki bilim öňki arkalanmış kodeýan nusgalary monodil maglumaty ullanýar. BART (Lewis et al., 2020) öňündeki bilim sistemasynda terjime edilen hatlaryň dogrylygyny ýüzeltmek üçin görkezildi. Ýöne olar diňe rumunça synanyşdylar! Iňlisçe BART ulanan Iňlisçe terjime edildi. Bu kagyzda Japon Patent Ofis Korpus 2.0 ulanarak Japon BART'yň etkinliýetini barlap bardyk. Biziň deneylerimiz Japon BART-yň hem Koreýan we Iňlis dilinde terjime edilmegiň dogrylygyny gowurap biljekdigini aýdýar.", 'af': 'In hierdie papier, ons introduseer ons TMU Neurale Masjien Vertaling (NMT) stelsel voorgestel vir die Patent-taak (Koreaanse Japanse en Engelse Japanse) van die 8de Werkshop op Asies Vertaling (Nakazawa et al., 2021). Onlangs het verskeie studie voorgestel vooraf-opgelei koder-dekoder modele gebruik van monolinglike data. Een van die voorafgevorderde modele, BART (Lewis et al., 2020), was vertoon om vertaling presisiteit te verbeter deur fyn-tuning met twee-tale data. Maar hulle het slegs Romaniese eksperimenteer! Engels vertaling gebruik Engels BART. In hierdie papier, ons ondersoek die effektiviteit van japanse BART deur Japan Patent Office Corpus 2.0 te gebruik. Ons eksperimente wys dat Japanse BART ook vertaling presies kan verbeter in Koreaanse Japanse en Engelse vertalings.', 'sw': 'Katika karatasi hii, tunautambulisha mfumo wetu wa Tafsiri ya Mashine ya Kifaransa (NMT) uliotolewa kwa ajili ya kazi ya Wazapani (Kijapani na Kiingereza) wa warsha ya 8 kuhusu Tafsiri ya Asia (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data.  Moja ya mifano ya zamani ya mafunzo, BART (Lewis et al., 2020), ilionyesha kuongeza uhakika wa tafsiri kwa kutumia taarifa za lugha mbili. Hata hivyo, walijaribu WaRomania pekee! Tafsiri ya Kiingereza kwa kutumia Kiingereza BART. Katika karatasi hii, tunachunguza ufanisi wa BART wa Japani kwa kutumia Ofisi ya Wagonjwa wa Japan 2.0. Majaribio yetu yanaonyesha kuwa BART ya Japani inaweza pia kuboresha ukweli wa tafsiri katika tafsiri za Kijapani na Kiingereza.', 'sq': 'Në këtë letër, ne prezantojmë sistemin tonë TMU Neural Machine Translation (NMT) të paraqitur për detyrën e patentave (Korean Japanese and English Japanese) të Workshop të 8-të mbi Translation Asian (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. Një nga modelet e paratrajnuar, BART (Lewis et al., 2020), u tregua se përmirëson saktësinë e përkthimit nëpërmjet rregullimit me të dhënat dygjuhëse. Megjithatë, ata eksperimentuan vetëm rumun! Përkthimi anglez duke përdorur anglisht BART. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0.  Eksperimentet tona tregojnë se BART japonez mund gjithashtu të përmirësojë saktësinë e përkthimit në përkthimet japoneze dhe angleze.', 'az': "Bu kańüńĪzda TMU Neural Machine Translation (NMT) sistemimizi Aziya √áeviri (Nakazawa et al., 2021) 8. Workshop on 8. Workshop for the Patent Task (Korean Japanese and English Japanese) il…ô t…ôblińü edirik. Son zamanlarda, bir ne√ß…ô t…ôhsil …ôvv…ôlc…ô t…ôhsil edilmiŇü koder-dekoder modell…ôri monodil veril…ônl…ôrd…ôn istifad…ô edir. √Ėn t…ôhsil edilmiŇü modell…ôrd…ôn biri BART (Lewis et al., 2020), iki dil m…ôlumatlarńĪ il…ô t…ôhsil edilm…ôsi il…ô t…ôhsil edilm…ôsi √ľ√ß√ľn t…ôhsil edilmiŇüdir. Ancaq onlar yalnńĪz Rumun t…ôcr√ľb…ôl…ôrini t…ôcr√ľb…ô etdil…ôr! ńįngiliz…ô BART vasit…ôsil…ô ńįngiliz…ô terc√ľm…ôsi. Bu kańüńĪzda Japon Patent Office Corpus 2.0 vasit…ôsil…ô Japon BART'un etkinlińüini incidirik. Bizim t…ôcr√ľb…ôl…ôrimiz Japonca BART h…ôm√ßin in Kore Japonca v…ô ńįngilizce √ß…ôtinlikl…ôrd…ô d…ô d…ôyiŇüiklik ed…ô bil…ôr.", 'am': 'በዚህ ገጽ፣ በእስያ ትርጉም ላይ ስምንተኛው workshop (ናkazawa et al., 2021) ለPatent ሥራ (የቆሬያዊ ጃፓንኛ እና እንግሊዘኛ) የTMU ኔural machine ትርጉም (NMT) ስርዓታችንን እናሳውቃለን፡፡ በቅርብ ጊዜ ብዙዎች ተማርከቶች በሞሎልቋል ዳታ የተጠቃሚ የencoder-decoder models በመጠቀም ይገልጻሉ፡፡ የቀድሞው ተማሪ ሞዴል BART (Lewis et al., 2020) በሁለት ቋንቋዎች ዳታ በመጠቀም ትርጉም እርግጠኛ ማድረግ ተገልጦአል፡፡ ነገር ግን ሮማኒያን ብቻ ፈተናቸው፡፡ እንግሊዘኛ ትርጉም BART በተጠቃሚ በዚህ ገጽ የጃፓን ፓንቲ ኮርፓስ 2.0 በመጠቀም የጃፓን BART ጥያቄን መርምረናል፡፡ ፈተናዎቻችን የጃፓን BART እና በቆሬያዊ ጃፓንኛ እና ኢንጂፓን ትርጓሜዎች ሁለቱም ትርጓሜዎችን ማሻሻል እንደሚችል ያሳያል፡፡', 'hy': 'Այս թղթի մեջ մենք ներկայացնում ենք մեր ԹՄԱ-ի նյարդային մեքենայի թարգմանման (ՆՄԹ) համակարգը, որը ներկայացվել է Ասիական թարգմանման 8-րդ աշխատասենյակի համար (Նակազավա և այլն., 2021 թ.): Վերջերս, մի քանի ուսումնասիրություններ առաջարկեցին նախապատրաստված կոդեր-կոդեր մոդելներ, որոնք օգտագործում են միալեզու տվյալներ: Պարզվեց, որ նախապատրաստված մոդելներից մեկը, Բարթը (Leouis et al., 2020), բարելավում է թարգմանման ճշգրտությունը երկլեզու տվյալների հետ կապված կերպով: Այնուամենայնիվ, նրանք փորձեցին միայն ռոմաներեն: Անգլերեն թարգմանություն օգտագործելով անգլերեն Բարտ: Այս թղթի մեջ մենք ուսումնասիրում ենք Ճապոնական Բարտի արդյունավետությունը՝ օգտագործելով Ճապոնիայի փաստաբանական գրասենյակ Կորպուս 2.0: Մեր փորձարկումները ցույց են տալիս, որ ճապոնական Բարթը կարող է նաև բարելավել թարգմանման ճշգրտությունը կորեացի ճապոնացի և անգլերենի թարգմանություններում:', 'bn': 'এই কাগজটিতে আমরা আমাদের টিএমউ নিউরাল মেশিন অনুবাদ (এনএমটি) সিস্টেমের পরিচয় প্রদান করেছি প্যাটেন্ট কাজের (কোরিয়ান জাপানী এবং ইংরেজী জাপানীয়) যা এশিয়ার সম্প্রতি বেশ কিছু গবেষণা প্রশিক্ষণের পূর্বে প্রশিক্ষিত এনকোডার-ডেকোডার মডেল প্রস্তাব করা হয়েছে মোনোলিভা প্রাক্তন প্রশিক্ষিত মডেলের মধ্যে একটি বার্ট (লেউস এন্ট এল ২০২০), দুই ভাষার তথ্যের মাধ্যমে অনুবাদের সঠিকভাবে উন্নত করা হয়েছে। তবে, তারা শুধুমাত্র রোমানীয় পরীক্ষা করেছে! ইংরেজি অনুবাদ বিআরটি ব্যবহার করে। এই পত্রিকায় আমরা জাপানের প্যাটেন্ট অফিস ব্যবহার করে জাপানী বার্টের কার্যকরী পরীক্ষা করি। আমাদের পরীক্ষা নির্দেশ করছে যে জাপানী বার্টি কোরিয়ান জাপানী এবং ইংরেজি ভাষায় অনুবাদের সঠিকভাবে উন্নত করতে পারে।', 'ca': "En aquest article introduïm el nostre sistema de traducció de màquines neuronals (NMT) submetit a la tasca de brevets (japonès coreans i anglès) de la 8ª taller sobre traducció asiàtica (Nakazawa et al., 2021). Recentment, molts estudis van proposar models de codificador pré-entrenats que utilitzaven dades monolingües. Un dels models pré-entrenats, BART (Lewis et al., 2020), va demostrar que millora la precisió de la traducció ajustant-se a les dades bilingües. No obstant això, només experimentaven rumà! traducció anglesa amb BART anglès. En aquest paper examinem l'eficacia del BART japonès utilitzant el Japan Patent Office Corpus 2.0. Els nostres experiments indican que el BART japonès també pot millorar la precisió de la traducció tant en coreans japonès com en anglès.", 'bs': 'U ovom papiru predstavljamo naš sistem neurološkog prevoda (NMT) TMU koji je predan za zadatak patenta (korejski japanski i engleski japanski) 8. radionice o azijskom prevodu (Nakazawa et al., 2021). Nedavno, nekoliko ispitivanja predložilo je predobučene modele kodera-dekodera koristeći monojezičke podatke. Jedan od predobučenih modela, BART (Lewis et al., 2020), pokazuje se da će poboljšati preciznost prevođenja putem fine-tuning sa dvojezičkim podacima. Međutim, oni su eksperimentirali samo rumunski! Engleski prevod koristeći engleski BART. U ovom papiru pregledavamo učinkovitost japanskog BART korpusa 2.0. Naši eksperimenti ukazuju na to da japanski BART također može poboljšati preciznost prevoda na korejskim japanskim i engleskim prevodima.', 'cs': 'V tomto článku představujeme náš systém TMU Neural Machine Translation (NMT) předložený k patentovému úkolu (korejská japonština a anglická japonština) osmého workshopu o asijském překladu (Nakazawa et al., 2021). V poslední době několik studií navrhlo předem trénované modely kodéru-dekodéru s využitím jednojjazyčných dat. Bylo ukázáno, že jeden z předškolených modelů BART (Lewis et al., 2020) zlepšuje přesnost překladu díky jemnému ladění s dvojjazyčnými daty. Nicméně experimentovali pouze rumunsky! Anglický překlad pomocí angličtiny BART. V tomto článku zkoumáme efektivitu japonského BART s využitím japonského patentového úřadu Corpus 2.0. Naše experimenty ukazují, že japonština BART může také zlepšit přesnost překladu v korejském japonštině a anglickém japonštině.', 'fi': 'Tässä artikkelissa esittelemme TMU Neural Machine Translation (NMT) -järjestelmämme, joka on toimitettu 8. Aasian kääntämisen työpajan patenttitehtävään (Nakazawa et al., 2021). Viime aikoina useissa tutkimuksissa on ehdotettu esikoulutettuja kooderi-dekooderimalleja, joissa käytetään monikielistä dataa. Yhden esikoulutetun mallin, BART (Lewis et al., 2020), osoitettiin parantavan käännöksen tarkkuutta hienosäätämällä kaksikielisiä tietoja. Mutta he kokeilivat vain romaniaa! Englanninkielinen käännös käyttäen englantia BART. Tässä artikkelissa tarkastellaan japanilaisen BART:n tehokkuutta Japanin patenttiviraston Corpus 2.0:n avulla. Kokemuksemme osoittavat, että japanin BART voi myös parantaa käännösten tarkkuutta sekä korean japanin että englannin japanin käännöksissä.', 'et': 'Käesolevas töös tutvustame TMU neuroalse masintõlke (NMT) süsteemi, mis on esitatud 8. Aasia tõlke seminari patendiülesandeks (Korea jaapani ja inglise jaapani keeles) (Nakazawa et al., 2021). Hiljuti pakuti mitmes uuringus välja eelnevalt koolitatud kodeerija-dekooderi mudelid, mis kasutasid ühekeelseid andmeid. Üks eelkoolitud mudelitest, BART (Lewis et al., 2020), tõendas tõlke täpsust kahekeelsete andmetega peenhäälestuse kaudu. Kuid nad katsetasid ainult rumeenia keelt! Inglise tõlge inglise keeles BART. Käesolevas töös uurime Jaapani BART efektiivsust, kasutades Jaapani Patendiameti Corpus 2.0. Meie eksperimendid näitavad, et jaapani BART võib parandada ka tõlke täpsust nii korea jaapani kui ka inglise jaapani tõlketes.', 'he': 'בעיתון הזה, אנחנו מציגים את מערכת המכונה העצבית TMU (NMT) שלנו שנשלחה למשימת הפטנטים (יפנית וקוריאנית) של Workshop 8 על התרגום אסיאטי (Nakazawa et al., 2021). לאחרונה, מספר מחקרים הציעו מודלים קודם-קודם מאומנים מראש בשימוש בנתונים מונולשונים. אחד מהדוגמנים המאמנים מראש, BART (Lewis et al., 2020), הוכח לשפר את מדויקת התרגום דרך התאים עם נתונים שתיים שפתיים. עם זאת, הם ניסו רק רומני! English translation using English BART.  בעיתון הזה, אנחנו בודקים את היעילות של BART היפני באמצעות משרד פטנטים יפני קורפוס 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.', 'sk': 'V tem prispevku predstavljamo sistem TMU nevral strojnega prevajanja (NMT), ki je bil predložen za nalogo patenta (korejska japonščina in angleščina japonščina) 8. delavnice o azijskem prevajanju (Nakazawa et al., 2021). V zadnjem času je več študij predlagalo vnaprej usposobljene modele kodirnikov-dekoderjev z uporabo enojezičnih podatkov. Eden od vnaprej usposobljenih modelov, BART (Lewis et al., 2020), je izboljšal natančnost prevajanja prek finega nastavitve z dvojezičnimi podatki. Vendar so eksperimentirali samo romunsko! Angleški prevod z uporabo angleškega BART. V tem prispevku preučujemo učinkovitost japonskega BART z uporabo japonskega patentnega urada Corpus 2.0. Naši eksperimenti kažejo, da lahko japonski BART izboljša natančnost prevodov v korejski japonski in angleški japonski prevodi.', 'jv': 'Nang mapun iki, kéné gunakake sistem tanggal (NMT) sing nyimpen kanggo nggawe patent kanggo nganggo dolanan sing wis pitik (japongan karo japongan ingkang Korea) ning wis asai Workspace nang pitik-terjamahan asita (nakutawa et al, 2020). plug-in-action metadata Nanging, dheweke entuk-ingkang rumane mungkin ! Terjamahan Inggris nang nggambar LPRT. Nang pemilih iki, awak dhéwé isih bakal nggawe barang japon barang nggambar barang patent Ofis 2.0 nggambar barang japon. Awak dhéwé éntuk ngerti barang, barang Hapon barang bisa nggawe turuné sak pangan anyar tentang kanggo barang Hapon karo Perancis Inggris.', 'ha': "A cikin wannan takardan, Munã introduce na tsarin TMU Tarjima na Neural Mashine (NMT) wanda aka yi wajen aikin Nagon Farawa (Koriya Japanese da Ingiriya) na 8. workworkload on Asian Translate (Nakazawa et al., 2021). A yanzu, wasu fitina masu amfani da data na monoli'ura da aka yi amfani da shiryoyin kode-kode-kode. Babu wani daga shiryoyin ayuka da aka yi amfani da shi gaba-tuned, BArT (Lewi et al., 2020), aka nuna shi to improve translation uranci through fin-tuning with data bilin languages. To, a cikin fitinar ba su zamo ba fãce runtsũma. @ item Spelling dictionary Ga wannan takardan, Munã jarraba aikin BAT na Jamapi da Corbas 2.0. Kayan jarrabayanmu, yana madaidaita cẽwa BERT za'a iya samar da fassarar taƙaita a cikin fassarar Yahũdiya da Ingiriya.", 'bo': 'ང་ཚོའི་ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོའི་TMU Neural Machine Translation (NMT)རིགས་འདིས་ཨ་རེ་ཤི་ཡིག་སྣོད་ཀྱི་ལས་འགུལ་ལ་བཤད་པ་སོགས། འཕྲལ་ཁམས་ཀྱི་ལྟ་བ་དག་གི་སྔོན་སྒྲིག་འཛུགས་པའི་སྔོན་སྒྲིག་འཛུགས་པའི་ཨིན་ཀོ་ཌིར་སྤྱོད་པའི་མ་དཔེ་གཞུང་མང་པོ་ཞི སྔོན་གྲངས་འཛིན་པའི་མིག་གཟུགས་རིས་གཅིག་ནི་BART (Lewis et al., 2020)ནང་དུ་སྔོན་གྲངས་སྒྲིག ཡིན་ནའང་། ཁོང་ཚོས་རོ་མ་ཡིན་ལས་བརྟག་ཞིབ་བྱེད་པ་རེད། English translation using English BART. ང་ཚོས་ཤོག་བྱང་འདིའི་ནང་དུ་ཉེ་ཧོང་གི་ཡུལ་སྤྱོད་པའི་ཉེ་ཧོང་གི་BART་གི་ལྕགས་འབྱུང་བ་ཞིག ང་ཚོའི་བརྟག་ཞིག་གིས་ཉེ་ཧོང་གི་རྨང་གཞིའི་ནང་ནས་སྐད་ཡིག་ཆ་ཉེ་ཧོང་དང་ཨིན་ཇིའི་སྐད་ཡིག'}
{'en': 'IITP at WAT 2021 : System description for English-Hindi Multimodal Translation Task IITP  at  WAT  2021: System description for  E nglish- H indi Multimodal Translation Task', 'ar': 'IITP at WAT 2021: وصف النظام لمهمة الترجمة الإنجليزية-الهندية متعددة الوسائط', 'es': 'IITP en el WAT 2021: descripción del sistema para la tarea de traducción multimodal inglés-hindi', 'pt': 'IITP no WAT 2021: Descrição do sistema para tarefa de tradução multimodal inglês-hindi', 'fr': 'IITP au WAT 2021\xa0: Description du système pour la tâche de traduction multimodale anglais-hindi', 'ja': 'WAT 2021におけるIITP ：英語-ヒンディー語マルチモーダル翻訳タスクのシステム説明', 'zh': '2021年WATIITP:英语 - 印地语多模样译职之统', 'hi': 'WAT 2021 में IITP: अंग्रेजी-हिंदी मल्टीमॉडल अनुवाद कार्य के लिए सिस्टम विवरण', 'ru': 'IITP на WAT 2021: Описание системы для задачи мультимодального перевода с английского на хинди', 'ga': 'IITP ag WAT 2021: Cur síos ar an gcóras don Tasc Aistriúcháin Ilmhódúil Béarla-Hiondúis', 'ka': 'IITP WAT 2021- ში: ინგლისური- ჰინდური მულტიმოდიალური განსაგულისხმების სისტემის გამოსახულება', 'hu': 'IITP a WAT 2021-en: Rendszerleírás az angol-hindi multimodális fordítási feladathoz', 'el': 'Περιγραφή Συστήματος για Πολυmodal Translation Task στα Αγγλικά-Χίντι', 'it': 'IITP a WAT 2021: descrizione del sistema per il compito di traduzione multimodale inglese-hindi', 'lt': 'IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task', 'kk': 'WAT 2021 жылы IITP: Ағылшын- хинди көптеген аудару тапсырмасының жүйелік сипаттамасы', 'mk': 'IITP на WAT 2021: Системски опис за англиско-хиндиско мултимодилно преведување задача', 'ms': 'IITP pada WAT 2021: Keterangan sistem untuk Tugas Terjemahan BerBermodal Inggeris-Hindi', 'ml': 'WAT 2021-ല്\u200d IITP: ഇംഗ്ലീഷ്- ഹിന്ദി- Multimodal Translation Task സിസ്റ്റം വിശദീകരണം', 'mt': 'IITP fl-WAT 2021: Deskrizzjoni tas-sistema għal Kompitu ta’ Traduzzjoni Multimodali Ingliż-Indjan', 'mn': 'WAT 2021 онд IITP: Англи-Хинди олон моделийн орчуулагчийн ажлын системийн тодорхойлолт', 'no': 'IITP ved WAT 2021: Systemskildring for engelsk- hindisk multimodal oversettelsesoppgåve', 'pl': 'IITP na WAT 2021: Opis systemu dla multimodalnego tłumaczenia angielsko-hindi', 'ro': 'IITP la WAT 2021: Descrierea sistemului pentru sarcina de traducere multimodală engleză-hindi', 'sr': 'IITP na WAT 2021: Sistemski opis za engleski-hindski multimodalni prevodni zadatak', 'so': 'IITP at WAT 2021: System description for Ingiriis- Hindi Multimodal Translation Task', 'sv': 'IITP på WAT 2021: Systembeskrivning för engelsk-hindi multimodal översättningsuppgift', 'si': 'WAT 2021 දී IITP: ඉංග්\u200dරීසිය- හින්දි ගොඩමොඩාල් භාෂාව වැඩකට පද්ධති විවෘත', 'ta': 'WAT 2021-ல் IITP: ஆங்கிலம்- Hindi Multimodal மொழிபெயர்ப்பு பணிக்கு System description', 'ur': 'WAT 2021 پر IITP: انگلیسی-ہندی Multimodal Translation Task کے لئے سیسٹم سفارش', 'vi': 'File tráo đổi vCalendar Comment', 'uz': 'WAT 2021- yilda IITP: Inglizcha- Hindi Multimodal tarjima vazifa uchun System description', 'bg': 'Описание на системата за английско-хинди мултимодална преводаческа задача', 'da': 'IITP på WAT 2021: Systembeskrivelse for engelsk-hindi multimodal oversættelsesopgave', 'nl': 'IITP op WAT 2021: Systeembeschrijving voor Nederlands-Hindi multimodale vertaaltaak', 'hr': 'IITP na WAT 2021: Sistemski opis za engleski-hindski multimodalni prevodni zadatak', 'de': 'IITP bei WAT 2021: Systembeschreibung für multimodale Übersetzungsaufgaben Englisch-Hindi', 'ko': 'WAT 2021 IITP: 영어 - 인도어 다중모드 번역 작업에 대한 시스템 설명', 'id': 'IITP di WAT 2021: Deskripsi sistem untuk Tugas Terjemahan Multimodal Inggris-Hindi', 'fa': 'IITP در WAT 2021: توصیف سیستم برای تابع ژنرال انگلیسی-هندی چندین مدل', 'sw': 'IITP kwenye WAT 2021: Maelezo ya mfumo kwa ajili ya Tafsiri ya Kiingereza-Hindi Multimodal', 'af': 'IITP na WAT 2021: Stelsel beskrywing vir Engels- Hindi Multimodal Vertaling Taak', 'sq': 'IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task', 'tr': 'IITP we WAT 2021-nji ýylda: Iňlisçe-Hindi Mulmodal terjime Görevi', 'hy': 'IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task', 'bs': 'IITP na WAT 2021: Sistemski opis za engleski-hindski multimodalni prevodni zadatak', 'az': 'WAT 2021-də IITP: İngilizce-Hindi çoxlu çeviri işi üçün sistem təfsiləsi', 'bn': '২০১২ সালে উইটটিপি: ইংরেজি- হিন্দি মাল্টিমোডাল অনুবাদের কাজের জন্য সিস্টেম বর্ণনা', 'am': 'በWAT 2021: System description for English- Hindi Multimodal Translation Task', 'et': 'IITP WAT 2021: inglise-hindi mitmeliigilise tõlke ülesande süsteemi kirjeldus', 'ca': 'IITP al WAT 2021: Descripció del sistema per a la tasca de traducció multimodal anglès-hindí', 'cs': 'IITP na WAT 2021: Popis systému pro multimodální překlad anglicko-hindštiny', 'fi': 'IITP WAT 2021: Englannin ja hindin multimodaalisen käännöksen järjestelmän kuvaus', 'he': 'IITP ב WAT 2021: תיאור מערכת עבור משימה תרגום multimodal אנגלי-הינדי', 'ha': 'IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task', 'bo': 'IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task', 'sk': 'IITP na WAT 2021: Opis sistema za angleško-hindijsko večmodalno prevajalsko nalogo', 'jv': 'IIBP nang WAT 2020 1: System description kanggo Inggris-endi Multimodal translation task'}
{'en': 'Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility. However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. One way to overcome this is to use the information from other modalities if available. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system. Multimodal information can help the NMT system to improve the  translation  by removing  ambiguity  on some phrases or words. We participate in the 8th Workshop on Asian Translation (WAT-2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50  BLEU points  for Evaluation and Challenge subset, respectively.', 'pt': 'A tradução automática neural (NMT) é uma tecnologia de tradução automática predominante hoje em dia devido à sua flexibilidade treinável de ponta a ponta. No entanto, o NMT ainda luta para traduzir corretamente em configurações de poucos recursos, especificamente em pares de idiomas distantes. Uma maneira de superar isso é usar as informações de outras modalidades, se disponíveis. A ideia é que, apesar das diferenças nas línguas, tanto os falantes da língua de origem quanto a de destino vejam a mesma coisa e a representação visual da origem e do destino seja a mesma, o que pode auxiliar positivamente o sistema. As informações multimodais podem ajudar o sistema NMT a melhorar a tradução, removendo a ambiguidade de algumas frases ou palavras. Participamos do 8º Workshop sobre Tradução Asiática (WAT - 2021) para tarefa de tradução multimodal inglês-hindi e alcançamos 42,47 e 37,50 pontos BLEU para o subconjunto Avaliação e Desafio, respectivamente.', 'ar': 'الترجمة الآلية العصبية (NMT) هي تقنية ترجمة آلية سائدة في الوقت الحاضر بسبب مرونتها القابلة للتدريب من طرف إلى طرف. ومع ذلك ، لا يزال NMT يكافح من أجل الترجمة بشكل صحيح في الأماكن منخفضة الموارد على وجه التحديد في أزواج اللغات البعيدة. تتمثل إحدى طرق التغلب على هذا في استخدام المعلومات من الأساليب الأخرى إذا كانت متوفرة. الفكرة هي أنه على الرغم من الاختلافات في اللغات ، يرى كل من متحدثي اللغة المصدر والهدف نفس الشيء والتمثيل المرئي لكل من المصدر والهدف هو نفسه ، مما يمكن أن يساعد النظام بشكل إيجابي. يمكن أن تساعد المعلومات متعددة الوسائط نظام NMT على تحسين الترجمة عن طريق إزالة الغموض في بعض العبارات أو الكلمات. نشارك في ورشة العمل الثامنة حول الترجمة الآسيوية (WAT - 2021) لمهمة الترجمة الإنجليزية-الهندية متعددة الوسائط وحققنا 42.47 و 37.50 نقطة BLEU للمجموعة الفرعية للتقييم والتحدي ، على التوالي.', 'es': 'La traducción automática neuronal (NMT) es una tecnología de traducción automática predominante hoy en día debido a su flexibilidad capacitable de principio a fin. Sin embargo, NMT todavía tiene dificultades para traducir correctamente en entornos de bajos recursos, específicamente en pares de idiomas distantes. Una forma de superar esto es utilizar la información de otras modalidades si están disponibles. La idea es que, a pesar de las diferencias en los idiomas, tanto el idioma de origen como el de destino ven lo mismo y la representación visual de la fuente y el destino es la misma, lo que puede ayudar positivamente al sistema. La información multimodal puede ayudar al sistema NMT a mejorar la traducción al eliminar la ambigüedad en algunas frases o palabras. Participamos en el 8º Workshop on Asian Translation (WAT - 2021) para la tarea de traducción multimodal inglés-hindi y logramos 42.47 y 37.50 puntos BLEU para el subconjunto Evaluación y Desafío, respectivamente.', 'fr': "La traduction automatique neuronale (NMT) est une technologie de traduction automatique prédominante de nos jours en raison de sa flexibilité de formation de bout en bout. Cependant, NMT a encore du mal à traduire correctement dans les environnements à faibles ressources, en particulier sur les paires de langues éloignées. Une façon de remédier à ce problème est d'utiliser les informations provenant d'autres modalités, le cas échéant. L'idée est que malgré les différences entre les langues, les locuteurs de la langue source et de la langue cible voient la même chose et la représentation visuelle de la source et de la cible est la même, ce qui peut aider positivement le système. Les informations multimodales peuvent aider le système NMT à améliorer la traduction en supprimant toute ambiguïté sur certaines phrases ou certains mots. Nous participons au 8e atelier sur la traduction asiatique (WAT - 2021) pour la tâche de traduction multimodale anglais-hindi et obtenons 42,47 et 37,50 points BLEU pour le sous-ensemble Evaluation et Challenge, respectivement.", 'zh': '神经机器翻译(NMT)者,今之主导地位机器翻译术也,以其有端到端可练灵活性。 然NMT犹难于资源匮乏之中正译,特于远对。 克此一法,用他信息(若有言)。 此心也,虽言语有异,但源言语使用者所见同,而言语与目同,此可以极助统也。 多模态信息,可以消短语单词之歧义NMT助其统改译。 与第八届亚洲译研讨会(WAT - 2021),施于英语 - 印地语多模式,评估与挑战子集各得42.4737.50 BLEU分。', 'ja': 'ニューラル・マシン・トランスレーション（ NMT ）は、エンドツーエンドのトレーニング可能な柔軟性のため、今日では主流の機械翻訳技術です。しかし、NMTは遠隔言語ペアに特化した低リソース設定では、依然として適切な翻訳に苦労しています。これを克服する1つの方法は、利用可能な場合、他の方法からの情報を使用することです。言語の違いにもかかわらず、ソースとターゲットの両方の言語の話者は同じものを見ており、ソースとターゲットの両方の視覚的表現は同じであり、システムを肯定的に支援することができるという考え方です。マルチモーダル情報は、いくつかのフレーズまたは単語の曖昧さを取り除くことによって、NMTシステムが翻訳を改善するのに役立ちます。英語-ヒンディー語マルチモーダル翻訳タスクのための第8回アジア翻訳ワークショップ（ WAT - 2021 ）に参加し、評価とチャレンジのサブセットでそれぞれ42.47と37.50のBLEUポイントを達成します。', 'hi': 'न्यूरल मशीन ट्रांसलेशन (एनएमटी) आजकल एक प्रमुख मशीन अनुवाद तकनीक है क्योंकि इसके अंत-से-अंत ट्रेन योग्य लचीलेपन के कारण। हालांकि, एनएमटी अभी भी विशेष रूप से दूर की भाषा जोड़े पर कम संसाधन सेटिंग्स में ठीक से अनुवाद करने के लिए संघर्ष करता है। इसे दूर करने का एक तरीका यह है कि यदि उपलब्ध हो तो अन्य तरीकों से जानकारी का उपयोग करें। विचार यह है कि भाषाओं में अंतर के बावजूद, स्रोत और लक्ष्य भाषा बोलने वाले दोनों एक ही चीज को देखते हैं और स्रोत और लक्ष्य दोनों का दृश्य प्रतिनिधित्व समान है, जो सिस्टम की सकारात्मक सहायता कर सकता है। मल्टीमॉडल जानकारी एनएमटी सिस्टम को कुछ वाक्यांशों या शब्दों पर अस्पष्टता को हटाकर अनुवाद में सुधार करने में मदद कर सकती है। हम अंग्रेजी-हिंदी बहुआयामी अनुवाद कार्य के लिए एशियाई अनुवाद (डब्ल्यूएटी - 2021) पर 8 वीं कार्यशाला में भाग लेते हैं और मूल्यांकन और चुनौती सबसेट के लिए क्रमशः 42.47 और 37.50 BLEU अंक प्राप्त करते हैं।', 'ru': 'Нейронный машинный перевод (NMT) является преобладающей технологией машинного перевода в настоящее время из-за его сквозной обучаемой гибкости. Тем не менее, NMT все еще пытается правильно переводить в условиях нехватки ресурсов, особенно на удаленных языковых парах. Одним из способов преодоления этой проблемы является использование информации, полученной с помощью других методов, если таковая имеется. Идея заключается в том, что, несмотря на различия в языках, носители как исходного, так и целевого языков видят одно и то же, а визуальное представление как исходного, так и целевого языков является одинаковым, что может оказать позитивную помощь системе. Мультимодальная информация может помочь системе НМТ улучшить перевод, устранив двусмысленность в некоторых фразах или словах. Мы участвуем в 8-м семинаре по азиатскому переводу (WAT - 2021) для задачи мультимодального перевода на английском и хинди и получаем 42,47 и 37,50 баллов BLEU для подмножества «Оценка» и «Вызов», соответственно.', 'ga': 'Is teicneolaíocht aistriúcháin mheaisín cheannasach é Neural Machine Translation (NMT) sa lá atá inniu ann mar gheall ar a solúbthacht in oiliúint ó cheann ceann go ceann. Mar sin féin, bíonn deacrachtaí ag NMT aistriúchán ceart a dhéanamh i suíomhanna íseal-acmhainne go sonrach ar phéirí teanga i bhfad i gcéin. Bealach amháin chun é seo a shárú ná an fhaisnéis ó mhodhanna eile a úsáid má tá sé ar fáil. Is é an smaoineamh atá ann, in ainneoin na ndifríochtaí i dteangacha, go bhfeiceann cainteoirí foinse agus sprioctheanga araon an rud céanna agus is ionann léiriú amhairc na foinse agus na sprice araon, rud a chuideoidh go dearfach leis an gcóras. Is féidir le faisnéis ilmhódach cuidiú leis an gcóras NMT an t-aistriúchán a fheabhsú trí athbhrí a bhaint as frásaí nó focail áirithe. Glacaimid páirt san 8ú Ceardlann ar Aistriúchán na hÁise (WAT - 2021) don tasc aistriúcháin ilmhódúil Béarla-Hiondúis agus bainimid amach 42.47 agus 37.50 pointe BLEU don fho-thacar Meastóireachta agus Dúshláin, faoi seach.', 'ka': 'ნეიროლური მაქინის გადატვირთვა მაგრამ, NMT უკვე ძალიან ძალიან ძალიან ძალიან ძალიან მსგავსი რესურსის პარამეტრებში განსაკუთრებულია განსხვავებას განსხვავებული ენის ზოგების ერთი გზა, რომ გადავიწყოთ ეს არის სხვა მოდულებებიდან ინფორმაციის გამოყენება. იდეა იყო, რომ ენაში განსხვავებულების მაგრამ, მსგავსი და მსგავსი ენაზე მუშაობელი იგივე რაღაც ხედავთ, და ვიზუალური გამოსახულება მსგავსი და მისახულება იგივეა, რომელიც შეუძლია და Multimodal ინფორმაცია შეიძლება NMT სისტემას დახმარება, რომ გადაწყვეტის შესახებ, რამდენიმე ფრაზების ან სიტყვების ამბიდვიტურებას წაშლა. ჩვენ 8-ი სამუშაო სამუშაო აზიანეთის შეცვლაზე (WAT - 2021) ინგლისური-ჰინდის მულტიმოდელური შეცვლაზე დავაწყებთ და 42,47 და 37,50 BLEU წერტილების გასაუმუშაო და გავამუშაო სამუშაო წერ', 'hu': 'A neurális gépi fordítás (NMT) manapság egyik legjelentősebb gépi fordítási technológia, mivel a teljes körű képzésre alkalmas rugalmasság. Azonban az NMT továbbra is küzd a megfelelő fordítással alacsony erőforrású beállításokban, különösen távoli nyelvpárokban. Ennek egyik megoldásának egyik módja, ha más módszerekből származó információkat használunk, amennyiben rendelkezésre állnak. Az elképzelés az, hogy a nyelvkülönbségek ellenére mind a forrás, mind a célnyelv beszélői ugyanazt látják, mind a forrás, mind a cél vizuális ábrázolása ugyanaz, ami pozitívan segítheti a rendszert. A multimodális információk segíthetik az NMT rendszert a fordítás javításában azáltal, hogy eltávolítják a kétértelműséget bizonyos kifejezésekben vagy szavakban. Részt veszünk az angol-hindi multimodális fordítási feladatokra vonatkozó 8. ázsiai fordítási workshopon (WAT - 2021), és 42,47, illetve 37,50 BLEU pontot érünk el az Értékelés és a Kihívás alcsoportban.', 'el': 'Η Νευρική Μηχανική Μετάφραση (NMT) είναι μια κυρίαρχη τεχνολογία μηχανικής μετάφρασης στις μέρες μας λόγω της πλήρους εκπαιδευόμενης ευελιξίας της. Ωστόσο, το NMT εξακολουθεί να αγωνίζεται να μεταφράσει σωστά σε ρυθμίσεις χαμηλού πόρου ειδικά σε μακρινά γλωσσικά ζεύγη. Ένας τρόπος για να ξεπεραστεί αυτό είναι η χρήση των πληροφοριών από άλλες μεθόδους, εφόσον είναι διαθέσιμες. Η ιδέα είναι ότι παρά τις διαφορές στις γλώσσες, τόσο οι ομιλητές της πηγής όσο και του στόχου βλέπουν το ίδιο πράγμα και η οπτική αναπαράσταση τόσο της πηγής όσο και του στόχου είναι η ίδια, γεγονός που μπορεί να βοηθήσει θετικά το σύστημα. Οι πολυμορφικές πληροφορίες μπορούν να βοηθήσουν το σύστημα να βελτιώσει τη μετάφραση αφαιρώντας ασάφειες σε ορισμένες φράσεις ή λέξεις. Συμμετέχουμε στο 8ο Εργαστήριο Ασιατικής Μετάφρασης (για την εργασία της πολυτροπικής μετάφρασης Αγγλικά-Χίντι) και επιτυγχάνουμε πόντους 42.47 και 37.50 για την Αξιολόγηση και την Πρόκληση αντίστοιχα.', 'it': "Neural Machine Translation (NMT) è oggi una tecnologia di traduzione automatica predominante grazie alla sua flessibilità end-to-end trainable. Tuttavia, NMT fatica ancora a tradurre correttamente in impostazioni a basso contenuto di risorse, in particolare su coppie di lingue lontane. Un modo per superare questo problema è utilizzare le informazioni provenienti da altre modalità, se disponibili. L'idea è che, nonostante le differenze nelle lingue, sia i parlanti di lingua di origine che di destinazione vedono la stessa cosa e la rappresentazione visiva sia della fonte che del target è la stessa, il che può aiutare positivamente il sistema. Le informazioni multimodali possono aiutare il sistema NMT a migliorare la traduzione eliminando l'ambiguità su alcune frasi o parole. Partecipiamo all'8° Workshop sulla Traduzione Asiatica (WAT - 2021) per il compito di traduzione multimodale inglese-hindi e raggiungiamo rispettivamente 42,47 e 37,50 punti BLEU per il sottoinsieme Valutazione e Sfida.", 'lt': 'Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility.  Tačiau NMT vis dar stengiasi tinkamai išversti mažai išteklių turinčias aplinkybes, konkrečiai nuotolinėms kalbų poroms. Vienas iš būdų tai įveikti yra, jei yra, naudotis informacija iš kitų būdų. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system.  Daugiamodali informacija gali padėti NMT sistemai gerinti vertimą pašalinant kai kurių frazių arba žodžių neaiškumus. Dalyvaujame 8-ajame seminare dėl Azijos vertimo (WAT – 2021 m.) anglų ir hindų daugiarūšio vertimo užduotyje ir siekiame atitinkamai 42,47 ir 37,50 BLEU taškų vertinimo ir iššūkių pogrupyje.', 'kk': 'Нейрондық машинаның аударуы (NMT) қазіргі уақытта машинаның аудару технологиясы өзінің соңғы- соңғы аяқталу мүмкіндігі үшін болады. Бірақ NMT әлі қашықтағы тіл екеуінде тұтас ресурстар баптауларында дұрыс аудару үшін көмектеседі. Бұл басқа әдістерден мәліметті басқа әдістерден қолдануға болады. Идея - тілдердің айырмашылығына қарай, көзі мен мақсатты тілдердің тілдері бір нәрсе көрінеді және көзі мен мақсаттың көрінісі бірдей, бұл жүйесіне оң жақсы көмектесе алады. Көптеген мәлімет NMT жүйесіне бірнеше сөздерді не сөздерді аудару үшін жақсартуға көмектесе алады. Біз Азия аудармаларының 8- ші жұмысына (WAT - 2021) ағылшын-хиндық көптеген аудармалар тапсырмасына қатынап 42,47 және 37,50 BLEU бағдарламаларының оқу және Challenge бөлігіне қатынап келеміз.', 'mk': 'Неурална машинска транслекција (НМТ) е претежна машинска транслекција технологија денес поради нејзината флексибилност од крај до крај. Сепак, НМТ сé уште се бори да се преведе соодветно во поставувања со ниски ресурси специфично на далечни јазички парови. Еден начин да се надмине ова е да се користат информациите од други модели ако се достапни. Идејата е дека и покрај разликите во јазиците, изворот и говорниците на јазикот на целта го гледаат истото и визуелното претставување на изворот и целта е истото, што може позитивно да му помогне на системот. Мултимодилните информации можат да му помогнат на NMT системот да го подобри преводот со отстранување на двозначноста на некои фрази или зборови. Ние учествуваме на осмиот работилник за азиски превод (WAT - 2021) за англиско-хиндиско мултимедално превод задача и достигнуваме 42,47 и 37,50 поени БЛЕУ за потсетот на евалуација и предизвик, односно.', 'ms': 'Perjemahan Mesin Neural (NMT) adalah teknologi terjemahan Mesin yang berkuasa sekarang kerana fleksibiliti yang boleh dilatih akhir-akhir. Namun, NMT masih berjuang untuk menerjemahkan dengan betul dalam tetapan sumber rendah khususnya pada pasangan bahasa jauh. Satu cara untuk mengatasinya adalah menggunakan maklumat dari modaliti lain jika tersedia. Ideanya ialah walaupun perbezaan dalam bahasa, sumber dan pembicara bahasa sasaran melihat perkara yang sama dan perwakilan visual dari sumber dan sasaran adalah sama, yang boleh membantu sistem secara positif. Maklumat berbilang-modal boleh membantu sistem NMT untuk memperbaiki terjemahan dengan membuang ambiguity pada beberapa frasa atau kata. Kami berpartisipasi di Workshop ke-8 tentang Terjemahan Asia (WAT - 2021) untuk tugas terjemahan multimodal bahasa Inggeris-Hindi dan mencapai 42.47 dan 37.50 titik BLEU untuk Subset Evaluation and Challenge, respectively.', 'mt': 'Illum il-ġurnata t-Traduzzjoni tal-Magni Newrali (NMT) hija teknoloġija predominanti tat-traduzzjoni tal-magni minħabba l-flessibbiltà ferrovjarja tagħha minn tarf sa tarf. Madankollu, l-NMT xorta qed ikollha diffikultà biex tittraduċi kif suppost f’ambjenti b’riżorsi baxxi speċifikament fuq pari ta’ lingwi distanti. Mod wieħed biex jingħeleb dan huwa li tintuża l-informazzjoni minn modalitajiet oħra jekk disponibbli. L-idea hija li minkejja d-differenzi fil-lingwi, kemm dawk li jitkellmu s-sors kif ukoll dawk fil-lingwa fil-mira jaraw l-istess ħaġa u r-rappreżentanza viżiva kemm tas-sors kif ukoll tal-mira hija l-istess, li tista’ tassisti b’mod pożittiv lis-sistema. L-informazzjoni multimodali tista’ tgħin lis-sistema NMT ittejjeb it-traduzzjoni billi tneħħi l-ambigwità fuq xi frażijiet jew kliem. Aħna qed jipparteċipaw fit-8 Workshop dwar it-Traduzzjoni Ażjatika (WAT - 2021) għall-kompitu tat-traduzzjoni multimodali Ingliż-Indjan u nilħqu 42.47 u 37.50 punti BLEU għas-sottosett tal-Evalwazzjoni u l-Isfida, rispettivament.', 'ml': 'ന്യൂറല്\u200d മെഷീന്\u200d പരിഭാഷ (NMT) ഇപ്പോള്\u200d അതിന്റെ അവസാനം അവസാനിപ്പിക്കുന്നതിനാല്\u200d പ്രധാനപ്പെട്ട മെഷീന്\u200d പരിശീലന വിദ്യാ എന്നാലും NMT ഇപ്പോഴും പ്രയോഗിക്കുന്നു, കുറഞ്ഞ വിഭവങ്ങളുടെ സജ്ജീകരണങ്ങളില്\u200d പ്രത്യേകിച്ച് ദൂരെയുള്ള ഭാ മറ്റു രീതികളില്\u200d നിന്നും വിവരങ്ങള്\u200d ഉപയോഗിക്കാന്\u200d ഒരു വഴി ഭാഷകളില്\u200d വ്യത്യാസങ്ങള്\u200d വേര്\u200dപെടുത്തിയാലും, സ്രോതസ്സ് സംസാരിക്കുന്നവരും അതേ കാണുന്നു. സ്രോതസ്സിന്\u200dറെയും ലക്ഷ്യസ്ഥാനത്തിന്\u200dറെയും കാഴ കുറച്ചു വാക്കുകളില്\u200d നിന്നും വാക്കുകളില്\u200d നീക്കം ചെയ്യുന്നതിനാല്\u200d പരിഭാഷപ്പെടുത്താന്\u200d NMT സിസ്റ്റത്തി ആഷ്യയിലെ പരിഭാഷകങ്ങളില്\u200d ഞങ്ങള്\u200d എട്ടാം വര്\u200dക്കേഷനില്\u200d പങ്കുചേര്\u200dക്കുന്നു. ഇംഗ്ലീഷ്- ഹിന്ദി-ഹിന്ദി മള്\u200dട്ടിമോഡാല്\u200d പരിഭാഷണ ജോലിയില്\u200d ഞങ്ങള്\u200d പങ്കുചേര്\u200dക', 'pl': 'Neuronalne tłumaczenie maszynowe (NMT) jest obecnie dominującą technologią tłumaczenia maszynowego ze względu na swoją elastyczność, którą można szkolić. Jednak NMT wciąż trudno się prawidłowo tłumaczyć w ustawieniach niskich zasobów, zwłaszcza w odległych parach językowych. Jednym ze sposobów na przezwyciężenie tego problemu jest wykorzystanie informacji z innych sposobów, jeśli są dostępne. Chodzi o to, że pomimo różnic w językach, zarówno mówcy języka źródłowego, jak i docelowego widzą to samo, a wizualna reprezentacja zarówno źródła, jak i celu jest taka sama, co może pozytywnie wspomóc system. Informacje multimodalne mogą pomóc systemowi NMT w ulepszeniu tłumaczenia poprzez usunięcie niejednoznaczności niektórych zwrotów lub słów. Uczestniczymy w VIII Warsztatach na temat tłumaczenia azjatyckiego (WAT.2021) dla zadania multimodalnego tłumaczenia angielsko-hindi i osiągamy odpowiednio punkty 42.47 i 37.50 BLEU dla podzbioru oceny i wyzwań.', 'mn': 'Өнөөдөр мэдрэлийн машины хөгжүүлэлт (NMT) нь машины хөгжүүлэлтийн технологи юм. Эцэст нь эцэст нь дасгал хөгжүүлэх боломжтой боломжтой. Гэвч NMT хэл хэл хоорондын хоорондоо тодорхой бага боловсролын тохиолдолд зөвхөн хөгжүүлэхэд тусалдаг. Нэг арга нь хэрвээ боломжтой болвол бусад арга замыг ашиглах юм. Энэ санаа нь хэлд ялгаатай ч эх үүсвэр болон зориулагдсан хэл хэлний хэлэгчид ижил зүйлийг харж байгаа ба эх үүсвэр болон зориулагдсан зүйл нь ижил юм. Энэ нь системд эерэг туслах боломжтой. Олон загварын мэдээлэл NMT системд зарим үгийг эсвэл үгийг сайжруулахад туслах боломжтой болно. Бид Азийн хөгжлийн 8-р практик дээр (WAT-2021) Англи-Хиндийн олон модул хөгжлийн даалгаврын даалгаврын даалгаврын даалгаврын даалгаврын даалгаврын даалгаврын даалгаврын даалгаврын даалгаврын даалгаврын даалгаврын даалгаврын да', 'ro': 'Traducerea automată neurală (NMT) este o tehnologie predominantă de traducere automată în prezent datorită flexibilității sale end-to-end trainable. Cu toate acestea, NMT încă se luptă să traducă corect în setări cu resurse reduse, în special pe perechile de limbi îndepărtate. O modalitate de a depăși acest lucru este de a utiliza informațiile din alte modalități, dacă sunt disponibile. Ideea este că, în ciuda diferențelor dintre limbi, atât vorbitorii de limbă sursă, cât și cei de limbă țintă văd același lucru, iar reprezentarea vizuală atât a sursei, cât și a țintei este aceeași, ceea ce poate ajuta pozitiv sistemul. Informațiile multimodale pot ajuta sistemul NMT să îmbunătățească traducerea prin eliminarea ambiguității asupra unor fraze sau cuvinte. Participăm la cel de-al 8-lea Atelier de Traducere Asiatică (WAT - 2021) pentru sarcina de traducere multimodală engleză-hindi și obținem 42,47 și 37,50 puncte BLEU pentru subsetul Evaluare și Provocare.', 'no': 'Neuralmaskineomsetjing (NMT) er ein hovudteknologi for omsetjing av maskinen i dag på grunn av fleksibilitet i slutten til slutten. NMT kjem imidlertid framleis til å oversette rett i låg ressursinnstillingar spesifikke på lange språkopar. Eit måte å overføra dette er å bruka informasjonen frå andre måtar dersom det er tilgjengeleg. Ideen er at selv om forskjeller i språk, både kjeldespråkprogrammen ser det same, og det visuelle representasjonen av både kjelde og målet er det same, som kan positivt hjelpa systemet. Multimodal informasjon kan hjelpa NMT- systemet for å forbetra omsetjinga ved å fjerna ambiguity på nokre frasar eller ord. Vi deltar i den 8. arbeidsområdet om asiatisk omsetjing (WAT – 2021) for multimodal omsetjing av engelsk-Hindi og oppnår 42,47 og 37,50 BLEU-punkt for undergruppe for evaluering og utfordring.', 'so': 'Turjumista gaadiidka (NMT) waa teknolojiyada tarjumaadda machine-ka oo ugu weyn maanta, dhammaadka ugu dambaysta iyo dhammaadka xilliga la tababaray. Si kastaba ha ahaatee NMT weli wuxuu u dadaalaa inay si fiican u tarjumaan qoraalka hoos-resource, si gaar ah ugu qoran labada luuqadood oo fog. Waa in macluumaadka loo isticmaalo qaababka kale haddii ay jiraan. Fikirada waxaa loola jeedaa, in kastoo ay kala duwanaato luuqadaha, afka aad ku leedahay iyo luqada caawimaadda waxay arkaan isla isku mid, isla qaabka aragtiduna waa isku mid, kaas oo si wanaagsan u caawinaya nidaamka. Macluumaad badan oo kala duduwan waxay caawinaysaa nidaamka NMT si uu u kordhino turjumaadda si uu u dhaqaajiyo hadal ama hadal. We participate in the 8th Workshop on Asian Translation (WAT - 2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU points for Evaluation and Challenge subset, respectively.', 'sr': 'Neuralno prevodenje mašine (NMT) je danas predominantna tehnologija prevoda mašine zbog njene fleksibilnosti za kraj do kraja. Međutim, NMT se i dalje bori da se pravilno prevodi u postavkama niskih resursa posebno na udaljenim jezičkim parovima. Jedan način da se ovo prevlada je da koristimo informacije iz drugih modaliteta ako je dostupno. Ideja je da unatoč različitim jezicima, i izvorima i govornicima jezika vide istu stvar i vizualno predstavljanje izvora i cilja je isti, što može pozitivno pomoći sistemu. Multimodalne informacije mogu pomoći NMT-u da poboljša prevod uklanjanjem nesposobnosti na neke reči ili reči. Mi sudjelujemo u 8. radionici o azijskom prevodu (WAT - 2021) za zadatak multimodalnog prevoda engleskog hinda i postignemo 42,47 i 37,50 BLEU bodova za podskup procjene i izazova.', 'sv': 'Neural Machine Translation (NMT) 채r idag en dominerande maskin철vers채ttningsteknik p책 grund av dess helt채ckande flexibilitet. NMT k채mpar dock fortfarande f철r att 철vers채tta korrekt i l책gresursinst채llningar specifikt p책 avl채gsna spr책kpar. Ett s채tt att komma till r채tta med detta 채r att anv채nda informationen fr책n andra metoder om den finns tillg채nglig. Tanken 채r att trots spr책kskillnader ser b책de k채ll- och m책lspr책kstalare samma sak och den visuella representationen av b책de k채lla och m책l 채r densamma, vilket positivt kan hj채lpa systemet. Multimodal information kan hj채lpa NMT-systemet att f철rb채ttra 철vers채ttningen genom att undanr철ja tvetydighet i vissa fraser eller ord. Vi deltar i den 8:e workshopen om asiatisk 철vers채ttning (WAT - 2021) f철r multimodal 철vers채ttning av engelsk-hindi och uppn책r 42,47 respektive 37,50 BLEU-po채ng f철r delupps채ttningen Utv채rdering respektive Utmaning.', 'si': 'න්\u200dයුරල් මැෂින් පරිවර්තන (NMT) අද දැන් ප්\u200dරධාන මැෂින් පරිවර්තන තාක්ෂණයක් වෙනුවෙන් ඉවරයෙන් ඉවරයෙන් ඉවර නමුත්, NMT තාමත් හොඳටම පහළ සම්බන්ධ සැකසුම් විශේෂ විශේෂයෙන් දුර භාෂා ජෝඩු වලින් අවධ මේක ප්\u200dරධානය කරන්න එක ප්\u200dරමාණය තමයි අනිත් ප්\u200dරමාණයෙන් තොරතුරු පාවිච්චි කරන්න. අදහසය තමයි භාෂාවට වෙනස් වෙනුවෙන් තියෙන්නේ, මුළු සහ ඉලක්කු භාෂාවක් වලින් එකම දේවල් දැක්කා හා මුළු සහ ඉලක්ෂාවක් වලින ගොඩක් මොඩල් තොරතුරුම NMT පද්ධතියට වාර්ථාව වැඩ කරන්න පුළුවන් විශ්වාසය සහ වචන සමහර විශ්වාස නිසා  අපි ඉංග්\u200dරීසි-හින්දි ගොඩමෝඩාල් වාර්ථාවක් වෙනුවෙන් 8වෙනි වැඩසටහන් අයිතියානු වාර්ථාව (WAT - 2021) වෙනුවෙන් ඉංග්\u200dරීසියානු විශ', 'ta': 'நெருக்கர் இயந்திர மொழிபெயர்ப்பு (NMT) இன்று ஒரு முக்கியமான இயந்திர மொழிபெயர்ப்பு தொழில்நுட்பம் தொழில்நுட்ப தொழில் எனினும், NMT இன்னும் முயற்சி செய்யும் குறைந்த மூலத்தின் அமைப்புகளில் சரியாக மொழிபெயர்ப்பிட முயற்சிக்க இதை வெற்றி பெற ஒரு வழி இருந்தால் மற்ற வகைகளிலிருந்து தகவலை பயன்படுத்துகிறது. மொழிகளில் மாறுபாடுகள் இருந்தாலும் மூலம் மற்றும் இலக்கு மொழிகள் பேசுபவர்களும் அதே பார்க்கிறார்கள் மற்றும் மூலத்தின் பார்வை மற்றும் இலக் பல்வகை தகவல் சில சொற்றொடர்கள் அல்லது வார்த்தைகளில் மொழிபெயர்ப்பை நீக்குவதற்கு NMT முறைமையை மேம்படுத்த உதவும். ஆசிய மொழிபெயர்ப்பு (WAT - 2021) மொழிபெயர்ப்பு பணிக்கு நாங்கள் எட்டாவது வார்ப்புகளில் பங்கிடப்படுகிறோம் ஆங்கிலம்- Hindi பல மொழிபெயர்ப்பு பணிக்கு மற்றும் நிறைவ', 'ur': 'نائورل ماشین ترجمہ (NMT) آج کے دن ایک بڑا ماشین ترجمہ ٹیکنالوجی ہے اس کی پایان پایان تک ٹیکنالوجی کی وجہ سے۔ However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. اس پر غالب ہونے کا ایک طریقہ یہ ہے کہ اگر موجود ہو تو دوسرے موجودات سے معلومات استعمال کریں۔ یہ خیال ہے کہ زبانوں میں اختلاف کے بغیر، سورج اور موجود زبان بولنے والے دونوں ایک چیز دیکھتے ہیں اور دونوں سورج اور موجود کی تصویر ایک ہی ہے، جو سیسٹم کی مدد کرسکتا ہے۔ Multimodal information can help the NMT system to improve the translation by removing ambiguity on some phrases or words. ہم انگلیسی-ہندی multimodal translation task کے لئے آسیا ترجمہ (WAT-2021) کے 8م کارشاپ میں شریک ہوتے ہیں اور 42.47 اور 37.50 BLEU پوینٹوں کو ارزیابی اور چالینگ سٹیٹ کے لئے موجود ہوتے ہیں۔', 'uz': "Bugun hozirda bir muhim mashina tarjima texnologiya sababchi bizning oxirigi va oxirigi foydalanish uchun. @ info Agar mavjud бўлса, boshqa usuldan maʼlumotni ishlatish uchun bir usul. Bu g'oya: Tillar o'zgarishlar borligiga aytganda, manba va qanday til tillar bir narsa ko'rinishini ko'rsatadi va manba va maqsadning ko'rinishi bir xil, bu tizimga yordam beradi. Name Biz Inglizcha- Hindiston multimodal tarjima vazifani o'rganish uchun Osiy tarjima (WAT-2021) 8 chi 讲习班iga অংশগ্রহণ qilamiz va qiymatni va Challenge kabi qanchalik 42.47 va 37.50 BLEU nuqtini bajaramiz.", 'vi': 'Dịch về máy thần kinh (NMB) là một công nghệ dịch chuyển chuyên nghiệp phổ biến ngày nay nhờ sự linh hoạt của nó. Tuy nhiên, NMT vẫn cố gắng dịch chính xác trong thiết lập ít tài nguyên đặc biệt về cặp ngôn ngữ xa. Một cách để vượt qua là sử dụng thông tin từ các phương thức khác nếu có. Ý tưởng là mặc dù có nhiều ngôn ngữ khác nhau, cả nguồn và ngôn ngữ đích đều thấy giống nhau, và hình ảnh của cả nguồn lẫn mục tiêu đều giống nhau, có thể hỗ trợ hệ thống. Thông tin đa phương có thể giúp hệ thống NMT cải thiện bản dịch bằng cách xóa bỏ sự mơ hồ về một số đoạn văn hay từ. Chúng tôi tham gia chương trình Dịch bởi Châu Á thứ 8th (WAT 2021) cho nhiệm vụ dịch chuyển đa truyền tiếng Anh-Hindi và đạt được địa phương 2.42.47 và 37.50 liên quan đến ban đánh giá và tỷ lệ thách đấu.', 'bg': 'Невровият машинен превод (НМТ) е преобладаваща технология за машинен превод в днешно време поради своята гъвкавост от край до край. Въпреки това, NMT все още се бори да преведе правилно в настройки с ниски ресурси, специално на отдалечени езикови двойки. Един от начините да се преодолее това е да се използва информацията от други модификации, ако има такава. Идеята е, че въпреки различията в езиците говорителите на изходния и целевия език виждат едно и също нещо и визуалното представяне на източника и целта е едно и също, което може положително да помогне на системата. Мултимодалната информация може да помогне на системата да подобри превода чрез премахване на неяснотата на някои фрази или думи. Участваме в 8-ия семинар по азиатски превод (УТ - 2021) за задача англо-хинди мултимодален превод и постигаме съответно 42,47 и 37,50 точки за поднабор Оценка и Предизвикателство.', 'nl': 'Neural Machine Translation (NMT) is tegenwoordig een overheersende machine translation technologie vanwege zijn end-to-end trainable flexibiliteit. Echter, NMT worstelt nog steeds om correct te vertalen in low-resource instellingen, specifiek op verre taalparen. Een manier om dit te overwinnen is door de informatie van andere modaliteiten te gebruiken indien beschikbaar. Het idee is dat, ondanks verschillen in talen, zowel de bron- als doeltaalsprekers hetzelfde zien en de visuele weergave van zowel de bron als het doel hetzelfde is, wat het systeem positief kan helpen. Multimodale informatie kan het NMT-systeem helpen om de vertaling te verbeteren door dubbelzinnigheid op bepaalde zinnen of woorden te verwijderen. We nemen deel aan de 8e Workshop on Asian Translation (WAT.2021) voor Nederlands-Hindi multimodale vertaaltaak en behalen respectievelijk 42.47 en 37.50 BLEU punten voor Evaluatie en Challenge subset.', 'da': 'Neural Machine Translation (NMT) er en dominerende maskinoversættelsesteknologi i dag på grund af dens end-to-end træningsdygtige fleksibilitet. NMT kæmper dog stadig med at oversætte korrekt i indstillinger med lav ressource specifikt på fjerntliggende sprogpar. En måde at overvinde dette på er at bruge oplysningerne fra andre metoder, hvis de foreligger. Tanken er, at på trods af forskelle i sprog, både kilde- og målsprogshøjtalere ser det samme, og den visuelle repræsentation af både kilde og mål er det samme, hvilket positivt kan hjælpe systemet. Multimodale oplysninger kan hjælpe NMT-systemet med at forbedre oversættelsen ved at fjerne tvetydighed på nogle sætninger eller ord. Vi deltager i den 8. workshop om asiatisk oversættelse (WAT - 2021) til multimodal oversættelsesopgave fra engelsk til hindi og opnår henholdsvis 42,47 og 37,50 BLEU point for undersættelsen Evaluering og Challenge.', 'hr': 'Neuralni prevod strojeva (NMT) je danas predsjednička tehnologija prevoda strojeva zbog njene fleksibilnosti praćene do kraja. Međutim, NMT se i dalje bori za pravo prevoditi u postavima niskih resursa posebno na udaljenim jezičkim parovima. Jedan način za prevladanje ovoga je koristiti informacije iz drugih načina ako je dostupno. Ideja je da, uprkos različitim jezicima, i izvornici i govornici jezika vidi istu stvar i vizualno predstavljanje izvora i cilja je isti, što može pozitivno pomoći sustavu. Multimodalne informacije mogu pomoći NMT sustavu za poboljšanje prevoda uklanjanjem nesposobnosti na nekim rečenicama ili rečenicama. Mi sudjelujemo u 8. radionici o azijskom prevodu (WAT - 2021) za zadatak multimodalnog prevoda engleskog hinda i postignemo 42,47 i 37,50 BLEU bodova za podskup procjene i izazova.', 'id': 'Perjemahan Mesin Neural (NMT) adalah teknologi terjemahan mesin yang berkuasa saat ini karena fleksibilitas yang bisa dilatih akhir-akhir. Namun, NMT masih berjuang untuk menerjemahkan dengan benar dalam pengaturan sumber daya rendah khusus pada pasangan bahasa yang jauh. Satu cara untuk mengatasi ini adalah menggunakan informasi dari modalitas lain jika tersedia. Idenya adalah meskipun perbedaan dalam bahasa, sumber dan pembicara bahasa sasaran melihat hal yang sama dan representation visual dari sumber dan sasaran adalah sama, yang dapat membantu sistem secara positif. Informasi multimodal dapat membantu sistem NMT untuk memperbaiki terjemahan dengan menghapus ambiguitas pada beberapa frasa atau kata. Kami berpartisipasi di Workshop ke-8 tentang Translation Asia (WAT - 2021) untuk tugas tradukti multimodal bahasa Inggris-Hindi dan mencapai 42,47 dan 37,50 poin BLEU untuk Subset Evaluation and Challenge, respectively.', 'de': 'Die neuronale maschinelle Übersetzung (NMT) ist heutzutage aufgrund ihrer durchgängig trainierbaren Flexibilität eine vorherrschende maschinelle Übersetzungstechnologie. Allerdings hat NMT immer noch Schwierigkeiten, in ressourcenarmen Einstellungen korrekt zu übersetzen, insbesondere auf entfernten Sprachpaaren. Eine Möglichkeit, dies zu überwinden, besteht darin, die Informationen aus anderen Modalitäten zu verwenden, sofern verfügbar. Die Idee ist, dass trotz der Unterschiede in den Sprachen sowohl die Ausgangs- als auch die Zielsprache die gleiche Sache sehen und die visuelle Darstellung der Quelle und des Ziels gleich ist, was das System positiv unterstützen kann. Multimodale Informationen können dem NMT-System helfen, die Übersetzung zu verbessern, indem Unklarheiten bei einigen Phrasen oder Wörtern beseitigt werden. Wir nehmen am achten Workshop on Asian Translation (WAT.2021) für multimodale Übersetzungsaufgaben Englisch-Hindi teil und erreichen 42.47 und 37.50 BLEU Punkte für Evaluation und Challenge Subset.', 'tr': 'NMT 횦철ne, NMT ent채gem 첵okary ressurs d체z체mlerinde edil uzak dil 챌iftlerde terjime etmek 체챌in m체cadele ed첵채r. Bu 첵erde 체stine 챌ykmak 체챌in bir 첵agda첵 bolsa, eger me흫ze힊 modlerden maglumatlary ulanmak 체챌in. Dillerde tapawutlaryna ra휓men, 챌e힊me we maksady dilleri흫 bir zadyny hem 챌e힊me we maksadyny흫 g철rn체힊i hem me첵dan챌asyny흫 bir 힊ekilde 체첵tgebilir. Ol sisteme m체mkin edip biler. 횉oklumodal maglumat NMT sistemine terjime edilmek 체챌in k채bir s철zler 첵a-da s철zler 첵itirmek 체챌in k철mek edip biler. Biz Azi첵a terjime etmek 체챌in 8-nji u첵rukda (WAT - 2021) I흫lis챌e-Hindiler multimodal terjime t채zisinde 챌yky힊 we 42.47 we 37.50 BLEU sany 챌yky힊 we 챌yky힊 toparynda 첵etirdik.', 'ko': '신경기계번역(NMT)은 끝에서 끝까지의 훈련 유연성 때문에 현재 주류의 기계번역 기술이다.그러나 NMT는 특히 원격 언어 쌍에서 저자원 환경에서 정확하게 번역하기 어렵다.이 문제를 극복하는 방법의 하나는 다른 방식으로 제공된 정보를 사용하는 것이다.이 아이디어는 언어가 다르더라도 원시 언어와 목표 언어 사용자가 보는 것은 똑같고 원시 언어와 목표 언어의 시각적 표현은 똑같기 때문에 시스템에 적극적인 도움이 된다는 것이다.다중모드 정보는 NMT 시스템이 일부 단어나 단어의 잘못된 뜻을 없애고 번역의 질을 향상시키는 데 도움을 줄 수 있다.영어-인디언 다중모드 번역 미션을 수행하는 제8회 아시아번역세미나(WAT-2021)에 참가해 평가와 도전 서브집에서 각각 42.47, 37.50 BLEU 점수를 받았다.', 'sw': 'Tafsiri ya Mashine ya Kijerumani (NMT) ni teknolojia muhimu ya kutafsiri mashine leo kwa sababu ya uwezekano wa mafunzo wa mwisho wa mwisho. Hata hivyo, NMT bado anajitahidi kutafsiri vizuri katika mazingira ya rasilimali chini hususani kwenye mazingira ya lugha mbali. Njia moja ya kushinda hili ni kutumia taarifa kutoka katika njia nyingine kama inapatikana. Wazo ni kwamba pamoja na tofauti tofauti katika lugha, na wazungumzaji wa lugha na lengo wanaona jambo hilo na uwakilishi wa kuonekana kwa chanzo na lengo hilo ni sawa, ambalo linaweza kusaidia mfumo. Taarifa nyingi zinaweza kusaidia mfumo wa NMT kuboresha tafsiri kwa kuondoa uchochezi wa maneno au maneno. Tumeshiriki katika warsha ya 8 kuhusu Tafsiri ya Asia (WAT - 2021) kwa ajili ya kazi ya utafsiri wa lugha za Kiingereza-Hindi na kupata pointi 42.47 na 37.50 BLEU kwa ajili ya kipindi cha Uchunguzi na Challenge.', 'af': "Nurale Masjien Vertaling (NMT) is vandag 'n voordekende masjien vertaling teknologie vandag vanweë sy einde- na- einde oefenbare fleksibiliteit. Maar NMT struikel nog reg om te vertaal in lae hulpbron instellings spesifieke op afgeleë taal paar. Een manier om hierdie te oorwin is om die inligting van ander modaliteite te gebruik indien beskikbaar. Die idee is dat, selfs verskille in tale, beide die bron en die doel taal sprekkers sien dieselfde ding en die visuele voorstelling van beide die bron en die doel is dieselfde, wat kan positief die stelsel assistent. Multimodale inligting kan die NMT stelsel help om die vertaling te verbeter deur onbedienheid op sommige frase of woorde te verwyder. Ons deelnadeer in die 8de Werkshop op Asiëse Vertaling (WAT - 2021) vir Engels-Hindi multimodale vertaling opdrag en bereik 42.47 en 37.50 BLEU punte vir Evaluering en Challenge subartikel.", 'fa': 'ترجمه ماشین عصبی (NMT) امروز یک تکنولوژی ترجمه ماشین بزرگی است به دلیل flexibility آموزش قابل پایان و پایان آن. ولی NMT هنوز برای ترجمه به درستی در تنظیمات منابع کم به طور خاص در جفت زبان دور تلاش می کند. یک راهی برای تغییر دادن این است که اگر موجودات موجود باشد از اطلاعات استفاده کنید. ایده این است که با وجود تفاوت در زبانها، هر دو گوش دهندگان منبع و زبان هدف یکسان را می بینند و نمایش دیده ای از منبع و هدف یکسان است، که می تواند به طور مثبتی سیستم را کمک کند. اطلاعات چندmodal می تواند سیستم NMT را کمک کند تا ترجمه را بهتر کند با حذف غیرقابلیت بر برخی از جمله\u200cها یا کلمات. ما در کارگاه هشتم روی ترجمه آسیا (WAT - 2021) برای کار تعلیم انگلیسی-هندی چندین مدال شرکت می\u200cکنیم و به طور مستقل 42.47 و 37.50 نقطه BLEU برای تحلیل ارزیابی و چالج رسیدیم.', 'hy': 'Նյարդային մեքենայի թարգմանությունը (NMT) հիմնական մեքենայի թարգմանման տեխնոլոգիա է այսօր, դրա վերջ-վերջ կրթության ճկունության պատճառով: Այնուամենայնիվ, NMT-ը դեռևս պայքարում է ճիշտ թարգմանել ցածր ռեսուրսների պայմաններում, հատկապես հեռավոր լեզվի զույգերի վրա: Սա հաղթահարելու միջոցն այն է, որ օգտագործենք տեղեկատվությունը այլ միջոցներից, եթե հասանելի է: Գաղափարն այն է, որ չնայած լեզվի տարբերություններին, աղբյուր և նպատակային լեզվի խոսնակները տեսնում են նույն բանը, ինչպես նաև աղբյուր և նպատակի տեսողական ներկայացումը նույն է, ինչը կարող է դրական օգնել համակարգին: Multimodal information can help the NMT system to improve the translation by removing ambiguity on some phrases or words.  Մենք մասնակցում ենք Ասիայի թարգմանության 8-րդ աշխատասենյակում (ԱՎԱT-2021) անգլերեն-հնդիկ բազմամոդալ թարգմանման խնդիրների համար և հասնում ենք 42.47 և 37.50 բլեուզ կետերի գնահատման և մարտահրավերի ենթախմբի:', 'sq': 'Translation Neural Machine (NMT) është një teknologji mbizotëruese e përkthimit të makinave sot për shkak të fleksibilitetit të trajnimit nga fundi në fund. Megjithatë, NMT ende lufton për të përkthyer siç duhet në rregullime me burime të ulëta veçanërisht në çifte gjuhësh të largëta. One way to overcome this is to use the information from other modalities if available.  Ideja është se pavarësisht nga dallimet në gjuhë, si burimi ashtu edhe gjuhët në shënjestër shohin të njëjtën gjë dhe përfaqësimi vizual i si burimit ashtu dhe shënjestrit është i njëjti, gjë që mund të ndihmojë pozitivisht sistemin. Informacioni multimodal mund të ndihmojë sistemin NMT për të përmirësuar përkthimin duke hequr pasigurinë në disa fraza apo fjalë. Ne marrim pjesë në seminarin e tetë mbi përkthimin aziatik (WAT - 2021) për detyrën e përkthimit multimodal anglisht-hindi dhe arrijmë respektivisht 42.47 dhe 37.50 pikë BLEU për nëngrupin e vlerësimit dhe sfidave.', 'bn': 'এখন নিউরেল মেশিন অনুবাদ (এনএমটি) একটি গুরুত্বপূর্ণ মেশিন অনুবাদ প্রযুক্তির কারণে তার শেষ থেকে শেষ পর্যন্ত প্রশিক্ষণের প্ তবে এনএমটি এখনো নীচের সম্পদের বৈশিষ্ট্যাবলীতে সঠিকভাবে অনুবাদ করার সংগ্রাম করেছেন। বিশেষ করে দূরবর্তী ভাষা এটি পরাজিত করার একটি উপায় হলো অন্যান্য উপাদান থেকে তথ্য ব্যবহার করা হবে। এই ধারণা হচ্ছে যে ভাষায় ভিন্ন ভিন্ন পার্থক্য সত্ত্বেও উৎস এবং লক্ষ্য ভাষার ভাষার কথাবার্তারা একই রকম দেখতে পায় আর উৎস ও লক্ষ্যবস্তুর দৃশ্য প্ মাল্টিমোডাল তথ্য এনএমটি সিস্টেমের অনুবাদ উন্নত করতে সাহায্য করতে পারে কিছু বাক্য অথবা শব্দের ব্যাপারে অনুবাদ আমরা এশিয়ার অনুবাদের আটতম ওয়ার্কশার্কে অংশগ্রহণ করেছি ইংরেজী ও হিন্দি অনুবাদের কাজের জন্য এবং পরিচালনা ও চ্যালেঞ্জ বিভাগের জন্য ৪২.', 'az': 'Nöral Makina Çevirməsi (NMT) bugünkü müəyyən edilən maşın çevirim teknolojisidir. Ancaq NMT hələ də uzaq dil çiftlərində düşük ressurs ayarlarında ədalətli tercümə etmək üçün mübahisə edir. Əgər mövcuddur, bu məlumatları başqa modlərdən istifadə etmək üçün bir yoldur. Bu fikir dillərin fərqli olmasına rağmen, mənbə və məqsəd dillərin söhbətçiləri də eyni şeyi görür və mənbənin və məqsədinin görsel göstərilməsi də eynidir ki, bu sistemə möhkəm yardım edə bilər. Çoxlu modal məlumat NMT sisteminə bəzi sözlər ya da sözlər istifadə edərək tercüməni yaxşılaşdırmağa kömək edə bilər. Biz İngilizce-Hindi çoxlu modal tercümə məqsədilə Asiya tercüməsi barəsində 8. Çalışma səviyyəsinə (WAT-2021) katılırıq və 42.47 və 37.50 BLEU ünvanlarını değerləşdirmək və çəkişmə məqsədilə yetişiririk.', 'am': 'የናውራዊ መኪን ትርጓሜ (NMT) ዛሬ በመጨረሻው ለፍጻሜ የተማረከ ብልሃት ነው፡፡ ነገር ግን NMT ገና ተጋድሞአል በዝቅተኛ-resource settings በተለይም በሩቅ ቋንቋ ሁለትን ለመተረጉት ነው። ይሄንን ለማሸንፍ አንድ መንገድ ቢገኝ መረጃዎችን ከሌላ ዓይነት ለመጠቀም ነው፡፡ የአሳቡ ልዩ ቋንቋ ቢሆንም፣ ምንጭ እና የቋንቋ ቋንቋዎች ግን ያንኑ ነገር እና የክፍለ መልዕክትና የዓይነት መልዕክት አንድ ነው፣ ሲስተም ይረዳል፡፡ Multimodal information can help the NMT system to improve the translation by removing some phrases or words. በእስያ ትርጉም (WAT-2021) ለኢንጂልኛ-Hindi multimodal ትርጉም ስራዎችን እናደርጋለን፡፡', 'bs': 'Neuralni prevod mašine (NMT) je danas predominantna tehnologija prevoda mašine zbog njene fleksibilnosti trenirane do kraja. Međutim, NMT se i dalje bori za pravo prevoditi u postavkama niskih resursa posebno na udaljenim jezičkim parovima. Jedan način za prevladanje ovoga je koristiti informacije iz drugih modaliteta ako je dostupno. Ideja je da, uprkos različitim jezicima, i izvornici i govornici jezika vide istu stvar i vizualno predstavljanje izvora i cilja je isti, što može pozitivno pomoći sistemu. Multimodalne informacije mogu pomoći NMT-u da poboljša prevod uklanjanjem ambiguitete na neke rečenice ili reči. Mi sudjelujemo u 8. radionici o azijskom prevodu (WAT - 2021) za zadatak multimodalnog prevoda engleskog hinda i postignemo 42,47 i 37,50 BLEU bodova za podskup procjene i izazova.', 'ca': "La traducció neural de màquines (NMT) és una tecnologia predominant de traducció de màquines avui en dia per la seva flexibilitat entrenable de final a final. Tot i així, la NMT encara lluita per traduir correctament en configuracions de baix recursos específicament en parelles de llenguatges distants. Una manera de superar això és utilitzar la informació d'altres modalitats si està disponible. La idea és que malgrat les diferències en les llengües, tant els oradors de fonts com els llengües d'objectiu veuen el mateix i la representació visual tant de fonts com d'objectiu és la mateixa, que pot ajudar positivament el sistema. La informació multimodal pot ajudar al sistema NMT a millorar la traducció eliminant ambigüitat en algunes frases o paraules. We participate in the 8th Workshop on Asian Translation (WAT - 2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU points for Evaluation and Challenge subset, respectively.", 'cs': 'Neurální strojový překlad (NMT) je dnes převládající technologií strojového překladu díky své komplexní trénovatelné flexibilitě. Nicméně, NMT stále bojuje správně překládat v nastavení s nízkými zdroji konkrétně na vzdálené jazykové páry. Jedním ze způsobů, jak to překonat, je použít informace z jiných způsobů, pokud jsou k dispozici. Myšlenka spočívá v tom, že i přes rozdíly v jazycích mluvčí zdrojového i cílového jazyka vidí totéž a vizuální reprezentace zdroje i cíle je stejná, což může pozitivně pomoci systému. Multimodální informace mohou pomoci NMT systému zlepšit překlad tím, že odstraní nejednoznačnost některých frází nebo slov. Účastníme se osmého workshopu o asijském překladu (WAT.2021) pro multimodální překlad anglicko-hindštiny a získáváme body 42.47 a 37.50 BLEU pro podmnožinu hodnocení a Challenge.', 'et': 'Neuraalne masintõlge (NMT) on tänapäeval valdav masintõlketehnoloogia tänu oma täielikule koolitatavale paindlikkusele. Ent NMT ei suuda ikka veel korralikult tõlkida vähese ressursiga seadetes, eriti kaugetel keelepaaridel. Üks võimalus selle ületamiseks on kasutada teavet muudest meetoditest, kui see on kättesaadav. Mõte on selles, et vaatamata erinevustele keeltes näevad nii lähte- kui sihtkeele kõnelejad sama asja ning nii lähte- kui sihtkeele visuaalne esitus on sama, mis võib süsteemi positiivselt aidata. Mitmeliigiline teave võib aidata NMT-süsteemil tõlget parandada, kõrvaldades mõnede fraaside või sõnade ebaselguse. Osaleme Aasia tõlke 8. seminaril (WAT - 2021) inglise-hindi mitmeliigilise tõlke ülesandeks ning saavutame vastavalt 42,47 ja 37,50 BLEU punkti hindamise ja väljakutse alamhulgas.', 'fi': 'Neuraalinen konekäännös (NMT) on nykyisin hallitseva konekäännöstekniikka, koska se on täysin koulutettavissa oleva joustavuus. NMT:llä on kuitenkin edelleen vaikeuksia kääntää oikein vähän resursseja sisältävissä asetuksissa erityisesti kaukaisissa kielipareissa. Yksi tapa ratkaista tämä on käyttää muista menettelyistä saatuja tietoja, jos niitä on saatavilla. Ajatuksena on, että kielieroista huolimatta sekä lähde- että kohdekielen puhujat näkevät saman asian ja sekä lähde- että kohdekielen visuaalinen esitys on sama, mikä voi tukea järjestelmää positiivisesti. Multimodaalinen tieto voi auttaa NMT-järjestelmää parantamaan käännöstä poistamalla epäselvyyttä joistakin lauseista tai sanoista. Osallistumme kahdeksanteen Aasian kääntämisen työpajaan (WAT - 2021) englannin-hindin multimodaaliseen käännöstyöhön ja saavutamme 42,47 ja 37,50 BLEU-pistettä arviointiin ja haasteeseen.', 'ha': "Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility.  A kasa, NMT yana ƙari yin fassara properly in lower-resource settings spesifically on parse away language. Yi amfani da maɓalli daga wasu hanyõyi idan yana da amfani da wannan. Fikiron ni ne cẽwa, kuma kõ da yaushe masu sãɓãni na cikin harshen, duk masu faɗi da ke faɗi da maganar da ake shagala, suna ganin abin da yake shi da ake gani da duk kwanan da ake amfani da shi daidai, wanda yana iya iya taimakon tsarin na'ura. @ action: button Tuna raba cikin ¦ãlin workin na 8th on Asian Translate (WAT - 2021) wa aikin Ingiriya-Hidi multimulmultiodal kuma munã sãmun henhold 42.47 da 37.50 BLEU points for evaluation and Challenge subt.", 'he': 'התרגום של מכונות עצביות (NMT) הוא טכנולוגית התרגום של מכונות בימים אלה בגלל הגמישות האימונית שלה. עם זאת, NMT עדיין נאבק לתרגם כראוי בסדרות משאבים נמוכים במיוחד על זוגות שפות רחוקות. דרך אחת להתגבר על זה היא להשתמש במידע ממונדיות אחרות אם זמינות. הרעיון הוא שלמרות ההבדלים בשפות, גם המקור וגם מדברי שפות המטרה רואים את אותו הדבר והייצוג ויזואלי של המקור וגם המטרה הוא אותו הדבר, שיכול לסייע באופן חיובי למערכת. מידע רב-מודלי יכול לעזור למערכת NMT לשפר את התרגום על ידי הסירה של אי-ספק על כמה ביטויים או מילים. אנחנו משתתפים בסWorkshop השמיני על התרגום אסיאטי (WAT - 2021) עבור משימה התרגום multimodal אנגלי-הינדי ולהשיג 42.47 ו-37.50 נקודות BLEU עבור הערכה ואתגר, בהתאם.', 'bo': 'ནུས་མེད་ལག་འཁོར་གྱི་གཞུང་ལུགས་འདིའི་ནང་དུ་བཏུབ་པའི་རྩིས་འཁོར་གཞུང་གི་དབྱེ་བ་ཞིག་རེད། ཡིན་ནའང་། NMT དེ་ཡང་དངོས་ཐོག་ཁུངས་ཀྱི་སྒྲིག་འགོད་ལ་ཉུང་བའི་སྒྲིག་འགོད་དག་གི་ཁྱད་པར་ཐག་རིང་གི་སྐད་ཆ་གཉ འདི་ལ་ཆ་ཕར་བཟོ་བྱེད་ཐབས་ལམ་གཅིག་ནི་གལ་སྲིད་པའི་ཐབས་ལམ་གཞན་ཞིག་ནས་ཆ་འཕྲིན་སྤྱོད་ཐུབ་པ་ཡིན། སྐད་རིགས་ནང་གི་ཁྱད་པར་ལ་ཡིན་ནའང་། ཐོག་ཁུངས་དང་དམིགས་ཡུལ་གྱི་སྐད་རིགས་སོ་སོའི་ནང་དུ་ཡིན Multimodal information can help the NMT system to improve the translation by removing ambiguity on some phrases or words. ང་ཚོས་ཨ་རིའི་སྐད་ཡིག་གཟུགས་ཅན་གྱི་ལས་ཀ་ཚོགས་ཀྱི་ནང་དུ་འགྲོ་བཞིན་པའི་བརྗོད་སྟོན་གཉིས་པ་དང་། ཨ་རིའི་ནང་དུ་ཡིག་གཟུགས་རྒྱ་ནག་གི་ནང་དུ་ཡོད་ཚད་དང་བསླ', 'jv': 'Nyural Majin Tarjamahan politenessoffpolite"), and when there is a change ("assertivepoliteness Isih beraksi kanggo ngilangno iki iso nggawe informasi ning kakéh modalité sing wis ana. ide punika dipun ajeng-ajeng langang, sampeyan bukané karo perusahaan bangsa luwih panelusuran ing larang nggawe sistem sing beraksi lan bukané, akeh iso nggawe sistem sing berarti. Multimodal information can help the NMT System to refresh the translation by removing ambject on some words or words. Awak dhéwé éntuk tanggal 8-uwong Workspace kanggo Terjamahan asit', 'sk': 'Nevralni strojni prevod (NMT) je danes prevladujoča tehnologija strojnega prevajanja zaradi svoje celovite fleksibilnosti, ki jo je mogoče trenirati. Vendar pa NMT še vedno težko pravilno prevaja v nastavitvah z nizkimi viri, posebej na oddaljenih jezikovnih parih. Eden od načinov za premagovanje tega je uporaba informacij iz drugih načinov, če so na voljo. Ideja je, da kljub razlikam v jezikih tako izvorni kot ciljni jezik vidijo isto stvar, vizualna predstavitev vira in cilja pa je enaka, kar lahko pozitivno pomaga sistemu. Multimodalne informacije lahko sistemu NMT pomagajo izboljšati prevod z odpravo dvoumnosti nekaterih stavkov ali besed. Sodelujemo na 8. delavnici o azijskem prevajanju (WAT - 2021) za nalogo večmodalnega prevajanja angleško-hindijščina in dosegamo 42,47 točk oziroma 37,50 točk BLEU za podnabor evalvacije oziroma izziva.'}
{'en': 'TMEKU System for the WAT2021 Multimodal Translation Task TMEKU  System for the  WAT 2021 Multimodal Translation Task', 'fr': 'Système TMEKU pour la tâche de traduction multimodale WAT2021', 'ar': 'نظام TMEKU لمهمة الترجمة متعددة الوسائط WAT2021', 'pt': 'Sistema TMEKU para a Tarefa de Tradução Multimodal WAT2021', 'es': 'Sistema TMEKU para la tarea de traducción multimodal WAT2021', 'zh': '用WAT2021多模态译者TMEKU统', 'ru': 'Система TMEKU для задачи мультимодального перевода WAT2021', 'ja': 'WAT 2021マルチモーダル翻訳タスク用TMEKUシステム', 'hi': 'WAT2021 मल्टीमॉडल अनुवाद कार्य के लिए TMEKU सिस्टम', 'ga': 'Córas TMEKU don Tasc Aistriúcháin Ilmhódúil WAT2021', 'el': 'Σύστημα TMEKU για το έργο Πολυmodal Translation', 'it': 'Sistema TMEKU per il compito di traduzione multimodale WAT201', 'ka': 'Name', 'mk': 'Name', 'hu': 'TMEKU rendszer a WAT2021 multimodális fordítási feladathoz', 'kk': 'WAT2021 көптеген аудармалы тапсырманың TMEKU жүйесі', 'lt': 'TMEKU sistema, skirta WAT2021 daugiarūšio vertimo darbui', 'ml': 'WAT2021 Multimodal Translation Task', 'ms': 'Sistem TMEKU untuk Tugas Terjemahan Multimodal WAT2021', 'mt': 'Sistema TMEKU għall-Kompitu ta’ Traduzzjoni Multimodali WAT2021', 'no': 'TMEKU- systemet for multimodal oversettelsesoppgåva for WAT2021', 'mn': 'WAT2021 олон modal Translation Task-ийн TMEKU System for the WAT2021 Multimodal Translation Task', 'pl': 'System TMEKU dla zadania multimodalnego tłumaczenia WAT2028', 'sr': 'TMEKU sistem za multimodalni prevodni zadatak WAT2021', 'si': 'Name', 'ro': 'Sistemul TMEKU pentru sarcina de traducere multimodală WAT201', 'sv': 'TMEKU-systemet för den multimodala översättningsuppgiften WAT2014', 'so': 'TMEKU System of the WAT2021 Multimodal Translation', 'ta': 'WAT2021 பல் மொழிபெயர்ப்பு பணிக்கான TMEKU அமைப்பு', 'ur': 'Name', 'vi': 'Hệ thống TMEKU for the WATE2021 Multi Translation Task', 'uz': 'WAT2021 Multimodal tarjima vazifalar uchun TMEKU tizim', 'bg': 'Система ТМЕКУ за задачата за мултимодален превод', 'nl': 'TMEKU-systeem voor de multimodale vertaaltaak WAT2025', 'hr': 'TMEKU sustav za multimodalni prevodni zadatak WAT2021', 'da': 'TMEKU System til WAT201s multimodale oversættelsesopgave', 'id': 'Sistem TMEKU untuk Tugas Terjemahan Multimodal WAT2021', 'ko': 'WAT2021 다중 모드 번역 작업을 위한 TMEKU 시스템', 'de': 'TMEKU System für die multimodale Übersetzungsaufgabe WAT2028', 'fa': 'Name', 'af': 'Name', 'tr': 'WAT2021 Çoklumodal terjime Görevi üçin TMEKU Sistemi', 'sw': 'Mfumo wa TMEKU kwa ajili ya kazi ya utafsiri wa WAT2021', 'am': 'WAT2021 Multimodal Translation Task', 'sq': 'TMEKU System for the WAT2021 Multimodal Translation Task', 'hy': 'ՎԱԹ2021-ի բազմամոդալ թարգմանման գործի համար', 'bn': 'WAT2021 বহুমোডাল অনুবাদ করার জন্য TMEKU সিস্টেম', 'bs': 'TMEKU sistem za multimodalni prevodni zadatak WAT2021', 'ca': 'TMEKU System for the WAT2021 Multimodal Translation Task', 'cs': 'Systém TMEKU pro multimodální překladatelskou úlohu WAT2028', 'et': 'TMEKU süsteem WAT12021 Multimodaalse tõlke ülesande jaoks', 'fi': 'TMEKU System for the WAT1016 Multimodal Translation Task', 'az': 'WAT2021 çoxlu modal tercümə işi üçün TMEKU Sistemi', 'jv': 'TOMEKuS System kanggo WAt2020 1 Multimodal translation task', 'he': 'מערכת TMEKU עבור משימה התרגום רב מודלית WAT2021', 'ha': 'KCharselect unicode block name', 'sk': 'Sistem TMEKU za nalogo večmodalnega prevajanja WAT12021', 'bo': 'TMEKU མ་དབྱིབས་རྒྱུན་ལྡན་གཞུང་གི་བྱ་འགུལ་ལ་དོ་དམ།WAT2021'}
{'en': 'We introduce our TMEKU system submitted to the English-Japanese Multimodal Translation Task for WAT 2021. We participated in the Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets. Our proposed system employs soft alignment of word-region for multimodal neural machine translation (MNMT). The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among all the participated systems. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use.', 'pt': 'Apresentamos nosso sistema TMEKU submetido à tarefa de tradução multimodal inglês-japonês para WAT 2021. Participamos da tarefa Flickr30kEnt-JP e tarefa multimodal Ambiguous MSCOCO sob a condição restrita usando apenas os conjuntos de dados fornecidos oficialmente. Nosso sistema proposto emprega alinhamento suave de região de palavra para tradução automática neural multimodal (MNMT). Os resultados experimentais avaliados na métrica BLEU fornecida pelo site de avaliação WAT 2021 mostram que o sistema TMEKU alcançou o melhor desempenho entre todos os sistemas participantes. Uma análise mais aprofundada do estudo de caso demonstra que alavancar o alinhamento da região da palavra entre as modalidades textual e visual é a chave para o aprimoramento do desempenho em nosso sistema TMEKU, o que leva a um melhor uso da informação visual.', 'es': 'Presentamos nuestro sistema TMEKU presentado a la Tarea de Traducción Multimodal Inglés-Japonés para el WAT 2021. Participamos en la tarea Flickr30Kent-JP y en la tarea ambigua multimodal de MSCOCO bajo la condición restringida utilizando solo los conjuntos de datos proporcionados oficialmente. Nuestro sistema propuesto emplea la alineación suave de la región de palabras para la traducción automática neuronal multimodal (MNMT). Los resultados experimentales evaluados en la métrica BLEU proporcionada por el sitio de evaluación de WAT 2021 muestran que el sistema TMEKU ha logrado el mejor rendimiento entre todos los sistemas participantes. Un análisis más detallado del estudio de caso demuestra que aprovechar la alineación palabra-región entre las modalidades textual y visual es la clave para mejorar el rendimiento de nuestro sistema TMEKU, lo que conduce a un mejor uso de la información visual.', 'fr': "Nous présentons notre système TMEKU soumis à la tâche de traduction multimodale anglais-japonais pour le WAT 2021. Nous avons participé à la tâche Flickr30Kent-JP et à la tâche multimodale ambiguë MSCOCO dans la condition contrainte en utilisant uniquement les ensembles de données officiellement fournis. Le système que nous proposons utilise un alignement souple de la région des mots pour la traduction automatique neuronale multimodale (MNMT). Les résultats expérimentaux évalués sur la métrique BLEU fournie par le site d'évaluation WAT 2021 montrent que le système TMEKU a atteint les meilleures performances parmi tous les systèmes participants. Une analyse plus approfondie de l'étude de cas montre que l'optimisation de l'alignement mot-région entre les modalités textuelles et visuelles est la clé de l'amélioration des performances de notre système TMEKU, ce qui permet une meilleure utilisation des informations visuelles.", 'ar': 'نقدم نظام TMEKU الخاص بنا المقدم إلى مهمة الترجمة الإنجليزية اليابانية متعددة الوسائط لـ WAT 2021. شاركنا في مهمة Flickr30kEnt-JP ومهمة MSCOCO متعددة الوسائط الغامضة في ظل ظروف مقيدة باستخدام مجموعات البيانات المقدمة رسميًا فقط. يستخدم نظامنا المقترح محاذاة ناعمة لمنطقة الكلمات للترجمة الآلية العصبية متعددة الوسائط (MNMT). تُظهر النتائج التجريبية التي تم تقييمها على مقياس BLEU المقدم من موقع تقييم WAT 2021 أن نظام TMEKU قد حقق أفضل أداء بين جميع الأنظمة المشاركة. يوضح التحليل الإضافي لدراسة الحالة أن الاستفادة من محاذاة منطقة الكلمات بين الأساليب النصية والمرئية هو المفتاح لتحسين الأداء في نظام TMEKU الخاص بنا ، مما يؤدي إلى استخدام أفضل للمعلومات المرئية.', 'ja': 'WAT 2021の英語-日本語マルチモーダル翻訳タスクに提出されたTMEKUシステムを紹介します。私たちは、公式に提供されたデータセットのみを使用して、制約条件下でFlickr 30 kEnt - JPタスクとあいまいなMSCOCOマルチモーダルタスクに参加しました。当社の提案するシステムは、マルチモーダルニューラルマシン翻訳（ MNMT ）のために単語領域のソフトアラインメントを採用しています。WAT 2021評価サイトによって提供されたBLEUメトリックで評価された実験結果は、TMEKUシステムがすべての参加システムの中で最高のパフォーマンスを達成していることを示しています。ケーススタディのさらなる分析は、テキストと視覚的なモダリティの間のワード-リージョンアラインメントを活用することが、TMEKUシステムのパフォーマンス向上の鍵であり、視覚的な情報のより良い使用につながることを示しています。', 'zh': '言交WAT 2021之英语 - 日语多式联运译者TMEKU系焉。 吾徒用官方之数集,与于受约束条件之下Flickr30kEnt-JP与Dimary MSCOCO多式焉。 其言系统用词区软对齐多模态神经机器翻译(MNMT)。 WAT 2021评估网站BLEU指标上估实验结果表明,TMEKU系统于诸参统中取最佳。 论事益分析表明,因文视式之间词区齐者TMEKU统性之要,而益用视息。', 'hi': 'हम अपने TMEKU प्रणाली WAT 2021 के लिए अंग्रेजी-जापानी मल्टीमॉडल अनुवाद कार्य के लिए प्रस्तुत पेश करते हैं। हमने केवल आधिकारिक तौर पर प्रदान किए गए डेटासेट का उपयोग करके विवश स्थिति के तहत Flickr30kEnt-JP कार्य और अस्पष्ट MSCOCO Multimodal कार्य में भाग लिया। हमारी प्रस्तावित प्रणाली मल्टीमॉडल न्यूरल मशीन अनुवाद (MNMT) के लिए शब्द-क्षेत्र के नरम संरेखण को नियोजित करती है। WAT 2021 मूल्यांकन साइट द्वारा प्रदान किए गए BLEU मीट्रिक पर मूल्यांकन किए गए प्रयोगात्मक परिणामों से पता चलता है कि TMEKU सिस्टम ने सभी भाग लेने वाले सिस्टमों के बीच सबसे अच्छा प्रदर्शन हासिल किया है। मामले के अध्ययन के आगे के विश्लेषण से पता चलता है कि पाठ्य और दृश्य तौर-तरीकों के बीच शब्द-क्षेत्र संरेखण का लाभ उठाना हमारे TMEKU सिस्टम में प्रदर्शन वृद्धि की कुंजी है, जो बेहतर दृश्य जानकारी उपयोग की ओर जाता है।', 'ga': 'Tugaimid isteach ár gcóras TMEKU a cuireadh faoi bhráid an Tasc Aistriúcháin Ilmhódúil Béarla-Seapáinis do WAT 2021. Ghlacamar páirt sa tasc Flickr30kEnt-JP agus tasc Ilmhódach débhríoch MSCOCO faoin gcoinníoll srianta ag baint úsáide as na tacair sonraí a cuireadh ar fáil go hoifigiúil amháin. Úsáideann ár gcóras atá beartaithe ailíniú bog de réigiún focal le haghaidh aistriúchán meaisín néar ilmhódach (MNMT). Léiríonn na torthaí turgnamhacha a ndearnadh meastóireacht orthu ar an méadrach BLEU arna sholáthar ag láithreán meastóireachta WAT 2021 go bhfuil an fheidhmíocht is fearr bainte amach ag córas TMEKU i measc na gcóras rannpháirteach go léir. Léiríonn anailís bhreise ar an gcás-staidéar gurb é ailíniú focal-réigiún idir na módúlachtaí téacsúla agus amhairc an eochair do fheabhsú feidhmíochta inár gcóras TMEKU, rud a fhágann go mbaintear úsáid níos fearr as faisnéis amhairc.', 'ru': 'Мы представляем нашу систему TMEKU, представленную в Задаче по мультимодальному переводу на английском и японском языках на WAT 2021. Мы участвовали в задаче Flickr30kEnt-JP и задаче Ambiguous MSCOCO Multimodal в ограниченном состоянии, используя только официально предоставленные наборы данных. Предлагаемая нами система использует мягкое выравнивание области слова для мультимодального нейронного машинного перевода (MNMT). Экспериментальные результаты, оцененные по метрике BLEU, предоставленной оценочной площадкой WAT 2021, показывают, что система TMEKU достигла наилучших показателей среди всех участвующих систем. Дальнейший анализ тематического исследования показывает, что использование выравнивания слов и областей между текстовыми и визуальными формами является ключом к повышению производительности в нашей системе TMEKU, что приводит к лучшему использованию визуальной информации.', 'hu': 'Bemutatjuk az angol-japán multimodális fordítási feladat keretében benyújtott TMEKU rendszerünket a WAT 2021-re. A Flickr30kEnt-JP feladatban és az Ambiguous MSCOCO Multimodális feladatban korlátozott feltételekkel vettünk részt, kizárólag a hivatalosan megadott adatkészleteket használva. Javasolt rendszerünk a multimodális neurális gépi fordításhoz (MNMT) a szó-régió lágy igazítását alkalmazza. A WAT 2021 értékelési helyszínen szolgáltatott BLEU metrikán értékelt kísérleti eredmények azt mutatják, hogy a TMEKU rendszer az összes résztvevő rendszer közül a legjobb teljesítményt érte el. Az esettanulmány további elemzése azt mutatja, hogy TMEKU rendszerünk teljesítményjavításának kulcsa a szó-régió igazítása a szöveg és a vizuális módok között, ami jobb vizuális információfelhasználáshoz vezet.', 'el': 'Παρουσιάζουμε το σύστημά μας που υποβλήθηκε στο έργο Πολυmodal Translation Αγγλικά-Ιαπωνικά για το WAT 2021. Συμμετείχαμε στην εργασία Flickr30kEnt-JP και στην αμφίβολη εργασία MSCOCO υπό την περιορισμένη προϋπόθεση χρησιμοποιώντας μόνο τα επίσημα παρεχόμενα σύνολα δεδομένων. Το προτεινόμενο σύστημα χρησιμοποιεί μαλακή ευθυγράμμιση λέξης-περιοχής για την πολυμορφική νευρωνική μηχανική μετάφραση (ΜΜΤ). Τα πειραματικά αποτελέσματα που αξιολογήθηκαν με τη μετρική που παρέχεται από τον χώρο αξιολόγησης δείχνουν ότι το σύστημα έχει επιτύχει την καλύτερη απόδοση μεταξύ όλων των συμμετεχόντων συστημάτων. Περαιτέρω ανάλυση της μελέτης περίπτωσης δείχνει ότι η αξιοποίηση της ευθυγράμμισης λέξης-περιοχής μεταξύ των κειμένων και οπτικών modalities είναι το κλειδί για την βελτίωση της απόδοσης στο σύστημά μας, γεγονός που οδηγεί σε καλύτερη χρήση οπτικών πληροφοριών.', 'ka': 'ჩვენ ჩვენი TMEKU-ის სისტემა გავაჩვენებთ, რომელიც ინგლისურ-იაპონურ მულტიმოდიალური გადაწყვეტილების რაოდენობაში WAT 2021. ჩვენ Flickr30kEnt-JP პარამეტრებში და მრავალური MSCOCO მრავალური დავალების შემდეგ შევცვალობით მხოლოდ მონაცემების გამოყენებით. ჩვენი საზოგადომის სისტემა მცირე სიტყვების რეგიონის მცირე დაწყება მცირემოდეალური ნეიროლური მაქანის გადაწყვეტისთვის (MNMT). ექსპერიმენტიური წარმოდგენები, რომლებიც BLEU მეტრიკაში გაუმუშავებული WAT 2021-ის გაუმუშავება, ჩვენებს, რომ TMEKU სისტემა უკეთესი გაუმუშავება ყველა დაწყვეტილი სის სხვა ანალიზაცია შესახებ შესახებ, რომ ტექსტულ და ვიზუალურ მოდიალებების განმავლობაში სიტყვების რეგიონის განმავლობაზე გამოყენება არის ჩვენი TMEKU სისტემაში გასაკეთებელი გასაკეთებელი გასაკეთებელი გასაკ', 'it': "Presentiamo il nostro sistema TMEKU sottoposto al compito di traduzione multimodale inglese-giapponese per WAT 2021. Abbiamo partecipato al compito Flickr30kEnt-JP e Ambiguous MSCOCO Multimodal in condizioni vincolate utilizzando solo i set di dati forniti ufficialmente. Il nostro sistema proposto utilizza l'allineamento morbido della parola-regione per la traduzione automatica neurale multimodale (MNMT). I risultati sperimentali valutati sulla metrica BLEU fornita dal sito di valutazione WAT 2021 mostrano che il sistema TMEKU ha raggiunto le migliori prestazioni tra tutti i sistemi partecipanti. Un'ulteriore analisi del caso studio dimostra che sfruttare l'allineamento parola-regione tra le modalità testuali e visive è la chiave per migliorare le prestazioni nel nostro sistema TMEKU, che porta a un migliore utilizzo delle informazioni visive.", 'lt': 'Įdiegime TMEKU sistemą, pateiktą anglų ir Japonijos daugiarūšio vertimo darbui „WAT 2021“. Mes dalyvavome Flickr30kEnt-JP užduotyje ir ambiguose MSCOCO daugiarūšio naudojimo užduotyse ribotos sąlygos naudojant tik oficialiai pateiktus duomenų rinkinius. Mūsų siūlomoje sistemoje naudojamas švelnus žodžių ir regiono suderinimas daugiarūšio nervinių mašinų vertimui (MNMT). Iš eksperimentinių rezultatų, įvertintų pagal BLEU metriją, pateiktą pagal WAT 2021 vertinimo vietą, matyti, kad TMEKU sistema pasiekė geriausius visų dalyvaujančių sistemų rezultatus. Tolesnė atvejų tyrimo analizė rodo, kad žodžių ir region ų derinimo tarp tekstinių ir vizualinių būdų sverto didinimas yra pagrindinis veiksnys gerinant mūsų TMEKU sistemą, dėl kurios geriau naudojama vizualinė informacija.', 'kk': 'Біз TMEKU жүйесімізді 2021 жылы WAT үшін ағылшын- жапон көптеген аудару тапсырмасына жіберіп тастадық. Біз Flickr30kEnt-JP тапсырмасына және бірнеше MSCOCO көпшілік тапсырмасына қатысу үшін тек әкімшілік берілген деректер жиындарын қолдану үшін шектелген жағдайда қатысу керек Біздің қолданылатын жүйеміз көпModal Neural Machine Translation (MNMT) үшін сөз аумағының жақсы түрлендірімін қолданады. ТМЕКУ жүйесі барлық қатысушылардың ең жақсы жұмыс істегенін көрсетеді. Мәтін және көрінетін әдістер арасындағы сөздер аумағының теңдеуі - TMEKU жүйесінде жақсы мәліметті қолдануға көмектесетін кілт.', 'ms': 'Kami memperkenalkan sistem TMEKU kami yang dihantar ke Tugas Terjemahan Multimodal Inggeris-Jepun untuk WAT 2021. Kami berpartisipasi dalam tugas Flickr30kEnt-JP dan tugas MSCOCO Multimodal Ambiguous di bawah keadaan yang diharamkan menggunakan hanya set data yang diberikan secara rasmi. Sistem yang diusulkan kami menggunakan penyesuaian lembut bagi kawasan perkataan untuk terjemahan mesin saraf multimodal (MNMT). Hasil percubaan yang diuji pada metrik BLEU yang disediakan oleh laman penilaian WAT 2021 menunjukkan bahawa sistem TMEKU telah mencapai prestasi terbaik di antara semua sistem yang berpartisipasi. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use.', 'mk': 'Го претставуваме нашиот TMEKU систем поднесен на англиско-јапонското мултимодилно преведување задача за WAT 2021. Ние учествувавме во задачата на Flickr30kEnt-JP и амбигиозната мултимедална задача на MSCOCO под ограничен услов користејќи само официјално обезбедени податоци. Нашиот предложен систем употребува меко прилагодување на регионот на зборови за мултимодилен нервен превод (MNMT). Експерименталните резултати проценети на метриката БЛЕУ обезбедена од веб-страницата за проценка WAT 2021 покажуваат дека системот TMEKU постигна најдобра резултат меѓу сите учествувачки системи. Понатамошната анализа на студијата на случајот покажува дека влијанието на зборот-регионот помеѓу текстуалните и визуелните модели е клучот за подобрување на перформансата во нашиот систем ТМЕКУ, што води до подобра употреба на визуелни информации.', 'ml': 'ഞങ്ങള്\u200d ഞങ്ങളുടെ ടെമെക്കു സിസ്റ്റം പരിചയപ്പെടുത്തുന്നത് വാട്ട് 2021-ന് ഇംഗ്ലീഷ്-ജപ്പാനീസ് മിടുട്ടിമോഡ ഞങ്ങള്\u200d ഫ്ലിക്കര്\u200d30കെന്\u200dറ്-ജെപിയുടെ ജോലിയില്\u200d പങ്കുചേര്\u200dത്തിരിക്കുന്നു. എംസ്കോയില്\u200d അംഗീകരിക്കുന്ന എംസ്കോ മിടുട് നമ്മുടെ പ്രൊദ്ദേശിക്കപ്പെട്ട സിസ്റ്റത്തിന്റെ വാക്കുകളുടെ സ്ഥാനത്ത് മെഫ്റ്റ് പ്രദേശങ്ങള്\u200dക്ക് വേണ്ടി മാ WAT 2021 വിന്യൂഷന്\u200d സൈറ്റില്\u200d നല്\u200dകിയ ബ്ലീയു മെട്രിക്കിന്\u200dറെ പരീക്ഷണ ഫലങ്ങള്\u200d വിലാസപ്പെടുത്തിയിരിക്കുന്നു ടെക്സ്കൂളിലും കാഴ്ചമുള്ള രീതികള്\u200dക്കും ഇടയിലുള്ള വാക്ക്- പ്രദേശത്തിനുമിടയിലുള്ള ഒരുക്കിവെക്കുന്നതിന്\u200dറെ കൂടുതല്\u200d അന്വേഷിക്കുന്നത് ടെമെക്', 'pl': 'Wprowadzamy nasz system TMEKU zgłoszony do zadania multimodalnego tłumaczenia angielsko-japońskiego dla WAT 2021. Uczestniczyliśmy w zadaniu Flickr30kEnt-JP oraz zadaniu Ambiguous MSCOCO Multimodal w warunkach ograniczonych wykorzystując tylko oficjalnie dostarczone zbiory danych. Proponowany przez nas system wykorzystuje miękkie wyrównanie regionu słowa do multimodalnego neuronowego tłumaczenia maszynowego (MNMT). Wyniki eksperymentalne oceniane na podstawie metryki BLEU dostarczonej przez stronę oceny WAT 2021 pokazują, że system TMEKU osiągnął najlepszą wydajność spośród wszystkich uczestniczących systemów. Dalsza analiza studium przypadku pokazuje, że wykorzystanie dostosowania słowa-regionu między modalnościami tekstowymi i wizualnymi jest kluczem do poprawy wydajności w naszym systemie TMEKU, co prowadzi do lepszego wykorzystania informacji wizualnych.', 'mn': 'Бид TMEKU системийг англи-Японы олон моделийн хөрөнгө хөрөнгө хөрөнгө хөрөнгө хөрөнгө оруулсан. Бид Flickr30kEnt Бидний санал өгсөн систем олон моделийн мэдрэлийн механикийн хөрөнгө (MNMT) хэлбэрийн хэмжээний энгийн хэмжээг ашигладаг. ТМЕКУ систем бүх оролцогчдын хамгийн сайн үйл ажиллагааг гаргасан гэдгийг харуулж байна. Мэдээллийн судалгааны дасгал шинжилгээ нь текстур болон харагдах арга замын хоорондох үгний бүс нутгийн тэгшитгэл нь бидний TMEKU системийн үйлдвэрлэлийн нэмэгдүүлэлтийн ач холбогдол юм. Энэ нь илүү харагдах мэдээллийн хэрэглээнд хүргэж', 'no': 'Vi introduserer TMEKU-systemet vårt som er sendt til den engelske-japanske multimodalet omsetjingsprogrammet for WAT 2021. Vi deltok på Flickr30kEnt-JP-oppgåva og omvendt MSCOCO-multimodal oppgåve under den avgrensa vilkåret med berre dei offisielle oppgjevne datasetta. Vårt foreslått systemet bruker mykt justering av ordområdet for multimodal neuralmaskinsomsetjing (MNMT). Den eksperimentelle resultaten evaluerte på BLEU-metrikken som er tilgjengeleg av evalueringsnettstaden WAT 2021 viser at TMEKU-systemet har oppnådd den beste utviklinga mellom alle delta systema. Dette er nøkkelen for å forbetra utviklingar i TMEKU-systemet vår, som fører til betre bruk av visuelle informasjon til å levera ordområdet mellom tekstuelle og visuelle måtar.', 'ro': 'Introducem sistemul nostru TMEKU depus la misiunea de traducere multimodală engleză-japoneză pentru WAT 2021. Am participat la sarcina Flickr30kEnt-JP și la sarcina Multimodală Ambiguă MSCOCO în condiția constrânsă folosind doar seturile de date furnizate oficial. Sistemul nostru propus utilizează alinierea ușoară a regiunii de cuvânt pentru traducerea automată neurală multimodală (MNMT). Rezultatele experimentale evaluate pe metrica BLEU furnizată de site-ul de evaluare WAT 2021 arată că sistemul TMEKU a obținut cea mai bună performanță dintre toate sistemele participate. Analiza ulterioară a studiului de caz demonstrează că valorificarea alinierii cuvânt-regiune între modalitățile textuale și vizuale este cheia îmbunătățirii performanței în sistemul nostru TMEKU, ceea ce duce la o mai bună utilizare a informațiilor vizuale.', 'sr': 'Predstavljamo naš TMEKU sistem podignut na engleski japanski multimodalni prevodni zadatak za WAT 2021. Učestvovali smo u zadatku Flickr30kEnt-JP-a i obični multimodalni zadatak MSCOCO-a pod ograničenim uvjetom koristeći samo službeno pružene kompete podataka. Naš predloženi sistem koristi meko usklađenje riječi-regiona za multimodalni prevod neuralnih strojeva (MNMT). Eksperimentalni rezultati koji su procijenjeni na metrici BLEU-a koje je pružio mjesto za procjenu WAT 2021 pokazuju da je sistem TMEKU postigao najbolji učinkovit među svim sudjelovanim sistemima. Daljnja analiza istraživanja slučajeva pokazuje da je ključ za poboljšanje provedbe u našem TMEKU sistemu, koji dovodi do boljih korištenja vizualnih informacija.', 'si': 'අපි අපේ TMEKU පද්ධතිය ප්\u200dරදානය කරනවා ඉංග්\u200dරීසිය- ජාපානි ගොඩක් මොඩිමෝඩාල් පද්ධතිය WAT 2021 වෙනුවෙන් ප්\u200d අපි Flickr30kENT-JP වැඩේ සහ ඇම්බිගුස් MSCOCO ගොඩමෝඩාල් වැඩේ සම්බන්ධ වෙලා තියෙන්නේ සාමාන්\u200dය විදිහට ප්\u200dරයෝජන විතරයි. අපේ ප්\u200dරශ්ණ පද්ධතිය පද්ධතියේ වචන- ප්\u200dරදේශයේ සාමාන්\u200dය සංවිධානය සඳහා විශ්වාස කරනවා (MNMT). පරීක්ෂණ ප්\u200dරතිචාර ප්\u200dරතිචාර පරීක්ෂණය විශ්වාස කරලා තියෙන්නේ WAT 2021 පරීක්ෂණ ස්ථානයෙන් ප්\u200dරතිචාර කරලා තියෙන්න ප්\u200dරශ්න විශ්ලේෂණයේ තවත් විශ්ලේෂණය පෙන්වන්නයි වචන ප්\u200dරදේශය සහ දර්ශණ ප්\u200dරමාණය අතර ප්\u200dරශ්නයක් පෙන්වන්නයි අපේ TMEKU පද්ධතියේ වැඩ', 'so': 'Waxaynu soo bandhignaa nidaamka TMEKU ee loo soo dhiibay shaqada tarjumaadda badan ee Ingiriis-Japanese WAT 2021. Waxaannu ka qayb galnay shaqada Flickr30kEnt-JP iyo Amarka Ambiguous MSCO Multimodal hoostooda iyadoo lagu isticmaalayo macluumaadka oo kaliya. nidaamkayaga la soo jeeday wuxuu u shaqeeyaa qoraal-hoos oo u qoran qoraalka hadalka oo kala duduwan tarjumaadda maskinada neurada (MNMT). Abaalka imtixaanka waxaa lagu qiimeeyay metrici BLEU ee lagu siiyay bogagga qiimeynta WAT 2021 waxay muuqatay in nidaamka TMEKU uu gaadhay si ugu fiican sameynta nidaamka ka qayb galay oo dhan. Fasirka dheeraad ah ee waxbarashada xaaladu wuxuu muujiyaa in la isbedelayo qoraalka iyo qaababka muuqashada u dhexeeya qoraalka iyo muuqashada waa furaha horumarinta nidaamka TMEKU.', 'sv': 'Vi introducerar vårt TMEKU-system som skickats in till den engelsk-japanska multimodala översättningsuppgiften för WAT 2021. Vi deltog i Flickr30kEnt-JP-uppgiften och Ambiguous MSCOCO Multimodal uppgiften under det begränsade villkoret med endast de officiellt tillhandahållna datauppsättningarna. Vårt föreslagna system använder mjuk justering av ordregion för multimodal neural maskinöversättning (MNMT). De experimentella resultaten som utvärderats på BLEU-mätaren från utvärderingsplatsen WAT 2021 visar att TMEKU-systemet har uppnått bästa prestanda bland alla deltagande system. Ytterligare analys av fallstudien visar att det är nyckeln till prestandaförbättring i vårt TMEKU-system att utnyttja ord-region-anpassning mellan text och visuella modaliteter, vilket leder till bättre visuell informationsanvändning.', 'ta': 'நாங்கள் WAT 2021 ஆங்கிலம்- ஜப்பானிய பல மொழிபெயர்ப்பு பணிக்கு வழங்கப்பட்ட எங்கள் TMEKU அமைப்பை அறிவிக்கிறோம். நாங்கள் Flickr30kEnt-JP பணியில் பங்கிட்டோம் மற்றும் Ambiguous MSCO Multimodal பணியில் உள்ள கட்டுப்படுத்தப்பட்ட நிலைமையில் மட்டும் வழங்கப்பட்ட எங்கள் பரிந்துரைக்கப்பட்ட அமைப்பு மென்மையான ஒழுங்கு வார்த்தை ஒழுங்குப்படுத்துகிறது பல முறைமையான புதிய இயந் WAT 2021 evaluation site வழங்கப்பட்ட BLEU மெட்ரிக் மீது சோதனையின் முடிவுகள் மதிப்பிடப்பட்டது TMEKU அமைப்பு அனைத்து பங்கீட்டு அமைப்புகளிலும் சிறந்த ச நிரல் மற்றும் பார்வை வகைகளுக்கிடையே உள்ள வார்த்தை- region ஒழுங்குப்படுத்தும் வார்த்தை ஒழுங்குப்படுத்தும் என்பதை காட்டுகிறது மேலும் விசை TMEKU', 'mt': 'Aħna nintroduċu s-sistema tagħna tat-TMEKU ppreżentata lill-Kompitu tat-Traduzzjoni Multimodali Ingliż-Ġappuniż għall-WAT 2021. Parteċipajna fil-kompitu Flickr30kEnt-JP u fil-kompitu multimodali ambigwu MSCOCO taħt il-kundizzjoni ristretta bl-użu biss tas-settijiet tad-dejta pprovduti uffiċjalment. Is-sistema proposta tagħna tuża allinjament dgħajjef tar-reġjun tal-kliem għat-traduzzjoni multimodali tal-magna newrali (MNMT). Ir-riżultati sperimentali evalwati fuq il-metrika BLEU ipprovduta mis-sit tal-evalwazzjoni WAT 2021 juru li s-sistema TMEKU kisbet l-aħjar prestazzjoni fost is-sistemi parteċipati kollha. Analiżi ulterjuri tal-istudju tal-każ turi li l-ingranaġġ tal-allinjament bejn il-kliem u r-reġjun bejn il-modalitajiet testwali u viżivi huwa ċ-ċavetta għat-titjib tal-prestazzjoni fis-sistema tagħna TMEKU, li twassal għal użu aħjar tal-informazzjoni viżiva.', 'ur': 'ہم نے اپنے TMEKU سیسٹم کو وات 2021 کے لئے انگلیسی-ژاپنی ملتی موڈال ترجمہ ٹاکس پر پیش کیا ہے۔ ہم نے فلیکر30kEnt-JP کا کام اور مضبوط MSCOCO Multimodal کے کام میں مشارکت کی محدود ہوئی شرط کے ذریعہ صرف رسمی ڈاٹ سٹ کے مطابق۔ ہماری پیشنهاد سیسٹم نے multimodal neural machine translation (MNMT) کے لئے لفظ منطقه کے نرم تفریق کا استعمال کرتا ہے. آزمائش نتیجے جو WAT 2021 کی ارزیابی سائٹ کے ذریعہ دیئے گئے بلیوس میٹریک پر مقرر کئے گئے ہیں دکھاتے ہیں کہ TMEKU سیستم نے تمام شرکت کرنے والے سیستموں کے درمیان بہترین فعالیت حاصل کی ہے. کیس تحقیقات کی اور زیادہ تحقیقات دکھاتی ہے کہ کلمات-منطقه کی تسبیح اور بصیرت کے درمیان تسبیح کرنا ہمارے TMEKU سیسٹم میں اضافہ کرنے کی کلید ہے، جو بہتر بصیرت معلومات کا استعمال کرتا ہے.', 'uz': "Biz WAT 2021 uchun ingliz- Yaponcha Multimodal tarjima vazifani ishga tayyorlanimiz. Biz Flickr30kEnt-JP vazifasiga ega bo'lgan va ajoyib MSCO Multimodal vazifasiga murakkab qildik, faqat faqat offisiy koʻrsatilgan maʼlumot sahifalarini ishlatish mumkin. Davom etilgan tizimmiz multimodal neural tarjimalari (MNMT) uchun so'zning soʻzni tarjima qiladi. TMEKU tizimi hamma qiymatga tayyorlangan tizimlarning eng eng yaxshi bajarish natijalariga qiymat beradi. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use.", 'vi': 'Chúng tôi xin giới thiệu hệ thống TMEKU của chúng tôi đã được đưa đến tác vụ đa truyền thông Anh-Nhật cho WAT 2021. Chúng tôi tham gia nhiệm vụ Flickr30kEnt-JP và nhiệm vụ phức tạp của MSCO dưới chế độ hạn chế chỉ dùng các tập tin được cung cấp chính thức. Hệ thống chúng tôi đề nghị áp dụng sự sắp xếp mềm của vùng chữ để dịch chuyển máy thần kinh đa phương (MX). Các kết quả thử nghiệm được đánh giá bằng hệ thống đo chính của WAT 2021 cho thấy rằng hệ thống TMEKU đã có hiệu suất tốt nhất trong tất cả các hệ thống đã có mặt. Thêm phân tích của nghiên cứu vụ án chứng minh rằng nhờ thao tác sắp xếp các từ-vùng giữa các phương thức cấu trúc và hình ảnh là mấu chốt để tăng cường hiệu suất trong hệ thống TMEKU, dẫn đến việc sử dụng thông tin thị giác tốt hơn.', 'nl': 'We introduceren ons TMEKU-systeem dat is ingediend bij de Engels-Japanse multimodale vertaaltaak voor WAT 2021. We namen deel aan de Flickr30kEnt-JP taak en Ambiguous MSCOCO Multimodal taak onder de beperkte voorwaarde gebruikend alleen de officieel verstrekte datasets. Ons voorgestelde systeem maakt gebruik van zachte uitlijning van woord-regio voor multimodale neurale machinevertaling (MNMT). De experimentele resultaten geëvalueerd op de BLEU-metric van de WAT 2021 evaluatiesite tonen aan dat het TMEKU-systeem de beste prestaties heeft bereikt onder alle deelnemende systemen. Verdere analyse van de casestudy toont aan dat het inzetten van woord-regio uitlijning tussen de tekstuele en visuele modaliteiten de sleutel is tot prestatieverbetering in ons TMEKU-systeem, wat leidt tot een beter gebruik van visuele informatie.', 'da': 'Vi introducerer vores TMEKU-system indsendt til den engelsk-japanske multimodale oversættelsesopgave for WAT 2021. Vi deltog i Flickr30kEnt-JP opgaven og Ambiguous MSCOCO Multimodal opgave under den begrænsede betingelse ved hjælp af kun de officielt leverede datasæt. Vores foreslåede system anvender blød justering af ord-region til multimodal neural maskinoversættelse (MNMT). De eksperimentelle resultater, der er evalueret på BLEU-metrikken fra WAT 2021 evalueringsstedet, viser, at TMEKU-systemet har opnået den bedste ydeevne blandt alle de deltagende systemer. Yderligere analyse af casestudiet viser, at udnyttelse af ord-region justering mellem tekst og visuelle modaliteter er nøglen til performance forbedring i vores TMEKU system, hvilket fører til bedre visuel informationsbrug.', 'id': 'Kami memperkenalkan sistem TMEKU kami yang dikirim ke Tugas Translation Multimodal Inggris-Jepang untuk WAT 2021. Kami berpartisipasi dalam tugas Flickr30kEnt-JP dan tugas MSCOCO Multimodal Ambiguous di bawah kondisi terbatas hanya menggunakan set data yang diberikan secara resmi. Sistem yang diusulkan kami menggunakan penyesuaian lembut dari daerah-kata untuk terjemahan mesin saraf multimodal (MNMT). Hasil percobaan yang diuji pada metrik BLEU yang diberikan oleh situs evaluasi WAT 2021 menunjukkan bahwa sistem TMEKU telah mencapai prestasi terbaik di antara semua sistem berpartisipasi. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use.', 'de': 'Wir stellen unser TMEKU-System vor, das der Englisch-Japanisch Multimodal Translation Task für WAT 2021 unterzogen wurde. Wir nahmen an der Flickr30kEnt-JP Aufgabe und der ambiguous MSCOCO Multimodal Aufgabe unter der eingeschränkten Bedingung teil und verwendeten nur die offiziell bereitgestellten Datensätze. Unser vorgeschlagenes System verwendet weiche Ausrichtung der Wortregion für multimodale neuronale maschinelle Übersetzung (MNMT). Die experimentellen Ergebnisse, die auf der BLEU-Metrik des WAT 2021 Evaluierungsstandorts ausgewertet wurden, zeigen, dass das TMEKU-System unter allen beteiligten Systemen die beste Leistung erzielt hat. Eine weitere Analyse der Fallstudie zeigt, dass die Nutzung der Wort-Region-Ausrichtung zwischen den textuellen und visuellen Modalitäten der Schlüssel zur Leistungssteigerung in unserem TMEKU-System ist, was zu einer besseren visuellen Informationsnutzung führt.', 'bg': 'Представяме нашата система ТМЕКУ, подадена към англо-японската мултимодална преводаческа задача за ВАТ 2021. Участвахме в задачата при ограничени условия, използвайки само официално предоставените набори от данни. Нашата предложена система използва меко подравняване на думата-регион за мултимодален неврален машинен превод (ММТ). Експерименталните резултати, оценени по показателя предоставен от обекта за оценка на УАТ 2021, показват, че системата ТМЕКУ е постигнала най-доброто представяне сред всички участващи системи. По-нататъшният анализ на казуса показва, че привличането на думата-регион между текстовите и визуалните модали е ключът към подобряване на производителността на нашата система, което води до по-добро използване на визуалната информация.', 'hr': 'Predstavljamo naš TMEKU sistem podignut na engleski japanski multimodalni prevodni zadatak za WAT 2021. Učestvovali smo u zadatku Flickr30kEnt-JP-a i obični multimodalni zadatak MSCOCO-a pod ograničenim uvjetom koristeći samo službeno pružene podatke. Naš predloženi sustav koristi meko usklađivanje riječi-regije za multimodalni prevod neuralnih strojeva (MNMT). Eksperimentalni rezultati procijenjeni na metrici BLEU-a pruženom mjestom ocjene WAT 2021 pokazuju da je sustav TMEKU postigao najbolji učinkovit među svim sudjelovanim sustavima. Daljnja analiza ispitivanja slučajeva pokazuje da je ključ za poboljšanje učinka u našem sustavu TMEKU-a, što vodi do boljih korištenja vizualnih informacija, učinkovitost usklađenja riječi i regija između tekstualnih i vizualnih načina.', 'fa': 'ما سیستم TMEKU را معرفی می\u200cکنیم که به دنبال ترجمه\u200cهای زیادی ژاپن و ژاپن برای WAT 2021 فرستاده شده است. ما در وظیفه\u200cی Flickr30kEnt-JP و مأموریت Multimodal MSCOCO با استفاده از مجموعه داده\u200cهای رسمی شرکت کردیم. سیستم پیشنهاد ما برای ترجمه\u200cهای ماشین\u200cهای عصبی چندmodal (MNMT) استفاده می\u200cکند. نتایج آزمایشی که روی متریک BLEU ارزیابی شده توسط محل ارزیابی WAT 2021 نشان داده است که سیستم TMEKU بهترین عملکرد بین تمام سیستم\u200cهای مشترک را رسید. تحلیل بیشتری از مطالعه پرونده نشان می\u200cدهد که تطبیق کردن منطقه\u200cی کلمات بین روش\u200cهای متن و دیده\u200cای کلید افزایش عملکرد در سیستم TMEKU ما است، که به استفاده از اطلاعات دیده\u200cتر منجر می\u200cشود.', 'tr': 'TMEKU sistemamyzy Iňlisçe-japonça Multimodal terjime görevini WAT 2021 üçin bellendik. Biz Flickr30kEnt-JP buýrukynda we Ambiguous MSCOCO Multimodal buýrukynda diňe resmi berilýän veri setirlerini ullanýarys. Biziň teklip eden sistemimiz multimodal neural maşynyň terjime (MNMT) üçin ýuwaş söz bölegiň ýerini ulanýar. WAT 2021-nji ýygnam meýdançasynda BLEU metriýasynda deňlenen çykyş netijesi TMEKU sisteminiň ähli chikany sistemalaryň arasynda iň gowy çykyşlygyny ýetirdi. Köp esasy analýusiniň daşyrky analýusiny tekst we görsel modalaryň arasynda söz bölegi çyzygymyzy täsirleşdirmek üçin TMEKU sistemimizde täzelleşdirmek üçin açary. Bu şekilde görsel maglumatyň ullanyşyny gowy gösterir.', 'sw': 'Tunawasilisha mfumo wetu wa TMEKU uliotolewa kwenye kazi ya Tafsiri ya Kiingereza na Kijapani kwa ajili ya WAT 2021. We participated in the Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets.  Mfumo wetu unapendekezwa unatumia usambazaji wa mkoa wa maneno kwa ajili ya kutafsiri mashine kadhaa ya neura (MNMT). Matokeo ya majaribio yalitathmini kwenye mbinu ya BLEU iliyotolewa na tovuti ya uchunguzi wa WAT 2021 yanaonyesha kuwa mfumo wa TMEKU umepata ufanisi bora zaidi miongoni mwa mifumo yote ya ushiriki. Uchambuzi zaidi wa utafiti huo unaonyesha kuwa upatikanaji wa maneno kati ya njia za msingi na kuona ni ufunguo wa kuongezeka katika mfumo wetu wa TMEKU, ambao unapelekea matumizi ya habari bora ya kuonekana.', 'ko': 'WAT 2021 영일 다중모드 번역 임무에 제출된 TMEKU 시스템을 소개합니다.저희는 공식적으로 제공한 데이터 집합만 사용하고 제한된 조건에서 Flickr30kEnt JP 작업과 모호 MSCOCO 다중모드 작업에 참여했습니다.우리가 제시한 시스템은 다중모드신경기계번역(MNMT)에서 단어 영역의 소프트 정렬을 채택했다.WAT 2021 평가 웹사이트에서 제공하는 BLEU 지표 평가 실험 결과에 따르면 TMEKU 시스템은 모든 참여 시스템에서 최상의 성능을 얻었다.사례 연구에 대한 진일보한 분석에 따르면 텍스트와 시각 모델 사이의 단어-구역 정렬을 이용하는 것은 우리의 TMEKU 시스템 성능 강화의 관건이고 이는 더욱 좋은 시각 정보 사용을 초래할 것이다.', 'af': 'Ons introduseer ons TMEKU stelsel aan die Engelse-Japanse Multimodale Vertaling Taak vir WAT 2021. Ons het gedeel in die Flickr30kEnt-JP taak en omgewing MSCOCO Multimodale taak onder die beperkte voorwaarde gebruik slegs die offisieel verskaf datastelle. Ons voorgestelde stelsel gebruik sagte lyn van woord- regione vir multimodale neurale masjien vertaling (MNMT). Die eksperimentele resultate wat op die BLES metrie gegee is deur die WAT 2021 evaluasie tuiste vertoon het dat die TMEKU stelsel die beste prestasie onder al die gedeeltelike stelsels bereik het. Verdere analisie van die geval studie bevestig dat die verwysing van woord-regionale belyning tussen die tekstuele en visuele modaliteite is die sleutel om voorspoediging te verhoog in ons TMEKU stelsel, wat lei na beter visuele inligting gebruik.', 'am': 'የቴሜKU ስርዓታችንን ለWAT 2021 ለመንግልዝኛ-ጃፓን Multimodal ትርጉም ስራዎችን እናቀርባታለን፡፡ በFlickr30kEnt-JP ስራ እና አካባቢ MSCO Multimodal ስራ በተገኘው ሥርዓት ዳታዎችን ብቻ በመጠቀም ተጋጠመን ነበር፡፡ በተዘጋጀው ስርዓታችን ለብዙኃላዊ የናውሬል መኪና ትርጓሜ (MNMT) የቃላት ክልል አቅራቢያ ቀላል ነው፡፡ የፈተናው ውጤቶች በWAT 2021 ማስታወቂያ ድረ ገጽ ላይ በቢሌউ ሜትሪክ ላይ የተሰኘው የTMEKU ስርዓት በተጋጠሙት ስርዓቶች ሁሉ መካከል የተሻለ ውጤት እንዳደረገ ያሳያል፡፡ የጉዳዩ ትምህርት ግንኙነትን በጽሑፍ እና በሚያይ መልዕክቶች መካከል ቃላትን-ክልል ማቀናቀል የሚያሳየው የቴሜክዩን ስርዓት ማድረግ መክፈቻ ነው፡፡', 'az': 'TMEKU sistemimizi WAT 2021 üçün İngilizce-Japonca çoxlu modal tercümə işinə göndərdik. Biz Flickr30kEnt-JP işin ə və müəyyən MSCOCO çoxlu modal işlərə sadəcə resmi verilən qurğular vasitəsilə istifadə etdik. Bizim təbliğ edilmiş sistemimiz çoxlu modal nöral maşına çevirilməsi (MNMT) üçün söz bölgesinin yumuşaq tərəflənməsini istifadə edir. TMEKU sisteminin bütün iştirak sistemlərinin ən yaxşı performansını başa çatdığını göstərir. Bu məsələnin daha çox analizi göstərir ki, textual və visual modüllər arasında söz bölgesinin tərəfləndirilməsi TMEKU sistemimizdə daha yaxşı görünür məlumatların istifadəsinə yol açar.', 'sq': 'Ne prezantojmë sistemin tonë TMEKU të paraqitur në detyrën anglo-japoneze të përkthimit multimodal për WAT 2021. Ne morëm pjesë në detyrën Flickr30kEnt-JP dhe detyrën Ambiguous MSCOCO Multimodal nën kushtin e kufizuar duke përdorur vetëm të dhënat zyrtarisht të ofruara. Sistemi ynë i propozuar përdor rregullimin e butë të fjalë-rajonit për përkthimin multimodal të makinave nervore (MNMT). Rezultatet eksperimentale të vlerësuara në metrikën BLEU të ofruar nga faqja e vlerësimit WAT 2021 tregojnë se sistemi TMEKU ka arritur performancën më të mirë midis të gjitha sistemeve të pjesëmarrshëm. Analiza e mëtejshme e studimit të rastit tregon se zgjidhja e përshtatjes fjalë-rajon midis modaliteteve tekstuale dhe vizuale është çelësi për përmirësimin e performancës në sistemin tonë TMEKU, i cili çon në përdorimin më të mirë të informacionit vizual.', 'bn': 'আমরা আমাদের টেমেকিউ সিস্টেমের পরিচয় করিয়ে দিচ্ছি যা ওয়াট ২০১২ সালের জন্য ইংরেজী মাল্টিমোডাল অনুবাদ করা হয়েছে। আমরা ফ্লিকার্৩০ কেন্দ্র-জেপি কাজে অংশগ্রহণ করেছি এবং বাধ্যতামূলক এমস্কো মাল্মোডাল কাজের অধীনে শুধুমাত্র আনুষ্ঠানিকভাবে প্ আমাদের প্রস্তাবিত ব্যবস্থা মাল্টিমোডাল নিউরেল মেশিন অনুবাদের জন্য শব্দ-অঞ্চলের নম্রভাবে স্থাপন করে। বিলিউ মেট্রিকে বিভিন্ন পরীক্ষার ফলাফলের মূল্য দেখা যাচ্ছে যে টিমেকিউ সিস্টেম সকল অংশগ্রহণকারী সিস্টেমের মধ্যে সবচেয়ে ভালো কার্যক এই কেস গবেষণার আরো বিশ্লেষণ প্রদর্শন করে যে টেক্সচুয়াল এবং দৃষ্টিভঙ্গির মধ্যে শব্দ-অঞ্চলের একত্রিত করার চাবি হচ্ছে আমাদের টেমেকেইউ সিস্টেমে বৃদ্ধি প', 'hy': 'Մենք ներկայացնում ենք մեր ԹՄԵԿU համակարգը, որը ներկայացվել է Անգլերեն-ճապոներեն բազմամոդալ թարգմանման առաջադրանքին, որը կատարվում է 2021 թվականին: Մենք մասնակցեցինք Flickr30kENT-JP-ի խնդրին և MSCOCOCOCOCOL-ի բազմամոդալ խնդրին սահմանափակ պայմաններում, օգտագործելով միայն պաշտոնապես տրամադրված տվյալների համակարգերը: Մեր նախագծված համակարգը օգտագործում է բառերի և տարածքի թեթև հարմարեցման բառերի միջավայրի համար բազմամոդալ նյարդային մեքենայի (MNMT) թարգմանման համար: Փորձարկման արդյունքները, որոնք գնահատվել են ԲԼԵՎ-ի մետրիկայի վրա, որը տրամադրվել է ՎԱԹ 2021-ի գնահատման կայքում, ցույց են տալիս, որ ԹՄԵԿ համակարգը հասել է ամենալավ արդյունքին բոլոր մասնակցում առկա համակար Հետագա ուսումնասիրության վերլուծությունը ցույց է տալիս, որ գրական և տեսողական միջոցների միջև բառի-տարածաշրջանի հարմարեցման ազդեցությունը մեր ԹՄԵԿU համակարգի արդյունավետության բարելավման կարևոր է, ինչը հանգեցնում է ավելի լավ տեսողական տեղեկատվության օգտագործ', 'bs': 'Predstavljamo naš TMEKU sistem predan engleskom-japanskom multimodalnom prevodnom zadatku za WAT 2021. Učestvovali smo u zadatku Flickr30kEnt-JP-a i obični multimodalni zadatak MSCOCO-a pod ograničenim uvjetom koristeći samo službeno pružene kompete podataka. Naš predloženi sistem koristi meko usklađenje riječi-regiona za multimodalni prevod neuralnih strojeva (MNMT). Eksperimentalni rezultati koji su procijenjeni na metrici BLEU-a koje je pružio mjesto procjene WAT 2021 pokazuju da je sistem TMEKU postigao najbolji učinkovit među svim sudjelovanim sistemima. Daljnja analiza istraživanja slučajeva pokazuje da je ključ za poboljšanje provedbe u našem TMEKU sustavu, što vodi do boljih korištenja vizualnih informacija.', 'ca': "Introduïm el nostre sistema TMEKU submetit a la tasca de traducció multimodal anglès-japonès per WAT 2021. Vam participar en la tasca Flickr30kEnt-JP i en la tasca ambigua MSCOCO Multimodal sota la condició restringida utilitzant només els conjunts de dades oficialment proporcionats. El nostre sistema proposat utilitza un alliniament suau de la regió de paraules per a la traducció multimodal de màquines neurals (MNMT). Els resultats experimentals avaluats en la mètrica BLEU proporcionada pel lloc d'avaluació WAT 2021 mostren que el sistema TMEKU ha aconseguit el millor rendiment entre tots els sistemes participats. Una altra anàlisi de l'estudi de cas demostra que aprofitar l'alliniament de paraules a regió entre les modalitats textuals i visuals és la clau per millorar el rendiment del nostre sistema TMEKU, que porta a un millor ús de la informació visual.", 'et': 'Tutvustame TMEKU süsteemi, mis on esitatud WAT 2021 inglise-jaapani multimodaalse tõlke ülesandele. Osalesime Flickr30kEnt-JP ülesandes ja Ambiguous MSCOCO Multimodal ülesandes piiratud tingimustes, kasutades ainult ametlikult esitatud andmekogumeid. Meie kavandatud süsteem kasutab sõna-piirkonna pehmet joondamist multimodaalse neuraalse masintõlke (MNMT) jaoks. WAT 2021 hindamiskoha BLEU mõõdikul hinnatud katsetulemused näitavad, et TMEKU süsteem on saavutanud kõigi osalenud süsteemide seas parima tulemuse. Juhtumiuuringu edasine analüüs näitab, et sõna-piirkondliku joondamise võimendamine teksti ja visuaalse modaalsuse vahel on võti meie TMEKU süsteemi jõudluse parandamisel, mis viib visuaalse info parema kasutamiseni.', 'fi': 'Esittelemme TMEKU-järjestelmämme, joka on toimitettu englannin-japanin multimodaaliseen käännöstehtävään WAT 2021. Osallistuimme Flickr30kEnt-JP-tehtävään ja Ambiguous MSCOCO Multimodal -tehtävään rajoitetussa tilassa käyttäen vain virallisesti toimitettuja aineistoja. Ehdotettu järjestelmä käyttää sanaalueen pehmeää linjausta multimodaaliseen neurokonekäännökseen (MNMT). WAT 2021 -arviointipaikan BLEU-mittarilla arvioidut kokeelliset tulokset osoittavat, että TMEKU-järjestelmä on saavuttanut parhaan suorituskyvyn kaikista osallistuneista järjestelmistä. Tapaustutkimuksen jatkoanalyysi osoittaa, että sana-alue-linjauksen hyödyntäminen tekstien ja visuaalisten muotojen välillä on avain suorituskyvyn parantamiseen TMEKU-järjestelmässä, mikä johtaa visuaalisen tiedon parempaan käyttöön.', 'cs': 'Představujeme náš systém TMEKU předložený do anglicko-japonského multimodálního překladu pro WAT 2021. Podíleli jsme se na úkolu Flickr30kEnt-JP a Ambiguálním multimodálním úkolu MSCOCO za omezených podmínek pomocí pouze oficiálně poskytnutých datových sad. Náš navržený systém využívá měkké zarovnání slovní oblasti pro multimodální neuronový strojový překlad (MNMT). Experimentální výsledky hodnocené na metrice BLEU poskytnuté na hodnotícím místě WAT 2021 ukazují, že systém TMEKU dosáhl nejlepšího výkonu ze všech zúčastněných systémů. Další analýza případové studie ukazuje, že využití sladění slova a oblasti mezi textovými a vizuálními modalitami je klíčem ke zlepšení výkonnosti v našem systému TMEKU, což vede k lepšímu využití vizuálních informací.', 'jv': 'Awak dhéwé nggunakake sistem tMEKu sing ngewat kanggo Terjamahan Inggris-Jepang Multimodal kanggo WAT 2020. Awak dhéwé wis ngubah mulai ing undo-type Sistem sing paling nggawe nyimpen tanggal nggawe barang-tambah kanggo terjamahan sistem multimodal Neral (MNMT). Rejalaké sing berarti nggawe barang nggawe barang nggawe barang-barang sing nyengkuyung nggawe barang "WAT 2020" nganggep kuwi sistem TOMEKA kang sampeyan akeh gawe sistem sing bisa nyengkuyung ing nggawe sistem sing bisa sudahan. Ndeleksyon langkung gadoh pirsak ngomong nik nggawe geraraning nglanggar aturan gambar nglanggar modalité textual karo iso nggambar kanggo nglanggar aturan kanggo nggawe sistem PMEKu, sing bisa ngelarang informasi sing luwih apik.', 'he': 'We introduce our TMEKU system submitted to the English-Japanese Multimodal Translation Task for WAT 2021.  השתתפנו במשימה Flickr30kEnt-JP ומשימה מרובת MSCOCO אמביגות תחת המצב המוגבל בשימוש רק קבוצות נתונים שנוספו באופן רשמי. Our proposed system employs soft alignment of word-region for multimodal neural machine translation (MNMT).  The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among all the participated systems.  ניתוח נוסף של מחקר המקרים מראה ששימוש מילים-אזור התאמה בין מודיאליות הרקסטיות ויזואליות הוא המפתח לשיפור ביצועים במערכת TMEKU שלנו, מה שמוביל להשתמש במידע ויזואלי טוב יותר.', 'ha': "Tuna ƙara fassarar TMEKU da aka wajabta zuwa Tafiyar tarjima na Ingiriya-japane mulmodal na WAT 2021. Mun yi rabo da aikin Flickr30 kEnt-JP da Ambassaous M-CO mulmodal Under the binding state only by the offisive-donated data set. @ item: inmenu Matarin jarrabai sun yi evaluated a kan metric BLEU da aka ba da WAT 2021 na nuna cewa, tsarin TMEKU ya sami mafi kyãwo a cikin duk tsarin da suka yi shirin. Furan Anarari na fassarar al'amarin ya nuna cewa, yana iya samarwa da tsarin kalma-region a tsakanin matsalan da tsari masu gani, shi ne maɓallin gyarata mai ƙaranci cikin tsarin TMEKU, da yana ƙara wa amfani da mafiya kyau ga gannai.", 'sk': 'Predstavljamo naš sistem TMEKU, predložen v angleško-japonsko multimodalno prevajalsko nalogo WAT 2021. Pri opravilu Flickr30kEnt-JP in Ambiguous MSCOCO Multimodal smo sodelovali pod omejenim pogojem z uporabo samo uradno zagotovljenih naborov podatkov. Naš predlagani sistem uporablja mehko poravnavo besedne regije za multimodalni nevronski strojni prevod (MNMT). Eksperimentalni rezultati, ocenjeni na merilu BLEU, ki jo zagotavlja ocenjevalna mesta WAT 2021, kažejo, da je sistem TMEKU dosegel najboljšo zmogljivost med vsemi sodelujočimi sistemi. Nadaljnja analiza študije primera kaže, da je izkoriščanje usklajenosti besed-regij med besedilnimi in vizualnimi modalinami ključnega pomena za izboljšanje zmogljivosti v našem sistemu TMEKU, kar vodi k boljši uporabi vizualnih informacij.', 'bo': 'ང་ཚོས་རང་གི་TMEKU་མ་ལག་ལ་དབྱིན་ཡིག་ཆ་རྩལ་བ་སྤྲོད་ཀྱི་ལས་འགུལ་སྤྲོད་བྱས་པ་ཡིན། WAT 2021 ང་ཚོས་Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets. ཨ་ན། ང་ཚོའི་འཆར་བཀོད་པའི་མ་ལག The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among the participating systems. ཡིན་ན་ལྟ་བུ་ཚོའི་དབྱེ་ཞིབ་ཀྱི་ཆ་འཕྲིན་དེ་ལས་ཡིག་གེ་ཡིག་དང་མཐོང་ནུས་ཀྱི་ཐབས་ལམ་དབར་གྱི་གྲལ་སྒྲིག་ཕྱོགས་སྟོན་པའི་གལ་ཆོག་འདི་ང་ཚོའི་TMEKU་གི་མ'}
{'en': 'Optimal Word Segmentation for  Neural Machine Translation  into Dravidian Languages D ravidian Languages', 'es': 'Segmentación óptima de palabras para la traducción automática neuronal a idiomas dravídicos', 'pt': 'Segmentação ideal de palavras para tradução automática neural em idiomas dravidianos', 'ar': 'تجزئة الكلمات المثلى لترجمة الآلة العصبية إلى لغات درافيدية', 'hi': 'द्रविड़ भाषाओं में तंत्रिका मशीन अनुवाद के लिए इष्टतम शब्द विभाजन', 'ja': 'ドラヴィダ語への神経機械翻訳のための最適なワードセグメンテーション', 'ru': 'Оптимальная сегментация слов для нейронного машинного перевода на дравидийские языки', 'fr': 'Segmentation optimale des mots pour la traduction automatique neuronale dans les langues dravidiennes', 'zh': '神经机器翻译成达罗毗荼语最佳分词', 'ga': 'An Deighilt Focal is Fearr le haghaidh Neural Machine Aistriú go Teangacha Dravidian', 'hu': 'Optimális szószegmentáció idegi gépi fordításhoz dravidi nyelvekre', 'ka': 'Name', 'el': 'Βέλτιστη τμηματοποίηση λέξεων για τη νευρωνική μηχανική μετάφραση σε δραβιδικές γλώσσες', 'it': 'Segmentazione ottimale delle parole per la traduzione automatica neurale in lingue dravidiane', 'lt': 'Optimali žodžių segmentacija, skirta neurologiniam mašinų vertimui į dravidines kalbas', 'kk': 'Невралдық машинаның аудару үшін дравидиялық тілдеріне оңтүстік сөз сегментациясы', 'mk': 'Оптимална секментација на зборови за превод на неврални машини на дравидиски јазици', 'ms': 'Segmentasi Perkataan Optimal untuk Terjemahan Mesin Neural ke Bahasa Dravidian', 'ml': 'ഡ്രാവിഡിയന്\u200d ഭാഷകളിലേക്കു് നെയുറല്\u200d മെഷീന്\u200d പരിഭാഷപ്പെടുത്തുന്നതിനുള്ള ഓപ്റ്റിമാല്\u200d വാക്ക് വ', 'mt': 'Segmentazzjoni ottimali tal-kliem għat-Traduzzjoni tal-Magna Newrali f’Lingwi Dravidjani', 'no': 'Optimalt ordsegmentasjon for neuralmaskinsomsetjing til dravidianske språk', 'pl': 'Optymalna segmentacja słowa dla neuronowego tłumaczenia maszynowego na języki drawidyjskie', 'mn': 'Дравидийн хэл руу мэдрэлийн машины хөгжлийн эерэг үг хэвлэлт', 'si': 'Name', 'sv': 'Optimal ordsegmentering för neural maskinöversättning till dravidiska språk', 'ta': 'திராவிடியாவின் மொழிகளில் மொழிபெயர்ப்புக்கான நெயுரல் இயந்திரத்திற்கான வார்த்தை பிரிவு', 'sr': 'Optimalna segmentacija reèi za neurološki prevod mašine na Dravidski jezik', 'ur': 'ڈراویڈی زبانوں میں نئورل ماشین ترجمہ کے لئے اپنا کلام سیگنٹمنٹ', 'so': 'Segment of words for Neural machine Translation into Dravidian languages', 'ro': 'Segmentarea optimă a cuvintelor pentru traducerea automată neurală în limbi dravidiene', 'uz': 'Name', 'vi': 'KCharselect unicode block name Dịch vào Ngôn ngữ quyến rũ', 'bg': 'Оптимална сегментация на думи за неврален машинен превод на дравидски езици', 'hr': 'Optimalna segmentacija riječi za prevod neuroloških strojeva na Dravidianske jezike', 'nl': 'Optimale woordsegmentatie voor neuronale machinevertaling naar Dravidische talen', 'da': 'Optimal ordsegmentering til neural maskinoversættelse til dravidiske sprog', 'fa': 'جدا کردن کلمه optimal for Neural Machine Translation into Dravidian Languages', 'de': 'Optimale Wortsegmentierung für neuronale maschinelle Übersetzung in dravidische Sprachen', 'ko': '신경기계가 델라비아 언어의 가장 좋은 분사로 번역되다', 'tr': 'Neural Maşynyň terjime edilen söz Segmentation for Optimal Word Segmentation for Neural Machine Translations into Dravidian Languages', 'sw': 'Segmentation of Optimal Word for Neural Machine Translation into lugha za Dravidian', 'af': 'Optimale Woord Segmentasie vir Neural Masjien Vertaling in Dravidian Taal', 'sq': 'Segmentacioni optimal i fjalëve për përkthimin e makinës nervore në gjuhët dravidiane', 'am': 'ምርጫዎች', 'az': 'N칬ral ma코in t톛rc칲m톛si 칲칞칲n Dravid dill톛rin톛 Optimal Kelimin Segmentation for Neural Machine Translation into Dravidian Languages', 'bn': 'Name', 'bs': 'Optimalna segmentacija riječi za prevod neuroloških strojeva na Dravidske jezike', 'hy': 'Նյարդային մեքենայի թարգմանման լավագույն բառերի սեգմենտացիան դեպի դրավիդիական լեզուներ', 'cs': 'Optimální segmentace slov pro neuronový strojový překlad do dravidských jazyků', 'et': 'Optimaalne sõna segmenteerimine neuroaalse masintõlke jaoks dravidia keeltesse', 'fi': 'Optimaalinen sanasegmentointi neurokonekääntämiseen dravidialaisille kielille', 'id': 'Segmentasi Perkataan Optimal untuk Translation Mesin Neural ke Bahasa Dravidia', 'ca': 'Segmentació óptima de paraules per la traducció de màquines neurones a llengües dravídiques', 'sk': 'Optimalna segmentacija besed za živčni strojni prevod v dravidske jezike', 'jv': 'Optical Word segmentation for Neral Masine translation in Dravideo Languages', 'he': 'סגמנטציה אופטימית של מילים לתרגום מכונת נוירולית לשפות דראבידיות', 'ha': 'KCharselect unicode block name', 'bo': 'ཆས་གཞུང་གི་འགོད་སྤྱོད་ལ་ཆུང་བའི་ཡིག་ཆ་ཆ་སྒྲིག་ཆ་རྣམས་པ།'}
{'en': 'Dravidian languages, such as  Kannada  and  Tamil , are notoriously difficult to translate by state-of-the-art neural models. This stems from the fact that these  languages  are morphologically very rich as well as being low-resourced. In this paper, we focus on subword segmentation and evaluate Linguistically Motivated Vocabulary Reduction (LMVR) against the more commonly used SentencePiece (SP) for the task of translating from  English  into four different  Dravidian languages . Additionally we investigate the optimal subword vocabulary size for each language. We find that SP is the overall best choice for segmentation, and that larger dictionary sizes lead to higher translation quality.', 'es': 'Los idiomas dravídicos, como el kannada y el tamil, son notoriamente difíciles de traducir mediante modelos neuronales de última generación. Esto se debe al hecho de que estos idiomas son morfológicamente muy ricos, además de tener pocos recursos. En este artículo, nos centramos en la segmentación de subpalabras y evaluamos la Reducción de Vocabulario Lingüísticamente Motivado (LMVR) en comparación con la SentencePiece (SP) más utilizada para la tarea de traducir del inglés a cuatro idiomas dravídicos diferentes. Además, investigamos el tamaño óptimo de vocabulario de subpalabras para cada idioma. Consideramos que SP es la mejor opción en general para la segmentación y que los tamaños de diccionario más grandes conducen a una mayor calidad de traducción.', 'fr': "Les langues dravidiennes, telles que le kannada et le tamoul, sont notoirement difficiles à traduire par des modèles neuronaux de pointe. Cela tient au fait que ces langues sont morphologiquement très riches et qu'elles disposent de peu de ressources. Dans cet article, nous nous concentrons sur la segmentation des sous-mots et évaluons la réduction de vocabulaire motivée par la linguistique (LMVR) par rapport au SentencePiece (SP) le plus couramment utilisé pour la traduction de l'anglais vers quatre langues dravidiennes différentes. De plus, nous étudions la taille optimale du vocabulaire des sous-mots pour chaque langue. Nous constatons que SP est le meilleur choix pour la segmentation, et que des tailles de dictionnaire plus grandes entraînent une meilleure qualité de traduction.", 'ar': 'تشتهر لغات درافيدية ، مثل الكانادا والتاميلية ، بصعوبة ترجمتها من خلال أحدث النماذج العصبية. ينبع هذا من حقيقة أن هذه اللغات غنية جدًا من الناحية الشكلية فضلاً عن كونها منخفضة الموارد. في هذه الورقة ، نركز على تجزئة الكلمات الفرعية وتقييم تقليل المفردات ذات الدوافع اللغوية (LMVR) مقابل SentencePiece (SP) الأكثر استخدامًا لمهمة الترجمة من الإنجليزية إلى أربع لغات Dravidian مختلفة. بالإضافة إلى ذلك ، نتحرى عن الحجم الأمثل لمفردات الكلمات الفرعية لكل لغة. وجدنا أن SP هو الخيار الأفضل بشكل عام للتجزئة ، وأن أحجام القاموس الأكبر تؤدي إلى جودة ترجمة أعلى.', 'pt': 'As línguas dravidianas, como Kannada e Tamil, são notoriamente difíceis de traduzir por modelos neurais de última geração. Isso decorre do fato de que essas línguas são morfologicamente muito ricas, além de terem poucos recursos. Neste artigo, nos concentramos na segmentação de subpalavras e avaliamos a Redução de Vocabulário Motivado Linguisticamente (LMVR) em relação ao SentencePiece (SP) mais comumente usado para a tarefa de traduzir do inglês para quatro idiomas dravidianos diferentes. Além disso, investigamos o tamanho ideal do vocabulário de subpalavras para cada idioma. Achamos que SP é a melhor escolha geral para segmentação e que tamanhos de dicionário maiores levam a uma maior qualidade de tradução.', 'ja': 'カンナダ語やタミル語などのドラヴィダ語は、最先端のニューラルモデルによって翻訳することが非常に困難です。これは、これらの言語が低資源であるだけでなく、形態的に非常に豊富であることに由来します。この論文では、サブワードのセグメンテーションに焦点を当て、英語から4つの異なるドラヴィダ語に翻訳する作業のために、より一般的に使用されるSentencePiece （ SP ）と比較して、Linguistically Motivated Vocabulary Reduction （ LMVR ）を評価します。さらに、各言語の最適なサブワードボキャブラリサイズを調査します。SPはセグメンテーションにとって全体的に最適な選択肢であり、辞書のサイズが大きいほど翻訳品質が向上します。', 'hi': 'कन्नड़ और तमिल जैसी द्रविड़ भाषाओं को अत्याधुनिक तंत्रिका मॉडल द्वारा अनुवाद करना कुख्यात रूप से मुश्किल है। यह इस तथ्य से उपजा है कि ये भाषाएं रूपात्मक रूप से बहुत समृद्ध होने के साथ-साथ कम संसाधन वाली हैं। इस पेपर में, हम सबवर्ड विभाजन पर ध्यान केंद्रित करते हैं और अंग्रेजी से चार अलग-अलग द्रविड़ भाषाओं में अनुवाद करने के कार्य के लिए अधिक आमतौर पर उपयोग किए जाने वाले वाक्यपीस (एसपी) के खिलाफ भाषाई रूप से प्रेरित शब्दावली कटौती (एलएमवीआर) का मूल्यांकन करते हैं। इसके अतिरिक्त हम प्रत्येक भाषा के लिए इष्टतम उपशब्द शब्दावली आकार की जांच करते हैं। हम पाते हैं कि एसपी विभाजन के लिए समग्र रूप से सबसे अच्छा विकल्प है, और यह कि बड़े शब्दकोश आकार उच्च अनुवाद गुणवत्ता की ओर ले जाते हैं।', 'ru': 'Дравидийские языки, такие как каннада и тамильский, как известно, трудно перевести с помощью современных нейронных моделей. Это обусловлено тем фактом, что эти языки с морфологической точки зрения очень богаты, а также не обеспечены достаточными ресурсами. В этой статье мы сосредоточимся на сегментации подслова и оцениваем сокращение лингвистически мотивированной лексики (LMVR) по сравнению с более часто используемым SentencePiece (SP) для задачи перевода с английского языка на четыре различных дравидийских языка. Кроме того, мы исследуем оптимальный размер подслова словаря для каждого языка. Мы находим, что SP является в целом лучшим выбором для сегментации, и что большие размеры словаря приводят к более высокому качеству перевода.', 'zh': '达罗毗荼语,如卡纳达语、泰米尔语,名难先进神经译。 此一实也,言形甚富,而资用匮乏。 于本文中,专注于子词分割,并据更常用句(SP)评语动机词汇减(LMVR),以成从英语翻译四达罗毗荼语之任。 又治言语最佳子词汇大小。 见 SP 为分段之最,而大者字典大小可致重译。', 'ga': 'Tá sé thar a bheith deacair teangacha Dravidian, mar shampla Cannadais agus Tamailis, a aistriú de réir samhlacha néarúla den scoth. Eascraíonn sé seo ón bhfíric go bhfuil na teangacha seo an-saibhir ó thaobh moirfeolaíochta de chomh maith le hacmhainní íseal. Sa pháipéar seo, dírímid ar dheighilt fofhocail agus déanaimid measúnú ar Laghdú Stór Focal Spreagtha Teangeolaíoch (LMVR) i gcomparáid leis an bPíosa SentencePiece (SP) is coitianta a úsáidtear chun aistriú ón mBéarla go ceithre theanga Dravidian éagsúla. Ina theannta sin déanaimid imscrúdú ar an méid stór focal is fearr le haghaidh gach teanga. Feictear dúinn gurb é SP an rogha iomlán is fearr maidir le deighilt, agus go mbíonn cáilíocht aistriúcháin níos airde mar thoradh ar fhoclóirí níos mó.', 'hu': 'A dravidi nyelveket, mint például a kannadát és a tamilt, hírhedten nehéz lefordítani a legkorszerűbb neurális modellekkel. Ez abból a tényből ered, hogy ezek a nyelvek morfológiailag nagyon gazdagok és alacsony erőforrásokkal rendelkeznek. Ebben a tanulmányban az alszószegmentációra összpontosítunk, és értékeljük a Nyelvi Motivációs Szókincscsökkentést (LMVR) a gyakrabban használt SentencePiece (SP) szemben az angol nyelvről négy különböző dravidi nyelvre történő fordításra. Ezenkívül megvizsgáljuk az egyes nyelvek optimális alszókincsméretét. Úgy találjuk, hogy az SP a legjobb választás a szegmentáláshoz, és hogy a nagyobb szótárméretek magasabb fordítási minőséget eredményeznek.', 'el': 'Οι δραβιδικές γλώσσες, όπως η Καννάντα και τα Ταμίλ, είναι διαβόητοι ότι είναι δύσκολο να μεταφραστούν με σύγχρονα νευρωνικά μοντέλα. Αυτό οφείλεται στο γεγονός ότι αυτές οι γλώσσες είναι μορφολογικά πολύ πλούσιες καθώς και χαμηλής περιεκτικότητας σε πόρους. Στην παρούσα εργασία, εστιάζουμε στην κατάτμηση των υπολέξεων και αξιολογούμε τη Γλωσσολογικά Κινητοποιημένη Μείωση Λεξιλαριού (σε σύγκριση με το πιο συνηθισμένο κείμενο για το έργο της μετάφρασης από τα Αγγλικά σε τέσσερις διαφορετικές δραβιδικές γλώσσες. Επιπλέον ερευνούμε το βέλτιστο μέγεθος λεξιλογίου υπολέξεων για κάθε γλώσσα. Διαπιστώνουμε ότι είναι η συνολική καλύτερη επιλογή για τμηματοποίηση, και ότι τα μεγαλύτερα μεγέθη λεξικών οδηγούν σε υψηλότερη ποιότητα μετάφρασης.', 'ka': 'ეპავიდიური ენები, როგორც კანნადია და ტამილია, უცნობიერად რთულია გარგუნოთ სახელის ნეიროლური მოდელებით. ეს ფექტიდან იქნება, რომ ეს ენები მოპოროლოგიურად ძალიან ბეჭდვით, როგორც ცოტა რესუპორტირებულია. ამ დომენტში, ჩვენ სუბსიტური სექმენტიკურად მოტოვირებული სიტყვებულის შემცირება (LMVR) დავუყენებთ სუბსიტურად გამოიყენებული SentencePiece (SP) სიტყვებულია ანგლისდან 4 განსხვავებული დირავიდიური ენაში გადატანის დამატებით ჩვენ ყველა ენისთვის სპექტალური სიტყვებლის ზომას შევხედავთ. ჩვენ ვიფიქრობთ, რომ SP არის ყველაზე საუკეთესო არჩევა სექმენტაციისთვის, და ეს დიდი სიტყვანის ზომა უფრო მეტი სიტყვანის გამოყენება.', 'it': "Le lingue dravidiane, come il kannada e il tamil, sono notoriamente difficili da tradurre con modelli neurali all'avanguardia. Ciò deriva dal fatto che queste lingue sono morfologicamente molto ricche e hanno scarse risorse. In questo articolo, ci concentriamo sulla segmentazione delle sottoparole e valutiamo la riduzione linguistica motivata del vocabolario (LMVR) rispetto al SentencePiece (SP) più comunemente usato per il compito di tradurre dall'inglese in quattro diverse lingue dravidiane. Inoltre esaminiamo la dimensione ottimale del vocabolario delle sottoparole per ogni lingua. Troviamo che SP sia la scelta migliore per la segmentazione e che le dimensioni dei dizionari più grandi portano a una migliore qualità della traduzione.", 'kk': 'Каннада және Тамил секілді дравидеяның тілдері невралдық моделдеріне аудару үшін білмейді. Бұл тілдер морфологиялық түрде бағатты және бағатты ресурстар. Бұл қағазда, біз субсөздерді сегментациялауға көздейміз және Linguistically Motivated Vocabulary Reduction (LMVR) сөздерді төрт әртүрлі дравид тілдеріне аудару тапсырмасына қарсы қолданылатын SentencePiece (SP) сөздеріне көздейміз. Қосымша, біз әрбір тіл үшін оның ішкі сөз сөздерінің оңтималы өлшемін зерттейміз. Біз SP - сегментацияның ең жақсы таңдауы және үлкен сөздік өлшемі үлкен аудармалардың сапасына көмектеседі.', 'lt': 'Dravidinės kalbos, pavyzdžiui, Kanada ir Tamil as, yra labai sunku išversti pagal naujausius nervų modelius. Tai atsiranda dėl to, kad šios kalbos yra morfologiškai labai turtingos ir mažai išteklių. Šiame dokumente daugiausia dėmesio skiriame subžodžių segmentacijai ir vertiname lingvistiškai motyvuotą žodyno mažinimą (LMVR) palyginti su dažniau naudojamu SentencePiece (SP) vertimui iš anglų į keturias skirtingas dravidines kalbas. Additionally we investigate the optimal subword vocabulary size for each language.  Mes manome, kad SP yra bendras geriausias pasirinkimas segmentuoti ir kad didesnių žodynų dydžių vertimo kokybė yra didesnė.', 'mk': 'Дравидските јазици, како што се Канада и Тамил, се познато тешки да се преведат со најсовремени нервни модели. Ова потекнува од фактот дека овие јазици се морфолошки многу богати, како и дека имаат ниски ресурси. Во овој весник, се фокусираме на сегментацијата на подзборовите и ја проценуваме лингвистички мотивираната речница намалување (LMVR) против почесто употребената SentencePiece (SP) за задачата на преведување од англиски на четири различни дравидски јазици. Additionally we investigate the optimal subword vocabulary size for each language.  Сметаме дека СП е целокупниот најдобар избор за сегментација и дека поголемите големини на речникот водат до повисок квалитет на превод.', 'ms': 'Bahasa Dravidia, seperti Kannada dan Tamil, sangat sukar diterjemahkan oleh model saraf yang terbaik. Ini berasal dari fakta bahasa-bahasa ini secara morfologik sangat kaya serta sedikit sumber. Dalam kertas ini, kami fokus pada segmen subkata dan menilai Penurangan Kata Bermotif Bahasa (LMVR) terhadap SentencePiece (SP) yang digunakan secara umum untuk tugas untuk menerjemahkan dari Bahasa Inggeris ke empat bahasa Dravidian yang berbeza. Selain itu, kami menyelidiki saiz vokbulari subkata optimal untuk setiap bahasa. Kami mendapati bahawa SP adalah pilihan terbaik keseluruhan untuk segmen, dan bahawa saiz kamus yang lebih besar membawa kepada kualiti terjemahan yang lebih tinggi.', 'mt': 'Il-lingwi dravidjani, bħall-Kannada u t-Tamil, huma magħrufa bħala diffiċli biex jiġu tradotti minn mudelli newrali l-aktar avvanzati. Dan jirriżulta mill-fatt li dawn il-lingwi huma morfoloġikament rikki ħafna kif ukoll b’riżorsi baxxi. F’dan id-dokument, niffokaw fuq is-segmentazzjoni tas-subkelma u nivvalutaw it-Tnaqqis Vokabulari Motivat Lingwistikament (LMVR) kontra s-SentencePiece (SP) użat b’mod aktar komuni għall-kompitu tat-traduzzjoni mill-Ingliż f’erba’ lingwi Dravidjani differenti. Barra minn hekk, ninvestigaw id-daqs vokabulari tas-subkelma ottimali għal kull lingwa. Aħna nsibu li SP hija l-aħjar għażla ġenerali għas-segmentazzjoni, u li daqsijiet akbar tad-dikjaratorji jwasslu għal kwalità ogħla tat-traduzzjoni.', 'ml': 'ദ്രാവിഡിയന്\u200d ഭാഷകള്\u200d, കാന്നാദ, താമില്\u200d പോലെയുള്ള ഭാഷകള്\u200d, സ്റ്റേറ്റ്- ഓ- ആര്\u200dട്ട് ന്യൂറല്\u200d മോഡലുകള്\u200d ഉപയോഗിക്കാന്\u200d പ് ഈ ഭാഷകള്\u200d മോര്\u200dഫോളജിക്കല്\u200d വളരെ സമ്പന്നരാണെന്നും കുറഞ്ഞ വിഭവങ്ങളില്\u200d നിന്നും സംഭവിക്കുന്നു. ഈ പത്രത്തില്\u200d ഞങ്ങള്\u200d ഇംഗ്ലീഷില്\u200d നിന്നും വ്യത്യസ്ത ഭാഷകളിലേക്ക് നാലു ഭാഷകളിലേക്ക് പരിഭാഷപ്പെടുത്തുന്നതിനുവേണ്ടി ശ്രദ്ധിക്കുകയും ലിങ്കിസ്റ്റിക്കല്\u200d മോട്ട കൂടുതല്\u200d ഓരോ ഭാഷയ്ക്കുമുള്ള ഉപവാക്ക് പദവിയുടെ വലിപ്പം നാം അന്വേഷിക്കുന്നു. എസ്പി പിന്നെ വേര്\u200dതിരിക്കുന്നതില്\u200d ഏറ്റവും മികച്ച തെരഞ്ഞെടുക്കുന്നതാണെന്ന് നമുക്ക് തോന്നുന്നു. അതിലെ വലിയ', 'no': 'Dravidianske språk, som Kannada og Tamil, er notorisk vanskeleg å oversette med state-of-the-art neural models. Dette stemmer frå faktum at desse språkane er morfologisk veldig rikke, og det er lav ressursert. I denne papiret fokuserer vi på underordsegmentasjon og evaluerer Linguistically Motivated Vocabulary Reduction (LMVR) mot den mest vanlege brukte SentencePiece (SP) for oppgåva om å omsetja frå engelsk til fire ulike dravidianske språk. I tillegg undersøker vi optimalt ordordordordordstorleik for kvar språk. Vi finn at SP er den heile beste valet for segmentasjon, og at store ordbokstorleikar fører til høgare omsetjingskvalitet.', 'pl': 'Języki drawidyjskie, takie jak kannada i tamil, są notorycznie trudne do tłumaczenia za pomocą najnowocześniejszych modeli neuronowych. Wynika to z faktu, że języki te są bardzo bogate morfologicznie, a także niskie zasoby. W artykule skupiamy się na segmentacji podsłów i ocenie lingwistycznie motywowanej redukcji słownictwa (LMVR) w porównaniu z częściej stosowanym SentencePiece (SP) do zadania tłumaczenia z języka angielskiego na cztery różne języki drawidyjskie. Dodatkowo badamy optymalny rozmiar słownictwa podsłów dla każdego języka. Uważamy, że SP jest ogólnie najlepszym wyborem dla segmentacji, a większe rozmiary słowników prowadzą do wyższej jakości tłumaczeń.', 'mn': 'Каннада болон Тамил зэрэг дравидийн хэл, урлагийн мэдрэлийн загвараар орчуулахад маш хэцүү. Энэ нь эдгээр хэлнүүд маш баян, бага хүчин зүйл болох боломжтой. Энэ цаасан дээр бид өөр хэлбэрээс англи хэлээс 4 өөр Дравидийн хэл руу орлуулахын тулд илүү ихэвчлэн хэрэглэгдсэн SentencePiece (SP) болон Linguistically Motivated Vocabulary Reduction (LMVR) дээр англи хэлний хэлбэрээс англи хэлний хэлбэрээс англи хэлний хэ Мөн бид хэл бүрийн эерэг үгийн хэмжээг судалж байна. Бид СП бол загварын хамгийн сайн сонголт ба том үеийн хэмжээ нь илүү өндөр орчуулах чадвартай болж байна.', 'ro': 'Limbile dravidiene, cum ar fi kannada și tamil, sunt notoriu dificil de tradus prin modele neurale de ultimă generație. Acest lucru rezultă din faptul că aceste limbi sunt morfologic foarte bogate și au resurse reduse. În această lucrare, ne concentrăm pe segmentarea subcuvintelor și evaluăm reducerea vocabularului motivat lingvistic (LMVR) împotriva celui mai frecvent utilizat SentencePiece (SP) pentru sarcina de traducere din engleză în patru limbi dravidiene diferite. În plus, investigăm dimensiunea optimă a vocabularului subcuvântului pentru fiecare limbă. Considerăm că SP este cea mai bună alegere pentru segmentare și că dimensiunile mai mari ale dicționarului duc la o calitate mai mare a traducerii.', 'sr': 'Dravidski jezici, kao što su Kannada i Tamil, su poznato teški prevoditi državnim neuralnim modelima. Ovo je od činjenice da su ovi jezici morfološki vrlo bogati kao i niski resursi. U ovom papiru, fokusiramo se na segmentaciju podriječi i procjenjujemo Lingistički motivisano smanjenje rečenika (LMVR) protiv češće upotrebljenog SentencePiece (SP) za zadatak prevodenja sa engleskog na četiri različite Dravidijske jezike. Osim toga, istražujemo optimalnu veličinu podriječja za svaki jezik. Smatramo da je SP najbolji izbor za segmentaciju i da veća veličina rečnika dovede do većeg kvaliteta prevođenja.', 'si': 'ඩ්\u200dරාවිඩියාන් භාෂාවල්, හරියට කැන්නාඩා සහ ටැමිල් වලින්, ස්ථානයේ ක්\u200dරියාත්මක න්\u200dයූරාල් මොඩේල් වලින්  මේක ඇත්තෙන් පිළිගන්නවා මේ භාෂාවල් මාර්ෆෝලෝජික විශේෂයෙන් ගොඩක් ප්\u200dරශ්ණයි, ඒ වගේම අ මේ පැත්තේ, අපි ප්\u200dරභාවිත විශේෂය සම්ප්\u200dරභාවිත විශේෂයෙන් ඉන්ග්\u200dරීසියෙන් වෙනස් භාෂාවක් හතරක් වෙනුවෙන් ප්\u200dරභාවිත විශේෂය කරලා භාෂාවිත තවත් අපි හැම භාෂාව සඳහා විශේෂ භාෂාව ප්\u200dරමාණය පරීක්ෂණය කරනවා. අපිට හොයාගන්න පුළුවන් සී.පී.පී.පී.පි.පී.පි.පී.පි.පි.පි.පි.පි.පි.පි.පි.පි.පි.පි.පි.', 'so': 'Luqadaha Dravidian, tusaale ahaan Kannada iyo Tamil, waa ku adag yihiin in lagu turjumo state-of-the-art models of neural. Tani waxay ka timaadaa in luqadaasu ay si morphological ah u hodan yihiin, sidoo kale in ay tahay in ay haystaan wax yar. Qoraalkan waxaynu ku kalsoonaynaa qeybinta hoose-word, waxaana ku qiimeynayaa kaartaynta afka afka ee luqada la Motivated (LMVR) oo ka gees ah waxa ugu badan ee lagu isticmaalay SentencePiece (SP) sababta lagu turjumo afka Ingiriiska oo afar luqadood oo kala duduwan oo Dravidian ah. Sidoo kale waxaynu baaraynaa qiyaastii ugu habboon qoraalka hoose ee luqad walba. Waxaynu ognahay in SP waa doorasho ugu wada wanaagsan ee la xiriira, tirada ugu waaweyn ee luqadda waxaa ku socda takhasuska turjumidda sare.', 'sv': 'Dravidiska språk, som kannada och tamil, är notoriskt svåra att översätta med state-of-the-art neurala modeller. Detta beror på att dessa språk är morfologiskt mycket rika och har låga resurser. I denna uppsats fokuserar vi på segmentering av underord och utvärderar Linguistically Motivated Vocabulary Reduction (LMVR) mot det vanligare använda SentencePiece (SP) för uppgiften att översätta från engelska till fyra olika dravidiska språk. Dessutom undersöker vi den optimala underordsstorleken för varje språk. Vi anser att SP är det överlag bästa valet för segmentering och att större ordlista storlekar leder till högre översättningskvalitet.', 'ta': 'Dravidian languages, such as Kannada and Tamil, are notoriously difficult to translate by state-of-the-art neural models.  இந்த மொழிகள் மொழிபெரும் வளர்ச்சியாகவும் குறைந்த வளர்ச்சியாகவும் இருக்கின்றன என்பது உண்மையிலிருந்து இத இந்த காக்கியத்தில், நாம் உப வார்த்தை துணை பிரிவில் கவனம் செலுத்தி மாற்றப்பட்ட சொல்லாக்கு குறைவு (LMVR) எதிராக மேலும் பொதுவாக பயன்படுத்தப்பட்ட வாக்கியம் (SP) வி கூடுதலாக ஒவ்வொரு மொழிக்கும் விருப்பமான துணை சொல்லு அளவை நாம் சோதிக்கிறோம். SP என்பது பிரிவின் மொத்த சிறந்த தேர்வு, அது பெரிய அகராதி அளவு அதிக மொழிபெயர்ப்பு தரமாகும்.', 'ur': 'ڈراویڈی زبانیں، جیسے کانڈا اور تامیل، ان کی ترجمہ کے لئے بہت مشکل ہیں. یہ حقیقت کی وجہ سے ہے کہ یہ زبانیں بہت ثروتمند ہیں اور بہت کم سرمایہ ہیں۔ اس کاغذ میں، ہم سوبرویڈ سگرمیٹ پر تمرکز کرتے ہیں اور لینگویسٹی طور پر حرکت کی آواز کاغذ (LMVR) کو چار مختلف ڈراویڈی زبانوں میں ترجمہ کرنے کے لئے استعمال کیا جاتا ہے SentencePiece (SP) کے بارے میں۔ اور ہم نے ہر زبان کے لئے اچھی سوب ورڈ کا سائز تحقیق کیا ہے۔ ہم دیکھتے ہیں کہ سپی سٹم کے لئے سب سے بہترین انتخاب ہے، اور یہ بہترین لفظ اندازے بہترین ترجمہ کیفیت کی پیش کرتی ہیں۔', 'uz': "Name Bu to'g'ri, bu tillar morfologikda juda hozir va kichkina murakkablik bo'ladi. Bu hujjatda biz inglizcha tildan to'rt turli Dravidian tillariga tarjima qilish uchun bir necha so'zlarni qiymatimiz va Lingʻatlik Motivated Vocabularni qiymatimiz. Qoʻshimcha biz har bir tillar uchun optimal subword vositasi oʻlchamini qidirishingiz. We find that SP is the overall best choice for segmentation, and that larger dictionary sizes lead to higher translation quality.", 'vi': 'Ngôn ngữ Drakvidian, như Kannada và Tamil, khó dịch được qua các mô hình thần kinh hiện đại. Điều này bắt nguồn từ sự thật rằng các ngôn ngữ này cũng có phong phú và cũng có nguồn lực ít ỏi. Trong tờ giấy này, chúng tôi tập trung vào phân đoạn chữ phụ và đánh giá khả năng ngôn ngữ dịch chuyển từ ngữ học đến giảm âm (LMVR) so với một đơn đơn thường dùng (SP) cho nhiệm vụ dịch từ tiếng Anh thành bốn ngôn ngữ Dravidian khác nhau. Chúng tôi còn điều tra kích thước ngôn ngữ tối ưu cho mỗi ngôn ngữ. Chúng tôi thấy Vi.P là lựa chọn tốt nhất cho phân đoạn, và các kích cỡ từ điển lớn hơn dẫn đến chất lượng dịch chuyển cao hơn.', 'nl': 'Dravidische talen, zoals Kannada en Tamil, zijn notorisch moeilijk te vertalen met behulp van state-of-the-art neurale modellen. Dit komt voort uit het feit dat deze talen morfologisch zeer rijk zijn en weinig middelen hebben. In dit artikel richten we ons op subwoordsegmentatie en evalueren we Linguïstisch Motivated Vocabulary Reduction (LMVR) ten opzichte van het meest gebruikte SentencePiece (SP) voor de taak van het vertalen van het Engels naar vier verschillende Dravidische talen. Daarnaast onderzoeken we de optimale subwoordwoordenschat voor elke taal. We vinden dat SP over het algemeen de beste keuze is voor segmentatie, en dat grotere woordenboekformaten leiden tot een hogere vertaalkwaliteit.', 'da': 'Dravidiske sprog, såsom kannada og tamil, er notorisk vanskelige at oversætte med state-of-the-art neurale modeller. Dette skyldes det faktum, at disse sprog er morfologisk meget rige såvel som lav ressource. I denne artikel fokuserer vi på segmentering af underord og evaluerer Linguistically Motivated Vocabulary Reduction (LMVR) mod det mere almindeligt anvendte SentencePiece (SP) til opgaven med at oversætte fra engelsk til fire forskellige dravidiske sprog. Derudover undersøger vi den optimale størrelse af underord ordforråd for hvert sprog. Vi finder ud af, at SP er det overordnede bedste valg til segmentering, og at større ordbogsstørrelser fører til højere oversættelseskvalitet.', 'bg': 'Дравидските езици, като Канада и Тамил, са известни трудни за превеждане чрез най-съвременните невронни модели. Това произтича от факта, че тези езици са морфологично много богати, както и са с ниски ресурси. В настоящата статия се фокусираме върху сегментацията на поддумите и оценяваме лингвистично мотивираното редуциране на речника спрямо по-често използваното за задачата превод от английски на четири различни дравидски езика. Освен това изследваме оптималния размер на речника на поддумите за всеки език. Ние намираме, че като цяло е най-добрият избор за сегментация и че по-големите размери на речниците водят до по-високо качество на превода.', 'hr': 'Dravidijski jezici, poput Kannada i Tamila, slavno su teški prevoditi state-of-the-art neural models. To je od činjenice da su ti jezici morfološki vrlo bogati kao i niski resursi. U ovom papiru, usredotočili smo se na segmentaciju podriječi i procjenjivali Lingvistički motivirano smanjenje riječi (LMVR) protiv češće upotrebljenog SentencePiece (SP) za zadatak prevoda iz engleskog na četiri različite Dravidijske jezike. Osim toga, istražujemo optimalnu veličinu podriječja za svaki jezik. Mi smatramo da je SP najbolji izbor za segmentaciju i da veća veličina riječnika dovede do višeg kvaliteta prevoda.', 'de': 'Dravidische Sprachen, wie Kannada und Tamil, sind notorisch schwer mit modernsten neuronalen Modellen zu übersetzen. Dies liegt daran, dass diese Sprachen sowohl morphologisch sehr reich als auch ressourcenarm sind. In dieser Arbeit konzentrieren wir uns auf die Segmentierung von Unterwörtern und evaluieren Linguistisch Motivierte Vocabulary Reduction (LMVR) gegenüber dem am häufigsten verwendeten SentencePiece (SP) für die Aufgabe der Übersetzung aus dem Englischen in vier verschiedene dravidische Sprachen. Zusätzlich untersuchen wir die optimale Wortschatzgröße für jede Sprache. Wir finden, dass SP insgesamt die beste Wahl für Segmentierung ist und dass größere Wörterbuchgrößen zu einer höheren Übersetzungsqualität führen.', 'ko': '카나다어나 테밀어 같은 델라위어는 가장 선진적인 신경모형으로 번역하기 어렵다는 것은 잘 알려져 있다.이런 언어들이 형태적으로 풍부하고 자원이 부족하다는 사실에서 비롯된 것이다.이 논문에서 우리는 하위 단어의 구분에 주목하고 언어 동기 어휘 축소(LMVR)와 더 자주 사용하는 문장 블록(SP)이 영어를 네 가지 다른 델라위어로 번역하는 임무에서의 역할을 평가한다.그 밖에 우리는 각 언어의 가장 좋은 자사 어휘량도 연구했다.우리는 SP가 절분에 가장 좋은 선택이고, 비교적 큰 사전 크기가 비교적 높은 번역 품질을 초래할 수 있다는 것을 발견했다.', 'id': 'Bahasa Dravidia, seperti Kannada dan Tamil, sangat sulit diterjemahkan oleh model saraf terbaik. Ini berasal dari fakta bahasa-bahasa ini secara morfologis sangat kaya serta sedikit sumber daya. Dalam kertas ini, kami fokus pada segmen subword dan mengevaluasi Reduction Vocabulary Motivated Linguistically (LMVR) melawan SentencePiece (SP) yang lebih biasa digunakan untuk tugas untuk menerjemahkan dari bahasa Inggris ke empat bahasa Dravidia yang berbeda. Additionally we investigate the optimal subword vocabulary size for each language.  Kami menemukan bahwa SP adalah keseluruhan pilihan terbaik untuk segmentasi, dan bahwa ukuran kamus yang lebih besar menyebabkan kualitas terjemahan yang lebih tinggi.', 'fa': 'زبان\u200cهای درویدیی، مثل کاندا و تامیل، با مدل\u200cهای عصبی ایالت هنر، ترجمه کردن خیلی سخت است. این از حقیقت است که این زبانها به صورت مورفولوژیکی بسیار ثروتمند و به صورت کمترین منابع هستند. در این کاغذ، ما روی بخش\u200cهای زیر کلمه تمرکز می\u200cکنیم و ارزیابی کاهش کلمه\u200cهای زبان\u200cگیری (LMVR) را در برابر کارهای ترجمه\u200cتر استفاده می\u200cکنیم که از انگلیسی به چهار زبان\u200cهای درویدی متفاوت ترجمه می\u200cکند. اضافه\u200cای از این، اندازه\u200cی کلمه\u200cهای زیر کلمه\u200cای optimal برای هر زبان تحقیق می\u200cکنیم. ما پیدا می\u200cکنیم که SP بهترین انتخاب برای جدایی است، و اندازه\u200cهای بزرگترین دوره\u200cدهنده\u200cها به کیفیت ترجمه بالاتر می\u200cرسند.', 'sq': "Gjuhat dravidiane, të tilla si Kannada dhe Tamil, janë shumë të vështira për t'u përkthyer nga modelet neuronale më të larta. Kjo vjen nga fakti se këto gjuhë janë morfologjikisht shumë të pasura si dhe janë me burime të ulëta. Në këtë letër, ne përqëndrohemi në segmentimin e nënfjalëve dhe vlerësojmë Reduktimin e Fjalëkalimit të Motivuar Lingjistikisht (LMVR) kundër SentencePiece (SP) më të përdorur për detyrën e përkthimit nga anglisht në katër gjuhë të ndryshme dravidiane. Përveç kësaj ne hetojmë madhësinë optimale të fjalëkalimit për çdo gjuhë. Ne zbulojmë se SP është zgjedhja më e mirë e përgjithshme për segmentimin dhe se madhësitë më të mëdha të fjalorit çojnë në cilësi më të lartë përkthimi.", 'sw': 'Lugha za Ki-Dravidi, kama vile Kannada na Tamil, ni vigumu kutafsiri kwa kiasi kikubwa na mifano ya kisasa. Hii inatokana na ukweli kuwa lugha hizi zina utajiri sana na pia kuwa rasilimali duni. Katika gazeti hili, tunalenga kwenye unyanyasaji wa maneno ya chini na kutathmini Upunguzo wa lugha iliyotolewa lugha ya Kiingereza (LMVR) dhidi ya matumizi ya mara nyingi zaidi ya SentencePiece (SP) kwa kazi ya kutafsiri kutoka Kiingereza hadi lugha nne tofauti za KiDravidi. Kwa kuongezea tunachunguza ukubwa wa lugha za chini ya maneno kwa kila lugha. Tunapata kwamba SP ni chaguo bora zaidi kwa ajili ya kujitenga, na ukubwa mkubwa wa lugha unapelekea kiwango kikubwa cha tafsiri.', 'tr': 'Kannada we Tamil ýaly öwrülen diller, sungat nural modellerinden terjime etmek kyn däldir. Bu diller morfolojik ýaly baý we ýokary derejede bolan ýagdaýdyr. Bu kagyzda biz sözleriň segmentasyna üns berip Lingülisçe göçürilýän sözleriň azaltmasyny (LMVR) Iňlislerden dört dürli Dravidiýa dillere terjime etmek üçin ullanýarys. Biz her dil üçin iň üst sözleriň ululykyny barlaýarys. SP segmentasiýa üçin iň gowy saýlaw bolandygyny we uly sözlük ölçüsi ýokary terjime etmegiň keyfiýasyna ýokar.', 'am': 'የድራቪዲያን ቋንቋዎች፣ እንደካናዳ እና ታሚሊ፣ በሀገር-of-the-art neural models ለመtranslate አስቸጋሪ ናቸው፡፡ ይህ ቋንቋዎች በሞፎሎጂ እጅግ ባለጠጎች እና ዝቅተኛ ሀብት እንዲሆኑ ነው፡፡ በዚህ ካላት፣ አዲስ ቃላት ማውጣት እና የቋንቋ ቋንቋ አቀማመጥ (LMVR) በተለየው ስንስቴንPiece (SP) ላይ ለመትረጉም ወደ አራት ልዩ ድራቪዲያን ቋንቋዎች ለማድረግ እናሳውቃለን፡፡ በተጨማሪም የቋንቋ ቃላት የመስመር ቃላት መጠን እናመርመራለን፡፡ SP ለማንበብ የሚሻለው ምርጫ እንደሆነ እናገኘዋለን፤ ይህም የመዝገበ መዝገብ መጠን ከፍተኛ ትርጉም ጥሩ ነው፡፡', 'af': 'Dravidiese tale, soos Kannada en Tamil, is nooit moeilik om deur state-of-the-art neurale modele te vertaal. Hierdie stem van die feit dat hierdie taal morfologies baie ryk is, en ook die lae-hulpbron. In hierdie papier, ons fokus op subwoord segmentasie en evalueer Linguistically Motivated Voorwoord Reduksie (LMVR) teen die meer gewoonlik gebruikte SentencePiece (SP) vir die taak van vertaling van Engels na vier verskillende Dravidian tale. Ons ondersoek die optimale subwoord woordeboekgrootte vir elke taal. Ons vind dat SP is die hele beste keuse vir segmentasie, en dat groter woordeboekgrootte lei na hoër vertaling kwaliteit.', 'bn': 'দ্রাভিডিয়ার ভাষা, যেমন কান্নাদা এবং তামিল, রাষ্ট্র-of-the-art নিউরেল মডেল দ্বারা অনুবাদ করা কঠিন। এই বাস্তবতা থেকে যে এই ভাষাগুলো নৈতিক ভাষায় অনেক ধনী এবং কম সম্পদের সাথে সমৃদ্ধ। এই পত্রিকায় আমরা সাবওয়ার্ডের বিভিন্ন ভিন্ন ভিন্ন ভাষায় অনুবাদ করার জন্য মনোযোগ প্রদান করি এবং ভিন্ন ভিন্ন দ্রাভিডিয়ান ভাষায় লিঙ্গিস্টিক্ত ভাষায় মোটাভিটেট ভাষা এছাড়াও আমরা প্রত্যেক ভাষার জন্য অপটিমাল সাবওয়ার্ডের শব্দভাণ্ডারের আকার তদন্ত করি। We find that SP is the overall best choice for segmentation, and that larger dictionary sizes lead to higher translation quality.', 'hy': 'Դրավիդիացի լեզուները, ինչպիսիք են Կանադան և Թամիլը, հայտնի է, որ դժվար են թարգմանել ամենաբարձր նյարդային մոդելների միջոցով: Սա կատարվում է այն փաստից, որ այս լեզուները մորֆոլոգիապես շատ հարուստ են, ինչպես նաև ցածր ռեսուրսներ ունեն: Այս թղթի մեջ մենք կենտրոնանում ենք ենթաբառերի սեգմետրացիայի վրա և գնահատում ենք Լեզվաբանական մոտիվացված բառարանի կրճատման (LMVR) համեմատած ավելի տարածված SenSenSencePiceն (SP) անգլերենից թարգմանելու համար չորս տարբեր դրավիդիական լեզուների: Ավելին, մենք ուսումնասիրում ենք յուրաքանչյուր լեզու բառարանի օպտիմալ չափը: Մենք հայտնաբերում ենք, որ SP-ը սեգմետրացիայի ընդհանուր լավագույն ընտրությունն է, և որ բառարանի մեծ չափերը հանգեցնում են բարձր թարգմանման որակի:', 'az': 'Kannada və Tamil kimi Dravid dilləri, sanat nöral modelləri ilə tercümə etmək çox çətin deyildir. Bu, bu dillərin morfolojik olaraq çox zengin və düşük qüvvətli olduğu təqdirdən gəlir. Bu kağızda, sözlərin segmentasyonuna odaklanır və Linguistically Motivated Vocabulary Reduction (LMVR) dillərini İngilizdən dörd müxtəlif Dravidian dillərə çevirirlər. Biz hər dil üçün optimal sözlər sözlərinin böyüklüyünü araşdırırıq. SP segmentasyon üçün ən yaxşı seçim olduğunu görürük, böyük sözlük böyüklüyü daha yüksək tercümə keyfiyyətinə sürükləyir.', 'ca': "Les llengües dravídiques, com el Kannada i el Tamil, són notoriament difícils de traduir per models neurals més avançats. Això prové del fet que aquestes llengües són morfològicament molt rics i que tenen pocs recursos. En aquest paper, ens centrem en la segmentació de subparaules i evaluem la Reducció de Vocabulari Motivada Lingüísticament (LMVR) en comparació amb la SentencePiece (SP) més comunament utilitzada per traduir d'anglès a quatre llengües dravídiques. A més, investigam la mida de vocabulari de subparaules optima per cada llenguatge. Trobem que SP és la millor opció global per a la segmentació, i que grans dimensions de diccionari porten a una qualitat de traducció més alta.", 'bs': 'Dravidski jezici, poput Kannada i Tamila, su poznato teški prevoditi državnim neuralnim modelima. To je od činjenice da su ovi jezici morfološki vrlo bogati kao i niski resursi. U ovom papiru, fokusiramo se na segmentaciju podriječi i procjenjujemo Lingistički motivirano smanjenje riječi (LMVR) protiv češće upotrebljenog SentencePiece (SP) za zadatak prevode iz engleskog na četiri različite Dravidijske jezike. Osim toga, istražujemo optimalnu veličinu podriječja za svaki jezik. Mi smatramo da je SP najbolji izbor za segmentaciju, i da veća veličina rečnika dovede do većeg kvaliteta prevođenja.', 'et': 'Dravidia keeli, nagu kannada ja tamil, on tuntud raske tõlkida kaasaegsete närvimudelite abil. See tuleneb asjaolust, et need keeled on morfoloogiliselt väga rikkad ja vähe ressursse. Käesolevas töös keskendume alamsõna segmenteerimisele ja hindame keeleliselt motiveeritud sõnavara vähendamist (LMVR) võrreldes sagedamini kasutatava SentencePiece (SP) ülesandega tõlkida inglise keelest nelja erineva draviidi keelde. Lisaks uurime iga keele jaoks optimaalset alamsõna sõnavara suurust. Me leiame, et SP on üldiselt parim valik segmenteerimiseks ja et suuremad sõnastikud toovad kaasa suurema tõlkekvaliteedi.', 'cs': 'Dravidské jazyky, jako je Kannáda a Tamilština, jsou notoricky obtížné překládat pomocí nejmodernějších neuronových modelů. To vyplývá ze skutečnosti, že tyto jazyky jsou morfologicky velmi bohaté a mají nízké zdroje. V tomto článku se zaměřujeme na segmentaci podslov a vyhodnocujeme lingvisticky motivovanou redukci slovní zásoby (LMVR) proti běžněji používanému SentencePiece (SP) pro úkol překladu z angličtiny do čtyř různých dravidských jazyků. Dále zkoumáme optimální velikost slovní zásoby pro každý jazyk. Zjišťujeme, že SP je celkově nejlepší volbou pro segmentaci a že větší velikosti slovníku vedou k vyšší kvalitě překladu.', 'fi': 'Dravidialaiset kielet, kuten Kannada ja Tamil, ovat tunnetusti vaikeasti käännettäviä viimeisimpien neuromallien avulla. Tämä johtuu siitä, että nämä kielet ovat morfologisesti hyvin rikkaita ja niillä on vähän resursseja. Tässä artikkelissa keskitymme alasanojen segmentointiin ja arvioimme Linguistically Motivated Vocabulary Reduction (LMVR) verrattuna yleisemmin käytettyyn SentencePieceen (SP) kääntämiseen englannista neljään dravidian kieleen. Lisäksi tutkimme kunkin kielen sanaston optimaalista kokoa. Mielestämme SP on yleisesti ottaen paras valinta segmentointiin, ja suuremmat sanakirjat johtavat parempaan käännöslaaduun.', 'sk': 'Dravidijske jezike, kot sta Kannada in Tamil, je znano težko prevesti z najsodobnejšimi nevronskimi modeli. To izhaja iz dejstva, da so ti jeziki morfološko zelo bogati in imajo majhne vire. V prispevku se osredotočamo na segmentacijo podbesed in ocenjujemo jezikovno motivirano besedišče redukcije (LMVR) glede na bolj pogosto uporabljeno SentencePiece (SP) za nalogo prevajanja iz angleščine v štiri različne dravidske jezike. Poleg tega raziskujemo optimalno velikost besednega besedišča za vsak jezik. Ugotavljamo, da je SP na splošno najboljša izbira za segmentacijo in da večje velikosti slovarjev vodijo k višji kakovosti prevodov.', 'he': 'שפות דראבידיות, כמו קננדה וטמיל, קשות לתרגם על ידי מודלים עצביים חדשים. זה מגיע מהעובדה שהשפות האלה עשירות מאוד באופן מורפולוגי, כמו גם בהן משאבים נמוכים. בעיתון הזה, אנו מתמקדים בסגמנטציה של מילים מתחת ולעריך חיסוך מילים מוטיבציה לינגולית (LMVR) נגד SentencePiece (SP) שימוש בדרך כלל למשימה של התרגום מאנגלית לארבעה שפות דרבידיות שונות. בנוסף, אנו חוקרים את גודל המילים המאופטימי של כל שפה. אנו מוצאים שSP היא הבחירה הטובה ביותר לגמרי למחלקה, ושגודלים גדולים יותר של מילון מובילים לאיכות התרגום גבוהה יותר.', 'jv': 'Slamet tresnane, kaya Kanadan lan Tamil, kuwi susah-susahe kanggo terjamah karo state-of-the-arts model Neral Ngomongke iki lak kelas kuwi banjur-barêng langkung wih apik lan tambah kuwi duluran. Nang pepulan iki, kita supokot karo segmentation apat word lan jerakno Linguistique politenessoffpolite"), and when there is a change ("assertivepoliteness Awak dhéwé luwih-luwih saben nggawe ngubah-saben gak dhéwé gerakan kanggo segmentation, lan ukuran diketeksi sing luwih apik kudu urip kuwi wis luwih apik.', 'ha': 'KCharselect unicode block name This stems from the fact that these languages are morphologically very rich as well as being low-resourced.  Daga wannan takardar, munã fokus a kan segment of subword and evaluate Reducion in Linguistically Motted Voice (LMV R) against the most commonly used in Pergala-Piece (SP) for the job of translation from Ingiriya zuwa harshen huɗu daban-dabam na Devdian. Ina ƙara, Munã jãyayya girmar abun maganar cikin kõwace harshe. Muna gane SP shi ne mafi kyaun zaɓen ci-sura, kuma yana da girma cikin dictionari yana ƙara zuwa sifar fassarar tarjima.', 'bo': 'སྒྲ་བརྙན་གྱི་སྐད་ཡིག་དང་། དཔེར་ན། ཀུན་ན་དང་ཏ་མིལ་དང་། དེ་ནི་སྐད་རིགས་འདི་ཚོ་ནི་ཆེས་ཉུང་བའི་དབྱིབས་མཐུན་དང་མཐུན་རྐྱེན་ཚད་ཁག་པོ་ཡིན་པ་ལས་མཐུན་ཡོད། འོག་གི་ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོས་མཐུན་གྲངས་སུ་འཇིག་སྣོད་ཀྱི་ནང་དུ་འདྲ་བ་དང་ སྐད་རིགས་ལ་སྤྲོད་ཀྱི་ཡིག ང་ཚོས་སྐད་རེ་རེའི་ནང་དུ་ཆེ་ཤོས་ཀྱི་ལག་ཆ་ཚིག་གི་ཆེ་ཆུང་དུ་བཙལ་ཞིབ་བྱེད། ང་ཚོས་SP ནི་ཆ་ཤས་གཅིག་གི་གདམ་ཁ་ཡོད་ཚད་ལྡན་མེད་པ་དེ་ཡིན་ཚུལ་བརྗོད་ཀྱི་ཆེ་ཆུང་ཆེ་ཤོས་ཀྱི་འགྱུར་བརྗ'}
{'en': 'Itihasa : A  large-scale corpus  for Sanskrit to English translation S anskrit to  E nglish translation', 'ar': 'Itihasa: مجموعة واسعة النطاق للترجمة من السنسكريتية إلى الإنجليزية', 'pt': 'Itihasa: Um corpus em larga escala para tradução do sânscrito para o inglês', 'es': 'Itihasa: un corpus a gran escala para la traducción del sánscrito al inglés', 'fr': "Itihasa\xa0: un corpus à grande échelle pour la traduction du sanskrit vers l'anglais", 'zh': 'Itihasa:梵文至英语翻译大语料库', 'ja': 'Itihasa:サンスクリット語から英語への翻訳のための大規模なコーパス', 'ru': 'Итихаса: Масштабный корпус для перевода с санскрита на английский', 'hi': 'Itihasa: संस्कृत से अंग्रेजी अनुवाद के लिए एक बड़े पैमाने पर कॉर्पस', 'ga': 'Itihasa: Corpas mórscála don aistriúchán Sanscrait go Béarla', 'ka': 'Itihasa: დიდი მაგალითი კორპუსი სანგლისკური განგორმაციისთვის', 'hu': 'Itihasa: Egy nagyszabású korpusz a szanszkrit-angol fordításhoz', 'el': 'Ιτιχάσα: Ένα μεγάλης κλίμακας σώμα για σανσκριτική προς αγγλική μετάφραση', 'it': "Itihasa: Un corpus su larga scala per la traduzione dal sanscrito all'inglese", 'kk': 'Итихаса: Санскриттің ағылшын аудармасына үлкен масштабтағы корпус', 'mk': 'Итихаса: Голем корпус за санскритски на англиски превод', 'lt': 'Itihasa: A large-scale corpus for Sanskrit to English translation', 'mt': 'Itihasa: Korpus fuq skala kbira għas-Sanskrit għat-traduzzjoni bl-Ingliż', 'ml': 'ഇതിഹാസ: സാന്\u200dസ്ക്രീറ്റിന് വേണ്ടി ഒരു വലിയ കോര്\u200dപ്പുസ് ഇംഗ്ലീഷ് പരിഭാഷ', 'mn': 'Итихаса: Санскритын англи хэлний орчуулалтын том хэмжээний корпус', 'ms': 'Itihasa: Korpus skala besar untuk terjemahan bahasa Inggeris dalam bahasa Sanskrit', 'ro': 'Itihasa: Un corpus pe scară largă pentru traducerea sanscrită în engleză', 'no': 'Itihasa: Eit stor korpus for Sanskrit til engelsk omsetjing', 'pl': 'Itihasa: Korpus na dużą skalę dla sanskrytu na angielski', 'si': 'ඉතිහාසා', 'sr': 'Itihasa: Veliki korpus za Sanskrit na engleski prevod', 'so': 'Itihasa', 'sv': 'Itihasa: En storskalig korpus för översättning från sanskrit till engelska', 'ta': 'Itihasa: சான்ஸ்கிரிட்டிற்கான ஒரு பெரிய அளவு கோர்புஸ் ஆங்கிலத்திற்கு மொழிபெயர்ப்பு', 'ur': 'Itihasa: Sanskrit کے لئے انگلیسی ترجمہ کے لئے ایک بڑی مقدار کورپوس', 'uz': 'Itihasa: Sanskrit uchun katta korpus inglizcha tarjima', 'vi': 'Itihasa: một tập đoàn lớn của tiếng Phạn với tiếng Anh', 'bg': 'Итихаса: мащабен корпус за превод от санскрит на английски', 'nl': 'Itihasa: Een grootschalig corpus voor vertaling van het Sanskriet naar het Engels', 'de': 'Itihasa: Ein umfangreiches Korpus für die Übersetzung von Sanskrit ins Englische', 'id': 'Itihasa: A large-scale corpus for Sanskrit to English translation', 'ko': 'Itihasa: 대규모 범어 영역 어료 라이브러리', 'da': 'Itihasa: Et stort korpus for sanskrit til engelsk oversættelse', 'fa': 'Itihasa: یک کورپوس بزرگ برای سانسکریت به ترجمه انگلیسی', 'hr': 'Itihasa: Veliki korpus za Sanskrit na engleski prevod', 'tr': 'Itasa: Sanskrit üçin Iňlisçe terjime etmek üçin uly bir korpus', 'af': "Itihasa: ' n groot skaal korpus vir Sanskrit na Engelske vertaling", 'sw': 'Itihasa: Kikosi kikubwa cha Sanskrit kwa ajili ya kutafsiri Kiingereza', 'am': 'ኢቲካ: ለሳንስክሮት ወደ ኢንጂልኛ ትርጓሜ የተደረገ ትልቅ ካርፓስ', 'hy': 'Իթիհասա՝ Սանսկրիտի մեծ կորպուս անգլերեն թարգմանելու համար', 'az': 'Itihasa: Sanskrit üçün İngilizə çevirilən böyük ölçülü korpus', 'bn': 'ইথিহাসা: সান্স্ক্রিটের জন্য বিশাল কোর্পাস ইংরেজি অনুবাদ', 'bs': 'Itihasa: Veliki korpus za Sanskrit na engleski prevod', 'sq': 'Itihasa: Një korpus në shkallë të madhe për përkthimin e sanskritit në anglisht', 'ca': 'Itihasa: Un corpus a gran escala per a traduir sànscrit a anglès', 'et': 'Itihasa: ulatuslik korpus sanskriti inglise keelde tõlkimiseks', 'fi': 'Itihasa: Laajamittainen korpus sanskritistä englantiin käännökselle', 'cs': 'Itihasa: A large scale corpus for sanskrit to English translation', 'jv': 'Itihasa:Cebutun langkung wigat kanggo Sankrit kanggo tarjamahan Inggris', 'he': 'איטיהאסה: קורפוס גדול לסנסקריט לתרגום אנגלי', 'sk': 'Itihasa: obsežen korpus za prevod sanskrta v angleščino', 'bo': 'སེན་སིཊི་ལ་དབྱིན་ཡིག་དང་སྐད་ཆ་ཆེན་པོ་ཞིག་གིས་', 'ha': 'KCharselect unicode block name'}
{'en': 'This work introduces  Itihasa , a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The  shlokas  are extracted from two  Indian epics  viz., The  Ramayana  and The  Mahabharata . We first describe the motivation behind the curation of such a  dataset  and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this  corpus  and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.', 'ar': 'يقدم هذا العمل Itihasa ، وهي مجموعة بيانات ترجمة واسعة النطاق تحتوي على 93000 زوج من السنسكريتية shlokas وترجماتهم الإنجليزية. يتم استخراج شلوكا من ملحمتين هنديتين ، رامايانا وماهابهاراتا. نصف أولاً الدافع وراء معالجة مجموعة البيانات هذه ومتابعة التحليل التجريبي لإبراز الفروق الدقيقة. ثم نقيس أداء نماذج الترجمة القياسية في هذه المجموعة ونبين أنه حتى أبنية المحولات الحديثة تؤدي أداءً ضعيفًا ، مما يؤكد على مدى تعقيد مجموعة البيانات.', 'pt': 'Este trabalho apresenta o Itihasa, um conjunto de dados de tradução em grande escala contendo 93.000 pares de shlokas sânscritos e suas traduções em inglês. Os shlokas são extraídos de dois épicos indianos, o Ramayana e o Mahabharata. Primeiro descrevemos a motivação por trás da curadoria de tal conjunto de dados e seguimos com a análise empírica para trazer suas nuances. Em seguida, comparamos o desempenho de modelos de tradução padrão neste corpus e mostramos que mesmo arquiteturas de transformadores de última geração têm um desempenho ruim, enfatizando a complexidade do conjunto de dados.', 'fr': "Ce travail présente Itihasa, un jeu de données de traduction à grande échelle contenant 93 000 paires de shlokas sanskrits et leurs traductions en anglais. Les shlokas sont extraits de deux épopées indiennes, à savoir le Ramayana et le Mahabharata. Nous décrivons d'abord la motivation qui sous-tend la conservation d'un tel ensemble de données et nous poursuivons par une analyse empirique pour en faire ressortir les nuances. Nous comparons ensuite les performances des modèles de traduction standard sur ce corpus et montrons que même les architectures de transformateurs de pointe fonctionnent mal, ce qui souligne la complexité de l'ensemble de données.", 'es': 'Este trabajo presenta Itihasa, un conjunto de datos de traducción a gran escala que contiene 93 000 pares de shlokas sánscritos y sus traducciones al inglés. Los shlokas están extraídos de dos epopeyas indias, El Ramayana y El Mahabharata. Primero describimos la motivación detrás de la conservación de un conjunto de datos de este tipo y hacemos un seguimiento con un análisis empírico para resaltar sus matices. Luego comparamos el rendimiento de los modelos de traducción estándar en este corpus y demostramos que incluso las arquitecturas de transformadores de última generación funcionan mal, haciendo hincapié en la complejidad del conjunto de datos.', 'ru': 'Эта работа знакомит с Итихасой, крупномасштабным набором данных для перевода, содержащим 93 000 пар санскритских шлок и их переводов на английский язык. Шлоки извлечены из двух индийских эпосов, а именно., The Ramayana и The Mahabharata. Сначала мы описываем мотивацию, лежащую в основе курирования такого набора данных, а затем проводим эмпирический анализ, чтобы выявить его нюансы. Затем мы сравниваем производительность стандартных моделей перевода на этом корпусе и показываем, что даже самые современные архитектуры трансформаторов работают плохо, подчеркивая сложность набора данных.', 'ja': '本作は、93,000組のサンスクリット語のシロッコとその英訳を含む大規模な翻訳データセットであるItihasaを紹介します。シュロカは、2つのインドの叙事詩から抽出されています。ラーマーヤナ朝、ラーマーヤナ朝、マハーバーラタ朝.まず、そのようなデータセットのキュレーションの背後にある動機を説明し、そのニュアンスを引き出すために経験的分析をフォローアップします。次に、このコーパスの標準翻訳モデルのパフォーマンスをベンチマークし、最先端の変圧器アーキテクチャでさえも、データセットの複雑さを強調して、パフォーマンスが悪いことを示します。', 'zh': '其言Itihasa,大译数集,含93,000梵语shlokas英文翻译。 Shlokas摘自两部印度史诗,即《罗摩衍那》及《摩诃婆罗多》也。 先言治此数集后动,然后实论以见其细微差别。 然后于此语料库上准转换模型之性而试之,虽先进转换器架构亦差,以强数集之复杂性。', 'hi': 'यह काम इतिहास, एक बड़े पैमाने पर अनुवाद डेटासेट का परिचय देता है जिसमें संस्कृत श्लोकों के 93,000 जोड़े और उनके अंग्रेजी अनुवाद शामिल हैं। श्लोक दो भारतीय महाकाव्यों अर्थात् रामायण और महाभारत से निकाले गए हैं। हम पहले इस तरह के डेटासेट के क्यूरेशन के पीछे प्रेरणा का वर्णन करते हैं और इसकी बारीकियों को बाहर लाने के लिए अनुभवजन्य विश्लेषण के साथ पालन करते हैं। फिर हम इस कॉर्पस पर मानक अनुवाद मॉडल के प्रदर्शन को बेंचमार्क करते हैं और दिखाते हैं कि यहां तक कि अत्याधुनिक ट्रांसफॉर्मर आर्किटेक्चर भी डेटासेट की जटिलता पर जोर देते हुए खराब प्रदर्शन करते हैं।', 'ga': 'Tugann an saothar seo isteach Itihasa, tacar sonraí aistriúcháin ar mhórscála ina bhfuil 93,000 péire de shlokas Sanscrait agus a gcuid aistriúcháin Béarla. Baintear na shlokas as dhá eipiciúil Indiach viz., an Ramayana agus The Mahabharata. Déanaimid cur síos ar dtús ar an spreagadh atá taobh thiar de choimeád tacar sonraí den sórt sin agus leanaimid ar aghaidh le hanailís eimpíreach chun a nuances a thabhairt amach. Déanaimid tagarmharcáil ansin ar fheidhmíocht na múnlaí caighdeánacha aistriúcháin ar an gcorpas seo agus léirímid go n-oibríonn fiú ailtireachtaí claochladán den scoth fiú, ag cur béime ar chastacht an tacair sonraí.', 'el': 'Το έργο αυτό εισάγει την Ιτιχάσα, ένα σύνολο δεδομένων μετάφρασης μεγάλης κλίμακας που περιέχει 93.000 ζεύγη σανσκριτικών σλόκας και τις αγγλική τους μεταφράσεις. Οι σλόκες εξάγονται από δύο ινδικά έπη, δηλαδή: Η Ραμαγιάνα και η Μαχαμπαράτα. Αρχικά περιγράφουμε το κίνητρο πίσω από την επιμέλεια ενός τέτοιου συνόλου δεδομένων και ακολουθούμε εμπειρική ανάλυση για να αναδείξουμε τις αποχρώσεις του. Στη συνέχεια, αξιολογούμε την απόδοση των τυποποιημένων μοντέλων μετάφρασης σε αυτό το σώμα και δείχνουμε ότι ακόμη και οι σύγχρονες αρχιτεκτονικές μετασχηματιστών αποδίδουν κακή απόδοση, τονίζοντας την πολυπλοκότητα του συνόλου δεδομένων.', 'ka': 'ეს სამუშაო იტასა, რომელიც სანგლისკრიტის შლოკას და მათი ანგლისური თავსუშაების 93 000 ზოგის გადაწყენება. შლოკაები ექსტრაქტირებულია ორი ინდიანეთის ეპიკისგან. პამაიანა და მაჰაბარატია. ჩვენ პირველად მოტივაციას ასეთი მონაცემების კურაციის შემდეგ აღწერეთ და ამოთრიკალური ანალიზაციის შემდეგ აღწერეთ მისი ნუანციების შემდეგ. შემდეგ ჩვენ სტანდარტუქციის მოდელების გამოყენება ამ კორპუსში და ჩვენ ჩვენ ჩვენ ჩვენ ჩვენ ჩვენებთ, რომ კიდევ სტანდარტუქციის რექტიქტუქტირების არქტიქტუქტირები ცოტა გამოყ', 'hu': 'Ez a munka bemutatja az Itihasa-t, egy nagyszabású fordítási adatkészletet, amely 93 000 pár szanszkrit slókát és azok angol fordítását tartalmazza. A shlokákat két indiai epikusból nyerik ki, azaz: A Ramayana és a Mahabharata. Először bemutatjuk az ilyen adatkészlet kurátora mögött rejlő motivációt, majd empirikus elemzéssel követjük az árnyalatokat. Ezután összehasonlítjuk a szabványos fordítási modellek teljesítményét ezen a korpuszon, és megmutatjuk, hogy még a legkorszerűbb transzformátor architektúrák is rosszul teljesítenek, hangsúlyozva az adatkészlet bonyolultságát.', 'it': "Questo lavoro introduce Itihasa, un set di dati di traduzione su larga scala contenente 93.000 paia di shloka sanscrito e le loro traduzioni in inglese. Le shloka sono estratte da due epics indiani vale a dire., Il Ramayana e il Mahabharata. In primo luogo descriviamo la motivazione alla base della cura di tale set di dati e seguiamo con analisi empiriche per tirarne fuori le sfumature. Analizziamo quindi le prestazioni dei modelli di traduzione standard su questo corpus e mostriamo che anche le architetture di trasformazione all'avanguardia funzionano male, sottolineando la complessità del set di dati.", 'kk': 'Бұл жұмыс 93 000 санскрит шлокасы мен ағылшынша аудармаларының үлкен масштабтағы аудармалар деректерін Итихасасын таңдайды. Шлокас екі Индиялық эпикалардан таратылады. Рамаяна және Махабарата. Біз біріншіден бұл деректер қорларының артындағы мотивациясын таңдап, оның нюанс тарту үшін эмпирикалық анализацияларына келеді. Содан кейін стандартты аудармалардың үлгілерін осы корпустың іске асырып, сондай-ақ әртүрлі түрлендіруші архитектуралардың әдістері жаман іске асырып, деректер жиының комплексін бағыттайды.', 'ms': 'Kerja ini memperkenalkan Itihasa, set data terjemahan skala besar yang mengandungi 93,000 pasangan shlokas Sanskrit dan terjemahan bahasa Inggeris mereka. The shlokas are extracted from two Indian epics viz.,  Ramayana dan Mahabharata. Pertama kita menggambarkan motivasi di belakang curasi set data seperti itu dan mengikuti dengan analisis empirik untuk mengeluarkan nuansinya. Kemudian kita benchmark prestasi model terjemahan piawai pada korpus ini dan menunjukkan bahawa bahkan arkitektur pengubah state-of-the-art berkesan buruk, menekankan kompleksiti set data.', 'lt': 'Šiame darbe pristatomas Itihasa, didelio masto vertimo duomenų rinkinys, kuriame yra 93 000 porų sanskritų šlokų ir jų vertimų anglų kalba. Šlokai išgaunami iš dviejų Indijos epikų, t. y. The Ramayana and The Mahabharata.  We first describe the motivation behind the curation of such a dataset and follow up with empirical analysis to bring out its nuances.  Tada lyginame standartinių vertimo modelių rezultatus šiame korpuse ir parodysime, kad netgi naujausios transformatorių architektūros veikia blogai, pabrėždamos duomenų rinkinio sudėtingumą.', 'mk': 'Оваа работа ја претставува Итихаса, голем набор на податоци за превод кој содржи 93.000 парови санскритски шлоки и нивни англиски преводи. Шлоките се извлечени од две индиски епики The Ramayana and The Mahabharata.  Прво ја опишуваме мотивацијата зад курацијата на ваков податок и следиме со емпирична анализа за да ги издадеме нејзините нијанси. Потоа ја проценуваме изведбата на стандардните преведувачки модели на овој корпус и покажуваме дека дури и најсовремените трансформаторски архитектури лошо работат, истакнувајќи ја комплексноста на податоците.', 'ml': 'ഈ പ്രവര്\u200dത്തിയില്\u200d 93,000 ജോടി സാന്\u200dസ്ക്രിറ്റ് ഷ്ലോക്കാസിനെയും ഇംഗ്ലീഷിന്\u200dറെ ഭാഷകങ്ങളെയും കൊണ്ടുള്ള ഒരു വലിയ വിവരങ്ങള ഷ്ലോക്കാസിനെ രണ്ടു ഇന്ത്യന്\u200d എപ്പിക്സില്\u200d നിന്നും പുറത്താക്കിയിരിക്കുന്നു. റാമായാന, മഹഹരാത്ത. നമ്മള്\u200d ആദ്യം ഇത്തരം ഡാറ്റാസെറ്റിന്റെ പിന്നിലുള്ള പ്രഭോഷണത്തെ വിശദീകരിക്കുകയും അതിന്റെ പൂര്\u200dണ്ണമായ വിശ്വാസം  പിന്നെ നമ്മള്\u200d ഈ കോര്\u200dപ്പുസില്\u200d സാധാരണ പരിഭാഷയുടെ മോഡലുകളുടെ പ്രവര്\u200dത്തനങ്ങള്\u200d ബെന്\u200dക്കിള്\u200d ചെയ്യുന്നു. പിന്നീട് കാണിക്കുന്നത് ആർട്ട് മാറ്', 'mt': 'Dan ix-xogħol jintroduċi Itihasa, sett ta’ dejta dwar traduzzjoni fuq skala kbira li fih 93,000 par ta’ shlokas Sanskriti u t-traduzzjonijiet tagħhom bl-Ingliż. L-shlokas huma estratti minn żewġ epiċi Indjani viz. Ir-Ramayana u l-Mahabharata. L-ewwel a ħna niddeskrivu l-motivazzjoni wara l-kurazzjoni ta’ sett ta’ dejta bħal dan u nasegwu b’analiżi empirika biex inħarġu n-nuanzi tiegħu. Imbagħad nagħmlu referenza għall-prestazzjoni tal-mudelli standard tat-traduzzjoni fuq dan il-korpus u nuru li anke l-arkitetturi tat-trasformaturi l-aktar avvanzati jwettqu prestazzjoni ħażina, b’enfasi fuq il-kumplessità tas-sett tad-dejta.', 'no': 'Dette arbeidet introduserer Itihasa, eit stor omsetjingsdata med 93 000 par av Sanskrit shlokas og engelske omsetjingar. shlokas er ekstrahert frå to indiske epics, viz. Ramayana og Mahabharata. Vi beskriver først motivasjonen bak kursinga av slike datasett og følgjer opp med empirisk analyse for å få ut nuansen sine. Vi benchmarkerer derfor utviklinga av standardmodeller for omsetjing på denne korpusen og viser at selv tilstandsformeringsarkitekturar utfører slik dårlig, som tyder på kompleksiteten av datasettet.', 'pl': 'Niniejsza praca przedstawia Itihasa, duży zestaw danych tłumaczeniowych zawierający 93.000 pary sanskrytycznych szlok i ich angielskie tłumaczenia. Słoki są ekstrahowane z dwóch eposów indyjskich, tj.: Ramayana i Mahabharata. Najpierw opisujemy motywację stojącą za kuracją takiego zbioru danych, a następnie analizę empiryczną, aby wykazać jego niuanse. Następnie porównujemy wydajność standardowych modeli tłumaczeniowych na tym korpusie i pokazujemy, że nawet najnowocześniejsze architektury transformatorów działają słabo, podkreślając złożoność zbioru danych.', 'ro': 'Această lucrare introduce Itihasa, un set de date de traducere la scară largă care conține 93.000 de perechi de shloka sanscrite și traducerile lor în limba engleză. Shlokas sunt extrase din două epice indiene și anume, Ramayana şi Mahabharata. Mai întâi descriem motivația din spatele curățării unui astfel de set de date și urmărim cu analiză empirică pentru a scoate în evidență nuanțele sale. Apoi analizăm performanța modelelor standard de traducere pe acest corpus și arătăm că chiar și arhitecturile de transformare de ultimă generație performează slab, subliniind complexitatea setului de date.', 'sr': 'Ovaj rad predstavlja Itihasu, veliku skupinu prevoda podataka sa 93.000 parova Sanskrit a shlokasa i njihovih engleskih prevoda. Šloki su izvuèeni iz dva indijska epika viz. Ramajana i Mahabharata. Prvo opisujemo motivaciju iza križanja takvih podataka i pratimo empiričku analizu kako bi izveli njegove nuance. Onda smo usmjereni učinkovitost standardnih modela prevođenja na ovom korpusu i pokazujemo da čak i arhitekture pretvarača umjetnosti čine loše, naglašavajući kompleksnost seta podataka.', 'si': 'මේ වැඩේ ඉතිහාසාව, ලොකු විශාල පරිවර්තන දත්ත සෙට් 93,000 සැන්ස්ක්\u200dරිට් ෂ්ලෝකාස් සහ ඔවුන්ගේ ඉංග්\u200dරීසි භ ශ්ලෝකාස් වල ඉන්දියානු ප්\u200dරශ්නයක් දෙකක් වලින් අරගෙන ඉන්නේ. රාමයානා සහ මහාබරාතා. අපි මුලින්ම දත්ත සූදානයක් පිටිපස්සේ ප්\u200dරතික්\u200dරියාව විස්තර කරනවා ඒ වගේම දත්ත සූදානයක් පිටිපස්සේ ඒවා  අපි පස්සේ මේ කෝර්පුස් වල ස්ථානික වාර්තාවක් නිර්මාණය කරනවා ඒ වගේම පෙන්වන්නේ ස්ථානික වාර්තාවක් නිර්මාණය කරනවා ඒ වගේම', 'so': 'Shaqadaasu waxay soo bandhigtaa Itihasa, taariikhda turjumista oo aad u weyn oo ku yaala 93,000 oo labo oo ka mid ah Sanskrit shlokas iyo turjumaadkooda Ingiriis. Shilokaska waxaa laga soo saaray laba baaritaan oo Indian ah. reer Raamaanna iyo reer Mahabharataana. Marka ugu horeysa waxaynu sawiraynaa dhaqdhaqaaqa daboolka danbiyada kadibna waxaynu ku soconnaa baaritaanka caqliga ah si aan u soo bixino nuurkiisa. We then benchmark the performance of standard translation models on this corpus and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.', 'ta': 'This work introduces Itihasa, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations.  ஷ்லோகாக்ஸ் இரண்டு இந்தியன் விக்ஸிலிருந்து வெளியேற்றப்பட்டது. ராமானானா மற்றும் மஹராதா. நாம் முதலில் இத்தகைய தகவல் அமைப்புக்கு பின்னால் ஊக்கத்தை விளக்குகிறோம் மற்றும் அதின் மூலம் வெளிப்படுத்துவதற்கு ம பின்னர் நாம் இந்த கோப்புகளில் நிலையான மொழிபெயர்ப்பு மாதிரிகளின் செயல்பாட்டை குறிப்பிடுகிறோம் மற்றும் இந்த குறிப்புகளின் சிக்கலை கூ', 'sv': 'Detta arbete introducerar Itihasa, en storskalig översättningsdata som innehåller 93 000 par sanskritshlokas och deras engelska översättningar. Shlokas extraheras från två indiska epics dvs., Ramayana och Mahabharata. Först beskriver vi motivationen bakom kurateringen av en sådan datauppsättning och följer upp med empirisk analys för att lyfta fram dess nyanser. Vi jämför sedan prestandan hos standardöversättningsmodeller på denna korpus och visar att även toppmoderna transformatorarkitekturer presterar dåligt, vilket betonar datauppsättningens komplexitet.', 'ur': 'یہ کام ایتهاسا کو معلوم کرتا ہے، ایک بڑی اسکیل ترجمہ ڈاٹ سٹ میں 93,000 جوڑے سنسکریت شلوکاس اور ان کی انگلیسی ترجمہ کرتی ہیں. شلوکاس دو انڈی اپیک viz سے نکالے گئے ہیں، رامیانا اور مہبوراتا ہم پہلی بار ایسے ڈاٹ سٹ کے پیچھے موثرت کو توصیح دیتے ہیں اور اس کی نائنست کو نکالنے کے لئے مضبوط تحلیل کے ساتھ اتباع کرتے ہیں۔ پھر ہم اس کورپوس پر استاندارڈ ترجمہ موڈل کے عملکرد کو بینچم کر رہے ہیں اور دکھاتے ہیں کہ اگرچہ ایست کی ترجمہ ترجمہ معماری مصنوعیت برابر عمل کرتی ہے اور دکھانے کی پیچیدگی کی تسبیح کرتی ہے۔', 'mn': 'Энэ ажил Итихасаг илтгэдэг. 93,000 санскрит шлокас болон Англи хэлний хөрөнгө оруулагдсан том хэмжээний хөрөнгө өгөгдлийн сангуудыг илтгэдэг. Шлокас хоёр Энэтхэг эпик визис гаргасан. Рамаяна, Махабарата. Эхлээд бид ийм өгөгдлийн сангийн урам зориулалтыг тайлбарлаж, нуанс гаргахын тулд эмператикийн шинжилгээг дагах болно. Тэгээд бид энэ корпус дээр стандарт орчуулах загварын үйл ажиллагааг багтана. Мөн урлагийн шилжүүлэгч архитектурууд ч гэсэн зөвхөн зөвхөн хийдэг, өгөгдлийн хэмжээсүүдийн цогцыг багтана.', 'uz': "@ info Slokaslar ikkita Hindiston epiks vizdan chiqaradi. The Ramayana and The Mahabharata.  Biz birinchi maʼlumot sahifadagi harakatni tasavvur qilamiz va o'z nuqtarni chiqarish uchun muvaffaqiyatlarni o'rganamiz. Keyin biz bu kompyuterdagi standard tarjima modellarining natijasini ko'rib chiqaramiz va shaxsiy o'zgartirish tuzuvchilarining holati yomon bajarayotganimizni ko'rsatamiz va maʼlumotlar sohasidagi murakkablarini ko'rsatadi.", 'vi': 'Việc này giới thiệu Itihasa, một bộ dữ liệu dịch lớn có chứa 3,000 đôi của Sanskrit shlokas và bản dịch tiếng Anh của họ. The shloka are extracted from two Indian epics viz. Ramayana và Mahabharata. Trước tiên, chúng tôi mô tả động cơ thúc đẩy việc quản lý một bộ dữ liệu như vậy và theo đuổi các phân tích có kinh nghiệm để bộc lộ các sắc thái. Sau đó, chúng tôi xem trọng hiệu quả của các mô hình dịch tiêu chuẩn trên tập thể này và cho thấy rằng các kiến trúc chuyển hóa hiện đại cũng rất tệ, nhấn mạnh tính phức tạp của bộ dữ liệu.', 'hr': 'Ovaj rad predstavlja Itihasu, veliku skupinu prevoda podataka sa 93.000 par Sanskrit a shlokasa i njihovih engleskih prevoda. Shlokas je izvučen iz dva indijska epika viz., i Ramayana i Mahabharata. Prvo opisujemo motivaciju iza zaključavanja takvog kompleta podataka i pratimo empiričku analizu kako bi izvršili njegove nuance. Onda smo usmjereni učinkovitost standardnih modela prevoda na ovom korpusu i pokazujemo da čak i arhitekture transformacije umjetnosti loše čine, naglašavajući kompleksnost seta podataka.', 'bg': 'Тази работа представя Итихаса, мащабен набор от данни за преводи, съдържащ 93 000 двойки санскритски шлока и техните преводи на английски език. Шлоките са извлечени от два индийски епика: Рамаяна и Махабхарата. Първо описваме мотивацията зад курирането на такъв набор от данни и проследяваме с емпиричен анализ, за да покажем нюансите му. След това сравняваме ефективността на стандартните модели за превод на този корпус и показваме, че дори най-съвременните трансформаторни архитектури работят слабо, подчертавайки сложността на набора от данни.', 'id': 'Pekerjaan ini memperkenalkan Itihasa, set data terjemahan skala besar yang mengandung 93.000 pasangan shloka Sanskrit dan terjemahan bahasa Inggris mereka. Shloka-nya dikeluarkan dari dua epik India viz. Ramayana dan Mahabharata. Kami pertama-tama menggambarkan motivasi di balik curasi dataset seperti itu dan mengikuti dengan analisis empiris untuk mengeluarkan nuansinya. Kemudian kita benchmark prestasi model terjemahan standar pada korpus ini dan menunjukkan bahwa bahkan arkitektur transformator state-of-the-art bekerja buruk, menekankan kompleksitas dataset.', 'da': 'Dette værk introducerer Itihasa, et stort oversættelsesdatasyt, der indeholder 93.000 par sanskrit shlokas og deres engelske oversættelser. Shlokaerne er udvundet fra to indiske epics, dvs., Ramayana og Mahabharata. Vi beskriver først motivationen bag kurateringen af et sådant datasæt og følger op med empirisk analyse for at fremhæve dets nuancer. Vi benchmark derefter ydeevnen af standardoversættelsesmodeller på dette korpus og viser, at selv state-of-the-art transformatorarkitekturer fungerer dårligt, hvilket understreger datasættets kompleksitet.', 'nl': "Dit werk introduceert Itihasa, een grootschalige vertaaldataset met 93.000 paren Sanskriet shloka's en hun Engelse vertalingen. De shloka's worden gewonnen uit twee Indiase epoken, namelijk: De Ramayana en de Mahabharata. We beschrijven eerst de motivatie achter de curatie van zo'n dataset en volgen op met empirische analyse om de nuances ervan naar voren te brengen. Vervolgens benchmarken we de prestaties van standaard vertaalmodellen op dit corpus en laten we zien dat zelfs state-of-the-art transformatorarchitecturen slecht presteren, wat de complexiteit van de dataset benadrukt.", 'ko': '이 작업은 93000쌍의 범어 shlokas와 영문 번역을 포함하는 대형 번역 데이터 집합인 Itihasa를 소개했다.shlokas는 두 편의 인도 사시를 뽑았다:.로마연나와 마호바로도.우리는 먼저 이러한 데이터 집합 배후의 동기를 묘사한 다음에 실증 분석을 통해 그 미세한 차이를 드러냈다.그 다음에 우리는 이 어료 라이브러리에서 표준 번역 모델의 성능에 대해 기준 테스트를 실시한 결과 가장 선진적인transformer 구조도 좋지 않은 것으로 나타났다. 이것은 데이터 집합의 복잡성을 두드러지게 했다.', 'fa': 'این کار ایتهاسا را معرفی می\u200cکند، یک مجموعه داده\u200cهای ترجمه بزرگ با 93.000 جفت سانسکریت شلوکاس و ترجمه\u200cهای انگلیسی آنها. شلوکاس\u200cها از دو اپیک هندی بیرون آورده می\u200cشوند. رامايانا و مهواراتا اولین بار انگیزه پشت کنترل چنین مجموعه داده ها را توصیف می کنیم و با تحلیل امپراطوری دنبال می کنیم تا نوانس های آن را بیاوریم. سپس ما عملکرد مدل\u200cهای ترجمه استاندارد را روی این کورپوس تغییر می\u200cدهیم و نشان می\u200cدهیم که حتی معماری\u200cهای تغییر\u200cدهنده\u200cی هنری بد انجام می\u200cدهند، و تسبیح پیچیدگی مجموعه\u200cی داده\u200cها را تسبیح می\u200cدهند.', 'de': 'Diese Arbeit stellt Itihasa vor, einen großen Übersetzungsdatensatz, der 93.000-Paare von Sanskrit-Slokas und deren englische Übersetzungen enthält. Die Slokas werden aus zwei indischen Epen extrahiert, nämlich: Das Ramayana und das Mahabharata. Zunächst beschreiben wir die Motivation für die Kuration eines solchen Datensatzes und folgen mit empirischen Analysen, um dessen Nuancen herauszuarbeiten. Wir vergleichen dann die Leistung von Standard-Übersetzungsmodellen auf diesem Korpus und zeigen, dass selbst modernste Transformatorarchitekturen schlecht funktionieren, was die Komplexität des Datensatzes unterstreicht.', 'tr': 'Bu işi Sanskrit Şloklarynyň 93,000 çift we iňlisleriň terjimelerinde uly bir terjime maglumatyny tanyşdyrýar. Bu şlokalar iki Hindistan epipikinden açylýar. Ramayana we Mahabharata. Biz ilkinji gezek ýaly veri setiriniň arkasynda motivasyny tassyýarys we nuwasyny çykmak üçin empirik analýusiýa bilen yzarlar. Soňra biz bu korpusyň standart terjime nusgalarynyň etkinleşigini basýarys we hatda sungat transformatörleriniň arhitekterileriniň gaty sarhoşlygyny emplaýarys.', 'af': "Hierdie werk introduseer Itihasa, 'n groot skaal vertaling dataset wat 93 000 paar van Sanskrit shlokas en hulle Engelske vertalings bevat. Die shlokas word uitgevoer van twee Indiese epics Die Ramayana en die Mahabharata. Ons beskryf eerste die motivasie agter die kurasie van so 'n datastel en volg met empiriese analisie om sy nuans uit te bring. Ons benchmarkeer dan die prestasie van standaard vertalingsmodelle op hierdie korpus en wys dat selfs staat-van-kuns-transformeeringsarkitekturke verkeerd uitvoer, en die kompleksiteit van die datastel bepaal.", 'am': 'This work introduces Itihasa, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations.  የሰሎካ ከሁለት የህንድ አፍሪካ ቪዛ አውጥተዋል:: ራማዕናና መሃሃራታ። አስቀድመን እንደዚህ የዳታ ሳጥን በኋላ ያለውን ንቅናቄን እናሳውቃለን፡፡ በዚያን ጊዜም የድምፅ ትርጉም ሞዴላዎችን በአካባቢው ላይ እናሳየዋለን፡፡ የ-የ-art ለውጥ መሠረት መሠረት እንኳ ደካማ ነው፡፡', 'az': 'Bu işlər, 93.000 Sanskrit shlokas və İngilizə çeviriləri olan böyük ölçülü çeviri verilən verilən Itihasa ilə tanışır. Şloklar İki Hindistan epika vizindən çıxarılır. Ramayana və Mahabharata. Biz ilk dəfə belə bir verilən qurğunun arxasındakı motivasiyanı təsdiqləyirik və onun nuans çıxartmaq üçün empirik analizi ilə təsdiqləyirik. Sonra biz bu korpus üstündə standart çevirim modellerinin performansını çəkirik və hətta sanatlı transformer arhitektarının çox pis əməllərini göstəririk, veri qutusu kompleksitəsini təsdiqləyirik.', 'sw': 'Kazi hii inaonyesha Itihasa, kituo cha taarifa za kutafsiri kwa kiwango kikubwa kina jumla ya jozi 93,000 za shlokas na tafsiri zao za Kiingereza. Wa-shlokas wametengwa na vizazi viwili vya kihindi, Ramayana na Mahabharata. Kwanza tunaelezea hamasa ya kufungiwa kwa kituo hicho cha taarifa na kufuatilia kwa uchambuzi wa msisitizo ili kuonyesha matukio yake. Kisha tunaelezea utendaji wa mifano ya tafsiri ya kawaida kwenye mashindano haya na kuonyesha kwamba hata hali ya mabadiliko ya sanaa yanafanya vibaya, na tunasisitiza utata wa seti za taarifa.', 'bn': 'এই কাজটি ইথিহাসাকে পরিচিত করে দেয়, যার মধ্যে ৯৩,০০০ জোড়া সান্স্ক্রিট শ্লোকাস এবং তাদের ইংরেজী অনুবাদ রয়েছে। শ্লোকাসেরা দুই ভারতীয় পরীক্ষা ভিজ থেকে বের করে আনা হয়েছে। রামায়ানা এবং মাহাবারাতা। We first describe the motivation behind the curation of such a dataset and follow up with empirical analysis to bring out its nuances.  তারপর আমরা এই কোর্পাসের উপর স্ট্যান্ডার্ডারেন্ড অনুবাদ মডেলের প্রদর্শন করে দেখাই এবং দেখাই যে এমনকি শিল্প পরিবর্তনের রাষ্ট্রের পরিবর্তনের কাঠামো খা', 'sq': 'This work introduces Itihasa, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations.  The shlokas are extracted from two Indian epics viz.,  Ramayana dhe Mahabharata. Ne së pari përshkruajmë motivimin pas kurimit të një grupi të tillë të dhënash dhe ndjekim me analizë empirike për të nxjerrë nuancat e saj. We then benchmark the performance of standard translation models on this corpus and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.', 'ca': "Aquesta obra introdueix Itihasa, un conjunt de dades de traducció a gran escala que conté 93.000 parells de shlokas sánscrits i les seves traduccions en anglès. The shlokas are extracted from two Indian epics viz.,  El Ramayana i el Mahabharata. Primer descrivim la motivació darrere de la curació d'aquest conjunt de dades i seguim amb l'anàlisi empírica per a mostrar les seves nuances. Llavors comparem el desempeny dels models de traducció standard en aquest corpus i demostrem que fins i tot les arquitectures de transformació d'última generació actuen malament, enfatizant la complexitat del conjunt de dades.", 'cs': 'Tato práce představuje Itihasa, rozsáhlý překladatelský datový soubor obsahující 93.000 páry sanskrtských šlok a jejich anglické překlady. Šloky jsou extrahovány ze dvou indických eposů, tj., Ramayana a Mahabharata. Nejprve popisujeme motivaci ke zpracování takového datového souboru a navazujeme na empirickou analýzu, abychom vyzdvihli jeho nuance. Následně porovnáme výkon standardních překladových modelů na tomto korpusu a ukážeme, že i moderní transformátorové architektury fungují špatně, což zdůrazňuje složitost datové sady.', 'hy': 'Այս աշխատանքը ներկայացնում է Իթիհազան, մեծ ծավալի թարգմանման տվյալների համակարգը, որը պարունակում է 93,000 զույգ սանդկրատական շլոկա և նրանց անգլերեն թարգմանություններ: The shlokas are extracted from two Indian epics viz.,  The Ramayana and The Mahabharata.  Առաջին հերթին մենք նկարագրում ենք նման տվյալների կառուցվածքի դրդապատճառը և հետևում ենք էմպրիկական վերլուծությանը, որպեսզի բացահայտենք դրա նյուանսները: Այնուհետև մենք համեմատում ենք ստանդարտ թարգմանման մոդելների արտադրությունը այս կորպոսի վրա և ցույց ենք տալիս, որ նույնիսկ ամենաբարձր ճարտարապետական կառուցվածքները վատ են աշխատում, շեշտելով տվյալների համակարգի բարդությունը:', 'et': 'Käesolev töö tutvustab Itihasa, ulatuslikku tõlkeandmekogumit, mis sisaldab 93 000 paari sanskriti šlokat ja nende inglise keele tõlkeid. Shlokad on ekstraheeritud kahest India eepilisest: Ramayana ja Mahabharata. Esmalt kirjeldame sellise andmekogumi kureerimise motivatsiooni ja jätkame empiirilise analüüsiga selle nüansside esiletõstmiseks. Seejärel võrdleme standardsete tõlkemudelite jõudlust sellel korpusel ja näitame, et isegi tipptasemel trafoarhitektuurid toimivad halvasti, rõhutades andmekogumi keerukust.', 'bs': 'Ovaj rad predstavlja Itihasu, veliku skupinu prevoda podataka sa 93.000 par Sanskrit a shlokasa i njihovih engleskih prevoda. Šloki su izvučeni iz dva indijska epika viz. Ramajana i Mahabharata. Prvo opisujemo motivaciju iza zaključavanja takvog kompleta podataka i pratimo empiričku analizu kako bi izveli njegove nuance. Onda smo usmjereni učinkovitost standardnih modela prevoda na ovom korpusu i pokazujemo da čak i arhitekture transformatora umjetnosti loše izvode, naglašavajući kompleksnost seta podataka.', 'fi': 'T채ss채 ty철ss채 esitell채채n Itihasa, laajamittainen k채채nn철stietosarja, joka sis채lt채채 93 000 paria sanskritin shlokaa ja niiden englanninkielisi채 k채채nn철ksi채. Shlokat on otettu kahdesta intialaisesta eeppisest채: Ramayana ja Mahabharata. Kuvaamme ensin aineiston kuratoinnin taustalla olevaa motivaatiota ja seuraamme empiirist채 analyysi채 sen vivahteiden esiin tuomiseksi. T채m채n j채lkeen vertaamme standardik채채nn철smallien suorituskyky채 t채ss채 korpusessa ja osoitamme, ett채 jopa uusimmat muuntaja-arkkitehtuurit toimivat huonosti korostaen aineiston monimutkaisuutta.', 'ha': "Wannan aikin yana ƙarfafa Itihasa, da data mai girma na ƙunsa da 93,000 nau'i na Sankrit shlokas da fassararsu na Ingiriya. An kashewar da kashi biyu na Hindu, Da Ramayana da Mahabharata. Kayyan da Muke bayyana juyi baka wani akan wannan da aka samu kuma Muke ƙara bayani da anayyar empirical dõmin ya sami damansa. Sa'an nan kuma Muke haƙur da cikakken misãlai masu fassari na daidaita a kan wannan nau'in, kuma Mu nuna cewa, ko da halin-halin-musamman-sanatarwa masu shige masu zartar da kuma, ana ƙayyade masu kamfata na danne-set.", 'he': 'העבודה הזאת מכירה את איטיהאסה, קבוצת נתונים של תרגום גדולה שמכילה 93,000 זוגות של שלוקות סנסקריטים והתרגומות האנגלית שלהם. השלוקס מווצאים משני אפיקות אינדיאניות. הרמיאנה והמאהבהרטה. ראשית אנחנו מתארים את המוטיבציה מאחורי הטיפול של קבוצת נתונים כזו ולהמשיך עם ניתוח אמפירית כדי להוציא את הניואנסים שלה. ואז נבדוק את ההופעה של דוגמני התרגום הסטנדרטיים על הקורפוס הזה ונראה שאפילו ארכיטקטורות המעברת המאומנות המאומנות מתבצעות גרועות, ומדגישות את מורכבות המידע.', 'jv': 'Wurung iki gunakake nggambar Itihasa, dadi nggambar luwih-manut kanggo nganggo The sh lokas iku dipujug-ujaran winih podho Ramayana karo Muhamarata. Awak dhéwé mbéwé énhadaké percokaké aké beraksi ning acara dadi sing beraksi karo hal-hal dadi sing dirakno ngono nggawe barang kejahatan Awak dhéwé nglanggar aturan sing perusahaan anyar tentang anyar tentang karo ngono kuwi nggawe barang-sistem sing dikarepaké itêrung.', 'sk': 'To delo predstavlja Itihaso, obsežen prevodni nabor podatkov, ki vsebuje 93.000 parov sanskrtskih šlok in njihovih angleških prevodov. Šloke so pridobljene iz dveh indijskih epikov: Ramayana in Mahabharata. Najprej opišemo motivacijo za kuracijo takega nabora podatkov in nadaljujemo z empirično analizo, da izpostavimo njegove nianse. Nato primerjamo učinkovitost standardnih prevajalskih modelov na tem korpusu in pokažemo, da tudi najsodobnejše transformatorske arhitekture delujejo slabo, kar poudarja kompleksnost podatkovnega nabora.', 'bo': 'ལས་ཀ་འདིས་སྔོན་ལྟའི་ནང་དུ་ཨིན་ཊི་ཧ་ཅང་གི་སྐྱེས་ཆེན་པོ་ཞིག་ཡིག་གནང་ཆེན་པོ་ཞིག་ལ་བཤད་ཀྱི་ཡོད། shlokas འདི་རྒྱ་གར་གྱི་ལྟ་བུའི་viz གཉིས་ལས་དབྱིབས་བཏང་ཡོད། ར་མའི་ཡ་ན་དང་མ་ཧ་རཊ་གཉིས་ཀྱིས་ ང་ཚོས་དང་པོ་མཇུག་གིས་འདི་དག་གི་གནས་ཚུལ་ཚགས་ཀྱི་རྗེས་སུ་འགྲེལ་བཤད་བྱས་ནས་དབྱེ་ཞིབ་ཡོད་པ་དང་། འུ་ཚོས་ཀྱི་མཐུད་སྣང་འདིའི་ནང་གི་ཚད་ལྡན་གྱི་དཔེ་དབྱིབས་བྱེད་ཀྱི་ལས་འགན་བྱས་ན།'}
{'en': 'NICT-5’s Submission To WAT 2021 : MBART Pre-training And In-Domain Fine Tuning For Indic Languages NICT -5’s Submission To  WAT  2021:  MBART  Pre-training And In-Domain Fine Tuning For Indic Languages', 'pt': 'Submissão do NICT-5 ao WAT 2021: pré-treinamento MBART e ajuste fino no domínio para idiomas índicos', 'ar': 'إرسال NICT-5 إلى WAT 2021: تدريب MBART المسبق والضبط الدقيق في المجال للغات الهندية', 'es': 'Presentación de NICT-5 al WAT 2021: preentrenamiento de MBART y ajuste fino en el dominio para idiomas índicos', 'fr': 'Soumission du NICT-5 au WAT 2021\xa0: Pré-formation MBART et réglage fin dans le domaine pour les langues indiennes', 'ja': 'NICT -5のWAT 2021への提出： MBART事前トレーニングとインディカー言語のドメイン内微調整', 'zh': 'NICT-5向WAT 2021提交:MBART预练印度语域内微调', 'ru': 'Представление NICT-5 к WAT 2021: Предварительное обучение MBART и тонкая настройка в пределах домена для языков индикаторов', 'hi': 'एनआईसीटी -5 का डब्ल्यूएटी 2021 को प्रस्तुत करना: एमबीएआरटी पूर्व-प्रशिक्षण और इंडिक भाषाओं के लिए इन-डोमेन फाइन ट्यूनिंग', 'ga': 'Aighneacht NICT-5 chuig WAT 2021: Réamhoiliúint MBART Agus Mionchoigeartú san Fhearann Do Theangacha Indeacha', 'hu': 'NICT-5 benyújtása WAT 2021: MBART előképzés és tartományon belüli finomhangolás indikus nyelvekhez', 'el': 'Υποβολή του στο Προπαραγγελία και λεπτή ρύθμιση για τις ινδικές γλώσσες', 'kk': "NICT-5's Submission to WAT 2021: MBART Pre-training and In-Domain Fine Tuning For Indic Languages", 'ka': "NICT-5's Submission to WAT 2021: MBART Pre-Training and In-Domain Fine Tuning for Indic Languages", 'it': 'Presentazione di NICT-5 a WAT 2021: MBART Pre-formazione e messa a punto di dominio per le lingue indiche', 'lt': 'NICT-5 pristatymas 2021 m. TAT: MBART parengiamasis mokymas ir nuodugnis koregavimas vidaus vietoje indinėms kalboms', 'mk': 'Предложението на НИЦТ-5 на WAT 2021: предобука на МБАРТ и фино прилагодување за индичките јазици', 'ms': 'Submission NICT-5 TO WAT 2021: MBART Pre-training and In-Domain Fine Tuning For Indic Languages', 'ml': '2021: എംബാര്\u200dട്ട് മുന്\u200dപ് പരിശീലിപ്പിക്കുന്നതും ഇന്റിക്ക് ഭാഷകള്\u200dക്ക് വേണ്ടി ഡോമെയിനില്\u200d നല്ല ട്രിങ്ങിങ്ങിന്\u200d', 'no': "NICT-5's Submission To WAT 2021: MBART Pre-Training And In-Domain Fine Tuning For Indic Languages", 'pl': 'Podanie NICT-5 do WAT 2021: MBART Pre-trening i In-Domain Fine Tuning dla języków indyjskich', 'mt': 'Is-Sottomissjoni tan-NICT-5 għall-WAT 2021: it-taħriġ minn qabel tal-MBART u t-Tuning Fin fl-Domain għal-Lingwi Indiċi', 'mn': "NICT-5's Submission to WAT 2021: MBART Pre-training and In-Domain Fine Tuning for Indic Languages", 'ro': 'Submiterea NICT-5 la WAT 2021: MBART Pre-training și reglare fină în domeniu pentru limbile indice', 'si': "NICT-5's Sub-Mision to WAT 2021: MBART pre-Training and In-Domain Fine Tuning for Indian Links", 'sr': 'NICT-5 podmission na WAT 2021:', 'so': "NICT-5's Submission To WAT 2021: MBART Pre-training and In-Domain Fine Tuning For Indic Languages", 'ur': "NICT-5's Submission to WAT 2021: MBART pre-training and in-Domain Fine Tuning for Indian languages", 'sv': 'NICT-5:s bidrag till WAT 2021: MBART Pre-utbildning och in-Domain finjustering för indiska språk', 'ta': '2021: MBART முன் பயிற்சி முன் மற்றும் டொமைன் மொழிகளுக்கு நல்ல பயிற்சி', 'vi': 'Phụ thuộc NIRT-5 đến WAT 2021: Sabart Pre-huấn luyện và in-Domain fine tuning for indic languages', 'uz': 'NICT-5 WAT 2021: MBART taʼminlovchi oldin va Domen tilida juda yaxshi tizim', 'bg': 'Предварително обучение и фина настройка в домейна за индийски езици', 'hr': 'NICT-5 je podnosio WAT 2021: MBART prije obuke i dobra prilagodba u domenu za indijske jezike', 'da': "NICT-5's indsendelse til WAT 2021: MBART Pre-training og In-Domain finjustering for indiske sprog", 'nl': "NICT-5's inzending aan WAT 2021: MBART Pre-training en In-Domain Fine Tuning voor Indische Talen", 'id': "NICT-5's Submission To WAT 2021: MBART Pre-training And In-Domain Fine Tuning For Indic Languages", 'de': "NICT-5's Submission To WAT 2021: MBART Pre-Training und In-Domain Feinabstimmung für indische Sprachen", 'sw': 'Ujumbe wa NICT-5 wa WAT 2021: MBART wa mafunzo ya kabla ya kujifunza na Kutumia lugha za Kihindi', 'ko': 'NICT-5 WAT 2021에 제출: 인도어 MBART 사전 교육 및 영역 내 미세 조정', 'fa': 'Submission of NICT-5 to WAT 2021: MBART pre-training and in-Domain good tuning for Indian languages', 'tr': "NICT-5'S Submission to WAT 2021: MBART Pre-training And In-Domain Fine Tuning For Indic Languages", 'af': 'NICT-5 se Submission na WAT 2021', 'sq': 'Submission of NICT-5 to WAT 2021: MBART Pre-training and In-Domain Fine Tuning for Indic Languages', 'am': "NICT-5's Submission To WAT 2021: MBART Pre-training And In-Domain Fine Tuning For Indic Languages", 'hy': 'ՆԻՏ-5-ի ներկայացումը 2021 թվականին․ ՄԲՏ-ի նախապատրաստման և բնագավառի ներկայացման ֆինանսավորման համար ինդիկ լեզուների համար', 'az': "NICT-5's Submission to WAT 2021: MBART Pre-training and In-Domain Fine Tuning for Indic Languages", 'bs': 'NICT-5-ova podnosnica za WAT 2021: MBART pre obuke i dobra prilagodba u domenu za indijske jezike', 'bn': '২০১২ সালে এনআইসিটি-৫ সাবিমিশন: এমবার্ট প্রশিক্ষণের পূর্বে এবং ইন্ডিক ভাষার জন্য ভাষার জন্য ভালো টিউনিং নিয়েছেন', 'ca': "NICT-5's Submission To WAT 2021: MBART Pre-training And In-Domain Fine Tuning For Indic Languages", 'cs': 'NICT-5 podání WAT 2021: MBART předškolení a In-Domain jemné ladění pro indické jazyky', 'et': 'NICT-5 esitamine WAT 2021: MBARTi eelkoolitus ja domeenisisene peen häälestamine india keeltele', 'fi': 'NICT-5:n toimitus WAT 2021: MBART-esikoulutus ja sisäinen hienosäätö indialaisille kielille', 'ha': '@ info: status', 'sk': 'Predložitev NICT-5 v WAT 2021: MBART Predusposabljanje in fino nastavitev v domeni za indijske jezike', 'he': "NICT-5's Submission To WAT 2021: MBART Pre-training And In-Domain Fine Tuning For Indic Languages", 'jv': "NILT-5's Subtask To WAT 2020 1: MBART before-curation and In-domain Fine Tuning For individual Languages", 'bo': "NICT-5's Submission To WAT 2021: MBART Pre-training And In-Domain Fine Tuning For Indic Languages"}
{'en': 'In this paper we describe our submission to the multilingual Indic language translation wtask MultiIndicMT under the team name NICT-5. This task involves  translation  from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by  fine-tuning  on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training.', 'ar': 'في هذه الورقة ، نصف تقديمنا إلى مهمة ترجمة اللغات الهندية متعددة اللغات "MultiIndicMT" تحت اسم الفريق "NICT-5". تتضمن هذه المهمة ترجمة من 10 لغات هندية إلى الإنجليزية والعكس صحيح. كان الهدف من المهمة هو استكشاف فائدة المناهج متعددة اللغات باستخدام مجموعة متنوعة من الهيئات الموازية وأحادية اللغة داخل المجال وخارج المجال. نظرًا للنجاح الأخير للتدريب المسبق على NMT متعدد اللغات ، قررنا استكشاف نموذج MBART مسبقًا للتدريب على مجموعة كبيرة أحادية اللغة تغطي جميع اللغات في هذه المهمة متبوعة بضبط دقيق متعدد اللغات على الشركات الصغيرة في المجال. أولاً ، لاحظنا أن قدرًا صغيرًا من التدريب المسبق متبوعًا بضبط دقيق على مجموعات صغيرة ثنائية اللغة يمكن أن تحقق مكاسب كبيرة عند عدم استخدام التدريب المسبق. علاوة على ذلك ، يؤدي الضبط الدقيق متعدد اللغات إلى مزيد من المكاسب في جودة الترجمة التي تتفوق بشكل كبير على الأساس القوي متعدد اللغات الذي لا يعتمد على أي تدريب مسبق.', 'es': 'En este artículo describimos nuestro envío a la tarea de traducción multilingüe al idioma índico «MultiAccmit» bajo el nombre del equipo «NICT-5». Esta tarea implica la traducción de 10 idiomas índicos al inglés y viceversa. El objetivo de la tarea era explorar la utilidad de los enfoques multilingües utilizando una variedad de corpus monolingües y paralelos dentro y fuera del dominio. Dado el éxito reciente de la capacitación previa multilingüe de NMT, decidimos explorar la capacitación previa de un modelo MBART en una gran colección de corpus monolingües que abarque todos los idiomas en esta tarea, seguido de un ajuste multilingüe en pequeños cuerpos dentro del dominio. En primer lugar, observamos que una pequeña cantidad de preentrenamiento seguido de un ajuste fino en pequeños cuerpos bilingües puede producir grandes ganancias cuando no se utiliza el preentrenamiento. Además, el ajuste multilingüe conduce a mayores mejoras en la calidad de la traducción, lo que supera significativamente a una base multilingüe muy sólida que no depende de ninguna formación previa.', 'fr': "Dans cet article, nous décrivons notre soumission à la tâche de traduction multilingue en langue indienne «\xa0MultiIndicMT\xa0» sous le nom d'équipe «\xa0NICT-5\xa0». Cette tâche implique la traduction de 10 langues indiennes vers l'anglais et vice-versa. L'objectif de la tâche était d'explorer l'utilité des approches multilingues utilisant une variété de corpus parallèles et monolingues dans le domaine et hors domaine. Compte tenu du récent succès de la pré-formation NMT multilingue, nous avons décidé d'explorer la pré-formation d'un modèle MBART sur une vaste collection de corpus monolingues couvrant toutes les langues dans cette tâche, suivie d'un réglage fin multilingue sur de petits corpus dans le domaine. Tout d'abord, nous avons observé qu'une petite quantité de pré-formation suivie d'un peaufinage sur de petits corpus bilingues peut produire des gains importants par rapport à la non-utilisation de la pré-formation. En outre, le peaufinage multilingue entraîne des gains supplémentaires en termes de qualité de traduction, ce qui surpasse largement une base multilingue très solide qui ne repose sur aucune formation préalable.", 'pt': 'Neste artigo, descrevemos nossa submissão à tarefa de tradução multilíngue do idioma índico “MultiIndicMT” sob o nome da equipe “NICT-5”. Esta tarefa envolve a tradução de 10 línguas índicas para o inglês e vice-versa. O objetivo da tarefa era explorar a utilidade de abordagens multilíngues usando uma variedade de corpora paralelos e monolíngues no domínio e fora do domínio. Dado o recente sucesso do pré-treinamento NMT multilíngue, decidimos explorar o pré-treinamento de um modelo MBART em uma grande coleção de corpus monolíngue cobrindo todos os idiomas nesta tarefa, seguido de ajuste fino multilíngue em pequenos corpora no domínio. Em primeiro lugar, observamos que uma pequena quantidade de pré-treinamento seguido de ajuste fino em pequenos corpora bilíngues pode gerar grandes ganhos quando o pré-treinamento não é utilizado. Além disso, o ajuste fino multilíngue leva a ganhos adicionais na qualidade da tradução, o que supera significativamente uma linha de base multilíngue muito forte que não depende de nenhum pré-treinamento.', 'ja': '本稿では、「NICT -5」というチーム名で、多言語インド語翻訳wtask「MultiIndicMT」への提出について説明します。このタスクには、10のインドの言語から英語への翻訳と、その逆の翻訳が含まれます。このタスクの目的は、様々なドメイン内およびドメイン外の並列および単一言語のコーラを使用した多言語アプローチの有用性を探ることでした。多言語NMT事前トレーニングの最近の成功を考慮して、このタスクのすべての言語をカバーする大規模な単一言語コーパスコレクションでのMBARTモデルの事前トレーニングと、小さなドメイン内コーパスでの多言語微調整を探ることにしました。まず、少量の事前トレーニングと、その後の小さなバイリンガル体の微調整は、事前トレーニングを使用しない場合よりも大きな利得をもたらす可能性があることを観察しました。さらに、多言語の微調整は、事前トレーニングに依存しない非常に強力な多言語ベースラインを大幅に上回る翻訳品質のさらなる向上につながります。', 'zh': '本文,我们述了我们以团队名"NICT-5"向多言语印度语译wtask"MultiIndicMT"提交的文件。 事涉10种印度语翻译成英语,反之亦然。 其务在探用诸域内与域外并行、单语语料库多言法之用。 鉴近多言 NMT 预练之成,探涵盖言大单语语料库合上预训练 MBART 形,然后多语料库于小域。 先之,不豫教,少豫教,然后微调小双语语料库,可以生大利。 又多言微进一步提高译,大优于不恃预培训者强言基线。', 'ru': 'В этой статье мы описываем наше представление на многоязычном переводе языка индикатора wtask «MultiIndicMT» под названием команды «NICT-5». Эта задача включает в себя перевод с 10 языков Indic на английский и наоборот. Цель этой задачи заключалась в изучении полезности многоязычных подходов с использованием различных внутридоменных и внедоменных параллельных и одноязычных корпусов. Учитывая недавний успех многоязычного предварительного обучения НМТ, мы решили изучить возможность предварительного обучения модели MBART на большой одноязычной коллекции корпусов, охватывающей все языки в этой задаче, с последующей многоязычной тонкой настройкой на небольших внутридоменных корпусах. Во-первых, мы заметили, что небольшое количество предварительной подготовки, за которой следует тонкая настройка на небольших двуязычных корпусах, может принести большие выгоды по сравнению с тем, когда предварительная подготовка не используется. Кроме того, многоязычная тонкая настройка приводит к дальнейшему повышению качества перевода, что значительно превосходит очень сильную многоязычную базовую линию, которая не зависит от какого-либо предварительного обучения.', 'hi': 'इस पत्र में हम बहुभाषी भारतीय भाषा अनुवाद wtask "MultiIndicMT" टीम नाम "NICT-5" के तहत हमारे सबमिशन का वर्णन करते हैं। इस कार्य में 10 भारतीय भाषाओं से अंग्रेजी में अनुवाद शामिल है और इसके विपरीत। कार्य का उद्देश्य विभिन्न प्रकार के इन-डोमेन और आउट-ऑफ-डोमेन समानांतर और मोनोलिंगुअल कॉर्पोरेट का उपयोग करके बहुभाषी दृष्टिकोणों की उपयोगिता का पता लगाना था। बहुभाषी एनएमटी पूर्व-प्रशिक्षण की हाल की सफलता को देखते हुए हमने इस कार्य में सभी भाषाओं को कवर करने वाले एक बड़े मोनोलिंगुअल कॉर्पस संग्रह पर एक MBART मॉडल को पूर्व-प्रशिक्षण का पता लगाने का फैसला किया, जिसके बाद छोटे इन-डोमेन कॉर्पोरेट पर बहुभाषी फाइन-ट्यूनिंग की गई। सबसे पहले, हमने देखा कि छोटे द्विभाषी कॉर्पोरेट पर ठीक-ट्यूनिंग के बाद पूर्व-प्रशिक्षण की एक छोटी राशि पूर्व-प्रशिक्षण का उपयोग नहीं किए जाने पर बड़े लाभ प्राप्त कर सकती है। इसके अलावा, बहुभाषी ठीक-ट्यूनिंग अनुवाद की गुणवत्ता में आगे लाभ की ओर जाता है जो एक बहुत ही मजबूत बहुभाषी आधार रेखा को काफी बेहतर बनाता है जो किसी भी पूर्व-प्रशिक्षण पर भरोसा नहीं करता है।', 'ga': 'Sa pháipéar seo déanaimid cur síos ar ár n-aighneacht chuig an tasc ilteangach aistriúcháin Indice “MultiIndicMT” faoin ainm foirne “NICT-5”. Is éard atá i gceist leis an tasc seo ná aistriúchán ó 10 dteanga Indice go Béarla agus vice-versa. Ba é cuspóir an taisc iniúchadh a dhéanamh ar fhóntas cineálacha cur chuige ilteangacha agus úsáid á baint as éagsúlacht de chorparáidí comhthreomhara agus aonteangacha laistigh agus lasmuigh den fhearann. Mar gheall ar chomh maith agus a d’éirigh le réamhoiliúint ilteangach NMT le déanaí bheartaíomar réamhoiliúint a dhéanamh ar mhúnla MBART ar chnuasach mór corpais aonteangach a chlúdódh gach teanga sa tasc seo agus mionchoigeartú ilteangach ar chorpas beag in-fhearainn ina dhiaidh sin. Ar an gcéad dul síos, thugamar faoi deara gur féidir le méid beag réamhoiliúint agus mionchoigeartú ar chorpora beaga dátheangacha dul chun cinn mór a bhaint amach nuair nach n-úsáidtear an réamhoiliúint. Ina theannta sin, tagann tuilleadh feabhais ar cháilíocht an aistriúcháin as mionchoigeartú ilteangach, rud a sháraíonn go mór bonnlíne an-láidir ilteangach nach bhfuil ag brath ar aon réamhoiliúint.', 'ka': "ამ დომენტში ჩვენ ჩვენი წარმოდგენებას მრავალენგური ინდიური ენაზე გადაწერა wtask 'MultiIndicMT' სახელის სახელით 'NICT-5'. ეს დავალება ახლა 10 ინდიური ენებიდან ინგლისურად და შემდეგ შეცვლა. დავალების მიზეზი იყო, რომ multilingual approaches გამოყენებული განსხვავებული დიომინის და გარეშე დიომინის პარალელი და მონოლენგური კოპორაზე გამოყენება. მრავალენგური NMT წინასწარმატების წარმატებით განვითარებით, ჩვენ განვითარებეთ MBART მოდელის წინასწარმატების მოდელის გავაკეთოთ დიდი მონოლენგური კორპუსის კოლექციაში, რომელიც ამ დავაკეთებული ყველა ენათ პირველად, ჩვენ დავხედავთ, რომ პატარა წინატრუნციის რაოდენობა, რომელიც მალკი ორიენგური კოპორაზე შეუძლიათ გავაკეთოთ დიდი წარმოდგენები, როცა წინატრუნცია არ გამ მეორე, მრავალენგური კონფიგურაცია იქნება უფრო მეტად დავიწყება საფორმაციის კაalitეში, რომელიც მნიშვნელოვანია ძალიან ძალიან მრავალენგური კონფიგურაცია, რომელიც არ იქ", 'hu': 'Ebben a tanulmányban bemutatjuk a többnyelvű indikus nyelvű fordításra történő benyújtásunkat a "MultiIndicaMT" csapatnév alatt "NICT-5". Ez a feladat magában foglalja a 10 indikus nyelvről angolra és fordítva. A feladat célja a többnyelvű megközelítések hasznosságának feltárása volt a különböző domain belüli és domain kívüli párhuzamos és egynyelvű korpuszok segítségével. Tekintettel a többnyelvű NMT előképzés közelmúltbeli sikerére, úgy döntöttünk, hogy egy MBART modell előképzését vizsgáljuk fel egy nagy, egynyelvű korpuszgyűjteményen, amely lefedi az összes nyelvet ebben a feladatban, majd többnyelvű finomhangolást követ a kisméretű korpuszokon. Először is megfigyeltük, hogy egy kis mennyiségű előképzés, majd finomhangolás a kis kétnyelvű corporákon, nagy előnyöket hozhat, ha az előképzést nem használják. Továbbá a többnyelvű finomhangolás további javulást eredményez a fordítási minőség terén, ami jelentősen meghaladja a nagyon erős többnyelvű alapvetőséget, amely nem támaszkodik semmilyen előképzésre.', 'el': 'Στην παρούσα εργασία περιγράφουμε την υποβολή μας στην πολύγλωσση ινδική μετάφραση με το όνομα της ομάδας "NICT-5". Το έργο αυτό περιλαμβάνει μετάφραση από δέκα ινδικές γλώσσες στα αγγλικά και αντίστροφα. Στόχος του έργου ήταν η διερεύνηση της χρησιμότητας των πολύγλωσσων προσεγγίσεων χρησιμοποιώντας μια ποικιλία από παράλληλα και μονογλωσσικά σώματα εντός και εκτός τομέα. Δεδομένης της πρόσφατης επιτυχίας της πολύγλωσσης προεκπαίδευσης αποφασίσαμε να διερευνήσουμε την προεκπαίδευση ενός μοντέλου σε μια μεγάλη μονογλωσσική συλλογή σωμάτων που καλύπτει όλες τις γλώσσες σε αυτό το έργο, ακολουθούμενη από πολύγλωσσο συντονισμό σε μικρά σώματα εντός του τομέα. Πρώτον, παρατηρήσαμε ότι μια μικρή ποσότητα προεκπαίδευσης που ακολουθείται από την τελειοποίηση σε μικρά δίγλωσσα σώματα μπορεί να αποφέρει μεγάλα κέρδη σε σχέση με την περίπτωση που η προεκπαίδευση δεν χρησιμοποιείται. Επιπλέον, η πολύγλωσση τελειοποίηση οδηγεί σε περαιτέρω βελτίωση της ποιότητας της μετάφρασης, η οποία ξεπερνά σημαντικά μια πολύ ισχυρή πολυγλωσσική βάση που δεν βασίζεται σε καμία προεκπαίδευση.', 'it': "In questo articolo descriviamo la nostra presentazione alla traduzione multilingue in lingua indica wtask 'MultiIndicaMT' sotto il nome del team 'NICT-5'. Questo compito prevede la traduzione da 10 lingue indiche in inglese e viceversa. L'obiettivo del compito era quello di esplorare l'utilità di approcci multilingui utilizzando una varietà di corpora paralleli e monolingue in-dominio e fuori-dominio. Dato il recente successo della pre-formazione NMT multilingue abbiamo deciso di esplorare la pre-formazione di un modello MBART su una vasta collezione di corpus monolingue che copre tutte le lingue in questo compito seguito da una messa a punto multilingue su piccoli corpora in-domain. In primo luogo, abbiamo osservato che una piccola quantità di pre-formazione seguita da una messa a punto di piccole corpore bilingue può produrre grandi guadagni rispetto a quando non viene utilizzato pre-formazione. Inoltre, la messa a punto multilingue porta a ulteriori miglioramenti nella qualità della traduzione, che supera significativamente una base di riferimento multilingue molto forte che non si basa su alcuna formazione preliminare.", 'lt': 'Šiame dokumente apibūdiname savo pranešimą daugiakalbiams indiškos kalbos vertimui wtask "MultiIndicMT" komandos pavadinimu "NICT-5". Ši užduotis apima vertimą iš 10 indinių kalbų į anglų ir atvirkščiai. Šios užduoties tikslas buvo išnagrinėti daugiakalbių metodų naudingumą naudojant įvairias lygiagrečias ir vienakalbes korporas vidaus ir ne srityse. Atsižvelgdami į neseniai sėkmingą daugiakalbį NMT parengiamąjį mokymą, nusprendėme išnagrinėti MBART model į dėl didelio monokalbinio korpuso rinkinio, apimančio visas kalbas šioje užduotyje, po to daugiakalbį tikslinimą dėl mažų korpuso. Pirma, pastebėjome, kad nedidelis pasirengimo mokymas, po kurio sureguliuojama maža dvikalbė korpora, gali duoti didelės naudos, palyginti su ankstesniu mokymu. Be to, daugiakalbis patobulinimas lemia tolesnį vertimo kokybės pagerėjimą, kuris gerokai viršija labai stiprią daugiakalbę bazę, kuri nesiremia jokiu ikimokymu.', 'kk': 'Бұл қағазда біз бірнеше тілді тілді аударуға "MultiIndicMT" командасы \'NICT-5\' атауында жіберімізді таңдаймыз. Бұл тапсырма 10 индикалық тілдерден ағылшын тіліне аудару және қайта-қайта. Тапсырманың мақсаты бірнеше доменде және доменден тыс параллель және монолинг корпорасын қолдану үшін көптеген тілдік жағдайлардың утилитасын зерттеу. Бірнеше тілді NMT алдын- ала оқыту үшін біз MBART үлгісін алдын- ала оқытуға арналған үлкен монолингі корпус жинақтарында, осы тапсырманың барлық тілдерін табуға арналған, кейінгі көп тілді корпорасында көп тілді о Біріншіден, біз кішкентай тілді корпораға кейін кейін кейін кейін кейін кейін кейін кейін кейінгі алдындағы бақылау кезінде үлкен жетістіктерді өзгертуге болады. Қосымша, көптеген тілдерді баптау көптеген аудармалардың сапасына көптеген көптеген көптеген тілдердің негізгі негізгі жолын өзгертеді.', 'ms': "Dalam kertas ini kami menggambarkan penghantaran kami kepada terjemahan bahasa berbilang bahasa India wtask 'MultiIndicMT' di bawah nama pasukan 'NICT-5'. NAME OF TRANSLATORS Tugas ini melibatkan terjemahan dari 10 bahasa India ke bahasa Inggeris dan sebaliknya. Tujuan tugas adalah untuk mengeksplorasi utiliti pendekatan berbilang bahasa menggunakan pelbagai dalam-domain dan luar-domain selari dan monobahasa corpora. Mengingat keberhasilan pembelajaran NMT berbilang bahasa baru-baru ini kami memutuskan untuk mengeksplorasi pembelajaran model MBART pada koleksi corpus monobahasa besar yang meliputi semua bahasa dalam tugas ini diikuti oleh penyesuaian berbilang bahasa pada corpora kecil dalam domain. Pertama, kami memperhatikan bahawa jumlah kecil praselatihan diikuti dengan penyesuaian baik pada corpora dua bahasa kecil boleh memberikan keuntungan besar jika praselatihan tidak digunakan. Selain itu, penyesuaian berbilang bahasa membawa kepada kemampuan terjemahan yang lebih lanjut yang jauh melebihi dasar berbilang bahasa yang sangat kuat yang tidak bergantung pada mana-mana praselatihan.", 'mk': 'Во овој весник го опишуваме нашето поднесување на мултијазичкиот индиски превод wtask „MultiIndicMT“ под името на тимот „NICT-5“. Оваа задача вклучува превод од 10 индиски јазици на англиски и обратно. Целта на задачата беше да се истражи корисноста на мултијазичните пристапи користејќи различни паралелни и монојазични корпора во домен и надвор од домен. Со оглед на неодамнешниот успех на мултијазичниот предобук на НМТ, одлучивме да го истражуваме предобуката на моделот на МБРТ за голема монојазична колекција на корпус, која ги покрива сите јазици во оваа задача, по кои следи мултијазичното финетизирање на малите корпуси во домен Прво, забележавме дека мала количина предобука, по која следи подобрување на малите двојјазични корпорации, може да даде големи профили отколку кога предобуката не се користи. Покрај тоа, мултијазичното финетизирање води до понатамошни зголемувања во квалитетот на преводот, што значително го надминува многу силниот мултијазичен основен резултат кој не се зависи од никаква предобука.', 'ml': "ഈ പത്രത്തില്\u200d 'NICT-5' എന്ന ടീമിന്\u200dറെ പേരില്\u200d 'MultiIndicMT' എന്ന പേരില്\u200d ഞങ്ങള്\u200d പല ഭാഷയിലുള്ള ഭാഷയിലേക്ക് ഞങ്ങളുടെ സന്ദേശം വിവരിക് 10 ഇംഗ്ലീഷ് ഭാഷകളില്\u200d നിന്നും ഇംഗ്ലീഷിലേക്കും വൈവിലേക്കും പരിഭാഷപ്പെടുത്തുന്ന ഈ ജോലി പ്രവര്\u200dത്തിക്കുന്നതിന്റെ ലക്ഷ്യം പല ഭാഷകങ്ങളുടെയും ഉപയോഗിക്കുന്നത് ഡൊമൈനിന്റെയും ഡൊമൈനില്\u200d നിന്നും അംഗീകരിക്കാത്തതിന്റെയും മ ഈ ജോലിയിലെ എല്ലാ ഭാഷകളെയും മൂല്ല ഭാഷകളെയും പരിശീലിപ്പിക്കുന്നതിന്\u200dറെ അടുത്ത വിജയം കൊണ്ട് നമ്മള്\u200d മുന്\u200dപ് പരിശീലിപ്പിക്കാന്\u200d തീരുമാനിച്ചു. ആദ്യം ഞങ്ങള്\u200d കണ്ടുപിടിച്ചു കൊണ്ടിരിക്കുന്നത് ചെറിയൊരു പരിശീലനത്തിന്\u200dറെ പിന്നാലെ ചെറിയൊരു പരിശീലനത്തിന്\u200dറെ മുന്\u200dകൂട്ടം മ അതിനുമുമ്പ്, പല ഭാഷകങ്ങളുടെ സുന്ദരമുള്ള വിവരങ്ങളിലേക്ക് കൂടുതല്\u200d വര്\u200dദ്ധിപ്പിക്കുന്നത് വിവരങ്ങളിലേക്ക് കൂടുതല്\u200d വര്\u200dദ്ധിപ്പിക്", 'mt': "F'dan id-dokument niddeskrivu s-sottomissjoni tagħna għat-traduzzjoni multilingwi tal-lingwa indika wtask 'MultiIndicMT' taħt l-isem tat-tim 'NICT-5'. Dan il-kompitu jinvolvi t-traduzzjoni minn 10 lingwi Indiċi għall-Ingliż u viċi versa. L-għan tal-kompitu kien li tiġi esplorata l-utilità ta’ approċċi multilingwi bl-użu ta’ varjetà ta’ korpi paralleli u monolingwi f’dominju u barra mid-dominju. Minħabba s-suċċess reċenti tat-taħriġ ta’ qabel l-NMT multilingwi ddeċidew li nesploraw mudell ta’ qabel it-taħriġ ta’ MBART dwar ġbir kbir ta’ korpus monolingwi li jkopri l-lingwi kollha f’dan il-kompitu segwit minn a ġġustament multilingwi fuq korpura żgħira fid-dominju. L-ewwel nett, osservajna li ammont żgħir ta’ taħriġ minn qabel segwit minn a ġġustament finat fuq korpura bilngwi żgħira jista’ jagħti qligħ kbir meta ma jintużax taħriġ minn qabel. Barra minn hekk, l-irfinar multilingwi jwassal għal aktar kisbiet fil-kwalità tat-traduzzjoni li b’mod sinifikanti jaqbeż linja bażi multilingwi b’saħħitha ħafna li ma tiddependix fuq l-ebda taħriġ minn qabel.", 'pl': "W niniejszym artykule opisujemy nasze zgłoszenie do wielojęzycznego tłumaczenia języka indyjskiego wtaak 'MultiIndicMT' pod nazwą zespołu 'NICT-5'. Zadanie to polega na tłumaczeniu z 10-języków indyjskich na angielski i odwrotnie. Celem zadania było zbadanie użyteczności podejść wielojęzycznych z wykorzystaniem różnych korpusów wewnątrz domeny i poza domeną równoległych i jednojęzycznych. Biorąc pod uwagę niedawny sukces wielojęzycznego szkolenia wstępnego NMT postanowiliśmy zbadać model przedszkoleniowy MBART na dużej jednojęzycznej kolekcji korpusów obejmującej wszystkie języki w tym zadaniu, a następnie wielojęzyczne dostrajanie małych korpusów wewnątrz domeny. Po pierwsze, zauważyliśmy, że niewielka ilość treningów przedszkoleniowych, po której następuje dopracowanie małych dwujęzycznych korpusów, może przynieść duże zyski w porównaniu z przedszkoleniami. Ponadto dostosowanie wielojęzyczne prowadzi do dalszego wzrostu jakości tłumaczeń, który znacznie przewyższa bardzo silną wielojęzyczną bazę podstawową, która nie opiera się na żadnym przedszkoleniu.", 'ro': "În această lucrare descriem transmiterea noastră la traducerea multilingvă în limba indică wtask 'MultiIndicaMT' sub numele echipei 'NICT-5'. Această sarcină implică traducerea din 10 limbi indice în engleză și invers. Obiectivul misiunii a fost de a explora utilitatea abordărilor multilingve folosind o varietate de corpore paralele și monolingve în domeniu și în afara domeniului. Având în vedere succesul recent al pregătirii NMT multilingve, am decis să explorăm pregătirea unui model MBART pe o colecție largă de corpuri monolingve care acoperă toate limbile în această sarcină urmată de reglarea fină multilingvă a corpurilor mici în domeniu. În primul rând, am observat că o cantitate mică de pre-antrenament urmată de reglarea fină a corporelor bilingve mici poate genera câștiguri mari față de atunci când pre-antrenament nu este utilizat. În plus, reglarea fină multilingvă conduce la creșteri suplimentare în ceea ce privește calitatea traducerii, care depășește semnificativ o bază de referință multilingvă foarte puternică, care nu se bazează pe nici o formă prealabilă.", 'mn': 'Энэ цаасан дээр бид олон хэл хэлний хөгжлийн хөгжлийн хөгжлийг "MultiIndicMT" гэдэг багийн нэр дээр тайлбарлаж байна. Энэ ажил 10 индийн хэлээс Англи хэл рүү орчуулах болон эсрэгээр нь орчуулах юм. Үүний зорилго нь олон хэлний тусламжуудын хэрэгцээг судлах нь олон төрлийн холбоотой, гадна холбоотой параллел болон ганц хэлний корпора ашиглаж байдаг. Олон хэлний NMT сургалтын өмнөх амжилтын тулд бид MBART загварын өмнө сургалтын загварыг нэг том хэлний корпус цуглуулалт дээр судлах шийдсэн юм. Энэ ажлын бүх хэлнүүдийг дүгнэж буй бүх хэлнүүдийг олон хэлний жижиг корпора дээр д Эхлээд бид жижиг хоёр хэл корпоратын тухай бага хэмжээний өмнө суралцах боломжгүй байх үед маш их ашиг авч чадна гэдгийг анзаарсан. Үүнээс гадна олон хэлний төлөвлөгөөс илүү олон хэлний төлөвлөгөө нь хэлэлцүүлэх чадварын тулд илүү их хүчтэй олон хэлний суурь шугамыг дамжуулдаг.', 'no': 'I denne papiret beskriver vi submisjonen vårt til fleirspråksomsetjinga wtask « MultiIndicMT » under gruppenamnet « NICT-5 ». Denne oppgåva involverer omsetjinga frå 10 indiske språk til engelsk og motsatt. Målet på oppgåva var å utforske verktøyet for fleirspråk tilnærmingar ved hjelp av ein del i domene og out-of-domene parallel og monolingual corpora. Gjennom det siste suksessen av fleirspråk NMT-føreøvinga, bestemte vi å utforska føreøvinga ein MBART-modell på eit stor monospråk korpussamling som dekkar alle språk i denne oppgåva etter fleirspråk fine-tuning på små korpora i domenet. Først har vi observert at ein liten mengd føreøvinga følgjande av fine-tuning på små bilinguelt korpora kan gje stor forståking over når føreøvinga ikkje vert brukt. I tillegg fører fleire språk finnstillingar til å få framleis forståking i omsetjingskvalitet som utfører mykje sterk fleirspråk baseline som ikkje er tilbakekalla på noen føreøvingskvalitet.', 'si': "මේ පත්තරේ අපි අපේ පුළුවන් විවෘත කරන්නේ වැඩි භාෂාවක් භාෂාවට wjob 'MultiIndicMT' කණ්ඩායමේ නම 'NICT-5' යට. මේ වැඩේ ඉංඩික් භාෂාවල් 10 වලින් ඉංග්\u200dරීසි වලින් වලින් භාෂාව සම්බන්ධ කරනවා. මේ වැඩේ ඉලක්කම තමයි විශේෂ භාෂාවක් ප්\u200dරයෝජනයක් ප්\u200dරයෝජනය කරනවා වගේ ප්\u200dරමාණයක් සහ ප්\u200dරමාණයක් සමහර විශේෂ සහ භාෂ අලුත් භාෂාවික NMT ප්\u200dරීසික්ෂණයේ අන්තිම වැඩිය සමහර විදිහට අපි තීරණය කළා MBART මොඩේල් එක්ක භාෂාවික කොර්පුස් සංග්රහනයේ ලොකු භාෂාවික මුලින්ම, අපි බලාපොරොත්තු කරලා තියෙනවා කියලා පොඩි ප්\u200dරධානයක් පුළුවන් පුළුවන් ප්\u200dරධානයක් ප්\u200dරයෝජනය කරන්නේ නැති වෙල ඉතින්, බොහොම භාෂාවක් විශ්වාස කරනවා විශ්වාස ක්\u200dරියාත්මක විශ්වාස කරනවා, ඒක ගොඩක් ශක්තිමත් බොහොම භාෂාවක් අධ්\u200dය", 'so': "Qoraalkan waxaynu ku qornaa warqaddayada loo soo dirayo turjumista luuqada kala duduwan ee Indic luqada wtask 'MultiIndicMT' kooxda magaceeda 'NICT-5'. Shaqadan waxaa ku qoran turjumista 10 luuqadood oo Indic ka soo baxa ingiriisiga iyo afka ingiriisiga. Ujeedada shaqadu waa in la baaraandegayo isticmaalka hababka luuqadaha kala duduwan oo lagu isticmaalayo shirkado kala duduwan ee gudaha iyo kuwa aan degaanka ka ahayn oo la isku mid ah iyo qofka luqada ah. Dhab ahaan guulaystii ugu dambeysay waxbarashada hore oo luuqadaha kala duduwan ee NMT, waxaynu go'aannay baaraandegista model MBART oo ku qoran urur badan oo ku qoran luqadaha afka muuqashada ah oo ku qoran dhammaan luuqadan, waxaana soo raacay qoraal luuqado kala duduwan oo ku qoran korporada yar. Marka ugu horeysay, waxaynu aragnay in tiro yar oo waxbarasho horumar ah oo lagu soo raaco iskuul-xiriir labada luqadood ah ayaa bixi kara faa'iido badan marka aan la isticmaalin waxbarasho horumar ah. Intaas waxaa dheer oo faa'iido badan oo luuqadaha kala duduwan ah ku socda faa'iido dheer oo ku saabsan takhasuska turjumaadda, kaas oo si muhiimka ah u muujiya heer aad u xoog badan oo luuqad kala duduwan oo aan ku xirnayn waxbarasho ka horeeya.", 'sv': "I denna uppsats beskriver vi vårt bidrag till den flerspråkiga indiska språköversättningen wtask 'MultiIndicaMT' under teamnamnet 'NICT-5'. Denna uppgift omfattar översättning från 10 indiska språk till engelska och vice versa. Syftet med uppgiften var att undersöka nyttan av flerspråkiga tillvägagångssätt med hjälp av en mängd olika parallella och enspråkiga corpora inom och utanför domänen. Med tanke på den senaste tidens framgångar med flerspråkig NMT pre-training bestämde vi oss för att undersöka pre-training en MBART-modell på en stor enspråkig korpussamling som täcker alla språk i denna uppgift följt av flerspråkig finjustering på små in-domain corpora. För det första konstaterade vi att en liten mängd pre-training följt av finjustering på små tvåspråkiga corpora kan ge stora vinster jämfört med när pre-training inte används. Dessutom leder flerspråkig finjustering till ytterligare förbättringar i översättningskvaliteten, vilket avsevärt överträffar en mycket stark flerspråkig baslinje som inte är beroende av någon fortbildning.", 'sr': "U ovom papiru opisujemo svoju predanost multijezičkom prevodu Indijskog jezika wtask 'MultiIndicMT' pod tim imenom 'NICT-5'. Ovaj zadatak uključuje prevod od 10 Indijskih jezika na engleski i suprotno. Cilj zadatka je bio istražiti korisnost multijezičkih pristupa koristeći različite paralelne i monojezičke korpore u domenu i van domena. S obzirom na nedavni uspeh multijezičkog predobuke NMT-a, odlučili smo da istražimo predobuku MBART model na velikoj monojezičkoj kolekciji korpusa koja pokriva sve jezike u ovom zadatku, a nakon toga je multijezička fin-tuning na malom korporaciji u domenu. Prvo, primetili smo da mala količina predobuke koja je praćena ispravnim napravljanjem malog dvojezičkog korpora može dobiti velike dobitke kada se ne koristi predobuka. Nadalje, multijezička finalizacija vodi do daljnjih dobića kvalitete prevoda koji značajno iznosi veoma jaku multijezičku početnu liniju koja se ne oslanja na bilo kakvu predobuku.", 'ta': "இந்த காகிதத்தில் நாம் பல மொழி சிந்திய மொழி மொழிபெயர்ப்பை விவரிக்கிறோம் 'பல IndicMT' அணி பெயரில் 'NICT-5'. 10 சிந்திக் மொழிகளிலிருந்து ஆங்கிலத்தில் இருந்து மொழிபெயர்ப்பை மொழிபெயர்ப்பில் உள்ளது. பணியின் தலைப்பு பல மொழி முறைகளின் பயன்பாட்டை பயன்படுத்துவதற்காக உள்ளமைப்பில் மற்றும் டோமைன் வெளியே இணைப்பு மற்றும் ஒற்றை மொழி நிறுவனத இந்த பணியில் உள்ள அனைத்து மொழிகளையும் மூடும் மொழிகளையும் மூடும் முன்பயிற்சியின் சமீபத்தில் NMT முன் வெற்றியடைந்த போது நாம் முன்பயிற்சி முறை தேட முடிவு  முதலில், நாங்கள் பார்த்தோம் சிறிய முன்பயிற்சி பின்னர் சிறிய இரண்டு மொழி நிறுவனத்திற்கு முன்பயிற்சி செய்யும் போது பெரிய மேலும், பல மொழிக்கு நன்றி குறிப்பிடுதல் மொழிப்பெயர்ப்பில் மொழிபெயர்ப்பில் மேலும் வளர்ச்சியை கொண்டு வருகிறது. அது முன் பயிற்ச", 'ur': "ہم اس کاغذ میں اپنے مضبوطی کو ملتی زبان انڈی زبان کی ترجمہ wtask 'MultiIndicMT' کی تیم نام 'NICT-5' کے نیچے بیان کرتے ہیں. یہ کام 10 انڈی زبانوں سے انگلیسی میں ترجمہ کرتا ہے اور دوسرے سے۔ اس کام کا موضوع یہ تھا کہ بہت سی زبان کے مطابق مطابق مطابق ڈومین میں اور خارج دامین کے مطابق مختلف طریقے اور ایک زبان کوپرا کے مطابق استعمال کریں. بہت سی زبان NMT کی پیش تربینی کے اخیر موفقیت کے باعث ہم نے ایک بڑی سی زبان کورپوس کالکور پر ایک MBART موڈل کا تحقیق کرنے کا فیصلہ کیا تھا جو اس کام میں تمام زبانوں کو پورے کرتا تھا اور اس کے بعد بہت سی زبان کے مطابق ڈومین کورپور پر بہت سی سی ٹ اولوں سے ہم نے دیکھا کہ تھوڑی کم پیش آموزش کی تعلیم کے پیچھے چھوڑی دو زبان کی جگہ پر بہت بڑی فائدہ اٹھائی جاتی ہے جب اس سے پہلے آموزش نہ ہوتی ہے اور اس کے علاوہ، بہت سی زبان کی پاکیزگی تنظیم کی تعلیم کی کیفیت میں اضافہ ہوتی ہے جو ایک بہت قوی متعدد زبان کی بنسٹ لین پر زیادہ اضافہ نہیں کرتا۔", 'uz': "Bu takarda biz bir necha tillar tili tarjima qiladigan tashkilotni 'NICT-5' guruh nomida 'MultiIndicMT' tashkilotga qaramamiz. Bu vazifa 10 Hindistondan Inglizchaga va vice versa tilga tarjima qiladi. Vazifaning maqsadi, bir necha tillar qo'llanmalarning foydalanishini o'rganish uchun qo'l qo'llanmalarning turli qo'shilga qo'shilishi mumkin. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora.  Birinchi marta, biz bir necha o'rganishdan keyin bir necha o'rganishni ko'rsatdik, ikki tillar kompaniyadan yaxshi ko'p o'rganishni o'rganishga ega bo'ladi. Ko'pchilik tili suhbati tarjima sifatida koʻproq muvaffaqiyatlarni bajaradi. Bu bir necha tillar asboblarini o'zgartiradi. Bu bir avval o'rganishga ishlatmaydigan juda katta ko'pchilik asboblarni bajaradi.", 'vi': 'Trong tờ giấy này chúng tôi mô tả sự đệ trình của chúng tôi đến dịch ngôn ngữ Ấn đa dạng: đa indicMT. Dưới tên đội "NIST-5". Nhiệm vụ này gồm việc dịch từ mười ngôn ngữ Ấn sang tiếng Anh và ngược lại. Nhiệm vụ này nhằm mục đích khám phá sự hữu ích của các phương pháp đa dạng, sử dụng các phương pháp khác nhau trong và ngoài vùng song song song và độc ngôn. Xét thấy thành công gần đây của việc sản xuất ra NMT nhiều năm trước chúng tôi đã quyết định nghiên cứu mô hình MB: trên một bộ sưu tập Tập thể lớn, chung ngôn ngữ, bao gồm tất cả các ngôn ngữ trong nhiệm vụ này, và sau đó là chỉnh sửa lại nhiều loại nhỏ thuộc hạ. Trước hết, chúng tôi nhận thấy rằng một lượng nhỏ tiền đào tạo sau đó là việc chỉnh sửa nhỏ, hai chữ có thể đạt được lợi nhuận lớn hơn khi không sử dụng tiền đào tạo. Thêm vào đó, việc chỉnh sửa đa dạng mang lại nhiều hiệu quả hơn trong chất lượng dịch, khả năng đạt được một cơ sở đa dạng rất mạnh mà không dựa vào tiền đào tạo.', 'bg': 'В настоящата статия описваме нашето представяне на многоезичен индийски превод под името на екипа "НИКТ-5". Тази задача включва превод от 10 индийски езика на английски и обратно. Целта на задачата е да се проучи ползата от многоезичните подходи, използващи различни паралелни и едноезични корпори в домейна и извън домейна. Предвид неотдавнашния успех на многоезичната предварителна подготовка по НМТ решихме да проучим предварителната подготовка на модел на МБАРТ върху голяма едноезична колекция от корпуси, обхващаща всички езици в тази задача, последвана от многоезична фина настройка на малки корпуси в домейна. Първо, забелязахме, че малко количество предобучение, последвано от фино настройване на малки двуезични корпуси, може да доведе до големи печалби в сравнение с това, когато предобучение не се използва. Освен това многоезичното фино настройване води до по-нататъшни подобрения в качеството на превода, което значително надхвърля много силната многоезична база, която не разчита на предварителна подготовка.', 'nl': "In dit artikel beschrijven we onze inzending aan de meertalige Indische taalvertaling wtaak 'MultiIndicMT' onder de teamnaam 'NICT-5'. Deze taak omvat vertaling van tien Indische talen naar het Engels en vice versa. Het doel van de taak was het nut van meertalige benaderingen te onderzoeken met behulp van een verscheidenheid van in-domain en out-of-domain parallelle en monolinguale corpora's. Gezien het recente succes van meertalige NMT pre-training hebben we besloten om een MBART model voor te trainen op een grote monolingual corpuscollectie die alle talen in deze taak omvat gevolgd door meertalige fine-tuning op kleine in-domain corpora's. Ten eerste zagen we dat een kleine hoeveelheid pre-training gevolgd door fine-tuning op kleine tweetalige corpora grote winsten kan opleveren ten opzichte van pre-training niet wordt gebruikt. Bovendien leidt meertalige finetuning tot verdere verbeteringen in de vertaalkwaliteit, die aanzienlijk beter presteert dan een zeer sterke meertalige baseline die niet afhankelijk is van een pre-training.", 'da': "I denne artikel beskriver vi vores indsendelse til den flersprogede indiske sprogoversættelse wtask 'MultiIndicaMT' under teamnavnet 'NICT-5'. Denne opgave omfatter oversættelse fra 10 indiske sprog til engelsk og omvendt. Formålet med opgaven var at undersøge nytten af flersprogede tilgange ved hjælp af en række parallelle og ensprogede corpora inden for og uden for domænet. I betragtning af den nylige succes med flersproget NMT-foruddannelse besluttede vi at undersøge foruddannelsen af en MBART-model på en stor ensproget korpussamling, der dækker alle sprog i denne opgave efterfulgt af flersproget finjustering på små inden for domænet corpora. For det første bemærkede vi, at en lille mængde før-træning efterfulgt af finjustering på små tosprogede korpora kan give store gevinster, når før-træning ikke anvendes. Desuden fører flersprogede finjusteringer til yderligere forbedringer i oversættelseskvaliteten, hvilket betydeligt overstiger en meget stærk flersproget basislinje, der ikke er afhængig af nogen forudgående uddannelse.", 'hr': "U ovom papiru opisujemo naše podatke multijezičkom prevodu Indijskog jezika wtask 'MultiIndicMT' pod tim imenom 'NICT-5'. Ovaj zadatak uključuje prevod od 10 Indijskih jezika na engleski i suprotno. Cilj zadatka je bio istražiti korisnost multijezičkih pristupa koristeći različite paralelne i monojezičke korpore u domenu i van domena. S obzirom na nedavni uspjeh višejezičkih NMT predobuke, odlučili smo istražiti predobuku MBART model a o velikoj monojezičnoj kolekciji korpusa koja pokriva sve jezike u ovom zadatku, nakon čega je višejezička naprava na malom korporaciji u domenu. Prvo smo primijetili da je mala količina predobuke slijedila napravljenje malog dvojezičkog tijela mogla dobiti velike dobiće nakon što se ne koristi predobuka. Nadalje, multijezička finalizacija dovede do daljnjih dobića kvalitete prevoda koji značajno iznosi veoma jaku multijezičku početnu liniju koja se ne oslanja na bilo kakvu predobuku.", 'de': "In diesem Beitrag beschreiben wir unsere Einreichung zur mehrsprachigen indischen Übersetzung wtask 'MultiIndicMT' unter dem Teamnamen 'NICT-5'. Diese Aufgabe beinhaltet die Übersetzung von 10-indischen Sprachen ins Englische und umgekehrt. Ziel der Aufgabe war es, den Nutzen multilingualer Ansätze mit einer Vielzahl von in-domain und out-of-domain parallelen und monolingualen Korpora zu untersuchen. Angesichts des jüngsten Erfolgs des mehrsprachigen NMT-Vortrainings haben wir beschlossen, ein MBART-Modell auf einer großen monolingualen Korpussammlung zu untersuchen, die alle Sprachen in dieser Aufgabe abdeckt, gefolgt von mehrsprachiger Feinabstimmung auf kleinen in-domain Korpora. Zunächst haben wir beobachtet, dass eine kleine Menge an Vortraining gefolgt von Feinabstimmung auf kleinen zweisprachigen Korpora große Gewinne erzielen kann, wenn das Vortraining nicht verwendet wird. Darüber hinaus führt die mehrsprachige Feinabstimmung zu weiteren Qualitätssteigerungen bei der Übersetzung, die eine sehr starke mehrsprachige Ausgangsbasis deutlich übertrifft, die auf keine Vorkenntnisse angewiesen ist.", 'id': "Dalam kertas ini kami menggambarkan pengiriman kami ke terjemahan bahasa berbilang bahasa India wtask 'MultiIndicMT' di bawah nama tim 'NICT-5'. Tugas ini melibatkan terjemahan dari 10 bahasa India ke bahasa Inggris dan sebaliknya. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora.  Mengingat keberhasilan baru-baru ini dari prapelatihan multibahasa NMT kami memutuskan untuk mengeksplorasi prapelatihan model MBART pada koleksi corpus monobahasa besar yang meliputi semua bahasa dalam tugas ini diikuti oleh penyesuaian multibahasa pada corpora kecil dalam domain. Firstly, we observed that a small amount of pre-training followed by fine-tuning on small bilingual corpora can yield large gains over when pre-training is not used.  Selain itu, penyesuaian berbilang bahasa mengarah ke keuntungan lanjut dalam kualitas terjemahan yang signifikan melebihi dasar berbagai bahasa yang sangat kuat yang tidak bergantung pada praselatihan apapun.", 'fa': 'در این کاغذ ما تحویل ما را به ترجمه زبان\u200cهای زیادی زبان هندی توصیف می\u200cکنیم wtask «MultiIndicMT» زیر اسم تیم «NICT-5». این کار شامل ترجمه از ۱۰ زبان هندی به انگلیسی و در عوض است. هدف این وظیفه این بود که استفاده از دسترسی های زیادی زبان را با استفاده از گونه\u200cای از دامنه\u200cها و خارج از دامنه\u200cها متفاوت و تنها زبان بررسی کنیم. با توجه به موفقیت اخیر پیش آموزش های زیادی زبان NMT تصمیم گرفتیم پیش آموزش یک مدل MBART را در یک مجموعه کورپوس بزرگ تک زبان بگیریم که همه زبانها در این کار را پوشانده می\u200cشود، و بعد از تعلیم زیادی زبان در کورپورا کوچک در دومین است. اول ما مشاهده کردیم که یک مقدار پیش آموزشی کوچک از پیش آموزشی پیروی شده است که در شرکت کوچک دو زبان می تواند در زمانی که پیش آموزشی استفاده نمی شود، سود بزرگی فراهم کند. علاوه بر این، تنظیم بسیاری از زبان\u200cها به پیروزی بیشتری در کیفیت ترجمه می\u200cرسد که به طور معنی یک پایین\u200cخط بسیار قوی بسیاری از زبان\u200cهای زیادی که بر هیچ پیش\u200cآموزش اعتماد ندارد.', 'ko': "본고에서 우리는 우리가 여러 언어의 인도어 번역 임무인'MultiIndicmT'에 제출한 팀명을'NICT-5'로 묘사했다.이 임무는 인도어를 10가지 영어로 번역하는 것과 관련이 있는데, 반대로도 마찬가지다.이 임무의 목적은 각종 역내와 역외 평행 자료 라이브러리와 단어 자료 라이브러리를 사용하는 다중 언어 방법의 효용을 탐색하는 것이다.다중 언어 NMT 예비 교육이 최근에 얻은 성공을 감안하여 우리는 본 임무의 모든 언어를 포함하는 대형 단일 언어 자료 라이브러리 집합에서 MBART 모델의 예비 교육을 탐색한 다음에 소형 영역 내 자료 라이브러리에서 다중 언어 마이크로 조정을 진행하기로 결정했다.우선, 우리는 예훈련을 사용하지 않는 것보다 작은 이중 언어 자료 라이브러리에서 소량의 예훈련을 한 다음에 미세하게 조정하면 큰 수익을 얻을 수 있다는 것을 관찰했다.그 밖에 다국어 마이크로스피커는 번역의 질을 더욱 높일 수 있고 어떠한 예비 교육에 의존하지 않는 매우 강한 다국어 기선보다 훨씬 우수하다.", 'tr': "Bu kagyzda biziň gönderişimizi 'MultiIndicMT' toparyň adynda 'NICT-5' gönderilmegimizi ýazyp berýäris. Bu zady 10 Indiki dilden iňlisçe terjime edip görýär. Görevinin amacı, domuslu ve dış domuslu paralel ve monolingual korpora kullanarak çoklu dil yaklaşımlarının kullanımını keşfetmek. Öň köp dilli NMT öňünden öňki okuwçylygynyň üstünliklerine görä biz MBART nusgyny öň-okuwçylygynyň uly monolingual korpus jemgyýetinde gözlemege karar berdik. Birinjisi, biz öňki bilim öňki bilim sistemasynda kiçi bilim korporatynyň yzynda kiçi bir öňki bilim taýýarlanmagynyň uly gazanlygy bolup biler diýip pikir etdik. Mundan soňra, köp dilli täzelikler terjime etmek kynçylygynda ýeterlik gazanýar. Bu ýerde hiç hili öň-okuwçylyga ynanmaýan gaty güýçli bir multi dil baseliniň üstüne täsirleýär.", 'sq': "Në këtë letër ne përshkruajmë dorëzimin tonë në përkthimin e gjuhës indike shumëgjuhëse wtask 'MultiIndicMT' nën emrin e ekipit 'NICT-5'. Kjo detyrë përfshin përkthimin nga 10 gjuhë indike në anglisht dhe vice versa. Objektivi i detyrës ishte të eksploronte përdorimin e metodave shumëgjuhësore duke përdorur një varietet të korprave paralele dhe monogjuhësore në domeni dhe jashtë domeni. Duke pasur parasysh suksesin e kohëve të fundit të parastërvitjes shumëgjuhëse të NMT, vendosëm të eksplorojmë parastërvitjen e një modeli MBART mbi një koleksion të madh të korpusit monogjuhës që mbulon të gjitha gjuhët në këtë detyrë, pasuar nga rregullimi shumëgjuhës në korpusin e vogël në domeni. Së pari, ne vëzhguam se një sasi e vogël paratrajnimi i ndjekur nga rregullimi i mirë në korpra të vogël dygjuhëse mund të sjellë fitime të mëdha kur paratrajnimi nuk përdoret. Përveç kësaj, rregullimi shumëgjuhës shpie në fitime të mëtejshme në cilësinë e përkthimit që në mënyrë të konsiderueshme tejkalon një bazë shumë të fortë shumëgjuhëse që nuk mbështetet në asnjë parastërvitje.", 'af': "In hierdie papier beskrywe ons onderskrywing aan die multilinguele Indiese taal vertaling wtask 'MultiIndicMT' onder die span naam 'NICT-5'. Hierdie taak involveer vertaling van 10 Indiese taal in Engels en omgekies. Die doel van die taak was om die nutsprogram van multitaalske toegang te ondersoek deur 'n verskeie in-domein en uit-domein paraleel en monotaalske korpora te gebruik. Omdat ons die onlangse sukses van multitaalske NMT voor-oefening besluit het om 'n MBART model op 'n groot monotaalske korpus versameling te ondersoek wat alle tale in hierdie taak oordeel deur multitaalske fintuning op klein in-domein korpora volg. Eerste, ons het aangesien dat 'n klein hoeveelheid voor-onderwerp volg deur fyn-tuning op klein twee tale korpora groot verskaf kan gee oor wanneer voor-onderwerp nie gebruik word nie. Verder, meerderlingse fin-tuning lei na verdere verskaffings in vertalingskwaliteit wat betekenlik 'n baie sterk multitaalske basislien uitvoer wat nie op enige vooraf-oefening vertrou nie.", 'sw': "Katika karatasi hii tunaelezea ujumbe wetu wa tafsiri ya lugha ya Kihindi kwa lugha nyingi za lugha za Kihindi iitwayo 'MultiIndicMT' chini ya timu yenye jina la 'NICT-5'... Kazi hii inahusisha kutafsiri kutoka lugha 10 za Kihindi hadi Kiingereza na makamu. Lengo la kazi ni kutafuta matumizi ya mbinu za lugha mbalimbali kwa kutumia makampuni mbalimbali ya ndani na nje ya ndani yanalinganisha na utaratibu wa lugha. Kutokana na mafanikio ya hivi karibuni ya mafunzo ya lugha mbalimbali ya NMT tuliamua kuchunguza mtindo wa mafunzo ya awali wa MBART kwenye mkusanyiko mkubwa wa lugha za kimonolinguzi unaoandika lugha zote katika kazi hii na kufuatia mafunzo ya lugha mbalimbali kwenye makampuni madogo ya ndani. Kwanza, tuliona kuwa kiasi kidogo cha mafunzo ya kabla yaliyofuatiliwa na mafunzo ya vizuri kwa kampuni ndogo ya lugha mbili yanaweza kuleta mafanikio makubwa wakati mafunzo yasitumiwa. Zaidi ya hayo, mafunzo ya lugha mbalimbali yanapelekea mafanikio zaidi katika kiwango cha tafsiri ambacho kinaonyesha msingi mkubwa wa lugha mbalimbali ambao hautegemea mafunzo yoyote ya kabla.", 'am': 'በዚህ ገጽ ለብዙ ቋንቋ ቋንቋ ትርጓሜያችንን እናሳውቃለን፡፡ ይህ ስራ ከ10 የህንድ ቋንቋዎች ወደ እንግሊዘኛ እና ዋና-versa ትርጓሜዎችን ያገባዋል፡፡ የስራው ጉዳዩ የብዙ ቋንቋዎች የደረጃዎች ጥቃት በመጠቀም እና ከዶሜን እና በሙሉ ቋንቋ ኮርፖርት ለመጠቀም ነው፡፡ የቀድሞው የብዙልቋንቋዎች የNMT ቀድሞ ትምህርት በመጠቀም ምክንያት የMBART ሞዴል በዚህ ስራ ውስጥ ያሉትን ቋንቋዎች ሁሉ የሚሸፍኑት የብዙልቋዊ ቋንቋ ደጋፊዎች በዲሞናዊ ኮርፖርት ላይ የተሰኘውን የሞክረናል፡፡ በመጀመሪያው፣ አስቀድሞ ትምህርት በተከተለ ትንሽ የሁለት ቋንቋዎች ኮርፖርት ላይ ጥሩ ፍላጎቶችን በመጠቀም ጊዜ ትልቅ ትልቅ ሀብት እንዲያገኝ አየን፡፡ በተጨማሪም የቋንቋዎች ደጋፊ ጥቅም በማንኛውም በፊት ተማሪ በማይታመን እጅግ የበረታች የቋንቋ ቋንቋዎች መደገፊያ ጥቅም የሚያደርገው ትርጉም ይሄዳል፡፡', 'hy': 'Այս թղթի մեջ մենք նկարագրում ենք մեր ներկայացումը բազմալեզու ինդիկ լեզվի թարգմանմանմանը և հարցնում ենք "Բազմալեզու ինդիկմատ" թիմի անունով "ՆԻՏ-5": This task involves translation from 10 Indic languages into English and vice-versa.  The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora.  Եթե հաշվի առնենք NMT-ի բազմալեզու նախապատրաստման վերջին հաջողությունը, մենք որոշեցինք ուսումնասիրել MBAR-ի նախապատրաստման մոդելը մի մեծ միալեզու կորպուսի հավաքածուի վրա, որը ներառում է այս խնդրի բոլոր լեզուները, հետո բազլեզու բարելավում փոքր բնա Առաջին հերթին, մենք նկատեցինք, որ փոքրիկ երկլեզու մարմնի վերականգնումը հետևում է փոքրիկ նախապատրաստման փոքր քանակությամբ, կարող է մեծ շահույթ ստանալ այն դեպքում, երբ նախապատրաստումը չի օգտագործվում: Ավելին, բազմալեզու բարելավումը հանգեցնում է թարգմանության որակի շարունակական բարձրացումներին, որոնք նշանակալիորեն գերազանցում են շատ ուժեղ բազլեզու հիմքերին, որոնք չեն հիմնված որևէ նախապատրաստման վրա:', 'bs': "U ovom papiru opisujemo naše podatke multijezičkom prevodu Indijskog jezika wtask 'MultiIndicMT' pod tim imenom 'NICT-5'. Ovaj zadatak uključuje prevod od 10 Indijskih jezika na engleski i suprotno. Cilj zadatka je bio istražiti korisnost višejezičkih pristupa koristeći različite paralelne i monojezičke korpore u domenu i van domena. S obzirom na nedavni uspjeh višejezičkih NMT predobuke, odlučili smo da istražimo predobuku MBART model o velikoj monojezičnoj kolekciji korpusa koja pokriva sve jezike u ovom zadatku, a nakon toga je višejezička finalizacija na malom korporaciji u domenu. Prvo smo primijetili da mala količina predobuke koja je praćena napravljenjem malog dvojezičkog korporacija može dobiti velike dobiće kada se ne koristi predobuka. Nadalje, multijezička finalizacija vodi do daljnjih dobića kvalitete prevoda koji značajno iznosi veoma jaku multijezičku početnu liniju koja se ne oslanja na bilo kakvu predobuku.", 'cs': "V tomto článku popisujeme náš příspěvek k vícejazyčnému překladu indického jazyka wtaak 'MultiIndicMT' pod názvem týmu 'NICT-5'. Tento úkol zahrnuje překlad z deseti indických jazyků do angličtiny a naopak. Cílem úkolu bylo prozkoumat užitečnost vícejazyčných přístupů s využitím různých paralelních a monojazyčných korpusů v doméně i mimo doménu. Vzhledem k nedávnému úspěchu vícejazyčného NMT předškolení jsme se rozhodli prozkoumat předškolení MBART modelu na velké monojazyčné korpusové sbírce pokrývající všechny jazyky v tomto úkolu a následně vícejazyčné jemné ladění malých in-doménových korpusů. Za prvé jsme pozorovali, že malé množství předškolení následované jemným laděním na malých dvojjazyčných korpusech může přinést velké zisky oproti předškolení. Kromě toho vícejazyčné jemné ladění vede k dalšímu zvýšení kvality překladu, který výrazně překonává velmi silný vícejazyčný základ, který není závislý na žádném předškolení.", 'az': "Bu kağızda bizim göndərməyimizi çoxlu dil Indik dillərin çevirilməsi wtask'MultiIndicMT' adında 'NICT-5' adlı təkrarlayırıq. Bu işin 10 Indik dilindən İngilizce dilinə çevirilməsi və digər tərəfindən istifadə edir. Bu işin məqsədi, çoxlu dil tərzlərinin faydalanılığını keşfetmək idi, domena və dış domena paralel və monodil korporasını istifadə etmək idi. Çoxlu dil NMT öyrənməsinin son başarısına görə, bu işdə bütün dilləri örtüyən böyük bir monodil korpus koleksiyonundan MBART modelini keşfetməyə karar verdik, sonra da küçük domenə korporasında çoxlu dil təmizlənməsi üçün. İlk dəfə, biz gördük ki, küçük iki dil korporasının təhsil edilməsindən sonra küçük təhsil təhsil edilməsindən əvvəl təhsil edilmədiyi zaman böyük qənimətlər yetirə bilər. Daha sonra, çoxlu dil təmizlənməsi təmizlənməsi üçün daha çox qədər təmizlənməyə yol göstərir ki, əvvəlcə təmizlənməyən çoxlu dil təmizlənməsinə güclü bir çoxlu təmizlənməsi üçün daha çox güclü bir çoxlu təmizlənməsi.", 'bn': "এই কাগজটিতে আমরা মাল্টিভাষায় ভাষার ভাষার অনুবাদ উইকাজ 'মাল্টিভিন্ডিক এমটি' দলের নাম 'NICT-5' নামের অধীনে আমাদের প্রতি এই কাজে ১০ ভাষা থেকে ইংরেজি এবং ভাইস ভাষায় অনুবাদ করা হয়েছে। কাজের উদ্দেশ্য হচ্ছে বিভিন্ন ধরনের ডোমেইন এবং ডোমেইনের বাইরে প্যারালেল এবং মোনোলিভাল কোর্পোরা ব্যবহার করে বহুভাষার উপায়ের ব্যবহার ব্যবহ মাল্টিভাষার এনএমটি প্রশিক্ষণের সাম্প্রতিক সাফল্যের কারণে আমরা সিদ্ধান্ত নিয়েছিলাম একটি বিশাল মোনোলিভাল ভাষার সংগ্রহের উপর এক মডেল খুঁজে বের করতে যাচ্ছিলাম, যা এই ক প্রথমত, আমরা দেখেছি যে ছোট্ট দুই ভাষার কোর্পোরায় সুন্দর প্রশিক্ষণ পূর্ব প্রশিক্ষণের পরে দুই ভাষার প্রশিক্ষণের মাধ্যমে বিশাল অর্জনে এছাড়াও, অনুবাদের মানে অনুবাদের ক্ষেত্রে বহুভাষার ভাষার ভাষায় আরো অর্জনের ফলাফল প্রদান করে যা অনেক শক্তিশালী বহুভাষার বেসাইলাইনের বে", 'fi': "Tässä artikkelissa kuvailemme toimitustamme monikieliseen intialaiseen käännökseen wtask 'MultiIndicMT' tiimin nimellä 'NICT-5'. Tähän tehtävään kuuluu käännös kymmenestä indiankielisestä kielestä englanniksi ja päinvastoin. Tehtävän tavoitteena oli tutkia monikielisten lähestymistapojen hyödyllisyyttä käyttämällä erilaisia toimialueen sisäisiä ja ulkopuolisia rinnakkaisia ja yksikielisiä korpusia. Koska monikielinen NMT-esikoulutus on viime aikoina onnistunut, päätimme tutkia MBART-mallin esikoulutusta suuressa yksikielisessä korpuskokoelmassa, joka kattaa kaikki tämän tehtävän kielet, minkä jälkeen monikielinen hienosäätö pienissä domain-korpusissa. Ensinnäkin huomasimme, että pieni määrä esikoulutusta, jota seuraa hienosäätö pienillä kaksikielisillä korpusilla, voi tuottaa suuria hyötyjä verrattuna, kun esikoulutusta ei käytetä. Lisäksi monikielinen hienosäätö parantaa käännösten laatua entisestään, mikä on huomattavasti parempi kuin hyvin vahva monikielinen perusmalli, joka ei perustu mihinkään esikoulutukseen.", 'et': "Selles töös kirjeldame meie esitamist mitmekeelsele india keele tõlkele wtack 'MultiIndicMT' meeskonna nime all 'NICT-5'. See ülesanne hõlmab tõlkimist 10 india keelest inglise keelde ja vastupidi. Ülesande eesmärk oli uurida mitmekeelsete lähenemisviiside kasulikkust, kasutades erinevaid domeenisiseseid ja -väliseid paralleelseid ja ühekeelseid korpuseid. Arvestades mitmekeelse NMT eelkoolituse hiljutist edu, otsustasime uurida MBARTi mudelit suure ühekeelse korpusekogumi kohta, mis hõlmab kõiki selle ülesande keeli, millele järgneb mitmekeelne täpsustamine väikeste domeenisiseste korpuste puhul. Esiteks täheldasime, et väike hulk eelkoolitust, millele järgneb peenhäälestus väikestele kakskeelsetele korpustele, võib anda suurt kasu, kui eelkoolitust ei kasutata. Lisaks toob mitmekeelne täpsustamine kaasa tõlkekvaliteedi edasise paranemise, mis ületab märkimisväärselt väga tugeva mitmekeelse lähtetaseme, mis ei sõltu eelkoolitusest.", 'ca': "In this paper we describe our submission to the multilingual Indic language translation wtask 'MultiIndicMT' under the team name 'NICT-5'.  This task involves translation from 10 Indic languages into English and vice-versa.  The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora.  Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora.  Firstly, we observed that a small amount of pre-training followed by fine-tuning on small bilingual corpora can yield large gains over when pre-training is not used.  A més, l'ajustament multilingüe porta a més guanys en la qualitat de traducció que superen significativament una base multilingüe molt forta que no es basa en cap pré-entrenament.", 'jv': 'Nang pepul iki kita dadi nggawe ngubah kita bangkat endik kanggo tarjamahan wtask \'Multiindividual MT\' lan nganggo nambah \'NIC-5\' Wunggu iki lak kelompok tarjamah kanggo 10 nggambar barang kelas nang Inggris lan mulai. The goal of the task was to istrage the Utty of multilanguage accesses use a variant of in-domain and out-of-domain Paraleland and Monlanguage Body. Ngerti nggawe barang pengguna multi-language NMT prelimisar praksi kita gewis disindelah cara-cara nggawe unyangno sistem MBART model nang akeh sistem sistem dumadhingan dumadhakan kapan tanggal nggawe winih dhéwé multi-language Fin-tuning on small in-domain corora. Nalika iku, kita ngomongke tindang ngomong nek, akeh diunting bantuan ngono nggawe nguasai supra-ingkang supra-ingkang supra-ingkang ora bisa diandelak bantuan liyane, ora iso dianggap bantuan liyane, sedhaya bisa dianggap politenessoffpolite, politenessoffpolite"), and when there is a change ("assertive', 'ha': "Daga wannan takardan da Muke bayyana jujjamarmu zuwa fassarar harshen na mulki-lingui wjob 'multi-IndicMT' a ƙarƙashin the team suna 'NICT-5'... @ info Bayinsa na aikin ni ne dõmin su nẽmi amfani da wasu hanyõyi masu mulki-lingui da ke yi amfani da wasu daban-cikin-Domin da bã ya iya daidaita da wata mataimaki na sauri. Gida babban rabo na farko na NMT na mulki-lingui, sai muka zãɓe mu cire wani motsi na MWT a kan ƙanshi mai girma na rubutu da dukkan harshen na musamman da ke ƙaranci wannan aikin da aka biyar da multi-lingui-tuning na ƙarami cikin-Domin. Na farkon, ba mu gani ba da abu kaɗan na gabã-wa da tuntunkuɗe wa makampuni na ƙarami ta ƙara mafiya girma idan ba za ta yi amfani da mafunzo ba. Furan haka, tun masu sanyi na mulki-lugha na ƙara ga mafiya amfani da tsarin fassarar, da kuma yana ƙarfafa wani salon mai ƙarfi na multilala wanda ba ya dõgara ga wani na zaman shawara.", 'sk': "V tem prispevku opisujemo našo predložitev večjezičnemu prevodu v indijski jezik wtask 'MultiIndicMT' pod imenom ekipe 'NICT-5'. Ta naloga vključuje prevod iz 10 indijskih jezikov v angleščino in obratno. Cilj naloge je bil raziskati uporabnost večjezičnih pristopov z uporabo različnih vzporednih in enojezičnih korpusov znotraj in zunaj domene. Glede na nedavni uspeh večjezičnega predusposabljanja NMT smo se odločili, da raziščemo predusposabljanje MBART model na veliki enojezični zbirki korpusov, ki zajema vse jezike v tej nalogi, ki mu sledi večjezično fino nastavitev majhnih domenskih korpusov. Najprej smo opazili, da majhna količina predtreninga, ki mu sledi natančna nastavitev na majhnih dvojezičnih korpusih, lahko prinese velike dobičke v primerjavi s tem, ko predtreninga ni uporabljena. Poleg tega večjezično natančno prilagajanje povzroči nadaljnje izboljšanje kakovosti prevodov, ki znatno presega zelo močno večjezično osnovo, ki se ne zanaša na nobeno predusposabljanje.", 'bo': 'འུ་ཅག་གི་ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོའི་སྐྱེས་སྐད་རིགས་སྐད་ཀྱི་སྐད་རིགས་ནང་དུ་བཤད་པ་ཡིན། དངོས་ཡིག་ལས་སྐད་རིགས་༡༠་ནང་དུ་དབྱིན་ཡིག་གནང་བ་འདི་འགྲེལ་བཙུགས་ཡོད། The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. དང་པོ་མ་ཟད། ང་ཚོར་མཐོང་ནི་སྔོན་གྲངས་འབྱོར་གྱི་ལས་རྒུལ་ཆུང་ཉུང་བའི་སྔོན་གྲངས་སྒྲིག་འཛིན་པ་ཆུང་ཉུང་བའི་མཐུན་རྐྱེན་སྐྱེས་ ད་དུང་། སྐད་ཡིག་ཆ་ཕན་ཆེར་སྒྲིག་ཀྱིས་ཚོར་སྐྱེས་ཚད་གཞན་ལས་མཐུན་རྐྱེན་བཟོ་བྱེད་ཀྱི་ཡོད།', 'he': "In this paper we describe our submission to the multilingual Indic language translation wtask 'MultiIndicMT' under the team name 'NICT-5'.  המשימה הזו כוללת תרגום מ-10 שפות אינדיות לאנגלית והפך. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora.  בהתחשב בהצלחה האחרונה של אימון מראש של NMT רבות שפות החלטנו לחקור מודל מראש אימון MBART על אוסף גופוס מונושפתי גדול מכסה את כל השפות במשימה הזו, ואחרי כך התאמה רבת שפות על גופורה קטנה בתחום. Firstly, we observed that a small amount of pre-training followed by fine-tuning on small bilingual corpora can yield large gains over when pre-training is not used.  Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training."}
{'en': 'How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task GPU  in 100 hours?  C o AS ta L  at  M ulti I ndic MT  Shared Task', 'ar': 'إلى أي مدى يمكن أن نحقق مع GPU واحد في 100 ساعة؟ CoAStaL في MultiIndicMT Shared Task', 'es': '¿Hasta dónde podemos llegar con una GPU en 100 horas? Coastal en MultiIncmit - Tarea', 'pt': 'Até onde podemos chegar com uma GPU em 100 horas? CoAStaL na Tarefa Compartilhada MultiIndicMT', 'fr': "Jusqu'où pouvons-nous aller avec un GPU en 100 heures\xa0? Tâche partagée CoAStal et MultiIndicMT", 'ja': '100時間以内に1つのGPUでどこまで到達できますか？ MultiIndicMT共有タスクのCoAStaL', 'hi': 'हम 100 घंटे में एक GPU के साथ कितनी दूर जा सकते हैं? MultiIndicMT साझा कार्य में CoAStaL', 'zh': '一 GPU 在 100 时能几许? CoAStaL at MultiIndicMT Shared Task', 'ru': 'Как далеко мы можем зайти с одним графическим процессором за 100 часов? CoAStaL при мультииндикальной общей задаче MT', 'ga': 'Cé chomh fada agus is féidir linn a fháil le GPU amháin i 100 uair? CoAStaL ag Tasc Comhroinnte MultiIndicMT', 'ka': 'რამდენი გადავიტანოთ ერთი GPU 100 საათში? CoAStaL MultiIndicMT გაყოფილი დავალებაში', 'hu': 'Milyen messzire jutunk egy GPU-val 100 óra alatt? CoAStaL a MultiIndicaMT megosztott feladatnál', 'el': 'Πόσο μακριά μπορούμε να φτάσουμε με ένα GPU σε εκατό ώρες; CoAStaL στην κοινή εργασία MultiIndicMT', 'kk': '100 сағатта бір GPU-мен қанша қайта жеткізе аламыз? CoAStaL көпIndicMT ортақ тапсырмасында', 'it': 'Quanto lontano possiamo arrivare con una GPU in 100 ore? CoAStaL presso MultiIndicaMT Shared Task', 'lt': 'Kaip toli galime pasiekti su vienu GPU per 100 valandų? CoAStaL at MultiIndicMT Shared Task', 'mk': 'Колку далеку можеме да стигнеме со една GPU за 100 часа? CoAStaL на MultiIndicMT споделена задача', 'ml': '100 മണിക്കൂറിനുള്ളില്\u200d നമുക്കൊരു ജിപിയുവിന്\u200dറെ അടുത്ത് എത്ര ദൂരെ എത്താന്\u200d കഴിയും? CoAStaL at MultiIndicMT Shared Task', 'ms': 'Berapa jauh kita boleh mendapatkan dengan satu GPU dalam 100 jam? CoAStaL pada Tugas Berkongsi MultiIndicMT', 'mt': "Kemm nistgħu nimxu 'l bogħod b'GPU waħda f'100 siegħa? CoAStaL f’Kompitu Kondiviż MultiIndicMT", 'mn': '100 цагт нэг GPU-тай хэр хол болох вэ? CoAStaL MultiIndicMT хуваалтын ажил', 'pl': 'Jak daleko możemy zajść z jednym procesorem GPU w stu godzin? CoAStaL w MultiIndicMT Shared Task', 'no': 'Kor langt kan vi få med ein GPU i 100 timar? CoAStaL ved delt multiIndicMT- oppgåve', 'ro': 'Cât de departe putem ajunge cu un GPU în 100 de ore? CoAStaL la activitatea partajată MultiIndicaMT', 'so': 'ilaa intee ilaa 100 saacadood ayaannu ku heli karnaa GPU? CoAStaL at MultiIndicMT Shared Shaqada', 'si': 'පැය 100ක් පස්සේ අපි කොච්චර දුරටත් GPU එකක් එක්ක ගන්න පුළුවන්ද? MultiIndicMT සමාගත වැඩේ CoAStaL', 'sv': 'Hur långt kan vi komma med en GPU på 100 timmar? CoAStaL vid delad uppgift MultiIndicaMT', 'ta': '100 மணி நேரத்தில் ஒரு ஜிபியு கிடைக்கும் எவ்வளவு? MultiIndicMT பகிர்ந்த பணியில் CoAStaL', 'ur': 'ہم 100 گھنٹوں میں ایک جی پی یو کے ساتھ کتنی دور کر سکتے ہیں؟ MultiIndicMT Shared Task پر CoAStaL', 'sr': 'Koliko daleko možemo dobiti s jednom GPU za 100 sati? CoAStaL na multiIndicMT zajednièkom zadatku', 'vi': 'Chúng ta có thể đi được bao xa với một GPU trong 100 giờ? Comment=SềEđịa chềEcủa KDE Comment', 'uz': "100 soatda bir GPU bilan qancha zo'p kelib olamiz? MultiIndicMT Shared vazifalarda CoAStaL", 'nl': 'Hoe ver kunnen we komen met één GPU in 100 uur? CoAStaL bij MultiIndicMT Shared Task', 'bg': 'Колко далеч можем да стигнем с един GPU за 100 часа? CoAStaL в MultiIndicMT споделена задача', 'hr': 'Koliko daleko možemo dobiti s jednom GPU za 100 sati? CoAStaL na multiIndicMT zajedničkom zadatku', 'da': 'Hvor langt kan vi komme med én GPU på 100 timer? CoAStaL ved MultiIndicaMT Shared Task', 'de': 'Wie weit können wir mit einer GPU in 100 Stunden kommen? CoAStaL bei MultiIndicMT Shared Task', 'ko': '100시간 동안 GPU 한 개로 얼마나 멀리 갈 수 있습니까?해안 다중 지표 공유 임무', 'id': 'Seberapa jauh kita bisa mendapatkan dengan satu GPU dalam 100 jam? CoAStaL at MultiIndicMT Shared Task', 'fa': 'تا 100 ساعت ديگه چقدر ميتونيم با يه جي پي يو بريم؟ CoAStaL در کار مشترک multiIndicMT', 'sw': 'Mpaka sasa tunaweza kupata GPU moja ndani ya masaa 100? CoAStaL katika Hifadhi ya MultiIndicMT', 'sq': 'Sa larg mund të arrijmë me një GPU në 100 orë? CoAStaL në MultiIndicMT', 'af': 'Hoe ver kan ons met een GPU in 100 uur kry? CoAStaL by MultiIndicMT Gedeelde Opdrag', 'tr': '100 sagat içinde bir GPU bilen näçe uzaklaşdyryp bileris? MultiIndicMT Paýlaşmış Görevde CoAStaL', 'am': 'ከ100 ሰዓት ውስጥ አንድ GPU ጋር እንዴት እናገኛለን? MultiIndicMT Shared Task', 'hy': 'Ինչքա՞ն հեռու կարող ենք հասնել 100 ժամվա ընթացքում մեկ GPU-ով: Comment', 'az': '100 saat i√ßind…ô bir GPU il…ô n…ô q…ôd…ôr uzaqlaŇüabilirik? MultiIndicMT paylaŇüńĪlan iŇül…ôrd…ô CoAStaL', 'bn': 'How far can we get with one GPU in 100 hours?  MultiIndicMT শেয়ার করার কোয়াস্তাল', 'bs': 'Koliko daleko možemo dobiti s jednom GPU za 100 sati? CoAStaL na multiIndicMT zajedničkom zadatku', 'cs': 'Jak daleko se můžeme dostat s jedním GPU za sto hodin? CoAStaL na MultiIndicMT Shared Task', 'et': 'Kui kaugele me jõuame ühe GPU-ga 100 tunniga? CoAStaL at MultiIndicMT Shared Task', 'fi': 'Kuinka pitkälle pääsemme yhdellä näytönohjaimella sadassa tunnissa? CoAStaL at MultiIndicMT Shared Task', 'ca': 'A quin punt podem arribar amb un GPU en 100 hores? CoAStaL a MultiIndicMT Shared Task', 'jv': 'Pilih pirang maneh, dhewe mau ngerti bareng-toleh GNU saben lawang metu? CoASta L nang Multiendic MT Ngawe Mesang', 'ha': 'Yãya wuri zã mu iya sami GPU guda cikin masakiyar 100? KCharselect unicode block name', 'he': 'כמה רחוק נוכל להגיע עם GPU אחת תוך 100 שעות? CoAStaL at MultiIndicMT Shared Task', 'sk': 'Kako daleč lahko pridemo z enim GPU v 100 urah? CoAStaL v skupni nalogi MultiIndicMT', 'bo': 'འུ་ཚོས་GPU་གཅིག་ཆུ་ཚིག་༡༠༠་ནང་དུ་ག་རེ་བ་འབྲི་དགོས་སམ། CoAStaL འདི་སྣ་འབྲས་སྔོན་སྒྲིག་ཆ་རྐྱེན་གྱི་བྱ་འགུལ་གྱིས'}
{'en': 'This work shows that competitive  translation  results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single  GPU  for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and  metrics .', 'ar': 'يوضح هذا العمل أنه يمكن الحصول على نتائج الترجمة التنافسية في بيئة مقيدة من خلال دمج أحدث التطورات في الذاكرة وتحسين الحوسبة. نقوم بتدريب وتقييم نماذج الترجمة الكبيرة متعددة اللغات باستخدام وحدة معالجة رسومات واحدة لمدة أقصاها 100 ساعة والحصول على 4-5 نقاط BLEU من أعلى إرسال على لوحة المتصدرين. نحن أيضًا نقيس خطوط الأساس القياسية في مجموعة PMI ونعيد اكتشاف أوجه القصور المعروفة في أنظمة ومقاييس الترجمة.', 'fr': "Ce travail montre que des résultats de traduction compétitifs peuvent être obtenus dans un environnement restreint en intégrant les dernières avancées en matière de mémoire et d'optimisation du calcul. Nous formons et évaluons de grands modèles de traduction multilingues à l'aide d'un seul GPU pendant un maximum de 100 heures et obtenons entre 4 et 5 points BLEU de la meilleure soumission au classement. Nous comparons également les bases de référence standard sur le corpus PMI et redécouvrons les lacunes bien connues des systèmes de traduction et des métriques.", 'es': 'Este trabajo demuestra que se pueden obtener resultados de traducción competitivos en un entorno limitado mediante la incorporación de los últimos avances en optimización de memoria y computación. Entrenamos y evaluamos modelos de traducción multilingüe de gran tamaño utilizando una sola GPU durante un máximo de 100 horas y nos encontramos entre 4 y 5 puntos BLEU de la mejor presentación en la tabla de clasificación. También comparamos las líneas de base estándar en el corpus de PMI y redescubrimos las deficiencias conocidas de los sistemas y métricas de traducción.', 'pt': 'Este trabalho mostra que resultados competitivos de tradução podem ser obtidos em um ambiente restrito, incorporando os mais recentes avanços em otimização de memória e computação. Treinamos e avaliamos grandes modelos de tradução multilíngue usando uma única GPU por um máximo de 100 horas e alcançamos 4-5 pontos BLEU do envio superior na tabela de classificação. Também comparamos linhas de base padrão no corpus do PMI e redescobrimos deficiências conhecidas de sistemas e métricas de tradução.', 'ja': 'この研究は、メモリと計算最適化の最新の進歩を組み込むことによって、制約された環境で競争力のある翻訳結果を得ることができることを示しています。単一のGPUを使用して最大100時間大規模な多言語翻訳モデルをトレーニングおよび評価し、リーダーボード上位の提出物から4〜5 BLEUポイント以内に入手します。また、PMIコーパスの標準ベースラインをベンチマークし、翻訳システムと指標の既知の欠点を再発見します。', 'zh': '明以合内存计优化之最新进展,得竞争力于受限之设。 以单 GPU 训练及评估大言译模,最多 100 时,并于排行榜上得 4-5 BLEU 点内。 又准 PMI 语料库上基线,重见译者系统指标众所周知。', 'hi': 'इस काम से पता चलता है कि प्रतिस्पर्धी अनुवाद परिणाम स्मृति में नवीनतम प्रगति को शामिल करके एक विवश सेटिंग में प्राप्त किए जा सकते हैं और अनुकूलन की गणना करते हैं। हम अधिकतम 100 घंटों के लिए एक ही जीपीयू का उपयोग करके बड़े बहुभाषी अनुवाद मॉडल को प्रशिक्षित और मूल्यांकन करते हैं और लीडरबोर्ड पर शीर्ष सबमिशन के 4-5 BLEU बिंदुओं के भीतर आते हैं। हम पीएमआई कॉर्पस पर मानक आधार रेखाओं को भी बेंचमार्क करते हैं और अनुवाद प्रणालियों और मीट्रिक की अच्छी तरह से ज्ञात कमियों को फिर से खोजते हैं।', 'ru': 'Эта работа показывает, что конкурентоспособные результаты перевода могут быть получены в ограниченных условиях путем включения последних достижений в области памяти и оптимизации вычислений. Мы обучаем и оцениваем крупные многоязычные модели перевода с использованием одного графического процессора в течение максимум 100 часов и получаем в пределах 4-5 баллов BLEU от лучшего представления в таблице лидеров. Мы также сравниваем стандартные базовые линии на корпусе PMI и повторно обнаруживаем хорошо известные недостатки систем перевода и метрик.', 'ga': 'Léiríonn an saothar seo gur féidir torthaí iomaíocha aistriúcháin a fháil i suíomh srianta tríd an dul chun cinn is déanaí sa chuimhne agus barrfheabhsú ríomh a ionchorprú. Déanaimid oiliúint agus measúnú ar mhúnlaí móra aistriúcháin ilteangacha ag baint úsáide as GPU amháin ar feadh 100 uair an chloig ar a mhéad agus faigheann muid laistigh de 4-5 phointe BLEU ón aighneacht is fearr ar an gclár ceannairí. Déanaimid tagarmharcáil freisin ar bhunlínte caighdeánacha ar chorpas an PMI agus athaimsímid easnaimh aitheanta sna córais aistriúcháin agus sa mhéadracht.', 'ka': 'ეს სამუშაო აჩვენებს, რომ კონპექციენტიური გადაწყვეტილება შესაძლებელია დარწყვეტილი პარამეტრებში, რომელიც ახალი გადაწყვეტილება მეხსიერებში და გამომუშაოთა. ჩვენ უფრო დიდი მრავალენგური თავისუფლების მოდელების გავამწავლობთ და გავამწავლობთ 100 საათში ერთი GPU გამოყენებული და მივიღეთ 4-5 BLEU წერტილებში თავისუფრო თავისუფრო თავისუფ ჩვენ ასევე კომპოსს PMI კორპუსში სტანდარტატური ბაზი ხაზები დავიწყებთ და გავიღებთ ძალიან უცნობილი პროცემები სისტემის და მეტრიკის განსხვავება.', 'el': 'Η εργασία αυτή δείχνει ότι τα ανταγωνιστικά αποτελέσματα μετάφρασης μπορούν να επιτευχθούν σε περιορισμένο περιβάλλον ενσωματώνοντας τις τελευταίες εξελίξεις στη βελτιστοποίηση μνήμης και υπολογισμού. Εκπαιδεύουμε και αξιολογούμε μεγάλα πολύγλωσσα μοντέλα μετάφρασης χρησιμοποιώντας ένα μόνο για το μέγιστο 100 ώρες και αποκτούμε εντός 4-5 πόντους από την κορυφαία υποβολή στον πίνακα κατάταξης. Επίσης, αξιολογούμε τις τυποποιημένες γραμμές βάσης στο σώμα και ανακαλύπτουμε εκ νέου γνωστές ελλείψεις των μεταφραστικών συστημάτων και των μετρήσεων.', 'it': "Questo lavoro mostra che i risultati di traduzione competitivi possono essere ottenuti in un'impostazione limitata incorporando gli ultimi progressi nell'ottimizzazione della memoria e del calcolo. Formiamo e valutiamo modelli di traduzione multilingue di grandi dimensioni utilizzando una singola GPU per un massimo di 100 ore e otteniamo entro 4-5 punti BLEU dalla migliore presentazione nella classifica. Analizziamo anche le linee di base standard sul corpus PMI e rileviamo carenze ben note dei sistemi di traduzione e delle metriche.", 'hu': 'Ez a munka azt mutatja, hogy a versenyképes fordítási eredmények korlátozott beállításokban érhetők el a memória és a számítási optimalizálás legújabb fejlesztéseinek beépítésével. Nagy többnyelvű fordítási modelleket képzünk és értékelünk egyetlen GPU segítségével maximum 100 órán keresztül, és 4-5 BLEU ponton belül jutunk el a ranglistán lévő leadásoktól. Ezenkívül összehasonlítjuk a PMI korpusz standard alapvető értékeit, és újra felfedezzük a fordítási rendszerek és mutatók jól ismert hiányosságait.', 'kk': 'Бұл жұмыс жадындағы ең жаңа жағдайды жаттырып, оптимизациялауды есептеп тұрып, тұжырымды аудару нәтижесін шектелген параметрлерде алуға болады дегенді көрсетеді. Біз 100 сағат көп GPU арқылы үлкен көп тілді аудару үлгілерін оқу және оқу үшін көп тілді аудару үлгілерін оқу және бағыттау батырмасының жоғарғы жұмысын 4-5 BLEU нү Мұндай-ақ біз PMI корпус бойынша стандартты негізгі сызықтарды таңдап, аудармалар жүйелері мен метрикалардың белгілі жеткіліктерін қайта табуыз.', 'lt': 'Šis darbas rodo, kad konkurencingi vertimo rezultatai gali būti pasiekti ribotos sąlygos, įtraukiant naujausią pažangą atminties ir skaičiavimo optimizavimo srityje. Mes mokome ir vertiname didelius daugiakalbius vertimo modelius, naudojančius vieną GPU ne ilgiau kaip 100 valandų, ir pasiekiame 4–5 BLEU taškus nuo pirmosios lentelės pateikimo. Mes taip pat lyginame standartines bazines PMI korpus gaires ir vėl nustatome gerai žinomus vertimo sistemų ir metrinių duomenų trūkumus.', 'ms': 'This work shows that competitive translation results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization.  Kami melatih dan menilai model terjemahan berbilang bahasa besar menggunakan GPU tunggal selama maksimum 100 jam dan mendapatkan dalam 4-5 titik BLEU dari penghantaran atas papan pemimpin. Kami juga benchmark garis dasar piawai pada korpus PMI dan mengesan semula kekurangan yang diketahui dalam sistem terjemahan dan metrik.', 'mk': 'Оваа работа покажува дека конкурентните резултати на преводот може да се добијат во ограничено поставување со вклучување на последните напредоци во меморијата и компјутерската оптимизација. Ние тренираме и оценуваме големи мултијазични модели на превод користејќи една GPU за максимално 100 часа и влегуваме во 4-5 БЛЕУ точки од највисоката поднесувачка на водечката табла. Ние, исто така, ги проценуваме стандардните бази на ПМИ корпусот и повторно откриваме познати недостатоци во преводните системи и метриката.', 'ml': 'ഈ ജോലി കാണിക്കുന്നത് മെമ്മറിയില്\u200d ഏറ്റവും അടുത്ത പുരോഗങ്ങള്\u200d ചേര്\u200dക്കുന്നതിനാല്\u200d മത്സരിക്കുന്ന വിവരങ്ങളുടെ ഫലങ്ങള്\u200d നിര്\u200dബന്ധ നമ്മള്\u200d ഒരു ജിപിയു ഉപയോഗിച്ച് ഒരു മണിക്കൂര്\u200d ഉപയോഗിച്ച് വലിയ പല്ലിഭാഷയുടെ പരിശീലനം പരിശീലിക്കുകയും വിലയിക്കുകയും ചെയ്യുന്നു. നേതാവി പിമിഐ കോര്\u200dപ്പുസിന്\u200dറെ ബെന്\u200dച്മാര്\u200dക്ക് സ്റ്റാര്\u200dഡ് ബെസ്റ്റര്\u200dമാര്\u200dക്ക് ബെസ്റ്റാര്\u200dക്ക് ലൈനുകളും പരിഭാഷപ്രക്', 'mt': 'This work shows that competitive translation results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization.  Aħna nħarrġu u jevalwaw mudelli kbar ta’ traduzzjoni multilingwi bl-użu ta’ GPU uniku għal massimu ta’ 100 siegħa u niksbu fi ħdan 4-5 punti BLEU mill-ogħla sottomissjoni fuq it-tabella ewlenija. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and metrics.', 'pl': 'Praca ta pokazuje, że konkurencyjne wyniki tłumaczeń można uzyskać w ograniczonych warunkach poprzez uwzględnienie najnowszych osiągnięć w optymalizacji pamięci i obliczeń. Szkolimy i oceniamy duże wielojęzyczne modele tłumaczeń przy użyciu jednego procesora graficznego przez maksymalnie 100 godzin i uzyskujemy w ciągu 4-5 punkty BLEU najwyższej klasyfikacji. Porównujemy również standardowe linie bazowe na korpusie PMI i ponownie odkrywamy dobrze znane niedociągnięcia systemów tłumaczeniowych i wskaźników.', 'mn': 'Энэ ажлын үр дүн нь тогтвортой байдлын үр дүн гаргаж болох боломжтой байдлыг ойлгож, санамжлалтын шинэ хөгжлийг нэмсээр тооцоолох боломжтой болгодог гэдгийг харуулж байна. Бид 100 цагт нэг GPU-г ашиглан олон хэлний орчуулалтын загварыг суралцаж, үнэлгээ үзүүлж, удирдагч буудалд 4-5 BLEU цэгүүдийн хамгийн өндөр дамжуулалтын цэгүүдийг авч ирнэ. Мөн бид ПMI корпус дээрх стандарт суурь шугам багтаж, хөрөнгө оруулах систем болон метрикийн алдагдлыг дахин нээлттэй олж мэддэг.', 'no': 'Dette arbeidet viser at konkurentære omsetjingslingsresultatet kan hentast i ein begrenset innstilling ved å inkludere dei siste avanserte i minne og rekna ut optimalisering. Vi treng og evaluerer store fleirspråksomsetjingsmodular med ein enkel GPU for maksimum av 100 timar og får inn i 4-5 BLEU-punkt av den øvste submissisjonen på lederbordet. Vi finn også standardsbaselinjer på PMI-korpusen og oppdag berre kjente korta for omsetjingssystemet og metrikk.', 'ro': 'Această lucrare arată că rezultatele traducerii competitive pot fi obținute într-o setare restricționată prin încorporarea celor mai recente progrese în materie de memorie și optimizare a calculului. Antrenăm și evaluăm modele mari de traducere multilingvă folosind o singură GPU timp de maxim 100 de ore și obținem în termen de 4-5 puncte BLEU de trimiterea de top pe clasament. De asemenea, analizăm standardele de referință ale corpului PMI și redescoperim deficiențele bine cunoscute ale sistemelor de traducere și metrice.', 'si': 'මේ වැඩේ පෙන්වන්නේ තර්ජනය වාර්ථාවක් පත්තිය ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප අපි ගොඩක් භාෂාවික වාර්තාවක් අනුවර්තනය කරනවා පැය 100 ක් විතරයි GPU එකක් භාවිත කරනවා ඒ වගේම ලොකු බ්ලූස් පැත්තු 4-5 ඇතුලට ල අපි PMI කෝර්පුස් එක්ක ප්\u200dරමාණ ප්\u200dරමාණ ප්\u200dරමාණ ප්\u200dරමාණ ප්\u200dරමාණය සහ ආපහු හොයාගන්න පුළුවන් විදිහට පරිවා', 'sr': 'Ovaj rad pokazuje da su rezultati prevoda konkurentnih mogućnosti dobiti u ograničenom stanju uključujući najnoviji napredak u pamćenje i računati optimizaciju. Vježbamo i procjenjujemo velike multijezičke modele prevoda koristeći jednog GPU za maksimalno 100 sati i dobijemo unutar 4-5 BLEU bodova najvišeg podataka na vodećoj ploči. Takoðe smo standardne osnovne linije na PMI korpusu i ponovo otkrili poznate nedostatke prevodnih sustava i metrika.', 'so': 'Shaqadaasu waxay muuqataa in arimaha turjumaadda ee iskutallaabta ah lagu heli karo qoraal adag oo lagu qorayo horumarinta ugu dambeeya xusuusta iyo xisaabitaanka. Tusaalada turjumaadda luuqadaha badan waxaynu ku tababarannaa oo qiimeynaynaa ugu badnaan 100 saacadood oo kaliya GPU kadibna waxaynu heli karnaa 4-5 barxadaha ugu sarreeya looga soo diro hogaamiyada. Sidoo kale waxaynu ku qornaa qoraal-bannaan oo ku qoran PMI korpus, oo waxaynu dib u ogaanaynaa dhamaanka aad u yaqaan nidaamka turjumaadda iyo metricyada.', 'ta': 'இந்த வேலை காட்டுகிறது நினைவகத்தில் புரிந்த மொழிபெயர்ப்பு முடிவுகளை நினைவகத்தில் சேர்த்து மற்றும் கணிப்பு முன்னேற நாங்கள் ஒரு ஜிபியு ஒரு சிறிய 100 மணி நேரத்தை பயன்படுத்தி பெரிய பல மொழிமாற்ற மாதிரிகளை பயிற்சி மதிப்பிடுகிறோம் மற்றும் தலைப்புப்பலகைய நாங்கள் பிமிஐ கார்ப்ஸ் மீது நிலையான அடிப்படையாள கோடுகள் மற்றும் மொழிபெயர்ப்பு அமைப்புகள் மற்றும் மீண்டும் நன்றாக', 'sv': 'Detta arbete visar att konkurrenskraftiga översättningsresultat kan erhållas i en begränsad inställning genom att införliva de senaste framstegen inom minne och beräkningsoptimering. Vi tränar och utvärderar stora flerspråkiga översättningsmodeller med en enda GPU i maximalt 100 timmar och kommer inom 4-5 BLEU-poäng från toppinlämningen på topplistan. Vi benchmarkar även standardreferensvärden på PMI-korpusen och upptäcker välkända brister i översättningssystem och mätvärden.', 'ur': 'یہ کام دکھاتا ہے کہ مسابقه ترجمہ نتیجے ایک محدود تنظیم تنظیم میں حاصل کئے جاتے ہیں، ذکر میں نہایت اچھی پیشرفت اور اچھی کامپیوتر کا شمار کرتا ہے. ہم ایک GPU کے مطابق 100 گھنٹے کے لئے بڑے متعدد زبان کی ترجمہ موڈل کو ترکین اور ارزش کرتے ہیں اور 4-5 BLEU پوینٹوں میں لیڈربورڈ پر سب سے زیادہ تحویل کے ذریعہ حاصل کرتے ہیں. ہم نے بھی پمیئ کی کورپوس پر استاندارڈ بنچم لینٹ بنا دی ہے اور پیغمبر کی سیستم اور متریک کی بہت معلوم ناکامیابی کو دوبارہ آگاہ کریں۔', 'uz': "Name Biz bir nechta 100 soatdan foydalanish uchun katta tillar tarjima modellarini o'rganamiz va qiymatimiz va eng yuqori sarlavhasidagi 4-5 BLEU nuqta ichida olib kelamiz. Biz PMI corpusida benchmark standard baseline va tarjima tizim va metrik tizimlarining aniqlangan qisqartmalarni qayta aniqlashimiz mumkin.", 'vi': 'Công trình này cho thấy kết quả dịch kinh doanh có thể đạt được trong giới hạn bằng cách nhập vào những bước tiến mới nhất trong trí nhớ và khả năng sử dụng máy tính. Chúng tôi đào tạo và đánh giá các mô hình dịch lớn đa dạng bằng một GPU đơn vị trong tối đa giờ hàng trăm và nhận được trong 4-5 nguyên tắc của bảng xếp hạng trên cùng. Chúng tôi cũng tiêu điểm những đường hầm tiêu chuẩn trên cơ thể PMi và tái khám phá những thiếu sót nổi của hệ thống dịch chuyển và đo lường.', 'bg': 'Тази работа показва, че конкурентни резултати от превода могат да бъдат получени в ограничена обстановка чрез включване на най-новите постижения в оптимизацията на паметта и изчисленията. Ние тренираме и оценяваме големи многоезични преводни модели с помощта на един GPU за максимум 100 часа и получаваме в рамките на 4-5 точки от най-високото представяне в класацията. Също така сравняваме стандартните базови линии на корпуса и преоткриваме добре познатите недостатъци на системите и показателите за превод.', 'da': 'Dette arbejde viser, at konkurrencedygtige oversættelsesresultater kan opnås i en begrænset indstilling ved at indarbejde de seneste fremskridt inden for hukommelse og beregningsoptimering. Vi træner og evaluerer store flersprogede oversættelsesmodeller ved hjælp af en enkelt GPU i maksimalt 100 timer og kommer inden for 4-5 BLEU point fra den øverste indsendelse på ranglisten. Vi sammenligner også standardbasislinjer på PMI-korpustet og genopdager velkendte mangler i oversættelsessystemer og -målinger.', 'hr': 'Ovaj rad pokazuje da se rezultati natjecanja prevoda mogu dobiti u ograničenom stanju uključujući najnoviji napredak pamćenja i računati optimizaciju. Vježbamo i procjenjujemo velike multijezičke modele prevoda koristeći jednog GPU za maksimalno 100 sati i dobijemo unutar 4-5 BLEU bodova najvišeg podataka na vodećoj ploči. Također smo standardne osnovne linije na PMI korpusu i ponovno otkrili poznate nedostatke prevodnih sustava i metrika.', 'nl': 'Dit werk toont aan dat concurrerende vertaalresultaten kunnen worden verkregen in een beperkte omgeving door de nieuwste ontwikkelingen in geheugen- en rekenoptimalisatie op te nemen. We trainen en evalueren grote meertalige vertaalmodellen met één GPU voor maximaal 100 uur en krijgen binnen 4-5 BLEU punten van de hoogste inzending op het klassement. Ook benchmarken we standaardbaselines op het PMI-corpus en ontdekken we bekende tekortkomingen van vertaalsystemen en metrics.', 'de': 'Diese Arbeit zeigt, dass wettbewerbsfähige Übersetzungsergebnisse unter eingeschränkten Bedingungen erzielt werden können, indem die neuesten Fortschritte in der Speicher- und Rechenoptimierung berücksichtigt werden. Wir trainieren und bewerten große mehrsprachige Übersetzungsmodelle mit einer einzigen GPU für maximal 100 Stunden und erhalten innerhalb von 4-5 BLEU-Punkten die Top-Einreichung in der Bestenliste. Wir benchmarken auch Standardbaselines auf dem PMI-Korpus und entdecken bekannte Mängel von Übersetzungssystemen und Metriken.', 'fa': 'این کار نشان می\u200cدهد که نتایج ترجمه رقابتی در یک تنظیمات محدودیت می\u200cتواند با شامل آخرین پیشرفت\u200cها در حافظه و حساب optimization را دریافت کند. ما با استفاده از یک جی پی یو برای حداکثر ۱۰۰ ساعت آموزش می\u200cکنیم و ارزیابی می\u200cکنیم مدل\u200cهای ترجمه\u200cهای زیادی زبان\u200cها را با استفاده از یک جی پی یو و در حداکثر ۱۰۰ ساعت می\u200cگیریم و در حداکثر ۴ ما همچنین خطوط پایین\u200cهای استاندارد را روی کورپوس PMI نشان می\u200cدهیم و ناکامی\u200cهای شناخته شده\u200cای از سیستم\u200cهای ترجمه و متریک را دوباره کشف می\u200cکنیم.', 'id': 'Kerja ini menunjukkan bahwa hasil terjemahan kompetitif dapat diperoleh dalam pengaturan terbatas dengan memasukkan kemajuan terbaru dalam memori dan pengoptimisasi komputer. Kami melatih dan mengevaluasi model terjemahan multibahasa besar menggunakan GPU tunggal selama maksimum 100 jam dan mendapatkan dalam 4-5 titik BLEU dari pengiriman atas papan pemimpin. Kami juga benchmark standar dasar pada corpus PMI dan menemukan kembali kekurangan yang dikenal dari sistem terjemahan dan metrik.', 'ko': '이 작업은 메모리와 계산 최적화 분야의 최신 진전을 결합시켜 제한된 환경에서 경쟁력 있는 번역 결과를 얻을 수 있음을 나타낸다.GPU 1개를 사용하여 최대 100시간 동안 대형 다국어 번역 모델을 교육하고 평가하며 차트에서 BLEU 점수 4~5개를 획득했습니다.우리는 또한 PMI 자료 라이브러리의 표준 기선에 대해 기준 테스트를 실시했고 번역 시스템과 지표의 모두가 알고 있는 결함을 재발견했다.', 'sw': 'Kazi hii inaonyesha kwamba matokeo ya tafsiri yenye ushindani yanaweza kupatikana katika kitengo cha kulazimika kwa kuingiza maendeleo ya hivi karibuni katika kumbukumbu na kuhesabu matumaini. Tunafunza na kutathmini mifano makubwa ya tafsiri ya lugha kwa kutumia GPU moja kwa kiasi kikubwa cha masaa 100 na kufikia ndani ya pointi 4-5 BLEU ya ujumbe wa juu kwenye kiongozi. Pia tunaweka misingi ya msingi wa bendera kwenye viungo vya PMI na kutambua upya ukosefu unaofahamika vizuri wa mfumo wa kutafsiri na mitindo.', 'tr': 'Bu işe täsirli terjime netijesi yada hatda iň soňky ösümlikleri we optimizasiýany dahyl edip, çykylýan çykyş düzümlerde bolup biljekdigini görkez. 100 sagat iň köp GPU ullanýan we uly dil terjime modellerini çykýan we çykýan we 4-5 BLEU zolakynda iň üst basyş noktalary bar. Biz hem PMI köpüsünde standart temel hatlary çykarp, terjime sistemlerinin we metriklerinin bilinen iğnelerini tekrar keşfetdik.', 'af': "Hierdie werk vertoon dat mededingste vertaling resultate in 'n beperkte instelling kan ontvang word deur die nuutste vordering in geheue en bereken optimalisasie te inkorporeer. Ons trein en evalueer groot multilinguele vertalingsmodele met 'n enkele GPU vir 'n maksimum van 100 uur en kry binne 4-5 BLES punte van die bo-onderskyning op die lederboard. Ons ook benchmark standaard basisline op die PMI corpus en herontdek goed bekende kortpad van vertalingsstelsels en metries.", 'sq': 'Ky punë tregon se rezultatet konkurruese të përkthimit mund të arrihen në një rregullim të kufizuar duke përfshirë përparimet e fundit në kujtesë dhe optimizimin e kompjuterit. Ne trajnojmë dhe vlerësojmë modele të mëdha përkthimi shumëgjuhës duke përdorur një GPU të vetme për një maksimum prej 100 orësh dhe arrijmë brenda 4-5 pikave BLEU nga paraqitja më e lartë në bord. Ne gjithashtu përcaktojmë linjat bazë standarte për korpusin PMI dhe rishkruajmë mungesa të njohura të sistemeve të përkthimit dhe metrikëve.', 'bn': 'এই কাজ দেখাচ্ছে যে প্রতিযোগিতা অনুবাদের ফলাফল স্মৃতির সাম্প্রতিক উন্নয়ন এবং সম্পূর্ণ অপারেশনের মধ্যে যোগাযোগ করতে পারে। আমরা একটি জিপিউ ব্যবহার করে বিশাল মাল্টিভাল ভাষায় অনুবাদ মডেল প্রশিক্ষণ ও মূল্যায়ন করি এবং নেতৃবোর্ডে সর্বোচ্চ প্রদানের ৪-৫ বিলিউ বিনি এছাড়াও আমরা পিএমআই কোর্পাসে বেনম্যার্ক স্ট্যান্ডার্ডলাইন এবং অনুবাদ সিস্টেম এবং মেট্রিকের ভালো পরিচিত কমানো আবিষ্', 'am': 'ይሄ ሥራ የሚታወቀው ትርጉም ፍሬዎችን በመታሰቢያ እና በቁጥጥር ማሳሰቢያ በመጠቀም በሚችል እርግጠኛ ማግኘት ይችላል፡፡ ብዙ ቋንቋዎች ትርጉም ሞዴላዎችን እናስተምረዋለን፡፡ We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and metrics.', 'az': 'Bu işin müəllif tərcümə sonuçlarını müəyyən edilmiş bir qurğuda istifadə edə biləcəyini göstərir, yaddaşların son avanslarını birləşdirən və optimizasyonu hesablayaraq. Biz 100 saat boyunca bir GPU istifadə edərək çox dilli çevirim modellərini təhsil edir və 4-5 BLEU nöqtələrində liderboarda ən yüksək təhsil edilir. Biz həmçinin PMI korpusu ilə standart səhifələri təkrarlayır və təkrarlama sistemlərinin və metriklərin bilinmiş təkrarlarını yenidən keşf edirik.', 'hy': "Այս աշխատանքը ցույց է տալիս, որ մրցակցության թարգմանման արդյունքները կարող են ստանալ սահմանափակ միջոցով' ներառելով հիշողության վերջին զարգացումները և հաշվարկելով լավագույնը: Մենք վարժեցնում և գնահատում ենք բազմալեզվով թարգմանման մեծ մոդելներ՝ օգտագործելով միակ GPU-ը մեծամասնությամբ 100 ժամ, և ստանում ենք 4-5 բլեւ կետի ընթացքում առաջնորդության տախտակի գլխավոր ներկայացումից: Մենք նաև համեմատում ենք ՊՄԻ կորպուսի ստանդարտ հիմքերը և վերաբացահայտում ենք հայտնի թարգմանման համակարգերի և մետրիկայի բացակայությունները:", 'bs': 'Ovaj rad pokazuje da se rezultati konkurentnog prevoda mogu dobiti u ograničenom stanju uključujući najnoviji napredak u pamćenje i računati optimizaciju. Vježbamo i procjenjujemo velike multijezičke modele prevoda koristeći jednog GPU za maksimalno 100 sati i dobijemo unutar 4-5 BLEU bodova najvišeg podataka na vodećoj ploči. Također smo standardne bazne linije na PMI korpusu i ponovo otkrili poznate nedostatke prevodnih sustava i metrika.', 'ca': "Aquesta feina demostra que els resultats de traducció competitius es poden obtenir en un entorn restringit incorporant els últims avanços en la memòria i l'optimització computacional. Ensenyem i evaluem grans models de traducció multillengües fent servir un GPU únic durant un máxim de 100 hores i arribem entre 4 i 5 punts BLEU després de la presentació superior a la taula principal. També comparam les línies de base estándar del corpus de la IMP i redescobrim les deficiències conegudes dels sistemes de traducció i les mètriques.", 'et': 'See töö näitab, et konkurentsivõimelised tõlketulemused on võimalik saavutada piiratud tingimustes, kaasates uusimad edusammud mälu ja arvutuse optimeerimisel. Koolitame ja hindame suuri mitmekeelseid tõlkemudeleid ühe GPU abil maksimaalselt 100 tunni jooksul ning saame tulemuste tabelis 4-5 BLEU punkti. Samuti võrdleme PMI korpuse standardseid aluseid ning avastame uuesti tuntud puudusi tõlkesüsteemides ja mõõdikutes.', 'cs': 'Tato práce ukazuje, že konkurenční výsledky překladu lze získat v omezeném prostředí začleněním nejnovějších pokroků v oblasti optimalizace paměti a výpočtu. Školíme a vyhodnocujeme velké vícejazyčné překladové modely pomocí jednoho GPU maximálně sto hodin a získáváme do 4-5 BLEU bodů nejlepší podání na žebříčku. Rovněž porovnáváme standardní základní linie na korpusu PMI a znovu objevujeme známé nedostatky překladových systémů a metrik.', 'fi': 'Tämä työ osoittaa, että kilpailukykyisiä käännöstuloksia voidaan saada rajoitetussa ympäristössä yhdistämällä muistin ja laskennan optimoinnin viimeisimmät edistysaskeleet. Harjoittelemme ja arvioimme suuria monikielisiä käännösmalleja yhdellä näytönohjaimella enintään 100 tunnin ajan ja saamme 4-5 BLEU-pisteen sisällä tulostaulukon korkeimmasta toimituksesta. Vertailemme myös PMI-korpusen standardilähtökohtia ja selvitämme uudelleen käännösjärjestelmien ja mittareiden tunnettuja puutteita.', 'jv': '@title: window Awak dhéwé nglanggar lan kuwi nggawe model sing itlanjut luwih luwih dumateng liya sing sampeyan NGU kuwi nggawe balah kanggo utéh 10 oras lan tek gawe tanggal 4-5 B luwih sing bakal terus tambah sing paling dhéwé. Awak dhéwé éntuk bendhèwèké awak dhéwé éntuk basa sing nggawe barang nggawe barang nggawe barang tarjamahan karo Metiks.', 'sk': 'To delo kaže, da je mogoče konkurenčne rezultate prevajanja doseči v omejenih okoliščinah z vključitvijo najnovejših napredkov v optimizaciji pomnilnika in računalniškega sistema. Velike večjezične prevajalske modele usposabljamo in ocenjujemo z enim grafičnim upravljalnikom največ 100 ur in pridobimo 4-5 točk BLEU od najboljše predložitve na lestvici lestvic. Prav tako primerjamo standardne osnovne linije PMI korpusa in ponovno odkrivamo dobro znane pomanjkljivosti prevajalskih sistemov in meritev.', 'he': 'העבודה הזאת מראה שתוצאות התרגום תחרותיות יכולות להשיג בסביבה מוגבלת על ידי שילוב את התקדמות האחרונות בזיכרון ולחשבון אופטימיזציה. We train and evaluate large multilingual translation models using a single GPU for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard.  אנחנו גם מנתחים קווי בסיס סטנדרטיים על הקורפוס PMI ולגלות מחדש חסרות ידועות היטב של מערכות תרגום ומטריקה.', 'ha': "This work shows that competitive translation results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization.  Tuna ƙidãya da evalutar misãlai masu yawa na fassarar multilala, ko kuma muna amfani da wata GPU guda zuwa a tsakanin 100 sa'a kuma ka kai guda a shekara 4-5 na BLEU na ƙarƙashin da ke saman da aka saka kan shaiden. Tuna sami da bangon-bangon basketa a kan komai na Prime I kuma Mu re-gane shortcuts-known na'urar fassarar da metrics.", 'bo': 'This work shows that competitive translation results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. ང་ཚོས་དབྱིབས་སྡུད་དང་སྐད་ཡིག We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and metrics.'}
{'en': 'Language Relatedness and Lexical Closeness can help Improve Multilingual NMT : IITBombay@MultiIndicNMT WAT2021 NMT :  IITB ombay@ M ulti I ndic NMT   WAT 2021', 'es': 'La relación lingüística y la cercanía léxica pueden ayudar a mejorar la NMT multilingüe: IITBombay @MultiIndicNMT WAT2021', 'pt': 'A relação linguística e a proximidade léxica podem ajudar a melhorar o NMT multilíngue: IITBombay@MultiIndicNMT WAT2021', 'fr': 'La parenté linguistique et la proximité lexicale peuvent aider à améliorer la NMT multilingue\xa0: IITBombay @MultiIndicNMT WAT2021', 'ar': 'يمكن أن يساعد ارتباط اللغة والقرب المعجمي في تحسين NMT متعدد اللغات: IITBombay @ MultiIndicNMT WAT2021', 'ru': 'Языковая родство и лексическая близость могут помочь улучшить многоязычие NMT: IITBombay@MultiIndicNMT WAT2021', 'ja': '言語の関連性と語彙の近さは、多言語NMTの改善に役立ちます： IITBombay @ MultiIndicNMT WAT 2021', 'hi': 'भाषा संबंधितता और लेक्सिकल निकटता बहुभाषी एनएमटी को बेहतर बनाने में मदद कर सकती है: IITBombay@MultiIndicNMT WAT2021', 'zh': '语相关性与词汇近性可以助善多言NMT:IITBombay@MultiIndicNMT WAT2021', 'ga': 'Is féidir le Gaolmhaireacht Teanga agus Géire Foclaíochta cabhrú le NMT Ilteangach a Fheabhsú: IITBombay@MultiIndicNMT WAT2021', 'ka': 'ენის შესახებ და ლექსიკური დახმარება შესაძლებელია მრავალენგური NMT- ის შესახებ: IITBombay@MultiIndicNMT WAT2021', 'hu': 'A nyelvi kapcsolat és a lexikai közelség segíthet a többnyelvű NMT javításában: IITBombay@MultiIndicNMT WAT201', 'el': 'Η γλωσσική σχέση και η λεξική εγγύτητα μπορούν να βοηθήσουν στη βελτίωση της πολυγλωσσίας: IITBombay@MultiIndicNMT WAT2028', 'it': "La relazione linguistica e la vicinanza lessicale possono aiutare a migliorare l'NMT multilingue: IITBombay@MultiIndicNMT WAT201", 'kk': 'Тілдердің қатынасы және лексикалық қатынасы көмектесуі көмектеседі: Multilingual NMT IITBombay@MultiIndicNMT WAT2021', 'mk': 'Јазичноста и лексикалната близина може да помогнат во подобрувањето на мултијазичниот НМТ: IITBombay@MultiIndicNMT - WAT2021', 'lt': 'Kalbos ryšys ir leksinis glaudesnis ryšys gali padėti pagerinti daugiakalbį NMT: IITBombay@MultiIndicNMT - WAT2021', 'ms': 'Hubungan Bahasa dan Kedekatan Leksikal boleh membantu memperbaiki NMT Berbahasa: IITBombay@MultiIndicNMT WAT2021', 'ml': 'ഭാഷ ബന്ധപ്പെടുത്തുന്നതും ലെക്സിക്കല്\u200d അടുക്കുന്നതും കൂടുതല്\u200d ഭാഷ NMT മുന്\u200dകൂട്ടുക IITBombay@MultiIndicNMT WAT2021', 'mt': 'Ir-Relazzjoni mal-Lingwi u l-qrubija Lessika jistgħu jgħinu fit-Titjib tal-NMT Multilingwi: IITBombay@MultiIndicNMT WAT2021', 'mn': 'Холбооны хамаарал, лексикийн хамаарал олон хэлний NMT-г сайжруулж чадна: IITBombay@MultiIndicNMT WAT2021', 'no': 'Språkverdighet og leksisk klokkeslett kan hjelpa til å forbedra fleirspråk NMT: IITBombay@MultiIndicNMT WAT2021', 'pl': 'Związek językowy i bliskość leksykalna mogą pomóc w poprawie wielojęzycznego NMT: IITBombay@MultiIndicNMT WAT2021', 'ro': 'Relația lingvistică și apropierea lexică pot ajuta la îmbunătățirea NMT multilingvă: IITBombay@MultiIndicNMT WAT201', 'sr': 'Relativnost jezika i leksička zatvaranja mogu pomoći unaprjeđivanju multijezičkih NMT-a: IITBombay@MultiIndicNMT WAT2021', 'so': 'La xiriirka luqada iyo xiriirka Leksikal waxay caawimaad u yeelan karaan horumarinta luqadaha badan ee NMT: IITBombay@MultiIndicNMT WAT2021', 'si': 'භාෂාව සම්බන්ධතාවය සහ ලෙක්සිකාල් ක්\u200dලෝසින්ස් සම්බන්ධතාවය පුළුවන් විශේෂ භාෂාවක IITBombay@MultiIndicNMT WAT2021', 'ta': 'மொழி தொடர்புகள் மற்றும் லெக்சிக்கல் மூடுதல் NMT மேம்படுத்தலை உதவ முடியும்: IITBombay@MultiIndicNMT - WAT2021', 'sv': 'Språkrelationer och Lexisk Närhet kan hjälpa till att förbättra flerspråkig NMT: IITBombay@MultiIndicNMT Vatten2021', 'ur': 'زبان کی نسبت اور لکسیسی کلاسنس بہت سی زبان NMT کی عمدہ کی مدد کر سکتی ہے: IITBombay@MultiIndicNMT WAT2021', 'vi': 'Sự liên hệ ngôn ngữ và gần gũi ngôn ngữ có thể giúp NMT đa ngôn: IITBombay@MultiIndicNMT WAND2021', 'uz': 'Tilning bogʻlamalari va Leksikal uzoqligi IITBombay@MultiIndicNMT  WAT2021', 'nl': 'Taalverwantschap en Lexische Dichtheid kunnen helpen Meertalige NMT te verbeteren: IITBombay@MultiIndicNMT WAT2028', 'hr': 'Relativnost jezika i leksička zatvaranja mogu pomoći unaprjeđivanju multijezičkih NMT-a: IITBombay@MultiIndicNMT WAT2021', 'bg': 'Езиковата свързаност и лексикалната близост могат да помогнат за подобряване на многоезичната НМТ: IITBombay@MultiIndicNMT УТ2021', 'da': 'Sprogrelaterethed og Lexical Closeness kan hjælpe med at forbedre flersproget NMT: IITBombay@MultiIndicNMT WAT201', 'ko': '언어 관련성과 어휘 친밀도는 다국어 NMT를 향상시키는 데 도움이 된다.IITBombay@MultiIndicNMTWAT2021', 'de': 'Sprachverwandtschaft und Lexikale Nähe können helfen, mehrsprachige NMT zu verbessern: IITBombay@MultiIndicNMT WAT2028', 'id': 'Hubungan Bahasa dan Kedekatan Lexik dapat membantu meningkatkan NMT Berbahasa Berbahasa: IITBombay@MultiIndicNMT WAT2021', 'fa': 'نسبت به زبان و بستنی لکسیکی می\u200cتواند کمک کند که NMT چندین زبان را بهتر کند: IITBombay@MultiIndicNMT WAT2021', 'af': 'Taal Relatedness and Lexical Closeness can help Improve Multilingual NMT: IITBombay@MultiIndicNMT WAT2021', 'sw': 'Uhusiano wa lugha na Ubunifu wa Kilexico unaweza kusaidia kuboresha lugha nyingi NMT: IITBombay@MultiIndicNMT WAT2021', 'sq': 'Lidhja gjuhësore dhe afërsia leksikale mund të ndihmojnë përmirësimin e NMT shumëgjuhës: IITBombay@MultiIndicNMT WAT2021', 'am': 'የቋንቋ ግንኙነት እና የሜክሲካ ዝቅተኛ ስህተት የብዙ ቋንቋዎች NMT ማድረግ ይችላል: IITBombay@MultiIndicNMT WAT2021', 'tr': "Diller baglanyşyk we Lexical Baglanyşyk Çoklu diller NMT'i geliştirmekde kömek edip biler: IITBombay@MultiIndicNMT - WAT2021", 'bs': 'Relativnost jezika i leksička zatvaranja mogu pomoći unaprjeđivanju multijezičkih NMT-a: IITBombay@MultiIndicNMT WAT2021', 'bn': 'ভাষার সম্পর্ক এবং লেক্সিক্যাল কাছাকাছি সাহায্য করতে পারে: IITBombay@MultiIndicNMT * WAT2021', 'ca': 'La relació lingüística i la proximitat lèxica poden ajudar a millorar la MTN multilingüe: IITBombay@MultiIndicNMT - WAT2021', 'hy': 'Լեզու հարաբերությունը և լեքսիկական մոտիկությունը կարող են օգնել բարելավել բազլեզու NMT-ը. IITBombay@MultiIndicNMT - 2021 թվականին', 'az': "Dil bağlılığı və Lexical Closeness çoxlu dil NMT'i yaxşılaşdırmağa yardım edə bilər: IITBombay@MultiIndicNMT WAT2021", 'et': 'Keele seotus ja leksiline lähedus võivad aidata parandada mitmekeelset NMT: IITBombay@MultiIndicNMT WAT2021', 'cs': 'Jazyková souvislost a Lexická blízkost mohou pomoci zlepšit vícejazyčné NMT: IITBombay@MultiIndicNMT WAT2028', 'fi': 'Kielen yhteys ja Lexical Läheisyys voivat auttaa parantamaan monikielistä NMT: IITBombay@MultiIndicNMT WAT2021', 'jv': 'Gambar Kemerdekaan lan Leksial Closenness iso ngéwangi Bukak Multilengkang NMT:  IITBombay@MultiIndicNMT WAT 2020', 'he': 'Language Relatedness and Lexical Closeness can help Improve Multilingual NMT:  IITBombay@MultiIndicNMT  WAT2021', 'ha': '@ item Text character set IITBombay@MultiIndicNMT  WAT2021', 'sk': 'Jezikovna povezanost in leksična bližina lahko pomagata izboljšati večjezično NMT: IITBombay@MultiIndicNMT WAT12021', 'bo': 'སྐད་ཡིག་ཆ་དང་འབྲེལ་བ་དང་གཟུགས་རིས་འདྲ་བའི་ཆ་འཕྲིན་སྐོར་ཡར་རྒྱས་གཏོང་ལ་རོགས། IITBombay@MultiIndicNMT ... WAT2021'}
{'en': 'Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for  multiple languages . This paper describes our submission (Team ID : CFILT-IITB) for the MultiIndicMT : An Indic Language Multilingual Task at WAT 2021. We train multilingual NMT systems by sharing  encoder and decoder parameters  with language embedding associated with each token in both  encoder  and decoder. Furthermore, we demonstrate the use of transliteration (script conversion) for  Indic languages  in reducing the lexical gap for training a multilingual NMT system. Further, we show improvement in performance by training a multilingual NMT system using languages of the same family, i.e.,  related languages .', 'ar': 'حققت الترجمة الآلية العصبية متعددة اللغات أداءً رائعًا من خلال تدريب نموذج ترجمة واحد للغات متعددة. تصف هذه الورقة تقديمنا (معرف الفريق: CFILT-IITB) لـ MultiIndicMT: مهمة هندية متعددة اللغات في WAT 2021. نقوم بتدريب أنظمة NMT متعددة اللغات من خلال مشاركة معلمات التشفير وفك التشفير مع تضمين اللغة المرتبطة بكل رمز مميز في كل من وحدة التشفير وفك التشفير. علاوة على ذلك ، نوضح استخدام التحويل الصوتي (تحويل البرنامج النصي) للغات الهندية في تقليل الفجوة المعجمية لتدريب نظام NMT متعدد اللغات. علاوة على ذلك ، نظهر تحسنًا في الأداء من خلال تدريب نظام NMT متعدد اللغات باستخدام لغات من نفس العائلة ، أي اللغات ذات الصلة.', 'fr': "La traduction automatique neuronale multilingue a atteint des performances remarquables en formant un modèle de traduction unique pour plusieurs langues. Cet article décrit notre soumission (ID d'équipe\xa0: CFILT-IITB) pour le MultiIndicMT\xa0: An Indic Language Multilingual Task au WAT 2021. Nous formons des systèmes NMT multilingues en partageant les paramètres de l'encodeur et du décodeur avec intégration de la langue associée à chaque jeton dans l'encodeur et le décodeur. En outre, nous démontrons l'utilisation de la translittération (conversion de script) pour les langues indiennes afin de réduire l'écart lexical pour la formation d'un système NMT multilingue. En outre, nous montrons une amélioration des performances en formant un système NMT multilingue utilisant des langues de la même famille, c'est-à-dire des langues apparentées.", 'pt': 'A tradução automática neural multilíngue alcançou um desempenho notável ao treinar um único modelo de tradução para vários idiomas. Este artigo descreve nossa submissão (ID da equipe: CFILT-IITB) para o MultiIndicMT: An Indic Language Multilingual Task no WAT 2021. Treinamos sistemas NMT multilíngues compartilhando parâmetros de codificador e decodificador com incorporação de idioma associada a cada token no codificador e no decodificador. Além disso, demonstramos o uso de transliteração (conversão de script) para línguas índicas na redução da lacuna lexical para o treinamento de um sistema NMT multilíngue. Além disso, mostramos melhora no desempenho treinando um sistema NMT multilíngue usando idiomas da mesma família, ou seja, idiomas relacionados.', 'es': 'La traducción automática neuronal multilingüe ha logrado un rendimiento notable al entrenar un único modelo de traducción para varios idiomas. Este documento describe nuestra presentación (ID del equipo: CFILT-IITB) para el Multi-Indicmt: An Indic Language Multilingual Task at WAT 2021. Entrenamos sistemas NMT multilingües compartiendo parámetros de codificador y decodificador con la incrustación de idioma asociada a cada token tanto en el codificador como en el decodificador. Además, demostramos el uso de la transliteración (conversión de escritura) para idiomas índicos para reducir la brecha léxica necesaria para el entrenamiento de un sistema NMT multilingüe. Además, demostramos una mejora en el rendimiento mediante la formación de un sistema NMT multilingüe que utiliza idiomas de la misma familia, es decir, idiomas relacionados.', 'zh': '多语言神经机器翻译以多种语言教习单译模形而取卓异之性。 本文引WAT 2021上为MultiIndicMT:印度语多言事者(团队ID:CFILT-IITB)。 共编码器解码器参数以练言 NMT ,嵌以编码器解码器。 此外展印度语音译(脚本转换)于减练多言NMT系统词汇差。 此外同家之言(即相关语)多言 NMT 统以见性。', 'hi': 'बहुभाषी तंत्रिका मशीन अनुवाद ने कई भाषाओं के लिए एक एकल अनुवाद मॉडल को प्रशिक्षित करके उल्लेखनीय प्रदर्शन हासिल किया है। यह पेपर मल्टीइंडिकएमटी के लिए हमारे सबमिशन (टीम आईडी: CFILT-IITB) का वर्णन करता है: WAT 2021 में एक इंडिक भाषा बहुभाषी कार्य। हम दोनों एन्कोडर और डिकोडर में प्रत्येक टोकन के साथ जुड़े भाषा एम्बेडिंग के साथ एन्कोडर और डिकोडर पैरामीटर साझा करके बहुभाषी NMT सिस्टम को प्रशिक्षित करते हैं। इसके अलावा, हम एक बहुभाषी एनएमटी प्रणाली के प्रशिक्षण के लिए लेक्सिकल गैप को कम करने में भारतीय भाषाओं के लिए लिप्यंतरण (स्क्रिप्ट रूपांतरण) के उपयोग का प्रदर्शन करते हैं। इसके अलावा, हम एक ही परिवार की भाषाओं, यानी संबंधित भाषाओं का उपयोग करके एक बहुभाषी एनएमटी प्रणाली को प्रशिक्षित करके प्रदर्शन में सुधार दिखाते हैं।', 'ja': '多言語ニューラル・マシン・トランスレーションは、複数の言語の単一の翻訳モデルをトレーニングすることで、卓越したパフォーマンスを達成しています。本稿では、WAT 2021におけるMultiIndicMT: An Indic Language Multilingual Taskの提出物（チームID ： CFILT - IITB ）について説明します。エンコーダとデコーダの両方で、エンコーダとデコーダパラメータを、各トークンに関連付けられた言語埋め込みと共有することで、多言語NMTシステムをトレーニングします。さらに、多言語NMTシステムをトレーニングするための語彙ギャップを減らすためのインディック言語のトランスリット（スクリプト変換）の使用を実証します。さらに、同じファミリーの言語、すなわち関連言語を使用して多言語NMTシステムをトレーニングすることによって、パフォーマンスの向上を示します。', 'ru': 'Многоязычный нейронный машинный перевод достиг замечательной производительности, обучив одну модель перевода для нескольких языков. В этой статье описывается наше представление (идентификатор команды: CFILT-IITB) для многоязычной задачи MultiIndicMT: An Indic Language Multilingual Task at WAT 2021. Мы обучаем многоязычные системы NMT, делясь параметрами кодера и декодера с языковым встраиванием, связанным с каждым токеном, как в кодере, так и в декодере. Кроме того, мы демонстрируем использование транслитерации (преобразования скриптов) для языков индикаторов в уменьшении лексического разрыва для обучения многоязычной системы НМТ. Кроме того, мы демонстрируем улучшение производительности путем обучения многоязычной системы НМТ с использованием языков одной и той же семьи, т.е. родственных языков.', 'ga': 'Tá feidhmíocht thar na bearta bainte amach ag Aistriúchán Inneall Néarach Ilteangach trí oiliúint a chur ar mhúnla aonair aistriúcháin d’iltheangacha. Déanann an páipéar seo cur síos ar ár n-aighneacht (Foireann ID: CFILT-IITB) don MultiIndicMT: Tasc Ilteangach Teanga Indíreach ag WAT 2021. Cuirimid oiliúint ar chórais ilteangacha NMT trí pharaiméadair ionchódóra agus díchódóra a roinnt le leabú teanga a bhaineann le gach comhartha san ionchódóir agus sa díchódóir araon. Ina theannta sin, léirímid an úsáid a bhaintear as traslitriú (tiontú scripte) le haghaidh teangacha Indeacha chun an bhearna foclóireachta a laghdú chun córas ilteangach NMT a oiliúint. Ina theannta sin, léirímid feabhas ar fheidhmíocht trí chóras NMT ilteangach a oiliúint a úsáideann teangacha an teaghlaigh chéanna, i.e., teangacha gaolmhara.', 'ka': 'Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for multiple languages. ამ დოკუმენტი ჩვენი მომხმარება (Team ID: CFILT- IITB) MultiIndicMT: ინდიური მრავალენგური დავალება WAT 2021-ში. ჩვენ მრავალენგური NMT სისტემების გაგრძნობით კოდირების და დეკოდირების პარამეტრებით, რომლებიც ყველა ტექნონისთვის კოდირებით და დეკოდირებით დაკავშირებ დამატებით, ჩვენ ინდენური ენებისთვის ტრანსლიტაციის (სკრიპტის გადაცვლის) გამოყენებას გამოყენება ლექსიკალური განსხვავებას მრავალენგური NMT სისტემისთვის განსხვავ დამატებით, ჩვენ ჩვენ ჩვენ აჩვენებთ მრავალენგური NMT სისტემის გამოყენებით ერთი ოჯახის ენების შესაძლებლობა, ანუ, შესაძლებელი ენების შესახებ.', 'el': 'Η πολύγλωσση Νευρική Μηχανική Μετάφραση έχει επιτύχει αξιοσημείωτη απόδοση εκπαιδεύοντας ένα μόνο μοντέλο μετάφρασης για πολλές γλώσσες. Η παρούσα εργασία περιγράφει την υποβολή μας (Ομάδα ID: CFILT-IITB) για το ΠολυΙνδικό ΜΤ: Μια Ινδική Γλώσσα Πολυγλωσσική Εργασία στο WAT 2021. Εκπαιδεύουμε πολυγλωσσικά συστήματα μοιράζοντας παραμέτρους κωδικοποιητή και αποκωδικοποιητή με ενσωμάτωση γλώσσας που σχετίζεται με κάθε σήμα τόσο στον κωδικοποιητή όσο και στον αποκωδικοποιητή. Επιπλέον, καταδεικνύουμε τη χρήση της μεταγραφής (μετατροπή σεναρίων) για τις ινδικές γλώσσες στη μείωση του λεξικού χάσματος για την εκπαίδευση ενός πολυγλωσσικού συστήματος NMT. Επιπλέον, επιδεικνύουμε βελτίωση στην απόδοση εκπαιδεύοντας ένα πολύγλωσσο σύστημα που χρησιμοποιεί γλώσσες της ίδιας οικογένειας, δηλαδή συγγενικές γλώσσες.', 'hu': 'A többnyelvű neurális gépi fordítás kiemelkedő teljesítményt ért el azáltal, hogy egyetlen fordítási modellt képzett több nyelvre. Ez a tanulmány bemutatja a beadványunkat (Team ID: CFILT-IITB) a MultiIndicaMT: Indikus nyelvű többnyelvű feladathoz a WAT 2021-en. Többnyelvű NMT rendszereket képzünk úgy, hogy megosztjuk a kódoló és dekódoló paramétereket az egyes tokenekhez kapcsolódó nyelvi beágyazással mind a kódolóban, mind a dekódolóban. Ezenkívül bemutatjuk, hogy az indikus nyelvek transzliterációját (script konverzióját) használják a többnyelvű NMT rendszer képzésének lexikális résének csökkentésében. Továbbá javulást mutatunk a teljesítményben azáltal, hogy egy többnyelvű NMT rendszert képzünk ugyanazon család nyelvein, azaz rokon nyelveken.', 'kk': 'Көптеген тілдердің нейрал машинаның аудармасы бірнеше тілдер үшін бір аудармас үлгісін оқыту үшін белгілікті болды. Бұл қағаз біздің көпIndicMT үшін жіберімізді (Топ ID: CFILT- IITB) анықтайды: WAT 2021 жылы бірнеше тілді тапсырманың индикалық тілі. Біз көп тілді NMT жүйелерін кодерді ортақтастырып, декодтау параметрлерін тілді ендіру үшін әрбір белгілерді кодтау мен декодтау үшін оқыдық. Сонымен қатар, біз индикалық тілдер үшін көп тілді NMT жүйесін оқыту үшін лексикалық қашықтығын азайту үшін транслитерация (скрипттерді аудару) қолдануын көрсетедік. Сонымен қатар, біз бір отбасының тілдерін қолдану үшін бірнеше тілді NMT жүйесіне жұмыс істеу жүйесінің жақсартуын көрсетедік.', 'it': "La traduzione automatica neurale multilingue ha raggiunto prestazioni notevoli grazie alla formazione di un unico modello di traduzione per più lingue. Questo articolo descrive la nostra presentazione (Team ID: CFILT-IITB) per il MultiIndicaMT: An Indic Language Multilingual Task a WAT 2021. Formiamo sistemi NMT multilingue condividendo i parametri dell'encoder e del decoder con l'embedding del linguaggio associato a ciascun token sia nell'encoder che nel decoder. Inoltre, dimostriamo l'uso della traslitterazione (conversione script) per le lingue indiche per ridurre il gap lessicale per la formazione di un sistema NMT multilingue. Inoltre, mostriamo un miglioramento delle prestazioni attraverso la formazione di un sistema NMT multilingue utilizzando lingue della stessa famiglia, cioè lingue correlate.", 'lt': 'Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for multiple languages.  Šiame dokumente apibūdinamas mūsų pasiūlymas (grupės ID: CFILT-IITB) dėl daugiakalbės MT: Indinės kalbos užduotis pagal 2021 m. WAT. Mokome daugiakalbes NMT sistemas dalijantis koduotojo ir dekoderio parametrais su kalbos įterpimu, susijusiu su kiekvienu ženklu tiek koduotoje, tiek dekodere. Be to, demonstruojame transliteracijos (scenarijaus konversijos) naudojimą indinėms kalboms mažinant lexinę spragą mokymui daugiakalbei NMT sistemai. Be to, mes rodome, kad veiksmingumas pagerėjo mokydami daugiakalbę NMT sistemą, naudojanči ą tos pačios šeimos kalbas, t. y. susijusias kalbas.', 'ml': 'പല ഭാഷകള്\u200dക്കുള്ള ഒരു ട്രെന്\u200dഷന്\u200d മോഡല്\u200d പരിശീലിക്കുന്നതിനാല്\u200d പല ഭാഷകള്\u200dക്കും ഒരു മാതൃകയായി മാറ്റിയിട്ടുണ്ട്. ഈ പത്രത്തില്\u200d ഞങ്ങളുടെ കീഴ്പ്പെടുത്തുന്നതിനെ (ടീം ഐഡി: CFILT-IITB) വിവരിച്ചുകൊടുക്കുന്നുണ്ട്: WAT 2021-ല്\u200d ഒരു ഇന്റിക്ക് ഭാഷ നമ്മള്\u200d പല ഭാഷയിലുള്ള NMT സിസ്റ്റം പരിശീലിപ്പിക്കുന്നു. കോഡിഡര്\u200d പങ്കെടുക്കുന്നതും ഭാഷ അടയാളങ്ങളുമായി അടയാളങ്ങളുമായി കോ ഇന്ത്യഭാഷകള്\u200dക്കുള്ള ട്രാന്\u200dസ്ക്രിപ്റ്റേഷന്\u200d (സ്ക്രിപ്റ്റ് മാറ്റി) ഉപയോഗിക്കുന്നതിനായി ഞങ്ങള്\u200d കാണിച്ചുകൊടുക്കുന്ന അതിനുശേഷം, നമ്മള്\u200d പ്രവര്\u200dത്തനത്തില്\u200d മെച്ചപ്പെടുത്തുന്നത് കാണിക്കുന്നു. ഒരേ കുടുംബത്തിന്റെ ഭാഷകള്\u200d ഉപയോഗിച്ച് ഒരു', 'ms': 'Terjemahan Mesin Neural Berbahasa telah mencapai prestasi yang luar biasa dengan melatih model terjemahan tunggal untuk berbagai bahasa. Kertas ini menggambarkan penghantaran kami (ID Pasukan: CFILT-IITB) untuk MultiIndicMT: Tugas Berbahasa Bahasa Indik pada WAT 2021. Kami melatih sistem NMT berbilang bahasa dengan berkongsi parameter pengekod dan pengekod dengan penyambungan bahasa yang berkaitan dengan setiap token dalam pengekod dan pengekod. Selain itu, kami menunjukkan penggunaan transliterasi (penukaran skrip) untuk bahasa India dalam mengurangi ruang leksik untuk melatih sistem NMT berbilang bahasa. Lagipun, kami menunjukkan kemajuan dalam prestasi dengan melatih sistem NMT berbilang bahasa menggunakan bahasa keluarga yang sama, iaitu bahasa berkaitan.', 'mt': 'It-Traduzzjoni Multilingwi tal-Makkinarju Newrali kisbet prestazzjoni notevoli bit-taħriġ ta’ mudell wieħed ta’ traduzzjoni għal diversi lingwi. Dan id-dokument jiddeskrivi s-sottomissjoni tagħna (ID tat-Tim: CFILT-IITB) għall-MultiIndicMT: Kompitu Multilingwi Indiku fil-WAT 2021. Aħna nħarrġu sistemi NMT multilingwi billi nqasmu l-parametri tal-kodifikatur u tad-dekoder mal-inkorporazzjoni tal-lingwa assoċjata ma’ kull token kemm fil-kodifikatur kif ukoll fid-dekoder. Barra minn hekk, nagħmlu xhieda tal-użu tat-translitterazzjoni (konverżjoni tal-iskripti) għal-lingwi Indiċi fit-tnaqqis tad-distakk lexiku għat-taħriġ ta’ sistema NMT multilingwi. Barra minn hekk, nagħmlu titjib fil-prestazzjoni bit-taħriġ ta’ sistema NMT multilingwi bl-użu ta’ lingwi tal-istess familja, jiġifieri lingwi relatati.', 'mn': 'Ихэнх хэлний мэдрэлийн машины хөгжүүлэлт олон хэлний нэг хөгжүүлэлтийн загварыг сургалтын тулд гайхалтай үйл ажиллагааг гаргасан. Энэ цаас бидний олон ИндийцMT-д хэдэн хэлний олон хэлний даалгаврыг (Баг ID: CFILT-IITB) тайлбарладаг. Бид олон хэл NMT системийг коддогч, декоддогч параметрлүүдтэй хуваалцаж, коддогч, декоддогч хэл бүрт холбогдсон томъёог суралцаж байна. Үүнээс гадна бид индик хэл дээр хэлний NMT системийг суралцах үед лектикийн ялгааг багасгах боломжтой илэрхийлэл хөгжүүлэхийг харуулж байна. Дараа нь бид олон хэлний NMT системийг ижил гэр бүлийн хэл, т.е. холбоотой хэл ашиглаж ажиллагааны сайжруулалтыг харуулдаг.', 'no': 'Fleirspråk Neuralmaskinsomsetjing har oppnådd merkelige utvikling ved å trenga ein enkelt omsetjingsmodul for fleire språk. Denne papiret beskriver vårt oppføring (gruppe ID: CFILT- IITB) for multiIndicMT: Eit indesk språk multispråk på WAT 2021. Vi treng fleirspråk NMT- systemar ved å dele koder og dekoder- parametrar med språk innebygging som er assosiert med kvar teikn i både koder og dekoder. I tillegg viser vi bruken av transliterasjon (konvertering av skript) for indiske språk i å redusera leksiske mellomrom for å opplæra eit multispråk NMT- system. I tillegg viser vi forbetringar i utviklinga ved å trenge eit multispråk NMT-system med språk av same familie, dvs. tilhøyrande språk.', 'pl': 'Wielojęzyczne neuronowe tłumaczenie maszynowe osiągnęło niezwykłą wydajność dzięki szkoleniu jednego modelu tłumaczenia dla wielu języków. Niniejszy artykuł opisuje naszą zgłoszenie (Team ID: CFILT-IITB) do MultiIndicMT: An Indic Language Multilingual Task w WAT 2021. Szkolimy wielojęzyczne systemy NMT poprzez współdzielenie się parametrami kodera i dekodera z osadzeniem języka związanym z każdym tokenem zarówno w koderze, jak i dekoderze. Ponadto demonstrujemy wykorzystanie transliteracji (konwersji skryptów) dla języków indyjskich w zmniejszeniu luki leksykalnej dla szkolenia wielojęzycznego systemu NMT. Ponadto pokazujemy poprawę wydajności poprzez szkolenie wielojęzycznego systemu NMT wykorzystującego języki tej samej rodziny, tj. języki pokrewne.', 'ro': 'Traducerea automată neurală multilingvă a obținut performanțe remarcabile prin instruirea unui singur model de traducere pentru mai multe limbi. Această lucrare descrie depunerea noastră (ID echipă: CFILT-IITB) pentru MultiIndicaMT: O activitate multilingvă de limbă indică la WAT 2021. Instruim sisteme NMT multilingve prin partajarea parametrilor encoder și decoder cu încorporarea limbii asociate fiecărui token atât în encoder, cât și în decoder. Mai mult, demonstrăm utilizarea transliterației (conversia script-ului) pentru limbile indice în reducerea decalajului lexical pentru formarea unui sistem multilingv NMT. Mai mult, demonstrăm îmbunătățirea performanței prin instruirea unui sistem multilingv NMT folosind limbi din aceeași familie, adică limbi conexe.', 'sr': 'Mnogjezički Neuralni prevod mašine postigao je izvanrednu funkciju obučavanjem jednog model a prevoda za višestruke jezike. Ovaj papir opisuje naše podnošenje (Tim ID: CFILT-IITB) za MultiIndicMT: Indijski multijezički zadatak na WAT 2021. Vežbamo multijezičke NMT sisteme podijeljenjem kodera i dekodera parametara sa ugrađenjem jezika povezanom sa svakim znakom u koderu i dekoderu. Osim toga, mi pokazujemo korištenje transliteracije (konvertacije skripta) za indijske jezike u smanjenju leksičkog praznika za obuku multijezičkog NMT sistema. Nadalje, pokazujemo poboljšanje učinka vežbanjem multijezičkog NMT-ovog sistema koristeći jezike iste porodice, tj. povezanih jezika.', 'mk': 'Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for multiple languages.  Овој весник го опишува нашето поднесување (ИД на тимот: CFILT-IITB) за MultiIndicMT: Индиски јазик мултијазична задача на WAT 2021. We train multilingual NMT systems by sharing encoder and decoder parameters with language embedding associated with each token in both encoder and decoder.  Furthermore, we demonstrate the use of transliteration (script conversion) for Indic languages in reducing the lexical gap for training a multilingual NMT system.  Покрај тоа, покажуваме подобрување на резултатите со обуката на мултијазичен НМТ систем кој користи јазици од истото семејство, т.е. поврзани јазици.', 'so': 'Turjumista machadka luqadaha badan ee Neural ah wuxuu ku soo bandhigay bandhig aad u fiican, waxayna ku tababartay tusaale turjum oo kaliya oo luuqado kala duduwan. Warqaddan waxaa ku qoran muuqashada kooxda ID (CFILT-IITB) ee MultiIndicMT: Shaqad luuqad badan oo Indic language WAT 2021. Waxaannu ku tababarinnaa nidaamka NMT ee luuqadaha kala duduwan, si aan u wadanno koordirada iyo koordirida parameters oo luqada ku qoran, taasoo la xiriira calaamad kasta oo ku qoran koodeynta iyo koodeynta. Furthermore, waxaynu muujinnaa isticmaalka qoraalka (bedelka qoraalka) ee luqada Indianka si aan u fududayno burburka leksikada ee waxbarashada nidaamka NMT ee luuqadaha kala duduwan. Sidoo kale horumarinta sameynta waxaynu tusnaynaa nidaamka NMT e e luuqadaha kala duduwan oo aan ku isticmaalno luuqadaha qoyska isku mid ah, tusaale ahaan luuqadaha la xiriira.', 'si': 'ගොඩක් භාෂාවක් න්\u200dයූරාල් මැෂින් පරිවර්තනය විශ්වාස කරන්න පුළුවන් විශේෂ ක්\u200dරියාත්මක විද මේ පැත්තේ අපේ පිළිගන්න (කණ්ඩායම ID: CFILT-IITB) විශ්වාස කරනවා MultiIndicMT: ඉන්දික භාෂාවක් ගොඩක් භාෂාවක් වැඩ WAT 2021 වල. අපි ගොඩක් භාෂාවික NMT පද්ධතිය සංවේදනය කරන්නේ කෝඩර් සහ ඩිකොඩර් පරාමිතිය සමග භාෂාව සම්බන්ධ කරනවා හැම ටොකෙන ඉතින්, අපි ඉන්ඩික් භාෂාවට භාෂාව ප්\u200dරවේශනය (ස්ක්\u200dරිප්ටර් වෙනස් කරන්න) භාෂාව ප්\u200dරවේශනය ප්\u200dරවේශනය කරන්න ප්\u200dරවේශනය ක තවත්, අපි ප්\u200dරශ්නයක් පෙන්වන්නේ වැඩි භාෂාවක් NMT පද්ධතියක් එකම පවුලේ භාෂාවක් භාවිත කරන්න, ඉතින්, සම්', 'ta': 'பல மொழிகளில் நெருக்கல் இயந்திரம் மொழிபெயர்ப்பு மொழிகளுக்கு ஒரே மொழிபெயர்ப்பு மாதிரியை பயிற்சி செய்து கொண் இந்த காகிதத்தில் எங்கள் கூட்டத்தின் அடையாளம் (குழு அடையாளம்: CFILT-IITB) பல்சிநிரல்களுக்கு: WAT 2021 ல் ஒரு சிந்திக் மொழி பல மொழி பணி We train multilingual NMT systems by sharing encoder and decoder parameters with language embedding associated with each token in both encoder and decoder.  மேலும், நாம் சிந்த மொழிகளுக்கான மாற்றம் (சிறுநிரல் மாற்றம்) பயன்படுத்தலை காட்டுகிறோம். பல மொழிகள் NMT முறைமையை பயிற்சிக்கு லெ மேலும், நாம் செயல்பாட்டில் முன்னேற்றத்தை காட்டுகிறோம் அதே குடும்பத்தின் மொழிகளை பயன்படுத்தி பல மொழிகள் NMT அமைப்', 'sv': 'Multilingual Neural Machine Translation har uppnått enastående prestanda genom att utbilda en enda översättningsmodell för flera språk. Denna uppsats beskriver vårt bidrag (Team ID: CFILT-IITB) för MultiIndicaMT: An Indic Language Multilingual Task vid WAT 2021. Vi utbildar flerspråkiga NMT-system genom att dela kodare och avkodare parametrar med språkinbäddning associerad med varje token i både kodare och avkodare. Vidare visar vi användningen av transliteration (scriptkonvertering) för indiska språk för att minska lexikala gap för utbildning av ett flerspråkigt NMT-system. Vidare visar vi förbättring av prestanda genom att utbilda ett flerspråkigt NMT-system med språk från samma familj, dvs besläktade språk.', 'ur': 'Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for multiple languages. This paper describes our submission (Team ID: CFILT-IITB) for the MultiIndicMT: An Indian Language Multilingual Task at WAT 2021. ہم multilingual NMT systems train by sharing encoder and decoder parameters with language embedding associated with each token in both encoder and decoder. اور اس کے علاوہ، ہم انڈی زبانوں کے لئے ترنسلیٹ (اسکریٹ تبدیل) کی استعمال کو دکھاتے ہیں ایک multilingual NMT سیستم کی آموزش کے لئے لکسیکل فاصلہ کم کرنے کے لئے۔ اور ہم ایک خاندان کی زبانوں کے مطابق ایک multilingual NMT سیسٹم کی تعلیم کے ذریعہ عملکرد میں بہترین عملکرد دکھاتے ہیں، یعنی مرتبہ زبانوں کے مطابق۔', 'vi': 'Dịch đa ngôn ngữ thần kinh đã đạt được trình độ đáng chú ý nhờ huấn luyện một mô hình dịch đơn cho nhiều ngôn ngữ. Tờ giấy này mô tả sự đệ trình của chúng ta (Team ID: CFO-lTB) cho tổ chức đa miền: Một tổ chức đa ngôn ngữ riêng tại WAT 2021. Chúng tôi đào tạo các hệ thống NMT đa dạng bằng cách chia sẻ mã hóa và mã giải bằng cách lắp ghép ngôn ngữ liên quan đến mỗi vật trong hộp mã và bộ giải. Chúng tôi cũng chứng minh việc chuyển chữ (chuyển đổi văn bản) cho ngôn ngữ Ấn để giảm lỗ ngôn ngữ văn học để đào tạo một hệ thống NMT đa dạng. Thêm nữa, chúng tôi thấy hiệu quả tốt hơn nhờ huấn luyện một hệ thống NMT đa ngôn ngữ cùng một gia đình, tức là ngôn ngữ liên quan.', 'uz': "Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for multiple languages.  Bu sahifa, MultiIndicMT uchun qanchalik tilning bir necha tillar vazifasi WAT 2021 yilda qanchalik tillar. Biz bir necha tilda NMT tizimni kodlash va kodlash usuli bilan bogʻ'liq boʻlgan tilning parametrlarini kodlash va kodlash va kodlash usulida bogʻliq boʻlgan. Ko'rsatganda, biz bir necha til NMT tizimni o'rganish uchun transliteratsiya (skriptni almashtirish) foydalanishni ko'rsatdik. Ko'rsatganda, biz bir xil tilning tillari bilan bir necha NMT tizimni o'rganish uchun bajarish muvaffaqiyatlarini ko'rsamiz, balki huddi bog'liq tillar.", 'da': 'Flersproget neural maskinoversættelse har opnået bemærkelsesværdige resultater ved at træne en enkelt oversættelsesmodel til flere sprog. Dette papir beskriver vores indsendelse (Team ID: CFILT-IITB) til MultiIndicaMT: An Indic Language Multilingual Task på WAT 2021. Vi træner flersprogede NMT-systemer ved at dele encoder og dekoder parametre med sprogindlejring forbundet med hvert token i både encoder og dekoder. Desuden demonstrerer vi brugen af translitteration (script konvertering) til Indiske sprog til at reducere den leksikalske kløft til træning af et flersproget NMT system. Desuden viser vi forbedringer i ydeevnen ved at træne et flersproget NMT-system med sprog fra samme familie, dvs. beslægtede sprog.', 'bg': 'Многоезичният неврален машинен превод постигна забележителни резултати чрез обучението на един модел за превод за множество езици. Тази статия описва нашето представяне (Идентификация на екипа: Многоезична задача на индийския език в УАТ 2021 г. Обучаваме многоезични системи чрез споделяне на параметри на кодера и декодера с вграждане на езика, свързан с всеки символ както в кодера, така и в декодера. Освен това демонстрираме използването на транслитерация (конвертиране на скрипт) за индийски езици за намаляване на лексикалната пропаст за обучение на многоезична НМТ система. Освен това показваме подобрение в ефективността чрез обучение на многоезична система за НМТ, използваща езици от едно и също семейство, т.е. сродни езици.', 'nl': 'Meertalige Neural Machine Translation heeft opmerkelijke prestaties bereikt door één vertaalmodel voor meerdere talen te trainen. Dit artikel beschrijft onze inzending (Team ID: CFILT-IITB) voor de MultiIndicMT: An Indic Language Multilingual Task bij WAT 2021. We trainen meertalige NMT-systemen door encoder- en decoderparameters te delen met taalinbedding geassocieerd met elk token in zowel encoder als decoder. Verder demonstreren we het gebruik van transliteratie (scriptconversie) voor Indische talen om de lexicale kloof voor het trainen van een meertalig NMT systeem te verkleinen. Verder tonen we verbetering in prestaties door een meertalig NMT-systeem te trainen dat talen van dezelfde familie gebruikt, d.w.z. verwante talen.', 'hr': 'Mnogjezički Neuralni prevod uređaja postigao je izvanrednu funkciju obučavanjem jednog model a prevoda za višestruke jezike. Ovaj papir opisuje naše podatke (Tim ID: CFILT-IITB) za MultiIndicMT: Indijski multijezički zadatak na WAT 2021. Vježbamo višejezičke NMT sustave dijelom kodera i dekodera parametara s ugrađenjem jezika povezanom sa svakim znakom u koderu i dekoderu. Osim toga, mi pokazujemo korištenje transliteracije (konvertacije skripta) za indijske jezike u smanjenju leksičkog praznika za obuku multijezičkog NMT sustava. Nadalje, pokazujemo poboljšanje učinka vježbanjem multijezičkog NMT sustava koristeći jezike iste obitelji, tj. povezanih jezika.', 'de': 'Mehrsprachige neuronale maschinelle Übersetzung hat bemerkenswerte Leistung erreicht, indem ein einziges Übersetzungsmodell für mehrere Sprachen trainiert wurde. Dieser Beitrag beschreibt unsere Einreichung (Team ID: CFILT-IITB) für die MultiIndicMT: An Indic Language Multilingual Task am WAT 2021. Wir trainieren mehrsprachige NMT-Systeme, indem wir Encoder- und Decoderparameter mit Spracheinbettung teilen, die jedem Token sowohl im Encoder als auch im Decoder zugeordnet ist. Darüber hinaus demonstrieren wir den Einsatz von Transliteration (Skriptkonvertierung) für indische Sprachen, um die lexikalische Lücke für das Training eines mehrsprachigen NMT-Systems zu verringern. Darüber hinaus zeigen wir eine Leistungssteigerung, indem wir ein mehrsprachiges NMT-System trainieren, das Sprachen derselben Familie verwendet, d.h. verwandte Sprachen.', 'id': 'Multilingual Neural Machine Translation telah mencapai prestasi luar biasa dengan melatih model terjemahan tunggal untuk berbagai bahasa. Kertas ini menjelaskan pengiriman kami (Tim ID: CFILT-IITB) untuk MultiIndicMT: Sebuah Bahasa Indic Multilingual Task di WAT 2021. Kami melatih sistem NMT berbagai bahasa dengan berbagi parameter pengekode dan pengekode dengan pengekode bahasa yang terkait dengan setiap token dalam pengekode dan pengekode. Selain itu, kami menunjukkan penggunaan transliterasi (konversi skrip) untuk bahasa India dalam mengurangi ruang leksik untuk melatih sistem NMT berbilang bahasa. Lagipula, kami menunjukkan peningkatan dalam prestasi dengan melatih sistem NMT berbagai bahasa menggunakan bahasa dari keluarga yang sama, yaitu bahasa yang berhubungan.', 'fa': 'ترجمه ماشین عصبی چندین زبان با آموزش یک مدل ترجمه برای زبانهای متعدد به اجرای معرفی رسید. این کاغذ تسلیم کردن ما را توصیف می\u200cکند (شناسۀ تیم: CFILT-IITB) برای MultiIndicMT: یک کار زبان زیادی زبان هندی در WAT 2021. ما سیستم NMT multilingual Training by sharing encoder and decoder parameters with language embedding associated with each token in both encoder and decoder. علاوه بر این، ما استفاده از ترجمه (تبدیل نوشته) برای زبان\u200cهای هندی را نشان می\u200cدهیم تا فاصله\u200cهای زبانی را برای آموزش یک سیستم NMT multilingual کاهش دهیم. ما با آموزش یک سیستم NMT multilingual با استفاده از زبان\u200cهای همان خانواده، یعنی زبان\u200cهای ارتباطی، بهترین عملکرد را نشان می\u200cدهیم.', 'sw': 'Tafsiri ya Mashine ya Kifaransa ya lugha mbalimbali imefanikiwa kwa mafunzo ya utafsiri kwa lugha nyingi. Gazeti hili linaelezea ujumbe wetu (utambulisho wa timu: CFILT-IITB) kwa ajili ya Kihindi: Kazi ya lugha ya Kihindi katika WAT 2021. Tunafundisha mfumo wa NMT wa lugha mbalimbali kwa kushirikisha kodi na kupunguza parameter kwa lugha inayohusiana na kila ishara ya kodi na kodi. Zaidi ya hayo, tunaonyesha matumizi ya usambazaji (mabadiliko ya script) kwa lugha za Kihindi kwa kupunguza upepo wa lexico kwa ajili ya mafunzo mfumo wa NMT wa lugha mbalimbali. Zaidi ya hayo, tunaonyesha maendeleo ya utendaji kwa kufundisha mfumo wa NMT wa lugha mbalimbali kwa kutumia lugha za familia moja, yaani lugha zinazohusiana na lugha hizo.', 'tr': 'Birnäçe diller, näçe diller üçin bir terjime nusgasyny bejermek üçin ajaýyp ukyp bilen ýetirdi. Bu kagyz MultiIndicMT üçin biziň gönderişimizi (Tim ID: CFILT-IITB) suratlandyrýar: An Indic Language Multilingual Task at WAT 2021. Biz köp dilli NMT sistemlerini ködleme we kodlemek bilen bilim kodlemek bilen ylalaşyp öwredýäris. Mundan soňra, biz indiki diller üçin terjime (skript üýtgetmek) ulanyşyny köp diller NMT sistemini okuw etmek üçin lektik boşluklary azaltmak üçin görkeýäris. Mundan soňra, biz birnäçe dilli NMT sistemasyny bir maşgalanyň dillerini ulanyp etmäge täzelikleri gowurak görkezip görkeýäris.', 'ko': '다언어 신경기계 번역은 다양한 언어를 위한 단일 번역 모델을 훈련함으로써 현저한 성능을 얻었다.본고는 WAT 2021에서 제출한 다국어 작업(팀 ID: CFILT-IITB)을 설명합니다.우리는 인코더와 디코더의 매개 변수와 인코더와 디코더의 모든 영패와 관련된 언어 삽입을 공유함으로써 다중 언어 NMT 시스템을 훈련한다.그 밖에 우리는 인도어의 음역(스크립트 변환)이 어휘 격차를 줄여 다국어 NMT 시스템을 훈련하는 데 응용되는 것을 보여 주었다.그 밖에 우리는 같은 가족의 언어(즉 관련 언어)를 사용하여 다중 언어 NMT 시스템을 교육함으로써 성능 개선을 보여 주었다.', 'af': "Veelvuldige Neurale Masjien Vertaling het betekende prestasie bereik deur 'n enkele vertaling model vir veelvuldige tale te oefen. Hierdie papier beskryf ons onderskrywing (Tim ID: CFILT- IITB) vir die MultiIndicMT: ' n Indiese Taal Veelvuldige Opdrag by WAT 2021. Ons trein multilinglike NMT stelsels deur te deel koder en dekoder parameters met taal inbêring wat met elke token geassosieer is in beide enkoder en dekoder. Ons wys ook die gebruik van transliterasie (skrip omskakeling) vir Indiese tale in die verduur van die leksiese gap vir die onderwerp van 'n multitaalske NMT stelsel. Verder vertoon ons verbetering in prestasie deur 'n multilinglike NMT-stelsel te oefen deur te gebruik tale van dieselfde familie, i.e. verwante tale.", 'sq': 'Përkthimi shumëgjuhës i Makinës Neurale ka arritur performancë të shquar duke trajnuar një model të vetëm përkthimi për gjuhë të shumëgjuhës. This paper describes our submission (Team ID: CFILT-IITB) for the MultiIndicMT: An Indic Language Multilingual Task at WAT 2021.  Ne trajnojmë sistemet shumëgjuhëse NMT duke ndarë parametrat e koduesit dhe dekoderit me përfshirjen e gjuhës të lidhur me çdo token në koduesin dhe dekoderin. Përveç kësaj, ne demonstrojmë përdorimin e transliteracionit (konvertimin e script) për gjuhët indike në reduktimin e çarjes lexike për trajnimin e një sistemi NMT shumëgjuhës. Further, we show improvement in performance by training a multilingual NMT system using languages of the same family, i.e., related languages.', 'bn': 'মাল্টিভাষী নিউরাল মেশিন অনুবাদ পাওয়া গেছে বেশ কয়েকটি ভাষার জন্য একটি অনুবাদ মডেল প্রশিক্ষণের মাধ্যমে। এই পত্রিকা আমাদের প্রতিষ্ঠানের (টিম আইডি: সিফিল্ট-IITB) বিষয়টি মাল্টিইন্ডিকিএমটি: ওয়াট ২০১১-এ ইন্ডিক ভাষার মাল্টিভাষার কা আমরা বহুভাষায় এনএমটি সিস্টেম প্রশিক্ষণ করি এনকোডার শেয়ার করে এবং কোডোডারের মাধ্যমে ভাষার প্যারামিটারের সাথে যোগাযোগ করে প্রত এছাড়াও, আমরা ইন্ডিক ভাষার জন্য ট্রান্সলিকেশন (স্ক্রিপ্ট পরিবর্তন) ব্যবহারের ব্যবহার প্রদর্শন করছি যেখানে মাল্টিভাষায় এনএমটি সিস্টে এছাড়াও, আমরা এক পরিবারের ভাষা ব্যবহার করে বহুভাষী এনএমটি সিস্টেমের প্রশিক্ষণের মাধ্যমে ভাষার উন্নতি প্রদর্শন করি, যেমন সংশ্', 'am': 'የቋንቋ ቋንቋዎች የኔural machine ትርጉም አንድ ትርጉም ሞዴል ለብዙዎች ቋንቋዎች በመግጠም አግኝቷል፡፡ ይህ ገጽ የቡድን አዳራሲ (የቡድን ID: CFILT-IITB) ለMultiIndicMT: የህንድ ቋንቋ ብዙልቋ ቋንቋ ስራ በWAT 2021 የሚል ነው፡፡ We train multilingual NMT systems by sharing encoder and decoder parameters with language embedding associated with each token in both encoder and decoder.  በተጨማሪም፣ ለህንድ ቋንቋዎች የተቀናቀለውን (የጽሑፍ መለወጥ) የተጠቃሚ ስርዓት በብዙ ቋንቋዎች የNMT ሲስተማርን ለማሳነስ እናሳያቸዋለን፡፡ በተጨማሪም፣ የብዙ ቋንቋዎች የNMT ሲስተማርን በአንድ ወገን ቋንቋ በመጠቀም እናሳየዋለን፡፡', 'hy': "Բազլեզու նյարդային մեքենայի թարգմանությունը հաջողվել է զարմանալի արդյունք' սովորեցնելով մեկ թարգմանման մոդել բազմալեզուների համար: Այս հոդվածը նկարագրում է մեր ներկայացումը (Թիմի ID: FIILT-IITB) ՄիջինդիքՄԹ-ի համար՝ Ինդիկ լեզվի բազլեզվի առաջադրանքն է 2021 թվականին: Մենք վարժեցնում ենք բազմալեզու NMT համակարգեր՝ կիսվելով կոդավորման և կոդավորման պարամետրերով լեզվի ներգրավման հետ, որը կապված է յուրաքանչյուր նշանի հետ, և կոդավորման մեջ: Ավելին, մենք ցույց ենք տալիս, որ հնդկական լեզուների համար տրանսգրաֆիտացիայի օգտագործումը նվազեցնում է բազլեզու NMT համակարգի ուսումնասիրելու լեքսիկական բացառությունը: Ավելին, մենք ցույց ենք տալիս արդյունքների բարելավումը՝ սովորեցնելով բազլեզու NMT համակարգ, որը օգտագործում է նույն ընտանիքի լեզուներ, այսինքն, կապված լեզուներ:", 'az': 'Çoxlu dil Nöral Makina Çeviri çoxlu dillər üçün təkrarlama modeli təhsil edərək mükəmməl performans qəbul etdi. Bu kağıt çoxlu IndicMT üçün bizim göndərməyimizi (Team ID: CFILT-IITB) təsdiqləyir: WAT 2021-də Hindi dili çoxlu dil işləri təsdiqləyir. Biz çoxlu dil NMT sistemlərini kodlayıcı və dekoder parametrlərini hər token ilə birlikdə yazılmış dillər ilə paylaşıraq təhsil edirik. Daha sonra, biz Indik dillərin çoxlu dil NMT sistemini təhsil etmək üçün leksik boşluğunu azaltmaq üçün transliterasyon (skript dönüşünü) istifadəsini göstəririk. Daha sonra, biz çoxlu dil NMT sistemini istifadə edərək eyni ailənin dillərini istifadə edərək göstəririk.', 'ca': "La traducció multilingüe de màquines neuronales ha aconseguit un desempeny notable formant un únic model de traducció per múltiples llengües. Aquest article descriu la nostra presentació (ID de l'equip: CFILT-IITB) per a la MultiIndicMT: Una tasca multillengua de llenguatge índic al WAT 2021. Entrenem sistemes NMT multilingües compartint paràmetres de codificador i decodificador amb l'incorporació de llenguatge associada a cada fitxa tant en codificador com en decodificador. A més, demostram l'ús de la transliteració (conversió d'escriptura) per a les llengües índiques per reduir la diferència lèxica per formar un sistema multilingüe de MTN. També demostram millor rendiment formant un sistema multilingüe de MTN utilitzant llengües de la mateixa família, és a dir, llengües relacionades.", 'cs': 'Vícejazyčný neuronový strojový překlad dosáhl pozoruhodného výkonu tím, že trénuje jeden překladový model pro více jazyků. Tento článek popisuje náš příspěvek (Team ID: CFILT-IITB) pro MultiIndicMT: An Indic Language Multilingual Task na WAT 2021. Trénujeme vícejazyčné NMT systémy sdílením parametrů kodéru a dekodéru s jazykovým vložením spojeným s každým tokenem v kodéru i dekodéru. Dále demonstrujeme využití transliterace (konverze skriptů) pro indické jazyky při snižování lexikální mezery pro výcvik vícejazyčného NMT systému. Dále ukazujeme zlepšení výkonnosti školením vícejazyčného NMT systému používajícího jazyky stejné rodiny, tj. příbuzné jazyky.', 'et': 'Mitmekeelne neuraalne masintõlge on saavutanud märkimisväärse tulemuse, koolitades ühe tõlkemudeli mitme keele jaoks. Käesolevas artiklis kirjeldatakse meie esitamist (meeskonna ID: CFILT-IITB) programmi MultiIndicMT: India keele mitmekeelne ülesanne WAT 2021 raames. Koolitame mitmekeelseid NMT-süsteeme, jagades kodeerija ja dekooderi parameetreid keele manustamisega, mis on seotud iga märgiga nii kodeerijas kui dekooderis. Lisaks demonstreerime transliteratsiooni (skriptide teisendamise) kasutamist india keeltes mitmekeelse NMT süsteemi koolitamise leksikaalse lõhe vähendamisel. Lisaks näitame tulemuslikkuse paranemist mitmekeelse NMT süsteemi koolitamisega, mis kasutab sama perekonna keeli, st seotud keeli.', 'bs': 'Mnogjezički Neuralni prevod mašine postigao je izvanrednu funkciju obučavanjem jednog model a prevoda za višestruke jezike. Ovaj papir opisuje naše podatke (Tim ID: CFILT-IITB) za MultiIndicMT: Indijski multijezički zadatak na WAT 2021. Vježbamo multijezičke NMT sisteme podijeljenjem kodera i dekodera parametara sa ugrađenjem jezika povezanom sa svakim znakom u koderu i dekoderu. Osim toga, mi pokazujemo korištenje transliteracije (konvertacije skripta) za indijske jezike u smanjenju leksičkog praznika za obuku multijezičkog NMT sistema. Nadalje, pokazujemo poboljšanje učinka vježbanjem multijezičkog NMT-ovog sistema koristeći jezike iste porodice, tj. povezanih jezika.', 'fi': 'Monikielinen neurokonekäännös on saavuttanut huomattavan suorituskyvyn kouluttamalla yhden käännösmallin useille kielille. Tässä artikkelissa kuvataan artikkelimme (Team ID: CFILT-IITB) MultiIndicMT: An Indic Language Multilingual Task at WAT 2021. Koulutamme monikielisiä NMT-järjestelmiä jakamalla kooderi- ja dekooderiparametreja kieliupotuksella, joka liittyy kuhunkin tunnukseen sekä kooderiin että dekooderiin. Lisäksi demonstroimme transliteraation (skriptin muunnoksen) käyttöä indialaisissa kielissä monikielisen NMT-järjestelmän opetuksessa. Lisäksi näytämme suorituskyvyn parantuneen kouluttamalla monikielistä NMT-järjestelmää, jossa käytetään saman perheen kieliä eli sukulaisia kieliä.', 'jv': 'Suara Inggal Nyural Mas Terjamahan kang sampeyan akeh operasi sing mengko nggawe model terjamahan kanggo sak idiwang. Ngetong iki soko nggawe seneng nggawe cara-cara (Group ID: CFILE LT-IIBB) kanggo Multiindividual MT: an individual Language Multilanguage task at WAT 2020 1. Awak dhéwé luwih sistem multilanggar NMT seneng pisan koder karo ndekoder karo nggawe barang nggawe layar politenessoffpolite"), and when there is a change ("assertivepoliteness Lah, awak dhéwé menehi luwih cara-luwih kanggo nggawe sistem NMT sing wisata luwih lenggal, ta.d. uga luwih-luwih sing wis ana sak bantuan.', 'ha': "@ label Wannan takardan na faɗaɗa'inmu (Team ID: CFILT-IITB) wa multi-IndicMT: An Indic Lugha Multanci Taifa na WAT 2021. Tuna kõre wasu na'urar NMT-na'ura da yin shirin kodi da kode da ke cikin harshen wanda ke haɗa da duk ãyar da ke cikin kode da kode. Furan haka, za mu nuna amfani da littafin transliteratori (muhalli na manuscripta) wa harshen Indic cikin ƙarami da za'a ƙara gaura na littafin da aka yi wa amfani da na tsari na'urar NMT na'ura masu yawa. Furan, Munã nũna mafiya kyau ga aikin aiki da Muke tafiyar da wani na'ura na NMT na'ura da wasu harshe na'ura masu yawa, misali, harshen masu husũma.", 'sk': 'Večjezični nevralni strojni prevod je dosegel izjemno uspešnost z usposabljanjem enega samega prevajalskega modela za več jezikov. Ta prispevek opisuje našo predložitev (ID ekipe: CFILT-IITB) za MultiIndicMT: večjezična naloga indijskega jezika na WAT 2021. Usposabljamo večjezične NMT sisteme tako, da delimo parametre kodirnika in dekodirnika z vgradnjo jezika, povezano z vsakim žetonom v kodirniku in dekodirniku. Poleg tega smo prikazali uporabo transliteracije (pretvorbe skripta) za indijske jezike pri zmanjševanju leksikalne vrzeli pri usposabljanju večjezičnega NMT sistema. Poleg tega pokažemo izboljšanje učinkovitosti z usposabljanjem večjezičnega NMT sistema, ki uporablja jezike iste družine, tj. sorodne jezike.', 'bo': 'སྐད་རིགས་དབྱིན་མཐུན་གྱི་མ་ལག་འཁྱེར་གྱི་གཞུང་བསྒྱུར་ནི་སྟབས་བདེ་ཞིག་ཡིན་པ་ལས་སྐད་རིགས་གཅིག་ལས་ཕར་སྐད ཤོག་བྱང་འདིས་ང་ཚོའི་དབྱིབས་སྤྱིར་བཏང་བ་ཡིན་པ(Team ID: CFILT-IITB)for the MultiIndicMT: An Indic Language Multilingual Task at WAT 2021 We train multilingual NMT systems by sharing encoder and decoder parameters with language embedding associated with each token in both encoder and decoder. Furthermore, we demonstrate the use of transliteration (script conversion) for Indic languages in reducing the lexical gap for training a multilingual NMT system. ད་ལས་ཀྱང་། ང་ཚོར་སྐད་རིགས་དབྱེ་སྟངས་ཀྱི་སྤྱོད་སྟངས་མང་པོ་ཞིག་གིས་', 'he': 'Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for multiple languages.  This paper describes our submission (Team ID: CFILT-IITB) for the MultiIndicMT: An Indic Language Multilingual Task at WAT 2021.  אנו מאמן מערכות NMT רבות שפות על ידי שיתוף פארמטרים של קודד ופרמטרים של קודד עם קישור שפות הקשור לכל סימן גם בקודד וגם בקודר. Furthermore, we demonstrate the use of transliteration (script conversion) for Indic languages in reducing the lexical gap for training a multilingual NMT system.  Further, we show improvement in performance by training a multilingual NMT system using languages of the same family, i.e., related languages.'}
{'en': 'Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task S amsung  R & D  Institute  P oland submission to  WAT  2021 Indic Language Multilingual Task', 'ar': 'يقدم معهد سامسونج للبحث والتطوير في بولندا إلى مهمة متعددة اللغات للغة الهندية لعام 2021', 'fr': 'Soumission de Samsung R&D Institute Poland à la tâche multilingue WAT 2021 Indic Language', 'pt': 'Apresentação do Samsung R&D Institute Polônia para a tarefa multilíngue de idioma índico WAT 2021', 'es': 'Presentación del Samsung R&D Institute Poland para la tarea multilingüe de idiomas índicos', 'ja': 'サムスンR&amp;D研究所ポーランドWAT 2021インディク言語多言語タスクへの提出', 'zh': '三星研发研究所波兰WAT 2021印度语多言事', 'hi': 'सैमसंग आर एंड डी इंस्टीट्यूट पोलैंड ने डब्ल्यूएटी 2021 इंडिक लैंग्वेज बहुभाषी टास्क को प्रस्तुत किया', 'ru': 'Научно-исследовательский институт Samsung в Польше представил на рассмотрение многоязычную задачу WAT 2021 Indic Language Multilingual Task', 'ga': 'Aighneacht Samsung R&D Institute An Pholainn chuig Tasc Indeach Teanga Ilteangacha WAT 2021', 'el': 'Υποβολή του Ινστιτούτου Ε&Α Πολωνίας στην Ινδική Γλώσσα Πολυγλωσσική Εργασία', 'it': 'Il Samsung R&S Institute Polonia ha presentato il proprio progetto WAT 2021 Indic Language Multilingual Task', 'lt': 'Samsung MTTP Institute Poland submission to WAT 2021 Indic Language Multilingual Task', 'hu': 'A Samsung K+F Intézet Lengyelország benyújtása a WAT 2021 Indic Language Multilingual Task részére', 'ka': 'Samsung R&D ინსტისტიტუტი პოლინდია WAT 2021 ინდიური მრალიენგური დავალება', 'mk': 'Истражувачкиот Институт Samsung R&D Полска поднесува на WAT 2021 Индиски јазик мултијазична задача', 'kk': 'Samsung R&D институт Польша 2021 индикалық тіл көптілік тапсырмасына жіберу', 'ml': 'സാംസംഗ് R&ഡി ഇന്\u200dസ്റ്റിറ്റിട്ട് പോളണ്ട് വാട്ട് 2021 ഇന്ത്യൂട്ടിലേക്ക് സമര്\u200dപ്പിക്കുക', 'mn': 'Samsung R&D Institute Poland WAT 2021 Indic Language Multilingual Task', 'no': 'Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task', 'ms': 'Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task', 'mt': 'Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task', 'si': 'සැම්සන්ග් R&D සංස්ථානය පොල්ලෑන්ඩ් සංස්ථානය WAT 2021 ඉන්ඩික් භාෂාව ගොඩක් භාෂාවික ව', 'ro': 'Institutul de Cercetare și Dezvoltare Samsung Polonia a participat la misiunea multilingvă WAT 2021 Indic Language', 'sr': 'Samsung R&D Institut Poljska podnesenja na WAT 2021 Indički multijezički zadatak', 'ta': 'Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task', 'so': 'Samsung R&D Institute Poland submission to WAT 2021 Indic language Multilingual Task', 'pl': 'Samsung R&D Institute Polska zgłoszenie do WAT 2021 Język Indyczny Wielojęzyczne Zadanie', 'sv': 'Samsung R&D Institute Polen anmäler sig till WAT 2021 Indic Language Multilingual Task', 'ur': 'Samsung R&D Institute Poland submission to WAT 2021 Indian Language Multilingual Task', 'uz': "Samsung R&D Institute Polanda WAT 2021 Hindiston tillariga ko'plab tillar vazifasiga joʻnatish", 'vi': 'Công việc ngôn ngữ chung của nước biển Samsung.', 'bg': 'Институтът за научноизследователска и развойна дейност на Полша представи предложение за многоезична задача за индийски език', 'da': 'Samsung R&D Institute Polen indsender til WAT 2021 Indic Language Multilingual Task', 'nl': 'Samsung R&D Institute Polen inzending aan WAT 2021 Indische Taal Meertalige Taak', 'hr': 'Institut za istraživanje i razvoj Samsung Poljska podnesenje WAT 2021 Multilingual Task', 'id': 'Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task', 'ko': '삼성연구개발원 폴란드 WAT 2021 인도어 다국어 미션 제출', 'de': 'Samsung R&D Institute Poland Submission to WAT 2021 Indic Language Multilingual Task', 'sw': 'Samsung R&D Taasisi ya Poland imewasilisha kazi ya lugha ya Kihindi ya WAT 2021', 'tr': 'Samsung R&D Institute Polşa WAT 2021 Indiki diller köp diller taýýarlamak', 'fa': 'موسسه شناسایی و آزمایشگاه سامانگ لهستان تحویل به کار زیادی زبان هندی WAT 2021', 'af': 'Samsung R&D Institute Poland onderskrywing na WAT 2021 Indiese taal veelvuldige taak', 'am': 'ሳምsung R&D ኢንተርኔት ፖላንድ ወደ WAT 2021 የህንድ ቋንቋ ብዙ ቋንቋ ስራዎችን አቅርብ', 'az': 'Samsung R&D Institute Polonya WAT 2021 Hindi dil çoxlu dil işinə göndərilir', 'sq': 'Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task', 'bs': 'Samsung R&D Institut Poljska podnesenja WAT 2021 Multilingual Task', 'bn': 'স্যামসাঙ্গ R&D ইনস্টিটিউট পোল্যান্ড ওয়াট ২০২১ ইন্ডিক ভাষায় মাল্টিভাষার কাজ', 'cs': 'Společnost Samsung R&D Institute Polsko předložila WAT 2021 Indic Language Multilingual Task', 'fi': 'Samsung T&K Institute Poland -julkaisu WAT 2021 Indic Language Multilingual Task -ohjelmaan', 'ca': 'Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task', 'hy': 'Սամսունի Հետազոտություն և զարգացման ինստիտուտը Փոլլանդիան ներկայացնում է 2021 թ.', 'et': 'Samsungi teadus- ja arendustegevuse instituut Poola esitab WAT 2021 india keele mitmekeelse ülesande', 'sk': 'Samsung R&R Institute Poljska predložitev za večjezično nalogo WAT 2021 Indic Language', 'he': 'מוסד R&D Samsung פולין ההעברה למשימה רבת שפות של WAT 2021 לשפה אינדית', 'ha': 'Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task', 'jv': 'Saturdayg R&D institut polan kanggo metu WAT 2020 1 Mbak Language Multilanguage tasks', 'bo': 'Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task'}
{'en': 'This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R&D Institute Poland. The task covered translation between 10 Indic Languages (Bengali,  Gujarati ,  Hindi ,  Kannada ,  Malayalam ,  Marathi ,  Oriya ,  Punjabi ,  Tamil  and  Telugu ) and  English . We combined a variety of techniques :  transliteration , filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models. We applied an effective approach to low-resource training that consist of pretraining on backtranslations and tuning on parallel corpora. We experimented with two different domain-adaptation techniques which significantly improved translation quality when applied to monolingual corpora. We researched and applied a novel approach for finding the best  hyperparameters  for ensembling a number of  translation models . All techniques combined gave significant improvement-up to +8  BLEU  over baseline results. The quality of the  models  has been confirmed by the human evaluation where SRPOL models scored best for all 5 manually evaluated languages.', 'ar': 'تصف هذه الورقة التقديم إلى مهمة WAT 2021 الهندية متعددة اللغات من قبل معهد سامسونج للبحث والتطوير في بولندا. غطت المهمة الترجمة بين 10 لغات هندية (البنغالية ، الغوجاراتية ، الهندية ، الكانادية ، المالايالامية ، المهاراتية ، الأوريا ، البنجابية ، التاميلية والتيلجو) والإنجليزية. لقد قمنا بدمج مجموعة متنوعة من التقنيات: الترجمة الصوتية ، والتصفية ، والترجمة العكسية ، وتكييف المجال ، وتقطير المعرفة ، وأخيراً تجميع نماذج NMT. لقد طبقنا نهجًا فعالًا للتدريب منخفض الموارد يتكون من التدريب المسبق على الترجمات العكسية وضبط المؤسسات الموازية. لقد جربنا تقنيتين مختلفتين لتكييف المجال مما أدى إلى تحسين جودة الترجمة بشكل كبير عند تطبيقها على الشركات أحادية اللغة. لقد بحثنا وطبقنا نهجًا جديدًا للعثور على أفضل المعلمات الفائقة لتجميع عدد من نماذج الترجمة. أعطت جميع التقنيات مجتمعة تحسنًا كبيرًا - حتى +8 BLEU على نتائج خط الأساس. تم تأكيد جودة النماذج من خلال التقييم البشري حيث سجلت نماذج SRPOL أفضل النتائج لجميع اللغات الخمس التي تم تقييمها يدويًا.', 'fr': "Cet article décrit la soumission au WAT 2021 Indic Language Multilingual Task par le Samsung R&D Institute Poland. La tâche a couvert la traduction entre 10 langues indiennes (bengali, gujarati, hindi, kannada, malayalam, marathi, oriya, pendjabi, tamoul et télougou) et l'anglais. Nous avons combiné diverses techniques\xa0: translittération, filtrage, rétro-traduction, adaptation de domaine, distillation des connaissances et enfin assemblage de modèles NMT. Nous avons appliqué une approche efficace à la formation à faibles ressources qui consiste en une pré-formation sur les backtranslations et un réglage sur des corpus parallèles. Nous avons expérimenté deux techniques différentes d'adaptation de domaine qui ont considérablement amélioré la qualité de la traduction lorsqu'elles sont appliquées à des corpus monolingues. Nous avons étudié et appliqué une nouvelle approche pour trouver les meilleurs hyperparamètres permettant d'assembler un certain nombre de modèles de traduction. Toutes les techniques combinées ont permis une amélioration significative - jusqu'à +8 UEBL par rapport aux résultats de base. La qualité des modèles a été confirmée par l'évaluation humaine où les modèles SRPOL ont obtenu les meilleurs résultats pour les 5 langues évaluées manuellement.", 'pt': 'Este documento descreve o envio para a tarefa multilíngue de idioma índico WAT 2021 pelo Samsung R&D Institute Polônia. A tarefa abrangeu a tradução entre 10 línguas índicas (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil e Telugu) e Inglês. Combinamos uma variedade de técnicas: transliteração, filtragem, retrotradução, adaptação de domínio, destilação de conhecimento e, finalmente, ensembling de modelos NMT. Aplicamos uma abordagem eficaz ao treinamento com poucos recursos que consiste em pré-treinamento em retrotraduções e ajuste em corpora paralelos. Experimentamos duas técnicas diferentes de adaptação de domínio que melhoraram significativamente a qualidade da tradução quando aplicadas a corpora monolíngues. Pesquisamos e aplicamos uma nova abordagem para encontrar os melhores hiperparâmetros para agrupar vários modelos de tradução. Todas as técnicas combinadas deram uma melhora significativa - até +8 BLEU em relação aos resultados da linha de base. A qualidade dos modelos foi confirmada pela avaliação humana onde os modelos SRPOL pontuaram melhor para todos os 5 idiomas avaliados manualmente.', 'es': 'Este artículo describe la presentación a la tarea multilingüe del idioma índico WAT 2021 por parte del Instituto de I+D de Samsung de Polonia. La tarea abarcó la traducción entre 10 idiomas índicos (bengalí, gujarati, hindi, kannada, malayalam, marathi, oriya, punjabi, tamil y telugu) e inglés. Combinamos una variedad de técnicas: transliteración, filtrado, retrotraducción, adaptación de dominios, destilación de conocimientos y, finalmente, ensamblaje de modelos de NMT. Aplicamos un enfoque eficaz a la capacitación de bajos recursos que consiste en la formación previa en traducciones inversas y la puesta a punto de corpus paralelos. Experimentamos con dos técnicas diferentes de adaptación de dominio que mejoraron significativamente la calidad de la traducción cuando se aplicaron a cuerpos monolingües. Investigamos y aplicamos un enfoque novedoso para encontrar los mejores hiperparámetros para ensamblar una serie de modelos de traducción. Todas las técnicas combinadas proporcionaron una mejora significativa, hasta +8 BLEU con respecto a los resultados iniciales. La calidad de los modelos ha sido confirmada por la evaluación humana, en la que los modelos SRPOL obtuvieron mejores resultados en los 5 idiomas evaluados manualmente.', 'ja': '本稿では、Samsung R&amp;D Institute PolandによるWAT 2021 Indic Language Multilingual Taskへの提出について述べる。 このタスクは、10のインドの言語（ベンガル語、グジャラート語、ヒンディー語、カンナダ語、マラヤーラム語、マラーティー語、オリヤー語、パンジャーブ語、タミル語、テルグ語）と英語の間の翻訳をカバーしました。 私たちは、トランスリット、フィルタリング、バックトランスレーション、ドメイン適応、ナレッジディスティレーション、そして最後にNMTモデルのアンサンブルという、さまざまなテクニックを組み合わせました。 バックトランスレーションの事前トレーニングと平行コーパスのチューニングで構成される低リソーストレーニングに効果的なアプローチを適用しました。 私たちは、単一言語のコーラに適用すると翻訳品質を大幅に向上させる2つの異なるドメイン適応技術を実験しました。 私たちは、いくつかの翻訳モデルを組み立てるための最高のハイパーパラメータを見つけるための新しいアプローチを研究し、適用しました。 すべての技法を組み合わせると、ベースライン結果よりも最大+8のBLEUが有意に改善した。 モデルの質は、SRPOLモデルが手動で評価された5つの言語すべてで最高のスコアを記録したヒト評価によって確認されています。', 'zh': '本文述三星研发研究所波兰WAT 2021印度语多言事。 涵盖10种印度语(孟加拉语、古吉拉特语、印地语、卡纳达语、马拉雅拉姆语、马拉地语、奥里亚语、旁遮普语、泰米尔语、泰卢固语)、英语间译。 合诸术:音译,漉,回译,域,知蒸馏,终NMT合。 宜用一法于低资源培训,兼并行语料库之调。 试二异域以应术,当用于单语语料库,可以显重译质。 臣等考用一新法,以求整合诸译模形之最参数。 凡术合而得显善 - 比基线而高+8 BLEU。 凡模质已经人工评证,其SRPOL在诸5种手动评语得分最高。', 'hi': 'यह पेपर सैमसंग आर एंड डी इंस्टीट्यूट पोलैंड द्वारा डब्ल्यूएटी 2021 इंडिक लैंग्वेज बहुभाषी टास्क को प्रस्तुत करने का वर्णन करता है। इस कार्य में 10 भारतीय भाषाओं (बंगाली, गुजराती, हिंदी, कन्नड़, मलयालम, मराठी, उड़िया, पंजाबी, तमिल और तेलुगु) और अंग्रेजी के बीच अनुवाद शामिल था। हमने विभिन्न प्रकार की तकनीकों को जोड़ा: लिप्यंतरण, फ़िल्टरिंग, बैकट्रांसलेशन, डोमेन अनुकूलन, ज्ञान-आसवन और अंत में एनएमटी मॉडल का ensembling। हमने कम संसाधन प्रशिक्षण के लिए एक प्रभावी दृष्टिकोण लागू किया जिसमें बैकट्रांसलेशन पर प्रीट्रेनिंग और समानांतर कॉर्पोरेट पर ट्यूनिंग शामिल है। हमने दो अलग-अलग डोमेन-अनुकूलन तकनीकों के साथ प्रयोग किया, जो मोनोलिंगुअल कॉर्पोरेट पर लागू होने पर अनुवाद की गुणवत्ता में काफी सुधार करते हैं। हमने कई अनुवाद मॉडलों को घेरने के लिए सबसे अच्छा हाइपरपैरामीटर खोजने के लिए एक उपन्यास दृष्टिकोण पर शोध और लागू किया। सभी तकनीकों ने संयुक्त रूप से महत्वपूर्ण सुधार दिया - बेसलाइन परिणामों पर +8 BLEU तक। मॉडल की गुणवत्ता की पुष्टि मानव मूल्यांकन द्वारा की गई है जहां एसआरपीओएल मॉडल ने सभी 5 मैन्युअल रूप से मूल्यांकन की गई भाषाओं के लिए सबसे अच्छा स्कोर किया है।', 'ru': 'В этой статье описывается подача заявки на выполнение многоязычной задачи WAT 2021 Indic Language Multilingual Task от Института исследований и разработок Samsung в Польше. Задача заключалась в переводе с 10 языков индика (бенгальский, гуджаратский, хинди, каннада, малаялам, маратхи, ория, пенджабский, тамильский и телугу) на английский язык. Мы объединили различные методы: транслитерацию, фильтрацию, обратную трансляцию, адаптацию домена, дистилляцию знаний и, наконец, сборку моделей НМТ. Мы применили эффективный подход к малоресурсному обучению, который состоит из предварительного обучения обратным переводам и настройки на параллельные тела. Мы экспериментировали с двумя различными методами адаптации домена, которые значительно улучшили качество перевода при применении к одноязычным телам. Мы исследовали и применили новый подход к поиску лучших гиперпараметров для построения ряда моделей перевода. Все методы в совокупности дали значительное улучшение - до +8 БЛЮ по сравнению с исходными результатами. Качество моделей было подтверждено оценкой человеком, где модели SRPOL получили лучшие оценки для всех 5 языков, оцениваемых вручную.', 'ga': 'Déanann an páipéar seo cur síos ar an aighneacht do Thasc Ilteangach Teanga Indic WAT 2021 ó Samsung R&D Institute An Pholainn. Chuimsigh an tasc aistriúchán idir 10 dTeanga Indeacha (Beangáilis, Gúisearáitis, Hiondúis, Cannadais, Mailéalaimis, Maraitis, Oraisis, Puinseáibis, Tamailis agus Teileagúis) agus Béarla. Chuireamar teicníochtaí éagsúla le chéile: traslitriú, scagadh, aisaistriúchán, oiriúnú fearainn, driogadh eolais agus ar deireadh samhlacha NMT a chur le chéile. Chuireamar cur chuige éifeachtach i bhfeidhm maidir le hoiliúint íseal-acmhainne a chuimsigh réamhoiliúint ar aisaistriúcháin agus tiúnadh ar chorparáidí comhthreomhara. Bhaineamar triail as dhá theicníc éagsúla um oiriúnú fearainn a chuir feabhas suntasach ar cháilíocht an aistriúcháin nuair a cuireadh i bhfeidhm é ar chorpora aonteangacha. Rinneamar taighde agus chuireamar i bhfeidhm cur chuige nua chun na hipearpharaiméadair is fearr a aimsiú chun roinnt samhlacha aistriúcháin a chur le chéile. Tháinig feabhas suntasach ar na teicnící go léir a cuireadh le chéile - suas le +8 BLEU thar thorthaí bonnlíne. Deimhníodh cáilíocht na múnlaí sa mheastóireacht dhaonna, áit a bhfuair samhlacha SRPOL an scór is fearr do na 5 theanga a ndearnadh measúnú láimhe orthu.', 'ka': 'ამ დოკუმენტი განახსენებს WAT 2021 ინდენური მრავალენგური რაოდენობას პოლინდის Samsung R&D ინსტიტუტი. 10 ინდიური ენების (ბენდალის, დუჯაპატის, ჰინდის, კანნადის, მალაილამის, მარატის, ორია, პუნჯაბთ, რამილის და ტელუგის) და ანგლისური შორის შესახებ. ჩვენ განსხვავებული ტექნექციები შემყვებით: ტრანსლიტრაცია, ფილტრირება, საბოლოო გადაწყვეტა, დემომინის აკაპრაცია, ცნობიერების განსხვავება და საბოლოოდ NMT მო ჩვენ გამოყენეთ ეფექტიური პროგრამა, რომელსაც პარალელი კოპორაზე გადაწყვეტილება და გადაწყვეტილების შემთხვევაში. ჩვენ ექსპერიმენტირებდით ორი განსხვავებული დიომინის ადაპტიფიკაციის ტექნოგიებით, რომლებიც მნიშვნელოვანად უფრო უფრო უფრო უფრო უფრო უ ჩვენ შევასწავლიეთ და გამოყენეთ პრომენტის პროგრამის შესაძლებლობად საუკეთესო ჰიპერ პარამეტრების შესაძლებლობად, რამდენიმე მოდელების შესაძლებლობად ყველა შეერთებული ტექნექტიკები გავამუშავებენ მნიშვნელოვანი შესაძლებლობა - სამუშაო +8 BLEU-ის შემდეგ. მოდელების კაalitეტი იქნება ადამიანის განსაზღვრებით, სადაც SRPOL მოდელები ყველა ხუთი მანძილურად განსაზღვრებულია.', 'el': 'Η παρούσα εργασία περιγράφει την υποβολή στο έργο Ινδικής Γλώσσας από το Ινστιτούτο Έρευνας και Ανάπτυξης της Πολωνίας. Το έργο κάλυπτε τη μετάφραση μεταξύ 10 Ινδικών Γλωσσών (Μπενγκάλι, Γκουτζαράτι, Χίντι, Καννάντα, Μαλαγιάλαμ, Μαραθί, Ορίγια, Παντζάμπι, Ταμίλ και Τελούγκου) και Αγγλικών. Συνδυάσαμε ποικίλες τεχνικές: μεταγραφή, φιλτραρίσματος, οπισθοχώρηση, προσαρμογή τομέων, απόσταξη γνώσης και τελικά σύνθεση μοντέλων NMT. Εφαρμόσαμε μια αποτελεσματική προσέγγιση στην εκπαίδευση με χαμηλούς πόρους που συνίσταται στην προεπιλογή σε μεταγλώσεις και συντονισμό σε παράλληλα σώματα. Πειραματιστήκαμε με δύο διαφορετικές τεχνικές προσαρμογής τομέων οι οποίες βελτίωσαν σημαντικά την ποιότητα της μετάφρασης όταν εφαρμόστηκαν σε μονογλωσσικά σώματα. Ερευνήσαμε και εφαρμόσαμε μια νέα προσέγγιση για την εύρεση των καλύτερων υπερπαραμέτρων για τη σύνθεση ενός αριθμού μεταφραστικών μοντέλων. Όλες οι τεχνικές σε συνδυασμό έδωσαν σημαντική βελτίωση μέχρι +8 BLEU σε σχέση με τα αποτελέσματα της βάσης. Η ποιότητα των μοντέλων έχει επιβεβαιωθεί από την ανθρώπινη αξιολόγηση όπου τα μοντέλα βαθμολογήθηκαν καλύτερα για όλες τις 5 χειροκίνητα αξιολογούμενες γλώσσες.', 'hu': 'Ez a tanulmány bemutatja a Samsung K+F Intézet Lengyelország WAT 2021 Indic Language Multilingual Task előterjesztését. A feladat 10 indiai nyelv (Bengáli, Gudzsárati, Hindi, Kannada, Malayalam, Marathi, Oriya, Pundzsábi, Tamil és Telugu) és angol fordítására terjedt ki. Számos technikát kombináltunk: transzliteráció, szűrés, visszafordítás, domain adaptáció, tudás-desztilláció és végül NMT modellek összeállítása. Hatékony megközelítést alkalmaztunk az alacsony erőforrású képzésekre, amelyek a visszafordítások előkészítéséből és a párhuzamos corporák hangolásából állnak. Két különböző domain adaptációs technikával kísérleteztünk, amelyek jelentősen javították a fordítás minőségét az egynyelvű korpuszokra. Egy új megközelítést kutattunk és alkalmaztunk a legjobb hiperparaméterek megtalálására számos fordítási modell összeállításához. Valamennyi technika kombinálva jelentős javulást eredményezett - akár +8 BLEU a kiindulási eredményekhez képest. A modellek minőségét megerősítette az emberi értékelés, ahol az SRPOL modellek mind az 5 manuálisan értékelt nyelven a legjobban értékelték.', 'it': "Questo articolo descrive la presentazione al WAT 2021 Indic Language Multilingual Task da parte del Samsung R&S Institute Poland. Il compito ha riguardato la traduzione tra 10 lingue indiche (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil e Telugu) e l'inglese. Abbiamo combinato una varietà di tecniche: traslitterazione, filtraggio, backtranslation, adattamento del dominio, distillazione della conoscenza e infine assemblaggio di modelli NMT. Abbiamo applicato un approccio efficace alla formazione a basso contenuto di risorse che consiste nel pre-training sulle backtranslations e tuning sui corpi paralleli. Abbiamo sperimentato due diverse tecniche di adattamento del dominio che hanno migliorato significativamente la qualità della traduzione quando applicate ai corpora monolingue. Abbiamo studiato e applicato un approccio innovativo per trovare i migliori iperparametri per l'assemblaggio di un certo numero di modelli di traduzione. Tutte le tecniche combinate hanno dato un miglioramento significativo - fino a +8 BLEU rispetto ai risultati basali. La qualità dei modelli è stata confermata dalla valutazione umana in cui i modelli SRPOL hanno ottenuto il miglior punteggio per tutte e 5 le lingue valutate manualmente.", 'ms': 'Kertas ini menggambarkan penghantaran kepada Tugas Bahasa Indik WAT 2021 oleh Samsung R&D Institute Poland. Tugas ini meliputi terjemahan antara 10 Bahasa India (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil dan Telugu) dan bahasa Inggeris. We combined a variety of techniques: transliteration, filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models.  We applied an effective approach to low-resource training that consist of pretraining on backtranslations and tuning on parallel corpora.  Kami eksperimen dengan dua teknik penyesuaian-domain yang berbeza yang meningkatkan kualiti terjemahan secara signifikan apabila dilaksanakan pada korpra monobahasa. Kami menyelidiki dan melaksanakan pendekatan baru untuk mencari hyperparameter terbaik untuk mengumpulkan sejumlah model terjemahan. Semua teknik bergabung memberikan peningkatan yang signifikan - sehingga +8 BLEU atas hasil asas. Kualiti model telah disahkan oleh penilaian manusia di mana model SRPOL mencetak skor terbaik untuk semua 5 bahasa yang diteliti secara manual.', 'kk': 'Бұл қағаз 2021 жылы "Самбус R&D Институтты Польша" Институтты бағыттау үшін "WAT 2021" индикалық тілдер көп тілді тапсырмасына жіберілген тапсырманы та Тапсырма 10 индикалық тілдер арасында аударылған (Бенгали, Гуджарати, хинди, Каннада, Малайалам, Марати, Ория, Пунджаби, Тамил және Телугу) және ағылшын тілдер арасында. Біз бірнеше түрлі техникаларды біріктірдік: транслитерация, сүзгі, артқа аудару, доменнің адаптациясы, білім- дистриляциясы және соңында NMT үлгілерін ендіру. Барлық корпораға арналған аудармаларды және баптауларды бақылау үшін төмен ресурстардың оқытуына эффективті тәртібін қолдандық. Біз монолингі корпораға қолданғанда аудармалардың сапасын өзгертілген екі түрлі доменді адаптациялау техникаларымен тәжірибедік. Біз бірнеше аудармалар үлгілерін табу үшін жақсы гиперпараметрлерді іздеу үшін романдық тәртібін қолдандық. Бүкіл біріктірілген технологиялар негізгі нәтижелерден +8 BLEU дегенге көп жақсартылды. Үлгілердің сапасы, SRPOL үлгілері барлық 5 тілдерге қолмен бағалаған тілдер үшін ең жақсы нәтижесін бағалады.', 'lt': 'Šiame dokumente aprašomas Lenkijos mokslinių tyrimų ir technologijų plėtros instituto „Samsung“ pateiktas „WAT 2021“ Indic Language Multilingual Task“. Ši užduotis buvo susijusi su vertimu tarp 10 indinių kalbų (bengalų, gujaračio, Hindi, Kanados, Malayalamo, maračio, Orijos, Pundžabino, tamalio ir telugės) ir anglų kalbomis. Sujungėme įvairius metodus: transliteraciją, filtravimą, grįžtamąjį vertimą, srities pritaikymą, žinių distiliavimą ir galiausiai NMT modelių rinkimą. We applied an effective approach to low-resource training that consist of pretraining on backtranslations and tuning on parallel corpora.  Eksperimentavome su dviem skirtingais srities pritaikymo metodais, kurie gerokai pagerino vertimo kokybę taikant monokalbinę korporą. Mes ištyrėme ir taikėme naują metodą, kad rastume geriausius hiperparatorius keliems vertimo modeliams surinkti. Visi derinti metodai gerokai pagerėjo – iki +8 BLEU, palyginti su pradiniais rezultatais. Modelių kokybę patvirtino žmogaus vertinimas, kuriame SRPOL modeliai geriausiai įvertino visose penkiose rankiniu būdu įvertintose kalbose.', 'mk': 'This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R&D Institute Poland.  Оваа задача покрива превод помеѓу 10 индиски јазици (Бенгали, Гујарати, Хинди, Канада, Малајалам, Марати, Орија, Пунџаби, Тамили и Телугу) и англиски. We combined a variety of techniques: transliteration, filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models.  Апликавме ефикасен пристап кон тренингот со ниски ресурси кој се состои од претренинг на преведувањата и прилагодување на паралелната корпора. Експериментиравме со две различни техники за адаптација на домените кои значително го подобрија квалитетот на преводот кога се применуваат на монојазични корпора. Истражувавме и применивме нов пристап за најдување на најдобрите хиперпараметри за собирање на број преводни модели. Сите техники заедно дадоа значително подобрување - до +8 БЛЕУ во однос на резултатите од основата. Квалитетот на моделите е потврден од човечката проценка каде што моделите на СРПОЛ најдобро постигнаа оценки за сите 5 рачно проценети јазици.', 'mt': 'Dan id-dokument jiddeskrivi s-sottomissjoni lill-Ħidma Multilingwi tal-Lingwa Indika tal-WAT 2021 mill-Istitut tar-Riċerka u l-Iżvilupp Samsung tal-Polonja. Il-kompitu kopra t-traduzzjoni bejn 10 Lingwi Indiċi (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil u Telugu) u l-Ingliż. Kombinajna varjetà ta’ tekniki: trasliterazzjoni, filtrazzjoni, traduzzjoni b’lura, adattament tad-dominju, distillazzjoni tal-għarfien u finalment ġabra ta’ mudelli NMT. Applikajna approċċ effettiv għat-taħriġ b’riżorsi baxxi li jikkonsisti minn taħriġ minn qabel fuq traduzzjonijiet ta’ wara u aġġustament fuq korpora parallela. Esperimentajna b’żewġ tekniki differenti ta’ adattament tad-dominju li tejbu b’mod sinifikanti l-kwalità tat-traduzzjoni meta applikati għal korpura monolingwi. Irriċerkajna u applikajna approċċ ġdid biex jinstabu l-a ħjar parametri iperparatteristiċi għall-ġabra ta’ għadd ta’ mudelli ta’ traduzzjoni. It-tekniki kollha kkombinati taw titjib sinifikanti - sa +8 BLEU fuq ir-riżultati tal-linja bażi. Il-kwalità tal-mudelli ġiet ikkonfermata mill-evalwazzjoni umana fejn il-mudelli SRPOL kisbu l-aħjar punteġġ għall-ħames lingwi kollha evalwati manwalment.', 'no': 'Denne papiret beskriver søknaden til vanskespråk 2021 av Samsung R&D Institute Poland i India. Oppgåva dekket omsetjinga mellom 10 indiske språk (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil og Telugu) og engelsk. Vi kombinerte fleire teknikk: transliterasjon, filtrering, tilbakeomsetjing, domenetilpassing, kunnskap-distillasjon og til slutt ensemblering av NMT-modeller. Vi har brukt ein effektiv tilnærming til låg ressursøving som inneheld av å trekke tilbakeomsetjingar og tilbakestillingar på parallelle korpora. Vi eksperimenterte med to forskjellige domeneadaptasjonssteknikk som betydelig forbetra omsetjingskvalitet når det er brukt til monospråk korpora. Vi forsøkte og brukte ein novel tilnærming for å finna dei beste hyperparametra for å gjera eit tal omsetjingsmodular. Alle teknikke kombinerte gjev signifikante forbedring – opp til +8 BLEU over baseline resultat. Kvaliteten på modelane er stadfestig av menneskelige evaluering der SRPOL-modeller viste for alle 5 manuelt evaluerte språk.', 'mn': 'Энэ цаас 2021 оны WAT хэлний олон хэлний ажлыг Самбун R&D Институтын Польшад тайлбарлаж байна. Энэ үйл ажил нь 10 индийн хэл (Бенгали, Гуджарати, Хинди, Каннада, Малайамь, Марати, Ория, Пунджаби, Тамил, Телугу) болон Англи хэл хооронд орчуулсан. Бид олон төрлийн техникуудыг нэгтгэсэн: transliteration, filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models. Бид бага эдийн засгийн сургалтын тулд үр дүнтэй арга зам ашигласан. Энэ нь backtranslation, tuning on parallel corpora юм. Бид нэг хэлний корпора дээр хэрэглэх үед орчуулах чадварыг үнэхээр сайжруулсан хоёр өөр өөр зохион байгуулах техникуудыг туршиж үзсэн. Бид олон орчуулах загваруудыг олох хамгийн сайн гиперпараметрлүүдийг судалж, шинэ арга загварыг ашигласан. Бүх техникууд нийлүүлсэн нь үндсэн сайжруулалт гаргасан - үндсэн үр дүн дээр +8 BLEU хүртэл. Загварын сайн чанарыг хүн төрөлхтний дүгнэлт нь хүн төрөлхтний тооцоололтой баталсан. СРPOL загварууд 5 гарын дүгнэлт гарсан хэлний хувьд хамгийн сайн тооцоолж байсан.', 'ml': 'ഈ പത്രത്തില്\u200d WAT 2021 ഇൻഡിക്ക് ഭാഷ കൊടുക്കുന്നതിനെ വിശദീകരിക്കുന്നു. സാംസംഗ് R&ഡി ഇന്റിസ്റ്റിറ്റീറ്റ് പോളണ 10 ഇൻഡിക് ഭാഷകളുടെ ഇടയിലുള്ള പ്രഭാഷണം പൂര്\u200dത്തിയാക്കിയിരിക്കുന്നു. നമ്മള്\u200d വ്യത്യസ്ത തികവിദ്യകളെ കൂട്ടിചേര്\u200dത്തിരിക്കുന്നു: ട്രാന്\u200dസ്ലിറ്റരേഷന്\u200d, ഫില്\u200dറ്റര്\u200dട്ടിങ്ങ്, ബാക്ക്ട്രിന്\u200dഷന്\u200d,  കുറഞ്ഞ വിഭവങ്ങളുടെ പരിശീലനത്തിലേക്ക് ഞങ്ങള്\u200d ഒരു പ്രായോഗ്യം പ്രയോഗിച്ചു. അത് പിന്നിലെ ഭാഷകങ്ങളിലേക്ക് പെയ്യുന് നമ്മള്\u200d രണ്ടു വ്യത്യസ്ത ഡൊമെയിന്\u200d അഡാപ്റ്റേഷന്\u200d ടെക്കിനിക്കുകള്\u200d കൊണ്ട് പരീക്ഷിച്ചു. അത് മോണോളില്\u200d ഭാഷ കോര്\u200dപ്പോര നമ്മള്\u200d പരിശോധിച്ചു ഒരു നോവലിന്റെ പ്രോഗത്തില്\u200d പ്രയോഗിച്ചു കൊണ്ടിരുന്നു അത്യുത്തമമായ ഹൈപ്പര്\u200dപാരാമീറ്റര്\u200d  എല്ലാ സാങ്കേതികവിദ്യ കൂട്ടത്തിലുമുള്ള മെച്ചപ്പെടുത്തിയിരുന്നു - ബേസ്ലൈന്\u200d ഫലങ്ങള്\u200dക്ക് മുകളില്\u200d  മോഡലുകളുടെ ഗുണവും മനുഷ്യരുടെ വിലാസവും സ്ഥിരപ്പെടുത്തിയിരിക്കുന്നു. എസ്ആര്\u200dപോഎല്\u200d മോഡലുകള്\u200d എല്ലാ 5 ഭാഷകള്\u200dക്കും മുന', 'pl': 'Niniejszy artykuł opisuje zgłoszenie do WAT 2021 Indic Language Multilingual Task przez Samsung R&D Institute Poland. Zadanie obejmowało tłumaczenie pomiędzy 10-językami indyjskimi (bengalski, gujarati, hindi, kannada, malajalam, marathi, oriya, pundżabi, tamil i telugu) a angielskim. Połączyliśmy różnorodne techniki: transliterację, filtrowanie, backtranslację, adaptację domeny, destylację wiedzy i wreszcie zestaw modeli NMT. Zastosowaliśmy skuteczne podejście do szkoleń o niskich zasobach, które polegają na wstępnym treningu na tłumaczeniach backtranslation i dostrajaniu korpusów równoległych. Eksperymentowaliśmy z dwoma różnymi technikami adaptacji domen, które znacząco poprawiły jakość tłumaczenia w przypadku korpusów jednojęzycznych. Zbadaliśmy i zastosowaliśmy nowatorskie podejście do znalezienia najlepszych hiperparametrów do zespołu wielu modeli tłumaczeniowych. Wszystkie techniki w połączeniu dały znaczącą poprawę do +8 BLEU w porównaniu z wynikami wyjściowymi. Jakość modeli została potwierdzona przez ludzi oceną, w której modele SRPOL osiągnęły najlepsze wyniki dla wszystkich pięciu ręcznie ocenianych języków.', 'ro': 'Această lucrare descrie depunerea la misiunea multilingvă de limbă indică WAT 2021 de către Institutul de Cercetare și Dezvoltare Samsung Polonia. Sarcina a vizat traducerea între 10 limbi indice (bengali, gujarati, hindi, kannada, malayalam, marathi, Oriya, punjabi, tamil și telugu) și engleză. Am combinat o varietate de tehnici: transliterare, filtrare, backtranslation, adaptarea domeniului, distilarea cunoștințelor și, în cele din urmă, ansamblarea modelelor NMT. Am aplicat o abordare eficientă a instruirii cu resurse reduse care constau în pregătirea pe traduceri înapoi și tuning pe corpore paralele. Am experimentat două tehnici diferite de adaptare a domeniului care au îmbunătățit semnificativ calitatea traducerii atunci când sunt aplicate corporelor monolingve. Am cercetat și aplicat o abordare nouă pentru găsirea celor mai buni hiperparametri pentru ansamblarea unui număr de modele de traducere. Toate tehnicile combinate au dat o îmbunătăţire semnificativă - până la +8 BLEU faţă de rezultatele iniţiale. Calitatea modelelor a fost confirmată de evaluarea umană în care modelele SRPOL au obținut cel mai bun scor pentru toate cele 5 limbi evaluate manual.', 'sr': 'Ovaj papir opisuje podnošenje Indijskog multijezičkog zadatka za WAT 2021 od Instituta Samsung R&D Poljske. Zadatak je pokriven prevodom između 10 Indijskih jezika (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil i Telugu) i engleskog jezika. Kombinirali smo razne tehnike: transliteracija, filtriranje, prevod, adaptacija domena, destilacija znanja i konačno osiguranje NMT modela. Prijavili smo efikasan pristup obuci niskih resursa koji se sastoji od pretvaranja na prevode i prilagodbe paralelnog korpora. Eksperimentirali smo sa dve različite tehnike adaptacije domena koje su značajno poboljšale kvalitet prevoda kada su primjenjene na monojezičku korporu. Istraživali smo i primijenili novi pristup za pronalaženje najboljih hiperparametara za uključenje broja modela prevođenja. Svi kombinirani tehnici pružali su značajno poboljšanje - do +8 BLEU zbog početnih rezultata. Kvaliteta modela je potvrđena ljudskim procjenama gde su modeli SRPOL najbolje procenili za sve 5 ručno procenih jezika.', 'ta': 'This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R&D Institute Poland.  The task covered translation between 10 Indic Languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil and Telugu) and English.  நாங்கள் பல்வேறு தொழில்நுட்பங்களை இணைத்தோம்: மாற்றுதல், வடிகட்டி, பின்மொழிப்பெயர்ப்பு, டொமைன் ஒழுங்குதல், அறிவு பிரிப்பு, மற்ற நாங்கள் குறைந்த மூலத்திற்கு பயிற்சியை பயன்படுத்தினோம் அது பின் மொழிபெயர்ப்பு மொழிபெயர்ப்பு மற்றும் இணைப்பு கோர நாங்கள் இரண்டு வேறு வித்தியாசமான களம் ஒதுக்கும் தொழில்நுட்பத்தை சோதித்தோம். அது மொழிமாற்று தரம் மொழிபெயர்ப்பு ம மொழிபெயர்ப்பு மாதிரிகளை சிறந்த அளபுருக்களை கண்டுபிடிக்க நாங்கள் ஆராய்ச்சி செய்து புதிய முறைமையை பயன்படுத அனைத்து தொழில்நுட்ப முறைகளும் முக்கியமான முன்னேற்றத்தை கொடுத்தது -அடிப்படைக்கோட்டின் முடிவு மாதிரிகளின் தரம் மனித மதிப்பினால் உறுதிப்படுத்தப்பட்டது எஸ்ஆர்போஎல் மாதிரிகள் எல்லா 5 மொழிகளுக்கும் கைமுறைமையாக மத', 'si': 'මේ පත්තේ WAT 2021 ඉන්ඩික් භාෂාවක් ගොඩක් භාෂාවක් වැඩේ විස්තර කරනවා සැමස්මින් R&D සංස්ථාපනය පොල්ලෑ වැඩේ ඉන්දික භාෂාවක් 10 (බෙන්ගාලි, ගුජරාටි, හින්දි, කැන්නාඩා, මලායාලාම්, මාරාති, ඔරියා, පුන්ජාබි, ටැමිල් හා ටෙලුගු වල අ අපි විවිධ ප්\u200dරවේශයක් සම්බන්ධ කළා: ප්\u200dරවේශනය, පසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසු අපි පසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපසුපස අපි පරීක්ෂණය කළා වෙනස් ඩෝමින් එක්ක භාෂාවික කොර්පෝරා වලට භාෂාවිත විශේෂයෙන් විශේෂයෙන්  අපි පරීක්ෂණය කළා සහ ප්\u200dරවේශනයක් ප්\u200dරවේශනය කළා සඳහා හොඳම හායිපර් ප්\u200dරමාණයක් හොයාගන්න හොයාගත්තා. සම්බන්ධ විද්\u200dයාවයේ සියලුම විශේෂ ප්\u200dරවෘත්තියක් වැඩ කරලා තියෙනවා - මූලික ප්\u200dරතිප්\u200dරතිප්\u200d මොඩල් වලගේ කුණුත්වය තහවුරු කරලා තියෙනවා මිනිස්සු විශ්ලේෂණයෙන් SRPOL මොඩල් වලට හොඳම අවශ්\u200dය 5 කරලා තියෙනව', 'so': 'Kanu warqaddaas wuxuu ku qoran warqada loo soo dhiibay WAT 2021 Maamulka luqada ee muusikada ee Samsung R&D Institute Poland. Shaqada waxaa ku qoran turjumista 10 luuqadaha Indic (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil iyo Telugu) iyo Ingiriis. Waxyaabo kala duduwan ayaan isugu soo bandhignay: baaritaanka, baaritaanka, dib-tarjumidda, bedelka deegaanka, aqoonta iyo ugu dambaysta sameynta modellada NMT. Waxaannu dalbannay qaab faa’iido ah oo ku saabsan waxbarashada hoose-resource, kaas oo ah mid ka mid ah bakteriga turjumaadda iyo sameynta korporada lambarka ah. Waxaannu ku jirrabnay laba qalabka isbedelka ee gudaha oo kala duduwan, taasoo aad u bedeshay qiimaha turjumidda marka lagu codsaday shirkadaha muuqaalka ah. Waxaannu baaraynay oo codsanay qaab saxda ah si aan u helno heerarka ugu wanaagsan ee aad u sameynayso tusaalooyin turjuman. All techniques combined gave significant improvement - up to +8 BLEU over baseline results.  Tusaalada waxaa lagu xaqiijiyey qiimeynta biniaadaha SRPOL ee lagu qiimeeyo dhammaan shanta luuqadood oo si rasmi ah.', 'ur': 'This paper describes the submission to the WAT 2021 Indian Language Multilingual Task by Samsung R&D Institute Poland. 10 انڈی زبانوں کے درمیان (بنگالی, گوجراتی, ہندی, کانڈا, مالایالام, ماراتی, اوریا, پنجابی, تامیل اور ٹولوگو) اور انگلیسی کے درمیان ترجمہ کا پورا کیا گیا ہے۔ ہم نے طرح طرح طرح طرح کی تکنیک جمع کیے: ٹرانسلیٹ، فیلٹرینگ، پچھلی ترجمہ، ڈومین ترجمہ، علم-جدائی اور بالآخر NMT موڈل کے پیدا کیے۔ ہم نے کم سرمایہ کی تعلیم کے لئے ایک عمدہ طریقہ لازم کیا ہے جو پچھلی تعلیم اور سائل کورپور پر ترینگ کرنے کے ساتھ ہے۔ ہم نے دو مختلف ڈومین کی تعمیر تکنیک کے ساتھ آزمائش کی جو ایک زبان کوپرا کے لئے استعمال ہونے کے بعد زیادہ ترجمہ کیفیت کو بہتر کر دیا۔ ہم نے ایک نئی طریقہ کا تحقیق کیا اور ایک نئی طریقہ لایا کہ بہترین ہیر پارامیٹر تلاش کرنے کے لئے بہترین ترجمہ موڈل کو جمع کرنے کے لئے۔ سارے ٹیکنیکوں کو جمع کیا گیا تھا، بنیاس لین نتیجے پر +8 بلیوس تک اضافہ کیا گیا۔ مدلکوں کی کیفیت انسان کی ارزیابی کے ذریعہ مطمئن ہوئی ہے جہاں SRPOL مدلکوں نے تمام پنج زبانوں کے لئے اچھا ارزیابی کیا ہے۔', 'sv': 'Denna uppsats beskriver inlämningen till WAT 2021 Indic Language Multilingual Task av Samsung R&D Institute Polen. Uppgiften omfattade översättning mellan 10 indiska språk (bengali, gujarati, hindi, kannada, malayalam, marathi, Oriya, punjabi, tamil och telugu) och engelska. Vi kombinerade en mängd olika tekniker: transliteration, filtrering, backtranslation, domänanpassning, kunskapsdestillation och slutligen ensemblering av NMT-modeller. Vi tillämpade ett effektivt tillvägagångssätt för utbildning med låg resurs som består av fortbildning på backtranslations och tuning på parallella corpora. Vi experimenterade med två olika domänanpassningstekniker som avsevärt förbättrade översättningskvaliteten när de applicerades på enspråkiga corpora. Vi undersökte och tillämpade ett nytt tillvägagångssätt för att hitta de bästa hyperparametrarna för att sammanställa ett antal översättningsmodeller. Alla tekniker kombinerade gav signifikant förbättring - upp till +8 BLEU jämfört med baslinjen resultat. Modellernas kvalitet har bekräftats av den mänskliga utvärderingen där SRPOL-modellerna fick bäst betyg för alla 5 manuellt utvärderade språk.', 'vi': 'Tờ giấy này mô tả việc viện nghiên cứu và phát ngôn ngữ riêng của Samsung. Nhiệm vụ là dịch vụ giữa mười ngôn ngữ Ấn (Bengali, Gujarati, Hidi, Kannada, Malayaham, Marath, Oriya, Punjabi, Tamil và Telug) và Anh. Chúng tôi đã kết hợp rất nhiều kỹ thuật: chuyển dạng, lọc, bản dịch ngược, sửa chữa miền, chưng cất kiến thức và kết hợp các mô hình NMB. Chúng tôi đã áp dụng một phương pháp hiệu quả cho việc đào tạo ít tài nguyên bao gồm việc gấp lại bản dịch và độ hợp nhất. Chúng tôi đã thí nghiệm với hai kỹ thuật sửa chữa miền khác nhau. Cách này đã cải thiện chất lượng dịch thuật được áp dụng cho tàu ngầm. Chúng tôi đã nghiên cứu và sử dụng một phương pháp mới để tìm ra các siêu tham số tốt nhất để kết hợp với một số mô hình dịch. Tất cả các kỹ thuật kết hợp đã tiến triển đáng kể từ đầu đến May trên kết quả cơ bản. Chất lượng của các mô hình đã được xác nhận bởi đánh giá con người nơi mô hình Sxoay được đánh dấu tốt nhất cho cả năm ngôn ngữ được đánh giá bằng tay.', 'uz': "Name Name Biz bir necha teknikalarni birlashtirdik: transliteratsiya, filterlash, backtarjima tarjima, domen adaptation, илм-ajratish va oxirigi NMT modellarini birlashtirish. Biz qanchalik manbalar tajribasini qo'llab berdik. Bu ta'lim tarjimalarni tajriba qilish va parallel kompaniyada o'zgartirish mumkin. Biz ikki boshqa domen-adaptsiya texnologiyani o'rganib ko'rib chiqardik. Bu o'zgarishni o'zgartirish imkoniyatini o'zgartiradi. We researched and applied a novel approach for finding the best hyperparameters for ensembling a number of translation models.  All techniques combined gave significant improvement - up to +8 BLEU over baseline results.  Modellarning sifatida, inson qiymatlari bilan SRPOL modellari hamma 5 tilning qoʻlbola qiymatlari uchun eng yaxshi qiymatga ishlatiladi.", 'da': 'Dette dokument beskriver indsendelsen til WAT 2021 Indic Language Multilingual Task fra Samsung R&D Institute Polen. Opgaven omfattede oversættelse mellem 10 indiske sprog (bengali, gujarati, hindi, kannada, malayalam, marathi, Oriya, punjabi, tamil og telugu) og engelsk. Vi kombinerede en række teknikker: transliteration, filtrering, backtranslation, domænetilpasning, viden-destillation og endelig sammensætning af NMT modeller. Vi anvendte en effektiv tilgang til lav ressource træning, der består af forudtræning på backtranslations og tuning på parallelle corpora. Vi eksperimenterede med to forskellige domænetilpasningsteknikker, som betydeligt forbedrede oversættelseskvaliteten, når de blev anvendt på ensprogede corpora. Vi undersøgte og anvendte en ny tilgang til at finde de bedste hyperparametre til sammensætning af en række oversættelsesmodeller. Alle teknikker kombineret gav signifikant forbedring - op til +8 BLEU i forhold til baseline resultater. Modellernes kvalitet er blevet bekræftet af den menneskelige evaluering, hvor SRPOL modeller scorede bedst for alle 5 manuelt evaluerede sprog.', 'hr': 'Ovaj papir opisuje prijedlog Mnogjezičkog zadatka Indijskog jezika za WAT 2021 od strane Instituta Samsung R&D Poljske. Zadatak je pokriven prevodom između 10 Indijskih jezika (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil i Telugu) i engleskog jezika. Kombinacija smo raznih tehnika: transliteracija, filtriranje, prevod, adaptacija domena, destilacija znanja i konačno osiguranje NMT modela. Primjenili smo učinkoviti pristup obuci niskih resursa koji se sastoji od pretvaranja povratnih prevoda i prilagođenja paralelnim tijelom. Eksperimentirali smo s dvije različite tehnike adaptacije domena koje su značajno poboljšale kvalitet prevoda kada su primjenjene na monojezičku korporu. Istraživali smo i primijenili novi pristup za pronalaženje najboljih hiperparametara za osiguranje broja modela prevoda. Sva kombinirana tehnika dala je značajno poboljšanje - do +8 BLEU zbog početnih rezultata. kvalitet modela je potvrđena ljudskim procjenama gdje su modeli SRPOL najbolje procijenili za sve 5 ručno procijenjenih jezika.', 'nl': 'Dit artikel beschrijft de inzending aan de WAT 2021 Indic Language Multilingual Task door Samsung R&D Institute Poland. De taak bestond uit vertalingen tussen 10-Indische talen (Bengaals, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil en Telugu) en Engels. We combineerden verschillende technieken: transliteratie, filtering, backtranslation, domeinadaptatie, kennisdestillatie en tenslotte ensembling van NMT modellen. We hebben een effectieve aanpak toegepast voor low-resource training die bestaat uit pretraining op backtranslations en tuning op parallelle corpora. We experimenteerden met twee verschillende domeinadaptatietechnieken die de vertaalkwaliteit aanzienlijk verbeterde bij toepassing op eentalige corpora. We hebben een nieuwe aanpak onderzocht en toegepast voor het vinden van de beste hyperparameters voor het samenstellen van een aantal vertaalmodellen. Alle technieken gecombineerd gaven significante verbetering tot +8 BLEU ten opzichte van baseline resultaten. De kwaliteit van de modellen is bevestigd door de menselijke evaluatie waarbij SRPOL-modellen het beste scoorden voor alle vijf handmatig geëvalueerde talen.', 'bg': 'Настоящата статия описва представянето на многоезичната задача за индийски език от Института за научноизследователска и развойна дейност на Полша. Задачата обхваща превод между 10 индийски езика (бенгалски, гуджарати, хинди, каннада, малаялам, марати, ория, пънджаби, тамилски и телугу) и английски език. Комбинирахме различни техники: транслитерация, филтриране, обратен превод, адаптация на домейна, знание-дестилация и накрая ансамблиране на модели НМТ. Приложихме ефективен подход към нискоресурсно обучение, което се състои в предобучение на обратни преводи и настройка на паралелни корпуси. Експериментирахме с две различни техники за адаптация на домейна, които значително подобриха качеството на превода, когато се прилагат към едноезични корпуси. Проучихме и приложихме нов подход за намиране на най-добрите хиперпараметри за съставяне на редица модели на превод. Всички техники комбинирани дават значително подобрение - до +8 спрямо изходните резултати. Качеството на моделите е потвърдено от човешката оценка, при която моделите имат най-добри резултати за всички 5 ръчно оценени езика.', 'ko': '본고는 삼성폴란드연구개발원이 WAT 2021 인도어 다국어 임무에 제출한 상황을 묘사한다.이 임무는 인도어 10종(방글라데시, 구자라트어, 인디어, 카나다어, 말레이시아어, 마라디어, 올리아어, 방차프어, 테밀어와 테루고어)과 영어 간의 번역을 포함한다.우리는 다양한 기술을 결합시켰다. 음역, 필터, 반역, 영역 적응, 지식 추출, 마지막으로 NMT 모델의 융합이다.우리는 저자원 교육에서 효과적인 방법을 채택했는데, 반역에 대한 예비 교육과 평행어 자료 라이브러리를 조정하는 것을 포함한다.우리는 두 가지 서로 다른 분야의 적응 기술을 시험했는데 단어 어료 라이브러리에 응용할 때 번역의 질을 현저히 향상시켰다.우리는 여러 개의 번역 모델을 통합시키기 위해 새로운 방법을 연구하고 응용했다.모든 기술의 결합은 기선 결과보다 +8 BLEU 높은 현저한 개선을 가져왔다.인공 평가는 모델의 질을 증명했는데 그 중에서 SRPOL 모델은 모든 5가지 수동 평가 언어 중 가장 높은 점수를 받았다.', 'de': 'Dieses Papier beschreibt die Einreichung zur WAT 2021 Indic Language Multilingual Task durch Samsung R&D Institute Poland. Die Aufgabe umfasste Übersetzungen zwischen 10-indischen Sprachen (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil und Telugu) und Englisch. Wir kombinierten eine Vielzahl von Techniken: Transliteration, Filterung, Rückübersetzung, Domänenanpassung, Wissensdestillation und schließlich Ensemble von NMT-Modellen. Wir haben einen effektiven Ansatz für ressourcenschonendes Training angewendet, das aus Vortraining auf Backtranslations und Tuning auf parallelen Korpora besteht. Wir experimentierten mit zwei verschiedenen Domänenanpassungstechniken, die die Übersetzungsqualität signifikant verbesserten, wenn sie auf einsprachige Korpora angewendet wurden. Wir haben einen neuartigen Ansatz erforscht und angewendet, um die besten Hyperparameter für die Zusammenstellung einer Reihe von Übersetzungsmodellen zu finden. Alle kombinierten Techniken zeigten eine signifikante Verbesserung von bis zu +8 BLEU gegenüber den Ausgangsergebnissen. Die Qualität der Modelle wurde durch die menschliche Evaluation bestätigt, bei der SRPOL-Modelle für alle fünf manuell bewerteten Sprachen am besten abschneiden.', 'id': 'Kertas ini menjelaskan pengiriman ke WAT 2021 Bahasa Indic Multilingual Task oleh Samsung R&D Institute Poland. Tugas ini meliputi terjemahan antara 10 Bahasa India (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil dan Telugu) dan bahasa Inggris. Kami menggabungkan berbagai teknik: transliterasi, filtrasi, backtranslation, adaptasi domain, pengetahuan-destilasi dan akhirnya mengumpulkan model NMT. Kami menerapkan pendekatan efektif untuk pelatihan sumber daya rendah yang terdiri dari pelatihan di penerjemah belakang dan tuning pada corpora paralel. We experimented with two different domain-adaptation techniques which significantly improved translation quality when applied to monolingual corpora.  Kami menyelidiki dan menerapkan pendekatan baru untuk menemukan hyperparameter terbaik untuk mengatur sejumlah model terjemahan. Semua teknik bersama memberikan peningkatan yang signifikan - sampai +8 BLEU atas hasil dasar. Kualitas model telah dikonfirmasi oleh evaluasi manusia di mana model SRPOL mencetak nilai terbaik untuk semua 5 bahasa yang diteliti secara manual.', 'fa': 'این کاغذ تحویل دادن به عملیات زیادی زبان هندی WAT 2021 توسط موسسه شناسایی و توسط لهستان Samsung R&D توصیف می\u200cکند. این وظیفه بین ۱۰ زبان هندی (بنگالی, گوجراتی, هندی, کاندا, مالایالام, ماراتی, اوریا, پنجابی, تامیل و تلوگو) و انگلیسی است. ما متنوع تکنولوژی را ترکیب کردیم: ترجمه، فیلترینگ، ترجمه پشتی، ترجمه\u200cسازی دامنین، توسعه\u200cسازی علم و توسعه\u200cسازی مدل NMT. ما یک روش موثری برای آموزش کمترین منابع استفاده کردیم که از ترجمه\u200cهای پشتیبانی و ترجمه\u200cهای پارالی بر شرکت پارالی است. ما با دو تکنیک تغییر دادن دومین متفاوت آزمایش کردیم که وقتی به یک شرکت زبان کاربرد کیفیت ترجمه را به طور معنی بهتر کرد. ما تحقیق کردیم و یک روش نویسی برای پیدا کردن بهترین پارامتر هایپر برای تعداد مدل ترجمه استفاده کردیم. تمامی تکنیک\u200cهای متحد به عنوان توسعه\u200cهای زیادی - تا +۸ BLEU در نتیجه\u200cهای پایه\u200cخط پایین، بهتر شد. کیفیت مدل\u200cها توسط ارزیابی انسان تایید شده است که مدل\u200cهای SRPOL برای همه پنج زبان\u200cهای ارزیابی دستی بهتر بود.', 'tr': 'Bu kagyz WAT 2021-nji ýylyň Hindi dilleriniň köp dil meselelerini Samsung R&D Institute Polşanyň tarapyndan bellenilýär. 10 Indik diller arasynda terjime edildi (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil we Telugu) we Iňlisçe. Biz çeşitli teknikleri birleştirdik: transliteration, filtering, backtranslation, domain adaptation, knowledge-distillation ve sonunda NMT modellerinin birleşmesi. Biz iň az kaynaklar eğitimi üçin etkinlik bir ýagdaý uygulapdyk. Bu parallel korpora terjime etmek we önüme geçirmek üçin bar. Biz iki farklı domeny adilama teknikleri ile denedik. Bu şekilde monolingual korpora uygulanan terjime kalitesini has gowurarak geliştirdik. Biz araşdyrdyk we ýagdaýda bir ýagdaý çykyş nusgalaryny ahtarmak üçin iñ gowy hiperparameterleri tapdyk. Birleşen hemme teknikler üýtgeşik gelişmeleri üçin +8 BLEU-a çevrildi. Modelleriň kalitesi adamlaryň çykyşyna görä SRPOL modelleriniň beýleki diller üçin gowy görkezilýän çykyşyna tarapyndan tassyklanýar.', 'sw': 'This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R&D Institute Poland.  The task covered translation between 10 Indic Languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil and Telugu) and English.  Tuliunganisha mbinu mbalimbali: utafsiri, kuchuja, kutafsiri, kubadilisha ndani, mabadiliko ya maarifa na hatimaye kutengeneza mifano ya NMT. Tulitumia mbinu yenye ufanisi wa mafunzo ya chini ya rasilimali ambayo ni pamoja na kutengeneza uvunjifu wa tafsiri na kusambaa kwa kampuni ya usambazaji. Tulijaribu kwa mbinu mbili tofauti za kuboresha mahali ambazo zilibadili kiwango kikubwa cha tafsiri pale ambapo zilitumika kwa makampuni ya kiutaifa. Tulitafiti na tukatumia mbinu za riwaya kwa kutafuta vifaa bora zaidi kwa kuweka pamoja na mifano kadhaa ya tafsiri. Teknolojia zote zilizounganishwa zimesababisha maboresho makubwa - hadi +8 BLEU juu ya matokeo ya msingi. Ukubwa wa mifano imethibitishwa na utafiti wa binadamu ambapo mifano ya SRPOL ilipunguza vizuri kwa lugha zote mitano zinazopitiwa kwa manufaa.', 'af': "Hierdie papier beskrywe die onderskrywing aan die WAT 2021 Indiese taal veelvuldige taak deur Samsung R&D Institute Polen. Die taak bedek vertaling tussen 10 Indiese tale (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil en Telugu) en Engels. Ons het 'n verskillende teknike gekombineer: transliterasie, filtering, terugvertaling, domein aanpassing, kennis-destilasie en eindelik ensembleering van NMT modele. Ons het 'n effektief toegang aangepas tot lae-hulpbronne onderwerp wat bestaan van terugvertaling en tuning op parallele korpora. Ons het eksperimenteer met twee verskillende domein-adaptasie-teknike wat betekenlik verbeter vertalingskwaliteit wanneer aangepas word na monolinglike korpora. Ons het deursoek en toegewend 'n roman toegang vir die beste hiperparameters te vind vir 'n aantal vertaling modele. Alle teknike gekombineerde het betaling verbetering gegee - tot by +8 BLEU oor basilyn resultate. Die kwaliteit van die modelles is bevestig deur die menslike evaluering waar SRPOL modelles beste gemaak het vir alle 5 handgemaakte tale.", 'sq': 'Ky dokument përshkruan paraqitjen në WAT 2021 Indic Language Multilingual Task nga Samsung R&D Institute Poland. Detyra mbuloi përkthimin midis 10 gjuhëve indike (bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil dhe Telugu) dhe anglisht. Ne kombinuam një shumëllojshmëri teknikash: transliteracion, filtrim, përkthimi prapa, përshtatje në domeni, distillacion-njohuri dhe përfundimisht mbledhje të modeleve NMT. Kemi aplikuar një qasje efektive për trajnimin e burimeve të ulëta që përbëhet nga paratrajnimi në përkthimet prapa dhe tuning në korpra paralele. Eksperimentuam me dy teknika të ndryshme të përshtatjes në domeni që përmirësuan ndjeshëm cilësinë e përkthimit kur aplikoheshin në korpra monogjuhësore. Kemi kërkuar dhe aplikuar një metodë të re për gjetjen e hiperparametrave më të mira për të mbledhur një numër modelesh përkthimi. Të gjitha teknikat e kombinuara dhanë përmirësim të rëndësishëm - deri në +8 BLEU lidhur me rezultatet bazë. Kualiteti i modeleve është konfirmuar nga vlerësimi njerëzor ku modelet SRPOL shënuan më mirë për të 5 gjuhët e vlerësuara manualisht.', 'hy': "Այս հոդվածը նկարագրում է Սամսում Հետազոտություն և զարգացման ինստիտուտի կողմից Պոլանիայի ԱՄՆ 2021 թվականի անդիկ լեզվի բազլեզու առաջադրանքին ներկայացումը: Այս խնդիրը ներառում էր թարգմանությունը 10 հնդկական լեզուների (Բենգալի, Գուջարատիի, Հինդի, Կանադայի, Մալայալամի, Մարաթիի, Օրիայի, Պունջաբի, Թամիլի և Թելուգու) և անգլերենի միջև: Մենք համադրեցինք տարբեր տեխնիկաներ' տրանսգրականություն, ֆիլտրում, հետադարձ թարգմանություն, բնագավառի ադապտացիա, գիտելիքի-դիսլիլացիա և վերջապես NMT մոդելների համախմբում: Մենք կիրառեցինք ցածր ռեսուրսների ուսումնասիրության արդյունավետ մոտեցում, որը կազմում է նախադասություն վերադարձ թարգմանությունների վրա և զուգահեռ կոպորա: Մենք փորձեցինք երկու տարբեր տիեզերական ադապտացիոն տեխնիկա միջոցով, որոնք կարևոր բարելավեցին թարգմանման որակը, երբ կիրառվում էին միալեզու մարմնի վրա: Մենք ուսումնասիրեցինք և կիրառեցինք նոր մոտեցում գտնելու համար լավագույն հիպերպարամետրերը բազմաթիվ թարգմանման մոդելներ համադրելու համար: Բոլոր տեխնիկաները միասին նշանակալի զարգացում տվեցին մինչև +8 ԲԼԵՎ հիմնական արդյունքների համեմատ: Մոդելների որակը հաստատվել է մարդկային գնահատման արդյունքում, որտեղ SSPOL մոդելները լավագույն գնահատել են ձեռքով գնահատված բոլոր հինգ լեզուների համար:", 'am': 'ይህ ገጾች ሰምsung R&ዲ ኢንተርኔት ፖሎንድ ወደWAT 2021 የየቋንቋ ቋንቋ ቋንቋ ስራውን ይናገራል፡፡ ስራው በ10 የህንድ ቋንቋዎች (በኣንጋሊ, ጉጃራቲ, ኪንዲ, ካናዳ, ማላይላም፥ ማራታይ፥ ኦርያ፥ ፓንጃብ፥ ታሚል እና ቴልጉግ) እና እንግሊዝኛ መካከለኛ ትርጉም ነው። We combined a variety of techniques: transliteration, filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models.  እና በፓርላማ ኮርፖርት ላይ መፍትርት እና በመግለጫ ላይ መፍጠር የሚያደርገውን የዝቅተኛ የክፍለ ሀብት ማኅበረሰብ አካባቢ ነው፡፡ በሁለት ልዩ ዶሜን-አካባቢ ጥያቄዎችን በመፍጠር በሞሎንቋል ኮርፖርተር በመጠቀም ጊዜ ትርጉም ጥያቄን አበጅተናል፡፡ ብዙ ትርጉም ዓይነቶችን ለማግኘት የሚሻሉትን አፍሪሜተር ማግኘት እና የመረጃን ሥርዓት አለብን፡፡ ጥያቄዎች ሁሉ በጥያቄ ውጤቶች ላይ የሚደረገውን ትልቅ ማድረግ ሰጥተዋል፡፡ የሞዴሎቹ ብዛት የሰው ድምፅ አረጋገጠው፤ SRPOL ሞዴል ለ5 ቋንቋዎች ሁሉ በመጠቀም የተሻለ ነው፡፡', 'az': 'Bu kańüńĪt, Samsung R&D Institute Polonya t…ôr…ôfind…ôn WAT 2021 Hindi dil √ßoxlu dil iŇüin…ô g√∂nd…ôril…ôni t…ôsdiql…ôyir. Bu iŇüin 10 Indik dill…ôrin (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil v…ô Telugu) v…ô ńįngiliz…ô arasńĪndakńĪ √ßeviri √∂rt√ľlm√ľŇüd√ľr. Biz m√ľxt…ôlif teknikl…ôri birl…ôŇüdirdik: transliteration, filtering, backtranslation, domain adaptation, knowledge-distillation v…ô sonunda NMT modell…ôrini birl…ôŇüdirdik. Biz d√ľŇü√ľk ressurs t…ôhsilin…ô etkili bir approach uyguladńĪq ki, paralel korpora t…ôhsil edilm…ôsi v…ô t…ôhsil edilm…ôsi bar…ôsind…ô t…ôhsil edildi. Biz bir dil korporasńĪna uyńüunlaŇüdńĪńüńĪ zaman d…ôyiŇüiklik keyfiyy…ôtini √ßox yaxŇüńĪlaŇüdńĪran iki m√ľxt…ôlif domain-adaptation teknikl…ôri il…ô t…ôcr√ľb…ô etdik. Biz bir ne√ß…ô √ßeviri modell…ôrini birl…ôŇüdirm…ôk √ľ√ß√ľn …ôn yaxŇüńĪ hiperparametrl…ôri tapmaq √ľ√ß√ľn yeni bir yol uyguladńĪq. B√ľt√ľn birl…ôŇüdirilmiŇü teknikl…ôr, baŇülangńĪ√ß sonu√ßlarńĪnda +8 BLEU-…ô q…ôd…ôr m√∂hk…ôm uzlaŇüdńĪrmańüa baŇüladńĪ. Modell…ôrin keyfiyeti insan deńüerlendirm…ôsi il…ô t…ôsdiql…ônir ki, SRPOL modell…ôrin b√ľt√ľn 5 dill…ôrin …ôn yaxŇüńĪsńĪnńĪ q…ôd…ôr t…ôsdiql…ômiŇüdir.', 'ca': "Aquest article descriu la presentació al WAT 2021 Indic Language Multilingual Task d'Samsung R&D Institute Polònia. La tasca abarca la traducció entre 10 llengües índiques (bengalès, gujarati, Hindi, Kannada, Malayalam, Marati, Oriya, Punjabi, Tamil i Telugu) i anglès. Vam combinar una varietat de tècniques: transliteració, filtració, traducció inversa, adaptació de dominis, destilació del coneixement i finalment agrupació de models NMT. Vam aplicar un enfocament eficaç a l'entrenament de baix recursos que consisteix en pré-entrenar en traduccions posteriors i ajustar en corpora paral·lela. We experimented with two different domain-adaptation techniques which significantly improved translation quality when applied to monolingual corpora.  Vam investigar i aplicar un enfocament nou per trobar els millors hiperparamètres per ensenyar una sèrie de models de traducció. All techniques combined gave significant improvement - up to +8 BLEU over baseline results.  La qualitat dels models ha estat confirmada per l'evaluació humana on els models SRPOL van puntuar millor per a les 5 llengües evaluades manualment.", 'bs': 'Ovaj papir opisuje predlog Mnogjezičkog zadatka Indijskog jezika za WAT 2021 od strane Instituta Samsung R&D Poljske Poljske. Zadatak je pokriven prevodom između 10 Indijskih jezika (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil i Telugu) i engleskog jezika. Kombinirali smo razne tehnike: transliteracija, filtriranje, backtranslation, adaptacija domena, destilacija znanja i konačno osiguranje NMT modela. Prijavili smo efikasan pristup obuci niskih resursa koji se sastoji od pretvaranja prema prevodima i prilagodbi paralelnog korpora. Eksperimentirali smo sa dvije različite tehnike adaptacije domena koje su značajno poboljšale kvalitet prevoda kada su primjenjene na monojezičku korporu. Istraživali smo i primijenili novi pristup za pronalaženje najboljih hiperparametara za uključenje broja modela prevoda. Sva kombinirana tehnika pružala je značajno poboljšanje - do +8 BLEU zbog početnih rezultata. kvalitet modela je potvrđena ljudskim procjenama gdje su modeli SRPOL najbolje procijenili za sve 5 ručno procijenjenih jezika.', 'bn': 'This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R&D Institute Poland.  এই কর্মটি ভাষার মধ্যে ১০ টি ভাষার অনুবাদ প্রকাশ করেছে (বাংলা, গুজারাতি, হিন্দি, কান্নাদা, মালায়ালাম, মারাথি, ওরিয়া, পাঞ্জাবি, তামিল এবং টেলুগু) এব আমরা বিভিন্ন ধরনের প্রযুক্তিগুলো একত্রিত করেছি: ট্রান্সলিটারেশন, ফিল্টারিং, ব্যাক-অনুবাদ, ডোমেইন অ্যাডাপেটশন, জ্ঞান-বিচ আমরা নীচের সম্পদ প্রশিক্ষণের কাছে কার্যকর উপায় প্রয়োগ করেছিলাম যেখানে ব্যাকভাবে অনুবাদ এবং প্যারালেল কর্পোরার প্যানাল আমরা দুটি বিভিন্ন ডোমেইন-আপেটশন কৌশল দ্বারা পরীক্ষা করেছি যা অনুবাদের মান বাড়িয়ে দিয়েছে যখন সাধারণ ভাষায় কোর্পোরায় প আমরা গবেষণা করেছি এবং একটি উপন্যাস প্রয়োগ করেছি সেরা হাইপার্পারামিটার খুঁজে বের করার জন্য। সকল প্রযুক্তি একত্রিত হয়েছে বেসাইলাইনের ফলাফলের ব্যাপারে গুরুত্বপূর্ণ উন্নতি প্রদান করেছে। এই মডেলের মান নিশ্চিত করা হয়েছে মানুষের মূল্য যেখানে SRPOL মডেলগুলো সকল ৫ ভাষার মুল্যায়ন করা হয়েছে।', 'cs': 'Tento článek popisuje předložení do WAT 2021 Indic Language Multilingual Task společnosti Samsung R&D Institute Polsko. Úkol zahrnoval překlad mezi deseti indickými jazyky (bengálština, gudžarátština, hindština, kannáda, malajálština, maratština, orija, pundžábština, tamilština a telugu) a angličtinou. Kombinovali jsme různé techniky: transliteraci, filtraci, backtranslaci, adaptaci domén, destilaci znalostí a nakonec sestavení NMT modelů. Aplikovali jsme efektivní přístup k školení s nízkými zdroji, které spočívá v předškolení zpětných překladů a ladění paralelních korpusů. Experimentovali jsme se dvěma různými technikami adaptace domén, které výrazně zlepšily kvalitu překladu při aplikaci na monojazyčné korpusy. Zkoumali jsme a aplikovali nový přístup k nalezení nejlepších hyperparametrů pro sestavení řady překladových modelů. Všechny techniky v kombinaci přinesly významné zlepšení až do +8 BLEU oproti výsledkům základních výsledků. Kvalita modelů byla potvrzena lidským hodnocením, kde SRPOL modely skórovaly nejlépe pro všechny pět manuálně hodnocených jazyků.', 'fi': 'Tässä artikkelissa kuvataan Samsungin tutkimus- ja kehitysinstituutin Polandin WAT 2021 Indic Language Multilingual Task -ohjelmaan osallistumista. Tehtävään kuului kymmenen indiankielen (bengali, gujarati, hindi, kanada, malayalam, marathi, oriya, punjabi, tamil ja telugu) ja englannin käännös. Yhdistimme erilaisia tekniikoita: transliteraatio, suodatus, takaisinkääntäminen, domain adaptointi, tietämyksen tislaus ja lopulta NMT-mallien kokoonpano. Sovellimme tehokasta lähestymistapaa vähävaraiseen koulutukseen, joka koostuu takaisinkäännösten esikoulutuksesta ja rinnakkaiskorpusten virityksestä. Kokeilimme kahta eri domain-adaptointitekniikkaa, jotka paransivat merkittävästi käännöksen laatua, kun niitä sovellettiin monikielisiin korpusiin. Tutkimme ja sovelsimme uudenlaista lähestymistapaa löytääksemme parhaat hyperparametrit useiden käännösmallien muodostamiseen. Kaikki tekniikat yhdistettynä paransivat merkittävästi - jopa +8 BLEU:n lähtötasoon verrattuna. Mallien laadun on vahvistanut ihmisen arviointi, jossa SRPOL-mallit saivat parhaan tuloksen kaikilla viidellä manuaalisesti arvioidulla kielellä.', 'et': 'Käesolevas artiklis kirjeldatakse Samsungi Poola teadus- ja arendustegevuse instituudi poolt WAT 2021 india keele mitmekeelse ülesande esitamist. Ülesanne hõlmas tõlkimist 10 india keele (bengali, gujarati, hindi, kannada, malajalami, marathi, oriya, punjabi, tamili ja telugu) ja inglise keele vahel. Kombineerisime erinevaid tehnikaid: transliteratsioon, filtreerimine, tagasitõlkimine, domeeni kohandamine, teadmiste destilleerimine ja lõpuks NMT mudelite komplekteerimine. Me rakendasime efektiivset lähenemisviisi vähese ressursiga koolitusele, mis seisneb tagantõlkete eelõpetamises ja paralleelsete korpuste häälestamises. Katsetasime kahte erinevat domeeni kohandamise tehnikat, mis parandasid oluliselt tõlkekvaliteeti ühekeelsete korpuste puhul. Uurisime ja rakendasime uudset lähenemisviisi, et leida parimad hüperparameetrid mitmete tõlkemudelite komplekteerimiseks. Kõik tehnikad kombineeritud andsid olulise paranemise - kuni +8 BLU võrreldes algtasemega. Mudelite kvaliteeti on kinnitanud inimhinnang, kus SRPOLi mudelid said parima hinnangu kõigis viies käsitsi hinnatud keeles.', 'sk': 'Ta prispevek opisuje predložitev večjezične naloge indijskega jezika WAT 2021 s strani Samsung R&R Institute Poljska. Naloga je zajemala prevajanje med 10 indijskimi jeziki (bengalščina, gudžarati, hindijščina, kanada, malajalam, maratij, orija, pundžabi, tamilščino in telugu) in angleščino. Združili smo različne tehnike: transliteracijo, filtriranje, nazaj prevajanje, domensko prilagajanje, znanje-destilacijo in končno sestavljanje NMT modelov. Uporabili smo učinkovit pristop k izobraževanju z nizkimi viri, ki je sestavljeno iz predusposabljanja o nazaj prevajanju in uglaševanja vzporednih korpusih. Eksperimentirali smo z dvema različnima tehnikama domenske prilagoditve, ki sta občutno izboljšala kakovost prevajanja pri uporabi enojezičnih korpusov. Raziskovali smo in uporabili nov pristop za iskanje najboljših hiperparametrov za sestavljanje številnih prevajalskih modelov. Vse tehnike so skupaj prinesle znatno izboljšanje - do +8 BLEU glede na izhodiščne rezultate. Kakovost modelov je potrdila ocena ljudi, kjer so modeli SRPOL najbolje ocenili za vseh 5 ročno ocenjenih jezikov.', 'ha': 'Wannan takardan na describe the inpution to the WAT 2021 Indian language multilingui Tani na Samsung R & D Installat Poland. @ item Spelling dictionary Mun haɗa masu shiryi masu yawa: transliteratori, filtering, baktranslation, adaptation ɗin Domen, rarrabẽwa da ilmi kuma ƙarshen cikin misãlai na NMT. Mun applied wani mataimaki mai amfani zuwa mafarin-resource, wanda ke sami da bakwai-fassarori da tuntunna a kan parallel Corpo. Mun jarraba kowanki biyu masu adadin-Domin-adapti, wanda ya kyautata tsarin fassarar da shi mai girma a lokacin da aka amfani da shi. Ba mu sanar da kuma muka applied wani hanyoyi na nowaya dõmin ka sami mafi kyaun Hyperparameters dõmin ka sami wasu misãlai na fassarar. Duk technical da aka haɗa shi, sun ga mai kyau - up to+8 BLEU over matsalari na basalin. Tsarin shiryoyin aka tabbatar da shi a ƙaddara mutum, inda misalin SRPSL suka yi ƙari mafi kyauta ga duk lingui 5 da hannayen aka ƙayyade.', 'he': "This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R&D Institute Poland.  המשימה מכסה את התרגום בין 10 שפות אינדיות (בנגלי, גוג'ראטי, הינדי, קאנדה, מליאלאם, מרתיי, אורייה, פונג'אבי, טמיל וטלוגו) ואנגלית. שיחדנו מגוון של טכניקות: טרנסליטציה, סינון, תרגום אחורי, התאמה לתחום, מידע-שטח ולבסוף התערבות של דוגמנים NMT. השתמשנו גישה יעילה לאימוני משאבים נמוכים שמתכילים באימונים מראש על תרגומות אחוריות ומתאים על גופורה מקבילה. ניסונו עם שתי טכניקות שיפוי תורגם שונות ששיתפרו באופן משמעותי את איכות התרגום כשהופעלו על גופורה מונושפתית. חקרנו והפעלנו גישה חדשה למצוא את היפרפרמטרים הטובים ביותר לארגון מספר דוגמנים של תרגום. כל הטכניקות ביחד נתנו שיפור משמעותי - עד +8 BLEU על תוצאות הבסיס. The quality of the models has been confirmed by the human evaluation where SRPOL models scored best for all 5 manually evaluated languages.", 'bo': 'ཤོག་བྱང་འདིས་WAT 2021 སྒེར་གྱི་སྐད་ཡིག་ཆ་སྐད་ཀྱི་ལས་འགུལ་གྱི་ནང་དུ་བཏོན་གཏོང་བ་རེད། བྱ་འགུལ་འདིའི་ནང་དུ་ཨིན་ཌི་སི་ཡ་༡༠་དབུས་བསྒྱུར་བའི་སྐད་ཡིག་དང་། We combined a variety of techniques: transliteration, filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models. ང་ཚོས་རྒྱབ་སྐྱོར་ཆེན་དང་མཐུན་སྣུམ We experimented with two different domain-adaptation techniques which significantly improved translation quality when applied to monolingual corpora. We researched and applied a novel approach for finding the best hyperparameters for ensembling a number of translation models. ལག་རྩལ་ཆ་མཉམ་བསྡོམས་པའི་ལག་རྩལ་ཆེ་ཤོས་ཡར་རྒྱས་གཏོང་། མིག', 'jv': 'Perkara iki rambarang nggawe tarjamahan urip nggawe WAT 2020 1 Perkara ono nggawe tarjamahan kanggo 10 Inggal (Bengal, Gujarati, Hong, Kanadan, malayam, Marati, Oriya, Punjabi, Tamil lan telu) lan Inggris. We combed a variant of Methods: translation, filling, backtranslation, domain modification, knowness-separatelltion and end ensembedding of NMT modes. Awak dhéwé éntuk aplikasi kanggo ngerasakno kang éntuk-ingkang éntuk sing wis nguasai perusahaan terjamahan karo nggugu bantayan We piled with Two Variable domain-Adjustment Methods that badly advanced translation quality when used to Monlanguage Body. Awak dhéwé milihé karo aplikasi ngono nganggo dolanan ping mbukak kanggo ngilanggar kapan paramétrum sing luwih nggawe nggawe barang model itolet. Tulung teknik sing ditambah akeh nesaturan akeh luwih dumadhi -wis atung + 8 B luwih dumadhi sing ditambah barang kalité modelo sing wis ditambah dadi nggawe bener tentang wong modèl CULOL kuwi tindan luwih apik kanggo kalah 5 ulih basa manut.'}
{'en': 'Multilingual Machine Translation Systems at WAT 2021 : One-to-Many and Many-to-One Transformer based NMT WAT  2021: One-to-Many and Many-to-One Transformer based  NMT', 'pt': 'Sistemas de tradução automática multilíngue no WAT 2021: NMT baseado em transformadores um para muitos e muitos para um', 'fr': 'Systèmes de traduction automatique multilingues au WAT 2021\xa0: NMT basé sur un transformateur un-à-plusieurs et plusieurs-à-un', 'es': 'Sistemas de traducción automática multilingüe en el WAT 2021: NMT basado en transformadores uno a muchos y muchos a uno', 'ar': 'أنظمة الترجمة الآلية متعددة اللغات في WAT 2021: NMT القائم على محول واحد إلى متعدد ومتعدد إلى واحد', 'ru': 'Многоязычные системы машинного перевода на WAT 2021: один к многим и много к одному Трансформатор на основе NMT', 'hi': 'WAT 2021 में बहुभाषी मशीन अनुवाद प्रणाली: एक-से-कई और कई-से-एक ट्रांसफॉर्मर आधारित NMT', 'ja': 'WAT 2021での多言語機械翻訳システム：ワンツーマンとマルチトランスフォーマーベースのNMT', 'zh': '2021 年 WAT 展会上多言语机器翻译系统:基于双多对一变压器之 NMT', 'ga': 'Córais Ilteangacha Aistriúcháin Meaisín ag WAT 2021: Claochladán Aon-ar-Mhonar agus Go leor le Duine atá bunaithe ar NMT', 'ka': 'Multilingual Machine Translation Systems at WAT 2021: One-to-Many and Many-to-One Transformer based NMT', 'hu': 'Többnyelvű gépi fordító rendszerek a WAT 2021-en: Egy-a-sokhoz és sok-az-egyhez transzformátor alapú NMT', 'el': 'Πολυγλωσσικά συστήματα μηχανικής μετάφρασης στο WAT 2021: Ένα-προς-πολλά και πολλά-προς-ένα μετασχηματιστή βασισμένο σε μετασχηματιστή', 'kk': '2021 жылы WAT бағдарламасында көп тілді машинаны аудару жүйелері: NMT негізінде бір- мен көп және бір- мен бір түрлендіру жүйелері', 'mk': 'Мултијазични машински транслативни системи на WAT 2021: НМТ базиран на еден кон многумина и многумина кон еден', 'it': 'Sistemi di traduzione automatica multilingue a WAT 2021: NMT basato su trasformatori one-to-many e many-to-one', 'lt': '2021 m. WAT daugiakalbės mašinų vertimo sistemos: vienas į daugelį ir vienas į daugelį transformatorių pagrįstos NMT', 'mt': 'Sistemi Multilingwi ta’ Traduzzjoni tal-Magni fl-WAT 2021: NMT ibbażat fuq Trasformatur minn Wieħed għal Ħafna għal Wieħed', 'ms': 'Sistem Terjemahan Mesin Berbahasa pada WAT 2021: NMT berasaskan Satu-ke-Banyak dan Banyak-ke-Satu', 'ml': 'WAT 2021-ല്\u200d പല ഭാഷ മെഷിന്\u200d പരിഭാഷ സിസ്റ്റത്തില്\u200d: ഒന്നില്\u200d നിന്നും അധികമായി ഒരു ട്രാന്\u200dസ്ഫോര്\u200dമാര്\u200d അടിസ്ഥാനമായ NMT', 'mn': '2021 оны WAT-д олон хэлний машин хөгжүүлэх систем: Нэг-ээс олон, Нэг-ээс олон Трансформатор суурилсан NMT', 'no': 'Multispråk maskinsomsetjingssystemer ved WAT 2021: Ein til mange og mange til én transformeringssystemer basert på NMT', 'pl': 'Wielojęzyczne systemy tłumaczeń maszynowych w WAT 2021: jeden-na-wiele i wiele-na-jeden transformator oparty na transformatorach NMT', 'ro': 'Sisteme de traducere automată multilingvă la WAT 2021: NMT bazată pe transformatori unu-la-mulți și mulți-la-unu', 'sr': 'Mnogjezički sistemi prevoda mašine na WAT 2021: jedan do mnogih i mnogih na jednom transformatoru bazirani na NMT-u', 'si': 'ගොඩක් භාෂාවක් පද්ධතිය පද්ධතිය WAT 2021: එක- වෙනුවෙන් ගොඩක් හා ගොඩක් වෙනුවෙන් පද්ධතිය NMTName', 'so': 'Translation Systems at WAT 2021: One-to-Many and many-One Transformer based NMT', 'sv': 'Flerspr책kiga maskin철vers채ttningssystem p책 WAT 2021: En-till-m책nga och m책nga-till-en transformatorbaserade NMT', 'ta': 'WAT 2021-ல் பல மொழி இயந்திரம் மொழிபெயர்ப்பு அமைப்புகள்:', 'ur': 'Multilingual Machine Translation Systems at WAT 2021: One-to-Many and Many-to-One Transformer based NMT', 'uz': '@ info: whatsthis', 'vi': 'Hệ thống dịch máy móc đa ngôn ngữ tại WAT 2021: Một-to-Many và Many-to-one Transormer based NMT', 'da': 'Flersprogede maskinoversættelsessystemer på WAT 2021: En-til-mange og mange-til-en transformatorbaseret NMT', 'hr': 'Mnogjezički sustavi prevoda strojeva na WAT 2021: jedan do mnogih i mnogih na jednom transformatoru bazirani na NMT-u', 'nl': 'Meertalige Machine Translation Systemen op WAT 2021: One-to-Many en Many-to-One Transformer gebaseerde NMT', 'bg': 'Многоезични системи за машинен превод на ВАТ 2021: един към много и много към един трансформатор базиран НМТ', 'de': 'Mehrsprachige maschinelle Übersetzungssysteme am WAT 2021: One-to-Many und Many-to-One Transformer basierende NMT', 'fa': 'سیستم\u200cهای ترجمه ماشین\u200cهای زیادی زبان در WAT 2021: یک به بسیاری و بسیاری به یک ترجمه\u200cکننده NMT', 'id': 'Multilingual Machine Translation Systems at WAT 2021: One-to-Many and Many-to-One Transformer based NMT', 'ko': 'WAT 2021의 다국어 기계 번역 시스템: NMT 기반의 일대다 및 다대일 변압기', 'sw': 'Mfumo wa Tafsiri ya Mashindano ya Kilugha katika WAT 2021: Mfumo wa Tafsiri kwa moja na nyingi na MT anayeishi NMT', 'tr': "Çoklu dilli Maşynyň terjime sistemleri WAT 2021'de: bir-we bir-we bir-we bir terjime edip NMT tabanly", 'sq': 'Sistemet Multilingual Machine Translation Systems në WAT 2021: NMT me bazë një-në-shumë dhe shumë-në-një-transformues', 'am': 'የቋንቋ ቋንቋዎች መኪን ትርጉም ሲስተም WAT 2021: አንድ-to-many እና ብዙ-to-One Transformer based NMT', 'az': 'WAT 2021-də çoxlu dil Makina Çeviri Sistemləri: NMT tabanlı bir-çoxlu və bir-çoxlu Transformer sistemi', 'af': 'Veelvuldige Masjien Vertaling Sistemes op WAT 2021: Een na- veel en veel na- een Transformeerder gebaseer NMT', 'bn': 'ওয়াট ২০২১-এ অনেক ভাষাভাষী মেশিন অনুবাদ সিস্টেম: এক-থেকে অনেক এবং অনেক-থেকে এক ট্রান্সফার্নার ভিত্তিক NMT', 'hy': 'Բազլեզու մեքենայի թարգմանման համակարգերը 2021 թվականին. մեկ-շատերին և շատերին-մեկի թարգմանման հիմնված NMT', 'ca': 'Sistemes de traducció multilingües de màquines al WAT 2021: NMT basat en un a molts i molts a un transformador', 'cs': 'Vícejazyčné strojové překladatelské systémy na WAT 2021: One-to-Many a Many-to-One transformátory založené na NMT', 'fi': 'Monikieliset konekäännösjärjestelmät WAT 2021: yksi moneen ja monta yhteen muuntajapohjainen NMT', 'et': 'Mitmekeelsed masintõlke süsteemid WAT 2021: üks-paljudele ja palju-ühele transformaatoril põhinevad NMT', 'bs': 'Multilingual Machine Translation Systems na WAT 2021: One-to-Many i Many-to-One transformer bazirani NMT', 'ha': 'Multilingual Machine Translation Systems at WAT 2021: One-to-Many and Many-to-One Transformer based NMT', 'jv': 'Multi-Linguial Mas Terjamahan Sistem nang WAT 2020: Teka-Teka-Kelompok lan akeh-Teka-Teka Transformer siji NMT', 'he': 'מערכות תרגום מכונות רבות בשפה ב-WAT 2021:', 'sk': 'Večjezični sistemi strojnega prevajanja na WAT 2021: NMT na podlagi transformatorjev enega na mnogo in mnogo na enem', 'bo': 'Multilingual Machine Translation System at WAT 2021: One-to-Many and Many-to-One Transformer based NMT'}
{'en': 'In this paper, we present the details of the systems that we have submitted for the WAT 2021 MultiIndicMT : An Indic Language Multilingual Task. We have submitted two separate multilingual NMT models : one for  English  to 10 Indic languages and another for 10 Indic languages to English. We discuss the implementation details of two separate multilingual NMT approaches, namely one-to-many and many-to-one, that makes use of a shared decoder and a shared encoder, respectively. From our experiments, we observe that the multilingual NMT systems outperforms the bilingual baseline MT systems for each of the language pairs under consideration.', 'ar': 'في هذه الورقة ، نقدم تفاصيل الأنظمة التي قدمناها لـ WAT 2021 MultiIndicMT: مهمة هندية متعددة اللغات. لقد قدمنا نموذجين منفصلين متعددي اللغات NMT: أحدهما للغة الإنجليزية إلى 10 لغات هندية والآخر لـ 10 لغات هندية إلى الإنجليزية. نناقش تفاصيل تنفيذ طريقتين منفصلتين متعددتي اللغات لـ NMT ، وهما واحد لكثير ومتعدد لواحد ، والذي يستخدم وحدة فك ترميز مشتركة وجهاز تشفير مشترك ، على التوالي. من تجاربنا ، نلاحظ أن أنظمة NMT متعددة اللغات تتفوق في الأداء على أنظمة الترجمة الآلية الأساسية ثنائية اللغة لكل من أزواج اللغات قيد الدراسة.', 'es': 'En este artículo, presentamos los detalles de los sistemas que hemos presentado para el MultiIndicMT WAT 2021: Una tarea multilingüe en lengua india. Hemos presentado dos modelos distintos de NMT multilingües: uno para inglés a 10 idiomas índicos y otro para 10 idiomas índicos al inglés. Discutimos los detalles de implementación de dos enfoques NMT multilingües separados, a saber, uno a muchos y muchos a uno, que utilizan un decodificador compartido y un codificador compartido, respectivamente. A partir de nuestros experimentos, observamos que los sistemas de NMT multilingües superan a los sistemas de MT bilingües de referencia para cada uno de los pares de idiomas considerados.', 'fr': "Dans cet article, nous présentons les détails des systèmes que nous avons soumis pour le WAT 2021 MultiIndicMT\xa0: An Indic Language Multilingual Task. Nous avons soumis deux modèles NMT multilingues distincts\xa0: un pour l'anglais vers 10 langues indiennes et un autre pour 10 langues indiennes vers l'anglais. Nous discutons des détails de mise en œuvre de deux approches NMT multilingues distinctes, à savoir un-à-plusieurs et plusieurs-à-un, qui utilisent respectivement un décodeur partagé et un encodeur partagé. Nos expériences nous ont permis de constater que les systèmes NMT multilingues surpassent les systèmes de TA bilingue de base pour chacune des paires de langues considérées.", 'pt': 'Neste artigo, apresentamos os detalhes dos sistemas que enviamos para o WAT 2021 MultiIndicMT: An Indic Language Multilingual Task. Enviamos dois modelos NMT multilíngues separados: um para inglês para 10 idiomas índicos e outro para 10 idiomas índicos para inglês. Discutimos os detalhes de implementação de duas abordagens NMT multilíngues separadas, ou seja, um para muitos e muitos para um, que fazem uso de um decodificador compartilhado e um codificador compartilhado, respectivamente. De nossos experimentos, observamos que os sistemas NMT multilíngues superam os sistemas MT de linha de base bilíngues para cada um dos pares de idiomas considerados.', 'ja': '本稿では、WAT 2021 MultiIndicMT: An Indic Language Multilingual Taskのために提出したシステムの詳細を紹介します。私たちは、2つの別々の多言語NMTモデルを提出しました。1つは、英語から10のインドの言語、もう1つは、10のインドの言語から英語です。私たちは、共有デコーダと共有エンコーダをそれぞれ利用する、２つの別々の多言語NMTアプローチ、すなわち1対多と複数対1のアプローチの実装の詳細について説明します。私たちの実験から、多言語NMTシステムは、検討されている各言語ペアについて、バイリンガルベースラインMTシステムよりも優れていることが観察されます。', 'zh': '本文引为WAT 2021 MultiIndicMT:印度语多言系统之详细信息。 二言 NMT 模:一施于英语 10 种印度语,一施于 10 种印度语于英语。 论独立之多言 NMT 法之成细故,各用其解码器与共编码器之双多对一。 以吾实验观之,于所虑每言是,多言 NMT 善双语基线 MT 。', 'hi': 'इस पेपर में, हम उन प्रणालियों का विवरण प्रस्तुत करते हैं जिन्हें हमने WAT 2021 MultiIndicMT: एक इंडिक भाषा बहुभाषी कार्य के लिए प्रस्तुत किया है। हमने दो अलग-अलग बहुभाषी एनएमटी मॉडल प्रस्तुत किए हैं: एक अंग्रेजी के लिए 10 भारतीय भाषाओं के लिए और दूसरा अंग्रेजी में 10 भारतीय भाषाओं के लिए। हम दो अलग-अलग बहुभाषी एनएमटी दृष्टिकोणों के कार्यान्वयन विवरणों पर चर्चा करते हैं, अर्थात् एक-से-कई और कई-से-एक, जो क्रमशः एक साझा डिकोडर और एक साझा एनकोडर का उपयोग करता है। हमारे प्रयोगों से, हम देखते हैं कि बहुभाषी एनएमटी सिस्टम विचाराधीन भाषा जोड़े में से प्रत्येक के लिए द्विभाषी बेसलाइन एमटी सिस्टम को मात देता है।', 'ru': 'В этой статье мы представляем подробную информацию о системах, которые мы представили для многоязычной задачи WAT 2021 MultiIndicMT: An Indic Language Multilingual Task. Мы представили две отдельные многоязычные модели НБ: одну для английского языка на 10 языков индикаторов и другую для 10 языков индикаторов на английский язык. Мы обсуждаем детали реализации двух отдельных многоязычных подходов NMT, а именно один ко многим и много к одному, которые используют общий декодер и общий кодер, соответственно. Из наших экспериментов мы наблюдаем, что многоязычные системы NMT превосходят двуязычные базовые системы MT для каждой из рассматриваемых языковых пар.', 'ga': 'Sa pháipéar seo, cuirimid i láthair sonraí na gcóras atá curtha isteach againn don WAT 2021 MultiIndicMT: Tasc Indic Language Multilingual. Tá dhá mhúnla ilteangacha NMT curtha isteach againn: ceann amháin le haghaidh Béarla go 10 dteanga Indice agus ceann eile do 10 dteanga Indice go Béarla. Déanaimid plé ar shonraí cur chun feidhme dhá chur chuige ilteangacha NMT ar leith, is iad sin ceann ar go leor agus go leor le duine, a bhaineann úsáid as díchódóir roinnte agus ionchódóir roinnte, faoi seach. Ón dturgnaimh a rinneamar, tugaimid faoi deara go n-éiríonn níos fearr leis na córais ilteangacha NMT ná na córais MT bonnlíne dátheangacha do gach ceann de na péirí teangacha atá faoi chaibidil.', 'el': 'Στην παρούσα εργασία, παρουσιάζουμε τις λεπτομέρειες των συστημάτων που έχουμε υποβάλει για το πολυγλωσσικό έργο της Ινδικής γλώσσας. Έχουμε υποβάλει δύο ξεχωριστά πολύγλωσσα μοντέλα: ένα για αγγλικά σε δέκα ινδικές γλώσσες και ένα άλλο για δέκα ινδικές γλώσσες στα αγγλικά. Συζητούμε τις λεπτομέρειες εφαρμογής δύο ξεχωριστών πολυγλωσσικών προσεγγίσεων, δηλαδή ένα προς πολλά και πολλά προς ένα, που χρησιμοποιεί έναν κοινό αποκωδικοποιητή και έναν κοινό κωδικοποιητή αντίστοιχα. Από τα πειράματά μας, παρατηρούμε ότι τα πολυγλωσσικά συστήματα υπερτερούν των δίγλωσσων βασικών συστημάτων ΜΤ για κάθε ένα από τα υπό εξέταση γλωσσικά ζεύγη.', 'hu': 'Ebben a tanulmányban bemutatjuk azon rendszerek részleteit, amelyeket a WAT 2021 MultiIndicaMT: An Indic Language Multilingual Task keretében benyújtottunk. Két különálló, többnyelvű NMT modellt küldtünk be: az egyik angolra 10 indikus nyelvre, a másikra 10 indikus nyelvre angolra. Megvitatjuk két különálló, többnyelvű NMT megközelítés megvalósításának részleteit, nevezetesen egy-a-sokhoz és sok-az-egyhez, amelyek egy megosztott dekódolót, illetve egy megosztott kódolót használnak. Kísérleteink alapján megállapítjuk, hogy a többnyelvű NMT rendszerek minden egyes vizsgált nyelvpárban felülmúlják a kétnyelvű alapvető MT rendszereket.', 'ka': 'ამ დომენტში ჩვენ გავაჩვენოთ სისტემის დეტალები, რომლებიც ჩვენ გავამუშავეთ WAT 2021 MultiIndicMT: ინდიური მრავალენგური დავალება. ჩვენ ორი განსხვავებული მრავალენგური NMT მოდელები გადატანა: ერთი ანგლისური 10 ინდიური ენაზე და ერთი 10 ინდიური ენაზე ანგლისური ენაზე. ჩვენ განსაკუთრებული მრავალენგური NMT დახმარებას განსაკუთრებულ განსაკუთრებული განსაკუთრებული განსაკუთრებულ განსაკუთრებულების განსაკუთრებულების განსაკუთრებულების განსაკუთრებულების განსაკუთრებულების განსაკ ჩვენი ექსპერიმენტებიდან ჩვენ დავხედავთ, რომ მრავალენგური NMT სისტემები უფრო გავაკეთებს ორი ენგური MT სისტემები ყველა ენგური ზოგებისთვის.', 'it': "In questo articolo presentiamo i dettagli dei sistemi che abbiamo presentato per il WAT 2021 MultiIndicaMT: An Indic Language Multilingual Task. Abbiamo presentato due modelli NMT multilingue separati: uno per l'inglese a 10 lingue indiche e un altro per 10 lingue indiche all'inglese. Discutiamo i dettagli di implementazione di due approcci NMT multilingue separati, vale a dire uno a molti e molti a uno, che fanno uso di un decoder condiviso e un encoder condiviso, rispettivamente. Dai nostri esperimenti, osserviamo che i sistemi NMT multilingue superano i sistemi MT di base bilingue per ciascuna delle coppie linguistiche in esame.", 'lt': 'Šiame dokumente pristatome išsamią informaciją apie sistemas, kurias pateikėme pagal WAT 2021 MultiIndicMT: Indinės kalbos daugiakalbė užduotis. Mes pateikėme du atskirus daugiakalbius NMT modelius: vieną anglų kalbai – 10 indinių kalbų, o kitą – 10 indinių kalbų anglų kalbai. Mes diskutuojame apie dviejų atskirų daugiakalbių NMT metodų įgyvendinimo detales, būtent vieno-daugelio ir daugelio-vieno metodų, kuriais atitinkamai naudojamas bendras dekoderis ir bendras kodorius. From our experiments, we observe that the multilingual NMT systems outperforms the bilingual baseline MT systems for each of the language pairs under consideration.', 'kk': 'Бұл қағазда, біз 2021 жылдың көп индикалық тілдер тапсырмасына жіберген жүйелердің егжей- тегжейін таңдаймыз. Біз екі бөлек тілді NMT үлгілерін жібердік: бірі ағылшын тілде 10 индикалық тілде, бірінші тілде 10 индикалық тілде ағылшын тілде. Біз екі бөлек тілдегі NMT қатынау үшін жұмыс істеу егжей- тегжейін айтып тұрмыз, бұл - бір-біріне, бір-біріне, бір-біріне, бөлек декодерді және бөлек кодерді қолдану үшін. Біздің тәжірибеміздің көптілік NMT жүйелерінің көптілік тілдерінің MT жүйелерін түсініп тұратын тілдер жүйелерінің екі тілдерінің негізгі жүйелерін жасайды.', 'mk': 'Во овој весник ги претставуваме деталите на системите кои ги поднесовме за WAT 2021 MultiIndicMT: Индиски јазик Multilingual Task. Ние поднесовме два одделни мултијазични модели на НМТ: еден за англиски на 10 индиски јазици и друг за 10 индиски јазици на англиски. Разговараме за деталите за спроведувањето на двата одделни мултијазички пристапи на НМТ, имено еден до многу и многу до еден, кои користат заеднички декодер и заеднички кодер, односно. Од нашите експерименти, набљудуваме дека мултијазичните НМТ системи ги надминуваат двојјазичните основни МТ системи за секој од разгледуваните јазички парови.', 'ms': 'In this paper, we present the details of the systems that we have submitted for the WAT 2021 MultiIndicMT: An Indic Language Multilingual Task.  Kami telah menghantar dua model NMT berbilang bahasa yang terpisah: satu untuk bahasa Inggeris ke 10 bahasa India dan satu lagi untuk 10 bahasa India ke bahasa Inggeris. Kami membincangkan perincian pelaksanaan dua pendekatan NMT berbilang bahasa yang terpisah, iaitu satu-ke-banyak dan banyak-ke-satu, yang menggunakan dekoder terkongsi dan pengekod terkongsi, berdasarkan. Dari eksperimen kami, kami memperhatikan bahawa sistem NMT berbilang bahasa melebihi sistem MT dasar dua bahasa untuk setiap pasangan bahasa yang sedang dipertimbangkan.', 'ml': 'ഈ പത്രത്തില്\u200d ഞങ്ങള്\u200d വാട്ട് 2021 മിടുക്കന്\u200d ഇന്റിക്ലിഡിക്എം: ഒരു ഇന്റിക്ക് ഭാഷ മിടുല്\u200dലിങ്ഗല്\u200d ടാസ്കില്\u200d ചെയ്ത സിസ്റ്റുകളു നമ്മള്\u200d രണ്ടു വേര്\u200dതിരിച്ചു വേര്\u200dതിരിച്ചിരിക്കുന്നു മാതൃകങ്ങള്\u200d: ഒന്ന് ഇംഗ്ലീഷിലേക്ക് 10 ഇംഗ്ലീഷിലേക് നമ്മള്\u200d രണ്ടു വേര്\u200dതിരിച്ചു പല ഭാഷ പ്രവര്\u200dത്തനങ്ങളുടെ പ്രവര്\u200dത്തനത്തിന്റെ വിവരങ്ങള്\u200d സംസാരിക്കുന്നു. ഒന്നില്\u200d നിന്നും അധികം മാത്രം മാര്\u200dഗങ്ങള്\u200dക നമ്മുടെ പരീക്ഷണങ്ങളില്\u200d നിന്ന് നമ്മള്\u200d നോക്കുന്നു, പല ഭാഷകങ്ങളുടെ NMT സിസ്റ്റം എല്ലാ ഭാഷ ജോട്ടുകാര്\u200dക്കും വിചാരിക്കുന്', 'mt': 'F’dan id-dokument, qed nippreżentaw id-dettalji tas-sistemi li ppreżentajna għall-WAT 2021 MultiIndicMT: Kompitu Multilingwi Indiku. Intbagħtu żewġ mudelli separati ta’ NMT multilingwi: wieħed għall-Ingliż sa 10 lingwi Indiċi u ieħor għal 10 lingwi Indiċi għall-Ingliż. Aħna niddiskutu d-dettalji tal-implimentazzjoni ta’ żewġ approċċi separati multilingwi tal-NMT, jiġifieri minn wieħed għal ħafna u minn ħafna għal wieħed, li jagħmlu użu minn dekoder kondiviż u kodifikatur kondiviż, rispettivament. Mill-esperimenti tagħna, naraw li s-sistemi NMT multilingwi jaqbżu s-sistemi MT billingwi fil-linja bażi għal kull wieħed mill-pari lingwistiċi kkunsidrati.', 'no': 'I denne papiret viser vi detaljane om systema vi har sendt til WAT 2021 MultiIndicMT: ein indesk språk multispråk oppgåve. Vi har sendt to separate multilingual NMT-modeller: ein for engelsk til 10 indiske språk og ein for 10 indiske språk til engelsk. Vi diskuterer implementeringsdetaljane om to separate multilingual NMT-tilnærmingar, dvs. ein til mange og mange til ein, som gjer bruk av ei delt dekoder og ei delt koder. Fra eksperimentene våre observerer vi at multispråksystemene NMT utfører dei bilinguelte baselinjesystemene MT for kvar av språksparene som er gjennomsikt.', 'pl': 'W niniejszym artykule przedstawiamy szczegóły systemów, które zgłosiliśmy do WAT 2021 MultiIndicMT: An Indic Language Multilingual Task. Przesłaliśmy dwa oddzielne wielojęzyczne modele NMT: jeden dla języków angielskich do 10-języków indyjskich i drugi dla 10-języków indyjskich do angielskiego. Omówiono szczegóły wdrożenia dwóch odrębnych wielojęzycznych podejść NMT, a mianowicie one-to-many i many-to-one, które wykorzystują odpowiednio wspólny dekoder i wspólny koder. Z naszych eksperymentów zauważamy, że wielojęzyczne systemy NMT przewyższają dwujęzyczne systemy MT dla każdej z rozważanych par językowych.', 'mn': 'Энэ цаасан дээр бид WAT 2021 MultiIndicMT-д зохион байгуулсан системийн нарийвчлалыг тайлбарлаж байна. Энэтхэг хэлний олон хэлний ажил. Бид хоёр ялгаатай олон хэл NMT загварыг дамжуулж байна: нэг нь Англи хэл дээр 10 индийн хэл дээр, нэг нь 10 индийн хэл дээр Англи хэл дээр. Бид хоёр олон хэлний NMT ойлголтын хэрэглээний тухай ярилцаж байна. Энэ нь нэгээс олон, нэгээс олон, нэгээс олон, хуваалцаагүй шийдвэр, хуваалцаагүй кодлогчийг ашигладаг. Бидний туршилтаас олон хэлний NMT системүүд хэлний хоёр хоёр хэлний суурь шугамны MT системүүдийг ажиглаж чадна.', 'ro': 'În această lucrare, prezentăm detaliile sistemelor pe care le-am prezentat pentru WAT 2021 MultiIndicaMT: An Indic Language Multilingual Task. Am trimis două modele NMT multilingve separate: unul pentru limba engleză la 10 limbi indice și altul pentru 10 limbi indice la engleză. Discutăm detaliile implementării a două abordări NMT multilingve separate, și anume unu-la-mulți și multe-la-unu, care utilizează un decodor partajat și, respectiv, un encoder partajat. Din experimentele noastre, observăm că sistemele multilingve NMT depășesc sistemele MT bilingve de bază pentru fiecare dintre perechile lingvistice luate în considerare.', 'sr': 'U ovom papiru predstavljamo detalje sustava koje smo predali za WAT 2021 MultiIndicMT: Multilingual Task Indičkog jezika. Predložili smo dva različita multijezička NMT modela: jedan za engleski na 10 Indijskih jezika, a drugi za 10 Indijskih jezika na engleski jezik. Razgovaramo o detaljima provedbe dve odvojene multijezičke pristupe NMT-a, a to je jedna na mnoge i mnoge na jedan, koja koristi zajednički dekoder i zajednički koder. Iz naših eksperimenata, posmatramo da multijezički NMT sistemi iznosi dvojezički početni MT sistem za svaki od razmatranih jezičkih parova.', 'so': 'Warqadan waxaan ku soo qoraynaa macluumaadka nidaamka aan u soo dhiibnay WAT 2021 MultiIndicMT: Shaqada luuqada luuqadaha ee Indic. We have submitted two separate multilingual NMT models: one for English to 10 Indic languages and another for 10 Indic languages to English.  Waxaannu kala sheekeynaynaa qoraal ku saabsan kooxaha lagu soo dejiyo laba habboon oo kala duduwan oo luqada kala duduwan oo ah hal-ka-badan iyo mid-ka-badan, kaas oo u isticmaalaya qeyb-qayb ah codcodsiga. Imtixaanadeena, waxaynu ka fiirinnaa in nidaamka NMT ee luuqadaha kala duduwan ay soo saaraan nidaamka MT-da ee labada luqadood oo ah labada nooc ee luqada lagu fiirsado.', 'sv': 'I denna uppsats presenterar vi detaljer om de system som vi har lämnat in för WAT 2021 MultiIndicaMT: An Indic Language Multilingual Task. Vi har skickat in två separata flerspråkiga NMT-modeller: en för engelska till 10 indiska språk och en annan för 10 indiska språk till engelska. Vi diskuterar implementeringsdetaljer för två separata flerspråkiga NMT-metoder, nämligen en-till-många och många-till-en, som använder sig av en delad avkodare respektive en delad kodare. Från våra experiment konstaterar vi att de flerspråkiga NMT-systemen presterar bättre än de tvåspråkiga baslinjeMT-systemen för vart och ett av de språkpar som övervägs.', 'ta': 'இந்த காகிதத்தில், நாம் WAT 2021 MultiIndicMT க்கு கொடுத்திருக்கும் அமைப்புகளின் விவரங்களை கொடுக்கிறோம்: ஒரு சிறிய மொழி பல மொழி பண நாம் இரண்டு தனிப்பட்ட பல மொழி NMT மாதிரிகளை கொடுத்துள்ளோம்: ஒன்று ஆங்கிலத்திற்கு 10 இச்சிட்ட மொழிகளுக்கு மற்றொ நாம் இரண்டு தனிப்பட்ட பல மொழி அணுகுகளின் விவரங்களைப் பற்றி விவாதம் செய்கிறோம், அது ஒரு முதலில் இருந்து பல மொழிகளில் இருந்து ஒன்று மற்றும் ஒரு முதலில் இர எங்கள் பரிசோதனைகளில் இருந்து, பல மொழி NMT அமைப்புகள் ஒவ்வொரு மொழி ஜோடிகளுக்கும் இரு மொழி அடிப்படை MT அமைப்புகளை வெளியேற்றுகிறது', 'si': 'මේ පත්තරේ අපි පෙන්වන්නේ WAT 2021 MultiIndicMT වලින් භාෂාවක් ගොඩක් භාෂාවක් වැඩක් වලින්. අපි වෙනස් භාෂාවක් අන්තිම භාෂාවක් අන්තිම භාෂාවක් දෙකක් දාලා තියෙනවා: එක ඉංග්\u200dරීසි භාෂාවක් 10 වල අපි වෙනස් භාෂාත්මක NMT අවස්ථාවක් දෙකක් ගැන ප්\u200dරවේශනය විස්තර දෙයක් කතා කරනවා, එක්කෙන් ගොඩක් හා ගොඩක් අවස්ථාවක් වෙනුවෙන්, ඒක අපේ පරීක්ෂණයෙන්, අපි බලාපොරොත්තු කරනවා වැඩි භාෂාත්මක NMT පද්ධතියේ දෙවෙනි භාෂාත්මක MT පද්ධතියට පරීක්ෂණය කරන්න', 'ur': 'اس کاغذ میں ہم نے ان سیستموں کی جزئیات پیش کیں جن کو ہم نے WAT 2021 MultiIndicMT کے لئے پیش کیے ہیں: ایک Indic Language Multilingual Task. ہم نے دو مختلف متفرق زبان NMT موڈل پیش کیے ہیں: ایک انگلیسی سے 10 زبان تک اور ایک 10 زبان تک انگلیسی تک۔ ہم نے دو مختلف متفرق زبان NMT کے پاس آتے ہیں، یعنی ایک سے بہت سے اور ایک سے بہت سے، جو ایک مشترک دیکوڈر اور ایک مشترک کوڈر کا استعمال کرتا ہے۔ ہمارے آزمائش سے ہم دیکھتے ہیں کہ بہت سی زبان کی NMT سیستموں نے ہر زبان کے جوڑوں کے لئے دو زبان کی بنسطل MT سیستموں کو فعال کر دیا ہے۔', 'uz': "Bu hujjatda biz WAT 2021 MultiIndicMT uchun joʻnatilgan tizimlar haqida maʼlumotni hosil qilamiz: Hindik tillari bir necha tillar vazifa. Biz bir xil tildagi ikkita xil tili NMT modellarini yaratdik: bir inglizcha tildan 10 xiddiy tilga, boshqa 10 Indik tillariga Inglizchaga ega tilga. Biz bir necha tildan bir necha xil tilning murakkablarini bajaramiz, bu bir dan bir ko'p va bir necha ko'pchilikdan bir necha ko'p narsalarni ishlatish mumkin. Bu bir xil kodlash va bir xil kodlash qoidadan foydalanadi. Biz tajribamizdan ko'pchilik NMT tizimlari bir tilning ikkita asosiy MT tizimini o'ylab turadi.", 'vi': 'Trong tờ giấy này, chúng tôi giới thiệu chi tiết về hệ thống mà chúng tôi đã gửi cho công tác đa ngôn ngữ Ấn Độ. Chúng tôi đã gửi hai mẫu NMT riêng biệt: một cho tiếng Anh đến mười ngôn ngữ Ấn và một cho mười ngôn ngữ Ấn cho tiếng Anh. Chúng ta thảo luận chi tiết ứng của hai phương pháp NMT riêng biệt, nhất là một đến nhiều và nhiều đến một, mà sử dụng một bộ giải đã chia sẻ, và một bộ mã chia sẻ. Từ các thí nghiệm của chúng tôi, chúng tôi quan sát rằng hệ thống NMT đa dạng vượt trội các hệ thống đường truyền truyền dẫn đường truyền cho mỗi cặp ngôn ngữ được cân nhắc.', 'bg': 'В настоящата статия представяме подробности за системите, които сме внесли за УАТ 2021 Многоезична задача на индийския език. Подадохме два отделни многоезични модела: един за английски на 10 индийски езика и друг за 10 индийски езика на английски. Обсъждаме детайлите по внедряването на два отделни многоезични подхода, а именно един към много и много към един, които използват споделен декодер и споделен кодер, съответно. От нашите експерименти наблюдаваме, че многоезичните НМТ системи превъзхождат двуезичните базови МТ системи за всяка от разглежданите езикови двойки.', 'nl': 'In dit artikel presenteren we de details van de systemen die we hebben ingediend voor de WAT 2021 MultiIndicMT: An Indic Language Multilingual Task. We hebben twee aparte meertalige NMT modellen ingediend: één voor Engels tot 10 Indische talen en een andere voor 10 Indische talen naar Engels. We bespreken de uitvoeringsdetails van twee afzonderlijke meertalige NMT-benaderingen, namelijk één-op-vele en vele-op-één, die gebruik maken van respectievelijk een gedeelde decoder en een gedeelde encoder. Uit onze experimenten zien we dat de meertalige NMT systemen beter presteren dan de tweetalige baseline MT systemen voor elk van de taalparen in kwestie.', 'hr': 'U ovom papiru predstavljamo detalje sustava koje smo predali za multiIndicMT WAT 2021: MultiIndicMT zadatak Indijskog jezika. Predložili smo dva različita multijezička NMT modela: jedan za engleski na 10 Indijskih jezika, a drugi za 10 Indijskih jezika na engleski jezik. Razgovaramo o detaljima provedbe dvije odvojene multijezičke pristupe NMT-a, a to je jedna na mnogim i mnogim na jedan, koja koristi zajednički dekoder i zajednički koder. Iz naših eksperimenata, posmatramo da multijezički NMT sustavi iznosi dvojezički početni MT sustav za svaki od razmatranih jezičkih parova.', 'da': 'I denne artikel præsenterer vi detaljerne om de systemer, vi har indsendt til WAT 2021 MultiIndicaMT: An Indic Language Multilingual Task. Vi har indsendt to separate flersprogede NMT modeller: en for engelsk til 10 indiske sprog og en anden for 10 indiske sprog til engelsk. Vi diskuterer implementeringsdetaljerne for to separate flersprogede NMT-tilgange, nemlig en-til-mange og mange-til-en, der gør brug af henholdsvis en delt dekoder og en delt koder. Ud fra vores eksperimenter konstaterer vi, at de flersprogede NMT-systemer overstiger de tosprogede baseline MT-systemer for hvert af de overvejede sprogpar.', 'id': 'Dalam kertas ini, kami memperkenalkan rincian sistem yang kami kirimkan untuk WAT 2021 MultiIndicMT: Sebuah Tugas Bahasa Indik Multibahasa. Kami telah mengirim dua model NMT berbagai bahasa yang terpisah: satu untuk bahasa Inggris ke 10 bahasa India dan satu lagi untuk 10 bahasa India ke bahasa Inggris. Kami mendiskusikan rincian implementasi dari dua pendekatan NMT berbagai bahasa yang terpisah, yaitu satu-ke-banyak dan banyak-ke-satu, yang menggunakan dekoder berbagi dan koder berbagi, sesuai dengan itu. Dari eksperimen kami, kami memperhatikan bahwa sistem NMT berbilang bahasa lebih berharga dari sistem MT dasar dua bahasa untuk setiap pasangan bahasa yang sedang dipertimbangkan.', 'ko': '본고에서는 WAT 2021 MultiIndicmT(인도어 다국어 임무)를 위해 제출한 시스템을 상세히 소개한다.우리는 이미 두 개의 독립된 다국어 NMT 모델을 제출했다. 하나는 영어에서 10가지 인도어에 사용되고, 다른 하나는 10가지 인도어에서 영어에 사용된다.우리는 두 가지 서로 다른 다중 언어 NMT 방법의 실현 세부 사항, 즉 일대다와 다대일을 토론했다. 이 두 가지 방법은 각각 공유 디코더와 공유 인코더를 사용한다.우리의 실험에서 우리는 고려한 모든 언어에 대해 다중 언어 NMT 시스템이 이중 언어 기선 기계 번역 시스템보다 우수하다는 것을 관찰했다.', 'fa': 'در این کاغذ، ما جزئیات سیستم\u200cهایی را که برای WAT 2021 MultiIndicMT ارائه داده\u200cایم را نشان می\u200cدهیم: یک کار زبان زیادی زبان هندی. ما دو مدل NMT متفاوت زبان متفاوت را فرستادیم: یکی برای انگلیسی تا ۱۰ زبان هندی و یکی برای ۱۰ زبان هندی به انگلیسی. ما در مورد جزئیات عملیات دو دسترسی NMT متفرق به زبان متفرق صحبت می کنیم، یعنی یک به بسیاری و بسیاری به یک، که از یک دیکوردر مشترک و یک کوردر مشترک استفاده می کند. از آزمایشات ما، مشاهده می\u200cکنیم که سیستم\u200cهای NMT چندین زبان بیشتر سیستم\u200cهای بنیادی MT را برای هر جفت زبان تحت توجه انجام می\u200cدهد.', 'sw': 'Katika karatasi hii, tunatoa maelezo ya mifumo ambayo tumewasilisha kwa ajili ya WAT 2021 MultiIndicMT: Kazi ya lugha ya Kihindi. Tumewasilisha mifano miwili tofauti ya NMT kwa lugha mbalimbali: moja kwa lugha 10 hadi Kihindi na nyingine kwa lugha 10 za Kihindi kwenda Kiingereza. Tunajadili maelezo ya utekelezaji wa namna mbili tofauti za NMT za lugha za lugha tofauti, yaani moja kwa moja na nyingi kwa moja, ambazo hutumia mfumo wa simu pamoja na kodi inayosambazwa kwa asilimia yake. Kutokana na majaribio yetu, tunaona kuwa mfumo wa NMT wa lugha mbalimbali unaonyesha mfumo wa MT wa msingi wa lugha mbili kwa kila mmoja wa wanandoa wa lugha unaoelewa.', 'tr': 'Bu kagyzda WAT 2021 MultiIndicMT üçin bellendiğimiz sistemleriň maglumatlaryny görkezýäris: Bir köp dilli zady indiki dil. Biz birnäçe köp dilli NMT nusgalaryny gönderdik: bir iňlisçe 10-a iňlisçe, bir iňlisçe 10-a iňlisçe gönderdik. Biz iki a ýratyn köp dilli NMT golaýynyň ýene-birden köp we birden-birden beýleki kodçyny ulanýan detaylaryny gürrüň edýäris. Biziň deneylerimizden, birnäçe dil NMT sistemalaryň düşünülen dilleriň her iki çift üçin bilim sistemalaryndan üstün tutýandygyny gözleýäris.', 'de': 'In diesem Beitrag stellen wir die Details der Systeme vor, die wir für die WAT 2021 MultiIndicMT: An Indic Language Multilingual Task eingereicht haben. Wir haben zwei separate mehrsprachige NMT-Modelle eingereicht: eines für Englisch bis 10 Indic Sprachen und eines für 10 Indic Sprachen auf Englisch. Wir diskutieren die Implementierungsdetails zweier getrennter mehrsprachiger NMT-Ansätze, nämlich One-to-Many und Many-to-One, die jeweils einen gemeinsamen Decoder und einen gemeinsamen Encoder verwenden. Aus unseren Experimenten beobachten wir, dass die mehrsprachigen NMT-Systeme die bilingualen Baseline-MT-Systeme für jedes der betrachteten Sprachpaare übertreffen.', 'af': "In hierdie papier vertoon ons die details van die stelsels wat ons voorgestuur het vir die WAT 2021 MultiIndicMT: â\x80\x99n Indiese Taal Veelvuldige Taak. Ons het twee aparte multitaal NMT-modelles ingestuur: een vir Engels tot 10 Indiese tale en ander vir 10 Indiese tale tot Engels. Ons bespreek die implementeringsdetails van twee aparte multilinguele NMT toegang, beteken een na-veel en baie na-een, wat gebruik van 'n gedeelde dekoder en 'n gedeelde enkoder, respektief. Van ons eksperimente, ons aanhou dat die multitaalske NMT-stelsels die twee tale baselyn MT-stelsels vir elke van die taal paar onder aandag uitvoer.", 'sq': 'Në këtë letër, ne paraqesim detajet e sistemeve që kemi paraqitur për WAT 2021 MultiIndicMT: Një detyrë gjuhësh indike shumëgjuhëse. Ne kemi paraqitur dy modele NMT shumëgjuhësh të veçanta: një për anglisht në 10 gjuhë indike dhe një për 10 gjuhë indike në anglisht. Ne diskutojmë detajet e zbatimit të dy qasjeve të veçanta shumëgjuhësore të NMT-së, veçanërisht një-në-shumë dhe shumë-në-një, që përdorin respektivisht një dekoder të përbashkët dhe një kodues të përbashkët. Nga eksperimentet tona, ne vëzhgojmë se sistemet shumëgjuhësore të NMT-së ekzistojnë më shumë se sistemet dy-gjuhësore të MT-së për secilin nga çiftet gjuhësore në konsideratë.', 'hy': 'Այս թղթի մեջ մենք ներկայացնում ենք այն համակարգերի մանրամասները, որոնք մենք ներկայացրել ենք 2021 թ. Մենք ներկայացրեցինք երկու առանձին բազմալեզու NMT-ի մոդել՝ մեկը անգլերենի համար տասը հնդկական լեզուներ, իսկ մյուսը՝ տասը հնդկական լեզուներ անգլերենի համար: Մենք քննարկում ենք NMT-ի երկու առանձին բազմալեզու մոտեցումների իրականացման մանրամասնությունները, այն է՝ մեկ-շատերին և շատերին-մեկին, որոնք օգտագործում են ընդհանուր կոդերը և մեկ-շատերին: Մեր փորձարկումների արդյունքում մենք նկատում ենք, որ բազլեզու NMT համակարգերը գերազանցում են երկլեզու հիմնական MT համակարգերը յուրաքանչյուր լեզվի զույգի համար:', 'bn': 'এই কাগজটিতে আমরা বিস্তারিত সিস্টেমের বিস্তারিত বিস্তারিত বর্ণনা করছি যা আমরা ওয়াট ২০২১ মাল্টিভাষার জন্য জমা দিয়েছি: একটি ইন্ড আমরা দুটি বিভিন্ন ভাষায় এনএমটি মডেল জমা দিয়েছি: একটি ইংরেজী থেকে ১০ ভাষায় এবং অন্যটি ১০ ভাষায় ইংরেজি ভাষায়। আমরা বিভিন্ন ভাষায় দুটি বিভিন্ন ভাষায় এনএমটি প্রবেশের বিস্তারিত বিস্তারিত বিস্তারিত আলোচনা করি, যেটি শেয়ার করা ডেকোডার এবং শেয়ার করা কোডার আমাদের পরীক্ষা থেকে আমরা দেখতে পাচ্ছি যে বহুভাষায় এনএমটি সিস্টেম প্রত্যেক ভাষার জোড়ার জন্য দুই ভাষার বেসাইলেইন এমটি সিস্ট', 'bs': 'U ovom papiru predstavljamo detalje sustava koje smo predali za multiIndicMT WAT 2021: MultiIndicMT zadatak Indijskog jezika. Predložili smo dva različita multijezička NMT modela: jedan za engleski na 10 Indijskih jezika, a drugi za 10 Indijskih jezika na engleski jezik. Razgovaramo o detaljima provedbe dvije odvojene multijezičke pristupe NMT-a, a to je jedna na mnoge i mnoge na jedan, koja koristi zajednički dekoder i zajednički koder. Iz naših eksperimenata, posmatramo da multijezički NMT sistemi nadmađuju dvojezički početni MT sistem za svaki od razmatranih jezičkih parova.', 'az': 'Bu kağızda WAT 2021 MultiIndicMT üçün göndərdiyimiz sistemlərin detaylarını göstəririk: Hindi dili çoxlu dil işi. Biz iki ayrı çoxlu dil NMT modellərini təbliğ etdik: bir İngilizce üçün 10 Indik dillərini və digərini 10 Indik dillərini İngilizce dillərinə təbliğ etdik. Biz iki müxtəlif dil NMT yaxınlığının istifadə edilməsini müzakirə edirik, bu da bir-birinə çox və çox-birinə, paylaşılmış bir dekoderi və paylaşılmış kodlayıcını istifadə edir. Bizim təcrübələrimizdən, çoxlu dil NMT sistemlərinin düşünülmüş dil çiftlərinin hər birinin iki səhifələrindən artıq MT sistemlərini yerinə yetirməsini gözləyirik.', 'ca': "En aquest article, presentem els detalls dels sistemes que hem presentat per al WAT 2021 MultiIndicMT: Una tasca multillengua de llenguatge índic. Hem presentat dos models de NMT multillengües diferents: un per anglès a 10 llengües índiques i un per 10 llengües índiques a anglès. Discutem els detalls d'implementació de dos enfocaments diferents multilingües de la NMT, a saber, d'un a molts i d'un a molts, que utilitzen un decodificador compartit i un codificador compartit, respectivament. Des dels nostres experiments, observem que els sistemes multilingües de MT superen els sistemes bilingües de MT de base per cada parell de llengües considerats.", 'am': 'በዚህ ገጽ ለWAT 2021 MultiIndicMT የሰጠነውን የድምጽ ዝርዝሮች እናቀርባታለን: የህንድ ቋንቋ ብዙልቋ ቋንቋ ስራ ነው፡፡ We have submitted two separate multilingual NMT models: one for English to 10 Indic languages and another for 10 Indic languages to English.  የሁለት ልዩ የልዩ ልዩ ቋንቋዎች የNMT ጥያቄዎችን እና በአንድ ለብዙዎች እና በአንድ ለብዙዎች የክፍል ክፍል እና የተካፈለ የክፍል አካባቢ የሚጠቀምን እና እና በተካፈለው የክፍል ኮድ እና እና እና እና በተካፈለው ይነግረናል፡፡ ከፈተናዎቻችን፣ የብዙ ልዩ ቋንቋዎች የNMT ሥርዓቶች ለሁሉም ቋንቋዎች ሁለት ሁለት ዓይነቶች የሚቆጠሩ MT ስርዓቶች እንዲያሳየፉ እናደርጋለን፡፡', 'et': 'Käesolevas artiklis tutvustame üksikasju süsteemidest, mille oleme esitanud WAT 2021 MultiIndicMT: Indic Language Multilingual Task. Oleme esitanud kaks eraldi mitmekeelset NMT mudelit: üks inglise keele 10 india keele ja teine 10 india keele kohta inglise keele. Arutleme kahe eraldi mitmekeelse NMT lähenemisviisi rakendamise üksikasju, nimelt üks paljudele ja palju ühele, mis kasutavad vastavalt ühist dekooderit ja ühist kodeerijat. Meie eksperimentide põhjal täheldame, et mitmekeelsed NMT süsteemid ületavad iga vaatlusaluse keelepaari kakskeelseid baassüsteeme.', 'fi': 'Tässä artikkelissa esittelemme yksityiskohtaiset tiedot järjestelmistä, jotka olemme toimittaneet WAT 2021 MultiIndicMT: An Indic Language Multilingual Task. Olemme toimittaneet kaksi erillistä monikielistä NMT-mallia: yksi englanniksi kymmeneen intialaiseen kieleen ja toinen kymmenelle intialaiselle kielelle englanniksi. Keskustelemme kahden erillisen monikielisen NMT-lähestymistavan toteutusyksityiskohdista, nimittäin yhdestä moneen ja monista yhteen, joissa käytetään jaettua dekooderia ja jaettua kooderia. Kokeistamme toteamme, että monikieliset NMT-järjestelmät ylittävät kaksikieliset lähtötason MT-järjestelmät kunkin tarkasteltavan kieliparin osalta.', 'cs': 'V tomto článku představujeme podrobnosti systémů, které jsme předložili pro WAT 2021 MultiIndicMT: An Indic Language Multilingual Task. Předložili jsme dva samostatné vícejazyčné NMT modely: jeden pro angličtinu do 10 indických jazyků a druhý pro 10 indických jazyků do angličtiny. Diskutujeme detaily implementace dvou samostatných vícejazyčných NMT přístupů, konkrétně jeden na mnoho a mnoho na jednoho, které využívají sdílený dekodér a sdílený kodér. Z našich experimentů pozorujeme, že vícejazyčné NMT systémy překonávají dvojjazyčné základní MT systémy pro každý z uvedených jazykových párů.', 'jv': 'Nang pepulan iki, kita akeh nggawe Detal kanggo sistem sing nyimpen kanggo nggawe WAT 2020 Multiindividual MT: Sing Language Multilanguage task. Awak dhéwé ngewat manut karo sistem sing sampeyan kanggo sampeyan NMT: sing sampeyan kanggo 10 Inggris kanggo 10 Inggris lan tanggal 10 Inggris kanggo Inggris. Awak dhéwé nggawe macem depurasi sistem sing sampeyan karo akeh multilanggar NMT sampeyan, siji-sampeyan karo akeh sampeyan sampeyan karo akeh sampeyan sampeyan. Sugeng dhéwé wis nguasai decoder lan akeh koder sampeyan sampeyan tambah. Awak dhéwé éntuk perintah sing nggawe sawak sistem NMT multinggambar kuwi nggawe sistem MT sing bisa Lingan kanggo sabên alih sing nggawe gerarané.', 'sk': 'V tem prispevku predstavljamo podrobnosti sistemov, ki smo jih predložili za WAT 2021 MultiIndicMT: An Indic Language Multilanguage Task. Predložili smo dva ločena večjezična NMT modela: enega za angleščino do 10 indijskih jezikov in drugega za 10 indijskih jezikov v angleščino. Razpravljamo o podrobnostih izvajanja dveh ločenih večjezičnih pristopov NMT, in sicer enega proti mnogim, ki uporabljata skupni dekodir oziroma skupni kodirnik. Iz naših eksperimentov ugotavljamo, da večjezični NMT sistemi presegajo dvojezične osnovne MT sisteme za vsakega od obravnavanih jezikovnih parov.', 'he': 'בעיתון הזה, אנחנו מציגים את הפרטים של המערכות ששלחנו עבור המשימה של WAT 2021 MultiIndicMT: שלחנו שני דוגמנים NMT רבותיים נפרדים: אחת לאנגלית ל-10 שפות אינדיות ואחרת ל-10 שפות אינדיות לאנגלית. אנחנו מדברים על פרטי ההפעלה של שתי גישות NMT רבות-שפות נפרדות, כלומר אחד-לרבים ורבים-לאחד, שמשתמשים במתקן משותף ומתקן משותף, בהתאם. מתוך הניסויים שלנו, אנו רואים שמערכות NMT רבות-שפות מעליפות את מערכות MT בסיסית שתיים-שפות לכל זוג השפה שנחשב.', 'ha': "Ga wannan takardan, Munã halatar da ko-daki na'ura na'ura waɗanda muka gabãtar da shi zuwa WAT 2021 multiIndicMT: An Indic language multilingui. Mun aika da misãlai biyu masu yawa na NMT: ɗayan cikin Ingiriya zuwa 10 harshen Indic kuma gudan zuwa harshen 10 cikin Ingiriya. Tuna jãyayya masu amfani da takardan kodi biyu masu haɗi na NMT cikin mulki-lingui biyu, kamar ɗabi'a-zuwa-guda da-guda, wanda ke yi amfani da kuma ko-kode mai share. Daga jarrabõyinmu, Munã ganin cewa wasu na'ura na NMT masu mulki na'ura na ƙarƙashin MT na samar da tsarin MT-na'ura biyu na'ura da aka yi bincike.", 'bo': 'འུ་ཅག་གིས་ཤོག་བྱང་འདིའི་ནང་དུ་WAT 2021 MultiIndicMT ་ལ་འཇུག་སའི་མ་ལག་གི་གསལ་བཤད་མང་ཙམ་སྟོན་ཡོད། ང་ཚོས་དབྱིབས་སྐད་ཡིག་གཟུགས་ཀྱི་དབྱེ་བ་གཉིས་སོ་སྤྲོད་ཡོད། གཅིག་ནི་ཨིན་རིའི་ནང་ལས་ སྐད་ཡིག་གཟུགས་ཀྱི་སྐད་ཡི ང་ཚོས་སྐད་ཡིག་ཆ་དབྱེ་བ་འདྲ་བའི་གསལ་བཤད་གཅིག་ལས་མང་པོ་ཞིག་གི་ནང་དུ་གཏོང་བ་རེད། ང་ཚོའི་བརྟག་ཞིབ་ལས་ སྐད་རིགས་ཀྱི་དབྱེ་ཞིབ་ལའང་། དབྱེ་ཚིག་གི་ཐབས་ལམ་སྣ་མང་པོ་ཞིག་ལའང་། སྐད་རིགས་ཆ་གཅིག་ལ་བལྟ་སྟངས་པར་'}
{'en': 'ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task ANVITA  Machine Translation System for  WAT  2021  M ulti I ndic MT  Shared Task', 'ar': 'نظام ANVITA للترجمة الآلية لمهمة WAT 2021 MultiIndicMT المشتركة', 'es': 'Sistema de traducción automática ANVITA para la tarea compartida MultiIncmit WAT 2021', 'fr': 'Système de traduction automatique ANVITA pour la tâche partagée WAT 2021 MultiIndicMT', 'pt': 'Sistema de Tradução Automática ANVITA para Tarefa Compartilhada WAT 2021 MultiIndicMT', 'zh': 'ANVITA WAT 2021多指标MT机器翻译统共之', 'ja': 'WAT 2021 MultiIndicMT共有タスク用ANVITA機械翻訳システム', 'hi': 'WAT 2021 MultiIndicMT साझा कार्य के लिए ANVITA मशीन अनुवाद प्रणाली', 'ru': 'Система машинного перевода ANVITA для совместной задачи WAT 2021 MultiIndicMT', 'ga': 'Córas Aistriúcháin Meaisín ANVITA do Thasc Comhroinnte MultiIndicMT WAT 2021', 'el': 'Σύστημα μηχανικής μετάφρασης ΑΝΒΙΤΑ για την κοινή εργασία του WAT 2021', 'hu': 'ANVITA gépi fordító rendszer WAT 2021 MultiIndicaMT megosztott feladathoz', 'ka': 'Name', 'lt': 'ANVITA mašinų vertimo sistema, skirta 2021 m. WAT daugiaindikatyviniam MT bendram uždaviniui', 'it': 'Sistema di traduzione automatica ANVITA per WAT 2021 MultiIndicaMT Shared Task', 'kk': 'WAT 2021 көп-IndicMT ортақтастырылған тапсырманың ANVITA машинаның аудару жүйесі', 'mk': 'АНВИТА машински преведувачки систем за WAT 2021 MultiIndicMT споделена задача', 'ms': 'Sistem Terjemahan Mesin ANVITA untuk Tugas Berkongsi WAT 2021 MultiIndicMT', 'mt': 'ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task', 'ml': 'WAT 2021 MultiIndicMT Shared Task', 'mn': 'ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task', 'pl': 'System tłumaczeń maszynowych ANVITA dla WAT 2021 MultiIndicMT Shared Task', 'no': 'ANVITA- maskineoversettelsystem for WAT 2021 MultiIndicMT- delt oppgåve', 'ro': 'Sistem de traducere automată ANVITA pentru WAT 2021 MultiIndicaMT Activitate partajată', 'so': 'ANVITA Mashine Translation System for WAT 2021 MultiIndicMT Shared Task', 'si': 'NAME OF TRANSLATORS', 'sv': 'ANVITA maskinĂ¶versĂ¤ttningssystem fĂ¶r WAT 2021 MultiIndicaMT delad uppgift', 'sr': 'ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task', 'ur': 'WAT 2021 MultiIndicMT Shared Task کے لئے ANVITA ماشین ترجمہ سیسٹم', 'ta': 'WAT 2021 பல்IndicMT பகிர்ந்த பணிக்கு ANVITA இயந்திர மொழிபெயர்ப்பு அமைப்பு', 'uz': 'Name', 'vi': 'Hệ thống dịch cơ bản bất mưa cho WAT 2021 multiẤn giao tác', 'bg': 'Система за машинен превод за Споделена задача', 'da': 'ANVITA maskinoversættelsessystem til WAT 2021 MultiIndicaMT delt opgave', 'nl': 'ANVITA Machine Translation Systeem voor WAT 2021 MultiIndicMT Gedeelde Taak', 'hr': 'ANVITA sustav prevoda strojeva za dijeljeni zadatak WAT 2021 MultiIndicMT', 'ko': 'WAT 2021 다중 지표 공유 작업을 위한 ANVITA 기계 번역 시스템', 'de': 'ANVITA Machine Translation System für WAT 2021 MultiIndicMT Shared Task', 'fa': 'Name', 'id': 'ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task', 'sw': 'Mfumo wa Tafsiri ya Mashiniki wa ANVITA kwa WAT 2021 MultiIndicMT', 'af': 'ANVITA Masjien Vertaling stelsel vir WAT 2021 MultiIndicMT Gedeelde Taak', 'tr': 'ANVITA Ullançy Terjime Sistemi WAT 2021 MultiIndicMT Paýlaşy Görevi', 'sq': 'ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task', 'az': 'WAT 2021 çoxlu IndicMT paylaşılan işlər üçün ANVITA Makina Çevir Sistemi', 'am': 'ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task', 'hy': 'Comment', 'bn': 'WAT 2021 বহুবার ইন্ডিক্টমেট শেয়ার করা কাজের জন্য ANVITA মেশিন অনুবাদ সিস্টেম', 'ca': 'ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task', 'bs': 'ANVITA uređajni prevodni sistem za dijeljeni zadatak WAT 2021 MultiIndicMT', 'cs': 'ANVITA Strojový překlad systém pro WAT 2021 MultiIndicMT Shared Task', 'et': 'ANVITA masintõlke süsteem WAT 2021 MultiIndicMT jagatud ülesanne', 'fi': 'ANVITA Machine Translation System WAT 2021 MultiIndicMT Jaettu tehtävä', 'jv': 'ANVista Mas Terjamahan Sistem kanggo WAT 2020 1 Multiindividual MT Ngawe task', 'sk': 'ANVITA sistem strojnega prevajanja za WAT 2021 MultiIndicMT skupno opravilo', 'ha': 'KCharselect unicode block name', 'he': 'מערכת תרגום מכונות ANVITA עבור משימה משותפת WAT 2021 MultiIndicMT', 'bo': 'ANVITA མ་ལག་གི་ཚིག་འདིའི་མ་ལག་གི་WAT 2021 MultiIndicMT རྩ་སྒྲིག་དབྱེ་བ'}
{'en': 'This paper describes ANVITA-1.0 MT system, architected for submission to WAT2021 MultiIndicMT shared task by mcairt team, where the team participated in 20 translation directions : EnglishIndic and IndicEnglish ; Indic set comprised of 10 Indian languages. ANVITA-1.0 MT system comprised of two multi-lingual NMT models one for the EnglishIndic directions and other for the IndicEnglish directions with shared encoder-decoder, catering 10 language pairs and twenty translation directions. The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization. Additionally, MultiIndicMT WAT 2021 corpora was distilled using a series of filtering operations before putting up for training. ANVITA-1.0 achieved highest AM-FM score for  EnglishBengali , 2nd for  EnglishTamil  and 3rd for  EnglishHindi , BengaliEnglish directions on official test set. In general, performance achieved by ANVITA for the IndicEnglish directions are relatively better than that of EnglishIndic directions for all the 10 language pairs when evaluated using BLEU and RIBES, although the same trend is not observed consistently when AM-FM based evaluation was carried out. As compared to  BLEU , RIBES and AM-FM based scoring placed ANVITA relatively better among all the task participants.', 'es': 'Este documento describe el sistema ANVITA-1.0 MT, diseñado para su presentación a la tarea compartida de Multi-IndicMT de WAT2021 por el equipo de mcairt, donde el equipo participó en 20 direcciones de traducción: inglés→índico e índico→inglés; conjunto índico compuesto por 10 idiomas indios. El sistema ANVITA-1.0 MT se compone de dos modelos NMT multilingües, uno para las direcciones inglés→índico y otro para las instrucciones Índico→inglés con codificador-decodificador compartido, que atiende a 10 pares de idiomas y veinte direcciones de traducción. Los modelos base se construyeron en base a la arquitectura Transformer y se capacitaron sobre corpora MultiIndiCMT WAT 2021 y se emplearon además la traducción inversa y la transliteración para el aumento selectivo de datos, y el conjunto de modelos para una mejor generalización. Además, los corpus de MultiIndiCMT WAT 2021 se destilaron mediante una serie de operaciones de filtrado antes de ponerlos en formación. ANVITA-1.0 obtuvo el puntaje más alto de AM-FM para las instrucciones en inglés→bengalí, segundo para inglés→tamil y tercero para inglés→hindi, bengalí→inglés en el conjunto de pruebas oficial. En general, el rendimiento alcanzado por ANVITA para las direcciones Indic → Inglés es relativamente mejor que el de las instrucciones Inglés → Índico para los 10 pares de idiomas cuando se evalúa con BLEU y RIBES, aunque la misma tendencia no se observa de manera consistente cuando se llevó a cabo la evaluación basada en AM-FM. En comparación con BLEU, la puntuación basada en RIBES y AM-FM colocó a ANVITA relativamente mejor entre todos los participantes en la tarea.', 'ar': 'تصف هذه الورقة نظام ANVITA-1.0 MT ، الذي تم تصميمه لتقديمه إلى مهمة WAT2021 MultiIndicMT المشتركة بواسطة فريق mcairt ، حيث شارك الفريق في 20 اتجاهًا للترجمة: الإنجليزية → الهندية والهندية → الإنجليزية ؛ تتكون المجموعة الهندية من 10 لغات هندية. يتكون نظام ANVITA-1.0 MT من نموذجين NMT متعددي اللغات أحدهما خاص بالاتجاهات الإنجليزية → الهندية والآخر للتوجيهات الهندية → الإنجليزية مع وحدة فك ترميز مشتركة ، ويوفر 10 أزواج لغوية وعشرين اتجاهًا للترجمة. تم بناء النماذج الأساسية بناءً على بنية المحولات وتم تدريبها على مجموعة MultiIndicMT WAT 2021 ، كما تم توظيف الترجمة الخلفية والترجمة الصوتية لزيادة البيانات الانتقائية ومجموعة النماذج من أجل تعميم أفضل. بالإضافة إلى ذلك ، تم تقطير مجموعة MultiIndicMT WAT 2021 باستخدام سلسلة من عمليات الترشيح قبل الاستعداد للتدريب. حققت ANVITA-1.0 أعلى درجة AM-FM للغة الإنجليزية → البنغالية ، والثانية للغة الإنجليزية → التاميلية والثالثة للغة الإنجليزية → الهندية والبنغالية → الاتجاهات الإنجليزية في مجموعة الاختبار الرسمية. بشكل عام ، الأداء الذي حققته ANVITA للإشارة إلى اللغة الإنجليزية أفضل نسبيًا من الإنجليزية → الاتجاهات الهندية لجميع أزواج اللغات العشرة عند تقييمها باستخدام BLEU و RIBES ، على الرغم من عدم ملاحظة نفس الاتجاه باستمرار عند التقييم القائم على AM-FM نفذت. بالمقارنة مع BLEU ، وضعت RIBES و AM-FM نتائج ANVITA أفضل نسبيًا بين جميع المشاركين في المهمة.', 'pt': 'Este artigo descreve o sistema ANVITA-1.0 MT, arquitetado para submissĂŁo Ă\xa0 tarefa compartilhada WAT2021 MultiIndicMT pela equipe mcairt, onde a equipe participou de 20 direĂ§Ăµes de traduĂ§ĂŁo: inglĂŞsâ†’Ă\xadndico e Ă\xadndicoâ†’inglĂŞs; Conjunto Ă\xadndico composto por 10 idiomas indianos. Sistema ANVITA-1.0 MT composto por dois modelos NMT multilĂ\xadngues um para as direĂ§Ăµes Ă\xadndicoâ†’inglĂŞs e outro para as direĂ§Ăµes Ă\xadndicoâ†’inglĂŞs com codificador-decodificador compartilhado, atendendo 10 pares de idiomas e vinte direĂ§Ăµes de traduĂ§ĂŁo. Os modelos bĂˇsicos foram construĂ\xaddos com base na arquitetura Transformer e treinados em corpora MultiIndicMT WAT 2021 e empregaram retrotraduĂ§ĂŁo e transliteraĂ§ĂŁo para aumento seletivo de dados e conjunto de modelos para melhor generalizaĂ§ĂŁo. AlĂ©m disso, os corpora MultiIndicMT WAT 2021 foram destilados usando uma sĂ©rie de operaĂ§Ăµes de filtragem antes de serem colocados em treinamento. ANVITA-1.0 alcanĂ§ou a pontuaĂ§ĂŁo AM-FM mais alta para InglĂŞsâ†’Bengali, 2Âş para InglĂŞsâ†’TĂ˘mil e 3Âş para InglĂŞsâ†’Hindi, Bengaliâ†’InglĂŞs instruĂ§Ăµes no conjunto de teste oficial. Em geral, o desempenho alcanĂ§ado pela ANVITA para as direĂ§Ăµes Ă\xadndicoâ†’inglĂŞs Ă© relativamente melhor que o das direĂ§Ăµes inglĂŞsâ†’Ă\xadndico para todos os 10 pares de idiomas quando avaliados usando BLEU e RIBES, embora a mesma tendĂŞncia nĂŁo seja observada de forma consistente quando a avaliaĂ§ĂŁo baseada em AM-FM foi realizado. Em comparaĂ§ĂŁo com BLEU, RIBES e pontuaĂ§ĂŁo baseada em AM-FM colocaram o ANVITA relativamente melhor entre todos os participantes da tarefa.', 'fr': "Cet article décrit le système de traduction automatique ANVITA-1.0, conçu pour être soumis à la tâche partagée WAT2021 MultiIndicMT par l'équipe mcairt, où l'équipe a participé à 20 directions de traduction\xa0: anglais→indic et indic→anglais\xa0; ensemble indic composé de 10 langues indiennes. Le système ANVITA-1.0 MT comprend deux modèles NMT multilingues, l'un pour les directions anglais→indic et l'autre pour les directions indic→anglais avec encodeur-décodeur partagé, offrant 10 paires de langues et vingt directions de traduction. Les modèles de base ont été construits sur la base de l'architecture Transformer et formés sur des corpus MultiIndicMT WAT 2021, puis ont utilisé la rétro-traduction et la translittération pour l'augmentation sélective des données, et un ensemble de modèles pour une meilleure généralisation. De plus, les corpus MultiIndicMT WAT 2021 ont été distillés à l'aide d'une série d'opérations de filtrage avant d'être mis en formation. ANVITA-1.0 a obtenu le score AM-FM le plus élevé pour l'anglais → le bengali, le 2e pour l'anglais → le tamoul et le troisième pour les instructions anglais→hindi, bengali→anglais sur l'ensemble de test officiel. En général, les performances obtenues par ANVITA pour les directions Indic→Anglais sont relativement meilleures que celles des directions anglais→indic pour les 10 paires de langues lorsqu'elles sont évaluées à l'aide de BLEU et RIBES, bien que la même tendance ne soit pas observée de manière cohérente lors de l'évaluation basée sur AM-FM. Par rapport à BLEU, les scores basés sur RIBES et AM-FM ont placé ANVITA relativement mieux parmi tous les participants à la tâche.", 'zh': '本文引ANVITA-1.0 MT系统,当系统由mcairt团队提交给WAT2021 MultiIndicMT共享,该团队参了20个译向:英语→印度语和印度语→英语。 印度语集10印度语言。 ANVITA-1.0 MT统以两多言NMT形,一以英语→印度语,一以印度语→英语,有共编码器 - 解码器,容10对20译。 大抵Transformer架构构,MultiIndicMT WAT 2021语料库上训练,更选择性翻译音译,以成其泛化。 此外,MultiIndicMT WAT 2021语料库于培训前用一系漉操作。 ANVITA-1.0于官方试集上得英语→Bengali最高AM-FM分数,英语→Tamil第二,英语→Hindi,孟加拉语→英语向第三。 凡用BLEU与RIBES评,ANVITA于印度语→英语方面之对优于英语→英语,虽基于AM-FM,未有始终如一之势也。 比之BLEU,RIBES基于AM-FM者评分使ANVITA在任参与者中为愈。', 'ja': '本稿では、mcairtチームがWAT 2021 MultiIndicMT共有タスクに提出するためにアーキテクチャ化されたANVITA-1.0 MTシステムについて説明します。チームは20の翻訳ディレクションに参加しました：英語インディック語と→英語インディック語。インディック→語セットは10のインドの言語で構成されています。 ANVITA-1.0 MTシステムは、2つの多言語NMTモデルで構成されています。1つ→は英語のインディック方向、もう1つは→英語のインディック方向で、エンコーダーデコーダーを共有し、10の言語ペアと20の翻訳方向に対応しています。 ベースモデルは、Transformerアーキテクチャに基づいて構築され、MultiIndicMT WAT 2021 corporaを介してトレーニングされ、選択的データ拡張のためにバック翻訳とトランスリット、およびより良い一般化のためのモデルアンサンブルをさらに採用しました。 さらに、MultiIndicMT WAT 2021コーラは、トレーニングのために準備する前に、一連のフィルタリング操作を使用して蒸留されました。 ANVITA-1.0は、英語の→ベンガル語、英語の→タミル語、英語の→ヒンディー語、ベンガル語の公式テストセットで最高の→AM - FMスコアを達成しました。 一般に、ANVITAがインディック→英語方向で達成したパフォーマンスは、BLEUとRIBESを使用して評価した場合、→10の言語ペアすべてのインディック英語方向よりも相対的に優れていますが、AM - FMベースの評価を行った場合、同じ傾向は一貫して観察されません。 BLEUと比較して、RIBESとAM - FMベースのスコアリングは、すべてのタスク参加者の中でANVITAを比較的優れたものにした。', 'hi': 'यह पेपर ANVITA-1.0 MT सिस्टम का वर्णन करता है, जिसे MCAIRT टीम द्वारा WAT2021 MultiIndicMT साझा कार्य को प्रस्तुत करने के लिए आर्किटेक्ट किया गया है, जहां टीम ने 20 अनुवाद दिशाओं में भाग लिया: अंग्रेजी→इंडिक और इंडिक→एंग्लिश; इंडिक सेट में 10 भारतीय भाषाएं शामिल थीं। ANVITA-1.0 MT प्रणाली में दो बहुभाषी NMT मॉडल शामिल हैं, जिनमें से एक अंग्रेजी→इंडिक दिशाओं के लिए और दूसरा साझा एन्कोडर-डिकोडर के साथ इंडिक→अंगलिश दिशाओं के लिए, 10 भाषा जोड़े और बीस अनुवाद दिशाओं के खानपान के लिए। बेस मॉडल को ट्रांसफॉर्मर आर्किटेक्चर के आधार पर बनाया गया था और मल्टीइंडिकएमटी डब्ल्यूएटी 2021 कॉर्पोरेट पर प्रशिक्षित किया गया था और आगे चयनात्मक डेटा वृद्धि के लिए वापस अनुवाद और लिप्यंतरण को नियोजित किया गया था, और बेहतर सामान्यीकरण के लिए मॉडल पहनावा। इसके अतिरिक्त, मल्टीइंडिकएमटी डब्ल्यूएटी 2021 कॉर्पोरेट को प्रशिक्षण के लिए रखने से पहले फ़िल्टरिंग ऑपरेशन की एक श्रृंखला का उपयोग करके आसुत किया गया था। ANVITA-1.0 ने अंग्रेजी→बेंगली के लिए उच्चतम AM-FM स्कोर, अंग्रेजी→तमिल के लिए दूसरा और आधिकारिक परीक्षण सेट पर अंग्रेजी→हिंदी, बंगाली→अंगलिश दिशाओं के लिए तीसरा हासिल किया। सामान्य तौर पर, भारतीय→अंगलिश दिशाओं के लिए ANVITA द्वारा प्राप्त प्रदर्शन BLEU और RIBES का उपयोग करके मूल्यांकन किए जाने पर सभी 10 भाषा जोड़े के लिए अंग्रेजी→इंडिक दिशाओं की तुलना में अपेक्षाकृत बेहतर होता है, हालांकि एएम-एफएम आधारित मूल्यांकन किए जाने पर एक ही प्रवृत्ति लगातार नहीं देखी जाती है। BLEU की तुलना में, RIBES और AM-FM आधारित स्कोरिंग ने सभी कार्य प्रतिभागियों के बीच ANVITA को अपेक्षाकृत बेहतर रखा।', 'ru': 'Этот документ описывает систему ANVITA-1.0 MT, архивированную для представления на WAT2021 MultiIndicMT разделенной задачей команды mcairt, где команда участвовала в 20 направлениях перевода: Английский→индикатор и→ Английский индикатор; Набор индикаторов состоит из 10 индийских языков. Система ANVITA-1.0 MT состоит из двух многоязычных моделей NMT, одна для указателей→ английского языка, а другая для указателей→ английского языка с общим кодером-декодером, 10 языковыми парами и 20 направлениями перевода. Базовые модели были построены на основе архитектуры Трансформатора и обучены по корпорациям MultiIndicMT WAT 2021, а также использовали обратный перевод и транслитерацию для селективного расширения данных и модельный ансамбль для лучшего обобщения. Кроме того, перед отправкой на обучение корпорации MultiIndicMT WAT 2021 прошли дистилляцию с использованием серии операций фильтрации. ANVITA-1.0 достигла наивысшего балла AM-FM для английского→бенгальского, 2-го для английского→тамильского и 3-го для английского→хинди, бенгальского→английского направления на официальном тестовом наборе. В целом, производительность, достигнутая ANVITA для указательных направлений→ английского языка, относительно лучше, чем для указательных направлений→ английского языка для всех 10 языковых пар при оценке с использованием BLEU и RIBES, хотя одна и та же тенденция не наблюдается последовательно, когда была проведена оценка на основе AM-FM. По сравнению с BLEU, RIBES и оценка на основе AM-FM поставили ANVITA относительно лучше среди всех участников задачи.', 'ga': 'Déanann an páipéar seo cur síos ar chóras ANVITA-1.0 MT, a ailtire le cur faoi bhráid an Tasc roinnte ag foireann mcairt chuig WAT2021 MultiIndicMT, áit ar ghlac an fhoireann páirt i 20 treoir aistriúcháin: Béarla → Indic agus Indic→Béarla; Tacar indeach comhdhéanta de 10 dteanga Indiach. Bhí córas ANVITA-1.0 MT comhdhéanta de dhá mhúnla ilteangacha NMT ceann amháin do na treoracha Béarla → Indice agus ceann eile don Indic→treoracha Béarla le comh-ionchódóir-díchódóir, ag freastal ar 10 bpéire teanga agus fiche treoir aistriúcháin. Tógadh na bunmhúnlaí bunaithe ar ailtireacht Trasfhoirmeora agus cuireadh oiliúint orthu thar corpora MultiIndicMT WAT 2021 agus baineadh úsáid as aisaistriúchán agus traslitriú breise chun sonraí roghnacha a mhéadú, agus ensemble samhlacha le haghaidh ginearálú níos fearr. Ina theannta sin, rinneadh driogadh ar chorpora MultiIndicMT WAT 2021 trí úsáid a bhaint as sraith oibríochtaí scagacháin sular cuireadh suas é le haghaidh oiliúna. Bhain ANVITA-1.0 an scór AM-FM ab airde amach don Bhéarla → Beangáilis, 2ú háit don Bhéarla → Tamailis agus 3 don Bhéarla → Hiondúis, Beangáilis → Treoracha Béarla ar an leagan trialach oifigiúil. Go ginearálta, tá feidhmíocht ANVITA do na treoracha Indeacha→Béarla réasúnta níos fearr ná an Bhéarla→Treonna indice do na 10 bpéire teanga go léir nuair a dhéantar iad a mheasúnú ag baint úsáide as BLEU agus RIBES, cé nach bhfuil an treocht chéanna le sonrú go comhsheasmhach nuair a dhéantar meastóireacht bunaithe ar AM-FM. rinneadh. I gcomparáid le BLEU, bhí scóráil bunaithe ar RIBES agus AM-FM níos fearr i measc na rannpháirtithe tasc go léir.', 'el': 'Η παρούσα εργασία περιγράφει το σύστημα ΑΝVITA-1.0 ΜΤ, σχεδιασμένο για υποβολή στο WAT2028 MultiIndicMT κοινή εργασία από την ομάδα όπου η ομάδα συμμετείχε σε 20 μεταφραστικές κατευθύνσεις: Αγγλικά Ινδικά και Ινδικά Αγγλικά. Ινδικό σύνολο αποτελούμενο από δέκα ινδικές γλώσσες. Το σύστημα αποτελείται από δύο πολυγλωσσικά μοντέλα το ένα για τις αγγλικές ινδικές κατευθύνσεις και το άλλο για τις ινδικές αγγλικές κατευθύνσεις με κοινό κωδικοποιητή-αποκωδικοποιητή, εξυπηρετώντας δέκα γλωσσικά ζεύγη και είκοσι μεταφραστικές κατευθύνσεις. Τα μοντέλα βάσης κατασκευάστηκαν με βάση την αρχιτεκτονική μετασχηματιστών και εκπαιδεύτηκαν πάνω από σώματα και χρησιμοποιήθηκαν περαιτέρω μεταγραφή και μεταγραφή για επιλεκτική αύξηση δεδομένων και σύνολο μοντέλων για καλύτερη γενίκευση. Επιπλέον, τα σώματα αποστολήθηκαν χρησιμοποιώντας μια σειρά λειτουργιών φιλτραρίσματος πριν από την εκπαίδευση. Η ΑΝVITA-1.0 πέτυχε την υψηλότερη βαθμολογία για τα Αγγλικά Βεγγαλικά, 2η για τα Αγγλικά Ταμίλ και 3η για τα Αγγλικά Χίντι, Βεγγαλικά Αγγλικά κατευθύνσεις στα επίσημα τεστ. Γενικά, οι επιδόσεις που επιτεύχθηκαν από το ANVITA για τις κατευθύνσεις ινδικής αγγλικής είναι σχετικά καλύτερες από αυτές των αγγλικών ινδικών κατευθύνσεων για όλα τα δέκα γλωσσικά ζεύγη όταν αξιολογήθηκαν με BLEU και RIBES, αν και η ίδια τάση δεν παρατηρείται με συνέπεια όταν πραγματοποιήθηκε αξιολόγηση βασισμένη στην AM-FM. Σε σύγκριση με την BLEU, η βαθμολογία RIBES και AM-FM τοποθέτησε την ANVITA σχετικά καλύτερη μεταξύ όλων των συμμετεχόντων στην εργασία.', 'ka': 'ამ წიგნის აღწერა ANVITA-1.0 MT სისტემა, რომელიც WAT2021 MultiIndicMT გაყოფილი პარამეტრების გაყოფილი მოკვირტის ჯგუფიდან აღწერა, სადაც ჯგუფი გაყოფილი 20 წიგნაში: ანგლისური ინდიური და ინდიური ანგლისურ ინდექური ნახვა 10 ინეთანური ენების შექმნა. ANVITA ბაზის მოდელები შექმნა ტრანფორმეტრის აქტიქტიქტურის ბაზეზე და მრავალ ინდისტიკMT WAT 2021-ის კოპორაზე განაკეთებულია და უფრო მეტად დამუშავებულია განაკეთება და ტრანფლიტურაცია მონაცემებ დამატებით, MultiIndicMT WAT 2021-ის კოპორაცია განსხვავებულია, რომელიც ფილტრირების კოპერაციების გამოყენებას წინ შემდეგ განსხვავება. ANVITA-1.0 მიიღეთ ყველაზე დიდი AM-FM მონაცემები ინგლისური ბენდალისთვის, მეორე ანგლისური რამილისთვის და მეორე ანგლისური ჰინდისთვის, ბენდალისური ანგლისური მონაცემების ყველაფერად, ANVITA-ს გამოიყენება ინდიური ანგლისური დაწყენებისთვის პარამეტრებით უკეთესია, ვიდრე ანგლისური ინდიური დაწყენებებისთვის ყველა 10 ენის ზოგებისთვის, როდესაც BLEU და RIBES გამოყენებულია, მაგრამ იგივე ტენდე როგორც BLEU-ზე, RIBES და AM-FM-ზე დამატებულია, ანVITA-ს ყველა უფრო უფრო მეტი დამატებულია.', 'it': "Questo articolo descrive il sistema MT ANVITA-1.0, progettato per l'invio al compito condiviso WAT201MultiIndicaMT dal team mcairt, dove il team ha partecipato in 20 direzioni di traduzione: Inglese Indico e Inglese Indico; Set indico composto da 10 lingue indiane. Sistema ANVITA-1.0 MT composto da due modelli NMT multilingue uno per le direzioni indiche inglesi e l'altro per le direzioni indiche inglesi con encoder-decoder condiviso, catering 10 coppie linguistiche e venti direzioni di traduzione. I modelli base sono stati costruiti sulla base dell'architettura Transformer e formati su corpora MultiIndicaMT WAT 2021 e hanno ulteriormente impiegato la traduzione e la traslitterazione posteriore per l'aumento selettivo dei dati e l'insieme di modelli per una migliore generalizzazione. Inoltre, MultiIndicaMT WAT 2021 corpora è stata distillata utilizzando una serie di operazioni di filtraggio prima di mettersi in formazione. ANVITA-1.0 ha ottenuto il punteggio AM-FM più alto per l'inglese bengalese, il secondo per l'inglese tamil e il terzo per l'inglese hindi, le indicazioni bengalesi inglesi sul set di test ufficiale. In generale, le prestazioni raggiunte da ANVITA per le direzioni Indico Inglese sono relativamente migliori di quelle delle direzioni Indico Inglese per tutte le 10 coppie linguistiche se valutate utilizzando BLEU e RIBES, anche se la stessa tendenza non è osservata costantemente quando è stata effettuata la valutazione AM-FM. Rispetto al BLEU, il punteggio basato su RIBES e AM-FM ha posizionato ANVITA relativamente meglio tra tutti i partecipanti al compito.", 'kk': 'Бұл қағаз ANVITA-1.0 MT жүйесін анықтайды, WAT2021 көп индикциялық және индикциялық ағылшын тілінде ортақ тапсырманы ортақтастыру үшін архитектеген, бұл команда 20 аудармалы бағыттарына қатынады: ағылшын тіл Үндік тілдері 10 түрлі. ANVITA- 1. 0 MT жүйесі бірінші тілді NMT үлгілерін бірінші ағылшын индикалық бағыттары және басқа индикалық ағылшын бағыттары ортақтастырылған кодер- декодері, 10 тілді екі және 20 аудармалы бағыттары үшін бірінші ті Негізгі үлгілер Түрлендіру архитектурасына негізделген және көптеген MultiIndicMT WAT 2021 корпора арқылы оқылған және таңдалған деректерді жақсарту үшін қайта аудару және транслитерация және жақсы жалпы жалпы түрлен Қосымша, Корпора 2021 жылдың көпIndicMT WAT бақылау үшін бірнеше сүзгілеу операцияларын қолданып бөлінген. ANVITA- 1. 0 ағылшын бенгали тілінің ең жоғары AM- FM нәтижесін, ағылшын тілінің 2- ші нәтижесін, ағылшын тілінің 3- ші нәтижесін ағылшын тілінде, бенгали ағылшын тілінің Ағылшын тілінде ANVITA бағыттарына жеткізілген жылдамдығы, BLEU және RIBES қолданылатын 10 тіл жиіліктерінің ағылшын тілінің бағыттауларынан сәйкес жақсы болады, бірақ AM- FM негізіндегі оқиғаларының бір тенденциясы тәуелсіз. БЛЕУ, RIBES және AM-FM негіздеген сұрақтар, ANVITA және барлық тапсырма қатысушылардың арасында салыстырып жақсы болды.', 'hu': 'Ez a tanulmány bemutatja az ANVITA-1.0 MT rendszert, amelyet a mcairt csapat WAT2011 MultiIndicaMT megosztott feladatához terveztek, ahol a csapat 20 fordítási irányban vett részt: angol indic és indic angol; Az indiai készlet 10 indiai nyelvből áll. Az ANVITA-1.0 MT rendszer két többnyelvű NMT modellből áll, az egyik az angol indikus irányba, a másik az indikus angol irányba, közös kódoló-dekódolóval, 10 nyelvpár és húsz fordítási irányba. Az alapmodelleket Transformer architektúra alapján építették, és a MultiIndicaMT WAT 2021 corpora felett képezték, továbbá visszafordítást és transzliterációt alkalmaztak a szelektív adatok nagyobbításához, valamint modelleket a jobb általánosításhoz. Ezenkívül a MultiIndicaMT WAT 2021 corporát számos szűrési művelet alkalmazásával lepárlították, mielőtt képzésre álltak volna. Az ANVITA-1.0 a legmagasabb AM-FM pontszámot érte el az angol bengáli, a második az angol tamil és a harmadik az angol hindi, a bengáli angol utasításokat a hivatalos tesztkészleten. Általánosságban az ANVITA által az indiai angol irányban elért teljesítmény viszonylag jobb, mint az angol indiai irányban mind a 10 nyelvpár esetében BLEU és RIBES használatával értékelve, bár az AM-FM alapú értékelés során nem figyelhető meg következetesen ugyanez a tendencia. A BLEU-hoz képest a RIBES és AM-FM alapú pontszámok viszonylag jobban helyezték az ANVITA-t az összes feladat résztvevője között.', 'lt': 'Šiame dokumente aprašoma ANVITA-1.0 MT sistema, sukurta pateikti WAT2021 MultiIndicMT bendrai mcairt komandos užduotims, kuriose komanda dalyvavo 20 vertimo krypčių: anglų indikas ir indikas anglų; Indinis rinkinys sudarytas iš 10 indijos kalbų. ANVITA-1.0 MT sistemą sudaro du daugiakalbiai NMT modeliai: vienas anglų indinių krypčių modelis ir kitas indinių anglų krypčių modelis su bendrais kodavimo kodais, kuriame yra 10 kalbų poros ir dvidešimt vertimo krypčių. Pagrindiniai modeliai buvo sukurti remiantis Transformer architektūra ir apmokyti per MultiIndicMT WAT 2021 korpra ir toliau buvo naudojami grįžtamasis vertimas ir transliteracija selektyviems duomenims padidinti bei modelių komplektas geresnei generalizacijai. Be to, prieš pradedant mokytis, MultiIndicMT WAT 2021 korpra buvo distiliuota naudojant įvairias filtravimo operacijas. ANVITA-1.0 pasiekė didžiausią AM-FM tašką anglų Bengalų, antrąjį anglų Tamil ų ir trečiąjį anglų Hindų, bengalų anglų nurodymus oficialiame bandymų rinkinyje. In general, performance achieved by ANVITA for the Indic English directions are relatively better than that of English Indic directions for all the 10 language pairs when evaluated using BLEU and RIBES, although the same trend is not observed consistently when AM-FM based evaluation was carried out.  Palyginti su BLEU, RIBES ir AM-FM vertinimu, ANVITA buvo palyginti geresnis visų užduoties dalyvių skaičius.', 'ms': 'Kertas ini menggambarkan sistem ANVITA-1.0 MT, dirancang untuk dihantar ke WAT2021 MultiIndicMT tugas berkongsi oleh pasukan mcairt, di mana pasukan berpartisipasi dalam 20 arah terjemahan: Inggeris Indic dan Inggeris Indic; Set indik terdiri dari 10 bahasa India. Sistem ANVITA-1.0 MT terdiri dari dua model NMT berbilang-bahasa satu untuk arah Inggeris Indic dan lain untuk arah Inggeris Indic dengan pengekod-dekoder berkongsi, catering 10 pasangan bahasa dan 20 arah terjemahan. Model asas dibina berdasarkan arkitektur Transformer dan dilatih melalui korpra MultiIndicMT WAT 2021 dan menggunakan lagi terjemahan dan transliterasi untuk peningkatan data selektif, dan ensemble model untuk peningkatan yang lebih baik. Lagipun, korpra MultiIndicMT WAT 2021 telah dipotong menggunakan siri operasi penapisan sebelum memasang untuk latihan. ANVITA-1.0 mencapai skor AM-FM tertinggi untuk Bengali Inggeris, kedua untuk Tamil Inggeris dan ketiga untuk Hindi Inggeris, arah Inggeris Bengali pada set ujian rasmi. Secara umum, prestasi yang dicapai oleh ANVITA untuk arahan Inggeris India adalah relatif lebih baik daripada arahan Inggeris India untuk semua 10 pasangan bahasa apabila diteliti menggunakan BLEU dan RIBES, walaupun trend yang sama tidak diawasi secara konsisten apabila penelitian berdasarkan AM-FM dilakukan. Berbanding dengan BLEU, nilai berasaskan RIBES dan AM-FM meletakkan ANVITA relatif lebih baik di antara semua peserta tugas.', 'ml': 'ഈ പത്രത്തില്\u200d ANVITA-1. 0 എംടി സിസ്റ്റത്തെ വാട്ട്2021 മള്\u200dട്ടിക്ഇന്\u200dഡിക്എം ടീമില്\u200d പങ്കെടുത്ത ജോലിയില്\u200d പങ്കുചേര്\u200dക്കാന്\u200d സ്ഥാപിക്കപ്പെട്ട 10 ഇന്ത്യന്\u200d ഭാഷകളില്\u200d ഉള്ള സിഡിക്ക് സജ്ജീകരിച്ചിരിക്കുന്നു. ANVITA- 1. 0 എംടി സിസ്റ്റത്തില്\u200d രണ്ടു പല ഭാഷകളുടെ NMT മോഡലുകളില്\u200d ഒന്ന് ഇംഗ്ലീഷ് ഇംഗ്ലീഷ് നേര്\u200dവഴികള്\u200dക്കും മറ്റൊരാള്\u200dക്കും വേണ്ടി പങ്കുചേര്\u200dന്ന കോഡോ ട്രാന്\u200dസ്ഫോര്\u200dമാന്\u200d ആര്\u200dക്ടിക്കേറ്റര്\u200d അടിസ്ഥാനത്തില്\u200d നിര്\u200dമ്മിക്കപ്പെട്ടിരിക്കുന്ന മോഡലുകള്\u200d നിര്\u200dമ്മിക്കപ്പെട്ടിരിക്കുന്നു. പല ഇന്\u200dഡിക്മെ കൂടാതെ, ട്രെയിനിങ്ങള്\u200dക്ക് വേണ്ടി പ്രയോഗിക്കുന്നതിനു മുമ്പ് ഒരു സിരിക്കല്\u200d ഫില്\u200dറ്റര്\u200dമിങ്ങ് പ്രവര്\u200dത്തനങ്ങള്\u200d ഉപയോഗ ANVITA-1. 0 ഇംഗ്ലീഷ് ബംഗാലിക്ക് ഏറ്റവും ഉയര്\u200dന്ന എംഎഫ്-എം സ്കോര്\u200d എത്തി. രണ്ടാമതായി ഇംഗ്ലീഷ് ടാമിലിനും മൂന്നാമത്തേതും ഇംഗ്ലീഷ് ഹ പൊതുവായി, ഇന്ത്യ ഇംഗ്ലീഷ് നേര്\u200dവഴികള്\u200dക്കായി ANVITA സമ്പാദിച്ച പ്രകടനം ബില്യൂ ഉപയോഗിക്കുമ്പോള്\u200d എല്ലാ ഭാഷകള്\u200dക്കും വിലാസപ്പെടുത്തുമ്പോള്\u200d ഇംഗ്ലീഷ് ഇംഗ്ലീഷ് വഴ ബ്ലൂയുടെ താല്\u200dപ്പര്യത്തില്\u200d റിബീസും എമ്-എഫം അടിസ്ഥാനമായ സ്കോരിങ്ങും ANVITA എല്ലാ ജോലിയിലും പങ്കാളികളുടെയും കൂട്ടത്തി', 'mk': 'This paper describes ANVITA-1.0 MT system, architected for submission to WAT2021 MultiIndicMT shared task by mcairt team, where the team participated in 20 translation directions: English Indic and Indic English;  Индиски сет сочинува 10 индиски јазици. АНВИТА-1.0 МТ систем се состои од два мултијазични НМТ модели, еден за англиските индиски насоки, а друг за индиски англиски насоки со заеднички кодер-декодер, со 10 јазични пари и 20 насоки за превод. Базичните модели беа изградени врз основа на трансформената архитектура и обучени врз корпората MultiIndicMT WAT 2021 и понатаму ги вработуваа преводите и транслитерацијата за селективно зголемување на податоците, и моделниот ансембл за подобра генерализација. Покрај тоа, мултииндиcMT WAT 2021 корпората беше дестилирана користејќи серија филтрирачки операции пред да се подготви за обука. АНВИТА-1.0 постигна највисока оценка на AM-FM за англиски бенгали, втора за англиски тамил и трета за англиски хинди, бенгали англиски насоки на официјалниот тест сет. Општо, резултатите постигнати од АНВИТА за индичките англиски насоки се релативно подобри од оние на англиските индиски насоки за сите 10 парови јазици кога се оценуваат користејќи БЛЕУ и РИБЕС, иако истиот тренд не се набљудува константно кога се спроведува оценката базирана на АМ-Ф Во споредба со БЛЕУ, резултатите на РИБЕС и АМ-ФМ ја ставија АНВИТА релативно подобро меѓу сите учесници на задачата.', 'mt': 'Dan id-dokument jiddeskrivi s-sistema ANVITA-1.0 MT, arkitettata għas-sottomissjoni lil WAT2021 MultiIndicMT kompitu kondiviż minn tim mcairt, fejn it-tim ipparteċipa f’20 direzzjoni ta’ traduzzjoni: Ingliż Indiku u Ingliż Indiku; Sett indiku magħmul minn 10 lingwi Indjani. Sistema ANVITA-1.0 MT magħmula minn żewġ mudelli NMT multilingwi wieħed għad-direzzjonijiet Indiċi Ingliż u ieħor għad-direzzjonijiet Indiċi Ingliż b’kodiċi-dekoder komuni, b’catering 10 pairs ta’ lingwi u għoxrin direzzjoni ta’ traduzzjoni. Il-mudelli bażiċi nbnew abbażi tal-arkitettura Transformer u mħarrġa fuq il-korpura MultiIndicMT WAT 2021 u impjegaw aktar traduzzjoni u traslitterazzjoni lura għal żieda selettiva fid-dejta, u ensemble mudell għal ġeneralizzazzjoni aħjar. Barra minn hekk, il-korpra WAT 2021 MultiIndicMT ġiet distillata permezz ta’ serje ta’ operazzjonijiet ta’ filtrazzjoni qabel ma bdiet taħriġ. ANVITA-1.0 achieved highest AM-FM score for English Bengali, 2nd for English Tamil and 3rd for English Hindi, Bengali English directions on official test set.  B’mod ġenerali, il-prestazzjoni miksuba minn ANVITA għad-direzzjonijiet Indiċi Ingliżi hija relattivament aħjar minn dik tad-direzzjonijiet Indiċi Ingliżi għall-10 pari lingwistiċi kollha meta evalwati bl-użu ta’ BLEU u RIBES, għalkemm l-istess tendenza mhijiex osservata b’mod konsistenti meta twettqet l-evalwazzjoni bbażata fuq AM-FM. Meta mqabbel mal-punteġġ ibbażat fuq BLEU, RIBES u AM-FM poġġew ANVITA relattivament aħjar fost il-parteċipanti kollha fil-kompitu.', 'mn': 'Энэ цаас ANVITA-1.0 MT системийг тайлбарлаж байна. WAT2021-д олон Индик MT-г Маккарт багийны хуваалцах ажлын архитектор зохион байгуулагдсан бөгөөд баг 20 хөрөнгө оруулагдсан: Англи хэлний, Индик хэлний хэлний хэлний хэлний Энэтхэг хэл нь 10 Энэтхэг хэл болсон. ANVITA Энэ суурь загварууд Трансформер архитектур дээр суурилсан бөгөөд MultiIndicMT WAT 2021 оны корпора дээр сургалт хийгдсэн бөгөөд сонголт өгөгдлийн нэмэгдүүлэлтийн хувьд илүү олон төрлийн загварууд болон хувьд ажиллаж байсан. Мөн олон ИндийцMT WAT 2021 оны корпора сургалтын төлөө хийхээс өмнө цэвэрлэх үйл ажиллагааг ашиглаж байсан. ANVITA-1.0 Англи хэлний бангалийн хамгийн өндөр AM-FM оноо, 2-р Англи хэлний тамил, 3-р Англи хэлний, Бенгали хэлний англи хэлний шалгалтын шалгалтын тухай хамгийн өндөр оноо гарсан. Ерөнхийдөө ANVITA-ын Индийнх Англи хэлний хэлбэрээс гаргасан үйл ажиллагаа БЛЕУ болон РИБЕС-г ашиглах үед бүх 10 хэлний холбоонуудын харьцангуй илүү дээр байдаг. Гэхдээ AM-FM-ын үндсэн дүгнэлт хийх үед адилхан тенденс байнга ажиглах BLEU-тэй харьцуулахад, RIBES болон AM-FM-ийн суурь зохиол нь ANVITA-г бүх ажлын оролцогчдын хооронд харьцуулахад илүү сайн болгосон.', 'no': 'Denne papiret beskriver ANVITA-1.0 MT- systemet, arkitektert for å senda til WAT2021 MultiIndicMT delt oppgåve av mcairt- gruppa, der gruppa delta i 20 oversettelsretningar: engelsk indisk og indisk engelsk. Indisk sett inneheld av 10 indiske språk. ANVITA-1.0 MT-systemet inneheld av to fleirspråk NMT-modeller éin for engelske indiske retningar og andre for indiske engelske retningar med delte koder-dekoder, katering av 10 språk-par og tjue oversettelsretningar. Grunnmodellen vart bygd basert på Transformeringsarkitekturen og trained over MultiIndicMT WAT 2021 korpora og framleis arbeida tilbakeomsetjing og transliterasjon for selektive økning av data og modellen for betre generalisering. I tillegg var korporasjonen «MultiIndicMT WAT 2021» distilert med ei rekke filtreringar før opprettinga for opplæring. ANVITA-1.0 oppnådd høgste AM-FM-poeng for engelsk Bengali, 2 for engelsk Tamil og 3 for engelsk hindisk, Bengalisk engelsk retningar på offisielle testsett. Generelt er effekten oppnådd av ANVITA for indiske engelske retningar relativt bedre enn det av engelske indiske retningar for alle 10 språkparene når det er evaluert med BLEU og RIBES, selv om det samme trenden ikkje er observert konsekvent når AM-FM-basert evaluering er utført. Som samanlikna med BLEU, er RIBES og AM-FM-basert oppløysing plassert ANVITA relativt bedre blant alle oppgåvedeltakarane.', 'pl': 'Niniejszy artykuł opisuje system ANVITA-1.0 MT, zaprojektowany do zgłoszenia do WAT2028 MultiIndicMT wspólnego zadania zespołu mcairt, w którym zespół uczestniczył w 20-tych kierunkach tłumaczenia: angielski indyjski i indyjski angielski; Zestaw indyjski składający się z dziesięciu języków indyjskich. System MT ANVITA-1.0 składający się z dwóch wielojęzycznych modeli NMT jeden dla kierunków angielskich Indic oraz drugi dla kierunków angielskich Indic ze wspólnym koderem-dekoderem, obsługujący 10-pary językowe i dwadzieścia kierunków tłumaczenia. Modele bazowe zostały zbudowane w oparciu o architekturę Transformera i przeszkolone na korporach MultiIndicMT WAT 2021, a następnie wykorzystano tłumaczenie wsteczne i transliterację do selektywnego powiększania danych oraz zespół modeli dla lepszego uogólnienia. Dodatkowo korpusy MultiIndicMT WAT 2021 zostały destylowane przy użyciu serii operacji filtrowania przed rozpoczęciem szkolenia. ANVITA-1.0 osiągnął najwyższy wynik AM-FM dla angielskiego bengalskiego, drugi dla angielskiego tamilskiego i trzeci dla angielskiego hindi, bengalskiego angielskiego na oficjalnym zestawie testowym. Ogólnie rzecz biorąc, wyniki osiągnięte przez ANVITA dla kierunków angielskiego indyjskiego są stosunkowo lepsze niż kierunki angielskiego indyjskiego dla wszystkich dziesięciu par językowych podczas oceny BLEU i RIBES, chociaż ten sam trend nie jest konsekwentnie obserwowany w przypadku oceny opartej na AM-FM. W porównaniu do BLEU, oceny RIBES i AM-FM umieściły ANVITA stosunkowo lepiej wśród wszystkich uczestników zadania.', 'si': 'මේ පැත්තේ ANVITA-1.0 MT පද්ධතිය විස්තර කරනවා, WAT2021 MultiIndicMT කණ්ඩායමේ කණ්ඩායමේ ක්\u200dරියාත්මක විස්තර කරනවා, කෙනෙක් අනුවාර්ථාපනය 20 පැත්තක් ව Indic set comprised of 10 Indian language. ANVITA-1.0 MT මූලික මොඩේල් නිර්මාණය කරලා තියෙන්නේ නිර්මාණකරු ස්ථාපනය සහ MultiIndicMT WAT 2021 කොර්පෝරා වලින් ප්\u200dරශ්නය කරලා තියෙන්නේ නිර්මාණය සහ ප්\u200dර තවත්, MultiIndicMT WAT 2021 කාර්පෝරා ප්\u200dරශ්නයක් වෙන්න කලින් ප්\u200dරශ්නයක් කරන්න කලින් ප්\u200dරශ්නයක් පාවිච්චි කරනවා. ANVITA සාමාන්\u200dයයෙන්, ඉන්දික ඉංග්\u200dරීසි ප්\u200dරකාරය සඳහා ANVITA වලින් ප්\u200dරාර්ථාව ප්\u200dරාර්ථා කරලා ඉංග්\u200dරීසි ඉංග්\u200dරීසි ප්\u200dරකාරයෙන් සියළුම 10 භාෂාව ප්\u200dරයෝජනය BLUE, RIBES සහ AM-FM ආධාරිත ස්කෝරින් එක්ක සාමාන්\u200dයයෙන් ANVITA එක්ක සාමාන්\u200dයයෙන් හොඳ වැඩක් තියෙනවා.', 'ro': 'Această lucrare descrie sistemul ANVITA-1.0 MT, arhitecturat pentru transmiterea la sarcina partajată WAT201MultiIndicaMT de echipa mcairt, unde echipa a participat la 20 de direcții de traducere: Engleză Indică și Engleză Indică; Set indic format din 10 limbi indiene. Sistemul ANVITA-1.0 MT cuprinde două modele NMT multilingve unul pentru direcțiile indice engleze și altul pentru direcțiile indice indice cu encoder-decoder comun, care servește 10 perechi de limbi și douăzeci de direcții de traducere. Modelele de bază au fost construite pe baza arhitecturii Transformer și instruite pe corpore MultiIndicaMT WAT 2021 și au folosit în continuare traducerea și transliterarea înapoi pentru mărirea selectivă a datelor și ansamblul de modele pentru o mai bună generalizare. În plus, corpora MultiIndicaMT WAT 2021 a fost distilată folosind o serie de operațiuni de filtrare înainte de a se pune pentru formare. ANVITA-1.0 a obținut cel mai mare scor AM-FM pentru limba engleză Bengali, al doilea pentru limba engleză tamilă și al treilea pentru limba engleză hindi, indicațiile Bengali engleză pe setul oficial de test. În general, performanțele obținute de ANVITA pentru direcțiile indice engleze sunt relativ mai bune decât cele ale direcțiilor indice engleze pentru toate cele 10 perechi de limbi atunci când sunt evaluate folosind BLEU și RIBES, deși aceeași tendință nu este observată în mod constant atunci când s-a efectuat evaluarea AM-FM. Comparativ cu BLEU, scorurile RIBES și AM-FM au plasat ANVITA relativ mai bine în rândul tuturor participanților la sarcini.', 'sr': 'Ovaj papir opisuje ANVITA-1.0 MT sistem, arhitektiran za predavanje WAT2021 MultiIndicMT zajedničkog zadatka Mekartskog tima, gde je tim sudjelovao u 20 prijevoznih uputa: engleski indički i indički engleski jezik; Indijski set sastoji od 10 Indijskih jezika. ANVITA-1.0 MT sistem sastoji od dva višejezičkih NMT modela jedan za engleske indijske upute, a drugi za indijske engleske upute sa zajedničkim koderom-dekoderom, hranom 10 jezičkih parova i dvadeset uputa za prevođenje. Bazni modeli su izgrađeni na temelju arhitekture transformera i obučeni preko korporacije MultiIndicMT WAT 2021. godine i dalje zaposleni povratni prevod i transliteracija za povećanje selektivnih podataka i model za bolju generalizaciju. Dodatno, korporacija MultiIndicMT WAT 2021 bila je destilirana korištenjem serije filtracionih operacija pre nego što je postavila za obuku. ANVITA-1.0 je postigao najviši rezultat AM-FM za engleski Bengali, 2 za engleski Tamil i 3 za engleski hindi, Bengalski engleski direkcije na službenom setu testova. Općenito, provedba koju je postigla ANVITA za indijske engleske direkcije relativno bolja od strane engleskih indijskih direkcija za sve 10 jezičkih parova kada su procenili koristeći BLEU i RIBES, iako isti trend se ne posmatra u konsekvenciji kada je provedena procjena na temelju AM-FM-a. U usporedbi sa BLEU-om, izviđanje na temelju RIBES i AM-FM postavilo je ANVITA relativno bolje među svim sudionicima zadatka.', 'sv': 'Denna uppsats beskriver ANVITA-1.0 MT system, som är utformat för inlämning till WAT201MultiIndicaMT delad uppgift av mcairt team, där teamet deltog i 20 översättningsinstruktioner: Engelska Indiska och Indiska Engelska; Indisk uppsättning bestående av 10 indiska språk. ANVITA-1.0 MT-systemet består av två flerspråkiga NMT-modeller en för den engelska indiska riktningen och en för den indiska engelska riktningen med delad encoder-dekoder, catering 10 språkpar och tjugo översättningsinstruktioner. Basmodellerna byggdes baserat på Transformer-arkitektur och utbildades över MultiIndicaMT WAT 2021 corpora och använde ytterligare bakåtöversättning och transliteration för selektiv dataökning och modellenensemble för bättre generalisering. Dessutom destillerades MultiIndicaMT WAT 2021 corpora med hjälp av en rad filtreringsoperationer innan de ställdes upp för utbildning. ANVITA-1.0 uppnådde högsta AM-FM poäng för engelska bengaliska, 2:a för engelska tamil och 3:e för engelska hindi, bengaliska engelska riktningar på officiella testset. Generellt sett är resultaten uppnådda av ANVITA för indisk engelska riktningar relativt bättre än för engelska indisk riktningar för alla de 10 språkparen vid utvärdering med BLEU och RIBES, även om samma trend inte observeras konsekvent när AM-FM-baserad utvärdering genomfördes. Jämfört med BLEU placerade RIBES och AM-FM-baserade poäng ANVITA relativt bättre bland alla uppgiftsdeltagare.', 'so': 'Warqadan ayaa ka qoran ANVITA-1.0 MT system, taas oo loo arkey in loo soo dhiibo WAT2021 MultiIndicMT shaqada loo qaybsado kooxda mcairt, meesha ay kooxda ka qeybqaaday 20 hagaajiyada turjumaadda: Ingiriis Indic and Indic Ingiriis; Taariikhda muusikada ee ku qoran 10 luuqadood oo Indian ah. ANVITA-1.0 MT waxaa ka mid ah laba model oo af badan oo NMT ah mid u qoran hagitaanka ingiriisiga ingiriisiga iyo mid kaloo u qoran hagitaanka ingiriisiga ee ku qoran codcoder-decoder, oo kooxa 10 labo oo af badan iyo labaatan hagaajiya turjumaadda. Tusaalada aasaasiga ah waxaa lagu dhisay dhismaha turjumista, waxaana lagu baray wax ka badan MultiIndicMT WAT 2021 shirkadda, waxaana lagu isticmaalay turjumista dib iyo tarjumista si loo kordhiyo data doorashada, waxaana lagu sameyn karaa tusaale-qaab si aad u fiican u dhalashadaan. Sidoo kale, Shirkadda badan IndicMT WAT 2021 waxaa lagu kala soocay isticmaalka shirkado filtering badan ka hor inta aan waxbarasho u sameyn. ANVITA-1.0 waxay gaadhay score ugu sareeya AM-FM ee afka ingiriiska Bengali, 2aad ee ingiriisiga Tamil iyo 3aad ee Ingiriis Hindi, hagaha afka Ingiriis ee ku qoran imtixaanka rasmi ah. Inta caadiga ah, bandhigyada ANVITA ee hagitaanka ingiriisiga ee ANVITA waa ka fiican yihiin hagitaanka ingiriisiga ee ku qoran 10 luqada oo dhan marka lagu qiimeynayo isticmaalka BLEU iyo RIBES, in kastoo aan la eegin isku dabiicadda marka qiimeynta AM-FM lagu sameeyo. As compared to BLEU, RIBES and AM-FM based scoring placed ANVITA relatively better among all the task participants.', 'ur': 'This paper describes ANVITA-1.0 MT system, architected for submission to WAT2021 MultiIndicMT shared task by mcairt team, where the team participated in 20 translation directions: English Indic and Indian English; ۱۰ انڈیان زبانوں میں شامل ہے. ANVITA بنسٹ موڈل ترنسفور معماری پر بنیاد بنائے گئے اور MultiIndicMT WAT 2021 کورپورا پر آموزش کی گئی اور اضافہ کی ترجمہ اور ترنسلیٹ کے لئے گزینٹیوں ڈیٹا اضافہ کرنے کے لئے استعمال کی گئی اور موڈل بہترین عمومی کے لئے استعمال کی گئی۔ اور اضافہ، MultiIndicMT WAT 2021 کورپورا ترکین کے لئے استعمال کرنے سے پہلے فیلٹرینگ عملیات کے ایک سری استعمال سے جدا کیا گیا تھا. ANVITA-1.0 انگلیسی بنگالی کے لئے سب سے بلند AM-FM اسکور پہنچ گئے، انگلیسی تامیل کے لئے دوسرا اور تیسرا انگلیسی هندی کے لئے، بنگالی انگلیسی انگلیسی طریقے رسمی امتحان سٹ کے لئے۔ عمومی طور پر، انڈیس انگلیسی طریقوں کے لئے ANVITA کے ذریعہ پہنچائے ہوئے عملکرد ان سے بہتر ہے جب BLEU اور RIBES کے ذریعہ مطابق مطابق کیا گیا تھا، اگرچہ ان ہی طریقوں کو اس طرح مطابق نہیں دیکھا جاتا جب AM-FM بنیاد مطابق مطابق مطابق کیا گیا تھا. BLEU کے مقابلہ میں، RIBES اور AM-FM بنی اسکور نے ANVITA کو تمام کام شرکت کرنے والوں میں نسبتا بہتر بنایا۔', 'ta': 'இந்த தாள் ANVITA- 1. 0 MT அமைப்பை குறிப்பிடுகிறது, WAT2021 MultiIndicMT பங்கிட்ட பணிக்கு உருவாக்கப்பட்டது, மிக்கார்ட் குழு 20 மொழிபெயர்ப்பு திசைகளில் பங்கி 10 இந்தியன் மொழிகளில் சிந்தி அமைப்பு. ANVITA- 1. 0 MT அமைப்பு இரண்டு பல- மொழி NMT மாதிரிகளில் ஒன்று ஆங்கிலத்தின் சிறு திசை தேர்வுகளுக்கும் மற்றும் மற்றும் மற்றும் சிந்திய ஆங்கிலத்தின் குறியீ அடிப்படை மாதிரிகள் மாற்று உருவாக்கப்பட்டது மற்றும் பயிற்சிக்கப்பட்டது MultiIndicMT WAT 2021 நிறுவனத்திற்கு மேலும் மீண்டும் பயிற்சி மற்றும் மீண்டும் மொழி மேலும், பயிற்சிக்கு அமைப்பதற்கு முன்னால் பல சிந்திக்MT WAT 2021 நிறுவனம் ஒரு சில வடிகட்டி செயல்பாடுகளை பயன்படுத்தி வித்த ANVITA- 1. 0 ஆங்கிலத்திற்கான ஆங்கிலத்திற்கு AM- FM மதிப்பெண்கள் அடைந்தது, ஆங்கிலத்திற்கு ஆங்கிலத்திற்கு 2 ஆங்கிலத்திற்கு மூன்றாவது,  பொதுவாக, ANVITA சிந்தி ஆங்கிலத்தின் திசைகளுக்கான செயல்பாடு செய்யப்பட்டது BLEU மற்றும் RIBES பயன்படுத்தி மதிப்பிடும் 10 மொழி ஜோடிக்கும் விட சிறப்பாக இருக்கும். ஆனால் AM-F பிலூயுக்கு ஒப்பிட்டால், RIBES மற்றும் AM-FM அடிப்படையில் மதிப்பெண்ணை ANVITA அனைத்து பணியில் பங்கீடுகளுக்கும் மிகவு', 'vi': 'Tờ giấy này mô tả hệ thống ANvitA-1 Một bộ Ấn gồm mười ngôn ngữ Ấn Độ. Hệ thống ANvitA-1.0 MTV gồm hai mẫu công ty NMT đa ngôn ngữ một cho hướng Ấn Anh và một cho hướng dẫn tiếng Anh theo Ấn với bộ mã hóa chia sẻ, tiệc tùng chục đôi ngôn ngữ và hai mươi hướng dịch. Cơ chế cơ bản được xây dựng dựa trên kiến trúc transformer và được đào tạo trên đa IndikMT WAT 2021 Hạ sĩ và được cải tạo lại để cung cấp thông tin cho các dữ liệu phân loại lọc, và chung kết mô hình cho một cách tổng hợp hơn. Thêm nữa, tổ chức phun nước Bước 2021 bằng một loạt các thao tác lọc trước khi lên để tập luyện. ANmuô ta-1.0 đạt được điểm AM-FM cao nhất của Bengali, thứ hai cho Tamil Anh và thứ ba cho tiếng Hindi, Bengali English directions on official thử thách set. Thông thường, khả năng mà ANvitA cung cấp hướng Tiếng Anh theo Ấn là tương đối tốt hơn hiệu ứng của hướng Ấn Anh cho tất cả các cặp ngôn ngữ ngữ khi đánh giá bằng tiếng bíp và RIBS, mặc dù xu hướng tương tự không được quan sát liên tục khi đánh giá AM-FM được thực hiện. So với tiếng bíp, RIBS và AM-FM dựa trên đánh giá ANvitA được đặt tương đối tốt hơn trong tất cả các thành viên nhiệm vụ.', 'uz': "Бу саҳифа ANVITA-1. 0 MT tizimini WAT2021 MultiIndicMT (mcairt guruhi) ga qaytarilgan vazifani aniqlash uchun yaratish mumkin. Bu yerda guruhi 20 tarjimalar yordamida qidirilgan: Inglizcha Indik va Inglizcha ingliz tilida qidirilgan. 10 Hindiston tilidagi Indik oʻrnatili. ANVITA-1.0 MT tizimi bir necha tillar NMT modellaridan biri Inglizcha xizmatning yoʻnalishi uchun va boshqa ingliz tilidagi ingliz tili yoʻnalishi uchun bir xizmatda yaratilgan va bitta tarjima yoʻnalishi uchun boshqa ingliz tilidagi kodlash qoidalari uchun bir xil tildan iborat. Name Ko'pchilik, MultiIndicMT WAT 2021 korpora taʼminlovchi uchun oldin bir necha filter amallar yordamida ajratilgan edi. ANVITA-1. 0 Inglizcha Bengali uchun eng Inglizcha Tamil uchun eng Inglizcha xindi uchun engliz tilida eng yuqori AM-FM scori, ingliz tili tilida 2 chi va 3 chi inglizcha Hindi uchun Bengalcha ingliz tili tili rasm sinov tizim sohasida. Umumiy, Xinduy ingliz tili tilidagi ANVITA (ANVITA) bajarilgan bajarishni angliz tilidagi ingliz tilining xizmatlaridan juda yaxshi ko'rinadi BLEU va RIBES yordamida qiymatga ega bo'lganda hamma 10 tildagi xizmatlardan yaxshi ko'rinadi, chunki bir xil AM-FM asosida qiymatni bajarayotganda bir xizmat BLEU bilan qiymatiga qiymatlar va AM-FM asosida ANVITA hamma vazifalar bilan yaxshi bo'lgan.", 'bg': 'Настоящата статия описва системата АНВИТА-1.0 МТ, проектирана за подаване на споделена задача от екипа на Макайрт, където екипът участва в 20 направления за превод: английски индийски и индийски английски; Индийски набор, състоящ се от 10 индийски езика. Система се състои от два многоезични модела едното за индийските посоки и другото за индийските посоки с споделен кодер-декодер, обслужващ 10 езикови двойки и двадесет преводни посоки. Базовите модели са изградени въз основа на архитектурата на трансформаторите и обучени върху корпуси и допълнително използват обратен превод и транслитерация за селективно увеличаване на данните и моделен ансамбъл за по-добро обобщаване. Освен това корпусите бяха дестилирани с помощта на серия от филтриращи операции, преди да се подготвят за обучение. АНВИТА-1.0 постигна най-висок резултат за английски бенгалски, втори за английски тамилски и трети за английски хинди, бенгалски английски указания на официалния тест комплект. Като цяло резултатите, постигнати от ANVITA за индийските посоки на английски език, са относително по-добри от тези на индийските посоки на английски език за всички 10 езикови двойки, когато се оценява с помощта на BLEU и RIBES, въпреки че една и съща тенденция не се наблюдава последователно, когато се извършва оценка на базата на AM-FM. В сравнение с Блеу, оценките на базата на RIBES и AM-FM поставят ANVITA сравнително по-добре сред всички участници в задачата.', 'hr': 'Ovaj papir opisuje sistem ANVITA-1.0 MT-a, arhitektiran za predavanje WAT2021 MultiIndicMT-a zajedničkom zadatku Mcairt tima, gdje je tim sudjelovao u 20 prijevoznih smjernica: engleski indički i indički engleski jezik; Indijski set sastoji od 10 Indijskih jezika. ANVITA-1.0 MT sustav sastoji od dva višejezičkih NMT modela jedan za engleske indijske upute, a drugi za indijske engleske upute sa zajedničkim koderom-dekoderom, hranom 10 jezičkih parova i dvadeset uputa za prevođenje. Osnovni modeli su izgrađeni na temelju arhitekture transformera i obučeni preko korporacije MultiIndicMT WAT 2021. godine i daljnje zaposleni povratni prevod i transliteracija za povećanje selektivnih podataka i model za bolju generalizaciju. Osim toga, korporacija MultiIndicMT WAT 2021. godine bila je destilirana koristeći niz filtriranih operacija prije nego što je postavila za obuku. ANVITA-1.0 je postigao najviši rezultat AM-FM za engleski Bengali, 2. za engleski Tamil i 3. za engleski hindi, Bengalski engleski upute o službenom setu testova. Općenito, učinkoviti postignuti ANVITA za indijske engleske upute su relativno bolji od onih engleskih indijskih uputstva za sve 10 jezičkih parova kada su procjenjivali koristeći BLEU i RIBES, iako se isti trend ne observera konsekventno kada je provedena procjena temeljne na AM-FM-u. U usporedbi s BLEU-om, izviđanje na temelju RIBES i AM-FM postavilo je ANVITA relativno bolje među svim sudionicima zadatka.', 'da': 'Denne artikel beskriver ANVITA-1.0 MT system, der er bygget til indsendelse til WAT201MultiIndicaMT delt opgave af mcairt team, hvor teamet deltog i 20 oversættelsesvejledninger: Engelsk Indisk og Indisk Engelsk; Indisk sæt bestående af 10 indiske sprog. ANVITA-1.0 MT system består af to flersprogede NMT modeller en til de engelske indiske retninger og en anden til de indiske engelske retninger med delt encoder-dekoder, catering 10 sprogpar og tyve oversættelsesretninger. Basismodellerne blev bygget baseret på Transformer arkitektur og uddannet over MultiIndicaMT WAT 2021 corpora og yderligere anvendt tilbage oversættelse og transliteration til selektiv dataforøgelse og model ensemble til bedre generalisering. Desuden blev MultiIndicaMT WAT 2021 corpora destilleret ved hjælp af en række filtreringsoperationer, før de satte op til træning. ANVITA-1.0 opnåede højeste AM-FM score for engelsk bengali, 2. for engelsk tamil og 3. for engelsk hindi, bengali engelsk retning på officielle testsæt. Generelt er resultaterne opnået af ANVITA for indisk engelsk retning relativt bedre end resultaterne for engelsk indisk retning for alle de 10 sprogpar, når de vurderes ved brug af BLEU og RIBES, selv om den samme tendens ikke observeres konsekvent, når AM-FM baseret evaluering blev udført. Sammenlignet med BLEU placerede RIBES og AM-FM baseret scoring ANVITA relativt bedre blandt alle opgavedarbejdere.', 'nl': 'Dit artikel beschrijft ANVITA-1.0 MT systeem, ontworpen voor indiening aan WAT2028 MultiIndicMT gedeelde taak door het mcairt team, waar het team deelnam aan 20 vertaalrichtingen: Engels Indic en Indic Engels; Indische set bestaande uit tien Indiase talen. ANVITA-1.0 MT systeem bestaande uit twee meertalige NMT modellen één voor de Engels Indische richtingen en andere voor de Indisch Engels richtingen met gedeelde encoder-decoder, catering tien taalparen en twintig vertaalrichtingen. De basismodellen werden gebouwd op basis van Transformer architectuur en getraind over MultiIndicMT WAT 2021 corpora en verder gebruikt back translation en transliteratie voor selectieve data augmentatie, en model ensemble voor betere generalisatie. Bovendien werd de MultiIndicMT WAT 2021 corpora gedistilleerd met behulp van een reeks filterbewerkingen voordat ze voor training gingen. ANVITA-1.0 behaalde de hoogste AM-FM score voor Engels Bengaals, 2e voor Engels Tamil en 3e voor Engels Hindi, Bengaals Engels aanwijzingen op officiële testset. Over het algemeen zijn de prestaties van ANVITA voor de Indisch Engels richtingen relatief beter dan die van Engels Indic richtingen voor alle 10 taalparen wanneer geëvalueerd met BLEU en RIBES, hoewel dezelfde trend niet consistent wordt waargenomen wanneer AM-FM gebaseerde evaluatie werd uitgevoerd. In vergelijking met BLEU plaatsten RIBES en AM-FM scores ANVITA relatief beter onder alle deelnemers aan de taak.', 'de': 'Dieses Papier beschreibt das ANVITA-1.0 MT-System, das für die Einreichung an WAT2028 MultiIndicMT vom mcairt-Team entwickelt wurde, wobei das Team an 20-Übersetzungsrichtungen teilnahm: Englisch Indic und Indic English; Indischer Satz bestehend aus zehn indischen Sprachen. ANVITA-1.0 MT-System bestehend aus zwei mehrsprachigen NMT-Modellen eines für die englisch-indischen Richtungen und eines für die indisch-englischen Richtungen mit gemeinsamem Encoder-Decoder, das zehn Sprachpaare und zwanzig Übersetzungsrichtungen abdeckt. Die Basismodelle wurden auf Basis der Transformer-Architektur erstellt und über MultiIndicMT WAT 2021-Korpora trainiert. Weiterhin wurden Backtranslation und Transliteration zur selektiven Datenaufwandlung und Modellensemble zur besseren Generalisierung eingesetzt. Zusätzlich wurden MultiIndicMT WAT 2021 Korpora mit einer Reihe von Filteroperationen destilliert, bevor sie für das Training vorbereitet wurden. ANVITA-1.0 erzielte die höchste AM-FM-Punktzahl für Englisch Bengali, Zweite für Englisch Tamil und Dritte für Englisch Hindi, Bengali Englisch Richtungen auf offiziellem Testset. Im Allgemeinen ist die Leistung von ANVITA für die Indic English Richtungen relativ besser als die von English Indic Richtungen für alle zehn Sprachpaare, wenn sie mit BLEU und RIBES ausgewertet werden, obwohl der gleiche Trend nicht konsistent beobachtet wird, wenn AM-FM basierte Evaluation durchgeführt wurde. Im Vergleich zu BLEU platzierte das RIBES- und AM-FM-basierte Scoring ANVITA unter allen Aufgabenteilnehmern relativ besser.', 'ko': '본고는 ANVITA-1.0 기계 번역 시스템을 묘사했다. 이 시스템은 mcairt팀에 제출된WAT2021 다중 인덱스 공유 임무를 위해 설계된 것이다. 이 팀은 20개의 번역 방향인 영어 인덱스와 인도 영어에 참여했다.인도어집은 10가지 인도어로 구성되어 있다.ANVITA-1.0 기계번역시스템은 두 개의 다국어 NMT 모델로 구성되어 있는데 하나는 영어 인도어 방향, 다른 하나는 인도어 영어 방향, 공유된 인코더-디코더가 있어 10개 언어 쌍과 20개 번역 방향을 지원한다.기초 모델은 Transformer 체계 구조를 바탕으로 구축하고 다중 지표인 WAT 2021 어료 라이브러리에서 훈련을 실시하며 번역과 음역을 이용하여 선택적인 데이터를 확충하고 모델 통합을 이용하여 더욱 좋은 범위를 확대한다.또한 교육에 앞서 일련의 필터링을 사용하여 MultiIndicatMT WAT 2021 자료 라이브러리를 추출했습니다.ANVITA-1.0은 공식 시험집에서 영어 방글라데시어의 AM-FM이 가장 높은 점수를 받았고, 영어 타밀어가 2위, 영어 인디언·방글라데시어·영어 방향이 3위를 차지했다.전반적으로 BLEU와 RIBES를 사용하여 평가를 할 때 ANVITA는 모든 10개 언어가 맞는 인도어의 영어 방향에 대한 표현이 영어 인도어 방향보다 상대적으로 우수했다. 비록 AM-FM을 바탕으로 한 평가에서 같은 추세를 일치하게 관찰하지 못했지만.RIBES와 AM-FM 기반 평점은 BLEU에 비해 모든 임무 참가자 중 안비타가 상대적으로 좋은 점수를 받았다.', 'id': 'Kertas ini menggambarkan sistem ANVITA-1.0 MT, arsitek untuk pengiriman ke WAT2021 MultiIndicMT tugas berbagi oleh tim mcairt, dimana tim berpartisipasi dalam 20 arah terjemahan: Inggris Indic dan Inggris Indic; dan Set India terdiri dari 10 bahasa India. Sistem ANVITA-1.0 MT terdiri dari dua model NMT berbagai bahasa satu untuk arah Inggris Indic dan lain untuk arah Inggris Indic dengan kode-dekoder berbagi, catering 10 pasangan bahasa dan 20 arah terjemahan. The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization.  Additionally, MultiIndicMT WAT 2021 corpora was distilled using a series of filtering operations before putting up for training.  ANVITA-1.0 mencapai skor AM-FM tertinggi untuk Bengali Inggris, kedua untuk Tamil Inggris dan ketiga untuk Hindi Inggris, arah Inggris Bengali pada set tes resmi. Secara umum, pertunjukan yang dilakukan oleh ANVITA untuk arah Inggris India relatif lebih baik dari arah Inggris India untuk semua 10 pasangan bahasa ketika diavaluasi menggunakan BLEU dan RIBES, meskipun trend yang sama tidak diawasi secara konsisten ketika evaluasi berdasarkan AM-FM dilakukan. Berbanding dengan BLEU, nilai berdasarkan RIBES dan AM-FM menempatkan ANVITA relatif lebih baik di antara semua peserta tugas.', 'fa': 'این کاغذ سیستم ANVITA-1.0 MT را توصیف می\u200cکند که برای تحویل به WAT2021 MultiIndicMT از تیم mcairt مشترک شده است، جایی که تیم در ۲۰ مسیر ترجمه شرکت کرد: انگلیسی هندی و انگلیسی هندی است. مجموعه هندی از ۱۰ زبان هندی است. سیستم ANVITA-1.0 MT از دو مدل NMT متعدد زبان یکی برای مسیرهای هند انگلیسی و دیگری برای مسیرهای انگلیسی هند با رمز رمز\u200cدهنده\u200cهای مشترک، ۱۰ جفت زبان و ۲۰ مسیر ترجمه شده است. مدلهای بنیادی بر اساس معماری تغییر\u200cپذیر ساخته شده\u200cاند و بر اساس شرکت MultiIndicMT WAT 2021 آموزش داده شده\u200cاند و بر اساس ترجمه\u200cپذیر و ترجمه\u200cپذیر برای افزایش داده\u200cهای برگزیده و مدل برای عمومی بهتر استخدام شده\u200cاند. در اضافه، شرکت MultiIndicMT WAT 2021 از مجموعه عملیات فیلترینگ قبل از آموزش آموزش جدا شد. ANVITA-1.0 به بالاترین امتیاز AM-FM برای بنگالی انگلیسی رسید، دوم برای تامیل انگلیسی و سوم برای هندی انگلیسی، بنگالی به سمت امتحان رسمی. عمومی، عملکرد که توسط ANVITA برای مسیرهای انگلیسی هند رسیده می شود نسبتا بهتر از مسیرهای هند انگلیسی برای تمام جفت های زبان ۱۰ زمانی که با استفاده از BLEU و RIBES ارزیابی می شود، با وجود اینکه زمانی که ارزیابی بر اساس AM-FM انجام می شود، همان ترند به طور کامل مشاهده نمی شود. در مقایسه با BLEU، آزمایش بنیاد RIBES و AM-FM، ANVITA را نسبتا بهتر از تمام مشتریان کار قرار داد.', 'sw': 'Gazeti hili linaelezea mfumo wa ANVITA-1.0 MT, ulioandaliwa kwa ajili ya kutoa ujumbe wa WAT2021 MultiIndicMT uliofanywa na timu ya mcairt, ambapo timu ilishiriki katika maelekezo 20 ya tafsiri: Kiingereza Kihindi na Kihindi; Mpango wa Kihindi umejumuisha lugha 10 za Kihindi. Mfumo wa ANVITA-1.0 wa MT umejumuisha mifano miwili ya lugha mbalimbali ya NMT moja kwa ajili ya maelekezo ya Kiingereza na nyingine kwa maelekezo ya Kiingereza yenye kodi ya lugha 10 na njia ishirini za kutafsiri. Mradi wa msingi ulijengwa kwa msingi wa ujenzi wa Transfer na kufundishwa zaidi ya makampuni ya MultiIndicMT WAT 2021 na pia walitumia tafsiri na kutafsiri kwa ajili ya kuongeza taarifa chaguo, na mifano kwa ajili ya uzalishaji bora. Kwa nyongeza, kampuni ya MultiIndicMT WAT 2021 ilitenganishwa kwa kutumia mfululizo wa shughuli za uchujaji kabla ya kuandaa mafunzo. ANVITA-1.0 achieved highest AM-FM score for English Bengali, 2nd for English Tamil and 3rd for English Hindi, Bengali English directions on official test set.  Kwa ujumla, utendaji uliotekelezwa na ANVITA kwa maelekezo ya Kiingereza kwa kiasi kikubwa ni bora kuliko kile cha maelekezo ya Kiingereza kwa ajili ya wanandoa wote wa lugha 10 ambapo kutathmini kutumia BLEU na RIBES, ingawa mwenendo huo hauonekani moja kwa moja wakati utafiti wa AM-FM uliofanywa. Kama ilivyolinganishwa na BLEU, RIBES na AM-FM iliweka ANVITA kuwa bora zaidi miongoni mwa washiriki wote wa kazi hiyo.', 'am': 'ይህ ፕሮግራም ANVITA-1.0 MT ስርዓት WAT2021 MultiIndicMT የተካፈለ ስራ በሜክራር ቴም የተካፈለ ነው፡፡ ይህ ጉዳዩ የቡድን 20 ትርጉም መንገድ ተጋርቷል፤ እንግሊዘኛ ኢንጂንግድ እና የኢንጂንግሊክ እንግሊዘኛ ነው፡፡ የህንድ ቋንቋዎች 10 የህንድ ቋንቋዎች ተጨማሪ ANVITA-1.0 MT ስርዓት 10 ቋንቋ ዓይነቶች እና ሀያ ትርጉም መንገዶች ለኢንግሊዘኛ ቋንቋዎች እና አንዱን ለመንግሊዝኛ ጉዳይ እና ሌሎችን ለኢንጂንግሊዘኛ መንገዶች በተለየ የኢንጂንግሊዘኛ ቀዳዳ-decoder የተለየ ነው፡፡ በMultiIndicMT WAT 2021 ኮፕሮፓ ላይ የተመሠረቱና በተለየ የዳታ አካባቢ እና በተለየ ትርጓሜ እና ትርጓሜ በተለየ መረጃ ማውጣት እና ሞዴል ለመሻል ማውጣት የተገኘ ነው፡፡ በተጨማሪም፣ MultiIndicMT WAT 2021 ኮርፖርት ለትምህርት ከመስጠት በፊት ብዙ ምርጫዎች የተለየ ነበር፡፡ ANVITA-1.0 የእንግሊዝኛ ቡንጋሊ፣ ሁለተኛ ለመንግሊዝኛ ታሚሊ እና ሦስተኛ ለመንግሊዝኛ ኪንዲ፣ በንጋልኛ እንግሊዘኛ ፈተና ላይ የመግለጫውን መንገድ አግኝቷል፡፡ በጠቅላላ፣ ANVITA ለኢንዲስ እንግሊዘኛ መንገዶች የሚደረገው የኢንጂል ጉዳይ ይሻላል፡፡ BLEU እና RIBES በተወሰነ ጊዜ በ10 ቋንቋ ሁለቶች ላይ ከኢንጂል ጉዳይ ይሻላል፡፡ As compared to BLEU, RIBES and AM-FM based scoring placed ANVITA relatively better among all the task participants.', 'sq': 'Ky dokument përshkruan sistemin ANVITA-1.0 MT, arkitektuar për paraqitjen në WAT2021 MultiIndicMT të detyrës së përbashkët nga ekipi mcairt, ku ekipi mori pjesë në 20 drejtime përkthimi: anglisht indik dhe anglisht indik; Set indik përbëhet nga 10 gjuhë indiane. Sistemi ANVITA-1.0 MT përbëhet nga dy modele NMT me shumëgjuhë njëra për drejtimet angleze indike dhe tjetra për drejtimet indike angleze me kodifikues të përbashkët, duke ushqyer 10 çifte gjuhësh dhe 20 drejtime përkthimi. Modelet bazë u ndërtuan bazuar në arkitekturën e Transformer dhe u trajnuan mbi korprën MultiIndicMT WAT 2021 dhe përdorën më tej përkthimin dhe transliteracionin për rritjen e të dhënave selektive dhe ansamblin model për gjeneralizimin më të mirë. Additionally, MultiIndicMT WAT 2021 corpora was distilled using a series of filtering operations before putting up for training.  ANVITA-1.0 arriti rezultatin më të lartë të AM-FM për anglisht Bengali, të dytin për anglisht Tamil dhe të tretin për anglisht Hindi, bengali Anglisht në set zyrtar testimi. Në përgjithësi, shfaqja e arritur nga ANVITA për drejtimet indike të Anglisë është relativisht më e mirë se ajo e drejtimeve indike të Anglisë për të gjitha dhjetë palët gjuhësh kur vlerësohen duke përdorur BLEU dhe RIBES, megjithëse e njëjta prirje nuk është vëzhguar vazhdimisht kur është kryer vlerësimi bazuar në AM-FM. Në krahasim me BLEU, pikëpamjet RIBES dhe AM-FM vendosën ANVITA relativisht më mirë midis të gjithë pjesëmarrësve të detyrës.', 'tr': 'Bu kagyz ANVITA-1.0 MT sistemini tashylaýar, WAT2021 MultiIndicMT goşulmaga arhitekte edildi, bu ýerde topar 20 terjime görniş yönünde goşulýar: Iňlisçe Indik we Indik iňlis dilinde goşulýar; 10 Hindistança dilinden bar. ANVITA Bu tabak nusgalar Transformer arhitekturyna daýanýar we MultiIndicMT WAT 2021 corpora tarapynda okuwýardy we yzyna yzyna terjime we terjime edilen hat baglanmasy üçin, we gowy döredilişim üçin örän nusgasy bar. Ayrıca MultiIndicMT WAT 2021 corpora okuw taýýarlamak üçin öňünden birnäçe filtreleme operasiýasyny ulanyp çykardy. ANVITA-1 Adatça, ANVITA indik diller üçin ýeterli hereket edilen iňlisçe görnüşler BLEU we RIBES bilen deňlenýän 10 dil çift üçin iňlisçe görnüşleri has gowydyr, emma AM-FM tarapynda deňlenýän çykyşlygynda bir terjime edilmedi. BLEU bilen görä, RIBES we AM-FM tabanly araçylyk ANVITA ähli işgärler arasynda has gowy görkezildi.', 'af': 'Hierdie papier beskrywe ANVITA-1.0 MT stelsel, argitekteer vir voorskrywing na WAT2021 MultiIndicMT gedeelde taak deur mcairt team, waar die team gedeel het in 20 vertalingsdireksies: Engels Indiese en Indiese Engels; Indiese stel gebruik van 10 Indiese tale. ANVITA-1.0 MT stelsel bestuur van twee multi-tale NMT modele een vir die Engelse Indiese rigtings en ander vir die Indiese Engelse rigtings met gedeelde koder-dekoder, katering 10 taal paar en twintig vertaling rigtings. Die basis modele is gebou gebaseer op Transformer-arkitektuur en onderrig oor MultiIndicMT WAT 2021 korpora en verdere werklike vertaling en transliterasie vir selektiewe data augmentasie en model vir beter generalisering. Additionally, MultiIndicMT WAT 2021 corpora was distillered using a series of filtering operations before putting up for training. ANVITA- 1. 0 het die hoogste AM- FM- telling vir Engels Bengali, 2 vir Engels Tamil en 3 vir Engels Hindi, Bengali Engels rigtings op offisiele toets stel bereik. Algemeen, die prestasie wat deur ANVITA vir die Indiese Engelske rigtings bereik is, is relativief beter as wat van Engelske Indiese rigtings vir al die 10 taal pare wanneer geevalueer word deur BLEU en RIBES, alhoewel die selfde trend is nie konsekvensief aanhoud wanneer AM-FM gebaseerde evaluasie uitgevoer is nie. Soos vergelyk met BLEU, RIBES en AM-FM gebaseerde skaring het ANVITA relativief beter onder al die taak deelnaders gestel.', 'az': "Bu kağıt ANVITA-1.0 MT sistemini təsbiq edir, WAT2021 MultiIndicMT vəzifəsinə daxil edilmiş, Macairt ekibi tarafından paylaşdığı məktəbə ilə birlikdə 20 tercümə yönəltməsi: İngilizce İngilizce və İngilizce İngilizcesinə daxil olmuşdur; Hindistanın 10 dilindən olub. ANVITA Üssü modellər Transformer arhitektüsünə dayanan və MultiIndicMT WAT 2021 korporasına təhsil edilmiş və seçimli məlumatları artırmaq üçün geri çevirilmiş və transliterasyon üçün daha yaxşı generalizasyon üçün təhsil edilmişdir. Üstəlik, MultiIndicMT WAT 2021 corpora təhsil etmədən əvvəl filtrləmə işləri istifadə etməkdən əvvəl ayrıldı. ANVITA Genel olaraq, ANVITA'nın indik İngilizə yönəltmələrinə gələn performansı, BLEU və RIBES vasitəsilə müəyyən olunduğu 10 dil çift üçün İngilizə indik yönəltmələrindən daha yaxşıdır, amma AM-FM tabanlı değerlendirmələri yerinə yetirdikdə eyni trend müəyyən edilməz. BLEU ilə, RIBES və AM-FM tabanlı scoring kimi ANVITA bütün işlər iştirakçıların arasında daha yaxşısını qoydu.", 'bn': 'এই পত্রিকাটি ANVITA-1. 0 MT সিস্টেমের ব্যাখ্যা করেছে যা ওয়াটি২০১২ মাল্টিক ইন্ডিক্টিএমটিকে প্রদান করার জন্য স্থাপন করা হয়েছে, যেখানে দল ম্যাকার্ট ট টি ১০ ভারতীয় ভাষায় ভাষার ভাষায় ভারতীয় সেট। ANVITA-1.0 MT system comprised of two multi-lingual NMT models one for the English Indic directions and other for the Indic English directions with shared encoder-decoder, catering 10 language pairs and twenty translation directions.  ট্রান্সফ্রান্সফার্ন আর্কাইকেটেক্টের ভিত্তিতে তৈরি করা হয়েছে এবং মাল্টিকিন্দিক এমটি ২০২১ কোর্পোরায় প্রশিক্ষণ প্রদান করা হয়েছে এবং বেছে নেওয়া তথ্য এছাড়াও, প্রশিক্ষণের জন্য প্রশিক্ষণের পূর্বে মাল্টিভিন্ডিক্ট ওয়াট ২০২১ কোর্পোরা বিচ্ছিন্ন করা হয়েছে। এনভিটা- ১. ০ ইংরেজি বাংলাদেশের জন্য ইংরেজী তামিল এবং ইংরেজি হিন্দি জন্য তৃতীয় পরীক্ষার জন্য বাংলা ইংরেজি পরীক্ষা নির্দেশের নির্দেশ সাধারণত, ইন্ডিক ইংরেজি নির্দেশের জন্য ANVITA প্রদর্শনের প্রদর্শনীতে অনেক ভালো কিছু ইংরেজী নির্দেশের চেয়ে ভালো কিছু ভালো কিছু নেই যখন বিলু এবং রিবিএস ব্যবহার করা হয়, তখন বিশেষ করে বি বিলুর তুলনায় রিবিস এবং এম-এমএফএম ভিত্তিক স্কোরের তুলনায় সকল কাজের অংশগ্রহণকারীদের মধ্যে ANVITA আকর্ষণীয় ভালো করে দিয়ে', 'cs': 'Tento článek popisuje systém ANVITA-1.0 MT, navržený pro předložení do WAT2025 MultiIndicMT sdíleného úkolu týmu mcairt, kde se tým podílel na dvaceti směrech překladu: anglická indická a indická angličtina; Indická sada složená z deseti indických jazyků. ANVITA-1.0 MT systém sestávající ze dvou vícejazyčných NMT modelů jeden pro anglicky indické směry a druhý pro indicky anglické směry se sdíleným kodérem-dekodérem, obsahující deset jazykových párů a dvacet překladových směrů. Základní modely byly vytvořeny na základě architektury Transformeru a trénovány na korpusech MultiIndicMT WAT 2021 a dále použity zpětný překlad a transliteraci pro selektivní rozšíření dat a modelový soubor pro lepší zobecnění. Kromě toho byly korpusy MultiIndicMT WAT 2021 destilovány pomocí série filtračních operací před připravením na trénink. ANVITA-1.0 získala nejvyšší AM-FM skóre pro anglické bengálštiny, druhé pro anglické tamilštiny a třetí pro anglické hindštiny, bengálské angličtiny na oficiálním testovacím setu. Obecně platí, že výkon dosažený ANVITA pro směry indické angličtiny je relativně lepší než výkon indických směrů angličtiny pro všechny deseti jazykové páry při hodnocení pomocí BLEU a RIBES, ačkoli stejný trend není pozorován konzistentně při hodnocení založeném na AM-FM. Oproti BLEU bodování založené na RIBES a AM-FM umístilo ANVITA relativně lepší mezi všemi účastníky úkolu.', 'bs': 'Ovaj papir opisuje sistem ANVITA-1.0 MT-a, arhitektiran za predavanje WAT2021 MultiIndicMT zajedničkog zadatka Mcairt tima, gdje je tim sudjelovao u 20 smjernica prevoda: engleski indički i indički engleski jezik; Indijski set sastoji od 10 Indijskih jezika. ANVITA-1.0 MT sustav sastoji od dva multijezičkih NMT modela jedan za engleske indijske upute i druge za indijske engleske upute sa zajedničkim koderom-dekoderom, jelom 10 jezičkih parova i dvadeset uputa za prevođenje. Bazni modeli su izgrađeni na temelju arhitekture transformera i obučeni preko korporacije MultiIndicMT WAT 2021 i daljnje zaposleni povratni prevod i transliteracija za povećanje selektivnih podataka i model za bolju generalizaciju. Osim toga, korporacija MultiIndicMT WAT 2021 bila je destilirana koristeći niz filtracionih operacija prije nego što je postavila za obuku. ANVITA-1.0 je postigao najviši rezultat AM-FM za engleski Bengali, 2. za engleski Tamil i 3. za engleski hindi, Bengalski engleski upute o službenom setu testova. Općenito, učinkovitost postignut od ANVITA za indijske engleske upute je relativno bolji od onih engleskih indijskih uputstva za svih 10 jezičkih parova kada su procjene koristeći BLEU i RIBES, iako se isti trend ne posmatra konsekventno kada je provedena procjena na temelju AM-FM-a. U usporedbi s BLEU-om, izviđanje na temelju RIBES i AM-FM postavilo je ANVITA relativno bolje među svim sudionicima zadatka.', 'ca': "Aquest article descriu el sistema ANVITA-1,0 MT, arquitectat per subministrar a WAT2021 MultiIndicMT tasca compartida per l'equip mcairt, on l'equip va participar en 20 direccions de traducció: anglès índic i anglès índic; Indic set comprised of 10 Indian languages.  ANVITA-1.0 MT consistia en dos models NMT multilingües, un per les direccions indiques angleses i altre per les direccions indiques angleses, amb codificador compartit, amb 10 parells de llengües i vint direccions de traducció. The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization.  A més, la corpora MultiIndicMT WAT 2021 va ser destilada utilitzant una sèrie d'operacions de filtració abans de preparar-se a l'entrenament. ANVITA-1.0 va aconseguir la puntuació AM-FM més alta en bengalès anglès, el segon en tamil anglès i el tercer en Hindi anglès, en anglès bengalès en un conjunt oficial de proves. En general, el rendiment aconseguit per ANVITA per les direccions indiques en anglès és relativament millor que el de les direccions indiques en anglès per tots els 10 parells de llengües quan s'utilitzen BLEU i RIBES, tot i que la mateixa tendència no es observa de manera constant quan s'ha fet l'evaluació basada en AM-FM. En comparació amb la BLEU, la puntuació basada en RIBES i AM-FM va posar l'ANVITA relativament millor entre tots els participants en la tasca.", 'fi': 'Tässä artikkelissa kuvataan ANVITA-1.0 MT-järjestelmää, joka on suunniteltu toimitettavaksi WAT12021 MultiIndicMT:n yhteiseen tehtävään mcairt-tiimin toimesta, jossa tiimi osallistui 20 käännössuuntaan: englanti indic ja indic englanti; Intian kieli koostuu kymmenestä intialaisesta kielestä. ANVITA-1.0 MT -järjestelmä koostuu kahdesta monikielisestä NMT-mallista, joista yksi on englanninkielinen indiikki-suunta ja toinen indiikki-englanti-suunta, jossa on jaettu kooderi-dekooderi, joka tarjoaa 10 kieliparia ja kaksikymmentä käännössuuntaa. Perusmallit rakennettiin Transformer-arkkitehtuuriin perustuen ja koulutettiin MultiIndicMT WAT 2021 -korpusilla. Lisäksi käytettiin takaisinkääntämistä ja transliterointia selektiiviseen datan lisäämiseen ja malliensembleä parempaan yleistämiseen. Lisäksi MultiIndicMT WAT 2021 -korpuset tislattiin käyttäen useita suodatustoimintoja ennen koulutukseen siirtymistä. ANVITA-1.0 saavutti korkeimmat AM-FM-pisteet englantilaisessa bengalissa, toiseksi englantilaisessa tamilissa ja kolmanneksi englantilaisessa hindissä, bengalilaisessa englannin suunnassa. Yleisesti ottaen ANVITA:n saavuttama suorituskyky indiaalisen englannin suuntien osalta on suhteellisen parempi kuin englannin indikaalien suuntien suorituskyky kaikilla 10 kieliparilla BLEU:n ja RIBESin avulla, vaikka samaa suuntausta ei havaita johdonmukaisesti AM-FM-pohjaisessa arvioinnissa. BLEU:hun verrattuna RIBES- ja AM-FM-pohjainen pisteytys sijoitti ANVITAn suhteellisen parempaan kaikkien tehtäväosallistujien keskuudessa.', 'et': "Käesolevas töös kirjeldatakse ANVITA-1.0 MT süsteemi, mis on arhitekteeritud esitamiseks WAT12021 MultiIndicMT jagatud ülesandele mcairt meeskond, kus meeskond osales 20 tõlkesuunas: inglise india ja india inglise keel; India keel koosneb 10 india keelest. ANVITA-1.0 MT süsteem koosneb kahest mitmekeelsest NMT mudelist, üks inglise india suunas ja teine india inglise suunas jagatud kodeerija-dekooderiga, toitlustades 10 keelepaari ja 20 tõlkesuunat. Põhimudelid ehitati Transformeri arhitektuuril põhinevatel ja koolitati MultiIndicMT WAT 2021 korpustel ning kasutati edaspidi tagasitõlkimist ja transliteratsiooni valikuliseks andmete suurendamiseks ning mudeli ansamblit paremaks üldistamiseks. Lisaks destilleeriti MultiIndicMT WAT 2021 korpused mitmesuguste filtreerimisoperatsioonide abil enne koolitusele minekut. ANVITA-1.0 saavutas ametlikus testikomplektis kõrgeima AM-FM skoori inglise bengali, teise inglise tamili ja kolmanda inglise hindi, bengali inglise suuniste puhul. Üldiselt on ANVITA tulemuslikkus indiaaninglise keele suunas suhteliselt parem kui inglise keele suunas kõigi 10 keelepaari puhul, kui hinnatakse BLEU ja RIBES'i abil, kuigi sama suundumust ei täheldata järjekindlalt AM-FM-põhise hindamise korral. Võrreldes BLEU-ga olid RIBESi ja AM-FM-i tulemused ANVITA suhteliselt paremad kõigi ülesandes osalejate seas.", 'hy': 'This paper describes ANVITA-1.0 MT system, architected for submission to WAT2021 MultiIndicMT shared task by mcairt team, where the team participated in 20 translation directions: English Indic and Indic English;  Հնդկական համակարգը կազմված է 10 հնդկական լեզուներից: Անդիտա-1.0 ՄԹ համակարգը կազմված է երկու բազլեզու ՆՄԹ մոդելներից մեկից անգլերեն ինդիկ ուղղությունների համար, իսկ մյուսից ինդիկ անգլերեն ուղղությունների համար՝ ընդհանուր կոդեր-կոդերով, տասը լեզու զույգ և քսան թարգմանման ուղղություններ The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization.  Additionally, MultiIndicMT WAT 2021 corpora was distilled using a series of filtering operations before putting up for training.  Անվիտա-1.0-ը հասավ ԱՄ-Ֆ-ի ամենաբարձր գնահատականի անգլերենի համար, երկրորդ անգլերենի թամիլի և երրորդ անգլերենի հինդի համար, բենգալացի անգլերենի ուղղությունների պաշտոնական փորձարկում: In general, performance achieved by ANVITA for the Indic English directions are relatively better than that of English Indic directions for all the 10 language pairs when evaluated using BLEU and RIBES, although the same trend is not observed consistently when AM-FM based evaluation was carried out.  Համեմատելով ԲԼԵՎ-ի հետ, Ռիբեսը և ԱՄ-ՖՄ-ի հիմնված գնահատականները ԱՆԻՏԱ-ն համեմատաբար ավելի լավ տեղադրեցին բոլոր գործընկերների մեջ:', 'jv': 'Perintah iki rambarang ANVOT-1.0 MT sistem, akeh nggawe geranggunakake karo WAT 2020 1 Multiindividual MT sing dikarolan nggawe barang macairt group, ndhéwé nggawe barang urip sing gambar tanggal nggo urjamahan: Inggris indek lan indek Inggris Suoro barang karo 10 barang ANVista-1.0 MT sistem sing komputer iki model Multi-Linguke NMT sampeyan kanggo langgambar tarjamahan ingkang basa gambar nggo langgambar tarjamahan ingkang Gruwe model sing wis ditambah basa karo architecture Transformer lan ditambah akeh lansangan karo Multiindividual MT WAT 2020 1 dumadhan ngono nggawe layang-lansangan ngono terjamahan karo alêrungsel kanggo ngerasakno dadi amplius, lan model model model model model model ngentambah kanggo kelangan kelangan langgar. Nyong-ngomong, Multiindividual WAT 2020 1 bahsa batasan kanggo ngilanggar oleh operasi sing dibutuhke batasan kanggo ngilangno nggawe aturan. ANV-1.0 pengguna sing luwih bantuan AM-FM sing kanggo ngilanggar Bengal, 2h kanggo Inggris Tamil lan 3 kanggo Inggris Hyang, Bengal Inggris barang resmi soko test offisisi. OPCIENTAR ngomong, akeh nglanggar tarjamahan kanggo tarjamahan ingles barang kelas luwih dumadhi kanggo tarjamahan ingles barang 10 Dino kiper karo bleus, RIBES karo AM-FM sing bisi dianggap perusahaan ANPIT supoyo barang sampeyan akeh luwih dumadhi kanggo ngubah operasi sing gak nggawe operasi tambah.', 'ha': "Wannan takardan na bayyana ANTA-1.0 MT, aka architected for Submit to WAT2021 mulIndicMT share job by mcairt team, where the team took part in 20 translation direction: English Indic and Indic English; KCharselect unicode block name ANTA-1.0 MT na ƙunsa da misãlai biyu masu multi-lingui na NMT guda wa shiryoyin Ingiriya na Ingiriya da shirin kode-cododer, mai category sau 10-nau'in da kuma shiryori labari. An samar da salon zane a kan tsarin Transformer na tsari kuma an yi wa shirin multi-IndicMT WAT 2021 koropa da aka yi amfani da shi ana ƙaranci fassarar da kuma transliteratori dõmin ƙarfafa data masu iya canza, da samun fanel wa zafi mai kyau. Ina ƙaranci, multi-IndicMT WAT 2021 Corpo aka rarraba shi da wasu shirin filtering operatsu na gabãnin ya daidaita wa mafarin. ANTA-1.0 ya sãmu tallanci AM-FM score wa Ingiriya Bangali, 2nd for English Tamilli and 3 for English Hidi, Bangali English-language on offline set. General, performance da ANTA aka samar da wa shiryoyin Ingiriya na Indic English are significantly mafi alhẽri daga that of English Indic direction for all the 10 languages par when an evaluate the use of BLEU and RiBES, ingawa ba'a wato the same tradition consist when the AM-FM-based evaluation was performed. Ina daidaita da BLEU, RiBES da AM-FM ya sanya ANTA mafi alhẽri daga duk mãsu aiki.", 'he': 'העיתון הזה מתאר את מערכת ANVITA-1.0 MT, ארכיטקטית להעברה למשימה משותפת WAT2021 MultiIndicMT על ידי צוות mcairt, שבו הצוות השתתף ב-20 כיוונים תרגום: אנגלית אינדיקאית ואנגלית אינדיקאית; קבוצה אינדיאנית מורכבת מ-10 שפות אינדיאניות. מערכת ANVITA-1.0 MT מורכבת משני דוגמנים NMT multi-lingual אחת לכיוונים האנגליים אינדיקאים ואחרת לכיוונים האנגליים אינדיקאים עם קודד-קודד משותף, מקייטר 10 זוגות שפות ועשרים כיוונים תרגום. הדוגמנים הבסיסיים נבנו בהתבסס על ארכיטקטורה טרנספורטרית ואימונים על גופורה MultiIndicMT WAT 2021 ועבדו נוסף תרגום ותרגום טרנסליטרציה עבור גידול נתונים סלקטיביים, וסמל דוגמנים עבור הגנרליזציה טובה יותר. בנוסף, גופורה MultiIndicMT WAT 2021 הושטה באמצעות סדרה של פעולות סינון לפני שהכנתה לאימון. ANVITA-1.0 השיג את נקודת AM-FM הגבוהה ביותר לבנגלית בנגאלית, השנייה לטמיל אנגלי ושלישית להינדית אנגלית, הנגלית אנגלית בכיוונים רשמיים. באופן כללי, ההופעה שנשגחה על ידי ANVITA לכיוונים האנגליים האינדיאנים היא יחסית טובה יותר מאשר הכיוונים האנגליים האינדיאנים לכל 10 זוגות שפות כאשר הוערכו באמצעות BLEU ו RIBES, למרות שאותה טנדרציה לא מתבוננת באופן קבוע כאשר הוערכה על בסיס AM-FM. בהשוואה ל BLEU, RIBES ו-AM-FM בוסס נקודת ציונים הוציאו ANVITA יחסית טוב יותר בין כל משתתפים במשימה.', 'sk': 'Ta prispevek opisuje sistem ANVITA-1.0 MT, arhitekturiran za predložitev WAT12021 MultiIndicMT skupne naloge ekipe mcairt, kjer je ekipa sodelovala v 20 smereh prevajanja: angleščina indijščina in indijščina angleščina; Indijski niz je sestavljen iz 10 indijskih jezikov. Sistem ANVITA-1.0 MT je sestavljen iz dveh večjezičnih NMT modelov enega za angleško indijsko smer in drugega za indijsko angleško smer s skupnim kodirnikom-dekoderjem, ki vsebuje 10 jezikovnih parov in dvajset prevajalskih smeri. Osnovni modeli so bili zgrajeni na podlagi arhitekture transformatorjev in usposobljeni za korpuse MultiIndicMT WAT 2021 ter nadalje uporabljeni nazaj prevajanje in transliteracijo za selektivno povečanje podatkov ter model ansambel za boljšo generalizacijo. Poleg tega so korpusi MultiIndicMT WAT 2021 destilirali z vrsto filtriranja, preden so se pripravili na usposabljanje. ANVITA-1.0 je dosegel najvišjo AM-FM rezultat za angleški bengali, 2. za angleški tamil in 3. za angleški hindijski in bengalski angleški smeri na uradnem testnem sklopu. Na splošno je učinkovitost, ki jo doseže ANVITA za indijske angleške smeri, relativno boljša od učinkovitosti angleške indijske smeri za vseh 10 jezikovnih parov pri ocenjevanju z uporabo BLEU in RIBES, čeprav enak trend ni dosledno opažen pri ocenjevanju na osnovi AM-FM. V primerjavi z BLEU je bila ocena RIBES in AM-FM, ki temelji na rezultatih, ANVITA relativno boljša med vsemi udeleženci naloge.', 'bo': 'ཤོག་བུ་འདིས་ANVITA-1.0 MT མ་ལག་གི་སྒྲིག་ཡིག་གཟུགས་རིས་WAT2021 MultiIndicMT རྣམས་ཀྱི་མཉམ་དུ་སྤྱད་པའི་ལས་འགན་ཞིབ་བྱེད་ཀྱི་ཡོད། སྔོན་ཤུགས་ཀྱི་སྒྲིག་སྟངས་དེ་རྒྱ་གར་གྱི་སྐད་ཡིག༡༠་ཞིག་ཡིན། ANVITA-1.0 MT system comprised of two multi-lingual NMT models one for the English Indic directions and other for the Indic English directions with shared encoder-decoder, catering 10 language pairs and twenty translation directions. The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization. མ་ཟད། སྤྱི་ཚོགས་ཀྱི་རྩིས་པ་(MultiIndicMT WAT 2021)དེ་ལས་གླེང་སྒྲུབ་མ་གཏོང་གི་སྔོན་གྱི་ཚགས་མ་ཡིག་གི་འཚོལ ANVITA-1.0 དེ་གིས་ཨིན་རིའི་བྱ་ཚིག་དང་། དབྱིན་ཡིག་གི་ཊ་མིལ་དང་། སྤྱི་ཚོགས་ཀྱི་བརྟག་སྟངས་ལ་མཐོ་ཤིག་ཡི་གེ། སྤྱིར་བཏང་། ANVITA་གི་གནས་སྟངས་ལ་རྒྱས་ཐག་ཐོག་ལས་མཐུན་རྐྱེན་བྱས་ན། ཨིན་ཡིག་གཟུགས་ཀྱི་ཕྱོགས་གཅིག་ལས་མཐུན་ཐག་ཡིན། དེ་རྒྱ་ནག་གི་ནང་ནས་དབྱིན་ཡིག་གཟུགས་ཕྱི་༡༠་ཙམ་རེ BLEU དང་RIBES་དང་AM-FM་དང་མཉམ་དུ་མཉམ་སྦྲགས་བཞིན་པའི་སྐོར་གསུངས་ནི་ANVITA་རྣམས་ལས་གནད་དོན་ཞིག་བྱས་པ་འདོད།'}
