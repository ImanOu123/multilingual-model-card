{'en': 'Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data F risian- D utch Data', 'ar': 'التحديات في شرح وتحليل البيانات المنطوقة والمبدلة بالشفرة والفريزية الهولندية', 'pt': 'Desafios na anotação e análise de dados falados, trocados por código, frísios-holandeses', 'fr': "Difficultés liées à l'annotation et à l'analyse des données parlées, à commutation de code et frison-néerlandais", 'es': 'Desafíos en la anotación y el análisis de datos hablados, con cambio de código y entre frisón y holandés', 'zh': '注与解析口语、代码切换、弗里斯兰语-荷兰语数挑战', 'ja': 'フリジア語-オランダ語データのコードスイッチによるアノテーションと解析の課題', 'hi': 'एनोटेटिंग और पार्सिंग बोली गई, कोड-स्विच्ड, फ्रिसियन-डच डेटा में चुनौतियां', 'ru': 'Проблемы, связанные с аннотацией и разбором речевых, кодовых, фризско-голландских данных', 'ga': 'Dúshláin maidir le Anótáil agus Parsáil Sonraí Labhartha, Cóid-aistrithe, Freaslainnise-Ollainnis', 'el': 'Προκλήσεις σε σχολιασμό και ανάλυση ομιλούμενων, αλλαγή κώδικα, Φριζιανά-ολλανδικά δεδομένα', 'ka': 'Name', 'it': "Sfide nell'annotazione e nell'analisi dei dati parlati, commutazione di codice, Friso-Olandese", 'kk': 'Жазбалау және талдау тізбектері, код ауыстырылған, фрис- голландша деректері', 'hu': 'Kihívások a beszéd jegyzetelésében és értelmezésében, kódkapcsolású, fríz-holland adatok', 'lt': 'Anotacijos ir analizavimo iššūkiai, kodų keitimas, prancūzų ir olandų duomenys', 'ml': 'കോഡ്- മാറ്റി, ഫ്രിസിഷ്യന്\u200d ഡേറ്റാName', 'mt': 'Sfidi fl-Annotazzjoni u l-Analiżi tal-Konflitti, Kodiċi mibdula, Data Franċiża-Olandiża', 'ms': 'Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data', 'no': 'Utfordringar i annotasjon og tolking av spoken, kodbytt, frisk- nederlandsk data', 'mk': 'Предизвики во анатирање и анализирање на зборови, промена на код, фризиско-холандски податоци', 'mn': 'Аннотаци болон талбарлах хэмжээний шаардлага, Код-өөрчлөгдсөн, Фризиан-Датч өгөгдлийн шаардлага', 'sr': 'Izazovi u Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data', 'pl': 'Wyzwania w komentowaniu i analizie mówionych, przełączaniu kodu, fryzyjsko-holenderskich danych', 'si': 'Name', 'so': 'Challenges in Annotation and Parsing, Cod-switched, Frisian-Dutch Data', 'sv': 'Utmaningar i att kommentera och tolka tal, kodväxlad, frisisk-holländska data', 'ro': 'Provocări în adnotarea și analizarea datelor vorbite, schimbarea codului, datele frisone-olandeze', 'ta': 'Name', 'ur': 'Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data میں چالنج', 'uz': 'Name', 'vi': 'Bài thi đấu giải thích và giải thích bằng ngôn ngữ', 'bg': 'Предизвикателства при анотирането и анализирането на говорени, кодово-променени, фризийско-холандски данни', 'nl': 'Uitdagingen in Annoteren en Parsen Gesproken, Code-switched, Fries-Nederlandse data', 'hr': 'Izazovi u Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data', 'da': 'Udfordringer i notering og fortolkning af talte, kodeskiftede, frisisk-hollandske data', 'de': 'Herausforderungen beim Annotieren und Parsen gesprochener, kodierter, friesisch-niederländischer Daten', 'id': 'tantangan dalam Annotasi dan Analisasi Bicara, Kode-switched, Data Frisian-Belanda', 'fa': 'challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data', 'tr': 'Annotating and Parsing Spoken, Kod-switched, Frisian-Dutch Data', 'sw': 'Changamoto katika Mjadala wa Kutangaza na Kuchapisha, Kubadilishwa kwa Code, Taarifa za UFrisian-Dutch', 'af': 'Opdragte in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data', 'sq': 'Sfidat në njoftimin dhe analizimin e të dhënave të folura, të ndryshuara me kod, të dhënave frizi-hollandeze', 'am': 'ምርጫዎች', 'az': 'Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data', 'hy': 'Խնդիրները նշում և վերլուծում խոսքերի, կոդի փոխակերպման, ֆրիսիական-հոլանդական տվյալների մեջ', 'bn': 'বিজ্ঞাপন এবং পার্সিং স্পুকেন, কোড- পরিবর্তন, ফ্রিসিয়ান-ডাচ তথ্যের চ্যালেঞ্জ', 'bs': 'Izazovi u Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data', 'ca': 'Els reptes en anotar i analitzar les dades parlades, canviades de codi, francés-holandeses', 'ko': '구어, 코드 변환, 프리스 네덜란드어 데이터 주석과 해석의 도전', 'cs': 'Výzvy v komentování a analýze mluvených dat, přepínání kódu, frízsko-holandská data', 'fi': 'Haasteet puhuttujen merkintöjen ja analysoinnin, koodinvaihtoisten, friisialaisten ja hollantilaisten tietojen osalta', 'et': 'Väljakutsed räägitud, koodiga vahetatud, friisi-hollandi andmete märgistamisel ja parsimisel', 'he': 'אתגרים בהעטפות ומחקרים מדברים, מחליפים קודים, מידע פריזי-הולנדי', 'sk': 'Izzivi pri označevanju in razčlenjanju govorjenih, preklopljenih s kodami, frizijsko-nizozemskih podatkov', 'ha': 'Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data', 'jv': 'Delokan Nanggang-Ngerawat lan Pansing Pikno, kode-bisa, Dong-Olayan Jejarang', 'bo': 'གསལ་བཤད་དང་ཞིབ་བཤེར་གྱི་ནང་དུ་ཁོང་ལ་དགོས་པ།'}
{'en': 'While high performance have been obtained for high-resource languages, performance on low-resource languages lags behind. In this paper we focus on the  parsing  of the low-resource language Frisian. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup. We propose to train a  parser  specifically tailored towards the target domain, by selecting instances from multiple  treebanks . Specifically, we use Latent Dirichlet Allocation (LDA), with word and character N-grams. We use a deep biaffine parser initialized with mBERT. The best single source treebank (nl_alpino) resulted in an  LAS  of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6  LAS  on the test data. Additional experiments consisted of removing  diacritics  from our Frisian data, creating more similar training data by cropping sentences and running our best  model  using XLM-R. These experiments did not lead to a better performance.', 'ar': 'في حين تم الحصول على أداء عالٍ للغات عالية الموارد ، فإن الأداء في اللغات منخفضة الموارد يتأخر. في هذه الورقة نركز على تحليل اللغة الفريزية منخفضة الموارد. نحن نستخدم عينة من البيانات المنطوقة تلقائيًا بتبديل الشفرة ، والتي تثبت أنها إعداد صعب. نقترح تدريب المحلل اللغوي المصمم خصيصًا نحو المجال المستهدف ، عن طريق اختيار مثيلات من بنوك شجرية متعددة. على وجه التحديد ، نستخدم تخصيص Latent Dirichlet (LDA) ، مع كلمة وحرف N-grams. نحن نستخدم محلل لغوي عميق للبيافيني تمت تهيئته بـ mBERT. نتج عن أفضل بنك شجرة أحادي المصدر (nl_alpino) معدل LAS قدره 54.7 في حين تفوق اختيارنا على البيانات على أفضل بنك شجرة واحد وأدى إلى 55.6 LAS في بيانات الاختبار. اشتملت التجارب الإضافية على إزالة علامات التشكيل من بياناتنا الفريزية ، وإنشاء بيانات تدريب أكثر تشابهًا عن طريق اقتصاص الجمل وتشغيل أفضل نموذج لدينا باستخدام XLM-R. لم تؤد هذه التجارب إلى أداء أفضل.', 'es': 'Si bien se ha obtenido un alto rendimiento para los lenguajes de recursos altos, el rendimiento en idiomas de bajos recursos va a la zaga. En este artículo nos centramos en el análisis del idioma frisón de bajos recursos. Utilizamos una muestra de datos de cambio de código y hablados espontáneamente, lo que demuestra ser una configuración desafiante. Proponemos entrenar un analizador específicamente diseñado para el dominio de destino, mediante la selección de instancias de varios bancos de árboles. Específicamente, utilizamos la asignación de Dirichlet latente (LDA), con N-gramas de palabras y caracteres. Utilizamos un analizador biafín profundo inicializado con mBert. El mejor banco de árboles de una sola fuente (nl_alpino) dio como resultado un LAS de 54,7, mientras que nuestra selección de datos superó al mejor banco de árboles de transferencia y condujo a 55,6 LAS en los datos de prueba. Los experimentos adicionales consistieron en eliminar los signos diacríticos de nuestros datos en frisón, crear datos de entrenamiento más similares recortando oraciones y ejecutando nuestro mejor modelo con XLM-R. Estos experimentos no condujeron a un mejor rendimiento.', 'fr': "Alors que des performances élevées ont été obtenues pour les langues à ressources élevées, les performances dans les langues à faibles ressources sont à la traîne. Dans cet article, nous nous concentrons sur l'analyse de la langue frisonne à faibles ressources. Nous utilisons un échantillon de données vocales spontanément commutées par code, ce qui s'avère être une configuration difficile. Nous proposons de former un analyseur spécialement adapté au domaine cible, en sélectionnant des instances parmi plusieurs banques d'arbres. Plus précisément, nous utilisons l'allocation de Dirichlet latente (LDA), avec des N-grammes de mots et de caractères. Nous utilisons un analyseur biaffine profond initialisé avec MBert. La meilleure banque d'arbres source unique (nl_alpino) a donné un LAS de 54,7 alors que notre sélection de données a surpassé la meilleure banque d'arbres de transfert unique et a conduit à 55,6 LAS sur les données de test. Des expériences supplémentaires ont consisté à supprimer les signes diacritiques de nos données frisonnes, à créer des données d'entraînement plus similaires en recadrant des phrases et en utilisant notre meilleur modèle à l'aide de XLM-R. Ces expériences n'ont pas permis d'améliorer les performances.", 'pt': 'Embora o alto desempenho tenha sido obtido para linguagens de alto recurso, o desempenho em linguagens de baixo recurso fica para trás. Neste artigo, focamos na análise da linguagem de poucos recursos Frisian. Usamos uma amostra de dados comutados por código e falados espontaneamente, o que prova ser uma configuração desafiadora. Propomos treinar um analisador específico para o domínio de destino, selecionando instâncias de vários bancos de árvores. Especificamente, usamos a Alocação de Dirichlet Latente (LDA), com N-grams de palavras e caracteres. Usamos um analisador biaffine profundo inicializado com mBERT. O melhor banco de árvore de fonte única (nl_alpino) resultou em um LAS de 54,7, enquanto nossa seleção de dados superou o melhor banco de árvore de transferência e levou a 55,6 LAS nos dados de teste. Experimentos adicionais consistiram em remover diacríticos de nossos dados frísios, criando dados de treinamento mais semelhantes cortando frases e executando nosso melhor modelo usando XLM-R. Esses experimentos não levaram a um melhor desempenho.', 'ja': '高リソース言語では高いパフォーマンスが得られていますが、低リソース言語ではパフォーマンスが遅れています。本稿では、低資源言語フリジア語の構文解析に焦点を当てる。私たちはコードスイッチされた自発的に話されたデータのサンプルを使用しています。これは難しいセットアップであることが証明されています。複数のツリーバンクからインスタンスを選択して、ターゲットドメインに特化したパーサーをトレーニングすることを提案します。具体的には、単語と文字Nグラムを含むLatent Dirichlet Allocation (LDA)を使用します。mBERTで初期化されたディープビアフィン構文解析器を使用します。最高の単一ソースツリーバンク（ nl_alpino ）は、54.7のLASをもたらしましたが、当社のデータ選択は、単一の最高の転送ツリーバンクを上回り、テストデータの55.6 LASにつながりました。追加の実験では、フリジアのデータからダイアクリティックを削除し、文章をトリミングしてより類似したトレーニングデータを作成し、XLM - Rを使用してベストモデルを実行しました。これらの実験は、より良いパフォーマンスにはつながりませんでした。', 'zh': '虽高资言性高,而低资源言性后矣。 本文中,专注低资源语弗里斯兰语解析。 吾以代码切换、自发之数示例,此一挑战性之设也。 臣等请择树库实以练专解析器。 具体来说,以潜狄利克雷分(LDA),带单词符N-gram。 一用 mBERT 初始化深双affine解析器。 最佳单源树库(nl_alpino)之LAS为54.7,而吾数择优于传输树库,并致55.6 LAS于测试数据。 其他实验删弗里斯兰数变音符,裁句创练,用XLM-R行最佳。 此实验未有善者也。', 'hi': 'जबकि उच्च-संसाधन भाषाओं के लिए उच्च प्रदर्शन प्राप्त किया गया है, कम-संसाधन भाषाओं पर प्रदर्शन पीछे है। इस पेपर में हम कम-संसाधन भाषा फ्रिसियन के पार्सिंग पर ध्यान केंद्रित करते हैं। हम कोड-स्विच्ड, अनायास बोले जाने वाले डेटा के नमूने का उपयोग करते हैं, जो एक चुनौतीपूर्ण सेटअप साबित होता है। हम एक पार्सर को विशेष रूप से लक्ष्य डोमेन की ओर सिलवाया प्रशिक्षित करने का प्रस्ताव करते हैं, कई ट्रीबैंक से उदाहरणों का चयन करके। विशेष रूप से, हम अव्यक्त Dirichlet आवंटन (LDA) का उपयोग करें, शब्द और चरित्र एन-ग्राम के साथ। हम mBERT के साथ शुरू किए गए एक गहरे biaffine पार्सर का उपयोग करते हैं। सर्वश्रेष्ठ एकल स्रोत ट्रीबैंक (nl_alpino) के परिणामस्वरूप 54.7 का एलएएस हुआ, जबकि हमारे डेटा चयन ने एकल सर्वश्रेष्ठ ट्रांसफर ट्रीबैंक को पछाड़ दिया और परीक्षण डेटा पर 55.6 एलएएस का नेतृत्व किया। अतिरिक्त प्रयोगों में हमारे फ्रिसियन डेटा से डायक्रिटिक्स को हटाने, वाक्यों को क्रॉप करके और एक्सएलएम-आर का उपयोग करके हमारे सबसे अच्छे मॉडल को चलाने के द्वारा अधिक समान प्रशिक्षण डेटा बनाना शामिल था। इन प्रयोगों ने बेहतर प्रदर्शन नहीं किया।', 'ru': 'Несмотря на то, что высокая производительность была достигнута для языков с большими ресурсами, производительность для языков с ограниченными ресурсами отстает. В этой статье мы сосредоточимся на разборе малоресурсного фризского языка. Мы используем выборку спонтанно произнесенных данных с переключением кода, которая оказывается сложной установкой. Мы предлагаем обучить парсер, специально адаптированный к целевому домену, путем выбора экземпляров из нескольких древовидных блоков. В частности, мы используем Latent Dirichlet Allocation (LDA) с N-граммами слов и символов. Мы используем глубокий биаффиновый парсер, инициализированный с помощью mBERT. Лучший банк деревьев с одним источником (nl_alpino) показал LAS 54,7, в то время как наш выбор данных превзошел лучший банк деревьев трансфера и привел к 55,6 LAS по тестовым данным. Дополнительные эксперименты состояли из удаления диакритических знаков из наших фризских данных, создания более похожих обучающих данных путем обрезки предложений и запуска нашей лучшей модели с использованием XLM-R. Эти эксперименты не привели к лучшей производительности.', 'ga': 'Cé go bhfuil ardfheidhmíocht bainte amach do theangacha ard-acmhainne, tá feidhmíocht ar theangacha íseal-acmhainne chun deiridh. Sa pháipéar seo dírímid ar pharsáil na Freaslainnise Freaslainnise. Bainimid úsáid as sampla de shonraí cód-aistrithe, a labhraítear go spontáineach, rud a chruthaíonn gur socrú dúshlánach é. Tá sé beartaithe againn parsálaí a oiliúint a bheidh saindeartha don spriocfhearann, trí chásanna a roghnú ó ilchúnna crann. Go sonrach, úsáidimid Leithdháileadh Dirichlet Folaigh (LDA), le focal agus carachtar N-gram. Bainimid úsáid as parsálaí domhain biaifín inisealaithe le mBERT. Bhí SAR de 54.7 mar thoradh ar an gcruach crann aonfhoinse is fearr (nl_alpino) ach d’fheidhmigh ár rogha sonraí níos fearr ná an banc crann aistrithe aonair is fearr agus ba é an toradh a bhí air ná 55.6 LAS ar na sonraí tástála. Is éard a bhí i dturgnaimh bhreise diacritics a bhaint as ár sonraí Freaslainnise, sonraí oiliúna níos cosúla a chruthú trí abairtí a ghearradh agus ár múnla is fearr a rith ag baint úsáide as XLM-R. Ní raibh feidhmíocht níos fearr mar thoradh ar na turgnaimh seo.', 'hu': 'Míg a nagy erőforrásokat igénylő nyelvek nagy teljesítményét érték el, az alacsony erőforrásokat igénylő nyelvek teljesítménye lemarad. Ebben a tanulmányban az alacsony erőforrású fríz nyelv elemzésére összpontosítunk. Kódkapcsolt, spontán beszélt adatokból álló mintát használunk, ami kihívást jelent. Javasoljuk, hogy egy speciálisan a céltartományra szabott elemzőt képezzünk, több fabank példányát választva. Konkrétan a Latent Dirichlet Allocation (LDA) szót és karaktert használjuk N-grammokkal. Egy mély biaffin elemzőt használunk, amelyet mBERT-vel inicializálunk. A legjobb egyforrású treebank (nl_alpino) 54,7 LAS értéket eredményezett, míg az adatválasztásunk meghaladta az egyetlen legjobb transzfer treebank értékét és 55,6 LAS értéket eredményezett a teszt adataiban. További kísérletek a diakritikusok eltávolítása a fríz adatokból, több hasonló edzési adat létrehozása a mondatok kivágásával és a legjobb modellünk XLM-R használatával történt. Ezek a kísérletek nem eredményeztek jobb teljesítményt.', 'el': 'Ενώ έχουν επιτευχθεί υψηλές επιδόσεις για γλώσσες υψηλής περιεκτικότητας, οι επιδόσεις στις γλώσσες χαμηλής περιεκτικότητας παραμένουν πίσω. Σε αυτή την εργασία εστιάζουμε στην ανάλυση της χαμηλής περιεκτικότητας στη φριζιανή γλώσσα. Χρησιμοποιούμε ένα δείγμα κωδικοποιημένων, αυθόρμητα μιλημένων δεδομένων, το οποίο αποδεικνύεται μια δύσκολη ρύθμιση. Προτείνουμε να εκπαιδεύσουμε έναν αναλυτή ειδικά προσαρμοσμένο στον τομέα προορισμού, επιλέγοντας περιπτώσεις από πολλαπλές τράπεζες δέντρων. Συγκεκριμένα, χρησιμοποιούμε τη Λατινική Κατανομή Διρίκλετ (με Ν-γράμματα λέξης και χαρακτήρων). Χρησιμοποιούμε έναν βαθύ αναλυτή μπιαφίνης αρχικοποιημένο με mBERT. Η καλύτερη τράπεζα δέντρων μίας πηγής (οδήγησε σε ένα LAS 54.7 ενώ η επιλογή δεδομένων μας ξεπερνούσε την καλύτερη τράπεζα δέντρων μεταφοράς και οδήγησε σε 55.6 LAS στα δεδομένα δοκιμής. Επιπρόσθετα πειράματα συνίσταντο στην αφαίρεση των διακοπτικών από τα φριζιανά δεδομένα μας, στη δημιουργία περισσότερων παρόμοιων εκπαιδευτικών δεδομένων με περικοπή προτάσεων και την εκτέλεση του καλύτερου μοντέλου μας χρησιμοποιώντας Αυτά τα πειράματα δεν οδήγησαν σε καλύτερη απόδοση.', 'ka': 'მაგრამ უფრო დიდი რესურსის ენათებისთვის მიღებულია, მაგრამ მარტივი რესურსის ენათებისთვის გამოსახულება. ამ დოკუნტში ჩვენ ფრისიანი ქვემოთ რესურსის ენის პანსუზაციაზე დავყენებთ. ჩვენ გამოყენებთ კოდის შეცვლა, სონტანოდ საუბრილო მონაცემების მონაცემების გამოყენება, რომელიც გამოწყენება შესაძლებელი მონაცემები. ჩვენ გვეძლევა განსხვავება პანსერტის განსაკუთრებით განსხვავებული მისაღების დიომინზე, რამდენიმე საბრძანების განსხვავებით. განსაკუთრებით, ჩვენ შევყენებთ Latent Dirichlet Allocation (LDA) სიტყვებით და სიტყვებით N- გრამით. ჩვენ გამოყენებთ ძალიან ბიფინის პანელერი, რომელიც mBERT-ით инициаლიზებულია. ყველაზე საუკეთესო მხოლოდ ერთი მხოლოდ საბეჭდო (nl_alpino) დაიწყება 54.7-ის LAS-ში, თუმცა ჩვენი მონაცემები მონიშნული მონაცემები ერთი საუკეთესო საბეჭდო საბეჭდო და დამატებული ექსპერიმენტები იყო დიაკრიტიკური ჩვენი ფრისიანი მონაცემების წაშლადან, უფრო სხვადასხვა მონაცემების შექმნა და XLM-R გამოყენებით ჩვენი უკეთესი მოდელს. ეს ექსპერიმე', 'lt': 'Nors daug išteklių turinčių kalbų rezultatų pasiekta, mažai išteklių turinčių kalbų rezultatai atsilieka. Šiame dokumente daugiausia dėmesio skiriame mažai išteklių turinčios friziškos kalbos analizei. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup.  Siūlome apmokyti analizatorių, specialiai pritaikytą tikslinei sričiai, pasirinkdami atvejus iš kelių medžių. Konkrečiai mes naudojame Latent Dirichlet Allocation (LDA) su žodžiu ir simboliu N-gramais. Naudojame gilią biffino analizatorių, inicijuotą mBERT. Geriausias vieno šaltinio medžio pagrindas (nl_alpino) lėmė 54,7 LAS, o mūsų duomenų atranka viršijo vieną geriausią perdavimo pagrindą ir 55,6 LAS bandymų duomenimis. Papildomi eksperimentai buvo diakritikų pašalinimas iš mūsų frizijos duomenų, panašių mokymo duomenų sukūrimas paspaudžiant sakinius ir naudojant XLM-R naudojamą geriausią model į. Šie eksperimentai nesukėlė geresnių rezultatų.', 'mk': 'И покрај тоа што се постигнати високи резултати за јазиците со високи ресурси, резултатите на јазиците со ниски ресурси се задржуваат. Во овој весник се фокусираме на анализирањето на фризискиот јазик со ниски ресурси. Користиме примерок на спонтано зборувани податоци кои се покажуваат како предизвикувачки поставување. Предлагаме да обучуваме анализатор специфично приспособен кон доменот на метата, со избор на примери од повеќе дрвја. Specifically, we use Latent Dirichlet Allocation (LDA), with word and character N-grams.  Ние користиме длабок биафински анализатор иницијализиран со mBERT. Најдобрата група на дрвја од еден извор (nl_alpino) резултираше со LAS од 54,7 додека нашиот избор на податоци ја надмина најдобрата група на дрвја од трансфер и доведе до 55,6 LAS на тестовите податоци. Дополнителни експерименти се состојуваа од отстранувањето на дијакритичарите од нашите фризиски податоци, создавање на послични податоци за обука со собирање реченици и управување со нашиот најдобар модел користејќи XLM-R. Овие експерименти не доведоа до подобра резултат', 'kk': 'Ресурстардың жоғары тілдері үшін жоғары істеу керек болғанда, төмен ресурс тілдерінің істеу керек. Бұл қағазда Фризияндың төмен ресурстар тілін талдау үшін көздейміз. Біз код ауыстырылған, автоматты түрде сөйлейтін деректер үлгісін қолданамыз. Бұл қиын баптау деген сияқты. Біз бірнеше орындағы мәліметтерді таңдап, мақсатты доменге өзгертілген талдаушы оқытуды ұсынамыз. Сонымен қатар, біз "Latent Dirichlet Allocation" (LDA) сөз мен N- граммалар таңбалармен қолданамыз. МБЕРТ арқылы инициализацияланған үлкен биафин талдаушысын қолданамыз. Ең жақсы жалпы көздегі требанды (nl_ alpino) 54. 7 тізімінің LAS болды, бірақ деректерді таңдағанда бір ең жақсы транспорттау требанды жасап, сынақ деректерінде 55. 6 LAS болды. Қосымша тәжірибелер біздің Фрис деректерімізден диаткритикаларды алып тастау, сөздерді қиып, XLM-R арқылы ең жақсы моделімізді орындау арқылы ұқсас оқыту деректерін құрып жатқан. Бұл тәжір', 'it': "Mentre le prestazioni elevate sono state ottenute per i linguaggi ad alto contenuto di risorse, le prestazioni sui linguaggi a basso contenuto di risorse rimangono indietro. In questo articolo ci concentriamo sull'analisi della lingua frisone a basso contenuto di risorse. Usiamo un campione di dati a commutazione di codice, parlati spontaneamente, che si rivela un setup impegnativo. Proponiamo di addestrare un parser su misura per il dominio di destinazione, selezionando istanze da più treebank. Nello specifico, utilizziamo Latent Dirichlet Allocation (LDA), con parole e caratteri N-grammi. Usiamo un analizzatore di biaffine profondo inizializzato con mBERT. Il miglior treebank single source (nl_alpino) ha portato a un LAS di 54,7 mentre la nostra selezione di dati ha superato il singolo treebank di trasferimento migliore e ha portato a 55,6 LAS sui dati di test. Ulteriori esperimenti consistevano nel rimuovere i diacritici dai nostri dati frisiani, creare dati di allenamento più simili ritagliando frasi ed eseguendo il nostro modello migliore utilizzando XLM-R. Questi esperimenti non hanno portato a prestazioni migliori.", 'ms': 'Sementara prestasi tinggi telah dicapai untuk bahasa sumber tinggi, prestasi bahasa sumber rendah tertinggal. Dalam kertas ini kita fokus pada penghuraian bahasa Frisian sumber rendah. Kami menggunakan sampel data yang ditukar-kod, yang bercakap secara spontan, yang membuktikan menjadi seting yang mencabar. Kami cadangkan untuk melatih penghurai yang disesuaikan secara khusus ke arah domain sasaran, dengan memilih contoh dari garis pokok berbilang. Secara khusus, kita gunakan Allocation Latent Dirichlet (LDA), dengan perkataan dan aksara N-gram. Kami menggunakan penghurai biaffin yang diawalkan dengan mBERT. Pangkalan pokok sumber tunggal terbaik (nl_alpino) menghasilkan LAS 54.7 semasa pemilihan data kita melebihi pangkalan pokok pemindahan tunggal terbaik dan membawa ke 55.6 LAS pada data ujian. Eksperimen tambahan terdiri daripada membuang diakritik dari data Frisian kami, mencipta data latihan yang lebih serupa dengan menguap kalimat dan menjalankan model terbaik kami menggunakan XLM-R. Eksperimen ini tidak membawa kepada prestasi yang lebih baik.', 'mn': 'Хэдийгээр өндөр боловсролын хэл дээр ажиллагаа гаргасан ч, бага боловсролын хэл дээр ажиллагаа үлдсэн. Энэ цаасан дээр бид Фризийн бага боловсролын хэлний талаар анхаарлаа хандуулдаг. Бид кодыг өөрчлөгдсөн, сэтгэл хөдлөл өгөгдлийн жишээ хэрэглэдэг. Энэ нь хэцүү байдал юм. Бид хэд хэдэн загваруудын жишээг сонгож зорилготой зорилготой зорилготой хэлбэрээр хуваарч сургуульд сургуульд сургаж байна. Ялангуяа бид Latent Dirichlet Allocation (LDA), N-граммын үг болон дүрсийг ашиглаж байна. Бид mBERT-тай эхлэгдсэн гүн гүнзгий биефин хуваагч ашиглаж байна. Хамгийн шилдэг эх үүсвэрийн загвар (nl_alpino) нь 54.7-ын ЛАС болсон юм. Гэхдээ бидний мэдээллийн сонголт нь хамгийн сайн шилдэг шилжүүлэлтийн загварыг дамжуулж, шалгалтын өгөгдлийн талаар 55.6 ЛАС болсон юм. Тэгээд нэмэлт туршилтууд бидний Фризийн өгөгдлийн хувьд илүү төстэй суралцах өгөгдлийг бүтээж, XLM-R-г ашиглан бидний хамгийн сайн загварын загварыг ашиглаж байдаг.', 'ml': 'ഉയര്\u200dന്ന വിഭവങ്ങളുടെ ഭാഷകള്\u200dക്ക് ഉയര്\u200dന്ന പ്രദര്\u200dശനം ലഭിച്ചിരിക്കുമ്പോള്\u200d, കുറഞ്ഞ വിഭവഭാഷകളില്\u200d പ്രകടനം  ഈ പത്രത്തില്\u200d ഞങ്ങള്\u200d കുറഞ്ഞ വിഭവങ്ങളുടെ ഭാഷ ഫ്രിസ്യന്\u200d പാര്\u200dജിങ്ങിനെ ശ്രദ്ധിക്കുന്നു. നമ്മള്\u200d ഒരു കോഡ് മാറ്റിയിട്ടുണ്ട്, സ്വയമായി സംസാരിക്കുന്ന ഡേറ്റാ ഉപയോഗിക്കുന്നു. അത് ഒരു വിലാല്\u200dക്കാലി നമ്മള്\u200d ഒരു പരിശീലിപ്പിക്കാന്\u200d പ്രത്യേകിച്ച് ലക്ഷ്യത്തിലേക്ക് പ്രത്യേകിച്ച് ടോമെയിനിലേക്ക് തിരഞ്ഞെടുക്ക പ്രത്യേകിച്ച്, നമ്മള്\u200d ലാറ്റെന്റ് ഡിറിച്ചില്ലെറ്റ് ഒലോക്ഷന്\u200d (LDA) ഉപയോഗിക്കുന്നു. വാക്കും അക്ഷരങ്ങളും N-  നമ്മള്\u200d ഒരു ആഴമുള്ള ബീഫിന്\u200d പരാജയപ്രകാരം ഉപയോഗിക്കുന്നു. The best single source treebank (nl_alpino) resulted in an LAS of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6 LAS on the test data.  കൂടുതല്\u200d പരീക്ഷണങ്ങള്\u200d നമ്മുടെ ഫ്രിസ്യന്\u200d വിവരങ്ങളില്\u200d നിന്ന് ഡയറിക്രിക്കുന്നവരെ നീക്കം ചെയ്യുന്നതിനായിരുന്നു. വിവരങ്ങള്\u200d വാങ്ങുന്നതിനാല്\u200d കൂ', 'no': 'Mens høg utviklinga er fått for høg ressursspråk, vil utviklinga på låg ressursspråk gå bak. I denne papiret fokuserer vi på tolking av den låg ressursspråket Frisian. Vi bruker eit prøve av kodbytt, spontane snakket data, som viser å vera eit vanskeleg oppsett. Vi foreslår å trena ei tolkar spesifikke tilpassa mot måldområdet ved å velja instansar frå fleire treebankar. Spesielt bruker vi Latent Dirichlet Allocation (LDA) med ord og teikn N- gramar. Vi bruker ein dyp biaffin- tolkar som er starta med mBERT. Den beste enkelte kjeldetrebanen (nl_alpino) resulterte i ein LAS med 54,7 mens datautvalet vårt utførte den enkelte beste overføringsbanken og førte til 55,6 LAS på test data. Ekstra eksperimenter best år av å fjerna diakritikk frå våre Frisiske data, oppretta meir liknande treningsdata ved å beskjera setningar og køyra våre beste modellen med XLM-R. Desse eksperimentene førte ikkje til ein bedre utvikling.', 'mt': 'Filwaqt li nkisbu prestazzjoni għolja għal lingwi b’riżorsi għoljin, il-prestazzjoni fuq lingwi b’riżorsi baxxi għadha lura. F’dan id-dokument niffokaw fuq l-analiżi tal-lingwa friża b’riżorsi baxxi. Aħna nużaw kampjun ta’ dejta mibdula bil-kodiċi, li titkellem b’mod spontanju, li turi li hija struttura ta’ sfida. Aħna nipproponu li nħarrġu analizzatur imfassal speċifikament lejn id-dominju fil-mira, billi nagħżlu każijiet minn għelieqi multipli tas-siġar. Speċifikament, aħna nużaw l-Allokazzjoni Latent Dirichlet (LDA), bil-kelma u l-karattru N-grammi. Aħna nużaw parser tal-biffina fond inizjalizzat b’mBERT. L-aħjar bank tas-siġar tas-sors uniku (nl_alpino) irriżulta f’LAS ta’ 54.7 filwaqt li l-għażla tad-dejta tagħna qabżet l-aħjar bank tas-siġar tat-trasferiment uniku u wasslet għal 55.6 LAS fuq id-dejta tat-test. Esperimenti addizzjonali kienu jikkonsistu fit-tneħħija tad-dijakritiċi mid-dejta Franċiża tagħna, il-ħolqien ta’ dejta ta’ taħriġ aktar simili permezz tas-sentenzi tal-għelejjel u t-tħaddim tal-a ħjar mudell tagħna bl-użu ta’ XLM-R. Dawn l-esperimenti ma wasslux għal prestazzjoni aħjar.', 'sr': 'Iako su dobili visoke funkcije za jezike visokog resursa, nastup na jezicima niskog resursa ostaje iza sebe. U ovom papiru fokusiramo se na analizu jezika niskog resursa Frisiana. Koristimo uzorak zamjene šifre, spontano govorenih podataka, koji dokazuje da je izazovna postavka. Predlažemo da treniramo analizatora posebno prilagođenog prema ciljnom domenu, birajući instance iz višestrukih treebana. Posebno, koristimo Latent Dirichlet Allocation (LDA), sa rijeèima i karakterom N-grama. Koristimo duboki analizator biafina inicijalizovan sa mBERT-om. Najbolji jedinstveni izvor treeban (nl_alpino) rezultirao je LAS od 54,7, dok je naš izbor podataka izneo jedinstveni najbolji treeban i doveo do 55,6 LAS na test podataka. Dodatni eksperimenti su sastavljeni od uklanjanja diakritika iz naših Frisijskih podataka, stvaranja sličnih podataka za obuku usvajajući rečenice i vodeći naš najbolji model koristeći XLM-R. Ovi eksperimenti nisu doveli do boljih izvođenja.', 'si': 'උත්සන්ධ භාෂාව සඳහා උත්සන්ධ භාෂාව ලැබෙනවා නමුත්, අඩුම භාෂා භාෂාව අඩුම භාෂාව පි මේ පත්තරේ අපි ප්\u200dරශ්න භාෂාව අඩු ප්\u200dරශ්න භාෂාව විශ්වාස කරනවා. අපි කෝඩ් ස්විච්ච් කරපු නිර්මාණයක් පාවිච්චි කරනවා, ස්වයංගයෙන් කතා කරපු දත්ත, ඒක ප්\u200dරශ්න විශා අපි සැකසුම් කරනවා විශේෂයෙන් විශේෂයෙන් ලක්ෂණ ඩෝමින් වලට පරීක්ෂණය කරන්න, විශේෂයෙන් විශේෂයෙන් ස විශේෂයෙන්ම, අපි ලෙට්ට් ඩිරිච්ලෙට් අන්තිමාණය (LDA) පාවිච්චි කරනවා, N- ග්\u200dරාම්ස් වචන සහ අක්ෂර අපි ම්බෙර්ට් එක්ක පටන් ගත්ත ගොඩක් බියාෆින් විශේෂකයක් භාවිත කරනවා. හොඳම ප්\u200dරමාණයක් ත්\u200dරීබැන්ක් (nl_alpino) පරීක්ෂණ දත්තේ ලැස් 54.7 වලින් පරීක්ෂණය කරලා තියෙනවා නමුත් අපේ දත්ත තෝරණය පරීක්ෂණය කරන එකම හොඳම අතර පරීක්ෂණය සම්පූර්ණයෙන් අපේ ප්\u200dරිසියාන් දත්තෙන් පිළිගන්න බැරි විදිහට පරීක්ෂණ දත්ත සිද්ධ වුණා, ප්\u200dරශ්ණ දත්ත සිද්ධ ව', 'pl': 'Podczas gdy uzyskano wysoką wydajność języków o wysokim zasobie, wydajność języków o niskim zasobie pozostaje w tyle. W niniejszym artykule skupiamy się na parsowaniu niskich zasobów języka fryzyjskiego. Wykorzystujemy próbkę przełączonych kodem, spontanicznie mówionych danych, co okazuje się trudną konfiguracją. Proponujemy szkolenie parsera specjalnie dopasowanego do domeny docelowej, wybierając instancje z wielu bank drzew. W szczególności używamy Latent Dirichlet Allocation (LDA), z N-gramami słowa i znaków. Używamy głębokiego parsera biafiny zainicjowanego mBERT. Najlepszy pojedynczy źródłowy bank drzew (nl_alpino) zaowocował LAS 54.7, podczas gdy nasz wybór danych przewyższył pojedynczy najlepszy bank drzew transferowych i doprowadził do 55.6 LAS na danych testowych. Dodatkowe eksperymenty polegały na usunięciu diakrytyki z naszych danych fryzyjskich, tworzeniu bardziej podobnych danych treningowych poprzez przycinanie zdań i uruchomieniu najlepszego modelu przy użyciu XLM-R. Eksperymenty te nie doprowadziły do lepszej wydajności.', 'so': 'Inta lagu helo muuqasho dheer oo luuqadaha sare ee luqadaha hoose ee rasmiga ah, waxyaabaha lagu sameeyo waxaa dib looga dhigaa. Warqadan waxaynu ku fiirsanaynaa baarlamaanka luqada Finnishka ee hoose. Waxaynu isticmaalnaa sameynta codsiga, oo si rasmi ah looga hadlo, taasoo caddaynaya inay tahay hab adag. Waxaynu soo jeedaynaa in aan ku tababarinno lambarada si gaar ah loogu talagalay guriga goalka, si aan u dooranno tusaalooyin kala duduwan qoraalka. Si gaar ah, waxaynu ugu isticmaalnaa Latent Dirichlet Allocation (LDA), hadal iyo xaraf N-gram. Waxaynu isticmaalnaa baaritaanka mool dheer oo lagu billaabiyey mBERT. Tan ugu wanaagsanayd treebank (nl_alpino) waxay sababtay LAS oo ka mid ah 54.7, iyadoo doorashadeedii macluumaadkayagu ay sameyn jireenka ugu wanaagsan ee treebank waxaana u keenay 55.6 LAS oo ku saabsan data imtixaanka. Imtixaanka dheeraadka ah waxaa ka mid ah dhaqdhaqaalaha macluumaadkayaga Frisian, waxayna sameyn jirrabaadyo la mid ah oo ka mid ah xafiiska beeraha iyo dhaqdhaqaalaha sameynta modelkeena ugu wanaagsan ee isticmaalaya XLM-R. Imtixaankaas ma uu sababin wax ka wanaagsan.', 'ta': 'அதிக மூலத்தின் மொழிகளுக்கான அதிக செயல்பாடு பெற்றது இந்த காகிதத்தில் நாம் குறைந்த மூலத்தின் பாக்கியத்தை கவனம் செலுத்துகிறோம். We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup.  இலக்கு களத்திற்கு குறிப்பிட்ட ஒரு பொருளை பயிற்சி செய்ய நாம் பரிந்துரைக்கிறோம், பல மூன்று கோடுகளில் இருந்து ந குறிப்பிட்டால், நாம் சொல்லும் எழுத்தும் N- கிராமுடன் அலங்காரத்தை பயன்படுத்துகிறோம். MBERT உடன் துவக்கப்பட்ட ஒரு ஆழமான பியாபின் பகுதியை பயன்படுத்துகிறோம். சிறந்த ஒற்றை மூலம் treebank (nl_ alpino) 54. 7 ல் ஒரு LAS வெளியேற்றினார். ஆனால் எங்கள் தரவு தேர்வு சிறந்த ஒற்றை மாற்று treebank செய்து சோதனை தரவில் 55. 6 LAS ஆக்க கூடுதல் சோதனைகள் எங்கள் ஃப்ரிசியன் தரவிலிருந்து டையாக்ரியன்களை நீக்குதல் இருந்தது, விளைச்சுட்டு வாக்கியங்களை உருவாக்கி எக்ஸ்எல்எம்- R பயன்படுத்', 'ro': 'În timp ce au fost obținute performanțe ridicate pentru limbile cu resurse ridicate, performanța pe limbile cu resurse reduse rămâne în urmă. În această lucrare ne concentrăm pe analizarea limbii frisone cu resurse reduse. Folosim un eșantion de date cu comutare de cod, vorbite spontan, ceea ce se dovedește a fi o configurație dificilă. Vă propunem să instruiți un parser special adaptat domeniului țintă, prin selectarea instanțelor din mai multe brake-uri. Mai exact, folosim Latent Dirichlet Allocation (LDA), cu cuvinte și caractere N-grame. Folosim un parser de biafine profund initializat cu mBERT. Cel mai bun treebank single source (nl_alpino) a rezultat într-un LAS de 54,7 în timp ce selecția noastră de date a depășit cel mai bun transfer treebank unic și a dus la 55,6 LAS pe datele de testare. Experimentele suplimentare au constat în eliminarea diacritică din datele noastre frisone, crearea mai multor date de antrenament similare prin decuparea frazelor și rularea celui mai bun model al nostru folosind XLM-R. Aceste experimente nu au dus la o performanță mai bună.', 'sv': 'Även om hög prestanda har uppnåtts för språk med hög resurs släpar prestandan på språk med låg resurs efter. I denna uppsats fokuserar vi på tolkningen av lågresursspråket frisiska. Vi använder ett urval av kodväxlade, spontant talade data, vilket visar sig vara en utmanande installation. Vi föreslår att träna en parser specifikt anpassad för måldomänen, genom att välja instanser från flera trädbanker. Specifikt använder vi Latent Dirichlet Allocation (LDA), med ord och tecken N-gram. Vi använder en djup biaffin parser initierad med mBERT. Den bästa singel source treebank (nl_alpino) resulterade i en LAS på 54,7 medan vårt dataval överträffade singel best transfer treebank och ledde till 55,6 LAS på testdata. Ytterligare experiment bestod av att ta bort diakritiker från våra frisiska data, skapa mer liknande träningsdata genom att beskära meningar och köra vår bästa modell med XLM-R. Dessa experiment ledde inte till en bättre prestanda.', 'ur': 'اچھی طرح عمدہ کامپیوتر بالا سروسیس زبانوں کے لئے حاصل کیا گیا ہے، کم سروسیس زبانوں کے پیچھے رہ جاتا ہے. اس کاغذ میں ہم کم سرمایہ کی زبان فریزین کے بارے میں تمرکز کرتے ہیں۔ ہم ایک نمونہ کا استعمال کرتے ہیں کوڈ-سوئٹ، اسپانیٹ سے بول دیے گئے ہیں، جو ایک مشکل سٹاپ ہے۔ ہم ایک پارچر کی ترینس کرنا چاہتے ہیں جو مخصوص طریقے سے موقع ڈومین کی طرف پھیلائی جاتی ہے، بہت سی ٹریب بانک سے موقعیتیں انتخاب کرتی ہیں۔ خاص طور پر، ہم Latent Dirichlet Allocation (LDA) کا استعمال کرتے ہیں، کلمات اور شخصت N-grams کے ساتھ۔ ہم نے mBERT کے ساتھ شروع کی ایک عمیق بیفن پارچر استعمال کیا۔ The best source treebank (nl_alpino) resulted in a LAS of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6 LAS on the test data. اور اضافہ آزمائش کی وجہ سے ہمارے فریس ڈیٹا سے دیاکریکٹی کو ہٹانے کے لئے تھا، کلمات کاٹنے کے ذریعہ اور ہمارے بہترین نمڈل کو XLM-R کے ذریعہ دوڑانے کے ذریعہ زیادہ برابر تربین ڈیٹا بناتے تھے. یہ آزمائش اچھی عمل', 'uz': "Chunki juda katta manbalar tillari uchun bajarishga ega bo'lganda, kam manbaning tillarida bajarishni bajaradi. In this paper we focus on the parsing of the low-resource language Frisian.  Biz avtomatik gapiradigan maʼlumotlardan foydalanamiz. Bu muammolar moslamasi mumkin. Biz bir nechta treeborni tanlash uchun qo'shilgan kompyuterni taʼminlashni talab qilamiz. Koʻrsatilgan, biz Yaqinda ochilgan Dirichlet kompyuterdan (LDA), so'z va belgi N- gram bilan foydalanamiz. Biz mBERT bilan boshlangan juda qiyin biaffin parameterdan foydalanamiz. Eng eng eng eng yaxshi manba treebank (nl_ alpino) 54. 7 yordamida boshqa maʼlumot tanlanganmiz bir eng eng eng eng yaxshi koʻpaytirilgan treebank va sinab maʼlumot bilan 55. 6 LAS ga erishildi. Koʻproq tajribalar Frisiy маълумотларимиздан diakritiklarni olib tashlashdir, bir xil taʼminlov maʼlumotni o'rganish va XLM-R yordamida eng yaxshi modelmizni ishga tushirish mumkin. Bu imtiyozlar yaxshi bajarishga sababdi.", 'vi': 'Một số hiệu suất cao dành cho các ngôn ngữ giàu có, nhưng khả năng ngôn ngữ ít tài nguyên vẫn chậm trễ. Trong tờ giấy này chúng ta tập trung vào việc phân tích ngôn ngữ trù phú Frisian. Chúng tôi sử dụng một mẫu dữ liệu đã được mã hóa thay đổi, tự động nói ra, một thiết lập đầy thử thách. Chúng tôi đề nghị huấn luyện một phân tách đặc biệt về miền đích, bằng cách chọn các trường hợp từ đa dạng ba bóng. Cụ thể, chúng tôi dùng Latent Dirichhlet Allocation (LDAP), with word and character N-grams. Chúng tôi dùng phân tách cà phê lát được khởi tạo bằng mBERT. The best single source treeback (nl*u alpino) đã dẫn đến một LAS of 45.7 trong khi chúng tôi đã chọn dữ liệu chỉ ra duy nhất một lần truyền giá và dẫn đến 5005.6 LAS trên dữ liệu thí nghiệm. Các thí nghiệm khác bao gồm việc loại bỏ Diacritics khỏi dữ liệu Frisian, tạo ra dữ liệu đào tạo tương tự bằng việc xén câu và chạy mô hình tốt nhất của chúng ta bằng XLM-R. Những thí nghiệm này không mang lại hiệu quả tốt hơn.', 'hr': 'Iako je napravljena visoka učinkovitost na jezicima visokih resursa, učinkovitost na jezicima niskih resursa ostaje iza nje. U ovom papiru fokusiramo se na analizu jezika niskog resursa Frisianca. Koristimo uzorak zamjene šifre, spontano govorenih podataka, koji dokazuje da je izazovna postavka. Predlažemo trenirati analizatora posebno prilagođenog prema ciljnom domenu, birajući instancije iz višestrukih područja. Posebno, koristimo Latent Dirichlet Allocation (LDA), s riječima i karakterom N-grama. Koristimo duboki analizator biafina inicijaliziran s mBERT-om. Najbolji jedinstveni izvor treeban (nl_alpino) rezultirao je LAS od 54,7, dok je naš izbor podataka iznosio jedinstveni najbolji prijenos treeban i doveo do 55,6 LAS na testne podatke. Dodatni eksperimenti su sastavljeni od uklanjanja dijamanata iz naših Frisijskih podataka, stvaranja sličnih podataka za obuku prijevozom rečenica i vodeći naš najbolji model koristeći XLM-R. Ovi eksperimenti nisu doveli do boljih učinka.', 'da': 'Mens der er opnået høj ydeevne for sprog med høj ressource, er ydeevnen på sprog med lav ressource bagud. I denne artikel fokuserer vi på tolkningen af det lave ressourcesprog frisisk. Vi bruger en prøve af kodekoblingte, spontant talte data, hvilket viser sig at være en udfordrende opsætning. Vi foreslår at træne en fortolker specifikt skræddersyet til måldomænet ved at vælge forekomster fra flere træbanker. Specielt bruger vi Latent Dirichlet Allocation (LDA), med ord og tegn N-gram. Vi bruger en dyb biaffin parser initialiseret med mBERT. Den bedste single source treebank (nl_alpino) resulterede i en LAS på 54,7, mens vores datavalg overgik den enkelte bedste transfer treebank og førte til 55,6 LAS på testdata. Yderligere eksperimenter bestod i at fjerne diakritikere fra vores frisiske data, skabe mere lignende træningsdata ved at beskære sætninger og køre vores bedste model ved hjælp af XLM-R. Disse eksperimenter førte ikke til en bedre præstation.', 'bg': 'Въпреки че е постигната висока производителност за езици с висок ресурс, производителността на езици с нисък ресурс изостава. В тази статия се фокусираме върху анализирането на нискоресурсния език фризийски. Използваме пример от кодово превключени, спонтанно говорени данни, което се оказва предизвикателна настройка. Предлагаме да се обучи анализатор, специално пригоден за целевия домейн, като се избират инстанции от множество дървесни ленти. По-конкретно, ние използваме латентно разпределение на дириклет (ЛДА), с дума и знак Н-грама. Използваме дълбок биафинов анализатор инициализиран с mBERT. Най-добрата единична дървесна банка (Нл_алпино) доведе до LAS от 54.7, докато нашият избор на данни надмина най-добрата единична дървесна банка за трансфер и доведе до 55.6 LAS на тестовите данни. Допълнителните експерименти се състояха в премахване на диакритиката от фризийските ни данни, създаване на повече подобни данни за обучение чрез изрязване на изречения и стартиране на най-добрия ни модел с помощта на Тези експерименти не доведоха до по-добро представяне.', 'nl': 'Hoewel er hoge prestaties zijn behaald voor talen met veel resources, blijven de prestaties op talen met weinig resources achter. In dit artikel richten we ons op het parsen van de low-resource taal Fries. We gebruiken een voorbeeld van code-switched, spontaan gesproken data, wat een uitdagende opstelling blijkt te zijn. We stellen voor om een parser te trainen die specifiek is afgestemd op het doeldomein, door instances uit meerdere boombanken te selecteren. Specifiek gebruiken we Latent Dirichlet Allocation (LDA), met woord en teken N-grammen. We gebruiken een diepe biaffine parser geïnitialiseerd met mBERT. De beste single source boombank (nl_alpino) resulteerde in een LAS van 54.7 terwijl onze dataselectie de beste transferboombank overtrof en leidde tot 55.6 LAS op de testdata. Aanvullende experimenten bestonden uit het verwijderen van diacritici uit onze Friese data, het creëren van meer vergelijkbare trainingsdata door zinnen bij te snijden en het uitvoeren van ons beste model met XLM-R. Deze experimenten leidden niet tot een betere prestatie.', 'de': 'Während hohe Leistung für ressourcenintensive Sprachen erzielt wurde, hinkt die Leistung bei ressourcenarmen Sprachen hinterher. In diesem Beitrag konzentrieren wir uns auf das Parsen der ressourcenarmen Sprache Friesisch. Wir verwenden ein Beispiel von code-geschalteten, spontan gesprochenen Daten, was sich als herausforderndes Setup erweist. Wir schlagen vor, einen Parser zu trainieren, der speziell auf die Zieldomäne zugeschnitten ist, indem Instanzen aus mehreren Baumbänken ausgewählt werden. Insbesondere verwenden wir Latent Dirichlet Allocation (LDA), mit Wort- und Zeichen-N-Gramm. Wir verwenden einen tiefen Biaffinparser, der mit mBERT initialisiert wurde. Die beste Single Source Treebank (nl_alpino) führte zu einem LAS von 54.7, während unsere Datenauswahl die beste Single Transfer Treebank übertraf und zu 55.6 LAS auf den Testdaten führte. Weitere Experimente bestanden darin, Diakritiken aus unseren friesischen Daten zu entfernen, mehr ähnliche Trainingsdaten durch Zuschneiden von Sätzen zu erstellen und unser bestes Modell mit XLM-R auszuführen. Diese Experimente führten nicht zu einer besseren Leistung.', 'id': 'Sementara prestasi tinggi telah diperoleh untuk bahasa sumber daya tinggi, prestasi bahasa sumber daya rendah tertinggal. Dalam kertas ini kita fokus pada penghuraian bahasa Frisian sumber daya rendah. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup.  Kami mengusulkan untuk melatih parser khusus disesuaikan menuju domain sasaran, dengan memilih contoh dari beberapa batang pohon. Secara spesifik, kita menggunakan Allokasi Latent Dirichlet (LDA), dengan kata dan karakter N-gram. Kami menggunakan parser biaffin dalam yang diinisialisasikan dengan mBERT. Pangkalan pohon sumber tunggal terbaik (nl_alpino) menghasilkan LAS 54,7 sementara pemilihan data kami melebihi panggkalan pohon transfer terbaik tunggal dan membawa ke 55,6 LAS pada data tes. Eksperimen tambahan terdiri dari menghapus diakritik dari data Frisian kami, menciptakan data pelatihan yang lebih mirip dengan menambah kalimat dan menjalankan model terbaik kami menggunakan XLM-R. Eksperimen ini tidak menyebabkan prestasi yang lebih baik.', 'fa': 'در حالی که عملکرد بالا برای زبانهای منابع بالا دریافت شده است، عملکرد روی زبانهای منابع کم باقی مانده است. در این کاغذ ما روی تجزیه کردن زبان کم منبع فریزین تمرکز می کنیم. ما از نمونه\u200cای از داده\u200cهای تغییر کد استفاده می\u200cکنیم، که ثابت می\u200cکند یک تنظیم مشکل است. ما پیشنهاد می\u200cکنیم که یک بازیگر ویژه\u200cای را به سمت دامنه هدف آموزش دهیم، با انتخاب نمونه\u200cها از بسته\u200cهای متعدد درخت\u200cها. به طور خاصی، ما با کلمه و شخصیت N-گرم استفاده می\u200cکنیم از تقسیم دیریکلت Latent (LDA). ما از یک تجزیه\u200cکننده\u200cی بیفاین عمیق استفاده می\u200cکنیم که با mBERT شروع شده است. بهترین ترکیب ترکیب منبع تنها (nl_alpino) به نتیجه یک LAS از 54.7 به وجود آورد، در حالی که انتخاب داده\u200cهای ما بهترین ترکیب ترکیب ترکیب تنها را برداشت و به 55.6 LAS در داده\u200cهای آزمایش برداشت. آزمایشات اضافه از حذف دیاکریک\u200cها از داده\u200cهای فریسی ما بودند، و داده\u200cهای آموزش مشابه\u200cتر از جمله\u200cهای جمع کردن جمله\u200cها و اجرای بهترین مدل\u200cهای ما با استفاده از XLM-R. این آزمایشات به انجام بهتر رهبری نکردند.', 'sw': 'Wakati mafanikio makubwa yamekuwa yakipatikana kwa lugha za rasilimali za juu, utendaji wa lugha za rasilimali zilizobaki. Katika karatasi hii tunajikita kwenye kuimba lugha ya KiFrisia yenye rasilimali duni. Tunatumia mifano ya kubadilishwa kwa kodi, taarifa zinazozungumzwa kwa wenyewe, ambayo inaonyesha kuwa ni seti ya changamoto. Tunazipendekeza kuwafundisha mchambuzi hasa unaoongozwa kuelekea eneo la lengo, kwa kuchagua matukio kutoka kwenye viwanja vingi vya mitatu. Kwa ujumla, tunatumia Umoja wa Kusini wa Dirichlet (LDA), kwa maneno na tabia ya N-grams. Tunatumia mchambuzi wa kina wa biaffine ulioanzishwa na mBERT. Chanzo bora zaidi cha treebank (nl_alpino) kilisababisha LAS ya 54.7 wakati uchaguzi wetu wa takwimu ulifanya usafirishaji bora zaidi wa mitebank na ulipelekea LAS 55.6 kwenye takwimu za jaribio. Majaribio mengine yalikuwa ni pamoja na kuondoa wagonjwa kutoka kwenye takwimu zetu za KiFrisia, kutengeneza takwimu za mafunzo kama hizo kwa kutengeneza mifano yetu bora kwa kutumia XLM-R.', 'ko': '고자원 언어는 이미 고성능을 얻었지만 저자원 언어의 성능은 뒤떨어졌다.본고는 주로 저자원 언어인 프리시안의 문법 분석을 연구한다.우리는 코드 전환, 자발적으로 나온 데이터 샘플을 사용했는데 이것은 도전적인 설정임을 증명한다.여러 개의 트리 라이브러리에서 실례를 선택해서 목표 영역에 대한 맞춤형 해상도를 훈련하는 것을 권장합니다.구체적으로 말하면, 우리는 잠재적인 디릭레 분배 (LDA) 와 단어와 문자의 N-gram을 사용한다.우리는 mBERT로 초기화된 심비아핀 해상도를 사용합니다.최적 단원 트리 라이브러리(nl alpino)의 LAS는 54.7이고 우리의 데이터 선택은 단일 최적 이동 트리 라이브러리보다 우수하며 테스트 데이터의 LAS는 55.6이다.다른 실험으로는 프리스식 데이터에서 변음 기호를 삭제하고 문장을 재단하여 비슷한 훈련 데이터를 만들고 XLM-R로 우리의 가장 좋은 모델을 실행하는 것이 포함된다. 이 실험들은 더 좋은 성능을 가져오지 못했다.', 'tr': 'Yüksek ressurs dilleri üçin ýokary ukyp edildi, iň-çeşme dilleriniň yzynda täsirler bar. Bu kagyzda biz Frisiýanyň iň köp ukyp dilini çözmek üçin üns berýäris. Biz köd üýtgedilmiş, spontaz gepleşilen maglumatyň örnekini ulanýarys. Bu kynçylyk düzümlenmesi üçin kanıtlaýar. Biz birden çoklu çubuktan örnekler seçmek üzere özellikle hedef domenya geçirilen bir ayıran öwrenmesini teklif ediyoruz. Adatça, biz Later Dirichlet Allocation (LDA), söz we karakter N-gramler bilen ullanyrys Biz mBERT bilen başlanýan bir çukur biafin çözümlerini ulanýarys. Iň gowy bir çeşme gatlaky (nl_alpino) 54.7 ýagdaýynda LAS sebäpli boldy. Maglumat saýlawymyz ýeke iň gowy gatlaky gatlakyny üstün etdi we testiň maglumatynda 55.6 LAS giddi. Ekstra deneyler Frisiýa maglumatlarymyzdan diakritikalary çykarmak bolupdyr, sözlerimizi kesip we iň gowy nusgasymyzy XLM-R ulanyp daşary çykarmak bilen meňzeşdirdi. Bu deneyler gowy bir hereket etmäge ýok edip bilmedi.', 'sq': 'Ndërsa paraqitja e lartë është arritur për gjuhët me burime të larta, paraqitja në gjuhët me burime të ulta mbetet prapa. Në këtë letër ne përqëndrohemi në analizimin e gjuhës me burime të ulëta friziane. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup.  Ne propozojmë të trajnojmë një analizues specifikisht të përshtatur drejt domenisë objektive, duke zgjedhur raste nga vende të shumta pemësh. Specifically, we use Latent Dirichlet Allocation (LDA), with word and character N-grams.  Ne përdorim një analizues biffine të thellë të inicializuar me mBERT. Banka më e mirë e një burimi (nl_alpino) rezultoi në një LAS 54.7 ndërsa zgjedhja jonë e të dhënave kaloi bankën më të mirë të transferimit dhe çoi në 55.6 LAS në të dhënat e testit. Eksperimente shtesë përbënin heqjen e diakritikëve nga të dhënat tona friziane, krijimin e të dhënave më të ngjashme të trajnimit duke mbledhur fjalë dhe duke drejtuar modelin tonë më të mirë duke përdorur XLM-R. Këto eksperimente nuk shpien në një performancë më të mirë.', 'af': "Terwyl hoë prestasie vir hoë-hulpbronne tale ontvang is, het prestasie op lae-hulpbronne tale agter verlaat. In hierdie papier fokus ons op die verwerking van die lae hulpbron taal Frisian. Ons gebruik 'n voorbeeld van kode-geskuif, spontaneël gepraat data, wat bevestig dat 'n pragtige opstelling is. Ons voorstel om 'n analyseer spesifieke na die doel domein te trein deur voorbeelde te kies van veelvuldige treebanks. Spesifieke, ons gebruik Latent Dirichlet Allocation (LDA), met woord en karakter N- grame. Ons gebruik 'n diep biaffine ontwerker wat geïnisialiseer is met mBERT. Die beste enkele bron treebank (nl_alpino) het resultaat in 'n LAS van 54.7 terwyl ons data keuse uitgevoer het die enkele beste oordrag treebank en gelei na 55.6 LAS op die toets data. Addisionele eksperimente het bestuur van die verwyder van diakrities van ons Frisiese data, skep meer gelyke onderwerp data deur die kruip van teikens en die bestuur van ons beste model gebruik van XLM-R. Hierdie eksperimente het nie na 'n beter uitvoering gelei nie.", 'am': 'ከፍተኛ የክፍለ ሀብት ቋንቋዎች ሲያገኙ፣ የዝናብ ቋንቋዎች ፍላጎት በኋላ ነው፡፡ በዚህ ገጽ የዋናው የፍሪሳውያንን ቋንቋ ማዘጋጀት ላይ እናስማማታለን፡፡ የኮድ ተለወጠን ምሳሌ እናስቀምጣለን፡፡ በተለየ አካባቢ አካላቢ ዶሜን በመምረጥ የተመሳሳይ ምርጫዎችን በመምረጥ እናሳውቃለን፡፡ በተለያይነት፣ Latent Dirichlet Allocation (LDA), በቃል እና በ-graph እናስቀምጣለን፡፡ በ mBERT የተጀመረ ጥልቅ የቢፊን ምርጫዎች እናስቀምጣለን፡፡ The best single source treebank (nl_alpino) resulted in an LAS of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6 LAS on the test data.  ጨዋታ ፈተናዎች ከፍሪሳዊ ዳታዎችን ለማስወግድ፣ እንደዚህ ብጤ የተሰናከረውን አስተማሪ ዳታዎችን በመፍጠር እና በXLM-R በመጠቀም የተሻለ ሞዴላዎችን ለመፈለግ ነው፡፡', 'hy': 'Մինչդեռ բարձր արտադրողականությունը հասել է բարձր ռեսուրսների լեզուների համար, ցածր ռեսուրսների լեզուների արտադրողականությունը հետաքրքիր է: Այս թղթի մեջ մենք կենտրոնանում ենք ցածր ռեսուրսների լեզվի վերլուծության վրա: Մենք օգտագործում ենք կոդի փոխակերպված, ինքնաբուխ խոսվող տվյալների նմուշ, որը պարզվում է դժվար կառուցվածք է: Մենք առաջարկում ենք դասակարգչի ուսուցանումը, որն առանձնահատուկ է նպատակային բնագավառի ուղղությամբ, ընտրելով բազմաթիվ ծառերից ստացված օրինակներ: Մասնավորապես, մենք օգտագործում ենք Վերջին դիրիկլետի (ԼԴԱ) բառերի և N-գրամանների հետ: Մենք օգտագործում ենք mBER-ի միջոցով հիմնված խորը բիաֆինի վերլուծողը: Ամենալավ մեկ աղբյուրի ծառի հիմքը (nl_AlPino) հանգեցրեց 54.7-ի LAS-ին, մինչդեռ մեր տվյալների ընտրությունը գերազանցեց ամենալավ փոխանցման ծառի հիմքը և հանգեցրեց 55.6 LAS-ին փորձարկման տվյալների վրա: Ավելի փորձարկումներ կազմակերպեցին մեր ֆրիզիացի տվյալներից դիաքնրիկներին հեռացնելը, ավելի նմանատիպ ուսուցման տվյալներ ստեղծելով վերբերյալ նախադասությունները և մեր լավագույն մոդելը XLM-R օգտագործելով: Այս փորձարկումները ավելի լավ արդ', 'bn': 'যখন উচ্চ সম্পদের ভাষার জন্য উচ্চভাষা পাওয়া গেছে, তখন কম সম্পদ ভাষায় প্রদর্শন করা হয়েছে। এই পত্রিকায় আমরা নীচের সম্পদ ভাষা ফ্রিসিয়ানের পার্গিং দিয়ে মনোযোগ দিচ্ছি। আমরা কোড পরিবর্তনের উদাহরণ ব্যবহার করি, স্বয়ংক্রিয়ভাবে কথা বলা তথ্য ব্যবহার করি, যা একটি চ্যালেঞ্জের ব্যবস্থা প্রমাণ করে। আমরা বিশেষ করে লক্ষ্য ডোমেইনের দিকে একটি প্রশিক্ষণ প্রশিক্ষণ দিতে প্রস্তাব করছি, বেশ কয়েকটি ত্রিব্যাংক থেকে অনুষ্ঠিত হয়েছে। বিশেষ করে, আমরা ল্যাটেন্ট ডিরিচেলেট অ্যালোকেশন (এলডিএ) ব্যবহার করি, শব্দ এবং অক্ষর এন-গ্রাম দিয়ে। আমরা একটি গভীর বিয়াফিন প্যারেজার ব্যবহার করি যা এমবের্টের সাথে শুরু করা হয়েছে। সবচেয়ে ভালো সূত্র ট্রিবাঙ্ক (এনএল_আলপিনো) এর ফলে ৫৪. ৭ সালের একটি ল্যাসের ফলে আমাদের তথ্য নির্বাচনের মধ্যে সবচেয়ে ভালো ট্রেইব্যান্সফার্নারে আরো পরীক্ষার মধ্যে রয়েছে আমাদের ফ্রিসিয়ান ডাটা থেকে ডায়ারিকারীদের সরিয়ে নেয়ার জন্য, তারা বিভিন্ন ধরনের প্রশিক্ষণের তথ্য তৈরি করে এবং এক্সএলএম-', 'az': 'Yüksek performans yüksək kaynaqlar dillərinə verilən halda, düşük kaynaqlar dillərində performans geri qalar. Bu kağızda biz düşük ressurs dilini Frisian dilinin ayırmasına odaklanırıq. Biz kodu dəyişdirilmiş, spontane danışmış məlumatların nümunələrini istifadə edirik, bu isə çətin bir qurğu göstərir. Biz müəyyən edilmiş məqsəd domeinə təhsil edilən bir parçacın təhsil etməyi təklif edirik, çoxlu a ğaç çubuqlarından örnəkləri seçərək. Biz Latent Dirichlet Allocation (LDA) sözləri və karakterləri N-gramləri ilə istifadə edirik. Biz mBERT ilə başlanğıçlı bir biafin parçacısını istifadə edirik. Ən yaxşısı tək mənbə çubuğu (nl_alpino) 54.7-lik LAS olaraq gəldi, amma məlumatlarımız seçilməsi tək təkrar təkrar çubuğunu təkrar etdi və sınama məlumatlarında 55.6 LAS təkrar etdi. XLM-R vasitəsilə ən yaxşı modellərimizi istifadə etmək üçün daha çox bənzər təhsil məlumatları yaratmaq və XLM-R vasitəsilə ən yaxşı modellərimizi istifadə etmək məqsədilə idi. Bu experimentlər daha yaxşı təhsil etməyə yol vermədi.', 'cs': 'Zatímco u jazyků s vysokými zdroji bylo dosaženo vysokého výkonu, výkon u jazyků s nízkými zdroji zaostává. V tomto článku se zaměřujeme na analýzu nízkoprostrojového jazyka fríštiny. Používáme vzorek kódově přepínaných, spontánně mluvených dat, což se ukáže jako náročné nastavení. Navrhujeme trénovat parser speciálně přizpůsobený cílové doméně výběrem instancí z více stromových bank. Konkrétně používáme Latent Dirichlet Allocation (LDA), s N-gramy slova a znaků. Používáme hluboký biafinový parser inicializovaný mBERT. Nejlepší single source stromová banka (nl_alpino) vyústila v LAS 54.7, zatímco náš výběr dat předčil nejlepší přenosovou stromovou banku a vedl k 55.6 LAS na testovacích datech. Další experimenty spočívaly v odstranění diakritiky z frízských dat, vytvoření podobnějších tréninkových dat oříznutím vět a spuštění našeho nejlepšího modelu pomocí XLM-R. Tyto experimenty nevedly k lepšímu výkonu.', 'et': 'Kuigi suure ressursiga keelte puhul on saavutatud suur jõudlus, jääb vähese ressursiga keelte puhul maha. Käesolevas töös keskendume parsimisele madala ressursiga keele friisi keel. Me kasutame koodiga vahetatud spontaanselt kõnelevate andmete näidist, mis osutub keeruliseks seadistuseks. Me teeme ettepaneku koolitada spetsiaalselt sihtdomeenile kohandatud parser, valides eksemplarid mitmest puupunktist. Täpsemalt kasutame Latent Dirichlet Allocation (LDA), sõna- ja märgiga N-grammid. Me kasutame sügavat biafiini parserit, mis on initsialiseeritud mBERT-ga. Parima ühe allika puupanga (nl_alpino) tulemuseks oli LAS 54,7, samas kui meie andmete valik ületas ühe parima ülekande puupanga ja viis 55,6 LAS testi andmetel. Täiendavad eksperimendid hõlmasid diakriitika eemaldamist meie friisi andmetest, sarnasemate treeningandmete loomist lausete kärpimise teel ja parima mudeli kasutamist XLM-R abil. Need eksperimendid ei viinud parema tulemuseni.', 'ca': "Tot i que s'han aconseguit alts resultats en llengües d'alt recurso, els resultats en llengües de baix recurso es retarden. En aquest article ens centrem en l'analització del francès amb baix recursos. Utilitzem una mostra de dades parlades espontàniament canviades de codi, que resulta ser una configuració difícil. We propose to train a parser specifically tailored towards the target domain, by selecting instances from multiple treebanks.  En concret, utilitzem l'Allocació Latent Dirichlet (LDA), amb paraula i caràcter N-grams. Utilitzem un analitzador biaffin profund inicializat amb mBERT. El millor banc d'arbres d'una sola font (nl_alpino) va resultar en un LAS de 54,7 mentre que la nostra selecció de dades va superar el millor banc d'arbres de transfer ència i va portar a 55,6 LAS en les dades de prova. Els experiments adicionals van consistir en eliminar diacrítics de les nostres dades frises, crear dades de formació més similars recollint frases i executant el nostre millor model fent servir XLM-R. Aquests experiments no van portar a millor rendiment.", 'fi': 'Vaikka suuriresurssisten kielten suorituskyky on saavutettu, vähäresurssisten kielten suorituskyky on jäljessä. Tässä artikkelissa keskitymme jäsentämiseen vähäresurssinen kieli friisi. Käytämme koodinvaihtoista, spontaanisti puhuttua dataa, joka osoittautuu haastavaksi kokoonpanoksi. Ehdotamme, että koulutetaan kohdeverkkotunnukselle räätälöity jäsentäjä valitsemalla esiintymiä useista puupankeista. Erityisesti käytämme Latent Dirichlet Allocation (LDA), jossa on sana ja merkki N-grammia. Käytämme syvää biafiininparseria, joka on alustettu mBERT:llä. Paras yhden lähteen puupankki (nl_alpino) tuotti LAS:n 54,7, kun taas meidän tietovalikoimamme ylitti yksittäisen parhaan siirtopuupankin ja johti 55,6 LAS:iin testitiedoissa. Lisäkokeet koostuivat diakriitikkojen poistamisesta friisilaisista tiedoista, samankaltaisten harjoitustietojen luomisesta lauseita leikkaamalla ja parhaan mallimme ajamisesta XLM-R:llä. Nämä kokeet eivät johtaneet parempaan suorituskykyyn.', 'bs': 'Iako je napravljena visoka učinkovitost za jezike visokog resursa, nastup na jezicima niskog resursa ostaje iza sebe. U ovom papiru fokusiramo se na analizu jezika niskog resursa Frisianca. Koristimo uzorak zamjene šifre, spontano govorenih podataka, koji dokazuje da je izazovna postavka. Predlažemo da treniramo analizatora posebno prilagođenog prema ciljnom domenu, birajući instance iz višestrukih područja. Posebno, koristimo Latent Dirichlet Allocation (LDA), sa riječima i karakterom N-grama. Koristimo duboki analizator biafina inicijaliziran sa mBERT-om. Najbolji jedinstveni izvor treeban (nl_alpino) rezultirao je LAS od 54,7, dok je naš izbor podataka iznosio jedinstveni najbolji prevoz treeban i doveo do 55,6 LAS na testne podatke. Dodatni eksperimenti su sastavljeni od uklanjanja diakritika iz naših Frijskih podataka, stvaranja sličnih podataka obuke usvajanjem rečenica i vodeći naš najbolji model koristeći XLM-R. Ovi eksperimenti nisu doveli do boljih izvođenja.', 'jv': 'ssoffpolite"), and when there is a change ("assertive Nang pepul iki, kita dipunduh langkung urip nggambar kelas Jejaran Awak dhéwé nggunakake sampler kanggo kode bisa-bisa dadi, dadi mesthi o nggambar barang seneng pisan. We proposal to vlan a PASSMENDAR PASSMENDAR PASSMENDAR IGNOPAL PASSMENDAR olar" is the abbreviation for "Line", Col is the abbreviation for "Column Two Label Ndheke supaya karo hal-hal sing ngrusak diakritik tentang ning awak dhéwé dolanan ping dolanan, nggawe data sing luwih nggawe dolanan nggawe kesempatan kanggo ngwala macem nggawe model sing luwih nggambar XLM-R. Perintah sing iki dadi sing bisa pasar awak dhéwé.', 'ha': "Akwai da za'a iya samar da sauri wa harshen masu sarki, aiki na bakin harshen-wuri-resource. Ga wannan karatun, Munã fokus wa paring of the lower-resource language Frisian. Tuna yi amfani da wani misali da aka canza kodi, da aka yi magana farat ɗaya, da za'a iya kasa zama tsarin mai tsõratar. Tunamaɗa mu kõre wani parser wanda aka ƙayyade shi zuwa duk aka goa, kuma za mu zãɓi misãlai daga bakin turu masu yawa. A ƙayyade, Munã yi amfani da Allocation na Naƙasan Dirikla (LDA), da maganar da takardar N-gram. Tuna amfani da wani parse mai ƙari wa biffine wanda aka fara da mBERT. Tan da mafi kyaun source trebank (nl_alpino) ta ƙara wata MAS na 54.7 alhãli kuwa zaɓallinmu na zaɓe ta kowacan transfer ta Treebank kuma ya zaɓi 55.6 lass a kan data ta jarraba. Wata jarrabo na ƙaranci na sami da ta tafiyar da diagon daga data masu Frisian, suna samun data masu kama da kwatanku da za'a yi danganta da cire-garwaya kuma ya yi tafiyar da misãlai masu kyãwo da amfani da XLM-R. Wannan jarrabõ ba ta ƙara wani mafiya tsari ba.", 'sk': 'Medtem ko je bila dosežena visoka zmogljivost jezikov z visokimi viri, zmogljivost jezikov z nizkimi viri zaostaja. V tem prispevku se osredotočamo na razčlenitev jezika z nizkimi viri frizijščine. Uporabljamo vzorec spontano govorjenih podatkov, ki se izkažejo za zahtevno nastavitev. Predlagamo usposabljanje razčlenjevalnika, ki je posebej prilagojen ciljni domeni, z izbiro primerkov iz več drevesnih zbirk. Natančneje uporabljamo Latent Dirichlet Allocation (LDA), z besedami in znaki N-grami. Uporabljamo globok biafin razčlenjevalec inicializiran z mBERT. Najboljša enovirna drevesna plošča (nl_alpino) je imela LAS 54,7, medtem ko je naša izbira podatkov presegla najboljšo prenosno drevesno ploščo in privedla do 55,6 LAS na testnih podatkih. Dodatni eksperimenti so vključevali odstranitev diakritikov iz frizijskih podatkov, ustvarjanje podobnih podatkov o treningu z obrezovanjem stavkov in izvajanje našega najboljšega modela z uporabo XLM-R. Ti eksperimenti niso privedli do boljše zmogljivosti.', 'he': 'While high performance have been obtained for high-resource languages, performance on low-resource languages lags behind.  בעיתון הזה אנו מתמקדים במחקר של שפת משאבים נמוכים פריזית. אנחנו משתמשים בדגימה של נתונים שנחלפו קודים, שדיברו באופן ספונטני, מה שמוכיח להיות התקנה מאתגרת. אנו מציעים לאמן מעבד מתוכנן במיוחד לכיוון תחום המטרה, על ידי לבחור מקרים ממספר עצים. במיוחד, אנו משתמשים בהחלטת דיריקלט לאנט (LDA), עם מילה ודמות N-גרם. אנחנו משתמשים במחקר ביאפין עמוק שנתחיל עם mBERT. בנק העץ המקור היחיד הטוב ביותר (nl_alpino) הוביל לאס.אס של 54.7 בזמן שבבחירת הנתונים שלנו עברה את בנק העץ היחיד הטוב ביותר והוביל ל-55.6 לאס.אס על הנתונים המבחנים. ניסויים נוספים כוללו להסיר מחתונים מהנתונים הפריסיים שלנו, ליצור נתונים אימונים דומים יותר על ידי גיבוי משפטים ולפעיל את המודל הטוב ביותר שלנו באמצעות XLM-R. ניסויים אלה לא הובילו להופעה טובה יותר.', 'bo': 'རྒྱུ་དངོས་ཐོག ང་ཚོས་ཤོག་བྱང་འདིའི་ནང་དུ་རྒྱ་ནག་མི་མང་ཆེ་བའི་སྐད་རིགས་ཕྱོགས་སྟོན་པ་ཚོར་བློ་གཏོང་ནི་ ང་ཚོས་མཚོན་རྟགས་ལ་བསྒྱུར་བཅོས་བྱས་པའི་མིག་གྲངས་ཀྱི་དཔེ་བརྗོད་ཞིག་སྤྱོད་ཀྱི་ཡོད། We propose to train a parser specifically tailored towards the target domain, by selecting instances from multiple treebanks. དམིགས་འཛུགས་ཀྱིས། ང་ཚོས་Latent Dirichlet Allocation (LDA)དང་ཐ་སྙད་དང་ཡིག་འབྲུ་N-gramདང་བེད་སྤྱོད་པ We use a deep biaffine parser initialized with mBERT. གསལ་ཤོག་མའི་ཐོག་མའི་འཇུག་སྣོད་གསལ་པོ(nl_alpino)དེ་འདྲ་བཤུ་ཐུབ་པ་ཡིན། ང་ཚོའི་གནས་སྡུད་འདེམས་ཀྱིས་གསལ་བཤད་ཀྱི་གནས་སྟངས་མང་ཤོས་མའི་འཇུག་སྣོད དབྱེ་ཚིག་གི་བརྟག་དཔྱད་ཆ་རྣམས་ང་ཚོའི་Frisian གནད་སྡུད་ཕྱིར་འཐེན་བྱས་ན་ཏེ།'}
{'en': 'Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation', 'ar': 'معالجة مجالات الموارد الصفرية باستخدام سياق مستوى المستند في ترجمة الآلة العصبية', 'es': 'Abordar dominios de recursos cero mediante el contexto de nivel de documento en la traducción automática neuronal', 'pt': 'Lidando com domínios de recurso zero usando contexto em nível de documento na tradução automática neural', 'fr': "Aborder les domaines sans ressources à l'aide du contexte au niveau du document dans la traduction automatique neuronale", 'ja': 'ニューラル・マシン・トランスレーションにおける文書レベルのコンテキストを使用したゼロリソース・ドメインへの対処', 'zh': '用神经机器翻译中文档级上下文寻址零资源域', 'hi': 'न्यूरल मशीन अनुवाद में दस्तावेज़-स्तर प्रसंग का उपयोग करते हुए शून्य-संसाधन डोमेन को संबोधित करना', 'ru': 'Обращение к доменам с нулевыми ресурсами с использованием контекста на уровне документов в нейронном машинном переводе', 'ga': 'Ag Dul i nGleic le Fearainn Nialais-Acmhainne ag Úsáid Comhthéacs Leibhéal Doiciméid san Aistriú Meaisín Néarthach', 'ka': 'Name', 'el': 'Αντιμετώπιση τομέων μηδενικού πόρου χρησιμοποιώντας το πλαίσιο επιπέδου εγγράφου στη νευρωνική μηχανική μετάφραση', 'hu': 'Zéró erőforrás-tartományok kezelése dokumentumszintű kontextus használatával az ideges gépi fordításban', 'it': 'Affrontare i domini a risorse zero utilizzando contesto a livello di documento nella traduzione automatica neurale', 'kk': 'Құжат- деңгейінің контексті нөл- ресурс домендеріне адрес ету', 'lt': 'Adresas nulinių išteklių sritims, naudojant dokumentų lygmens kontekstą vertimo nervinėmis mašinomis srityje', 'mk': 'Адресирање на домени со нула ресурси користејќи контекст на ниво на документ во преведување на неврални машини', 'ms': 'Alamat Domain Sumber-Sifar Mengguna Konteks Aras-Dokumen dalam Terjemahan Mesin Neural', 'ml': 'നെയുറല്\u200d മെഷീന്\u200d പരിഭാഷപ്പെടുത്തുന്ന രേഖയുടെ- നിലനില വിവരങ്ങള്\u200d ഉപയോഗിക്കുന്ന പൂര്\u200dണ്ണവിഭവങ്ങളുടെ ഡോമ', 'mt': 'Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation', 'mn': 'Нэг-нөөц зориулалт домайг хадгалах нь мэдрэлийн машины хөрөнгө оруулалт', 'no': 'Name', 'pl': 'Rozwiązywanie domen zerowych za pomocą kontekstu na poziomie dokumentów w neuronowym tłumaczeniu maszynowym', 'ro': 'Abordarea domeniilor cu resurse zero utilizând contextul la nivel de document în traducerea automată neurală', 'sr': 'Odrešenje domena nula resursa koristeći kontekst nivoa dokumenta u Neuralnom prevodu mašine', 'si': 'ශූර්ය සම්බන්ධ ක්\u200dෂේත්\u200dරය සම්බන්ධය භාවිත කරන්න', 'so': 'Addressing Zero-Resource Domains using Document-Level Content in Neural Machine Translation', 'sv': 'Hantera nollresursdomäner med hjälp av kontext på dokumentnivå i Neural maskinöversättning', 'ta': 'புதிய இயந்திரம் மொழிபெயர்ப்பில் ஆவணம்- நிலை உள்ளடக்கத்தை பயன்படுத்தி பூஜ்ஜிய- மூலங்கள் தளங்களை முகவரி', 'ur': 'نیورال ماشین ترجمہ میں دکھانے-سطح کنٹکسٹ استعمال کرتا ہے', 'uz': 'Name', 'vi': 'Đối xử với lĩnh vực Zero-tài nguyên Dùng ngữ độ Tài liệu chung trong phiên dịch máy thần kinh', 'bg': 'Разглеждане на домейни с нулеви ресурси чрез контекст на ниво документ в невралния машинен превод', 'nl': 'Zero-Resource domeinen aanpakken met behulp van context op documentniveau in neuronale machinevertaling', 'da': 'Løsning af nulressourcdomæner ved hjælp af kontekst på dokumentniveau i Neural maskinoversættelse', 'hr': 'Adresa domena nula resursa koristeći kontekst nivoa dokumenta u prijevozu neuroloških strojeva', 'de': 'Adressieren von Zero-Resource-Domänen mithilfe von Kontext auf Dokumentenebene in der neuronalen maschinellen Übersetzung', 'ko': '신경 기계 번역에서 문서급 상하문 처리 제로 자원 영역 사용하기', 'sw': 'Kuhusiana na Domene zenye rasilimali Zinazotumia Mazingira ya Hukumu-Lengo katika Tafsiri ya Mashine ya Neural', 'fa': 'نشان دادن دامنهای منبع صفر استفاده از محیط سطح سند در ترجمه ماشین عصبی', 'sq': 'Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation', 'tr': 'Senediň derejesi Kontekstini Neural Maşynyň terjimesinde ullanýa', 'af': 'Name', 'hy': 'Զերոռեսուրսների դաշտերի դիմելը՝ օգտագործելով փաստաթղթի մակարդակի կոնտեքստը նյարդային մեքենայի թարգմանման մեջ', 'am': 'Document-Level Context using Neural Machine translation', 'az': 'N칬ral Makin 칂evirm톛sind톛 S캼f캼r-Ressours Domenalar캼', 'id': 'Mengalami Domain Ressource-Zero Menggunakan Konteks Tingkat Dokumen dalam Translation Mesin Neural', 'bs': 'Adresa domena nula resursa koristeći kontekst nivoa dokumenta u neurološkom prevodu strojeva', 'cs': 'Řešení domén s nulovými zdroji pomocí kontextu na úrovni dokumentů v neuronovém strojovém překladu', 'et': 'Nullressursi domeenide käsitlemine neuromasintõlkes dokumenditaseme konteksti abil', 'bn': 'নিউরাল মেশিন অনুবাদের নথি- স্তর ব্যবহার করে জিরো- রোসোর্স ডোমেন ব্যবহার করা হচ্ছে', 'fi': 'Nollaresurssialueiden käsittely dokumenttitason kontekstin avulla neurokonekäännöksessä', 'ca': 'Adresar dominis de recursos zero utilitzant el context a nivell de document en la traducció de màquines neuronales', 'jv': 'Ngubah Resolusi Njaring', 'ha': '@ info: whatsthis', 'sk': 'Obravnavanje domen brez virov z uporabo konteksta na ravni dokumenta v nevralnem strojnem prevajanju', 'he': 'התייחסות למשפחות אפס משאבים בשימוש בתקשר רמת המסמכים בתרגום מכונת נוירואלית', 'bo': 'རྫུན་འབྱུང་མེད་པའི་ཁྱིམ་མཚོན་གྱི་གནས་ཁོངས་ལ་སྤྱོད་པའི་ཡིག་ཆའི་གནས་ཁོངས་ཁོངས་སྤྱོད་པ'}
{'en': 'Achieving satisfying performance in  machine translation  on domains for which there is no training data is challenging. Traditional supervised domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data. We show that when in-domain parallel data is not available, access to document-level context enables better capturing of domain generalities compared to only having access to a single sentence. Having access to more information provides a more reliable domain estimation. We present two document-level Transformer models which are capable of using large context sizes and we compare these  models  against strong Transformer baselines. We obtain improvements for the two zero-resource domains we study. We additionally provide an analysis where we vary the amount of context and look at the case where in-domain data is available.', 'ar': 'يعد تحقيق أداء مُرضٍ في الترجمة الآلية في المجالات التي لا توجد بها بيانات تدريبية أمرًا صعبًا. لا يعد التكيف التقليدي للمجال الخاضع للإشراف مناسبًا لمعالجة مجالات الموارد الصفرية هذه لأنه يعتمد على بيانات موازية في المجال. نوضح أنه في حالة عدم توفر البيانات المتوازية في المجال ، فإن الوصول إلى سياق مستوى المستند يتيح التقاطًا أفضل لعموميات المجال مقارنةً بالحصول على حق الوصول إلى جملة واحدة فقط. يوفر الوصول إلى مزيد من المعلومات تقديرًا أكثر موثوقية للمجال. نقدم نموذجين من المحولات على مستوى المستندات قادران على استخدام أحجام سياق كبيرة ونقارن هذه النماذج مع خطوط الأساس القوية للمحول. نحصل على تحسينات لاثنين من مجالات الموارد الصفرية التي ندرسها. بالإضافة إلى ذلك ، نقدم تحليلًا حيث نغير مقدار السياق وننظر في الحالة التي تتوفر فيها البيانات في المجال.', 'fr': "Il est difficile d'obtenir des performances satisfaisantes en matière de traduction automatique dans des domaines pour lesquels il n'existe pas de données de formation. L'adaptation de domaine supervisée traditionnelle ne convient pas pour traiter de tels domaines à ressources nulles, car elle repose sur des données parallèles dans le domaine. Nous montrons que lorsque les données parallèles dans le domaine ne sont pas disponibles, l'accès au contexte au niveau du document permet de mieux saisir les généralités du domaine par rapport à l'accès à une seule phrase. L'accès à davantage d'informations fournit une estimation de domaine plus fiable. Nous présentons deux modèles de transformateurs au niveau du document qui sont capables d'utiliser de grandes tailles de contexte et nous comparons ces modèles à des bases de référence solides pour les transformateurs. Nous obtenons des améliorations pour les deux domaines sans ressources que nous étudions. Nous fournissons également une analyse dans laquelle nous faisons varier la quantité de contexte et examinons le cas où des données internes au domaine sont disponibles.", 'es': 'Lograr un rendimiento satisfactorio en la traducción automática en dominios para los que no hay datos de capacitación es un desafío. La adaptación tradicional de dominios supervisados no es adecuada para abordar estos dominios de recursos cero porque se basa en datos paralelos dentro del dominio. Mostramos que cuando los datos paralelos dentro del dominio no están disponibles, el acceso al contexto a nivel de documento permite captar mejor las generalidades del dominio en comparación con tener acceso solo a una frase. Tener acceso a más información proporciona una estimación de dominio más confiable. Presentamos dos modelos de Transformer a nivel de documento que son capaces de usar grandes tamaños de contexto y comparamos estos modelos con líneas de base de Transformer sólidas. Obtenemos mejoras para los dos dominios de cero recursos que estudiamos. Además, proporcionamos un análisis en el que variamos la cantidad de contexto y analizamos el caso en el que hay datos dentro del dominio disponibles.', 'pt': 'Alcançar um desempenho satisfatório na tradução automática em domínios para os quais não há dados de treinamento é um desafio. A adaptação de domínio supervisionado tradicional não é adequada para abordar esses domínios de recurso zero porque depende de dados paralelos no domínio. Mostramos que quando os dados paralelos no domínio não estão disponíveis, o acesso ao contexto em nível de documento permite uma melhor captura de generalidades de domínio em comparação com o acesso apenas a uma única frase. Ter acesso a mais informações fornece uma estimativa de domínio mais confiável. Apresentamos dois modelos Transformer em nível de documento que são capazes de usar grandes tamanhos de contexto e comparamos esses modelos com linhas de base fortes do Transformer. Obtemos melhorias para os dois domínios de recursos zero que estudamos. Além disso, fornecemos uma análise em que variamos a quantidade de contexto e analisamos o caso em que os dados no domínio estão disponíveis.', 'ja': 'トレーニングデータがないドメインでの機械翻訳で満足のいくパフォーマンスを達成することは困難です。従来の監視対象ドメイン適応は、ドメイン内並列データに依存するため、そのようなゼロリソースドメインに対処するのに適していない。ドメイン内の並列データが利用できない場合、ドキュメントレベルのコンテキストへのアクセスは、単一の文にのみアクセスできるのと比較して、ドメインの一般性をよりよくキャプチャできることを示しています。より多くの情報にアクセスすることで、より信頼性の高いドメインの推定値が得られます。大規模なコンテキストサイズを使用できる2つの文書レベルのトランスフォーマーモデルを提示し、これらのモデルを強力なトランスフォーマーベースラインと比較します。私たちは、私たちが研究する2つのゼロリソースドメインの改善を取得します。さらに、コンテキストの量を変化させ、ドメイン内データが利用可能なケースを調べる分析を提供します。', 'zh': '无习数之域机器翻译性有挑战性。 旧制监域自宜不宜理此零资源域者,以其依于域内并行数据也。 臣等明方域内并行数据不可用时,与仅访单句相比,访文档级上下文可以善获域中一般性。 访更多信息,可供更可凭。 建二文档之 Transformer ,其可以大上下文大小,比之于强 Transformer 基线。 我得我两个零资源域改进。 供一分,变上下文数,视域中可用者。', 'hi': 'डोमेन पर मशीन अनुवाद में संतोषजनक प्रदर्शन प्राप्त करना जिसके लिए कोई प्रशिक्षण डेटा नहीं है, चुनौतीपूर्ण है। पारंपरिक पर्यवेक्षित डोमेन अनुकूलन ऐसे शून्य-संसाधन डोमेन को संबोधित करने के लिए उपयुक्त नहीं है क्योंकि यह इन-डोमेन समानांतर डेटा पर निर्भर करता है। हम दिखाते हैं कि जब इन-डोमेन समानांतर डेटा उपलब्ध नहीं होता है, तो दस्तावेज़-स्तर के संदर्भ तक पहुंच केवल एक वाक्य तक पहुंच होने की तुलना में डोमेन सामान्यताओं को बेहतर ढंग से कैप्चर करने में सक्षम बनाती है। अधिक जानकारी तक पहुँच होने से अधिक विश्वसनीय डोमेन अनुमान प्रदान करता है. हम दो दस्तावेज़-स्तरीय ट्रांसफॉर्मर मॉडल पेश करते हैं जो बड़े संदर्भ आकारों का उपयोग करने में सक्षम हैं और हम मजबूत ट्रांसफॉर्मर बेसलाइन के खिलाफ इन मॉडलों की तुलना करते हैं। हम दो शून्य-संसाधन डोमेन के लिए सुधार प्राप्त करते हैं जिनका हम अध्ययन करते हैं। हम इसके अतिरिक्त एक विश्लेषण प्रदान करते हैं जहां हम संदर्भ की मात्रा को अलग-अलग करते हैं और उस मामले को देखते हैं जहां इन-डोमेन डेटा उपलब्ध है।', 'ru': 'Достижение удовлетворительной производительности в машинном переводе на доменах, для которых нет данных обучения, является сложной задачей. Традиционная контролируемая адаптация домена не подходит для адресации таких доменов с нулевыми ресурсами, поскольку она опирается на внутридоменные параллельные данные. Мы показываем, что когда внутридоменные параллельные данные недоступны, доступ к контексту на уровне документа позволяет лучше фиксировать общие характеристики домена по сравнению с доступом только к одному предложению. Доступ к большему объему информации обеспечивает более надежную оценку домена. Мы представляем две модели трансформаторов на уровне документа, которые способны использовать большие размеры контекста, и мы сравниваем эти модели с сильными базовыми линиями трансформатора. Мы получаем улучшения для двух областей с нулевыми ресурсами, которые мы изучаем. Мы дополнительно предоставляем анализ, где мы изменяем объем контекста и смотрим на случай, когда внутридоменные данные доступны.', 'ga': 'Is dúshlánach é feidhmíocht shásúil san aistriúchán meaisín a bhaint amach ar fhearainn nach bhfuil aon sonraí oiliúna ina leith. Níl oiriúnú fearainn maoirsithe traidisiúnta oiriúnach chun aghaidh a thabhairt ar na fearainn acmhainní nialais sin toisc go mbraitheann sé ar shonraí comhthreomhara san fhearann. Léirímid nuair nach bhfuil sonraí comhthreomhara laistigh den fhearann ar fáil, go bhféadann rochtain ar chomhthéacs leibhéal na gcáipéisí ginearáltachtaí fearainn a ghabháil níos fearr i gcomparáid le rochtain a bheith againn ar abairt amháin. Soláthraíonn rochtain ar bhreis faisnéise meastachán fearainn níos iontaofa. Cuirimid dhá mhúnla Claochladáin ar leibhéal doiciméad i láthair atá in ann méideanna móra comhthéacs a úsáid agus cuirimid na samhlacha sin i gcomparáid le bonnlínte láidre Trasfhoirmeoirí. Faighimid feabhsuithe don dá réimse acmhainní nialasach a ndéanaimid staidéar orthu. Ina theannta sin cuirimid anailís ar fáil ina n-athraímid an méid comhthéacs agus breathnaíonn muid ar an gcás ina bhfuil sonraí in-fearainn ar fáil.', 'ka': 'მაქინის გაგრძელებაში სამუშაო სამუშაო სამუშაო სამუშაო სამუშაო სამუშაო სამუშაო სამუშაო მონაცემები არ არს ტრადიციონალური მონაცემებული დიომინის ადაპტიფიკაცია არ შესაძლებელია ასეთი ნულ რესურსის დიომინის მისაღებისთვის, რადგან ის დარწმუნება დიომინის პარალელი მონა ჩვენ ჩვენ აჩვენებთ, რომ როცა დოკუმენტის დოკუმენტის კონტექსტის მისამართლობა არ მიიღება დოკუმენტის დოკუმენტის კონტექსტის მისამართლად დიომინის გენ უფრო მეტი ინფორმაციის შესაძლებლობა უფრო დარწმუნებელი დიომინის განსაზღვრება. ჩვენ აჩვენებთ ორი დოკუმენტის განტრანფორმაციის მოდელები, რომლებიც ძალიან დიდი კონტექსტური ზომის გამოყენება და ჩვენ ეს მოდელები ძალიან ტრანფორმაციის ფ ჩვენ მივიღეთ ორი ნულ რესურსის დიომენებისთვის შესაძლებლობა. ჩვენ დამატებით ანალიზია, სადაც ჩვენ განსხვავებთ კონტექსტის რაოდენობას და დავხედავთ კონტექსტის მონაცემების შესახებ.', 'hu': 'Kihívást jelent a gépi fordítás kielégítő teljesítményének elérése olyan területeken, ahol nincsenek képzési adatok. A hagyományos felügyelt domain adaptáció nem alkalmas az ilyen nulla erőforrásból álló tartományok kezelésére, mivel tartományonbelüli párhuzamos adatokra támaszkodik. Megmutatjuk, hogy ha nem állnak rendelkezésre tartományonbeli párhuzamos adatok, akkor a dokumentumszintű kontextushoz való hozzáférés lehetővé teszi a tartomány általános jellemzőinek jobb rögzítését, mint egyetlen mondathoz való hozzáférés. A további információkhoz való hozzáférés megbízhatóbb domain becslést biztosít. Két dokumentumszintű Transformer modellt mutatunk be, amelyek képesek nagy kontextusméreteket használni, és ezeket a modelleket összehasonlítjuk az erős Transformer alapvonalakkal. Az általunk tanulmányozott két nulla erőforrás-tartomány fejlesztését érjük el. Emellett olyan elemzést is készítünk, amelyben változtatjuk a kontextus mennyiségét, és megvizsgáljuk azt az esetet, ahol a domain adatok állnak rendelkezésre.', 'el': 'Η επίτευξη ικανοποιητικών επιδόσεων στη μηχανική μετάφραση σε τομείς για τους οποίους δεν υπάρχουν δεδομένα κατάρτισης είναι πρόκληση. Η παραδοσιακή εποπτευόμενη προσαρμογή τομέα δεν είναι κατάλληλη για την αντιμετώπιση τέτοιων τομέων μηδενικού πόρου επειδή βασίζεται σε παράλληλα δεδομένα εντός του τομέα. Δείχνουμε ότι όταν δεν είναι διαθέσιμα παράλληλα δεδομένα εντός του τομέα, η πρόσβαση στο πλαίσιο σε επίπεδο εγγράφου επιτρέπει την καλύτερη καταγραφή γενικοτήτων του τομέα σε σύγκριση με την πρόσβαση μόνο σε μια πρόταση. Η πρόσβαση σε περισσότερες πληροφορίες παρέχει μια πιο αξιόπιστη εκτίμηση τομέα. Παρουσιάζουμε δύο μοντέλα μετασχηματιστών σε επίπεδο εγγράφου που είναι σε θέση να χρησιμοποιούν μεγάλα μεγέθη περιβάλλοντος και συγκρίνουμε αυτά τα μοντέλα με ισχυρές γραμμές βάσης μετασχηματιστή. Λαμβάνουμε βελτιώσεις για τους δύο τομείς μηδενικού πόρου που μελετούμε. Επιπλέον, παρέχουμε μια ανάλυση όπου διαφοροποιούμε την ποσότητα του πλαισίου και εξετάζουμε την περίπτωση όπου τα δεδομένα εντός του τομέα είναι διαθέσιμα.', 'kk': 'Компьютердің аудармасының үйлесімділігін жеткізу үшін оқыту мәліметі қиын емес. Дәстүрлі бақылау доменінің адаптациясы нөл ресурс домендеріне қатынау үшін керек емес, себебі ол доменде параллелі деректерге сенімді. Доменде параллель деректер қол жеткізбегенде, құжат деңгейіндегі контекстіне қатынау тек бір сөйлемеге қатынауға қатынау мүмкіндігі болады. Көбірек мәліметтерге қатынау керек болса, доменнің сенімді оқиғаларын көрсетеді. Біз үлкен контекстің өлшемдерін қолдануға мүмкін екі құжат деңгейіндегі түрлендіруші үлгілерін көрсетедік. Бұл үлгілерді күшті Трансформалер негізгі жолдарын Біз зерттеген екі нөл ресурс доменінің жақсартуларын аламыз. Біз қосымша, контекстің санын өзгертіп, домендегі деректер қайда бар болып тұрғанын қараймыз.', 'lt': 'Įgyvendinant mašin in į vertimą tomis sritimis, kuriose nėra mokymo duomenų, sunku pasiekti patenkinamus rezultatus. Tradicinis prižiūrimas domeno pritaikymas netinka tokiems nulinio išteklio domenams spręsti, nes jis grindžiamas lygiagrečiais domeno duomenimis. Mes rodome, kad kai lygiagrečių duomenų domene nėra, prieiga prie dokumentų lygmens konteksto leidžia geriau sugalvoti domeno bendruomenes, palyginti su galimybe naudotis tik vienu sakiniu. Galimybė gauti daugiau informacijos suteikia patikimesnį domeno vertinimą. Pateikiame du dokumentų lygmens Transformer modelius, kurie gali naudoti didelius konteksto dydžius, ir palyginame šiuos modelius su stipriomis Transformer bazinėmis linijomis. Mes pasiekiame patobulinimus dviejose nulinių išteklių srityse, kuriose mes tiriame. Be to, mes pateikiame analizę, kurioje skiriamės konteksto dydis, ir nagrinėsime atvejį, kai turime domeninius duomenis.', 'ms': 'Mencapai prestasi yang memuaskan dalam terjemahan mesin pada domain yang tidak ada data latihan adalah menantang. Penyesuaian domain yang diawasi tradisional tidak sesuai untuk mengatasi domain sumber-sifar sebab ia bergantung pada data selari dalam domain. Kami menunjukkan bahawa apabila data selari dalam domain tidak tersedia, akses ke konteks aras-dokumen membolehkan tangkapan lebih baik generaliti domain dibandingkan hanya mempunyai akses ke satu kalimat. Memiliki akses kepada maklumat lanjut menyediakan penilaian domain yang lebih dipercayai. Kami memperkenalkan dua model Transformer tahap dokumen yang mampu menggunakan saiz konteks besar dan kita membandingkan model ini dengan garis dasar Transformer yang kuat. Kami mendapat peningkatan untuk dua domain sumber sifar yang kami belajar. We additionally provide an analysis where we vary the amount of context and look at the case where in-domain data is available.', 'ml': 'ട്രെയിനിങ്ങളുടെ ഡേമെനുകളില്\u200d മെഷീന്\u200d പരിഭാഷത്തിന്റെ പ്രഭാഷണത്തില്\u200d സന്തോഷകരമായ പ്രവര്\u200dത്തനം ലഭ്യമാക് പാര്\u200dട്ടികമായി നിരീക്ഷിക്കപ്പെട്ട ഡൊമെന്\u200d അഡാപ്റ്റേഷന്\u200d ഇതുപോലുള്ള പൂജ്യ വിഭവങ്ങള്\u200d ഡോമെനില്\u200d അടയാളപ്പെടുത്തുന്നതി നമ്മള്\u200d കാണിച്ചുകൊടുക്കുന്നുണ്ടെങ്കില്\u200d ഡൊമെയിനിലെ പാരാളില്\u200d ഡേറ്റല്\u200d ഡേറ്റായുള്ള വിവരങ്ങള്\u200d ലഭ്യമാകുമ്പോള്\u200d ഒരു വാക്കി കൂടുതല്\u200d വിവരങ്ങള്\u200d ലഭ്യമാക്കുന്നതിന് വിശ്വസ്തനായ വീടുകള്\u200d കൂടുതല്\u200d വിശദീകരിക്കുന്നു. നമ്മള്\u200d രണ്ടു രേഖകളുടെ നിലവില്\u200d ട്രാന്\u200dസ്ഫോര്\u200dമാന്\u200dസ് മോഡലുകള്\u200d കാണിക്കുന്നു. അത് വലിയ കോണ്\u200dസ്റ്റെക്സ്റ്റ് വലിപ്പ നമ്മള്\u200d പഠിക്കുന്ന രണ്ട് പൂജ്യ വിഭവങ്ങളുടെ ഡോമെന്\u200dസിന് മെച്ചപ്പെടുത്തുന്നു. നമ്മള്\u200d കൂടുതല്\u200d ഒരു അന്വേഷണം നല്\u200dകുന്നു. അവിടെ നമ്മള്\u200d വ്യത്യസ്തമായിരിക്കുന്നു എന്നിട്ട് ഡൊമൈന്\u200d ഡേറ്റാറ്റ എവി', 'it': "Raggiungere prestazioni soddisfacenti nella traduzione automatica su domini per i quali non esistono dati di formazione è difficile. L'adattamento tradizionale dei domini supervisionati non è adatto per affrontare tali domini a risorse zero perché si basa su dati paralleli in-domain. Mostriamo che quando i dati paralleli in-domain non sono disponibili, l'accesso al contesto a livello di documento consente una migliore acquisizione delle generalità di dominio rispetto ad avere accesso solo a una singola frase. Avere accesso a più informazioni fornisce una stima del dominio più affidabile. Presentiamo due modelli di Transformer a livello di documento che sono in grado di utilizzare grandi dimensioni di contesto e confrontiamo questi modelli con linee di base di Transformer forti. Otteniamo miglioramenti per i due domini a risorse zero che studiamo. Forniamo inoltre un'analisi in cui variamo la quantità di contesto e guardiamo al caso in cui i dati in-domain sono disponibili.", 'mk': 'Добивањето задоволни резултати во машинскиот превод на домени за кои нема податоци за обука е предизвикувачки. Традиционалната надгледувана адаптација на домен не е соодветна за адресање на ваквите домени со нула ресурси бидејќи се потпира на паралелни податоци во домен. Покажуваме дека кога паралелните податоци во доменот не се достапни, пристапот до контекст на ниво на документ овозможува подобро зафатување на генералитетите на доменот во споредба со пристапот само на една реченица. Имањето пристап до повеќе информации обезбедува подоверлива проценка на доменот. Презентираме два модели на документно ниво Трансформер кои се способни да користат големи контекстни големини и ги споредуваме овие модели со силни бази на Трансформер. We obtain improvements for the two zero-resource domains we study.  Дополнително обезбедуваме анализа во која се разликува количината на контекст и го гледаме случајот каде се достапни податоци во доменот.', 'pl': 'Osiągnięcie satysfakcjonującej wydajności w tłumaczeniu maszynowym w dziedzinach, dla których nie ma danych szkoleniowych, jest wyzwaniem. Tradycyjna adaptacja nadzorowanej domeny nie jest odpowiednia do rozwiązywania takich domen zerowych, ponieważ opiera się ona na danych równoległych wewnątrz domeny. Pokazujemy, że gdy dane równoległe w domenie nie są dostępne, dostęp do kontekstu na poziomie dokumentu umożliwia lepsze uchwycenie ogólności domeny w porównaniu z dostępem tylko do jednego zdania. Dostęp do więcej informacji zapewnia bardziej wiarygodne oszacowanie domeny. Przedstawiamy dwa modele Transformera na poziomie dokumentów, które są w stanie wykorzystać duże rozmiary kontekstu i porównujemy te modele z silnymi liniami bazowymi Transformera. Uzyskujemy ulepszenia dla dwóch obszarów zerowych, które badamy. Dodatkowo przeprowadzamy analizę, w której zmieniamy ilość kontekstu i przyglądamy się przypadkowi, w którym dostępne są dane wewnątrz domeny.', 'mt': 'Il-kisba ta’ prestazzjoni sodisfaċenti fit-traduzzjoni bil-magna f’oqsma li għalihom m’hemmx dejta ta’ taħriġ hija sfida. L-adattament tradizzjonali tad-dominju sorveljat mhuwiex adattat għall-indirizzar ta’ dawn id-dominji b’riżorsi żero minħabba li jiddependi fuq dejta parallela fid-dominju. Aħna nuru li meta dejta parallel a fid-dominju ma tkunx disponibbli, l-a ċċess għall-kuntest fil-livell tad-dokument jippermetti qbid aħjar tal-ġeneralitajiet tad-dominju meta mqabbel ma’ aċċess għal sentenza waħda biss. L-a ċċess għal aktar informazzjoni jipprovdi stima aktar affidabbli tad-dominju. Aħna nippreżentaw żewġ mudelli Transformer fil-livell ta’ dokumenti li huma kapaċi jużaw daqsijiet ta’ kuntest kbar u aħna nqabblu dawn il-mudelli ma’ linji bażi Transformer b’saħħithom. Aħna nkisbu titjib għaż-żewġ oqsma ta’ riżorsi żero li nistudjaw. Barra minn hekk, nagħtu analiżi fejn ivarjaw l-ammont tal-kuntest u nħarsu lejn il-każ fejn id-dejta fid-dominju hija disponibbli.', 'mn': 'Машин хөгжлийн хөгжлийн хандлагатай үйл ажиллагааг олох нь хэцүү байдаг. Учир нь энэ нь холбоотой параллель өгөгдлийг хамаарч байгаа учраас, уламжлалтай удирдлагатай холбоотой загварын загвар зохицуулалт нь тийм 0-нүүрстөрөгчийн зохицуулалт тулгарч Бид холбоотой параллел өгөгдлийн хувьд баримт түвшинд хүрэх нь зөвхөн ганц өгөгдлийн хувьд зөвхөн нэг өгөгдлийн хувьд домены ерөнхийлөгчийн хувьд илүү сайн түвшинд хүрэх боломжтой Ихэнх мэдээлэл дээр хүртэл байх нь илүү итгэлтэй холбооны тооцоололтой байдаг. Бид эдгээр загваруудыг хүчтэй Трансформер суурь шугамын эсрэг харьцуулж чадна. Бид судалж байгаа хоёр тэрбум боловсролын газрыг сайжруулдаг. Мөн бид холбоотой мэдээллийн хэрэглэгдэх тохиолдолд өөрчлөгдсөн талаар шинжилгээ өгдөг.', 'no': 'Å oppnå tilfredsstillende utføringar i maskinsomsetjinga på domene som ikkje finst opplæringsdata er vanskeleg. Tradisjonalt oversikt domeneadaptasjon er ikkje passande for å adressa slike null- ressursdomene fordi det dependerer på parallelle data i domenet. Vi viser at når parallelle data i domenet ikkje er tilgjengeleg, kan tilgang til dokumentnivåkonteksten gjera at det er bedre å henta domenegeneralitet sammenlignet med berre å ha tilgang til ei enkelt setning. Når du har tilgang til meir informasjon, gjev eit betre domeneestimating. Vi presenterer to dokumentnivåtransformeringsmodeller som kan bruka stor kontekststorleik og sammenlignar desse modelane med sterke transformeringsbaselinjer. Vi får forbedringar for dei to null- ressursdomene vi studerer. Vi gjev tillegg ei analyse der vi varierer mengda kontekst og ser på tilfellet der data i domenet er tilgjengeleg.', 'ro': 'Obținerea performanțelor satisfăcătoare în traducerea automată pe domenii pentru care nu există date de formare este o provocare. Adaptarea tradițională a domeniilor supravegheate nu este adecvată pentru abordarea unor astfel de domenii cu resurse zero, deoarece se bazează pe date paralele în domeniu. Arătăm că atunci când nu sunt disponibile date paralele în domeniu, accesul la contextul la nivel de document permite o mai bună captare a generalităților domeniului în comparație cu accesul doar la o singură propoziție. Accesul la mai multe informații oferă o estimare mai fiabilă a domeniului. Prezentăm două modele de Transformer la nivel de document capabile să utilizeze dimensiuni mari de context și comparăm aceste modele cu linii de bază puternice ale Transformer. Obținem îmbunătățiri pentru cele două domenii cu resurse zero pe care le studiam. În plus, oferim o analiză în care variazăm cantitatea de context și analizăm cazul în care sunt disponibile date în domeniu.', 'si': 'මෙහෙයුම් පරිවර්තනයේ පරිවර්තනයේ සම්පූර්ණයෙන් ප්\u200dරශ්නයක් ලැබෙන්න, මෙහෙයුම් පරිවර්තනයේ කි පාරමාණික පරීක්ෂණය කරපු ඩොමේන් සමාන්\u200dය දත්ත සඳහා සුන්ධ පරීක්ෂණය සඳහා ප්\u200dරයෝජනය නැහැ. අපි පෙන්වන්නේ ඩොමේන් එක්ක සාමාන්\u200dය දත්ත අවස්ථාවක් නැති වෙලාවට, වාර්තාව සම්බන්ධයෙන් ප්\u200dරවේශ කරන්න පුළුවන් තරම්  වැඩි තොරතුරු ලබාගන්න ප්\u200dරවේශයක් විශ්වාස කරන්න පුළුවන් තරම් විශ්වාසිත විශ්වාස කරන අපි ලොකු සම්බන්ධ ප්\u200dරමාණය පාවිච්චි කරන්න පුළුවන් ලොකු සම්බන්ධ ප්\u200dරමාණය දෙකක් පෙනුම් කරනවා, අපි මේ මොඩේල් අපි ඉගෙන ගන්නේ ශූන්ය සම්පූර්ණ දෙකක් තියෙනවා. අපි තවත් විශ්ලේෂණයක් දෙන්නේ අපි පරික්ෂණයක් වෙනස් කරනවා කියලා, අපි පරික්ෂණයේ ප්\u200dරමාණයක් වෙනස් කරනවා ඒ', 'so': "Si aad u hesho fasax raalli ah oo ku qoran tarjumaadda machineedka, kuwaas oo aan macluumaadka waxbarashada lagu helin dhibaato la'aan. Adeegista guriga ee caadiga ah laguma habboona in lagu addeeco meelaha nooca ah, sababtoo ah waxay ku xiran tahay macluumaadka lambarka ah ee gudaha. Waxaynu tusnaynaa in marka macluumaadka lambarka ah ee domain aan helin, isticmaalka kooxda wargeyska waxaa suurtogal ah in la qabsado dhaqaalaha gudaha oo la barbarbarto si kaliya loo helo hal eray keliya. Wixii macluumaad dheeraad ah la heli karo waxaad ka heleysaa qiimeyn dheeraad ah oo ku aamin karta guriga. Waxaynu soo bandhignaa laba samooyin oo heerka wargeyska ah, kuwaas oo awoodi kara inay isticmaalaan qiyaastii aad u weyn, waxaynu isbarbardhignaa tusaalahaas oo ka gees ah saldhigyada xoogga badan ee Transformer. Horumarinta waxaan helaynaa labada meelood oo aan baranayno nooca ah. Sidoo kale waxaynu kaloo sameynaa baaritaanka, meesha aan ku kala duwananahay kooxda, waxaana fiirinnaa xaaladda lagu helo macluumaadka gudaha.", 'sr': 'Doživljavanje zadovoljavajućih učinka u prevodu mašine na domenama za koje nema podataka o obuci je izazovno. Tradicionalna nadzorna adaptacija domena nije prikladna za rješavanje takvih domena nula resursa jer se oslanja na paralelne podatke u domenu. Pokazujemo da, kada paralelni podaci u domenu nisu dostupni, pristup kontekstu na nivou dokumenta omogućava bolje hvatanje generaliteta domena u usporedbi sa samo pristupom jednoj rečenici. Imajući pristup više informacija pruža pouzdaniju procenu domena. Predstavljamo dva modela transformera na nivou dokumenta koji su sposobni da koriste velike veličine konteksta i uspoređujemo te modele protiv jakih osnovnih linija transformera. Dobili smo poboljšanje za dve domene od nule resursa koje proučavamo. Osim toga, pružamo analizu u kojoj menjamo količinu konteksta i pogledamo slučaj gde su podaci u domenu dostupni.', 'ta': 'கணினியின் மொழிபெயர்ப்பில் திருப்தியான செயல்பாட்டை பெறுகிறது. இதில் பயிற்சி தரவு சவால் இல்லை. பாரம்பரிய கண்காணிக்கப்பட்ட ட டொமைன் ஒதுக்கீடு இது பூஜ்ஜியமான மூலங்களை சேர்க்க பொருத்தமானது ஏனென்றால் இது டோமைன் இணைப்பு தரவ நாம் காட்டுகிறோம் களம் இணைப்பு தகவல் கிடைக்கும் போது, ஆவண- மட்டத்தின் சூழல் அணுகல் ஒரு வாக்கியத்தை மட்டும் அணுகுவதை ஒப்பிடும் களம மேலும் தகவல் அணுகல் கொண்டிருந்தால் அதிக நம்பிக்கைக்கும் உள்ளீட்டு கணக்கிடும். நாம் இரண்டு ஆவண- நிலை மாற்றி மாற்றும் மாதிரிகளை காண்பிக்கிறோம். அது பெரிய சூழல் அளவுகளை பயன்படுத்த முடியும். இவ்விரு நாம் படிக்கும் இரண்டு பூஜ்ஜிய மூலங்களின் முன்னேற்றங்களை கிடைக்கும். நாம் கூடுதலாக ஒரு ஆராய்ச்சியை வழங்குகிறோம் அதில் நாம் சூழ்நிலையின் அளவு வேறுபட்டிருக்கிறது மற்றும் களம் தக', 'sv': 'Att uppnå tillfredsställande prestanda inom maskinöversättning på domäner där det inte finns några träningsdata är en utmaning. Traditionell övervakad domänanpassning är inte lämplig för att adressera sådana nollresursdomäner eftersom den är beroende av parallella data inom domänen. Vi visar att när parallella data inom domänen inte finns tillgängliga möjliggör åtkomst till kontext på dokumentnivå bättre fånga domängeneraliteter jämfört med att bara ha tillgång till en enda mening. Att ha tillgång till mer information ger en mer tillförlitlig domänestimering. Vi presenterar två transformatormodeller på dokumentnivå som kan använda stora kontextstorlekar och vi jämför dessa modeller mot starka transformatorbaslinjer. Vi får förbättringar för de två nollresursdomäner vi studerar. Dessutom ger vi en analys där vi varierar mängden sammanhang och tittar på fallet där domändata finns tillgängliga.', 'ur': 'ڈومین پر ماشین ترجمہ کے مطابق کامیابی کو پہنچا رہا ہے جس کے لئے کوئی تعلیم ڈیٹ نہیں ہے، مشکل ہے. اس طرح صفر-سروسیس ڈومین کو اپنا ادرس کرنے کے لئے منظم تحت نظر والی ڈومین اضافہ کرنے کے لئے مناسب نہیں ہے کیونکہ یہ ڈومین میں پارالی ڈاٹ پر اعتماد کرتا ہے. ہم دکھاتے ہیں کہ جب ڈومین میں پارالیل ڈیٹا موجود نہیں ہوتا، ڈومین لئویل کنٹنسیٹ کی دسترسی بہتر ڈومین جرالیلیٹ کے پکڑنے کو امکان دیتی ہے صرف ایک جماعت کے دسترسی کے مطابق۔ اور زیادہ معلومات کے لئے دسترسی حاصل کرنے کے لئے بہت قابل اعتماد دار ڈومین کا مطابق ہے. ہم دو دفتر سطح ترنسفور موڈل پیش کرتے ہیں جو بڑے کنٹنسیٹ سایز استعمال کرنے کے قابل ہیں اور ہم ان موڈلوں کو قوی ترنسفور بنیس لین کے ساتھ مقایسہ کرتے ہیں. ہم ان دونوں صفر سروسیس ڈمنین کے لئے صلاحیت حاصل کرتے ہیں جو ہم پڑھتے ہیں۔ ہم اضافہ طور پر ایک تحلیل دیتے ہیں جہاں ہم کنٹکس کی مقدار بدل دیتے ہیں اور اس حالت کو دیکھتے ہیں جہاں دامین میں ڈیٹا موجود ہے.', 'uz': "Name Name Биз доменда парламент маълумотлар мавжуд бўлмаганида, ҳужжат даражасига кириши мумкин, фақат бир сўзга эга бўлиш мумкин бўлган доменинг ументларини қабул қилиш мумкин. Koʻproq maʼlumotga murojaat qilishni qo'shish mumkin. Biz ikkita hujjatning darajasi Transformer modellarini koʻrsatimiz. Ular katta context sizlarini ishlatish mumkin va biz bu modellarni tasavvur qilamiz. Biz o'rganib o'rganish ikkita null resource domain uchun yaxshi o'zlarimiz. We additionally provide an analysis where we vary the amount of context and look at the case where in-domain data is available.", 'vi': 'Khả năng đạt được trong việc dịch chuyển máy trên lĩnh vực không có dữ liệu đào tạo đầy thử thách. Truyền thống giám sát quản trị thích hợp không thích hợp để đối phó các miền không tài nguyên bởi vì nó dựa trên dữ liệu song song trong miền. Chúng tôi cho thấy khi dữ liệu song song trong miền không có sẵn, truy cập vào ngữ cảnh cấp tài liệu cho khả năng nắm giữ thông số miền tốt hơn so với chỉ có quyền sử dụng một câu. Việc truy cập thêm thông tin cung cấp một đánh giá miền đáng tin cậy hơn. Chúng tôi giới thiệu hai mô hình Biến hình cấp tài liệu có khả năng sử dụng kích cỡ bối cảnh lớn và chúng tôi so sánh các mô hình này với các bản nền biến hình mạnh. Chúng tôi có những cải tiến cho hai lĩnh vực không tài nguyên mà chúng tôi nghiên cứu. Chúng tôi cung cấp thêm một phân tích nơi chúng tôi thay đổi mức độ ngữ cảnh và xem xét trường hợp có dữ liệu nội bộ.', 'bg': 'Постигането на удовлетворяващи резултати в машинния превод в области, за които няма данни за обучение, е предизвикателство. Традиционното контролирано адаптиране на домейна не е подходящо за адресиране на такива домейни с нулеви ресурси, тъй като разчита на паралелни в домейна данни. Показваме, че когато паралелни данни в домейна не са налични, достъпът до контекст на ниво документ позволява по-добро улавяне на общи домейни в сравнение с достъпа само до едно изречение. Достъпът до повече информация осигурява по-надеждна оценка на домейна. Представяме два модела трансформатори на ниво документ, които могат да използват големи контекстни размери и сравняваме тези модели със силните базови линии на трансформаторите. Получаваме подобрения за двете области с нулеви ресурси, които изучаваме. Освен това предоставяме анализ, при който варираме размера на контекста и разглеждаме случая, в който има налични данни в областта.', 'da': 'Det er en udfordring at opnå tilfredsstillende ydeevne inden for maskinoversættelse på domæner, hvor der ikke findes træningsdata. Traditionel overvåget domænetilpasning er ikke egnet til at adressere sådanne nulressource-domæner, fordi den er afhængig af parallelle data i domænet. Vi viser, at når paralleldata på domænene ikke er tilgængelige, muliggør adgang til kontekst på dokumentniveau bedre indfangelse af domænenes generaliteter sammenlignet med kun at have adgang til en enkelt sætning. At have adgang til flere oplysninger giver et mere pålideligt domænebestimat. Vi præsenterer to transformatormodeller på dokumentniveau, som er i stand til at bruge store kontekststørrelser, og vi sammenligner disse modeller med stærke Transformer basislinjer. Vi opnår forbedringer for de to nulressource domæner, vi studerer. Vi leverer desuden en analyse, hvor vi varierer mængden af kontekst og ser på det tilfælde, hvor in-domæne data er tilgængelige.', 'de': 'Eine zufriedenstellende Leistung bei der maschinellen Übersetzung in Bereichen zu erzielen, für die keine Trainingsdaten vorliegen, ist eine Herausforderung. Die traditionelle überwachte Domänenanpassung eignet sich nicht für die Adressierung solcher Zero-Resource-Domänen, da sie auf parallelen In-Domain-Daten beruht. Wir zeigen, dass der Zugriff auf Kontext auf Dokumentenebene bei fehlenden parallelen Daten eine bessere Erfassung von Domänengemeinschaften ermöglicht, als nur Zugriff auf einen einzigen Satz zu haben. Der Zugriff auf weitere Informationen ermöglicht eine zuverlässigere Domain-Schätzung. Wir stellen zwei Transformer-Modelle auf Dokumentenebene vor, die große Kontextgrößen verwenden können und vergleichen diese Modelle mit starken Transformer-Basislinien. Wir erhalten Verbesserungen für die beiden Zero-Resource-Domänen, die wir untersuchen. Zusätzlich bieten wir eine Analyse an, bei der wir den Umfang des Kontexts variieren und den Fall betrachten, in dem In-Domain-Daten verfügbar sind.', 'ko': '훈련 데이터가 없는 분야에서 만족스러운 기계 번역 성능을 실현하는 것은 도전이다.전통적인 감독 영역이 있는 것은 이런 제로 자원 영역을 처리하는 데 적합하지 않다. 왜냐하면 영역 내의 병렬 데이터에 의존하기 때문이다.우리는 역내 병렬 데이터를 사용할 수 없을 때 한 문장만 방문하는 것보다 문서급 상하문에 접근하는 것이 역의 일반성을 더욱 잘 포착할 수 있음을 나타낸다.더 많은 정보를 방문하면 더욱 신뢰할 수 있는 지역 평가를 제공할 수 있다.우리는 두 개의 문서급 변환기 모델을 제시했는데 이들은 큰 상하문 크기를 사용할 수 있고 이런 모델을 강한 변환기 기선과 비교했다.우리는 연구한 두 개의 제로 자원 영역에 대해 개선을 진행하였다.그 밖에 우리는 분석을 제공했다. 그 중에서 우리는 상하 문장의 수량을 바꾸었고 역내 데이터가 사용할 수 있는 상황을 살펴봤다.', 'hr': 'Doživljavanje zadovoljavajućih učinka u prevodu strojeva na domenama za koje nema podataka o obuci je izazovno. Tradicionalna adaptacija domena nad nadzorom nije prikladna za rješavanje takvih domena nula resursa jer se oslanja na paralelne podatke u domenu. Pokazujemo da kad paralelni podaci u domenu nisu dostupni, pristup kontekstu na nivou dokumenta omogućava bolje hvatanje generalnosti domena u usporedbi s pristupom samo jednoj rečenici. Imajući pristup više informacija pruža pouzdaniju procjenu domena. Predstavljamo dva modela transformera na razini dokumenta koji su sposobni koristiti velike veličine konteksta i uspoređujemo te modele protiv jakih osnovnih linija transformera. Dobili smo poboljšanje za dvije domene od nule resursa koje proučavamo. Osim toga, pružamo analizu u kojoj mijenjamo količinu konteksta i pogledamo slučaj gdje su podaci u domenu dostupni.', 'nl': 'Het behalen van bevredigende prestaties in machinevertaling op domeinen waarvoor geen trainingsgegevens beschikbaar zijn, is een uitdaging. Traditionele begeleide domeinaanpassing is niet geschikt voor het aanpakken van dergelijke zero-resource domeinen omdat deze afhankelijk is van parallelle gegevens in het domein. We laten zien dat wanneer parallelle gegevens in het domein niet beschikbaar zijn, toegang tot context op documentniveau het mogelijk maakt om domeinalgemeenheden beter vast te leggen in vergelijking met toegang tot slechts één zin. Toegang tot meer informatie zorgt voor een betrouwbaardere domeinschatting. We presenteren twee Transformer modellen op documentniveau die grote contextformaten kunnen gebruiken en we vergelijken deze modellen met sterke Transformer baselines. We krijgen verbeteringen voor de twee zero-resource domeinen die we bestuderen. Daarnaast bieden we een analyse waarbij we de hoeveelheid context variëren en kijken naar het geval waarin in-domain data beschikbaar is.', 'id': 'Mencapai prestasi yang memuaskan dalam terjemahan mesin pada domain yang tidak ada data latihan adalah tantangan. Traditional supervised domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data.  Kami menunjukkan bahwa ketika data paralel dalam domain tidak tersedia, akses ke konteks tingkat dokumen memungkinkan menangkap lebih baik generalitas domain dibandingkan hanya memiliki akses ke satu kalimat. Having access to more information provides a more reliable domain estimation.  Kami mempersembahkan dua model Transformer tingkat dokumen yang mampu menggunakan ukuran konteks besar dan kami membandingkan model ini dengan garis dasar Transformer yang kuat. Kami mendapatkan peningkatan untuk dua domain sumber daya nol yang kami pelajari. Kami tambahan memberikan analisis di mana kita berbeda jumlah konteks dan melihat kasus di mana data dalam domain tersedia.', 'sw': 'Kupata ufanisi wa kutosha katika tafsiri ya mashine kwenye maeneo ambayo hakuna taarifa za mafunzo zinachangamoto. Uwekezaji wa huduma za kawaida unaoangaliwa kwa kitamaduni si sahihi kwa kuzungumzia maeneo ya rasilimali kama sifuri kwa sababu inategemea taarifa za usambazaji wa ndani. Tunaonyesha kwamba pale taarifa za usambazaji za ndani hazipatikani, upatikanaji wa mazingira ya kiwango cha nyaraka unawezesha kupata vizazi vizuri vya ndani ukilinganisha na kuwa na upatikanaji wa hukumu moja tu. Kupatia upatikanaji wa taarifa zaidi hutoa taarifa inayoaminika zaidi ya nyumbani. Tunaweza kuwasilisha mifano miwili ya kiwango cha dokumentari yenye uwezo wa kutumia ukubwa wa muktadha na tunalinganisha mifano hii dhidi ya misingi yenye nguvu ya Transformer. Tunapata maendeleo kwa ajili ya maeneo mawili yasiyo sifuri tunayosoma. Kwa kuongeza tunatoa uchambuzi ambako tunatofauti kiasi cha muktadha na kuangalia kesi ambapo taarifa za ndani zinapatikana.', 'tr': 'Makina terjime etmäge rugsat ýerine ýetirmek üçin hiç hili bilim maglumaty kynçylykly däldir. Däpli gözetlenýän domeny adaptasy şöyle bir nul-rekursor sahypalary barlamak üçin ýeterli däl, çünki bu domenyň paralel maglumatynda ynamly. Biz domain paralel maglumatlary ulaşandygyny görkezýäris, sened-derejesi konteksta süýtgetmek diňe bir sözlere erişini bar diňe gowy bir süýtgetmek mümkin edýäris. Köp maglumat üçin erişilemek ýere güýçli bir domeny hasaplamagyny töwek edýär. Biz iki sened derejesini terjime edip görkezip nusgalaryny belli bir kontekst ululykyny ulanyp bileris we bu nusgalary güýçli terjime edip görkezip bileris. Biz öwrenýän iki sany 0 resurs alany üçin gelişmeleri tapýarys. Hem de domenin verilerinin ulaşan durumda farklı bir çözümleme temin ediyoruz.', 'fa': 'رسیدن اجرای راضی در ترجمه ماشین در دامنه\u200cها که هیچ داده آموزش\u200cای برای آن نیست سخت است. تغییرات دامنی سنتی که تحت نظر قرار گرفته\u200cاند برای پذیرش چنین دامنه\u200cهای منابع صفر منابع منابع مناسب نیست چون به داده\u200cهای parallel دامنه اعتماد دارد. ما نشان می دهیم که هنگامی که داده\u200cهای پارالی در دامنه\u200cها در دسترسی ندارند، دسترسی به محیط سطح سند بهتر دستگیر کردن عمومی دامنه\u200cها را در مقایسه با دسترسی به یک جمله تنها می\u200cدهد. با دسترسی به اطلاعات بیشتری، ارزیابی دامنی قابل اعتماد بیشتری را پیشنهاد می\u200cدهد. ما دو مدل تغییر\u200cپذیر سطح سند را پیشنهاد می\u200cکنیم که قادر به استفاده از اندازه\u200cهای محیط بزرگ هستند و این مدل\u200cها را با خطوط\u200cهای بنیادی\u200cهای تغییر\u200cپذیر قوی مقایسه می\u200cکنیم. ما برای دو دامنه منبع صفر تحقیق می کنیم. ما به اضافه یک تحلیل پیشنهاد می کنیم که مقدار محیط را تغییر می دهیم و به پرونده نگاه کنیم که داده های داخلی داخلی موجود است.', 'sq': 'Të arrish shfaqje të kënaqshme në përkthimin e makinave në fusha për të cilat nuk ka të dhëna trajnimi është sfiduese. Përpërshtatja tradicionale e mbikqyrur e domain it nuk është e përshtatshme për trajtimin e domaineve të tilla zero-resource sepse mbështetet në të dhënat paralele në domain. Ne tregojmë se kur të dhënat paralele në domeni nuk janë në dispozicion, aksesi në kontekstin e nivelit të dokumentit lejon kapjen më të mirë të gjeneraliteteve të domenit krahasuar me të patur akses vetëm në një fjalim të vetëm. Duke pasur akses në më shumë informacion ofron një vlerësim më të besueshëm të domenit. Ne paraqesim dy modele Transformer në nivel të dokumentit që janë në gjendje të përdorin madhësi të mëdha konteksti dhe i krahasojmë këto modele me linjat bazë të forta Transformer. Ne arrijmë përmirësime për dy fushat zero-burimi që studiojmë. Përveç kësaj ne ofrojmë një analizë ku ndryshojmë sasinë e kontekstit dhe shikojmë rastin ku të dhënat në domeni janë në dispozicion.', 'hy': 'Հաճելի արդյունք ստանալը մեքենայի թարգմանման ոլորտներում, որոնց համար ոչ մի ուսուցման տվյալներ չկան, դժվար է: Ադապտացիոնալ ավանդական ադապտացիան տիեզերքում համապատասխան չէ զրո ռեսուրսների տիեզերքների լուծմանը, քանի որ այն հիմնված է տիեզերքում պարամեռ տվյալների վրա: Մենք ցույց ենք տալիս, որ երբ բնագավառի զուգահեռ տվյալները հասանելի չեն, փաստաթղթի մակարդակի կոնտեքստի հասանելիությունը հնարավորություն է տալիս ավելի լավ բնագավառել բնագավառի ընդհանուր տարբերությունները, համեմատած միայն Ավելի շատ ինֆորմացիայի հասանելիությունը ավելի վստահելի արտահայտություն է տալիս: Մենք ներկայացնում ենք երկու փաստաթղթի մակարդակի տրանսֆերմերի մոդել, որոնք կարող են օգտագործել մեծ կոնտեքստի չափսեր և համեմատում ենք այս մոդելները տրանսֆերմերի ուժեղ հիմնական գծերի հետ: Մենք կատարում ենք բարելավումներ մեր ուսումնասիրության երկու զրո ռեսուրսների ոլորտների համար: Մենք նաև կատարում ենք վերլուծություն, որտեղ մենք տարբերվում ենք կոնտեքստի քանակը և ուսումնասիրում ենք այն դեպքը, երբ տիեզերական տվյալները հասանելի են:', 'af': "Ontvang die versadiging van prestasie in masjien vertaling op domeine waarvoor daar geen onderwerking data is vanskeilig nie. Tradisjoneel ondersoekte domein aanpassing is nie geskik om sodanige nul- hulpbron domeine te adres nie, omdat dit op in- domein parallele data vertrou. Ons wys dat wanneer in- domein parallele data nie beskikbaar is nie, toegang tot dokumentvlak konteks kan beter opneem van domein genereliteit vergelyk word met slegs toegang tot 'n enkele seting. Het toegang tot meer inligting verskaf 'n meer vertroubare domein estimatie. Ons stel twee dokumentvlak Transformer modele voor wat moontlik is om groot konteksgrootte te gebruik en ons vergelyk hierdie modele teen sterk Transformer basisline. Ons kry verbeteringe vir die twee nulhulpbron domene wat ons studeer. Ons verskaf ook 'n analisie waar ons die hoeveelheid van konteks verander en kyk na die geval waar in-domein data beskikbaar is.", 'am': 'የሞክራዊ ትርጓሜዎችን በመስጠት የሚሞላ የድምፅ ማድረግ ማግኘት ነው፡፡ የባሕላዊው ተጠቃሚ ዶሜን አቀማመጥ እንደዚህ የ0-resource domains ለመጨመር አይገባም ፤ ምክንያቱም በዶሜን በተስማማው ዳታዎች ላይ ይታመናል። በዶሜን ውስጥ ተያያያይነት ዳታ ባይኖር፣ የሰነድ-ደረጃ context መግኘት የሚችል የዶሜን አዳራዎችን በአንድ ንግግር ብቻ ማግኘት ይችላል፡፡ Having access to more information provides a more reliable domain estimation.  ሁለት ሰነድ-ደረጃን መተላለፊያ ዓይነቶች እናቀርባለን፣ እነዚህንም ምሳሌዎች በኃይለኛ Transformer Baselines ላይ በሚችሉ ትልቅ ምሳሌዎች እናስተያየዋለን፡፡ ለምናስተምረው ለሁለቱ የክፍለ ሀብት አዳራጊዎችን አግኝተናል፡፡ በጨዋታ የውይይት መረጃዎች የትኛውን እንደተለየን እናሳውቃለን፡፡', 'az': 'Müəllif məlumatların heç bir təhsil məlumatları çətin olmadığı məlumatların maşın çevirilməsində təmizlənən performansını tapmaq. Növbəti təhlükəsizli domena uyğulaması böyük sıfır-ressurs domenalarını əlavə etmək üçün uyğun deyil, çünki o domena paralel məlumatlarına bağlı olar. Biz domena paralel məlumatların faydalanmadığı zaman, dökümə səviyyə məlumatlarına istifadə etmək ancaq bir cümləyə istifadə edilməyə müvəffəq edər. Daha çox məlumatlara istifadə etmək daha güvenilir domena hesabını verir. Biz iki səviyyə transformer modelini göstəririk ki, böyük kontekst ölçülərini istifadə edə bilər və bu modelləri güclü Transformer səviyyələri ilə qarşılaşdırırıq. Biz öyrəndiyimiz iki sıfır ressurs domeinlərin yaxşılıqlarını alırıq. Biz həmçin in məlumatların dəyişikliyini dəyişdirdiyimiz analizi təmin edirik və domena verilən məlumatların necə olduğuna baxsaq.', 'bn': 'মেশিন অনুবাদে প্রশিক্ষণের কোন তথ্য চ্যালেঞ্জ নেই। ঐতিহ্যবাহী পর্যবেক্ষিত ডোমেইন এডেপেটশন এই ধরনের শূন্য-সম্পদ ডোমেনে যোগাযোগ করার জন্য যথেষ্ট নয় কারণ এটি ডোমেইনের প্যারালেল ডাটা আমরা দেখাচ্ছি যে যখন ডোমেইন-এর প্যারালেল ডাটা পাওয়া যাবে না, তথ্যের স্তরে প্রবেশ করার ব্যবস্থা শুধুমাত্র একটি বাক্যে প্রবেশ করার তুলনায় ড আরো তথ্য প্রবেশ করার ক্ষেত্রে আরো বিশ্বস্ত ঘরের হিসাব প্রদান করে। আমরা দুই ডকুমেন্ট-স্তর ট্রান্সফ্রান্সফার মডেল উপস্থাপন করি যারা বিশাল প্রেক্ষিতের আকার ব্যবহার করতে সক্ষম এবং আমরা এই মডেল আমরা শুধুমাত্র দুটি সম্পদের জন্য উন্নতি পেয়েছি। We additionally provide an analysis where we vary the amount of context and look at the case where in-domain data is available.', 'cs': 'Dosažení uspokojivého výkonu při strojovém překladu v oblastech, pro které neexistují žádné tréninkové údaje, je náročné. Tradiční adaptace dohledu domény není vhodná pro řešení takových domén s nulovými zdroji, protože spoléhá na paralelní data v doméně. Ukazujeme, že když paralelní data v doméně nejsou k dispozici, umožňuje přístup k kontextu na úrovni dokumentu lépe zachytit doménové obecnosti ve srovnání s přístupem pouze k jedné větě. Přístup k dalším informacím poskytuje spolehlivější odhad domény. Představujeme dva modely Transformeru na úrovni dokumentů, které jsou schopny využít velké velikosti kontextu a porovnáváme tyto modely se silnými základními liniemi Transformeru. Získáváme zlepšení pro dvě domény s nulovými zdroji, které studujeme. Dále poskytujeme analýzu, kde měníme množství kontextu a podíváme se na případ, kdy jsou data v doméně k dispozici.', 'et': 'Masintõlke rahuldavate tulemuste saavutamine valdkondades, mille kohta puuduvad koolitusandmed, on keeruline. Traditsiooniline järelevalvealase domeeni kohandamine ei sobi selliste nullressursiga domeenide käsitlemiseks, sest see tugineb domeenisisestele paralleelsetele andmetele. Näitame, et kui domeenisisesed paralleelsed andmed ei ole kättesaadavad, võimaldab juurdepääs dokumenditasemel kontekstile paremini domeeni üldiste andmete jäädvustamist võrreldes juurdepääsuga ainult ühele lausele. Juurdepääs rohkemale teabele annab usaldusväärsema domeeni hinnangu. Esitleme kahte dokumenditasemel Transformeri mudelit, mis suudavad kasutada suuri kontekstisuuruseid ja võrdleme neid mudeleid tugevate Transformeri baasjoonetega. Me saame parandusi kahe null-ressursi valdkonna jaoks, mida me uurime. Lisaks pakume analüüsi, kus muudame konteksti hulka ja vaatame juhtumeid, kus domeenisisesed andmed on kättesaadavad.', 'bs': 'Postavljanje zadovoljavajućeg učinka u prevodu strojeva na domenama za koje nema podataka o obuci je izazovno. Tradicionalna adaptacija domena nad nadzorom nije prikladna za rješavanje takvih domena nula resursa jer se oslanja na paralelne podatke u domenu. Pokazujemo da kad paralelni podaci u domenu nisu dostupni, pristup kontekstu na nivou dokumenta omogućava bolje hvatanje generalnosti domena u usporedbi s pristupom samo jednoj rečenici. Imajući pristup više informacija pruža pouzdaniju procjenu domena. Predstavljamo dva modela transformera na nivou dokumenta koji su sposobni da koriste velike veličine konteksta i uspoređujemo te modele protiv jakih osnovnih linija transformera. Dobili smo poboljšanje za dvije domene od nula resursa koje proučavamo. Osim toga, pružamo analizu u kojoj se razlikujemo količina konteksta i pogledamo slučaj gdje su podaci u domenu dostupni.', 'ca': "Obtenir un rendiment satisfaccionant en la traducció màquina en dominis per als quals no hi ha dades d'entrenament és difícil. L'adaptació tradicional supervisada del domini no és adequada per abordar aquests dominis de recursos zero perquè es basa en dades paralleles en el domini. Mostrem que quan no hi ha dades paralleles en domini, l'accés al context a nivell de document permet capturar millor les generalitats de domini en comparació amb tenir accés a una sola frase. Tenir accés a més informació proporciona una estimació de domini més fiable. Presentam dos models de Transformer a nivell documental capaços d'utilitzar grans dimensions contextuals i comparem aquests models amb línies de base fortes de Transformer. Obtenim millors en els dos dominis de recursos zero que estudiem. També proporcionem una an àlisi en què variam la quantitat de context i mirem el cas en què hi ha dades en domini disponibles.", 'fi': 'Tyydyttävän suorituskyvyn saavuttaminen konekäännöksessä aloilla, joista ei ole koulutustietoja, on haastavaa. Perinteinen valvottu verkkotunnuksen mukauttaminen ei sovellu tällaisten nollaresurssialueiden käsittelyyn, koska se perustuu toimialueen sisäisiin rinnakkaistietoihin. Osoitamme, että kun verkkotunnuksen rinnakkaistietoja ei ole saatavilla, dokumenttitason kontekstin käyttö mahdollistaa verkkotunnuksen yleisten tietojen paremman tallentamisen verrattuna siihen, että käytettävissä on vain yksi lause. Lisätietojen saatavuus antaa luotettavamman verkkotunnuksen estimoinnin. Esittelemme kaksi asiakirjatason Transformer-mallia, jotka pystyvät käyttämään suuria kontekstikokoja ja vertaamme näitä malleja vahvoihin Transformer-lähtölinjoihin. Saamme parannuksia kahdelle tutkimallemme nollaresurssialueelle. Lisäksi tarjoamme analyysin, jossa muutamme kontekstin määrää ja tarkastelemme tapausta, jossa verkkotunnuksen tietoja on saatavilla.', 'jv': 'Jejaring Traditional super Vised domain modification is not valid for Address like null-source domain We show that when in Where am I Laptop" and "Desktop Awak dhéwé éntuk perusahaan kanggo ngerasakno iki dadi tanggal We additional information about an example', 'he': 'להשיג ביצועים מרוצים בתרגום מכונות בתחומים שאין להם נתונים אימונים זה מאתגר. Traditional supervised domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data.  אנחנו מראים שכאשר נתונים מקבילים בתחום אינם זמינים, גישה לקונקסט רמת המסמכים מאפשרת תפיסה טובה יותר של גנרליות התחום בהשוואה רק לגישה למשפט אחד. יש גישה ליותר מידע מספק ערכת תחום אמינה יותר. אנחנו מציגים שני דוגמנים מעברת מסמכים שיכולים להשתמש בגודלים קונטקסט גדולים ואנחנו משווים את הדוגמנים האלה עם קווי בסיס מעברת חזקים. אנחנו מקבלים שיפורים לשני תחומי משאבים אפס שאנחנו לומדים. בנוסף, אנחנו מספקים ניתוח שבו אנחנו משנים את כמות הקשר ולהסתכל על המקרה שבו נתונים בתחום זמינים.', 'ha': "Kumotsa rabo mai yarda cikin fassarar ɗin mashine a kan daidai wanda bãbu wani data na amfani da shi sai yana tsõratar. @ info: whatsthis Tuna nũna cewa, idan ba za'a iya sãmun data masu daidaita cikin-Domen ba, za'a iya amfani da matsayin-daraja ɗin-takardar, zai yarda in kãma masu motsi ɗin Domen sami da ya sami da kuma a sami da ya sami idan ba ya iya haɗi sofa guda kawai. Idan ana sami akan mutane, yana da amfani da masu aminci. Tuna halatar da misãlai biyu na takardar-daraja Transformer, wanda za'a iya iya amfani da girma cikin mazaɓa, kuma muna samfanar da waɗannan misãlai masu motsi zuwa fassaran mai ƙarfin Transformer. Tuna sami mafiya kyauta ga duk wuri biyu da Muke karatun. Tuna samar da wani anadi a inda muna sãɓã wa lokacin mazaɓa, kuma mu dũba idan an iya sãmu da data cikin guda.", 'sk': 'Doseganje zadovoljive učinkovitosti pri strojnem prevajanju na področjih, za katere ni podatkov o usposabljanju, je izziv. Tradicionalna nadzorovana prilagoditev domen ni primerna za obravnavanje takšnih domen brez virov, ker se zanaša na vzporedne podatke znotraj področja. Kadar vzporedni podatki v domeni niso na voljo, dostop do konteksta na ravni dokumenta omogoča boljše zajemanje splošnosti domene v primerjavi z dostopom samo do enega stavka. Dostop do več informacij zagotavlja zanesljivejšo oceno domene. Predstavljamo dva modela transformatorjev na ravni dokumenta, ki lahko uporabljata velike kontekstne velikosti in jih primerjamo z močnimi osnovnimi linijami transformatorjev. Dobivamo izboljšave za dve področji brez virov, ki jih preučujemo. Poleg tega zagotavljamo analizo, pri kateri spreminjamo količino konteksta in si ogledamo primer, v katerem so na voljo podatki v domeni.', 'bo': 'མ་ལག་གི་སྐབས་ཡིག་གནས་སྟངས་ལ་དཀའ་ངལ་ཅན་གྱི་ཡོད་ཚད་རྒྱལ་སྤྲོད་ཀྱི་ཡོད་ཚད་དཀའ་ངལ་མེད། Traditional supervised domain adaptation is not suitable for addressing such zero-resource domains because it relies on in-domain parallel data. We show that when in-domain parallel data is not available, access to document-level context enables better capturing of domain generalities compared to only having access to a single sentence. Having access to more information provides a more reliable domain estimation. ང་ཚོས་ཡིག ང་ཚོས་ཤེས་དུ་རྒྱུ་ནུས་ཀྱི་ཁོར་ཡུག་ཚང་གཉིས་ཀྱི་ཆ་རྐྱེན་ཡར་རྒྱས་གཏོང་བ་རེད། We additionally provide an analysis where we vary the amount of context and look at the case where in-domain data is available.'}
{'en': 'BERTologiCoMix : How does Code-Mixing interact with Multilingual BERT? BERT ologi C o M ix: How does Code-Mixing interact with Multilingual  BERT ?', 'ar': 'BERTologiCoMix: كيف يتفاعل خلط الشفرات مع Multilingual BERT؟', 'fr': 'BertologicoMix\xa0: Comment le mélange de code interagit-il avec le BERT multilingue\xa0?', 'es': 'BertoLogiComix: ¿Cómo interactúa la mezcla de códigos con BERT multilingüe?', 'pt': 'BERTologiCoMix: Como o Code-Mixing interage com o BERT multilíngue?', 'ja': 'BERTologiCoMix ： Code - MixingはMultilingual BERTとどのように相互作用しますか？', 'hi': 'BERTologiCoMix: कोड-मिक्सिंग बहुभाषी BERT के साथ कैसे बातचीत करता है?', 'zh': 'BERTologiCoMix曰:Code-Mix 何以与多言 BERT 交?', 'ru': 'BERTologiCoMix: Как Code-Mixing взаимодействует с Multilingual BERT?', 'ga': 'BERTologiCoMix: Conas a idirghníomhaíonn Códmheascadh le BERT Ilteangach?', 'hu': 'BERTologicoMix: Hogyan működik a kódkeverés a többnyelvű BERT-vel?', 'ka': 'BERTologiCoMix: როგორ მრავალენგური BERT-თან კოდი-მექსირება?', 'el': 'Πώς αλληλεπιδρά η ανάμειξη κώδικα με το πολύγλωσσο BERT;', 'it': 'BERTologicoMix: Come interagisce il Code-Mixing con BERT multilingue?', 'kk': 'BERTologiCoMix: Код- Mixing көптілік BERT- мен қалай үйлесімді?', 'lt': 'BERTologiCoMix: Kaip kodų mišinys sąveikauja su daugiakalbiu BERT?', 'ms': 'BERTologiCoMix: Bagaimana Pencampuran-Kod berinteraksi dengan BERT Berbahasa?', 'mk': 'BERTologiCoMix: Како комбинацијата на кодови интеракционира со Мултијазичниот BERT?', 'mt': 'BERTologiCoMix: Kif it-Taħlita tal-Kodiċi tinteraġixxi ma’ BERT Multilingwi?', 'mn': 'BERTologiCoMix: Код-Mixing нь олон хэл BERT-тэй хэрхэн харилцаж байна вэ?', 'no': 'BERTologiCoMix: Korleis interaksjonar med fleirspråk BERT?', 'ml': 'ബെര്\u200dടോളജിക്കോമിക്സ്: കോഡ്-മിക്സിങ്ങ് എങ്ങനെയുണ്ട് പല ഭാഷ ബെര്\u200dട്ടി?', 'pl': 'BERTologiCoMix: Jak mieszanie kodów współdziała z wielojęzycznym BERT?', 'sr': 'BERTologiCoMix: Kako miješanje koda interakcija sa višejezičkim BERT-om?', 'sv': 'BERTologicoMix: Hur samverkar kodblandning med flerspråkig BERT?', 'ta': 'பெர்டோலிகோமிக்ஸ்: குறியீடு- கலக்கும் இடைமுறை எப்படி இருக்கும்?', 'ro': 'BERTologicoMix: Cum interacţionează combinarea codurilor cu BERT multilingv?', 'si': 'BERTologiCoMisc: කොහොමද කෝඩ් මික්ස් එක්ක ගොඩභාෂාව BERT එක්ක සම්බන්ධ වෙන්නේ?', 'so': 'BERTologiCoMix: Sidee ayaa Code-Mixing loola xiriiraa luuqado badan oo BERT?', 'ur': 'BERTologiCoMix: کڈ میکسنگ کیسے Multilingual BERT کے ساتھ اپنا ارتباط کرتا ہے؟', 'uz': 'Bir nechta tillar BERT bilan bir nechta kod- Miks qanday aloqa qiladi?', 'vi': 'Kết hợp mã giao hoà với hỗn hợp ngôn ngữ rộng?', 'bg': 'Как взаимодейства смесването на кодове с многоезичния BERT?', 'nl': 'BERTologiCoMix: Hoe werkt Code-Mixing met Meertalige BERT?', 'hr': 'BERTologiCoMix: Kako miješanje koda interakcija s višejezičkim BERT-om?', 'da': 'BERTologicoMix: Hvordan interagerer kodeblanding med flersproget BERT?', 'de': 'BERTologiCoMix: Wie interagiert Code-Mixing mit Multilingual BERT?', 'id': 'BERTologiCoMix: Bagaimana Pengcampuran Kode berinteraksi dengan BERT Berbahasa?', 'ko': 'BERTOlogiCoMix: 코드 혼합은 다국어 BERT와 어떻게 상호작용합니까?', 'fa': 'BERTologiCoMix: چگونه تعامل قانونی با BERT چند زبان است؟', 'sw': 'BERTologiCoMix: Code-Mixing Inawezaje kuhusiana na lugha nyingi BERT?', 'tr': 'BERTologiCoMix: Kod-Mixing Multilingual BERT ile nasıl etkileşir?', 'af': 'BERTologiCoMix: Hoe doen Code-Mixing interaksie met Multilingual BERT?', 'am': 'BERTologiCoMix: Code-Mixing interact with Multilingual BERT እንዴት?', 'az': 'BERTologiCoMix: Kod-Mixing Multilingual BERT il…Щ nec…Щ …Щlaq…Щ edir?', 'bn': 'বের্টোলজিকোমিক্স: কোড-মিক্সিং মাল্টিভাষার বার্টের সাথে কিভাবে যোগাযোগ করে?', 'hy': 'BERTOLOCOMIX. Ինչպե՞ս է կոդի խառնուրդը փոխազդեցություն ունենում բազլեզու BERT-ի հետ:', 'bs': 'BERTologiCoMix: Kako miješanje koda interakcija sa višejezičkim BERT-om?', 'ca': 'BERTologiCoMix: Com interactua la combinació de codis amb el BERT multillenguatge?', 'cs': 'BERTologiCoMix: Jak komunikuje mixování kódu s vícejazyčným BERT?', 'et': 'BERTologiCoMix: Kuidas koodide segamine mõjutab mitmekeelset BERT-i?', 'fi': 'BERTologiCoMix: Miten koodien sekoittaminen on vuorovaikutuksessa monikielisen BERTin kanssa?', 'sq': 'BERTologiCoMix: Si ndërvepron përzierja e kodeve me BERT shumëgjuhës?', 'ha': '@ info: whatsthis', 'sk': 'BERTologiCoMix: Kako mešanje kod interagira z večjezičnim BERT?', 'jv': 'BERT logiCoMix: Kiopo kode-Mixing interactive with Multilanguage BERT?', 'he': 'איך מערבב קודים מתקשר עם BERT רבולוגי?', 'bo': 'BERTologiCoMix: How does Code-Mixing interact with Multilingual BERT?'}
{'en': 'Models such as mBERT and XLMR have shown success in solving Code-Mixed NLP tasks even though they were not exposed to such text during pretraining. Code-Mixed NLP models have relied on using  synthetically generated data  along with  naturally occurring data  to improve their performance. Finetuning mBERT on such  data  improves it’s code-mixed performance, but the benefits of using the different types of Code-Mixed data are n’t clear. In this paper, we study the impact of  finetuning  with different types of code-mixed data and outline the changes that occur to the  model  during such  finetuning . Our findings suggest that using naturally occurring code-mixed data brings in the best performance improvement after  finetuning  and that  finetuning  with any type of code-mixed text improves the responsivity of it’s attention heads to code-mixed text inputs.', 'ar': 'أظهرت نماذج مثل mBERT و XLMR نجاحًا في حل مهام البرمجة اللغوية العصبية المختلطة بالكود على الرغم من عدم تعرضهم لمثل هذا النص أثناء التدريب المسبق. اعتمدت نماذج البرمجة اللغوية العصبية المختلطة بالكود على استخدام البيانات التي تم إنشاؤها صناعيًا جنبًا إلى جنب مع البيانات التي تحدث بشكل طبيعي لتحسين أدائها. يعمل ضبط mBERT على مثل هذه البيانات على تحسين الأداء الممزوج بالشفرات ، لكن فوائد استخدام الأنواع المختلفة من البيانات المختلطة بالكود ليست واضحة. في هذه الورقة ، ندرس تأثير التوليف النهائي مع أنواع مختلفة من البيانات المختلطة بالشفرات ونلخص التغييرات التي تحدث للنموذج أثناء هذا التوليف النهائي. تشير النتائج التي توصلنا إليها إلى أن استخدام البيانات المختلطة بالشفرات التي تحدث بشكل طبيعي يؤدي إلى تحسين الأداء الأفضل بعد التوليف النهائي ، وأن التوليف النهائي مع أي نوع من النصوص المختلطة بالشفرات يحسن استجابة رؤوس انتباهه لمدخلات النص المختلط بالشفرات.', 'fr': "Des modèles tels que MBert et XLMR ont réussi à résoudre des tâches de PNL mixtes de code même s'ils n'ont pas été exposés à ce type de texte pendant la pré-formation. Les modèles NLP à code mixte se sont appuyés sur l'utilisation de données générées synthétiquement et de données naturelles pour améliorer leurs performances. Le réglage précis de MBerT sur de telles données améliore les performances de mixage de code, mais les avantages de l'utilisation des différents types de données mixtes de code ne sont pas clairs. Dans cet article, nous étudions l'impact du réglage fin avec différents types de données mixtes de code et décrivons les changements qui se produisent dans le modèle au cours de ce réglage fin. Nos résultats suggèrent que l'utilisation de données mixtes naturelles apporte la meilleure amélioration des performances après le réglage fin et que le réglage fin avec tout type de texte mixte de code améliore la réactivité de ses têtes attentives aux entrées de texte mixte de code.", 'pt': 'Modelos como mBERT e XLMR mostraram sucesso na resolução de tarefas de PNL de Code-Mixed, embora não tenham sido expostos a esse texto durante o pré-treinamento. Os modelos de PNL de Code-Mixed têm contado com o uso de dados gerados sinteticamente junto com dados de ocorrência natural para melhorar seu desempenho. O ajuste fino do mBERT nesses dados melhora seu desempenho de código misto, mas os benefícios de usar os diferentes tipos de dados de código misto não são claros. Neste artigo, estudamos o impacto do ajuste fino com diferentes tipos de dados mistos de código e descrevemos as mudanças que ocorrem no modelo durante esse ajuste fino. Nossas descobertas sugerem que o uso de dados mistos de código que ocorrem naturalmente traz a melhor melhoria de desempenho após o ajuste fino e que o ajuste fino com qualquer tipo de texto misto de código melhora a responsividade de suas cabeças de atenção para entradas de texto misto de código.', 'es': 'Modelos como mBert y XLMR han demostrado tener éxito en la resolución de tareas de PNL con código mixto a pesar de que no estuvieron expuestos a dicho texto durante el entrenamiento previo. Los modelos de PNL con código mixto se han basado en el uso de datos generados sintéticamente junto con datos de origen natural para mejorar su rendimiento. El ajuste fino de mBert en estos datos mejora su rendimiento de código mixto, pero los beneficios de usar los diferentes tipos de datos de código mixto no están claros. En este artículo, estudiamos el impacto del ajuste fino con diferentes tipos de datos de código mezclado y describimos los cambios que se producen en el modelo durante dicho ajuste. Nuestros hallazgos sugieren que el uso de datos mezclados de código natural aporta la mejor mejora del rendimiento después del ajuste fino y que el ajuste con cualquier tipo de texto mezclado de código mejora la capacidad de respuesta de sus cabezas de atención a las entradas de texto con código mixto.', 'zh': '若mBERT、XLMR之类已见代码混NLP成功,虽未接于预练之间。 代码混合NLP依生成之数,自然之数,以崇其性。 凡此数者 mBERT 可以崇代码合性,而用异类者代码合数之利未详也。 论其代码合数而微之,概述其变对模型。 臣等考结果表明,用自然代码混合数,可以入微,可以代码合文,可以重代码应。', 'ja': 'MBERTやXLMRなどのモデルは、事前トレーニング中にそのようなテキストにさらされていなかったにもかかわらず、Code - Mixed NLPタスクを解決することに成功しています。Code - Mixed NLPモデルは、パフォーマンスを向上させるために自然発生データと共に合成生成データを使用することに依存してきました。このようなデータでmBERTを微調整すると、コードミックスされたパフォーマンスが向上しますが、さまざまなタイプのコードミックスされたデータを使用するメリットは明確ではありません。本稿では，異なる種類のコード混合データを用いた微調整の影響を研究し，その微調整中にモデルに生じる変化を概説する．私たちの調査結果は、自然に発生するコード混合データを使用すると、微調整後のパフォーマンスが最高に向上し、あらゆる種類のコード混合テキストを使用して微調整すると、コード混合テキスト入力に対する注意の応答性が向上することを示唆しています。', 'hi': 'MBERT और XLMR जैसे मॉडलों ने कोड-मिश्रित एनएलपी कार्यों को हल करने में सफलता दिखाई है, भले ही वे प्रीट्रेनिंग के दौरान इस तरह के पाठ के संपर्क में नहीं थे। कोड-मिश्रित एनएलपी मॉडल ने अपने प्रदर्शन में सुधार करने के लिए स्वाभाविक रूप से होने वाले डेटा के साथ सिंथेटिक रूप से उत्पन्न डेटा का उपयोग करने पर भरोसा किया है। इस तरह के डेटा पर mBERT को ठीक करने से यह कोड-मिश्रित प्रदर्शन में सुधार करता है, लेकिन विभिन्न प्रकार के कोड-मिश्रित डेटा का उपयोग करने के लाभ स्पष्ट नहीं हैं। इस पेपर में, हम विभिन्न प्रकार के कोड-मिश्रित डेटा के साथ फाइनट्यूनिंग के प्रभाव का अध्ययन करते हैं और इस तरह के महीनीकरण के दौरान मॉडल में होने वाले परिवर्तनों को रेखांकित करते हैं। हमारे निष्कर्ष बताते हैं कि स्वाभाविक रूप से होने वाले कोड-मिश्रित डेटा का उपयोग करने से फाइनट्यूनिंग के बाद सबसे अच्छा प्रदर्शन सुधार होता है और किसी भी प्रकार के कोड-मिश्रित पाठ के साथ फाइनट्यूनिंग कोड-मिश्रित पाठ इनपुट के लिए इसके ध्यान सिर की जिम्मेदारी में सुधार करता है।', 'ru': 'Такие модели, как mBERT и XLMR, показали успех в решении задач Code-Mixed NLP, даже несмотря на то, что они не подвергались воздействию такого текста во время предварительного обучения. Смешанные с кодом модели NLP опирались на использование синтетически сгенерированных данных наряду с данными естественного происхождения для улучшения их производительности. Точная настройка mBERT на таких данных улучшает его кодо-смешанную производительность, но преимущества использования различных типов кодо-смешанных данных не ясны. В этой статье мы изучаем влияние тонкой настройки с различными типами смешанных кодов данных и описываем изменения, которые происходят в модели во время такой тонкой настройки. Наши результаты показывают, что использование встречающихся в природе данных со смешанным кодом приносит наилучшее улучшение производительности после тонкой настройки и что тонкая настройка с любым типом текста со смешанным кодом улучшает реакцию его глав внимания на входы со смешанным кодом.', 'ga': "Tá rath léirithe ag samhlacha ar nós mBERT agus XLMR maidir le tascanna Cód-Mheasctha NLP a réiteach cé nach raibh siad faoi lé téacs den sórt sin le linn réamhoiliúint. Bhraith samhlacha NLP Cód-mheasctha ar úsáid a bhaint as sonraí a ghintear go sintéiseach mar aon le sonraí a fhaightear go nádúrtha chun a bhfeidhmíocht a fheabhsú. Feabhsaítear a fheidhmíocht cód-mheasctha trí mhionchoigeartú mBERT ar shonraí den sórt sin, ach níl na buntáistí a bhaineann le húsáid na gcineálacha éagsúla sonraí Cód-Mheasctha soiléir. Sa pháipéar seo, déanaimid staidéar ar an tionchar a bhíonn ag mionchoigeartú le cineálacha éagsúla sonraí cód-mheasctha agus déanaimid cur síos ar na hathruithe a tharlaíonn ar an tsamhail le linn mionchoigeartaithe den sórt sin. Tugann ár dtorthaí le tuiscint go dtugann úsáid sonraí cód-mheasctha a tharlaíonn go nádúrtha an feabhsú feidhmíochta is fearr isteach tar éis mionchoigeartaithe agus go bhfeabhsaítear freagracht a chinnirí d'ionchur téacs cód-mheasctha trí mhionchoigeartú le haon chineál téacs cód-mheasctha.", 'hu': 'Az olyan modellek, mint az mBERT és az XLMR, sikeresek voltak a Code-Mixed NLP feladatok megoldásában, annak ellenére, hogy az előkészítés során nem voltak kitéve ilyen szövegnek. A kódkeverett NLP modellek a szintetikusan generált adatok és a természetesen előforduló adatok felhasználására támaszkodtak a teljesítményük javítása érdekében. Az mBERT finomítása az ilyen adatokon javítja kódkeverék teljesítményét, de a különböző típusú kódkeverék adatok használatának előnyei nem egyértelműek. Ebben a tanulmányban tanulmányozzuk a finomhangolás hatását különböző típusú kódkeverékes adatokkal, és felvázoljuk a modellben bekövetkező változásokat az ilyen finomhangolás során. Eredményeink azt sugallják, hogy a természetesen előforduló kódkeverékes adatok használata a finomhangolás után a legjobb teljesítmény javulást eredményezi, és hogy a finomhangolás bármilyen típusú kódkeverékes szöveggel javítja figyelmét a kódkeverékes szövegbevitelekre.', 'ka': 'მოდელები, როგორც mBERT და XLMR, გამოიჩვენეთ წარმატება, რომელიც Code-Mixed NLP დამუშაობაში, თუმცა ისინი არ იყო ასეთი ტექსტის გამოყენება საშუალებაში. Name mBERT-ის შესაძლებელება ამ მონაცემებზე იქნება, რომ ეს კოდის შესაძლებელობა, მაგრამ განსხვავებული ტიპების გამოყენება კოდის შესაძლებელობა არ არის წაშლა. ჩვენ ამ დოკუნეში განსხვავებული ტიპებით კოდის შესაბამისი მონაცემების შედეგების შეცვლა, რომლებიც მოდელის შემდეგ მოხდება. ჩვენი შესაძლებლობები გვაქვს, რომ სახელსაწარმოადგილურად გამოიყენება კოდის შემთხვევაში ყველაზე საუკეთესო პროცესტის შესაძლებლობა და რომ კოდის შემთხვევაში ყველა ტიპის კოდის შემთხვევაში ტექსტის შესაძლებლობა შესაძლებლო', 'el': 'Μοντέλα όπως το mBERT και το XLMR έχουν δείξει επιτυχία στην επίλυση εργασιών με μικτό κώδικα παρόλο που δεν εκτίθενται σε τέτοιο κείμενο κατά τη διάρκεια της προεπιλογής. Τα μικτά μοντέλα έχουν βασιστεί στη χρήση συνθετικών δεδομένων μαζί με φυσικά δεδομένα για τη βελτίωση της απόδοσής τους. Η τελειοποίηση σε τέτοια δεδομένα βελτιώνει την απόδοση του με μικτό κώδικα, αλλά τα οφέλη της χρήσης των διαφορετικών τύπων δεδομένων μικτού κώδικα δεν είναι σαφή. Στην παρούσα εργασία, μελετάμε τον αντίκτυπο του συντονισμού με διαφορετικούς τύπους δεδομένων μικτού κώδικα και σκιαγράφουμε τις αλλαγές που συμβαίνουν στο μοντέλο κατά τη διάρκεια αυτού του συντονισμού. Τα ευρήματά μας υποδηλώνουν ότι η χρήση φυσικών δεδομένων με μικτό κώδικα επιφέρει την καλύτερη βελτίωση της απόδοσης μετά τον συντονισμό και ότι ο συντονισμός με οποιοδήποτε τύπο κειμένου με μικτό κώδικα βελτιώνει την ανταπόκριση των επιστημόνων προσοχής του στις εισαγωγές κειμένου με μικτό κώδικα.', 'it': "Modelli come mBERT e XLMR hanno dimostrato successo nella risoluzione di attività NLP Code-Mixed anche se non sono stati esposti a tale testo durante il pretraining. I modelli NLP Code-Mixed hanno fatto affidamento sull'utilizzo di dati generati sinteticamente insieme a dati naturali per migliorare le loro prestazioni. Finetuning mBERT su tali dati migliora le sue prestazioni code-mixed, ma i vantaggi dell'utilizzo dei diversi tipi di dati Code-Mixed non sono chiari. In questo articolo, studiamo l'impatto della finetuning con diversi tipi di dati code-mixed e delineamo i cambiamenti che si verificano al modello durante tale finetuning. I nostri risultati suggeriscono che l'utilizzo di dati misti di codice presenti in natura apporta il miglior miglioramento delle prestazioni dopo la messa a punto e che la messa a punto con qualsiasi tipo di testo misto di codice migliora la reattività delle sue teste di attenzione agli input di testo misti di codice.", 'mk': 'Моделите како mBERT и XLMR покажаа успех во решавањето на задачите на NLP мешани со код иако не беа изложени на ваков текст за време на претренирање. Моделите на NLP мешани со код се потпираа на користење на синтетички генерирани податоци заедно со природни податоци за подобрување на нивната перформанса. Завршувањето на mBERT на вакви податоци ја подобрува својата комбинирана перформанса, но бенефициите од користењето на различните типови на комбинирани податоци не се јасни. Во овој весник го проучуваме влијанието на финетизирањето со различни типови на кодови мешани податоци и ги опишуваме промените кои се случуваат на моделот за време на ваквото финетизирање. Нашите откритија укажуваат на тоа дека употребата на природно појавени кодови-мешани податоци доведе до најдобро подобрување на перформансата по финетизирање и дека финетизирањето со било кој вид на кодови-мешан текст ја подобрува реакционалноста на нејзиното внимание на кодовите-мешаните текстови в', 'kk': 'mBERT және XLMR секілді үлгілер Code- Mixed NLP тапсырмаларын шешуде сәтті көрсетілді, бірақ олар бұл мәтініне тәртіпсіздік кезде тәртіпсіздік жоқ болмаса да. Код араластырылған NLP үлгілері синтетикалық құрылған деректерді қолдану үшін оның жылдамдығын жасау үшін табиғатты деректерді қолданады. Бұл деректерге mBERT дегенді табу оның код араластырылған әрекетін жақсартады, бірақ басқа түрлері Код араластырылған деректерді қолдану мүмкіндігі тазаламайды. Бұл қағазда, біз басқа түрлермен код араластырылған деректермен біріктіру нәтижесін зерттейміз және бұл түрлерде моделінің өзгерістерін таңдаймыз. Біздің табылған мәліметтеріміз, тәуелді код араластырылған деректерді қолданудың соңында жақсы жылдамдығын жақсарту және код араластырылған мәтіннің түрлері жақсы түрлері код араластырылған мәтіннің бағыттарының жауапшылығын жасайды.', 'ml': 'എംബെര്\u200dട്ടിയും എക്സ്\u200cഎല്\u200dഎംആരും പോലുള്ള മോഡലുകള്\u200d വിജയകരമായി കോഡ്- മിക്സിഡ് NLP ജോലികള്\u200d പരിഹരിക്കുന്നതില്\u200d വിജയം കാണിച് കോഡ്- Mixed NLP models have relied on synthetically generated data along with naturally occurring data to improve their performance. ഇതുപോലുള്ള ഡേറ്റായി mBERT കണ്ടുപിടിക്കുന്നത് കോഡ്- mixed പ്രവര്\u200dത്തനം മെച്ചപ്പെടുത്തുന്നതാണ്. പക്ഷെ വ്യത്യസ്ത തരം കോഡ്- മിക്സ ഈ പത്രത്തില്\u200d നമ്മള്\u200d വ്യത്യസ്ത തരത്തിലുള്ള കോഡ് കലര്\u200dത്തിയ വിവരങ്ങളുമായി ഫിന്തൂട്ട് ചെയ്യുന്നതിന്റെ പ്രഭാവം പഠിക്കു നമ്മുടെ കണ്ടുപിടികള്\u200d പറയുന്നത് സ്വാഭാവികമായി സംഭവിക്കുന്ന കോഡ് മിഷ്ടപ്പെട്ട ഡേറ്റ ഉപയോഗിക്കുന്നതിനു ശേഷം മികച്ച പ്രവര്\u200dത്തനങ്ങള്\u200d മെച്ചപ്പെടുത്തുന്നത', 'lt': 'Pavyzdžiui, mBERT ir XLMR pavyzdys sėkmingai išspręsė su kodais susietos NLP užduotis, nors prieš mokymą jie nebuvo paveikti tokiu tekstu. Kodais mišrūs NLP modeliai remiasi sintetiniais duomenimis ir natūraliai atsirandančiais duomenimis, kad pagerintų jų veiksmingumą. Tokių duomenų mBERT nustatymas pagerina su kodais susijusius veiksnius, tačiau nauda naudojant įvairius kodais susijusius duomenis nėra aiški. In this paper, we study the impact of finetuning with different types of code-mixed data and outline the changes that occur to the model during such finetuning.  Mūs ų išvados rodo, kad naudojant natūraliai susidarančius kodų mišinius duomenis geriausiai pagerinamas rezultatas po tobulinimo ir kad tobulinimas bet kokiu kodų mišinio teksto tipu pagerina jo dėmesio galvų atsakomybę į kodų mišinio teksto įvedimus.', 'no': 'Modellar som mBERT og XLMR har vist suksess i løysing av NLP- oppgåver med kodeflikt, selv om dei ikkje vart eksponerte til slik tekst under trekking. NLP-modeller med kodeflikt har relied på bruk av syntetisk genererte data saman med naturleg data for å forbetra utviklinga. Finn mBERT på slike data forbetrar at det er mellom kode, men fordel til å bruka dei ulike typane kode-mellom data er ikkje klart. I denne papiret studerer vi effekten av finetuning med ulike typar kodeflikte data og omriserer endringane som skjer i modellen under slike finetuning. Finningane våra tyder på at ved bruk av naturlig mellom kode-data fører til den beste forbetringa av utviklinga etter finetuning og at finetuning med alle typar kode-mellom tekst forbetrar den oppmerksomhetskopten til kode-mellom tekstinndata.', 'mn': 'МБЕРТ болон XLMR шиг загварууд Code-Mixed NLP ажлыг шийдэхэд амжилттай харуулж байна. Гэхдээ тэд ийм бичгийг суурилуулах үед харагдахгүй байсан ч гэсэн. Код-холбогдсон NLP загварууд өөрсдийн үйл ажиллагааг сайжруулахын тулд синтетик үүсгэсэн өгөгдлийг ашиглаж байдаг. МБЕРТ-г ийм өгөгдлийн талаар олох нь кодын төвөгтэй үйл ажиллагааг сайжруулдаг. Гэхдээ өөр төрлийн Код-Mixed өгөгдлийн хэрэглэх хэрэгцээ нь тодорхой биш. Энэ цаасан дээр бид өөр төрлийн код холбогдсон өгөгдлийн нөлөөг судалж, загварын өөрчлөлтүүд ийм сайхан холбогдох үед болдог. Бидний ололтууд нь байгалийн кодын төвөгтэй мэдээллийг ашиглах нь хамгийн сайн үйл ажиллагааны сайхан сайхан хөгжүүлэлт гаргаж ирдэг ба кодын төвөгтэй бүтээгдэхүүний хариу үйл ажиллагааг кодын төвөгтэй мөн өгүүлэлтийн дараа сайхан хөгжүүлэх боломжтой бол', 'ms': 'Model seperti mBERT dan XLMR telah menunjukkan sukses dalam memecahkan tugas NLP Kabur-Kod walaupun mereka tidak terkena teks semasa latihan. Model NLP Kabur-Kod telah bergantung pada menggunakan data yang dijana secara sintetik bersama dengan data yang berlaku secara alami untuk meningkatkan prestasi mereka. Menyelesaikan mBERT pada data seperti itu memperbaiki prestasi kod-campuran, tetapi keuntungan menggunakan jenis berbeza data Kod-campuran tidak jelas. Dalam kertas ini, kami mempelajari kesan penyesuaian dengan jenis-jenis data campuran-kod yang berbeza dan bincangkan perubahan yang berlaku pada model semasa penyesuaian tersebut. Penemuan kami menunjukkan bahawa menggunakan data campuran-kod yang berlaku secara alami membawa perkembangan prestasi terbaik selepas penyesuaian dan penyesuaian dengan mana-mana jenis teks campuran-kod meningkatkan responsiviti kepala perhatian kepada input teks campuran-kod.', 'pl': 'Modele takie jak mBERT i XLMR wykazały sukces w rozwiązywaniu zadań kodowo-mieszanych NLP, mimo że nie były one narażone na taki tekst podczas treningu wstępnego. Model NLP z kodem mieszanym polegał na wykorzystaniu syntetycznie generowanych danych wraz z naturalnie występującymi danymi w celu poprawy ich wydajności. Finetuning mBERT na takich danych poprawia jego wydajność w zakresie kodu mieszanego, ale korzyści płynące z korzystania z różnych typów danych Code-Mixed nie są jasne. W niniejszym artykule badamy wpływ finetuningu za pomocą różnych rodzajów danych mieszanych kodem i zarysujemy zmiany, jakie zachodzą w modelu podczas takiego finetuningu. Nasze ustalenia sugerują, że wykorzystanie naturalnie występujących danych mieszanych kodem przynosi najlepszą poprawę wydajności po dopracowaniu i że dopracowanie dowolnego rodzaju tekstu mieszanego kodem poprawia reakcję głowy uwagi na wejścia tekstu mieszanego kodem.', 'ro': 'Modele precum mBERT și XLMR au arătat succes în rezolvarea sarcinilor NLP cod-mixte, chiar dacă nu au fost expuse la un astfel de text în timpul pregătirii. Modelele NLP combinate cu cod s-au bazat pe utilizarea datelor generate sintetic împreună cu datele naturale pentru a le îmbunătăți performanța. Finetuning mBERT pe astfel de date îmbunătățește performanța mixtă de coduri, dar beneficiile utilizării diferitelor tipuri de date combinate de coduri nu sunt clare. În această lucrare, studiem impactul fintuning-ului cu diferite tipuri de date amestecate de cod și conturăm modificările care apar modelului în timpul unei astfel de fintuning. Rezultatele noastre sugerează că utilizarea datelor amestecate de coduri naturale aduce cea mai bună îmbunătățire a performanțelor după finetuning și că fintuning cu orice tip de text amestecat de coduri îmbunătățește răspunderea capurilor de atenție la intrarea textului amestecat de coduri.', 'si': 'මොඩල් වගේ mBERT සහ XLMR වගේ සාර්ථකය පෙන්වන්න පුළුවන් කෝඩ් මික්ස් NLP වැඩක් විස්තර කරනවා නමුත් ඔවුන් එහෙම පාළුවට ප්\u200d Name මෙහෙම දත්තේ සඳහා mBERT එක හොයාගන්න ඒක කෝඩ් මික්ස් කරන්න පුළුවන් වෙනවා, නමුත් වෙනස් වර්ගයක් කෝඩ් මික්ස් දත්ත ප්\u200dරයෝ මේ පත්තරේ අපි පරීක්ෂණය කරන්නේ වෙනස් වර්ගයක් කෝඩ් මිශ්රීත දත්ත සමග වෙනස් වර්ගයක් සඳහා ප්\u200dරශ්නයක් පරීක්ෂණය කරනවා ඒ අපේ හොයාගන්න පුළුවන් විදිහට ස්වභාවිතයෙන් ප්\u200dරවේශනය වෙනුවෙන් කෝඩ් මික්ස් දත්ත භාවිතා කරන්න හොඳම ප්\u200dරවේශනයක් ප්\u200dරවේශනය කරනවා සහ කෝඩ් මික්ස් පා', 'sr': 'Modeli poput mBERT i XLMR pokazali su uspjeh u rešavanju zadataka NLP-mišenih kodova iako nisu izloženi takvom tekstu tokom pretkivanja. Modeli NLP-mišenih kodova se oslanjaju na upotrebu sintetièki generiranih podataka zajedno sa prirodnim podacima kako bi poboljšali njihovu funkciju. Nabavljenje mBERT na takvim podacima poboljšava činjenicu kombiniranog koda, ali koristi od koristi različitih vrsta podataka o kombiniranju koda nisu jasni. U ovom papiru proučavamo uticaj finetuniranja sa različitim vrstama izmešanih podataka i navodimo promene koje se dešavaju modelu tokom takvog finetuniranja. Naši nalazi sugeriraju da korištenje prirodno pojavljujućih podataka između koda donosi najbolje poboljšanje učinka nakon finetuniranja i da finetuniranje sa bilo kakvom vrstom teksta između koda poboljšava odgovornost njegovih glava pažnje na ulazak teksta između koda.', 'so': 'Modelooyinka tusaale ahaan mBERT iyo XLMR waxay liibaansadeen in ay xajistaan shaqaalaha cod-Mixed NLP xittaa in kastoo uusan loo muujin qoraalkaas oo kale xittaa marka lagu soo daayay. Tusaalada NLP ee Code-Mixed waxay ku xiran yihiin isticmaalka isticmaalka macluumaadka la soo saaray si ay u hagaajiyaan performance. Finnishka mBERT ee macluumaadkaas waxaa hagaajiya muuqashada codsiga, laakiin faa’iidada isticmaalka macluumaadka kala duduwan ee kooxda isku xiran ma cadayn. In this paper, we study the impact of finetuning with different types of code-mixed data and outline the changes that occur to the model during such finetuning.  Shaqooyinkayada waxaa loola jeedaa in isticmaalka macluumaadka caadiga ah ee isku xiran uu sameeyo bedelka horumarinta shaqada ee ugu wanaagsan marka la dhalay dhalashada, iyo in la kordhiyo qoraal cayn kasta oo la isku xiriiray uu kordhiyo responsiga loo eego madaxa qoraalka la isku daray.', 'sv': 'Modeller som mBERT och XLMR har visat framgång när det gäller att lösa kodblandade NLP-uppgifter även om de inte utsattes för sådan text under förbehandlingen. Kodblandade NLP-modeller har förlitat sig på att använda syntetiskt genererade data tillsammans med naturligt förekommande data för att förbättra deras prestanda. Finetuning av mBERT på sådana data förbättrar dess kodblandade prestanda, men fördelarna med att använda de olika typerna av kodblandade data är inte tydliga. I denna uppsats studerar vi effekten av finjustering med olika typer av kodblandad data och skisserar de förändringar som uppstår i modellen vid sådan finjustering. Våra resultat tyder på att användningen av naturligt förekommande kodblandade data ger den bästa prestandaförbättringen efter finjustering och att finjustering med alla typer av kodblandad text förbättrar responsiviteten hos dess uppmärksamhet huvuden för kodblandade textinmatningar.', 'ur': 'Models such as mBERT and XLMR have shown success in solving code-Mixed NLP tasks even though they were not exposed to such text during pretraining. Code-Mixed NLP models have relied on using synthetically generated data along with naturally occurring data to improve their performance. اس طرح کے ڈیٹا پر mBERT کو فنِٹونگ کرنا اسے کڈ میکس کرنا ہے، لیکن کڈ میکس ڈیٹا کے مختلف طریقوں کے استعمال کے فائدہ پاک نہیں ہیں. اس کاغذ میں ہم مختلف قسموں کے کڈ میکس ڈیٹا کے ساتھ فائدہ پیدا کرنے کے تاثیر کو پڑھتے ہیں اور اس طرح کے مطابق موڈل کے ذریعہ تغییرات کو ظاہر کرتے ہیں۔ ہمارے نتیجے کی نشانی دیتے ہیں کہ طبیعی طور پر قائم ہونے والی کوڈ میکس ڈیٹے کے استعمال سے بہترین کاروباری اضافہ ہونے کے بعد اور یہ کہ کوڈ میکس ٹیکس کے ساتھ اضافہ ہونے کے بعد کوڈ میکس ڈیکس ڈیکس کو اضافہ کرتا ہے۔', 'ta': 'MBERT மற்றும் XLMR போன்ற மாதிரிகள் வெற்றிகரமாக குறியீடு- கலக்கப்பட்ட NLP பணிகள் தீர்வு செய்யும் போதும் மாதிரிகள் முன்னோக் குறியீடு- கலக்கப்பட்ட NLP மாதிரிகள் சார்ந்து செயல்பாட்டை மேம்படுத்த இயல்பான நடக்கும் தகவலுடன் தொடர்ந்து கொண்டு கூடி இந்த தரவில் mBERT கண்டுபிடித்தல் அது குறியீடு- கலப்பு செயல்பாட்டை மேம்படுத்துகிறது, ஆனால் வேறு வகையான குறியீடு- கலந்த தரவை பயன் இந்த காகிதத்தில், நாம் வேறு வகையான குறியீடு கலந்த தரவுடன் பின்தூட்டுதலின் விளைவுகளை படிக்கிறோம் மற்றும் இந்த முடிவில் ம எங்கள் கண்டுபிடிப்புகள் குறியீடு கலந்த தகவல்களை பயன்படுத்தி சிறந்த செயல் முன்னேற்றத்தை கொண்டு வருகிறது என்பது தெரிவிக்கிறது மற்றும் அது எந்த வகையான குறியீடு கல', 'mt': "Models such as mBERT and XLMR have shown success in solving Code-Mixed NLP tasks even though they were not exposed to such text during pretraining.  Il-mudelli NLP imħallta bil-kodiċi ddependew fuq l-użu ta’ dejta ġġenerata b’mod sintetiku flimkien ma’ dejta li sseħħ b’mod naturali biex itejbu l-prestazzjoni tagħhom. Finetuning mBERT on such data improves it's code-mixed performance, but the benefits of using the different types of Code-Mixed data aren't clear.  F’dan id-dokument, nistudjaw l-impatt tal-irfinar b’tipi differenti ta’ dejta mħallta bil-kodiċi u nippreżentaw il-bidliet li jseħħu fil-mudell matul tali irfinar. Our findings suggest that using naturally occurring code-mixed data brings in the best performance improvement after finetuning and that finetuning with any type of code-mixed text improves the responsivity of it's attention heads to code-mixed text inputs.", 'uz': "Name Name @ info Bu qogʻozda, biz turli kodlash turlari bilan ishlash natijasini o'rganamiz va bu yerda modeldagi o'zgarishlarni ko'rinamiz. Bizning murojaatlarimiz aytganimiz, oddiy qanday mix qilingan maʼlumot yordamida bir yaxshi bajarish imkoniyatini bajarishi mumkin va bir necha qanday qanday mix matnni oshirish imkoniyatini ko'paytirish imkoniyatini oshirish mumkin.", 'vi': 'Những mẫu như mBERT và XLMR đã cho thấy thành công trong việc giải quyết mã trộn độc lập NLP, mặc dù chúng không bị phơi nhiễm khi sản xuất trước. Các mô hình lập lập mã gien độc lập lập lập tư liệu sản xuất tổng hợp cùng với các dữ liệu hiện tại tự nhiên để nâng cao khả năng của chúng. Tìm hiểu mBERT trên dữ liệu này cải thiện khả năng phân phối mã của nó, nhưng lợi ích của việc s ử dụng các loại dữ liệu khác nhau không rõ. Trong tờ giấy này, chúng tôi nghiên cứu tác động của việc tinh chỉnh với các loại dữ liệu tổng hợp mã khác nhau khác nhau và mô tả những thay đổi xảy ra với mô hình trong thời gian hoàn chỉnh. Những kết quả của chúng tôi cho thấy việc s ử dụng các dữ liệu trộn code tự nhiên mang lại hiệu quả tốt nhất sau khi tinh chỉnh độ chín muồi và độ cẩn thận với bất kỳ kiểu văn bản mã trộn lẫn nhau tăng hàm lượng nhận thức của nó về các nội dung đoạn văn bản.', 'bg': 'Модели като mBERT и XLMR показват успех при решаването на задачи с кодово смесени НЛП, въпреки че не са били изложени на такъв текст по време на предтренирането. Моделите, смесени с кодове, разчитат на използването на синтетично генерирани данни заедно с естествени данни, за да подобрят ефективността си. Финетунирането на такива данни подобрява ефективността на смесените кодове данни, но ползите от използването на различните видове смесени кодове данни не са ясни. В настоящата статия изследваме въздействието на финото настройване с различни видове кодово смесени данни и очертаваме промените, които настъпват в модела по време на такова фино настройване. Нашите констатации предполагат, че използването на естествено срещащи се кодово смесени данни води до най-добро подобрение на производителността след фино настройване и че финото настройване с всякакъв вид кодово смесен текст подобрява отзивчивостта на вниманието му към кодово смесени текстови входове.', 'nl': 'Modellen zoals mBERT en XLMR hebben succes getoond in het oplossen van Code-Mixed NLP taken, hoewel ze niet werden blootgesteld aan dergelijke tekst tijdens pretraining. Code-Mixed NLP-modellen hebben vertrouwd op het gebruik van synthetisch gegenereerde gegevens samen met natuurlijk voorkomende gegevens om hun prestaties te verbeteren. Het finetunen van mBERT op dergelijke gegevens verbetert de code-mixed prestaties, maar de voordelen van het gebruik van de verschillende typen code-mixed data zijn niet duidelijk. In dit artikel bestuderen we de impact van finetuning met verschillende soorten code-mixed data en schetsen we de veranderingen die optreden aan het model tijdens dergelijke finetuning. Onze bevindingen suggereren dat het gebruik van natuurlijk voorkomende code-gemengde gegevens de beste prestatieverbetering oplevert na finetuning en dat finetuning met elk type code-gemengde tekst de responsiviteit van de aandachtskoppen voor code-gemengde tekst verbetert.', 'de': 'Modelle wie mBERT und XLMR haben sich bei der Lösung von Code-Mixed NLP-Aufgaben bewährt, obwohl sie während des Vortrainings keinem solchen Text ausgesetzt waren. Code-Mixed NLP-Modelle haben sich darauf verlassen, synthetisch generierte Daten zusammen mit natürlich vorkommenden Daten zu verwenden, um ihre Leistung zu verbessern. Die Feinabstimmung von mBERT auf solchen Daten verbessert seine Code-Mixed-Leistung, aber die Vorteile der Verwendung der verschiedenen Arten von Code-Mixed-Daten sind nicht klar. In diesem Beitrag untersuchen wir die Auswirkungen von Feinabstimmung mit verschiedenen Arten von Code-Mixed-Daten und skizzieren die Änderungen, die während einer solchen Feinabstimmung am Modell auftreten. Unsere Ergebnisse deuten darauf hin, dass die Verwendung von natürlich vorkommenden Code-Mixed-Daten die beste Leistungsverbesserung nach der Feinabstimmung bringt und dass die Feinabstimmung mit jeder Art von Code-Mixed-Text die Reaktionsfähigkeit der Aufmerksamkeit auf Code-Mixed-Texteingaben verbessert.', 'da': 'Modeller som mBERT og XLMR har vist succes med at løse Code-Mixed NLP opgaver, selvom de ikke blev udsat for sådan tekst under foruddannelse. Kodeblandede NLP modeller har stolet på at bruge syntetisk genererede data sammen med naturligt forekommende data for at forbedre deres ydeevne. Finetuning af mBERT på sådanne data forbedrer dens kodeblandede ydeevne, men fordelene ved at bruge de forskellige typer kodeblandede data er ikke klare. I denne artikel undersøger vi effekten af finjustering med forskellige typer kodeblandede data og skitserer de ændringer, der opstår i modellen under en sådan finjustering. Vores resultater tyder på, at anvendelse af naturligt forekommende kodeblandede data giver den bedste ydeevne forbedring efter finjustering, og at finjustering med enhver type kodeblandet tekst forbedrer responsiviteten af dens opmærksomhed hoveder til kodeblandede tekstinput.', 'hr': 'Modeli poput mBERT i XLMR pokazali su uspjeh u rješavanju zadataka NLP-pomiješanih kod iako nisu izloženi takvom tekstu tijekom pretkivanja. Modeli NLP mješane kodovima oslanjaju se na upotrebu sintetički proizvedenih podataka zajedno s prirodno pojavljivim podacima kako bi poboljšali njihovu učinku. Nabavljenje mBERT na takvim podacima poboljšava učinkovitost mješane kod, ali koristi od koristi različitih vrsta podataka mješane kod nisu jasne. U ovom papiru proučavamo učinak finetuniranja s različitim vrstama podataka pomiješanih kod i navodimo promjene koje se događaju modelu tijekom takvog finetuniranja. Naši nalazi sugeriraju da korištenje prirodno pojavljujućih podataka pomiješanih kod donosi najbolje poboljšanje učinka nakon finetuniranja i da finetuniranje sa bilo kojim vrstom teksta pomiješanog kodom poboljšava odgovornost njegovih glava pažnje na ulaze kod-pomiješanog teksta.', 'id': 'Models such as mBERT and XLMR have shown success in solving Code-Mixed NLP tasks even though they were not exposed to such text during pretraining.  Model NLP Kode-Mixed telah bergantung pada menggunakan data yang dihasilkan secara sintetis bersama dengan data yang terjadi secara alami untuk meningkatkan prestasi mereka. Menyelesaikan mBERT pada data seperti itu meningkatkan prestasi kode-campuran, tapi keuntungan dari menggunakan tipe berbeda data Kode-campuran tidak jelas. Dalam kertas ini, kami mempelajari dampak penelitian dengan tipe-tipe berbeda kode-campuran data dan menjelaskan perubahan yang terjadi pada model selama penelitian tersebut. Penemuan kami menunjukkan bahwa menggunakan data campuran kode yang terjadi secara alami membawa perkembangan prestasi terbaik setelah penentuan dan penentuan dengan setiap jenis teks campuran kode meningkatkan responsivitas kepala perhatian ke masukan teks campuran kode.', 'sw': 'Modeli kama mBERT na XLMR wameonyesha mafanikio katika kutatua kazi za NLP zilizochanganyika na Mipaka ya NLP hata kama hawakuonyeshwa kwa maandishi kama haya wakati wa kutengeneza matumizi. Mradi wa NLP ulioanganishwa na sheria wametegemea kwa kutumia taarifa zilizotengenezwa kwa pamoja na data zinazotokea kwa asili ili kuboresha utendaji wao. Kutafuta mBERT kwenye takwimu kama hizi inaboresha utendaji wa kodi unachanganyika, lakini faida za kutumia aina tofauti za taarifa za Miungano ya Mipaka hazina wazi. Katika gazeti hili, tunasoma madhara ya kutengeneza faini kwa aina mbalimbali za taarifa tofauti za kodi na kuonyesha mabadiliko yanayotokea kwa muundo huo wakati wa kutoa faini hiyo. Matokeo yetu yanapendekeza kwamba kwa kutumia taarifa zilizochanganyika kwa asili zinaleta maboresho bora ya utendaji baada ya kupunguza mafanikio na kwamba kupendeleza kwa aina yoyote ya ujumbe wa mfumo unaohusishwa inaongezea wajibu wake wa watu wake kwenye vipindi vya ujumbe wa ujumbe ulioanganishwa.', 'ko': 'mBERT와 XLMR 등 모델은 코드가 혼합된 NLP 임무를 성공적으로 해결했다. 비록 훈련 전에 이런 텍스트를 접하지 못했지만.코드 블렌드 NLP 모델은 작성된 데이터와 자연 발생된 데이터를 사용하여 성능을 향상시키는 데 의존합니다.이런 데이터에 대해 미세하게 조정하면 코드 혼합 성능을 높일 수 있으나, 서로 다른 유형의 코드 혼합 데이터를 사용하는 장점은 아직 명확하지 않다.본고에서 우리는 서로 다른 유형의 코드 혼합 데이터를 사용하여 마이크로스피커의 영향을 연구했고 이런 마이크로스피커 과정에서 모델이 발생하는 변화를 개괄했다.우리의 연구 결과에 따르면 자연적으로 발생하는 코드 혼합 데이터를 사용하면 마이크로스피커를 조정한 후에 가장 좋은 성능 개선을 가져왔고 어떤 종류의 코드 혼합 텍스트 마이크로스피커를 사용해도 코드 혼합 텍스트 입력에 대한 주의력을 높일 수 있다.', 'tr': 'mBERT we XLMR ýaly nusgalar Code-Mixed NLP täzeliklerini çözmek üçin başarnygy görkezildi. Ýagyrlamak wagtynda onuň ýaly metine görünmedikleri üçin hem. Kod Karışmış NLP modelleri öz etkinleşigini geliştirmek üçin syntetik üretilen maglumatlary ulanmakda ynamly boldy. Bu ýaly maglumatlarda mBERT öňlemek üçin munyň kodyň karmaşgalan etmäniň gowydyrýar, ýöne farklı hillerde Ködler karmaşgalan maglumatlaryň ýerini bozmaýar. Bu kagyzda biz farklı köd karışık maglumaty bilen finetmäniň täsirini öwrenýärdik we bu şekilde finetmäniň modinde bolan üýtgewleri çykar. Biziň tapylyklarymyz tebigly còd karışylykly maglumatlary ulanmak fin birleşmeginden soň iň gowy etkinlik gelişmesini saýlaýar we her hili còd karışylykly metin bilen geňleşmeginiň kellesiniň jogaplaryny gowurar.', 'fa': 'مدل\u200cهای مانند mBERT و XLMR موفقیت را در حل کار NLP متصل کد نشان داده\u200cاند، حتی با وجود اینکه آنها در طول تغییر کردن به این متن نشان نداده\u200cاند. Models NLP Mixed Code relied on using synthetically generated data along with naturally occurring data to improve their performance. پیدا کردن mBERT روی چنین داده ها فعالیت پیوند کد را بهتر می\u200cکند، ولی سودهای استفاده از نوع\u200cهای داده\u200cهای پیوند\u200cپیوند کد پاک نیستند. در این کاغذ، ما تأثیر فناوری با نوع داده های متفاوت کد را مطالعه می کنیم و تغییرات را که در طول چنین فناوری به مدل اتفاق می آورند، روشن می کنیم. نتیجه\u200cهایمان پیشنهاد می\u200cدهند که با استفاده از داده\u200cهای مختلف قانون طبیعی پیدا می\u200cشود، بهترین عملکرد را بعد از فن\u200cترکیب می\u200cکند، و این پاکیزه\u200cای با هر نوع متن مختلف قانون قانون، پاسخ\u200cگیری سرهای توجه\u200cاش به ورودهای متن مختلف قانون پیدا می\u200cکند.', 'sq': "Modelet si mBERT dhe XLMR kanë treguar sukses në zgjidhjen e detyrave NLP të përziera me kod edhe pse nuk janë ekspozuar për tekst të tillë gjatë parastërvitjes. Modelet NLP të përziera me kod janë mbështetur në përdorimin e të dhënave sintetikisht të gjeneratuara së bashku me të dhënat që ndodhin natyralisht për të përmirësuar performancën e tyre. Finetuning mBERT on such data improves it's code-mixed performance, but the benefits of using the different types of Code-Mixed data aren't clear.  Në këtë letër, ne studiojmë ndikimin e përmirësimit me lloje të ndryshme të të dhënave të përziera me kode dhe përshkruajmë ndryshimet që ndodhin në modelin gjatë përmirësimit të tillë. Zbulimet tona sugjerojnë se përdorimi i të dhënave të përziera me kod që ndodhin natyralisht sjell përmirësimin më të mirë të performancës pas përmirësimit dhe se përmirësimi me çdo lloj teksti të përzier me kod përmirëson përgjigjës in ë e kokave të vëmendjes s ë saj ndaj hyrjeve të tekstit të përzier me kod.", 'am': 'እንደ mBERT እና XLMR የኮድ-Mixed NLP ስራዎችን በመፍታት ሞዴል አግኝተዋል፡፡ Code-Mixed NLP models have relied on synthetically generated data along with naturally occurring data to improve their performance. አዲስ ዶሴ ፍጠር በዚህ ፕሮግራም፣ በተለያዩ ዓይነቶች የኮድ-መቀላቀል ዳታዎችን በመፍጠር እናስተምራለን፡፡ ፍጥረታችን በተለየ የኮድ-የተቀላቀለ ዳታዎችን በመጠቀም ከፍጥረት በኋላ የተሻለ የድምፅ ውጤት ማድረግ እና የኮድ-የተቀራረበ ጽሑፎችን ማቀናቀል የጽሑፍ አየር አየር የጽሑፎችን አካባቢ ማድረግ ያበረታታል፡፡', 'af': 'Models soos mBERT en XLMR het sukses gewys in die oplossing van kode- gemengde NLP opdragte selfs al is hulle nie aan sodanige teks voorgeskryf nie tydens voorrekening nie. Name Fineing mBERT op sulke data verbeter dit se kode gemengde prestasie, maar die voordele van die verskillende tipes van kode gemengde data is nie duidelik nie. In hierdie papier, ondersoek ons die effek van finetuning met verskillende tipes kode gemengde data en uitlyn die veranderinge wat aan die model voorkom tydens sodanige finetuning. Ons gevinde beveel dat die gebruik van natuurlik kode gemengde data in die beste prestasie verbetering na finetuning en dat finetuning met enige tipe kode gemengde teks verbeter die verantwoordelikheid van dit se aandagkoppe na kode gemengde teks inputs.', 'az': "MBERT v…ô XLMR kimi modell…ôr c√≤d-karńĪŇüńĪqlńĪ NLP iŇül…ôrini √ß…ôkm…ôkd…ô baŇüarńĪlńĪ g√∂st…ôrdil…ôr. Halbuki onlar bu m…ôktublara …ôvv…ôlc…ô g√∂st…ôrilm…ôdil…ôr. Kod-KarńĪŇüńĪqlńĪ NLP modell…ôri sintetik √ľr…ôkl…ônmiŇü m…ôlumatlarńĪ istifad…ô etm…ôk √ľ√ß√ľn t…ôbi…ôtl…ô g…ôl…ôn m…ôlumatlarńĪ il…ô birlikd…ô istifad…ô etdil…ôr. B√ľt √ľn m…ôlumatlarda mBERT'u tapmaq onun kodu karńĪŇütńĪrńĪlmńĪŇü performansńĪnńĪ yaxŇüńĪlaŇüdńĪrńĪr, amma c√≤d karńĪŇütńĪrńĪlmńĪŇü m…ôlumatlarńĪn m√ľxt…ôlif t√ľrl…ôrini istifad…ô etm…ôk faydalarńĪ a√ßńĪq deyildir. Bu kańüńĪzda, biz m√ľxt…ôlif c√≤d karńĪŇütńĪrńĪlmńĪŇü veril…ôr il…ô finetuning t…ôsirini √∂yr…ônirik v…ô modell…ô b√∂y√ľk finetuning sńĪrasńĪnda g…ôl…ôn d…ôyiŇüiklikl…ôri g√∂st…ôririk. Bizim tapńĪndńĪqlarńĪmńĪz t…ôbi…ôtli c√≤d karńĪŇütńĪrńĪlmńĪŇü veril…ôr istifad…ô etm…ôk √ľ√ß√ľn finetuning sonrasńĪnda …ôn yaxŇüńĪ performans yaxŇüńĪlńĪqlarńĪnńĪ g√∂st…ôrir v…ô h…ôr c√≤d karńĪŇütńĪrńĪlmńĪŇü metinl…ôrl…ô yaxŇüńĪlńĪq etm…ôk kodu baŇülńĪqlarńĪnńĪn sorumluluńüunu kodu karńĪŇütńĪrńĪlmńĪŇü metinl…ôr…ô yaxŇüńĪlaŇüdńĪrńĪr.", 'hy': 'Models such as mBERT and XLMR have shown success in solving Code-Mixed NLP tasks even though they were not exposed to such text during pretraining.  Կոդի խառնված ՆԼՊ մոդելները հիմնված են սինթետիկ կերպով ստեղծված տվյալների օգտագործման վրա, ինչպես նաև բնական տեղի ունեցող տվյալների վրա, որպեսզի բարելավեն իրենց արտադրողությունը: Այսպիսի տվյալների mBER-ի վերլուծումը բարելավում է կոդի խառնված արտադրողությունը, բայց կոդի խառնված տվյալների տարբեր տեսակների օգտագործման առավելությունը պարզ չէ: Այս թղթի մեջ մենք ուսումնասիրում ենք տարբեր տեսակի կոդի խառնված տվյալների օգնությամբ փոփոխությունների ազդեցությունը և ներկայացնում ենք մոդելի փոփոխությունները, որոնք տեղի են ունենում այդ փոփոխությունների ընթացքում: Մեր հայտնաբերությունները ցույց են տալիս, որ բնական կոդի խառնված տվյալների օգտագործումը լավագույն արդյունավետության զարգացման արդյունավետությունն է բերում փոքրացումից հետո, և որ փոքրացումը կոդի խառնված տեքստի ցանկացած տեսակի միջոցով բարելավում է ուշադրության գլխավոր հաղո', 'bs': 'Modeli poput mBERT i XLMR pokazali su uspjeh u rješavanju zadataka NLP-miješanih kod iako nisu izloženi takvom tekstu tijekom pretkivanja. Modeli NLP-miješanih kodova oslanjaju se na upotrebu sintetički proizvedenih podataka zajedno sa prirodnim podacima kako bi poboljšali njihovu učinku. Nabavljenje mBERT na takvim podacima poboljšava učinkovitost mešanog koda, ali koristi od koristi različitih vrsta podataka mešanog koda nisu jasni. U ovom papiru, proučavamo učinak finetuniranja sa različitim vrstama podataka pomiješanih kod i navodimo promjene koje se događaju modelu tijekom takvog finetuniranja. Naši nalazi sugeriraju da korištenje prirodno pojavljujućih podataka pomiješanih kod donosi najbolje poboljšanje učinka nakon finetuniranja i da finetuniranje sa bilo kakvom vrstom teksta pomiješanog kodom poboljšava odgovornost njenih glava pažnje na ulaze kod-pomiješanog teksta.', 'bn': 'এমবের্ট এবং এক্সএলএমআর-এর মত মডেল কোড-মিক্সেড এনএলপি কাজ সমাধানের জন্য সফল প্রদর্শন করেছে, যদিও বৃষ্টির সময় তারা এরকম টেক্সটের প্রতি প্ কোড-মিক্সেড এনএলপি মডেল সার্ভাবিকভাবে সৃষ্ট তথ্য ব্যবহার করে তাদের প্রভাব উন্নত করার জন্য নির্ভর করেছে। এরকম তথ্য নিয়ে mBERT খুঁজে পাওয়া যাচ্ছে এটি কোড-মিশ্রিত কর্মসূচি উন্নত করে, কিন্তু বিভিন্ন ধরনের কোড-মিক্সেড ডাটা ব্যবহার করার সু In this paper, we study the impact of finetuning with different types of code-mixed data and outline the changes that occur to the model during such finetuning.  আমাদের আবিস্কার পরামর্শ দেয় যে স্বাভাবিক কোড মিশ্রিত তথ্য ব্যবহার করে ফিনিশের পর সেরা কার্যক্রমের উন্নতি পাওয়া যায় এবং যে কোন ধরনের কোড মিশ্রিত টেক্সটের মাধ্যমে', 'et': 'Mudelid, nagu mBERT ja XLMR, on näidanud edu lahendada Code Mixed NLP ülesandeid, kuigi nad ei puutunud sellise tekstiga kokku eelõppe ajal. Koodidega segatud NLP mudelid on tuginenud sünteetiliselt genereeritud andmete ja looduslike andmete kasutamisele nende jõudluse parandamiseks. MBERTi viimine sellistele andmetele parandab selle koodisega seotud jõudlust, kuid erinevat tüüpi koodisega andmete kasutamise eelised ei ole selged. Käesolevas töös uurime peenhäälestuse mõju erinevat tüüpi koodisega andmetega ja kirjeldame mudelis sellise peenhäälestuse käigus toimuvaid muudatusi. Meie tulemused näitavad, et looduslikult esinevate koodisegatud andmete kasutamine parandab pärast peenhäälestust parimat jõudlust ning peenhäälestus mis tahes koodisegatud tekstiga parandab selle tähelepanu tundlikkust koodisegatud tekstisisenditele.', 'fi': 'Mallit, kuten mBERT ja XLMR, ovat onnistuneet ratkaisemaan Code Mixed NLP tehtäviä, vaikka ne eivät altistuneet tällaiselle tekstille esikoulutuksen aikana. Koodisekoiset NLP-mallit ovat hyödyntäneet synteettisesti tuotettua dataa ja luonnossa esiintyvää dataa suorituskyvyn parantamiseksi. MBERT:n hienoliittäminen tällaisiin tietoihin parantaa sen koodisekoitettua suorituskykyä, mutta erityyppisten koodisekoitettujen tietojen käytön edut eivät ole selkeitä. Tässä työssä selvitämme hienosäätön vaikutusta erityyppisillä koodisekoitetuilla tiedoilla ja hahmottelemme mallin muutoksia hienosäätön aikana. Löydöksemme viittaavat siihen, että luonnossa esiintyvän koodisekoitetun datan käyttäminen tuo parhaan suorituskyvyn parannuksen hienosäätön jälkeen ja että hienosäätö minkä tahansa koodisekoitetun tekstin kanssa parantaa sen huomion herkkyyttä koodisekoitettuihin tekstisyötteisiin.', 'ca': "Models com mBERT i XLMR han demostrat èxit en resoldre tasques NLP mixtes amb codi, encara que no estaven exposats a aquest text durant la pré-entrenament. Els models NLP combinats amb codis s'han basat en l'ús de dades sintèticament generades juntament amb dades naturals per millorar el seu rendiment. Finar mBERT en aquestes dades millora el rendiment combinat amb codi, però els beneficis d'utilitzar els diferents tipus de dades combinades amb codi no s ón clars. En aquest paper estudiem l'impacte de la finetzació amb diferents tipus de dades mixtes de codi i esboquem els canvis que ocorren al model durant aquesta finetzació. Els nostres descobriments suggereixen que utilitzar dades mixtes de codi que ocorren naturalment porta a la millor millora del rendiment després d'ajustar-se i que ajustar-se a qualsevol tipus de text mixte de codi millora la resposta de la seva atenció cap a entrades de text mixte de codi.", 'cs': 'Modely jako mBERT a XLMR prokázaly úspěch při řešení kódově smíšených NLP úloh, i když nebyly vystaveny takovému textu během předškolení. Modely NLP s kódovým smíšením spoléhají na využití synteticky generovaných dat spolu s přirozeně se vyskytujícími daty ke zlepšení jejich výkonu. Finetuning mBERT na takových datech zlepšuje jeho výkon smíšený kódem, ale výhody používání různých typů dat smíšených kódem nejsou jasné. V tomto článku studujeme vliv jemného ladění s různými typy kódově smíšených dat a nastíníme změny, ke kterým dojde v modelu během takového jemného ladění. Naše zjištění naznačují, že použití přirozeně se vyskytujících dat smíšených kódem přináší nejlepší zlepšení výkonu po jemném ladění a že jemné ladění s jakýmkoli typem smíšeného textu zlepšuje citlivost jeho pozornosti na vstupy smíšené kódem.', 'sk': 'Modeli, kot sta mBERT in XLMR, so pokazali uspeh pri reševanju nalog NLP mešanega s kodami, čeprav med predurjenjem niso bili izpostavljeni takšnemu besedilu. Modeli NLP mešanih kod so se zanašali na uporabo sintetično generiranih podatkov skupaj z naravnimi podatki za izboljšanje njihove učinkovitosti. Finetuniranje mBERT na takšnih podatkih izboljša njegovo zmogljivost z mešanimi kodami, vendar prednosti uporabe različnih vrst podatkov z mešanimi kodami niso jasne. V prispevku preučujemo vpliv finega uravnavanja z različnimi vrstami podatkov, ki so mešani s kodami, in opisujemo spremembe, ki se pojavijo v modelu med takšnim finim uravnavanjem. Naše ugotovitve kažejo, da uporaba naravno prihajajočih podatkov z mešanimi kodami prinaša najboljše izboljšanje učinkovitosti po finih nastavitvah in da finih nastavitev s katero koli vrsto besedila z mešanimi kodami izboljša odzivnost njegove pozornosti na vnose besedila z mešanimi kodami.', 'ha': "Modellun kamar mBERT da XLMR sun nuna babban rabo cikin solarin aikin Code-Mixed NLP ingawa ba su motsa zuwa wannan littãfi a lokacin da ake yi wa raining. @ action: button Babu amfani da yin amfani da wasu nau'i na Code-Mixed data ba'a bayyana. Ga wannan takardan, Munã karanta aikin fintuɗawa da wasu nau'i na kode-da-haɗe data kuma munãƙayyade musanyoyin da ke gabatar da shi a lokacin da aka buɗe shi. FantayinMu na gaya cewa, da amfani da data da ke cikin kodi-Mixed a naturally, yana ƙara mafarin aikin mafarin aiki a bayan finfinfincinsa kuma a gyarar da wani aina na kode-Mixed-matsayi, yana ƙara ajiya masu kallo wa'anarsa zuwa matsayin da aka haɗa.", 'bo': "Models such as mBERT and XLMR have shown success in solving code-Mixed NLP tasks even though they were not exposed to such text during pretraining. Code-Mixed NLP models have relied on using synthetically generated data along with naturally occurring data to improve their performance. Finetuning mBERT on such data improves it's code-mixed performance, but the benefits of using the different types of code-Mixed data are not clear. འོག་གི་ཤོག་བྱང་འདིའི་ནང་དུ་ང་ཚོས་རྗེས་སུ་འབྲེལ་བའི་རིགས ང་ཚོའི་མཐོང་སྣང་གིས་རང་རྒྱུན་ལྡན་ཡིག་ཆ་སྤྱད་ནས།", 'jv': 'model sing koyo mBERT karo XLMR iso nggawe luwih seneng nggawe Name fun Awak dhéwé éntuk kuwi, kita diwukno nggawe aturan itépakan karo perusahaan karo perusahaan kode-karo ngregani soko nggawe ngubah dhéwé cara model sing bisa ngelarane nggawe barang dumadhi. Fine', 'he': "דוגמנים כמו mBERT ו XLMR הראו הצלחה בפתרון משימות NLP מעורבות קוד למרות שהם לא נחשפו לטקסט כזה במהלך הלימודים. דוגמני NLP מעורבים קוד הסתמשו בשימוש נתונים יוצרים באופן סינטטי יחד עם נתונים מתרחשים באופן טבעי כדי לשפר את ביצועיהם. Finetuning mBERT on such data improves it's code-mixed performance, but the benefits of using the different types of Code-Mixed data aren't clear.  In this paper, we study the impact of finetuning with different types of code-mixed data and outline the changes that occur to the model during such finetuning.  הממצאים שלנו מציעים שהשימוש במידע מעורב קוד מתרחש באופן טבעי מביא את שיפור ההופעה הטוב ביותר לאחר הציון והציון עם כל סוג של טקסט מעורב קוד משפר את התגובה של ראשי תשומת לבו לתכניות טקסט מעורב קוד."}
{'en': 'Locality Preserving Loss : Neighbors that Live together, Align together', 'ar': 'المنطقة المحلية تحافظ على الخسارة: الجيران الذين يعيشون معًا ، يصطفون معًا', 'fr': "Locality Preserving Loss\xa0: des voisins qui vivent ensemble, s'alignent", 'pt': 'Localidade Preservando a Perda: Vizinhos que Vivem Juntos, Alinham Juntos', 'es': 'Preservar la pérdida de la localidad: vecinos que viven juntos, se alinean', 'zh': '地方性护损:同居,同邻', 'ru': 'Сохранение места потери: соседи, которые живут вместе, объединяются', 'ja': '地域の損失を守る：同居している近隣住民、同居している人', 'hi': 'नुकसान को संरक्षित करना: पड़ोसी जो एक साथ रहते हैं, एक साथ संरेखित करें', 'ga': 'Ceantar a Chaomhnú Caillteanas: Comharsana a Chónaíonn le chéile, Ailíniú le chéile', 'ka': 'ლოკალურობა დახმარება დაკავშირება: საზოგადოებები, რომელიც ერთად ცხოვრობენ', 'el': 'Τοπική Διατήρηση Απώλειας: Γείτονες που ζουν μαζί, ευθυγραμμίζονται μαζί', 'hu': 'Helyiség Veszteség megőrzése: Együtt élő szomszédok, Együttműködés', 'kk': 'Жергілікті Жергілікті сақтау', 'it': 'Conservare la perdita: vicini che vivono insieme, allinearsi insieme', 'lt': 'Vietos praradimą išsaugojantys kaimynai, gyvenantys kartu, suderinti kartu', 'mk': 'Локалноста зачувува загуба: соседите кои живеат заедно, разликуваат заедно', 'ml': 'സ്ഥാനം സൂക്ഷിക്കുന്നത് നഷ്ടമാണ്: ഒരുമിച്ച് ജീവിക്കുന്ന അടുത്ത ബോര്\u200dഡുകള്\u200d, ഒന്നിച്ച് ചേര്\u200dക', 'ms': 'Locality Preserving Loss: Neighbors that Live together, Align together', 'mt': 'Lokalità li żżomm it-Telf: Il-ġirien li jgħixu flimkien, jallinjaw flimkien', 'mn': 'Орон нутгийн хамгаалах алдаа: хамтдаа амьдардаг хөршүүд, хамтдаа', 'no': 'Lokalitetet Lagrar tap: nabolar som lever saman, juster saman', 'pl': 'Lokalność utrzymująca straty: Sąsiedzi, którzy mieszkają razem, wyrównują się razem', 'ro': 'Localitate care păstrează pierderile: vecini care trăiesc împreună, Aliniați împreună', 'sr': 'Lokalnost zaštite gubitke: susjedi koji žive zajedno, spojite se zajedno', 'si': 'ස්ථානය සුරකුණු නැතිවෙන්න: සමඟ ජීවත් වෙන්න අයින් වලින්, එකට සම්බන්ධ වෙන්න', 'so': 'Magaalada la ilaaliyo luminta: Degmooyinka wada nool, Isku wada simi', 'sv': 'Lokalitet Bevara förlust: Grannar som bor tillsammans, Justera ihop', 'ta': 'இடத்தை காண்பிக்கிறது: நெருங்குபோர்கள் ஒன்றாக வாழும், ஒன்றாக ஒருங்கிணைக்கும்', 'ur': 'خسارہ کی حفاظت کی جگہ: رشتہ دار جو ایک ساتھ زندگی کریں، ایک ساتھ متصل ہو جائیں،', 'uz': 'Manzilni saqlash', 'vi': 'Thay đổi sự sống chung với nhau', 'bg': 'Местността запазва загубите: съседи, които живеят заедно, подравняват се заедно', 'nl': 'Locality Preserving Loss: buren die samenwonen, uitlijnen op elkaar', 'hr': 'Lokalnost čuvajući gubitke: susjedi koji žive zajedno, spojiti se zajedno', 'da': 'Lokalitet Bevare tab: Naboer, der bor sammen, Juster sammen', 'de': 'Locality Preserving Loss: Nachbarn, die zusammen leben, sich ausrichten', 'fa': 'محل محافظت از دست دادن: همسایه\u200cها که با هم زندگی می\u200cکنند، همدیگر متصل می\u200cشوند', 'ko': '지방 보호 손실: 함께 사는 이웃, 한데 뭉치다', 'id': 'Lokalitas Menjaga Kehilangan: Tetangga yang tinggal bersama, Jajarkan bersama', 'sw': 'Wakazi wanaoishi pamoja, Kutengeneza pamoja', 'af': 'Lokasie Bewaarder Verlore: Naams wat saam lewe, Belyn saam', 'hy': 'Միջակառությունը կորուստ պահպանող հարևանները, որոնք միասին ապրում են, միասին հարմարեցնում են', 'tr': 'Ýeri Goramak Gazaplar:Birlikte yaşayan komşular, Birlikte Döndür', 'sq': 'Lokaliteti që ruan humbjen: Fqinjët që jetojnë së bashku, rregullojnë së bashku', 'az': 'Yerli Kayıpları Qoruyan: Birlikte yaşayan qonşular', 'am': 'ቦታ', 'bn': 'স্থানীয় সংরক্ষণ হারাচ্ছে: যারা একসাথে বাস করে, একসাথে একত্রিত', 'ca': 'Localitat Preservant Perde: Veïns que viuen junts, Alineant-se junts', 'cs': 'Lokalita zachování ztráty: Sousedi, kteří žijí spolu, zarovnávají se dohromady', 'et': 'Kohalik kahju säilitamine: naabrid, kes elavad koos, joonduda koos', 'bs': 'Lokalnost čuvajući gubitke: susjedi koji žive zajedno, spojiti se zajedno', 'fi': 'Paikallisuus Preserved Loss: Naapurit, jotka asuvat yhdessä, Tasaa yhdessä', 'he': 'מקום שמשמר על אבוד: שכנים שחיים יחד, מתייצבים יחד', 'sk': 'Lokacija ohranja izgubo: sosedje, ki živijo skupaj, poravnajte skupaj', 'ha': '@ action: button', 'bo': 'ས་གནས་སྟངས་ཉར་སྐྱོང་བའི་བཞག་པ། ཁྱིམ་མཚེས་ན་འཚོ་སྒྲིག་དང་། མཉམ་དུ་གྲལ་སྒྲིག་པ', 'jv': 'string" in "context_BAR_stringLink'}
{'en': 'We present a locality preserving loss (LPL) that improves the alignment between vector space embeddings while separating uncorrelated representations. Given two pretrained embedding manifolds, LPL optimizes a model to project an  embedding  and maintain its local neighborhood while aligning one  manifold  to another. This reduces the overall size of the dataset required to align the two in  tasks  such as crosslingual word alignment. We show that the LPL-based alignment between input vector spaces acts as a  regularizer , leading to better and consistent accuracy than the baseline, especially when the size of the training set is small. We demonstrate the effectiveness of LPL-optimized alignment on semantic text similarity (STS), natural language inference (SNLI), multi-genre language inference (MNLI) and cross-lingual word alignment (CLA) showing consistent improvements, finding up to 16 % improvement over our baseline in lower resource settings.', 'es': 'Presentamos una pérdida de preservación de localidad (LPL) que mejora la alineación entre las incrustaciones de espacio vectorial al tiempo que separa las representaciones no correlacionadas. Dados dos colectores de incrustación previamente entrenados, LPL optimiza un modelo para proyectar una incrustación y mantener su vecindario local mientras alinea un colector con otro. Esto reduce el tamaño total del conjunto de datos necesario para alinear los dos en tareas como la alineación de palabras en varios idiomas. Mostramos que la alineación basada en LPL entre los espacios vectoriales de entrada actúa como un regularizador, lo que conduce a una precisión mejor y consistente que la línea de base, especialmente cuando el tamaño del conjunto de entrenamiento es pequeño. Demostramos la eficacia de la alineación optimizada para LPL en la similitud semántica de texto (STS), la inferencia del lenguaje natural (SNLI), la inferencia del lenguaje multigénero (MNLI) y la alineación de palabras multilingües (CLA), mostrando mejoras consistentes, encontrando una mejora de hasta un 16% con respecto a nuestra línea de base en entornos de recursos más bajos .', 'ar': 'نقدم موقعًا يحافظ على الخسارة (LPL) الذي يحسن المحاذاة بين عمليات دمج الفضاء المتجه أثناء فصل التمثيلات غير المرتبطة. بالنظر إلى اثنين من مشعبات التضمين المحددة مسبقًا ، تعمل LPL على تحسين نموذج لإبراز التضمين والحفاظ على جواره المحلي أثناء محاذاة أحدهما مع الآخر. يؤدي ذلك إلى تقليل الحجم الإجمالي لمجموعة البيانات المطلوبة لمحاذاة الاثنين في مهام مثل محاذاة الكلمات بين اللغات. نظهر أن المحاذاة القائمة على LPL بين مسافات متجه الإدخال تعمل كمنظم ، مما يؤدي إلى دقة أفضل ومتسقة من خط الأساس ، خاصة عندما يكون حجم مجموعة التدريب صغيرًا. نبرهن على فعالية المحاذاة المحسّنة لـ LPL على تشابه النص الدلالي (STS) ، واستدلال اللغة الطبيعية (SNLI) ، واستدلال اللغة متعدد الأنواع (MNLI) ومحاذاة الكلمات عبر اللغات (CLA) التي تظهر تحسينات متسقة ، وإيجاد ما يصل إلى 16٪ التحسين على خط الأساس لدينا في إعدادات الموارد المنخفضة.', 'fr': "Nous présentons une perte de préservation de la localité (LPL) qui améliore l'alignement entre les incorporations dans l'espace vectoriel tout en séparant les représentations non corrélées. Avec deux collecteurs d'intégration préentraînés, LPL optimise un modèle pour projeter une intégration et maintenir son voisinage local tout en alignant un collecteur sur un autre. Cela réduit la taille globale de l'ensemble de données nécessaire pour aligner les deux dans des tâches telles que l'alignement de mots multilingues. Nous montrons que l'alignement basé sur la LPL entre les espaces vectoriels d'entrée agit comme un régularisateur, ce qui conduit à une précision meilleure et constante que la référence, en particulier lorsque la taille de l'ensemble d'entraînement est petite. Nous démontrons l'efficacité de l'alignement optimisé par LPL sur la similarité de texte sémantique (STS), l'inférence de langage naturel (SNLI), l'inférence de langage multi-genre (MNLI) et l'alignement de mots multilingues (CLA) montrant des améliorations constantes, constatant une amélioration allant jusqu'à 16\xa0% par rapport à notre base de référence dans des environnements à faibles ressources .", 'ja': '我々は、無関係な表現を分離しながら、ベクトル空間埋め込み間のアライメントを改善する局所性保存損失（ LPL ）を提示する。２つの事前に訓練された埋め込みマニホールドを考慮すると、ＬＰＬは、モデルを最適化して、埋め込みを投影し、１つのマニホールドを別のマニホールドに整列させながら、そのローカルな近傍を維持する。これにより、クロスリンガルワードアライメントなどのタスクで2つを整列させるために必要なデータセットの全体的なサイズが小さくなります。我々は、入力ベクトル空間間のLPLベースのアライメントが正規化器として機能し、特に訓練セットのサイズが小さい場合に、ベースラインよりも優れた一貫した精度をもたらすことを示す。我々は、セマンティックテキスト類似性（ STS ）、自然言語推論（ SNLI ）、多ジャンル言語推論（ MNLI ）、およびクロスリンガルワードアラインメント（ CLA ）に関するLPL最適化されたアラインメントの有効性を実証し、低いリソース設定におけるベースラインよりも最大16 ％の改善を見出しました。', 'zh': '建一局部性以持(LPL),改善向量嵌之对齐,离而不相关也。 给定二预练嵌流形,LPL 优化模影嵌其局邻域,而齐一流形于一流形。 此减跨语单词对齐等事务中对齐两者所需总体大小。 明输向量空间基于 LPL 齐充正则化器,而比基线有善准确性,尤当练集大小甚少。 吾证 LPL 优化之对齐在语义文本相似性 (STS)、自然语言推理 (SNLI)、多类言语推理 (MNLI) 与跨语单词齐 (CLA) 之有效性,见一体之改,见卑资置中比基线增 16%也。', 'pt': 'Apresentamos uma perda de preservação de localidade (LPL) que melhora o alinhamento entre embeddings de espaço vetorial ao separar representações não correlacionadas. Dado dois coletores de incorporação pré-treinados, o LPL otimiza um modelo para projetar uma incorporação e manter sua vizinhança local enquanto alinha um coletor a outro. Isso reduz o tamanho geral do conjunto de dados necessário para alinhar os dois em tarefas como alinhamento de palavras em vários idiomas. Mostramos que o alinhamento baseado em LPL entre espaços vetoriais de entrada atua como um regularizador, levando a uma precisão melhor e consistente do que a linha de base, especialmente quando o tamanho do conjunto de treinamento é pequeno. Demonstramos a eficácia do alinhamento otimizado para LPL na similaridade de texto semântico (STS), inferência de linguagem natural (SNLI), inferência de idioma multigênero (MNLI) e alinhamento de palavras em vários idiomas (CLA), mostrando melhorias consistentes, encontrando até 16% melhoria em relação à nossa linha de base em configurações de recursos mais baixos.', 'hi': 'हम एक स्थानीयता संरक्षण हानि (एलपीएल) प्रस्तुत करते हैं जो वेक्टर स्पेस एम्बेडिंग के बीच संरेखण में सुधार करता है, जबकि असंबद्ध अभ्यावेदन को अलग करता है। दो pretrained एम्बेडिंग मैनिफोल्ड को देखते हुए, LPL एक एम्बेडिंग प्रोजेक्ट करने और अपने स्थानीय पड़ोस को बनाए रखने के लिए एक मॉडल को अनुकूलित करता है, जबकि एक मैनिफोल्ड को दूसरे में संरेखित करता है। यह क्रॉसलिंगुअल शब्द संरेखण जैसे कार्यों में दोनों को संरेखित करने के लिए आवश्यक डेटासेट के समग्र आकार को कम करता है। हम दिखाते हैं कि इनपुट वेक्टर रिक्त स्थान के बीच एलपीएल-आधारित संरेखण एक नियमितकर्ता के रूप में कार्य करता है, जिससे बेसलाइन की तुलना में बेहतर और सुसंगत सटीकता होती है, खासकर जब प्रशिक्षण सेट का आकार छोटा होता है। हम शब्दार्थ पाठ समानता (एसटीएस), प्राकृतिक भाषा अनुमान (एसएनएलआई), बहु-शैली भाषा अनुमान (एमएनएलआई) और क्रॉस-लिंगुअल शब्द संरेखण (सीएलए) पर एलपीएल-अनुकूलित संरेखण की प्रभावशीलता का प्रदर्शन करते हैं, जो लगातार सुधार दिखाते हैं, कम संसाधन सेटिंग्स में हमारी आधार रेखा पर 16% तक सुधार पाते हैं।', 'ru': 'Мы представляем локализацию, сохраняющую потери (LPL), которая улучшает выравнивание между вложениями векторного пространства, разделяя некоррелированные представления. Учитывая два предварительно обученных встраивающих коллектора, LPL оптимизирует модель, чтобы спроецировать встраивание и поддерживать свою местную окрестность, совмещая один коллектор с другим. Это уменьшает общий размер набора данных, необходимый для выравнивания двух в таких задачах, как выравнивание по перекрестному языку. Мы показываем, что выравнивание на основе LPL между пространствами входных векторов действует как регуляризатор, что приводит к лучшей и согласованной точности, чем базовая линия, особенно когда размер обучающего набора мал. Мы демонстрируем эффективность LPL-оптимизированного выравнивания по семантическому сходству текста (STS), естественному языковому выводу (SNLI), многожанровому языковому выводу (MNLI) и межязычному выравниванию слов (CLA), демонстрируя последовательные улучшения, обнаруживая улучшение до 16% по сравнению с нашей базовой линией в более низких настройках ресурсов.', 'ga': 'Cuirimid i láthair caillteanas caomhnaithe ceantair (LPL) a fheabhsaíonn an t-ailíniú idir leabaithe spáis veicteora agus léiriúcháin neamhchomhghaolmhara á scaradh. I bhfianaise dhá iomadúil leabaithe réamhoilte, déanann LPL samhail a bharrfheabhsú chun leabú a thionscnamh agus a chomharsanacht áitiúil a chothabháil agus iomadúil amháin á ailíniú le chéile. Laghdaíonn sé seo méid iomlán an tacair sonraí a theastaíonn chun an dá cheann a ailíniú i dtascanna ar nós ailíniú focal trasteangach. Léiríonn muid go ngníomhaíonn an t-ailíniú LPL-bhunaithe idir spásanna veicteoir ionchuir mar rialtaitheoir, rud a fhágann go bhfuil cruinneas níos fearr agus comhsheasmhach ná an bunlíne, go háirithe nuair a bhíonn méid an tacair oiliúna beag. Léirímid éifeachtacht ailíniú optamaithe LPL ar chosúlacht shéimeantach téacs (STS), tátal teanga nádúrtha (SNLI), tátal teanga il-seánra (MNLI) agus ailíniú focal tras-teanga (CLA) ag taispeáint feabhsuithe comhsheasmhacha, ag aimsiú suas le 16%. feabhsú thar ár mbunlíne i suíomhanna acmhainní níos ísle.', 'hu': 'Bemutatunk egy olyan helyiségmegőrző veszteséget (LPL), amely javítja a vektortér beágyazásai közötti igazítást, miközben elválasztja a nem korrigált reprezentációkat. Két előkészített beágyazási csoporttal rendelkező LPL optimalizálja a modellt, hogy kivetítse a beágyazást és fenntartsa a helyi környéket, miközben egyik csoportot igazít a másikhoz. Ez csökkenti a kettő összehangolásához szükséges adatkészlet teljes méretét olyan feladatokban, mint például a keresztnyelvű szóigazítás. Megmutatjuk, hogy a bemeneti vektorterek közötti LPL alapú igazítás regularizálóként működik, ami jobb és következetes pontosságot eredményez, mint az alap, különösen akkor, ha az edzéskészlet mérete kicsi. Bemutatjuk az LPL-optimalizált igazítás hatékonyságát a szemantikus szöveghasonlóságra (STS), a természetes nyelvi következtetésre (SNLI), a többműfajú nyelvi következtetésre (MNLI) és a többnyelvű szóigazításra (CLA) vonatkozóan, amelyek következetes javulást mutatnak, alacsonyabb erőforrásbeállítások esetében akár 16%-os javulást találtunk az alapfelszereltséghez képest.', 'el': 'Παρουσιάζουμε μια απώλεια διατήρησης τοποθεσίας (που βελτιώνει την ευθυγράμμιση μεταξύ των διανυσματικών διαστημικών ενσωματώσεων ενώ διαχωρίζει μη διορθωμένες αναπαραστάσεις. Δεδομένου δύο προκαθορισμένων πολλαπλοτήτων ενσωμάτωσης, βελτιστοποιεί ένα μοντέλο για να προβάλει μια ενσωμάτωση και να διατηρήσει την τοπική γειτονιά του, ενώ ευθυγραμμίζει το ένα πολλαπλάσιο με το άλλο. Αυτό μειώνει το συνολικό μέγεθος του συνόλου δεδομένων που απαιτείται για τη ευθυγράμμιση των δύο σε εργασίες όπως η ευθυγράμμιση γλωσσών λέξεων. Δείχνουμε ότι η ευθυγράμμιση μεταξύ των διανυσματικών χώρων εισόδου λειτουργεί ως ρυθμιστής, οδηγώντας σε καλύτερη και συνεπή ακρίβεια από τη γραμμή βάσης, ειδικά όταν το μέγεθος του προπονητικού συνόλου είναι μικρό. Επιδεικνύουμε την αποτελεσματικότητα της βελτιστοποιημένης ευθυγράμμισης σε σημασιολογική ομοιότητα κειμένου (STS), συμπέρασμα φυσικής γλώσσας (SNLI), συμπέρασμα γλωσσών πολλαπλών ειδών (MNLI) και διαγώνια ευθυγράμμιση λέξεων (CLA) εμφανίζοντας συνεπείς βελτιώσεις, βρίσκοντας έως 16% βελτίωση έναντι της βάσης μας σε χαμηλότερες ρυθμίσεις πόρων.', 'ka': 'ჩვენ მხოლოდ მხოლოდ დახმარებას (LPL) რომელსაც გვექტორის სივრცე ინბექტირების შორის გაუქმედება, რომელსაც უფრო დახმარებული გამოსახულებების გაყოფილი. LPL მოდელის ოპტიმიზაცია, რომელიც შეიცვალობა და მუშაობის ლოკალური საზოგადოებას დაეყენება, როდესაც ერთი მანეთოლურად სხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვა. ეს შემცირებს მონაცემების ყველა ზომა, რომელიც საშუალო სიტყვების შემცირება. ჩვენ ჩვენ გამოჩვენებთ, რომ LPL-ის დამატებული დამატებული გვქტორის სივრცეების შორის დამატება იქნება როგორც რედალურაციელი, რომელიც უფრო მეტი და კონსტენსტური წესიერებას, განსაკუთრებ ჩვენ გამოჩვენებთ LPL-ოპტიმიზრებული სწორეობის ეფექტიურობა სემონტიკური ტექსტის სინამდვილეობით (STS), ჩვენი საბავშო ენის ინფრენციას (SNLI), მრავალგენერი ენის ინფრენციას (MNLI) და მრავალური სიტყვების სწორეობას (CLA) რომელიც ჩვენი', 'it': "Presentiamo una località preservando la perdita (LPL) che migliora l'allineamento tra incorporazioni di spazio vettoriale mentre separa le rappresentazioni non corrette. Dati due collettori di incorporazione pre-addestrati, LPL ottimizza un modello per proiettare un embedding e mantenere il suo quartiere locale allineando un collettore all'altro. Ciò riduce la dimensione complessiva del set di dati necessario per allineare i due in attività come l'allineamento delle parole cross-lingual. Mostriamo che l'allineamento basato su LPL tra gli spazi vettoriali di input agisce da regolarizzatore, portando a una precisione migliore e coerente rispetto alla base, specialmente quando le dimensioni del set di allenamento sono ridotte. Dimostriamo l'efficacia dell'allineamento ottimizzato per LPL sulla somiglianza semantica del testo (STS), inferenza del linguaggio naturale (SNLI), inferenza del linguaggio multi-genere (MNLI) e allineamento delle parole cross-lingual (CLA) mostrando miglioramenti costanti, trovando fino al 16% di miglioramento rispetto alla nostra base in impostazioni di risorse inferiori.", 'lt': 'Mes pateikiame vietą išsaugojantį nuostolius (LPL), kuris pagerina vektorių erdvės įdėjimų suderinimą ir atskiria nekoreguotus atvaizdus. Atsižvelgiant į du iš anksto apmokytus įdėjimo rinkinius, LPL optimizuoja model į, kad būtų galima projektuoti įdėjimą ir išlaikyti savo vietos kaimynystę, vieną rinkinį suderinant su kitu. Tai sumažina bendrą duomenų rinkinio dydį, reikalingą dviem užduotims suderinti, pavyzdžiui, tarpkalbiniam žodžių suderinimui. Mes parodome, kad LPL grindžiamas įvesties vektorių erdvių suderinimas veikia kaip reguliatorius, kuris užtikrina geresnį ir nuoseklų tikslumą nei pradinis, ypač kai mokymo rinkinio dydis yra mažas. Mes parodome, kad LPL optimizuotas suderinimas su semantiniu teksto panašumu (STS), gamtinėmis kalbų išvadomis (SNLI), daugiasluoksnių kalbų išvadomis (MNLI) ir tarpkalbiniu žodžių suderinimu (CLA) yra veiksmingas, parodant nuoseklius pagerėjimus, nustatant iki 16 % pagerėjimą, palyginti su pradiniu, mažesnių išteklių nustatymu.', 'kk': 'Біз жергілікті қорғау (LPL) жергілікті көрсетедік. Бұл вектор бос орын ендіру арасындағы жергілікті түрлендіру кезінде болады. Екі көпшілікті қосылу үшін LPL моделін ендіру және жергілікті қоршау үшін бір көпшілікті өзгерту үшін өзінің үлгісін өзгертеді. Бұл керек деректер қорларының жалпы өлшемін, мысалы, көшетілік сөздерді түзету үшін, екеуін тапсырмаларда түзету үшін қысқартады. Біз LPL негізінде келтірілген вектор бос орындар арасындағы тәртібі қадамдастыру ретінде әрекеттерді көрсетедік, негізгі жолдан жақсы және тәртіпсіздік дегенді көрсетедік, осымен қатар, оқыту ба Біз семантикалық мәтін ұқсас (STS), табиғи тілдер инференциясы (SNLI), көптеген тілдер инференциясы (MNLI) және көптеген тілдерді түрлендіру (CLA) және көптеген тілдерді түрлендіру әрекетін көрсету мүмкіндігін көрсетедік. Біздің негізгі ресурстар баптауларын', 'ml': 'നിര്\u200dണ്ണയില്ലാത്ത പ്രതിനിധികളെ വേര്\u200dക്ടര്\u200d സ്പെയിന്\u200dഡിങ്ങിന്\u200dറെ ഇടയിലുള്ള സജ്ജീകരണങ്ങള്\u200d മെച്ചപ്പെടുത്തുന്ന ഒരു ലോക രണ്ടു പ്രത്യേകിച്ചു കൊണ്ട് രണ്ട് പ്രത്യേകിച്ചു കൊണ്ട് എല്\u200dപിഎല്\u200d ഒരു മോഡലിനെ ഉപയോഗിക്കുന്നു. ഒരു പ്രോജക്റ്റ് ചെയ്യുന്നതിന്  ക്രോസ്ലിങ്കുള്ള വാക്ക് ചേര്\u200dത്തിരിക്കുന്നത് പോലുള്ള ജോലികളില്\u200d രണ്ടുപേരെയും ചേര്\u200dക്കാന്\u200d ആവശ്യപ്പെട്ട ഡ ഇന്\u200dപുട്ട് വെക്റ്റര്\u200d സ്പെയിസ്റ്റുകള്\u200dക്കിടയിലുള്ള LPL-അടിസ്ഥാനത്തിലുള്ള സജ്ജീകരണം നിയന്ത്രണമായി പ്രവര്\u200dത്തിക്കുന്നു. അത് ബെസ്റ്റ്ര സെമാന്റിക് ടെക്സ്റ്റ് സാമ്പത്തിക വാക്കുകളുടെ (STS), സ്വാഭാവികമായ ഭാഷ അപരിഹാരം (SNLI), പല-ജനറല്\u200d ഭാഷയുടെ അസംബന്ധം (MNLI) വാക്കുകളുടെയും കൂട്ടത്തിലുള്ള മെച്ചപ്പെടുത്തിയ വാക്കുകളുടെയും പ്ര', 'ms': 'Kami memperkenalkan kehilangan penyelamatan lokaliti (LPL) yang memperbaiki penyesuaian antara penyambungan ruang vektor semasa memisahkan perwakilan yang tidak terhubung. Memberikan dua manifold penyembedding yang dilatih dahulu, LPL optimizasikan model untuk projek penyembedding dan menyimpan lingkungannya setempat semasa menyesuaikan satu manifold kepada yang lain. Ini mengurangkan saiz keseluruhan set data yang diperlukan untuk menyesuaikan kedua-dua tugas seperti penyesuaian perkataan saling bahasa. We show that the LPL-based alignment between input vector spaces acts as a regularizer, leading to better and consistent accuracy than the baseline, especially when the size of the training set is small.  Kami menunjukkan kegunaan penyesuaian LPL-optimized pada persamaan teks semantik (STS), kesimpulan bahasa semulajadi (SNLI), kesimpulan bahasa berbilang jenis (MNLI) dan penyesuaian perkataan saling bahasa (CLA) menunjukkan peningkatan konsisten, mencari hingga 16% peningkatan atas asas kami dalam tetapan sumber yang lebih rendah.', 'mt': 'We present a locality preserving loss (LPL) that improves the alignment between vector space embeddings while separating uncorrelated representations.  Minħabba żewġ manifolds ta’ inkorporazzjoni mħarrġa minn qabel, LPL ottimizza mudell biex jipproġetta inkorporazzjoni u jżomm il-viċinat lokali tagħha filwaqt li jallinja manifold wieħed ma’ ieħor. Dan inaqqas id-daqs globali tas-sett tad-dejta meħtieġ biex it-tnejn jiġu allinjati f’kompiti bħall-allinjament tal-kliem bejn il-lingwi. We show that the LPL-based alignment between input vector spaces acts as a regularizer, leading to better and consistent accuracy than the baseline, especially when the size of the training set is small.  Aħna nuru l-effikaċja tal-allinjament ottimizzat mill-LPL dwar is-similarità tat-test semantiku (STS), l-inferenza tal-lingwa naturali (SNLI), l-inferenza tal-lingwa b’diversi ġeneri (MNLI) u l-allinjament tal-kliem bejn il-lingwi (CLA) li juri titjib konsistenti, li jsib titjib sa 16 % mil-linja bażi tagħna f’ambjenti ta’ riżorsi aktar baxxi.', 'pl': 'Przedstawiamy stratę zachowującą lokalność (LPL), która poprawia wyrównanie między osadzeniami przestrzeni wektorowej przy oddzielaniu nieskorelowanych reprezentacji. Biorąc pod uwagę dwa wstępnie przeszkolone kolektory osadzania, LPL optymalizuje model, aby projektować osadzenie i utrzymać lokalne sąsiedztwo, jednocześnie dostosowując jeden kolektor do drugiego. Zmniejsza to ogólny rozmiar zestawu danych wymaganych do wyrównania tych dwóch zadań, takich jak wyrównanie słów w wielojęzycznych. Pokazujemy, że wyrównanie oparte na LPL między wejściowymi przestrzeniami wektorowymi działa jako regulator, prowadząc do lepszej i spójnej dokładności niż baza wyjściowa, zwłaszcza gdy wielkość zestawu treningowego jest mała. Wykazujemy skuteczność wyrównania zoptymalizowanego przez LPL w zakresie podobieństwa tekstu semantycznego (STS), wnioskowania języka naturalnego (SNLI), wnioskowania języka wielogartunkowego (MNLI) i wyrównania słów wielojęzycznych (CLA) wykazując spójne ulepszenia, znajdując do 16% poprawy nad naszą bazą w niższych ustawieniach zasobów.', 'ro': 'Prezentăm o localitate care conservă pierderea (LPL) care îmbunătățește alinierea între încorporările spațiului vectorial în timp ce separă reprezentările necorelate. Având în vedere două colectoare de încorporare pre-instruite, LPL optimizează un model pentru a proiecta o încorporare și a menține vecinătatea sa locală în timp ce aliniază o colectoare la alta. Acest lucru reduce dimensiunea generală a setului de date necesar pentru alinierea celor două în activități, cum ar fi alinierea cuvintelor încrucișate. Arătăm că alinierea bazată pe LPL între spațiile vectoriale de intrare acționează ca regularizator, ducând la o precizie mai bună și consistentă decât baza de referință, mai ales atunci când dimensiunea setului de antrenament este mică. Demonstrăm eficiența alinierii optimizate LPL în ceea ce privește similitudinea textului semantic (STS), inferența limbajului natural (SNLI), inferența limbajului multigen (MNLI) și alinierea cuvintelor încrucișate (CLA) care arată îmbunătățiri constante, găsind până la 16% îmbunătățire față de bază în setările de resurse mai mici.', 'no': 'Vi presenterer ein lokalt lagring av tap (LPL) som forbetrar innrettinga mellom vektormellomrom medan du skiller ukraderte representasjonar. Gjennomsikt to innbygging av fleire mapper, vil LPL optimalisera eit modell for å prosjektera eit innbygging og beholda den lokale nabolagen medan du justerer ein fleire innbygging til eit anna. Dette reduserer den generelle storleiken på datasettet som krevst for å justera dei to i oppgåver som krysssprøvsprøvst ordjustering. Vi viser at LPL-baserte justeringa mellom innvektormellomrom fungerer som regulærar, som fører til bedre og konsistent nøyaktighet enn baselinja, spesielt når storleiken på opplæringsinnstillinga er liten. Vi demonstrerer effektiviteten av LPL-optimalisert justering på semantisk tekstsimilaritet (STS), naturspråk-infeksjon (SNLI), multi genre språk-infeksjon (MNLI) og krysspråk-ordjustering (CLA) som viser konsistent forbedringar, og finn opp til 16% forbedringar over vårt baselinje i lågare ressursinnstillingar.', 'sr': 'Predstavljamo lokalnu zaštitu gubitka (LPL) koja poboljšava poravnanje između vektorskih prostora u razdvajanju neovezanih predstavljanja. S obzirom na dve pretkišne komplekse, LPL optimizuje model za projektiranje integracije i održavanje lokalnog komšiluka dok se jedna prilagoðava nekoliko puta drugoj. To smanjuje ukupnu veličinu seta podataka potrebne za usklađivanje oboje u zadatke kao što je prekršteno usklađivanje riječi. Pokazujemo da poravnanje na LPL-u između prostora ulaznog vektora djeluje kao regularizator, što vodi do boljih i konsekventnih preciznosti od početne linije, posebno kada je veličina set a obuke mala. Pokazujemo učinkovitost optimiziranog poravnanja LPL-a o semantičkoj sličnosti teksta (STS), infekciji prirodnog jezika (SNLI), infekciji multi genre jezika (MNLI) i preko jezika poravnanja reči (CLA) koji pokazuju konsekventne poboljšanja, pronalaženje do 16% poboljšanja nad našom početnom linijom u nižim nastavama resursa.', 'si': 'Name ප්\u200dරීට්\u200dරේන්ඩ් සංවිධානයක් දෙකක් තියෙන්නේ, LPL මොඩේලයක් සංවිධානය කරන්න සහ ස්ථානික මුහුදානයක් ව්\u200dයාපෘතිකරණය මේක අවශ්\u200dය දත්ත සෙට්ටුවේ සම්පූර්ණ ප්\u200dරමාණය අඩු කරනවා වැඩවල් දෙකක් සම්පූර්ණය කරන්න, වර්ගභාෂාවක අපි පෙන්වන්නේ LPL- අධාරිත වෙක්ටර් අවස්ථාවක් ඇතුළු වෙක්ටර් අවස්ථාවක් අතර ප්\u200dරමාණකයෙක් විදියට ක්\u200dරියාත්මක කරනවා කියලා,  අපි ප්\u200dරකාශ කරනවා LPL-optimized Aliment on semantic text comparity (STS), Native language Infrence (SNLI), Multi-Genre language Infrence (MNLI) and cross-language word Aliment (CLA) show composent impropers, find up to 16% improper over our baseline in low source Settings.', 'so': 'Waxaannu soo saaraynaa gooni ku haysta khasaarada (LPL) oo kordhiya isbedelka meelaha waddanka ah oo ku dhex yaal waddanka waddanka marka loo kala sooco qofka aan loo kala soocayn. Sida uu labada noocyo oo lagu arag karo, LPL wuxuu u bedelaa tusaale in uu sameeyo mid ku habboon oo uu ku haysto deriskiisa degmada, iyadoo ay isbedeshaan mid kala duduwan. Taasi waxay hoos u dhigtaa qiyaastii ugu badnaan ee looga baahan yahay in labada shaqooyin lagu soo simo sida isbedelka hadalka luqada lagu kordhiyo. Waxaynu muujinnaa in LPL-ku hoos dhigashada goobaha wax lagu sameeyo ay u shaqeeyaan sida wax ilaaliya, taasoo sababtaa saxda aad u wanaagsan oo ku sii socota tan hoose, khusuusan marka ay tirada waxbarashadu yar tahay. Waxaynu muujinnaa shaqaalaha loo bedelay qoraalka qoraalka semantika ah (STS), dhibaatada afka asalka ah (SNLI), dhibaatada afka badan oo kala duduwan (MNLI) iyo isbedelka hadalka luuqadaha kala duduwan (CLA) oo muujinta hagaajinta oo la mid ah, waxaana sameynaya ilaa 16% kordhiska qoriga hoose resourceyda.', 'mk': 'We present a locality preserving loss (LPL) that improves the alignment between vector space embeddings while separating uncorrelated representations.  Со оглед на двата преобучени вградувачки множини, LPL оптимизира модел за проектирање на вградување и одржување на своето локално соседство, истовремено alining еден множин на друг. Ова ја намалува целокупната големина на компјутерот на податоци потребни за да ги израмнат двете задачи како што е крстојазичното израмнување на зборовите. We show that the LPL-based alignment between input vector spaces acts as a regularizer, leading to better and consistent accuracy than the baseline, especially when the size of the training set is small.  Ја демонстрираме ефикасноста на оптимизираното подрачје на LPL на семантичната текстова сличност (STS), природната јазичка инференција (SNLI), мултигенерна јазичка инференција (MNLI) и прекујазична подрачје на зборовите (CLA) покажувајќи константни подобрувања, откривајќи се до 16 отсто подобрување во', 'sv': 'Vi presenterar en lokalitetsbevarande förlust (LPL) som förbättrar justeringen mellan vektorrumsinbäddningar samtidigt som okorrelerade representationer separeras. Med två förbelagda inbäddningsrören optimerar LPL en modell för att projicera en inbäddning och upprätthålla dess lokala grannskap samtidigt som den anpassar en grenrör till en annan. Detta minskar den totala storleken på datauppsättningen som krävs för att justera de två i aktiviteter som korsspråklig ordjustering. Vi visar att den LPL-baserade justeringen mellan inmatningskortsutrymmena fungerar som en regularizer, vilket leder till bättre och konsekvent noggrannhet än baslinjen, särskilt när storleken på träningsuppsättningen är liten. Vi demonstrerar effektiviteten av LPL-optimerad justering för semantisk textlikhet (STS), naturlig språkinferens (SNLI), flergenres språkinferens (MNLI) och korsspråklig ordjustering (CLA) som visar konsekventa förbättringar, vilket visar upp till 16% förbättring jämfört med vår baslinje i lägre resursinställningar.', 'mn': 'Бид огторгуйн алдагдлыг хадгалах (LPL) гэдгийг харуулж байна. Энэ нь векторын орон зай хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо хо Хоёр давхар хөршүүн хоёр давхар загварын тулд LPL нь хоёр давхар хөршүүн хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо хоорондоо холбоотой заг Энэ нь хоёрыг хэлний хэлбэрээр тодруулах хэрэгтэй өгөгдлийн сангийн нийт хэмжээ багасгадаг. Бид LPL-ээр суурь вектор орон зайн хоорондын тэгшитгэл нь бага зэрэг байдаг. Энэ нь суурь шулуунаас илүү сайн, тогтмол зөв байдлыг харуулж байна. Ялангуяа сургалтын хэмжээ жижиг байхад. Бид СNLI, олон төрлийн хэл халдвар (MNLI) болон олон хэл хэл дамжуулалт (CLA) дээр тогтвортой сайжруулалтыг харуулсан LPL-ын сайжруулагдсан тэгшитгэлийн үр дүнг харуулж байна. Байгалийн хэл халдвар (STS), олон төрлийн хэл халдвар (MNLI) болон олон хэл хэл дамжуулалт (CLA) нь', 'ta': 'நாம் ஒரு உள்ளூர் நஷ்டத்தை காப்பாற்றுகிறோம் (LPL) இது நெறிய இடைவெளி இடைவெளியில் உள்ள ஒழுங்குப்படுத்தலை மேம்படுத்துகிறது.  இரண்டு குறிப்பிட்ட குறிப்பிட்ட கொடுக்கப்பட்டால், LPL ஒரு மாதிரி திட்டத்தை மாற்றுகிறது ஒரு உள்ளமைப்பு மற்றும் அதன் உள்ளூர்ந்தி இது மொத்த தரவுத்தளத்தின் அளவை குறைக்கும் மொத்தமான சொல்லை ஒழுங்குப்படுத்தல் போன்ற பணிகளில் இரண்டை ஒழுங்குபடுத் உள்ளீட்டு வெக்டார் இடைவெளிகளுக்கிடையே LPL அடிப்படையான ஒழுங்குப்பொருள் செயல்படுகிறது, அடிப்படைக்கோட்டை விட சிறந்த மற்றும் நிலையான சரிய பெமான்டிக் உரையின் ஒத்திசைக்கான (STS), இயல்பான மொழி குறைப்பு (SNLI), பல மரபணு மொழி குறைப்பு (MNLI) மற்றும் மொழி மொழி ஒத்திசைப்படுத்தல் (CLA) விளைவுகளைக் காண்பிக்கிறது, தொடர்ந்த முன்னேற்றமைப்பு', 'ur': 'ہم ایک مکانی حفاظت کرنے والی خسارہ (LPL) کو پیش کرتے ہیں جس نے ویکتور فضا انڈینگ کے درمیان تفریق کی حالت اضافہ کرتی ہے جب وہ بغیر تفریق کی حالت جدا کرتی ہے۔ دو پرٹرین ڈینڈنگ منڈ فولڈ کے ذریعے LPL ایک موڈل کو اچھی طرح رکھتا ہے کہ ایک مہینڈنگ کی پروژه کرے اور اس کی محلی محله کی حفاظت کرے اور ایک منڈ فولڈ دوسرے سے مہینڈ کرے۔ یہ دھوپ کے سارے اندازے کاٹ دیتا ہے جو دو کاموں کو کروسٹ زبان کے متصل کرنے کے لئے لازم ہے۔ ہم دکھاتے ہیں کہ LPL-based alignment between input vector spaces as a regularizer, leading to better and consistent accuracy than the baseline, especially when the size of the training set is small. ہم LPL-optimized alignment (STS), natural language inference (SNLI), multi-genre language inference (MNLI) اور cross-lingual word alignment (CLA) کے مثبت کو دکھاتے ہیں جو سیمنٹی ٹکسٹ برابری (STS) کے ذریعہ مثبت دیتے ہیں، اور ہمارے بنسٹ لین پر 16% improvement (SNLI) پائیں گے۔', 'vi': 'Chúng tôi có một địa điểm lưu giữ mất mát (LL) giúp cải thiện sự thẳng định giữa sự nhúng vào không gian vector trong khi tách các biểu tượng chưa chỉnh. Dựa vào hai hình ảnh nhúng được trước, LL dùng một mô hình để dự đoán sự nhúng vào và duy trì khu vực địa phương của nó, đồng thời căn thẳng hàng này theo chiều khác. Điều này làm giảm kích thước tổng thể của bộ dữ liệu cần thiết để xếp ngang hai nhiệm vụ như việc sắp xếp các từ ngữ rộng. Chúng tôi cho thấy sự thẳng hàng dựa trên LL giữa các khoảng vector hoạt động như một bộ chỉnh sửa, dẫn đến độ chính xác tốt hơn so với cơ sở, đặc biệt khi kích thước của bộ huấn luyện nhỏ. Chúng tôi chứng minh hiệu quả của việc chỉnh tinh chỉnh các từ theo độ phân biệt khái niệm về nét chữ giống (STS), ngụ ý ngôn ngữ tự nhiên (SNLl), ngụ ý ngôn ngữ đa thể (MNLL) và sắp xếp các từ ngữ khác nhau (CLAY) có hiệu quả cải tiến liên tục, tìm đến 16=.* cải tiến hơn so với hoàn cảnh thiết lập nguồn dưới tài nguyên.', 'uz': "We present a locality preserving loss (LPL) that improves the alignment between vector space embeddings while separating uncorrelated representations.  Ikkita tashkilotlarni ajratishda, LPL bir necha ko'pchilik bilan murojaat qiladigan modelni yaratish va lokal taqdimotini davom etish mumkin. Name @ info: whatsthis @ info: whatsthis", 'bg': 'Представяме загубата за запазване на локалността (LPL), която подобрява подравняването между вградените векторни пространства, като същевременно разделя некорегираните изображения. Предвид два предварително обучени вграждащи колектора, оптимизира модела, за да проектира вграждане и да поддържа местния си квартал, като същевременно подравнява един колектор към друг. Това намалява общия размер на набора от данни, необходим за подравняване на двете в задачи като кръстосано подравняване на думи. Показваме, че базираното подравняване между входните векторни пространства действа като регуляризатор, което води до по-добра и последователна точност от базовата линия, особено когато размерът на тренировъчния комплект е малък. Ние демонстрираме ефективността на оптимизираното подравняване на семантична текстова сходство (STS), естествен език (SNLI), многожанрово езиково подравняване (MNLI) и междуезично подравняване на думи (CLA), показвайки последователни подобрения, намирайки до 16% подобрение спрямо базовата база при по-ниски настройки на ресурсите.', 'hr': 'Predstavljamo lokalnu zaštitu gubitka (LPL) koja poboljšava poravnanje između vektorskih prostora u razdvajanju neovezanih predstavljanja. S obzirom na dvije pretkišne ugrađene manifolde, LPL optimizira model za projektiranje ugrađenja i održavanje lokalnog susjedstva dok se jedna manifold poravna drugoj. To smanjuje ukupnu veličinu seta podataka potrebne za usklađivanje oboje u zadatke poput prijenosa krstojezičkih riječi. Pokazujemo da poravnanje na LPL-u između prostora ulaznog vektora djeluje kao regularizator, što dovodi do boljih i konsekventnih preciznosti od početne linije, posebno kada je veličina set a obuke mala. Mi pokazujemo učinkovitost optimiziranog poravnanja LPL-a o semantičkoj sličnosti teksta (STS), infekciji prirodnog jezika (SNLI), infekciji multi genre jezika (MNLI) i preko jezičkog poravnanja riječi (CLA) koji pokazuju konsekvente poboljšanja, pronalaženje do 16% poboljšanja nad našom početnom linijom u nižim nastavama resursa.', 'nl': 'We presenteren een locality conserving loss (LPL) dat de uitlijning tussen vectorruimte insluitingen verbetert terwijl ongecorreleerde representaties worden gescheiden. Met twee vooraf getrainde embedding spruitstukken optimaliseert LPL een model om een embedding te projecteren en de lokale buurt te behouden terwijl het ene spruitstuk op het andere uitlijnt. Dit verkleint de totale grootte van de dataset die nodig is om de twee uit te lijnen in taken zoals crosslingual woorduitlijning. We laten zien dat de LPL-gebaseerde uitlijning tussen invoervectorruimten fungeert als een regularizer, wat leidt tot betere en consistente nauwkeurigheid dan de baseline, vooral wanneer de grootte van de trainingsset klein is. We demonstreren de effectiviteit van LPL-geoptimaliseerde uitlijning op semantische tekstgelijkenis (STS), natuurlijke taal-inferentie (SNLI), multi-genre taal-inferentie (MNLI) en cross-lingual word alignment (CLA) met consistente verbeteringen, waarbij tot 16% verbetering ten opzichte van onze basislijn in lagere resource-instellingen wordt gevonden.', 'da': 'Vi præsenterer en lokalitetsbevarende tab (LPL), der forbedrer justeringen mellem vektorpladsindlejringer, mens de adskiller ukorrekte repræsentationer. Med to forudtrænede indlejringsmanifolder optimerer LPL en model til at projicere en indlejring og vedligeholde sit lokale kvarter, mens den ene manifold justeres til en anden. Dette reducerer den samlede størrelse af det datasæt, der kræves for at justere de to i opgaver, f.eks. tværsproget ordjustering. Vi viser, at LPL-baseret justering mellem input vektorrum fungerer som regulerer, hvilket fører til bedre og konsekvent nøjagtighed end baseline, især når størrelsen på træningssættet er lille. Vi demonstrerer effektiviteten af LPL-optimeret justering på semantisk tekstlighed (STS), naturlig sproginference (SNLI), multi-genre sproginference (MNLI) og tværsproget ordjustering (CLA), der viser konsekvente forbedringer, og opdager op til 16% forbedring i forhold til vores baseline i lavere ressourceindstillinger.', 'ko': '우리는 상관없는 표현을 분리하는 동시에 벡터 공간 삽입 사이의 정렬을 개선하는 국부 유지 손실(LPL)을 제시했다.두 개의 예비 훈련의 삽입 유형을 정하고 LPL은 모델을 최적화하여 투영으로 삽입하고 국부적인 인접역을 유지하며 한 유형을 다른 유형과 맞춘다.이로써 크로스 언어 단어 정렬 등 작업에서 이 두 단어를 정렬하는 데 필요한 데이터 집합의 전체 크기를 줄였다.우리는 LPL 기반의 입력 벡터 공간 정렬이 정규화기로서 기선보다 더 좋고 일치하는 정밀도를 가지고 있으며 특히 훈련집이 비교적 작은 상황에서 이를 증명하였다.우리는 LPL 최적화 정렬이 의미 텍스트 유사성(STS), 자연 언어 추리(SNLI), 다중 장르 언어 추리(MNLI)와 다중 언어 단어 정렬(CLA)에 대한 유효성을 증명하고 일치된 개선을 보이며 낮은 자원 설정에서 기선보다 16% 높아졌다.', 'de': 'Wir präsentieren einen lokalitätserhaltenden Verlust (LPL), der die Ausrichtung zwischen Vektorraumeinbettungen verbessert und unkorrelierte Darstellungen trennt. Mit zwei vortrainierten Einbettungsverhältnissen optimiert LPL ein Modell, um eine Einbettung zu projizieren und seine lokale Nachbarschaft zu erhalten, während ein Verteiler auf einen anderen ausgerichtet wird. Dies reduziert die Gesamtgröße des Datensatzes, der erforderlich ist, um die beiden in Aufgaben wie der sprachübergreifenden Wortausrichtung auszurichten. Wir zeigen, dass die LPL-basierte Ausrichtung zwischen Eingangsvektorräumen als Regularizer fungiert und zu einer besseren und konsistenten Genauigkeit als die Baseline führt, insbesondere wenn die Größe des Trainingssets klein ist. Wir demonstrieren die Effektivität der LPL-optimierten Ausrichtung auf semantische Textähnlichkeit (STS), natürliche Sprachinferenz (SNLI), Multi-Genre-Sprachinferenz (MNLI) und crosslinguale Wortausrichtung (CLA) mit konsistenten Verbesserungen, wobei bis zu 16% Verbesserung gegenüber unserer Baseline in niedrigeren Ressourceneinstellungen festgestellt wird.', 'id': 'Kami mempersembahkan kehilangan yang memelihara lokalitas (LPL) yang meningkatkan penyesuaian antara penerbangan ruang vektor sementara memisahkan representation yang tidak terhubung. Mengingat dua manifold embedding terlatih sebelumnya, LPL optimisasi model untuk proyeksi embedding dan mempertahankan lingkungan lokalnya sementara menyesuaikan satu manifold ke yang lain. Ini mengurangi ukuran umum dari set data yang diperlukan untuk menyesuaikan kedua tugas seperti penyesuaian kata saling bahasa. Kami menunjukkan bahwa alignment berdasarkan LPL antara ruang vektor input bertindak sebagai regulariser, menyebabkan akurasi yang lebih baik dan konsisten dari dasar dasar, terutama ketika ukuran set latihan kecil. Kami menunjukkan efektivitas dari penyesuaian LPL-optimizasi pada persamaan teks semantis (STS), inferensi bahasa alam (SNLI), inferensi bahasa multi genre (MNLI) dan penyesuaian kata saling bahasa (CLA) menunjukkan peningkatan konsisten, menemukan sampai 16% peningkatan atas dasar kami dalam pengaturan sumber daya lebih rendah.', 'sw': 'Tunaweza kuweka eneo hilo linalolinda hasara (LPL) ambalo linaboresha uzalishaji kati ya anga la vector wakati wakitenga uwakilishi usio na ukweli. Kutokana na matukio mawili yanayojihisi kuingia, LPL huweka mfano wa kutengeneza mpango wa kuingia katika maeneo yake wakati wa kutambua aina moja kwa moja. Hii inapunguza ukubwa wa jumla wa seti zinazohitajika kufanya kazi hizo mbili kama vile upatikanaji wa maneno ya lugha. Tunaonyesha kuwa jukumu lililoko kwa LPL kati ya maeneo ya injini inafanya kazi kama mdhibiti, na kusababisha uhakika mzuri na kuendelea zaidi ya msingi, hasa pale ambapo ukubwa wa seti ya mafunzo ni ndogo. Tunaonyesha ufanisi wa kuongezeka kwa lugha ya asili ya LPL kwa maendeleo yanayofanana na simu za ujumbe (STS), uvumilivu wa lugha ya asili (SNLI), uvumilivu wa lugha mbalimbali (MNLI) na usambazaji wa maneno ya lugha (CLA) unaonyesha maendeleo yanayoendelea, na kufikia asilimia 16 zaidi ya maendeleo yetu ya msingi katika mazingira ya chini ya rasilimali.', 'fa': 'ما یک محل حفاظت از خسارت (LPL) را پیشنهاد می\u200cکنیم که در زمان جدا کردن نمایش\u200cهای غیرمتصل تغییر می\u200cکند تغییر بین فضای ویکتور را بهتر می\u200cکند. با توجه به دو چندین پولدار پیدا کردن پیش ریخته شده، LPL یک مدل را برای پروژه\u200cهای پیدا کردن و نگه داشتن محله\u200cی محلی خود در حالی که یکی را چندین برابر دیگر تنظیم می\u200cکند. این اندازه عمومی از مجموعه داده\u200cها که نیاز دارند برای تنظیم کردن آن دو در کار مانند تنظیم کردن کلمه\u200cهای عبور زبانی کاهش می\u200cدهد. ما نشان می دهیم که تطبیق LPL بنیاد بین فضای ویکتور ورودی به عنوان تطبیق\u200cکننده\u200cی معمولی عمل می\u200cکند که به دقیق بهتر و پایدار از خط پایین، مخصوصا وقتی اندازه مجموعه آموزش کوچک است. ما تاثیر تفاوت LPL-optimized alignment on semantic text similar (STS), natural language inference (SNLI), multi-genre language inference (MNLI) and cross-lingual word alignment (CLA) showing consistent improvements, finding up to 16% improvement over our baseline in lower resource settings.', 'tr': "Biz bir yerleşik kaybı (LPL) şeklinde gösteriyoruz ki, vektör uzay içerisindeki çizgileri birbirine ayırır. LPL iki öňündeki köp baglanyşyk üçin bir nusgany gaýşartýar we ýerleşki daşgalaryny birnäçe gezek bir nusgany bejermek üçin bir nusgany bejerýär. Bu, iki işi çatlaşdyrmak üçin gerekli datasetiniň bütin ululykyny azaltýar. LPL'de girdi vektör seleňleri arasynda düzgün bir şekilde işleýän çyzygy görkezýäris. Bu şekilde başlangyç sisteminden has dogrylyklar we dogrylyklar gerektiğini görkez. Biz LPL-optimized çyzygynyň (STS), tebigat dillerimiziň (SNLI), birnäçe jenar dilleriň (MNLI) we cross-lingual söz çyzygynyň (CLA) täsirli gelişmelerini görkeýäris we esasy çyzygymyzda 16% gelişmeleri bar.", 'am': 'የቦታ ክፍል (LPL) በተለየ የዌctor ቦታ ውስጥ በተለየ ጊዜ የክፍለ መልዕክቶችን በማሳመር እናሳድጋለን፡፡ ሁለትም የተከፋፈሉ መልዕክቶች ሲሰጡ፣ LPL አንዱን በአንድ ዓይነት በተለየ እና የአገር ጎረቤቱን በመግለጽ ምሳሌ ያሳያል፡፡ language የLPL-መሠረት በጥያቄው ቦታዎች መካከል መሠረት እንደሚያሳየው፣ የተስተማሪው መጠን ትንሽ ሲሆን ይሻላል እና ትክክለኛ እንደሆነ እናሳያቸዋለን፡፡ የLPL ጽሑፍ ብጤት (STS), የፍጥረት ቋንቋ ውጤት (SNLI)፣ የብዙ ቋንቋ ውጤት (MNLI) እና የቋንቋ ቋንቋ አካባቢነት (CLA) እና የክፍለ ክፍተት ማሳየትን እናሳየዋለን፡፡', 'af': "Ons stel 'n plaaslike verloor (LPL) wat verbeter die lyn tussen vektor spasie inbettings terwyl onkorrekte voorstellings skei word. Gien twee voortreënde inbêer manifolde, LPL optimaliseer 'n model na projek 'n inbêer en onderhou sy plaaslike nabooste terwyl een manifolde na 'n ander gelyk word. Hierdie verminder die oorspronklike grootte van die datastel wat benodig is om die twee in opdragte te aligneer soos kruistale woord oplyn. Ons wys dat die LPL-gebaseerde lyn tussen invoer vektor spasies werk as 'n regulariseerder, wat lei na beter en konsistente presisie as die basisline, veral wanneer die grootte van die onderwerking stel klein is. Ons wys die effektiviteit van LPL-optimaliseerde lyn op semantiese teks gelykbaarheid (STS), natuurlike taal inferensie (SNLI), multi genre taal inferensie (MNLI) en kruistale woord alignering (CLA) wat bestaande verbeteringe vertoon word, wat tot 16% verbetering oor ons basisline in onder hulpbron instellings vind.", 'hy': 'Մենք ներկայացնում ենք կորստ պահպանող տեղակայությունը (LPL), որը բարելավում է վեկտոր տարածքի ներդրումների միջև հավասարումը, առանձնացնում անկախված ներկայացումները: Եթե հաշվի առնենք երկու նախապատրաստված ներգրավման բազմաթիվ տարբերակներ, LPL-ը օպտիմացնում է մի մոդել ներգրավման նախագծելու և պահպանելու համար իր տեղական հարևանությունը, միաժամանակ միավորելով մեկը մյուսին: Սա նվազեցնում է տվյալների համաշխարհային չափսերը, որոնք անհրաժեշտ են երկուսին հարմարեցնելու այնպիսի խնդիրներում, ինչպիսիք են, օրինակ, երկլեզու բառերի հարմարեցման: Մենք ցույց ենք տալիս, որ LPL-ի հիմնված հարաբերությունը ներմուծքային վեկտորների տարածքների միջև գործում է որպես վերահսկողություն, ինչը հանգեցնում է ավելի լավ և համապատասխանատու ճշգրտության, քան հիմքային հատվածը, հատկապես երբ փոքր է վարժման համակարգի չա Մենք ցույց ենք տալիս LPL-ի օպտիմացված հարմարեցման արդյունավետությունը սեմանտիկ տեքստի նմանության (ՍՏՍ), բնական լեզվի եզրակացության (ՍՆԼԻ), բազմագենդերային լեզվի եզրակացության (ՄՆԼI) և լեզվի միջև բառերի հարմարեցման (ԿԼA) վրա, որոնք ցույց են տալիս համապատասխան բարելավ', 'az': 'Biz bir yerdə zərəri qoruyan (LPL) məlumatı göstəririk ki, vektör uzay məlumatının arasındakı müəyyən edilməsini daha yaxşılaşdırır. İki çoxluğuna görə, LPL bir modeli inşa etmək və yerli qonşuluğunu müəyyən etmək üçün bir çoxluğu ilə birləşdirir. Bu, hər ikisini çox dilli sözlər tərəfləndirməsi kimi işlərdə tərəfləndirmək üçün ehtiyacı olan veri qutusunun bütün böyüklüyünü azaldır. Biz LPL-ə dayanan giriş vektör uzaqlarının arasındakı tərəfləndirilməsini düzgün tərəfləndirir, bu tərəfləndirilənin böyüklüyü kiçik olduğu zaman daha yaxşı və düzgün tərəfləndirilməsini göstəririk. Biz semantik mətn similarlığı (STS), doğal dil infeksiyonu (SNLI), çoxlu cür dil infeksiyonu (MNLI) və çoxlu dil sözlərin müəyyənləşdirməsini (CLA) göstərdik, həmişəlik düzəltmələri göstərdik, əsas səhiflərimizdə 16%-ə qədər daha yaxşılıq tapırdıq.', 'sq': 'Ne paraqesim një humbje të ruajtur nga lokaliteti (LPL) që përmirëson rregullimin midis përfshirjeve të hapësirës vectore ndërsa ndajmë përfaqësime të pakorrelatuar. Duke pasur parasysh dy multiplikatorë të përfshirjes së parastërvitur, LPL optimizon një model për të projektuar një përfshirje dhe për të mbajtur lagjen e saj lokale ndërsa rendit një multiplikator në tjetrin. Kjo redukton madhësinë e përgjithshme të kompletit të të dhënave të nevojshme për të renditur të dy në detyra të tilla si zgjidhja e fjalëve ndërgjuhësore. Ne tregojmë se përputhja me bazë në LPL midis hapësirave të vektorit të hyrjes vepron si një rregullator, duke çuar në saktësi më të mirë dhe konsistente se baza, veçanërisht kur madhësia e grupit të trajnimit është e vogël. Ne demonstrojmë efektshmërinë e përshtatjes së LPL-optimizuar në ngjashmërinë semantike të tekstit (STS), inferencën natyrore të gjuhës (SNLI), inferencën gjuhësore të shumëgjenerëve (MNLI) dhe përshtatjen ndërgjuhësore të fjalëve (CLA) duke treguar përmirësime konsistente, duke gjetur deri në 16% përmirësim mbi bazën tonë në rregullime më të ulëta të burimeve.', 'bn': 'আমরা একটি স্থানীয় ক্ষতি সংরক্ষণ করছি (এলপিএল) যা ভেক্টরের স্পেসের বিভিন্ন প্রতিনিধিত্ব বিভিন্ন প্রতিনিধিদের বিভিন্ন ভিন্ দুটি প্রতিবেদনের দ্বারা ভাবছে, এলপিএল একটি মডেল প্রকল্পের মাধ্যমে একটি প্রজেক্ট করে স্থানীয় প্রতিবেশীর দ্বারা একে অন্যের দ্বিগুণ এটি ক্র্যাস্ভুগুয়েল শব্দ প্রতিযোগিতার মতো দুটি কাজের মধ্যে আড়াল করার জন্য প্রয়োজনীয় ডাটাসেটের মোট আকার কমিয়ে দে আমরা দেখাচ্ছি যে ইনপুট ভেক্টরের স্পেসের মধ্যে ভিত্তিক অবস্থান কাজ করে যাচ্ছে নিয়ন্ত্রণালয়ের হিসেবে, যার ফলে বেসার লাইনের চেয়ে ভালো এবং সাধারণ সেমেন্টিক টেক্সটের সমতুল্য (এসএনলি), প্রাকৃতিক ভাষার আক্রান্ত (এসএনলি), বহুজেনার ভাষার আক্রান্ত (এমএনলি) এবং ক্রিভাষাভাষায় শব্দের একত্রিত করার (সিএলএ) কার্যক্রমের কার্যক্রম প্রদর্শন করেছে, যা', 'bs': 'Predstavljamo lokalnu zaštitu gubitka (LPL) koja poboljšava poravnanje između vektorskih prostora u razdvajanju neovezanih predstavljanja. S obzirom na dvije pretkišne ugrađene manifolde, LPL optimizira model za projektiranje ugrađenja i održavanje lokalnog susjedstva dok se jedna prilagođuje manifold na drugu. To smanjuje ukupnu veličinu seta podataka potrebne za usklađivanje oboje u zadatke kao što su krstojezično usklađivanje riječi. Pokazujemo da poravnanje na LPL-u između prostora ulaznog vektora djeluje kao regularizator, što vodi do boljih i konsekventnih preciznosti od početne linije, posebno kada je veličina set a obuke mala. Pokazujemo učinkovitost optimiziranog poravnanja LPL-a o semantičkoj sličnosti teksta (STS), infekciji prirodnog jezika (SNLI), infekciji multi genre jezika (MNLI) i preko jezičkog poravnanja riječi (CLA) koji pokazuju konsekvente poboljšanja, pronalaženje do 16% poboljšanja nad našom početnom linijom u nižim nastavama resursa.', 'ca': "Presentam una pèrdua de preservació de la localitat (LPL) que millora l'allinjament entre les incorporacions de l'espai vector mentre separa representacions no correlacionades. Dant dos multiplicators d'incorporació pré-entrenats, LPL optimitza un model per projectar una incorporació i mantenir el seu barri local alçant un multiplicator a l'altre. Això redueix la mida global del conjunt de dades requerit per alliniar les dues en tasques com l'alliniament de paraules translingües. We show that the LPL-based alignment between input vector spaces acts as a regularizer, leading to better and consistent accuracy than the baseline, especially when the size of the training set is small.  Demonstrem l'eficacia de l'allinjament optimitzat per LPL en la similitud de text semàntic (STS), inferència de llenguatge natural (SNLI), inferència de llenguatge multigènere (MNLI) i allinjament de paraules translingües (CLA), mostrant millores consistents, trobant fins al 16% millora en comparació amb la nostra base de referencia en condicions de recursos inferiors.", 'et': 'Esitleme lokaalsuse säilitamise kadu (LPL), mis parandab vektoriruumi põimimiste joondumist, eraldades samal ajal korrigeerimata representatsioone. Arvestades kahte eeltreenitud manustamiskolleri, optimeerib LPL mudelit manustamise projekteerimiseks ja kohaliku naabruskonna säilitamiseks, joondades samal ajal ühe kollektori teisega. See vähendab nende kahe ülesande joondamiseks vajaliku andmekogumi üldist suurust, näiteks keeleülese sõna joondamiseks. Näitame, et LPL-põhine joondamine sisendvektoriruumide vahel toimib regulariseerijana, mis viib parema ja järjepideva täpsuseni kui algväärtus, eriti kui treeningkomplekti suurus on väike. Näitame LPL-i optimeeritud joondamise efektiivsust semantilise teksti sarnasuse (STS), loomuliku keele järelduse (SNLI), mitmežanri keele järelduse (MNLI) ja keeleülese sõna joondamise (CLA) osas, näidates järjepidevat paranemist, leides kuni 16% võrreldes algväiksemate ressursside seadetega.', 'fi': 'Esit채mme paikkakuntas채il철nt채h채vi철n (LPL), joka parantaa vektoriavaruuden upotusten v채list채 linjausta samalla kun erottaa korreloimattomat esitykset. Kun otetaan huomioon kaksi esivalmennettua upotussarjaa, LPL optimoi mallin projisoimaan upotuksen ja yll채pit채m채채n paikallista naapurustoaan samalla kun kohdistaa jakosarjan toiseen. T채m채 v채hent채채 niiden kahden tasaamiseen tarvittavan aineiston kokonaiskokoa esimerkiksi monikielisess채 sanatasauksessa. Osoitamme, ett채 LPL-pohjainen linjaus tulovektoriavaruuksien v채lill채 toimii s채채nn철stelij채n채, mik채 johtaa parempaan ja yhdenmukaiseen tarkkuuteen kuin l채ht철taso, varsinkin kun harjoitussarjan koko on pieni. Osoitamme LPL-optimoidun linjauksen tehokkuuden semanttiseen tekstin samankaltaisuuteen (STS), luonnollisen kielen p채채ttelyyn (SNLI), monikieliseen kielen p채채ttelyyn (MNLI) ja monikieliseen sanalinjaukseen (CLA).', 'cs': 'Představujeme lokalitu zachovávající ztrátu (LPL), která zlepšuje zarovnání mezi vkládáním vektorového prostoru při oddělení nekorelovaných reprezentací. Díky dvou předem trénovaným vkládacím potrubím optimalizuje LPL model tak, aby projektoval vkládání a udržoval jeho místní sousedství při srovnání jednoho potrubí s druhým. Tím se snižuje celková velikost datové sady potřebné k zarovnání obou úkolů, jako je například zarovnání více jazyků slov. Ukazujeme, že zarovnání založené na LPL mezi vstupními vektorovými prostory funguje jako regularizátor, což vede k lepší a konzistentní přesnosti než základní linie, zejména když je velikost tréninkové sady malá. Demonstrujeme efektivitu zarovnání optimalizovaného LPL na sémantické textové podobnosti (STS), inferenci přirozeného jazyka (SNLI), inferenci vícežánrových jazyků (MNLI) a zarovnání vícejazyčných slov (CLA) s konzistentními zlepšeními a nalezením až 16% zlepšení nad naší základní linií v nižších nastaveních zdrojů.', 'jv': 'Speaking politenessoffpolite"), and when there is a change ("assertivepoliteness Digawe Awak dhéwé éntuk LPL basan akèh dumadhing vector-terakhin langgambar luwih ingkang vector Awak dhéwé éntukno sistem sing nggawe LPL-Optimalisi sing luwih akeh gambar semantar, nglanga luwih dumadhi (S NLI), nggawe luwih akeh multigenr (MNLI) lan uga-langa sing luwih dumadhi kanggo nyebuturan langgar sampeyan (CLA) seneng langgar sampeyan luwih dumadhi, akeh dumadhi tanggal 16% kanggo nyebuturan kanggo awakdhéwé kanggo awakdhéwé sing', 'sk': 'Predstavljamo lokalno ohranjanje izgube (LPL), ki izboljšuje poravnavo med vgradnjo vektorskega prostora ob ločevanju nepopravljenih predstavitev. Glede na dva vgrajena kolektorja LPL optimizira model za projiciranje vgrajevanja in ohranjanje lokalne soseske, medtem ko poravnava enega kolektorja z drugim. To zmanjša skupno velikost nabora podatkov, potrebnega za uskladitev obeh opravil, kot je čezjezična poravnava besed. Pokazali smo, da poravnava na osnovi LPL med vhodnimi vektorskimi prostori deluje kot regulator, kar vodi do boljše in dosledne natančnosti kot osnovna vrednost, še posebej kadar je velikost nabora treningov majhna. Prikazujemo učinkovitost LPL optimizirane poravnave na semantični podobnosti besedila (STS), sklepanje naravnega jezika (SNLI), sklepanje večžanrskega jezika (MNLI) in čezjezično poravnavanje besed (CLA), kar kaže dosledne izboljšave, pri čemer smo ugotovili do 16% izboljšanje v primerjavi z našim izhodiščem v nižjih nastavitvah virov.', 'he': 'אנחנו מציגים אובדן שמשמר מקום (LPL) שמשתפר את ההתקשרות בין התקפות חלל ווקטורים בזמן הפרידה מייצגים לא קשורים. בהתחשב בשני מרכיבים מוכנים מראש, LPL אופטימיזם מודל כדי לפרויקט מרכיב ולשמור על השכונה המקומית שלו זה מפחיד את הגודל הכללי של קבוצת הנתונים הנדרשת כדי להאמין את שניהם במשימות כמו התאמת מילים דרך שפתיים. אנחנו מראים שההתאמה המבוססת על LPL בין מרחבי הוקטורים הכניסה פועלת כחוקר, שמוביל לדיוקת טובה ויותרת יותר מהתחילה, במיוחד כאשר הגודל של קבוצת האימונים הוא קטן. אנחנו מראים את היעילות של התאמה אופטימית LPL על דמיון טקסט סמנטי (STS), תוצאת שפת טבעית (SNLI), תוצאת שפת רבות-גנרס (MNLI) ו התאמת מילים דרך שפת (CLA) מראים שיפורים קבועים, מציאה עד 16% שיפור מעל הבסיס שלנו במקומות משאבים נמוכים יותר.', 'ha': "Tuna halarce wani lokal da ke tsare hasara (LPL) wanda ke kyautata juyi a tsakanin fili mai ƙẽtare fili da ke rarrabe masu belgili. Gida duk biyu aka suranta wanda aka shigar da shi, LPL yana amfani da wani motel wa ya zaɓe wani shirin ya yi amfani da shi, kuma yana tsare shi a lokacin da ya sami wata biyu zuwa wani. Wannan yana ƙarantar girmar da aka ƙayyade tsarin da aka buƙata zuwa juyi biyu cikin aikin da aka kama da tsaye maganar tsohonlanguage. Tuna nũna a samansa da LPL a tsakanin filayen cikin cikin shirin ayuka na cikin shirin ayuka, yana mai amfani da shiryarwa, kuma yana da tsari mai daidai daga rubutun, hususann idan girma ya zama ƙaranci. Tuna nũna aikin juyi na LPL da aka yi kwaɗayi a cikin littãfin na semantic (SNLI), kasar nau'in lugha na natura (SNLI), kasar nau'in multi-genre lugha (MNLI) da kalma masu daidaita (CL) na nuna mafiya ƙaranci mai daidaita, na sami up to 16%  mafiya kyau a kanana ƙaranci na fassarar resource.", 'bo': 'ང་ཚོས་རང་ཁུལ་གྱི་གནས་ཡུལ་དེ་མཛོད་ཁང་ཞིག་སྟོན་པའི་རྐྱེན་ཚད་དང་ཁྱད་ནས་ཕན་ཚུན་བརྗོད་ཀྱི་གནས་ཡུལ་སྟོང་བཅོས་པ LPL སྔོན་ཤུགས་ཅན་གྱི་གནས་སྡུད་བཟོ་བཅོས་པ་གཉིས་ཀྱིས་རྣམ་པ་སྒྲིག་འཇུག་བྱེད་པ་དང་རང་ཁུལ་གྱི་ཁྱིམ་མཚེས་གཡས་གཡོན་ལ་སྒྲིག འདིས་སྐད་ཡིག་ཆ་གི་ཆེ་ཆུང་ཡོངས་ཀྱི་ཚད་ཆེ་ཆུང་དུ་གཏོང་ཐུབ་པའི་བྱ་འགུལ་ནང་དུ་ཡོད་པ We show that the LPL-based alignment between input vector spaces acts as a regularizer, leading to better and consistent accuracy than the baseline, especially when the size of the training set is small. ང་ཚོས་LPL དང་ཆེ་མཐུན་རིམ་གྱི་གྲལ་སྒྲིག་ནི་ཕན་ཚུན་ཕྲག་མཐུན་དང་མཐུན་རྐྱེན་ཡིག་གཟུགས།'}
{'en': 'Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning', 'ar': 'التعلم التلوي القائم على المسار لتعلم تضمين الكلمات خارج المفردات', 'es': 'Metalearning basado en la trayectoria para el aprendizaje de incrustación de palabras fuera del vocabulario', 'pt': 'Meta-aprendizagem baseada em trajetória para aprendizado de incorporação de palavras fora do vocabulário', 'fr': "Méta-apprentissage basé sur la trajectoire pour l'apprentissage par intégration de mots hors vocabulaire", 'ja': 'ボキャブラリーの単語埋め込み学習のための軌跡ベースのメタ学習', 'zh': '基于轨迹之元学,用于词汇外词嵌学', 'hi': 'प्रक्षेपवक्र-आधारित मेटा-आउट-ऑफ-शब्दावली शब्द एम्बेडिंग लर्निंग के लिए सीखना', 'ru': 'Метаобучение на основе траектории для обучения внедрению слов из словаря', 'ga': 'Meta-Fhoghlaim Bunaithe ar Thrácht ar Fhoghlaim Focal As Focal A Leabú', 'ka': 'Name', 'hu': 'Pályaalapú metatanulás a szókincsen kívüli szó beágyazásához', 'el': 'Μεταμάθηση βασισμένη στην τροχιά για εκμάθηση εκτός λεξιλογίου', 'it': "Meta-apprendimento basato su traiettoria per l'apprendimento di incorporazione di parole fuori dal vocabolario", 'lt': 'Trajektoriniu būdu pagrįstas metamokymasis ne žodžio įrašymo mokymuisi', 'mk': 'Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning', 'kk': 'Сөздерден тыс сөздерді ендіру үшін мета оқыту негізінде', 'ml': 'പുറത്തുകടക്കുന്ന വാക്ക് എംബെഡിങ് പഠിപ്പിക്കുന്നതിനുള്ള ട്രെക്ട്രെക്റ്ററില്\u200d അടിസ്ഥാനമായ മെറ', 'ms': 'Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning', 'mn': 'Үнэндээ биш үгийн суралцах мета суралцах', 'pl': 'Meta-learning oparty na trajektorii dla nauki osadzania słów poza słownikiem', 'no': 'Comment', 'ro': 'Meta-învățare bazată pe traiectorie pentru încorporarea cuvintelor în afara vocabularului', 'sr': 'Naučenje meta-naučenja za uključenje rijeèi na putu', 'si': 'Name', 'so': 'Orod-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning', 'ta': 'வெளியேற்ற சொல்லாக்கு வார்த்தை உட்பொதிந்த கற்றுக்கான பாதையிலான மெடா கற்றுக்கொள்ளும்', 'ur': 'بیرون وکلاری لکھنے کے لئے تراژیکٹری-بنیادی مٹا-سیکھنے', 'sv': 'Bananbaserad metainlärning för Word Embedding Learning utanför ordförrådet', 'mt': 'Meta-Tagħlim Ibbażat fuq it-Trajettorja għal Tagħlim Mibarra mill-Vokabulari', 'uz': 'Name', 'vi': 'Quỹ đạo về siêu giáo dục cho ngôn ngữ thoát từ', 'bg': 'Метаобучение, базирано на траектория, за изучаване извън речника за вграждане на думи', 'hr': 'Metanaučenje na putu na osnovu meta-učenja za učenje izvan riječi', 'da': 'Banebaseret Meta-Learning for Out-Of-Vocabulary Word Embedding Learning', 'nl': 'Trajectorie-based meta-learning voor het inbedden van woorden buiten de woordenschat', 'id': 'Meta-Learning Berdasarkan Trajeksi untuk Learning Embedding Word Out-Of-Vocabulary', 'ko': '궤적 기반 어휘표 외 단어 삽입원 학습', 'fa': 'آموزش Meta-Learning Based-Trajectory for Out of Vocabulary Word Embedding Learning', 'de': 'Trajektorienbasiertes Meta-Learning für das Einbetten von Wörtern außerhalb des Wortschatzes', 'sw': 'Ufunzi wa Meta-Kutumiwa na Utoaji wa Kireno', 'sq': 'Metamësim bazuar në trajektori për mësimin e fjalëve jashtë fjalëkalimit', 'tr': 'Sözlerden daşary sözleri üçin Meta-Öwrenmek', 'am': 'መግለጫ', 'af': 'Name', 'bn': 'শব্দভাণ্ডার থেকে বেরিয়ে যাওয়ার জন্য ভিত্তিক মেটা শিক্ষা', 'az': 'D톛rgah캼 S칬zl칲k 캻fad톛 칐yr톛nm톛si 칲칞칲n Meta-칐yr톛nm톛si', 'hy': 'Խոսքերից դուրս ընդգրկվող բառերի մետասովորելը', 'cs': 'Metaučení založené na trajektorii pro vkládání slov mimo slovní zásobu', 'bs': 'Metanaučenje na putu na osnovu meta-učenja za učenje izvan riječi', 'et': 'Trajektoripõhine metaõpe sõnavara väliseks sõnavara manustamiseks', 'ca': 'Metaaprenentatge basat en la trajectòria per aprendre paraules incorporades fora del vocabulari', 'fi': 'Reittiohjainen metaoppiminen sanaston ulkopuoliseen sanaston upotusoppimiseen', 'jv': 'Layout', 'he': 'Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning', 'bo': 'Out-of-Vocabulary Word Embedding Learning', 'sk': 'Metaučenje na podlagi poti za učenje vključevanja besed zunaj besedišča', 'ha': 'KCharselect unicode block name'}
{'en': 'Word embedding learning methods require a large number of occurrences of a word to accurately learn its embedding. However, out-of-vocabulary (OOV) words which do not appear in the training corpus emerge frequently in the smaller downstream data. Recent work formulated OOV embedding learning as a few-shot regression problem and demonstrated that  meta-learning  can improve results obtained. However, the  algorithm  used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradient steps are used for parameter updates. In this work, we propose the use of Leap, a meta-learning algorithm which leverages the entire trajectory of the learning process instead of just the beginning and the end points, and thus ameliorates these two issues. In our experiments on a benchmark OOV embedding learning dataset and in an extrinsic evaluation, Leap performs comparably or better than MAML. We go on to examine which contexts are most beneficial to learn an OOV embedding from, and propose that the choice of contexts may matter more than the meta-learning employed.', 'pt': 'Os métodos de aprendizado de incorporação de palavras exigem um grande número de ocorrências de uma palavra para aprender com precisão sua incorporação. No entanto, palavras fora do vocabulário (OOV) que não aparecem no corpus de treinamento surgem com frequência nos dados menores de downstream. Trabalhos recentes formularam o aprendizado de incorporação OOV como um problema de regressão de poucos tiros e demonstraram que o meta-aprendizado pode melhorar os resultados obtidos. No entanto, o algoritmo usado, meta-aprendizagem agnóstica de modelo (MAML) é conhecido por ser instável e ter um desempenho pior quando um grande número de etapas de gradiente é usado para atualizações de parâmetros. Neste trabalho, propomos o uso do Leap, um algoritmo de meta-aprendizagem que potencializa toda a trajetória do processo de aprendizagem ao invés de apenas os pontos inicial e final, e assim ameniza essas duas questões. Em nossos experimentos em um conjunto de dados de aprendizado de incorporação OOV de referência e em uma avaliação extrínseca, o Leap tem um desempenho comparável ou melhor que o MAML. Continuamos examinando quais contextos são mais benéficos para aprender uma incorporação OOV e propomos que a escolha de contextos pode importar mais do que a meta-aprendizagem empregada.', 'es': 'Los métodos de aprendizaje de incrustación de palabras requieren un gran número de ocurrencias de una palabra para aprender con precisión su incrustación. Sin embargo, las palabras fuera de vocabulario (OOV) que no aparecen en el cuerpo de entrenamiento aparecen con frecuencia en los datos descendentes más pequeños. Un trabajo reciente formuló el aprendizaje de incrustación de OOV como un problema de regresión de pocos pasos y demostró que el meta-aprendizaje puede mejorar los resultados obtenidos. Sin embargo, se sabe que el algoritmo utilizado, meta-aprendizaje independiente del modelo (MAML), es inestable y tiene un rendimiento peor cuando se usa un gran número de pasos de gradiente para las actualizaciones de parámetros. En este trabajo, proponemos el uso de Leap, un algoritmo de meta-aprendizaje que aprovecha toda la trayectoria del proceso de aprendizaje en lugar de solo los puntos de inicio y final, y por lo tanto mejora estos dos problemas. En nuestros experimentos en un OOV de referencia que incorpora un conjunto de datos de aprendizaje y en una evaluación extrínseca, Leap funciona de manera comparable o mejor que MAML. Continuamos examinando qué contextos son más beneficiosos para aprender una incrustación de OOV, y proponemos que la elección de los contextos puede importar más que el meta-aprendizaje empleado.', 'ar': 'تتطلب أساليب التعلم لتضمين الكلمات عددًا كبيرًا من تكرارات الكلمة لتعلم تضمينها بدقة. ومع ذلك ، فإن الكلمات الخارجة عن المفردات (OOV) التي لا تظهر في مجموعة التدريب تظهر بشكل متكرر في بيانات المصب الأصغر. صاغ العمل الأخير التعلم المضمن لـ OOV باعتباره مشكلة انحدار قليلة اللقطات وأثبت أن التعلم التلوي يمكن أن يحسن النتائج التي تم الحصول عليها. ومع ذلك ، من المعروف أن الخوارزمية المستخدمة ، والتعلم التلوي الحيادي النموذج (MAML) غير مستقرة وتعمل بشكل أسوأ عند استخدام عدد كبير من خطوات التدرج لتحديثات المعلمات. في هذا العمل ، نقترح استخدام خوارزمية Leap ، وهي خوارزمية التعلم التلوي التي تعزز المسار الكامل لعملية التعلم بدلاً من مجرد نقاط البداية والنهاية ، وبالتالي تعمل على تحسين هاتين المسألتين. في تجاربنا على معيار OOV يتضمن مجموعة بيانات التعلم وفي تقييم خارجي ، يعمل Leap بشكل مشابه أو أفضل من MAML. ننتقل إلى دراسة السياقات الأكثر فائدة لتعلم تضمين OOV ، ونقترح أن اختيار السياقات قد يكون أكثر أهمية من التعليم التلوي المستخدم.', 'fr': "Les méthodes d'apprentissage de l'intégration de mots nécessitent un grand nombre d'occurrences d'un mot pour apprendre précisément son intégration. Cependant, les mots hors vocabulaire (OOV) qui n'apparaissent pas dans le corpus de formation apparaissent fréquemment dans les petites données en aval. Des travaux récents ont formulé l'intégration de l'apprentissage OOV comme un problème de régression ponctuel et ont démontré que le méta-apprentissage peut améliorer les résultats obtenus. Cependant, l'algorithme utilisé, le méta-apprentissage indépendant des modèles (MAML), est connu pour être instable et moins performant lorsqu'un grand nombre d'étapes de gradient sont utilisées pour les mises à jour des paramètres. Dans ce travail, nous proposons l'utilisation de Leap, un algorithme de méta-apprentissage qui tire parti de la trajectoire complète du processus d'apprentissage au lieu des points de départ et de fin, et améliore ainsi ces deux problèmes. Dans nos expériences sur un jeu de données d'apprentissage intégré OOV de référence et dans une évaluation extrinsèque, Leap a des performances comparables ou meilleures que MAML. Nous examinons ensuite quels contextes sont les plus bénéfiques pour apprendre à intégrer un OOV et proposons que le choix des contextes peut avoir plus d'importance que le méta-apprentissage employé.", 'zh': '词嵌学术须多出一词方能正学。 然未见训练语料库中词汇量外 (OOV) 单词常见下流数中。 近事销OOV为少镜头归,明元学可改也。 然已知元学(MAML)算法不定,多用梯度步长参数新时性差。 于是建议用Leap,此元学算法,其用学之全轨,非徒始终,改善二者也。 于我等准OOV嵌学数集实验与外在评估中,Leap与MAML相若。 臣等考上下文最利于学OOV嵌,而上下文择可重于所用元学。', 'ja': '単語の埋め込み学習方法は、その埋め込みを正確に学習するために単語の出現回数を大量に必要とします。 しかしながら、トレーニングコーパスに現れない語彙外（ OOV ）単語は、より小さな下流データに頻繁に現れる。 最近の研究では、OOV埋め込み学習を数発回帰問題として定式化し、メタ学習が得られた結果を改善できることを実証した。 しかしながら、使用されるアルゴリズム、モデル非推論メタ学習（ MAML ）は、不安定であり、多数の勾配ステップがパラメータの更新に使用されるときに悪化することが知られている。 この研究では、LEAPというメタラーニングアルゴリズムを使用することを提案しています。このアルゴリズムは、学習プロセスの軌跡全体を利用して、開始点と終了点だけでなく、これら2つの問題を改善します。 ベンチマークOOV埋め込み学習データセットの実験および外部評価では、LEAPはMAMLと同等またはより優れたパフォーマンスを発揮します。 次に、どのコンテキストがOOV埋め込みを学ぶのに最も有益かを検討し、採用されているメタラーニングよりもコンテキストの選択が重要である可能性があることを提案します。', 'hi': 'शब्द एम्बेडिंग सीखने के तरीकों के लिए एक शब्द की बड़ी संख्या में घटनाओं की आवश्यकता होती है ताकि इसके एम्बेडिंग को सही ढंग से सीखा जा सके। हालांकि, आउट-ऑफ-शब्दावली (ओओवी) शब्द जो प्रशिक्षण कॉर्पस में दिखाई नहीं देते हैं, वे छोटे डाउनस्ट्रीम डेटा में अक्सर उभरते हैं। हाल के काम ने कुछ-शॉट प्रतिगमन समस्या के रूप में सीखने को एम्बेड करने के लिए ओओवी तैयार किया और प्रदर्शित किया कि मेटा-लर्निंग प्राप्त परिणामों में सुधार कर सकता है। हालांकि, उपयोग किए जाने वाले एल्गोरिथ्म, मॉडल-अज्ञेयवादी मेटा-लर्निंग (एमएएमएल) को अस्थिर होने के लिए जाना जाता है और पैरामीटर अपडेट के लिए बड़ी संख्या में ग्रेडिएंट चरणों का उपयोग किए जाने पर बदतर प्रदर्शन करता है। इस काम में, हम लीप के उपयोग का प्रस्ताव करते हैं, एक मेटा-लर्निंग एल्गोरिथ्म जो केवल शुरुआत और अंत बिंदुओं के बजाय सीखने की प्रक्रिया के पूरे प्रक्षेपवक्र का लाभ उठाता है, और इस प्रकार इन दो मुद्दों को सुधारता है। एक बेंचमार्क ओओवी एम्बेडिंग लर्निंग डेटासेट पर हमारे प्रयोगों में और एक बाहरी मूल्यांकन में, लीप तुलनात्मक रूप से या एमएएमएल से बेहतर प्रदर्शन करता है। हम यह जांचने के लिए आगे बढ़ते हैं कि कौन से संदर्भ एक ओओवी एम्बेडिंग सीखने के लिए सबसे फायदेमंद हैं, और प्रस्ताव करते हैं कि संदर्भों की पसंद मेटा-लर्निंग से अधिक मायने रख सकती है।', 'ru': 'Методы обучения встраиванию слов требуют большого количества вхождений слова, чтобы точно изучить его встраивание. Тем не менее, слова из словаря (OOV), которые не появляются в учебном корпусе, часто появляются в меньших данных ниже по потоку. Недавняя работа сформулировала обучение внедрению OOV как проблему регрессии с несколькими выстрелами и продемонстрировала, что метаобучение может улучшить полученные результаты. Однако используемый алгоритм, модельно-диагностическое метаобучение (MAML), как известно, является нестабильным и работает хуже, когда для обновления параметров используется большое количество градиентных шагов. В этой работе мы предлагаем использовать Leap, мета-учебный алгоритм, который использует всю траекторию процесса обучения вместо только начальной и конечной точек, и таким образом улучшает эти две проблемы. В наших экспериментах по внедрению эталонного набора обучающих данных OOV и во внешней оценке Leap работает сравнительно или лучше, чем MAML. Далее мы рассмотрим, какие контексты являются наиболее полезными для изучения встраивания OOV, и предположим, что выбор контекстов может иметь большее значение, чем используемое метаобучение.', 'ga': 'Teastaíonn líon mór tarluithe focal le modhanna foghlama leabú focal chun a leabú a fhoghlaim go beacht. Mar sin féin, is minic a thagann focail as-foclóra (OOV) nach bhfuil sa chorpas oiliúna chun cinn sna sonraí níos lú iartheachtacha. Chruthaigh obair a rinneadh le déanaí an fhoghlaim OOV a leabú mar fhadhb cúlchéimnithe cúpla seat agus léirigh sé gur féidir leis an meitea-fhoghlaim na torthaí a fhaightear a fheabhsú. Mar sin féin, is eol, áfach, go bhfuil an t-algartam a úsáidtear, meitea-fhoghlaim samhail-agnostic (MAML) éagobhsaí agus go ndéanann sé níos measa nuair a úsáidtear líon mór céimeanna grádáin le haghaidh nuashonruithe paraiméadar. Sa saothar seo, molaimid go n-úsáidfí Leap, algartam meitea-fhoghlama a ghiarálann conair iomlán an phróisis foghlama seachas díreach na pointí tosaigh agus deiridh, agus mar sin a leasaíonn an dá shaincheist seo. Inár dturgnaimh ar thacar sonraí foghlama leabú OOV tagarmhairc agus i meastóireacht eistreach, feidhmíonn Leap inchomparáide nó níos fearr ná MAML. Leanaimid ar aghaidh le scrúdú a dhéanamh ar na comhthéacsanna is tairbhí chun leabú OOV a fhoghlaim uathu, agus molaimid go bhféadfadh go mbeadh níos mó tábhachta ag baint leis an rogha comhthéacs ná leis an meiteafhoghlaim a úsáidtear.', 'hu': 'A szóbeágyazási tanulási módszerek nagyszámú előfordulást igényelnek egy szó pontos megismeréséhez. Azonban a szókincsen kívüli (OOV) szavak, amelyek nem jelennek meg a képzési korpuszban, gyakran jelennek meg a kisebb downstream adatokban. A közelmúltbeli munkák az OOV beágyazásával kapcsolatos tanulást néhány lehetséges regressziós problémaként fogalmazták meg, és bebizonyították, hogy a metatanulás javíthatja az elért eredményeket. Azonban az alkalmazott algoritmus, a modell-agnosztikus metatanulás (MAML) ismert, hogy instabil és rosszabb teljesítményt jelent, ha nagy számú gradiens lépést használnak a paraméterek frissítéséhez. Ebben a munkában javasoljuk a Leap, egy metatanulási algoritmus használatát, amely a tanulási folyamat teljes pályáját kihasználja a kezdet és a végpont helyett, és így javítja ezt a két problémát. A tanulási adatokat beágyazó OOV benchmark kísérleteinkben és egy külső értékelésben a Leap összehasonlíthatóan vagy jobban teljesít, mint a MAML. Folytatjuk azt a vizsgálatot, hogy mely kontextusok a leginkább hasznosak az OOV beágyazásának tanulására, és javasoljuk, hogy a kontextusok kiválasztása jobban számíthat, mint az alkalmazott meta-tanulás.', 'ka': 'სიტყვების გასწავლების მეტი უნდა გასწავლის დიდი რაოდენობა სიტყვების მოვლენები, რომელიც მართლად გასწავლის შემდეგ. მაგრამ, სიტყვებლის გარეშე (OOV) სიტყვები, რომელიც არ ჩვენებენ კოპუსში განაკეთებული სიტყვებში, ხშირად ჩვენებენ პატარა ქვემოთ მონაცემებში. მიმდინარე სამუშაო მუშაო OOV სწავლება როგორც რამდენიმე რეგრესიის პრობლემა და გამოჩვენება, რომ მეტა სწავლება შეიძლება გაუკეთოთ მიღებული შედეგი. მაგრამ, გამოყენებული ალგორიტიმ, მოდელ- agnostic მეტა- სწავლება (MAML) უცნობიან არსებულია და უკეთესი გავაკეთება, როდესაც პარამეტრების ახლებისთვის გამოყენება დიდი რაოდენობ ამ სამუშაოში, ჩვენ მივიღებთ "Leap"-ის გამოყენება, მეტა-სწავლების ალგორიტიმ, რომელიც სწავლების პროცესის ყველა რპაექტორიის გამოყენება მხოლოდ დასაწყება და დასაწყება, და ამიტომ ამ ორი პრო ჩვენი ექსპერიმენტებში, რომელიც OOV-ს სწავლის მონაცემების შესაბამისად და ექსტრინსიკური განსაბამისად, Leap გავაკეთება მაMLზე შესაბამისად ან უკეთესი. ჩვენ გავაკეთებთ, რომელიც კონტექსტები უფრო საფრთხოლოდ გავისწავლოთ OOV, რომელიც შეიძლება შეიძლება მონიშნოთ კონტექსტების შესაძლებლობა უფრო მნიშვნელოვანი, ვიდრე მო', 'it': "I metodi di apprendimento per incorporare parole richiedono un gran numero di occorrenze di una parola per imparare con precisione la sua incorporazione. Tuttavia, le parole fuori vocabolario (OOV) che non compaiono nel corpus di formazione emergono frequentemente nei dati a valle più piccoli. Un recente lavoro ha formulato OOV embedding learning come un problema di regressione di pochi colpi e ha dimostrato che il meta-apprendimento può migliorare i risultati ottenuti. Tuttavia, l'algoritmo utilizzato, il meta-apprendimento agnostico modello (MAML) è noto per essere instabile e avere prestazioni peggiori quando un gran numero di passaggi gradienti sono utilizzati per gli aggiornamenti dei parametri. In questo lavoro, proponiamo l'utilizzo di Leap, un algoritmo di meta-apprendimento che sfrutta l'intera traiettoria del processo di apprendimento invece che i punti iniziali e finali, e quindi migliora queste due problematiche. Nei nostri esperimenti su un benchmark OOV embedding data set di apprendimento e in una valutazione estrinseca, Leap ha prestazioni paragonabili o migliori rispetto a MAML. Continuiamo ad esaminare quali contesti sono più utili per imparare un embedding OOV, e proponiamo che la scelta dei contesti possa essere più importante del meta-apprendimento impiegato.", 'mk': 'Методите за учење на зборови бараат голем број појавувања на збор за прецизно да се научи неговото вградување. Сепак, зборовите надвор од зборовите (ООВ) кои не се појавуваат во корпусот за обука често се појавуваат во помалите податоци. Неодамнешната работа формулираше ООВ, вклучувајќи го учењето како проблем со регресијата со неколку снимки и покажа дека метаучењето може да ги подобри постигнатите резултати. Сепак, користениот алгоритм, модел-агностичко метаучење (MAML) е познат како нестабилен и се врши полошо кога се користат голем број чекори од градиентите за наградување на параметрите. Во оваа работа, предложуваме употреба на Leap, алгоритм за метаучење кој ја користи целата траекторија на процесот на учење наместо само почетокот и крајните точки, и така ги подобрува овие два прашања. Во нашите експерименти на benchmark OOV вклучувајќи податоци за учење и во екстринзичка проценка, Leap функционира споредливо или подобро од MAML. Продолжуваме да ги испитуваме кои контексти се најкорисни за да се научи ООВ вклопување од, и да предложиме изборот на контексти да има значење повеќе од метаучењето користено.', 'kk': 'Сөздерді ендіру әдістері оның ендіруді дұрыс оқыту үшін сөздің көп саны керек. Бірақ оқыту корпусында көрсетілмеген сөздер жоқ (ООВ) сөздері көбірек төменгі деректерінде көрсетіледі. Жуырдағы жұмыс формуляцияланған ООВ бірнеше рет регрессия мәселесі болып, мета оқыту нәтижесін жақсартуға болады дегенді көрсету. Бірақ қолданылатын алгоритм, параметрлерді жаңарту үшін үлкен градиенттің қадамдары қолданылатында үлгі- агностикалық мета- оқыту (MAML) деген алгоритм деп белгіледі. Бұл жұмыс ішінде "Leap" дегенді қолданып, мета оқыту алгоритмі қолданып, оқыту процесінің толық траекториясын бастау мен соңғы нүктелердің орнына қолданып, осы екі мәселелерді амелиореттеді. Біздің OOV бағдарламасында оқыту деректер жиынын ендіру және сыртқы оқу үшін, Leap MAML дан салыстырылып не жақсы орындалады. Біз қай контексттерді оқу үшін ең пайдалы болатынын тексеру үшін жұмыс істеп, мәтіндердің таңдауы мета оқыту мүмкіндігінен артық болуын талап етеміз.', 'lt': 'Siekiant tiksliai išsiaiškinti žodžio įtraukimo metodus, reikia daug žodžio pasikartojimų. Tačiau mokymo korpuse nenurodyti žodžiai ne žodyne (OOV) dažnai atsiranda mažesniuose tolesniuose duomenise. Neseniai atliktas OOV darbas, kuriame mokymasis buvo įtraukiamas kaip kelių vaizdų regresijos problem a, ir parodė, kad metamokymasis gali pagerinti pasiektus rezultatus. However, the algorithm used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradient steps are used for parameter updates.  Šiame darbe siūlome naudoti Leap, metamokymosi algoritmą, kuris sutelkia visą mokymosi proceso trajektoriją vietoj tik pradžios ir pabaigos taškų ir taip pagerina šiuos du klausimus. Mūsų eksperimentuose su OOV lyginamuoju rodikliu, įtraukiančiu mokymosi duomenų rinkinį ir išoriniame vertinime Leap veikia palyginamai arba geriau nei MAML. Mes toliau nagrinėsime, kurios aplinkybės yra naudingiausios mokytis iš OOV, ir siūlome, kad aplinkybių pasirinkimas galėtų būti svarbesnis nei naudojamas metamokymasis.', 'el': 'Οι μέθοδοι εκμάθησης ενσωμάτωσης λέξεων απαιτούν μεγάλο αριθμό εμφανίσεων μιας λέξης για να μάθουν με ακρίβεια την ενσωμάτωσή της. Ωστόσο, λέξεις εκτός λεξιλογίου (OOV) που δεν εμφανίζονται στο εκπαιδευτικό σώμα εμφανίζονται συχνά στα μικρότερα μεταγενέστερα δεδομένα. Πρόσφατες εργασίες διατύπωσαν την ενσωμάτωση της μάθησης ως πρόβλημα παλινδρόμησης λίγων πυροβολισμών και κατέδειξαν ότι η μετα-μάθηση μπορεί να βελτιώσει τα αποτελέσματα που επιτυγχάνονται. Ωστόσο, ο αλγόριθμος που χρησιμοποιείται, η μετα-μάθηση χωρίς μοντέλο (είναι γνωστό ότι είναι ασταθής και αποδίδει χειρότερα όταν χρησιμοποιείται μεγάλος αριθμός βημάτων διαβάθμισης για ενημερώσεις παραμέτρων. Στην παρούσα εργασία, προτείνουμε τη χρήση ενός αλγόριθμου μετα-μάθησης που αξιοποιεί ολόκληρη την τροχιά της μαθησιακής διαδικασίας αντί μόνο την αρχή και το τέλος, και έτσι βελτιώνει αυτά τα δύο ζητήματα. Στα πειράματά μας σε ένα σύνολο δεδομένων αναφοράς που ενσωματώνει τη μάθηση και σε μια εξωτερική αξιολόγηση, το Leap αποδίδει συγκριτικά ή καλύτερα από το MAML. Συνεχίζουμε να εξετάζουμε ποια πλαίσια είναι πιο ωφέλιμα για να μάθουμε μια ενσωμάτωση OOV και προτείνουμε ότι η επιλογή των πλαισίων μπορεί να έχει μεγαλύτερη σημασία από τη μετα-μάθηση που χρησιμοποιείται.', 'ms': 'Kaedah penerbangan perkataan memerlukan sejumlah besar kejadian perkataan untuk mempelajari penerbangan secara tepat. Namun, perkataan luar-dari-vocabulari (OOV) yang tidak muncul dalam korpus latihan muncul sering dalam data turun yang lebih kecil. Kerja baru-baru ini membentuk OOV memasukkan pembelajaran sebagai masalah regresi beberapa tembakan dan menunjukkan bahawa pembelajaran meta boleh meningkatkan keputusan yang dicapai. Namun, algoritma yang digunakan, model-agnostic meta-learning (MAML) diketahui sebagai tidak stabil dan melakukan lebih buruk bila bilangan besar langkah gradien digunakan untuk kemaskini parameter. In this work, we propose the use of Leap, a meta-learning algorithm which leverages the entire trajectory of the learning process instead of just the beginning and the end points, and thus ameliorates these two issues.  Dalam eksperimen kami pada tanda referensi OOV memasukkan set data pembelajaran dan dalam penilaian ekstrinsik, Leap berfungsi dengan bandingan atau lebih baik daripada MAML. Kita terus memeriksa konteks mana yang paling berguna untuk belajar bahasa OOV, dan menyarankan bahawa pilihan konteks mungkin lebih penting daripada pembelajaran meta yang digunakan.', 'mt': "Il-metodi tat-tagħlim dwar l-inkorporazzjoni tal-kliem jeħtieġu numru kbir ta’ okkorrenzi ta’ kelma biex titgħallem b’mod preċiż l-inkorporazzjoni tagħha. Madankollu, kliem barra mill-vokabulari (OOV) li ma jidhrux fil-korp tat-taħriġ joħorġu ta’ spiss fid-dejta iżgħar downstream. Xogħol reċenti fformula OOV li jinkorpora t-tagħlim bħala problem a ta’ rigressjoni b’ftit skopijiet u wera li t-tagħlim meta jista’ jtejjeb ir-riżultati miksuba. However, the algorithm used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradient steps are used for parameter updates.  F’din il-ħidma, nipproponu l-użu ta’ Leap, algoritmu ta’ meta-tagħlim li jtejjeb it-trajettorja kollha tal-proċess ta’ tagħlim minflok biss il-bidu u l-punti finali, u b’hekk itejjeb dawn iż-żewġ kwistjonijiet. Fl-esperimenti tagħna fuq punt ta’ riferiment OOV li jinkorpora sett ta’ dejta tat-tagħlim u f’evalwazzjoni esterna, Leap iwettaq prestazzjoni komparabbli jew a ħjar mill-MAML. Aħna nkomplu jeżaminaw liema kuntesti huma l-aktar ta' benefiċċju biex nitgħallmu OOV li jinkorpora minnu, u nipproponu li l-għażla tal-kuntesti tista' tkun aktar importanti mill-meta-tagħlim użat.", 'ml': 'പഠിപ്പിക്കുന്ന വാക്കുകള്\u200d അതിന്റെ അകത്തേക്ക് കൃത്യമായി പഠിക്കാന്\u200d ഒരു വാക്കിന്റെ വലിയ സംഭവം ആവശ്യമാണ്. എന്നാലും പരിശീലനത്തിന്റെ കോര്\u200dപ്പുസില്\u200d പ്രത്യക്ഷപ്പെടാത്ത വാക്കുകള്\u200d പുറത്തിറങ്ങുന്ന വാക്കുകള്\u200d എപ്പോഴും ചെറിയ നദിയ അടുത്ത പ്രവര്\u200dത്തിക്കുന്ന പണിയില്\u200d ഒവി പഠിപ്പിക്കുന്നത് കുറച്ച് വെടിവെക്കപ്പെട്ട രീക്രഷന്\u200d പ്രശ്നങ്ങളായി പഠിക് എന്നാലും ഉപയോഗിക്കുന്ന ആല്\u200dഗോരിതം, മോഡല്\u200d- ആഗ്നോസ്റ്റിക്ക് മെറ്റാ പഠിപ്പിക്കുന്ന (MAML) അസ്ഥിരമായിരിക്കുന്നു എന്നും വലിയ സംഖ്യ ഗ ഈ ജോലിയില്\u200d, ലീപിന്റെ ഉപയോഗിക്കാന്\u200d ഞങ്ങള്\u200d പ്രായശ്ചിത്തം ചെയ്യുന്നു. അത് പഠിക്കുന്ന പ്രക്രിയയുടെ മുഴുവന്\u200d പ്രക്രിയയുടെയും പ്രക്രിയയുടെയ നമ്മുടെ പരീക്ഷണങ്ങളില്\u200d പഠിക്കുന്നതിന്\u200dറെ ഡാറ്റാസെറ്റിലും പുറത്തുള്ള വിലാസങ്ങളിലും, ലീപ് MAML കൂടുതല്\u200d ചേര്\u200dന്നതിനെക്കാള്\u200d നല നമ്മള്\u200d പരിശോധിക്കാന്\u200d പോകുന്നു ഏത് പ്രധാനപ്പെട്ട കാര്യങ്ങള്\u200dക്ക് ഏറ്റവും പ്രയോജനകരമാണെന്ന്. ഒരു ഓഓവിയില്\u200d നിന്നും പഠിക്കാന്\u200d പ്രായശ', 'no': 'Ordinnbygging av læringsmetodar krev eit stor tal oppføringar av eit ord for å lære nøyaktig innbygginga. Dette er imidlertid utrytta ordet (OOV) som ikkje dukkar opp i øvingskorpusen, ofte dukkar opp i dei mindre nedstrekingsdata. Nyleg utformert arbeid med OOV- innbygging som eit få- skritt regresjonsproblem og demonstrert at metalæring kan forbetra resultata som er henta. Det brukte algoritmen er imidlertid kjent til at modellen-agnostisk metalæring (MAML) er ikkje stabil og utfører verre når ein stor tal fargeovergangsteg vert brukt for oppdateringar av parametrar. I denne arbeiden foreslår vi bruk av Leap, ein metalæringsalgoritme som leverer heile trajektoren av læringsprosessen i staden for berre startpunktet og sluttpunktet, og dermed gjer desse to problema. I våre eksperimenter på eit benchmarkt OOV som innebygger læringsdatasett og i eit ekstrinsk evaluering utfører Leap sammenlignbare eller bedre enn MAML. Vi går fram til å undersøke kva kontekstar er mest nyttig for å lære ein OOV-innbygging frå, og foreslår at valet av kontekstar kan vere meir viktig enn den arbeida metalæringa.', 'ro': 'Metodele de învățare prin încorporarea cuvintelor necesită un număr mare de evenimente ale unui cuvânt pentru a învăța cu precizie încorporarea acestuia. Cu toate acestea, cuvintele în afara vocabularului (OOV) care nu apar în corpul de formare apar frecvent în datele mai mici din aval. Lucrările recente au formulat OOV încorporarea învățării ca o problemă de regresie a câtorva lovituri și au demonstrat că meta-învățarea poate îmbunătăți rezultatele obținute. Cu toate acestea, algoritmul utilizat, meta-învățarea agnostică a modelului (MAML) este cunoscut ca fiind instabil și performanță mai rea atunci când un număr mare de pași de gradient sunt utilizați pentru actualizările parametrilor. În această lucrare, propunem utilizarea Leap, un algoritm de meta-învățare care valorifică întreaga traiectorie a procesului de învățare în loc de doar punctele de început și sfârșit, și astfel ameliorează aceste două probleme. În experimentele noastre pe un set de date de învățare OOV de referință și într-o evaluare extrinsică, Leap performează comparativ sau mai bine decât MAML. Continuăm să examinăm care contexte sunt cele mai benefice pentru a învăța o încorporare OOV și propunem că alegerea contextelor poate conta mai mult decât meta-învățarea folosită.', 'mn': 'Ингээд суралцах үгний аргыг тодорхой ойлгохын тулд маш олон үгний үйл явдал хэрэгтэй. Гэхдээ сургалтын корпус дээр харагдахгүй үгсийн бус хэлнүүд нь ихэвчлэн бага доорх өгөгдлийн дотор гардаг. Саяхан ажлын тодорхойлогдсон ООВ суралцах нь хэд хэдэн шалтгаан регрессийн асуудал болж, мета суралцах нь үр дүнг сайжруулж чадна гэдгийг харуулсан. Гэвч хэрэглэгдсэн алгоритм, загвар-агностик мета суралцах (MAML) нь тогтворгүй байх болон параметр шинэчлэхэд маш олон градиент алхам ашиглах үед илүү муу байдаг. Энэ ажил дээр бид Лип-ын хэрэглээ, мета суралцах алгоритм гэдэг нь суралцах процессийн бүх зам замыг зөвхөн эхлэл, төгсгөл цэгүүдийн оронд нөлөөлдөг. Тэгээд эдгээр хоёр асуудлыг сайжруулдаг. Бидний OOV-ын банклайн туршилтууд суралцах өгөгдлийн сан болон гадна дүгнэлт дээр Leap нь MAML аас илүү харьцангуй эсвэл илүү хийдэг. Бид юу нөхцөл байдлын тухай OOV-г сурахын тулд хамгийн ашигтай вэ гэдгийг шалгаж, мөн нөхцөл байдлын сонголт нь мета суралцагчийн ажиллагаагүйгээс илүү чухал байх боломжтой гэж санал болно.', 'si': 'වචනය සම්බන්ධ විධානය සම්බන්ධ විධානය අවශ්\u200dය වෙනවා වචනයක් විශාල සංඛ්\u200dයාවක් වගේ සංඛ්යා නමුත්, ප්\u200dරධාන කොර්පුස් වලින් පෙනුම් වෙන්නේ නැති වචන වචන (OOOv) වලින් ප්\u200dරධාන කොර්පුස් වලින් ප්\u200dරධාන වි ප්\u200dරශ්නයක් වෙනුවෙන් ප්\u200dරශ්නයක් විදියට පස්සේ ප්\u200dරශ්නයක් වෙනුවෙන් ප්\u200dරශ්නයක් වෙනුවෙන් ප්\u200dරශ්නයක් ව නමුත්, භාවිත වෙන්න ඇල්ගෝරිතම්, මොඩල්- agnoසික් මෙටා- ඉගෙනීම (MAML) දැනගත්තා නොස්ථීර වෙන්න හා වැඩිය ග්\u200dරේඩියාන්ඩ් ස මේ වැඩේ අපි ලීප් වලට ප්\u200dරයෝජනය කරන්න ප්\u200dරයෝජනය කරනවා, මෙටා ඉගෙන ගන්න ඇල්ගෝරිතම් එකක්, ඒ වගේම ඉගෙන ගන්න ප්\u200dරයෝජනයේ සියළු ප්\u200dරයෝජනය අපේ පරීක්ෂණයේ බෙන්ච්මාර්ක් OOව් එක්ක ඉගෙන ගන්න දත්ත සෙට් එක්ක සහ ප්\u200dරශ්නයක් තියෙන්නේ, ලිප් එක MAML වඩා සමාන්\u200dය විද අපි පරීක්ෂණය කරන්න යන්නේ කොච්චර ප්\u200dරයෝජනයක් වඩා ප්\u200dරයෝජනයක් තියෙන්නේ OOOv එක ඉගෙන ගන්න, සහ ප්\u200dරයෝජනයක් තෝරාගන්න පුළුව', 'so': 'Hadal ku qoran qaababka waxbarashada waxaa loo baahan yahay dhacdooyin badan oo eray ah si sax ah loo barto korriinka. Si kastaba ha ahaatee erayo aan qoraal ahayn (OOV) oo aan ka muuqanayn xiliga waxbarashada waxaa inta badan ka soo baxa macluumaadka hooseeya. Recent work formulated OOV embedding learning as a few-shot regression problem and demonstrated that meta-learning can improve results obtained.  Si kastaba ha ahaatee algorithiga lagu isticmaalo, tusaale-agnostic barashada meta-agnostic (MAML) waxaa loo yaqaan inuu suunsanaan yahay oo uu sameeyo wax ka sii daran marka loo isticmaalo tallaabooyin badan oo darajada ah oo loo isticmaalo baaritaanka cusub. Markaas waxan waxaynu horumarinaynaa isticmaalka Leap, qoraalka waxbarashada oo meta-meta-meta-barashada oo dhamaan waddooyinka waxbarashada, taas oo ka bedela labadaas arimo oo dhan. Imtixaankayada ku qoran OOV oo ku qoran taariikhda waxbarashada iyo qiimeynta dibadda ah, Leap wuxuu sameeyaa si u eg ama ka wanaagsan MAML. Waxaan baaraynaa in aan baarito qaababka aad u faa’iido badan in aad ka barato OOV oo ka soo baxaya, waxaana soo jeedinaynaa in doorashooyinka qoraalka ay muhiim u tahay wax ka badan waxbarashada dugsiga ah.', 'sr': 'Metode za učenje reèi zahtevaju veliki broj dogaðaja reèi da precizno nauèi njegovo ukljuèenje. Međutim, riječi koje se ne pojavljuju u trening korpusu često pojavljuju u manjim podacima. Nedavni rad formulisan OOV ugrađenje učenja kao problem a sa regresijom nekoliko snimaka i pokazao je da metaučenje može poboljšati dobijene rezultate. Međutim, korišteni algoritam, zna se da je model-agnostički metaučenje (MAML) nestabilan i da se pogoršava kada se koristi veliki broj koraka gradient a za aktualizaciju parametara. U ovom poslu predlažemo korištenje Lipa, algoritma metaučenja koji utiče na cijeli put procesa učenja umjesto samo početka i krajnje tačke, i tako ameliorira te dve probleme. U našim eksperimentima na benchmarku OOV-a koji ugrađuje komplet podataka o učenju i u ekstrinskoj procjeni, Leap izvodi usporedno ili bolje od MAML-a. Nastavljamo da pregledamo koji su konteksti najkorisniji da naučimo OOV koji se uključuje i predlažemo da izbor konteksta može biti važan više od zaposlenog metaučenja.', 'sv': 'Lärningsmetoder för att bädda in ord kräver ett stort antal förekomster av ett ord för att korrekt lära sig hur det bäddas in. Orden utanför ordförrådet (OOV) som inte förekommer i träningskorpusen framträder dock ofta i de mindre nedströmsdata. Nyligen formulerade OOV inbäddning lärande som ett par-shot regressionsproblem och visade att meta-lärande kan förbättra uppnådda resultat. Den algoritm som används är dock känd för att vara instabil och prestera sämre när ett stort antal gradientsteg används för parameteruppdateringar. I detta arbete föreslår vi användningen av Leap, en meta-inlärningsalgoritm som utnyttjar hela banan i inlärningsprocessen istället för bara början och slutpunkterna, och därmed förbättrar dessa två frågor. I våra experiment på en benchmark OOV inbäddad inlärningsdata och i en extern utvärdering presterar Leap jämförbart eller bättre än MAML. Vi går vidare med att undersöka vilka sammanhang som är mest fördelaktiga att lära sig en OOV inbäddning av, och föreslår att valet av sammanhang kan vara viktigare än det metalärande som används.', 'ta': 'வார்த்தை உள்ளடக்கும் கற்றல் முறைமைகளில் அதிகமான நிகழ்வுகள் தேவைப்படுகிறது அதன் உள்ளிடுதலை சரியாக கற்றுக் கொள்ள வ ஆனால், பயிற்சி குறியீட்டில் தெரியாத சொல்லாக்கம் வெளியேற்ற (OOV) வார்த்தைகள் வெளியேறும் சொற்கள் தடவையில் குறைந் சமீபத்தில் OOV கற்றுக்கொள்ளும் கற்றுக்கொள்ளும் சில சுட்டு திருப்பப்படுத்தல் பிரச்சனையாக வடிவமைக்கப்பட்டது மற்றும் ம However, the algorithm used, model- agnostic meta-learning (MAML) is known to be stable and worse performing when a large number of gradient steps are used for parameter updates. இந்த வேலையில், நாம் லீப் பயன்படுத்தும் ஒரு மெடா கற்றுக்கொள்ளும் கல்வி முறைமையை பயன்படுத்துகிறோம். அது கற்றுக்கொள்ளும் செயல்களின் முழு பாதைகளை க எங்கள் பெங்க்மார்க் OOV தகவல் அமைப்பு மற்றும் வெளியேறும் மதிப்பில், லிப் MAML விட ஒப்பிட்ட அல்லது நல்லது செயல்படுகிறது. நாம் சென்று பார்க்கவும் எந்த பொருள்களுக்கு மிகவும் பயனுள்ளது என்பதை ஆராய்க்கவும் செல்கிறோம் ஒரு OOV உள்ளமைப்பை கற்றுக் கொள்ள, மற்றும', 'pl': 'Metody nauki osadzania słów wymagają dużej liczby występowań słowa, aby dokładnie nauczyć się jego osadzenia. Jednak słowa poza słownictwem (OOV), które nie pojawiają się w korpusie treningowym, często pojawiają się w mniejszych danych. Ostatnie prace sformułowały naukę osadzającą OOV jako problem regresji kilku strzałów i wykazały, że meta-learning może poprawić uzyskane wyniki. Jednak zastosowany algorytm meta-learning (MAML) jest znany jako niestabilny i działa gorzej, gdy do aktualizacji parametrów używana jest duża liczba kroków gradientowych. W niniejszej pracy proponujemy zastosowanie algorytmu Leap, który wykorzystuje całą trajektorię procesu uczenia się zamiast punktu początkowego i końcowego, a tym samym usprawnia te dwie kwestie. W naszych eksperymentach z referencyjnym zestawem danych dotyczących nauki OOV oraz w ocenie zewnętrznej Leap działa porównywalnie lub lepiej niż MAML. Następnie zbadamy, które konteksty są najbardziej korzystne do nauki osadzenia OOV i proponujemy, że wybór kontekstów może mieć więcej znaczenia niż zastosowane meta-learning.', 'ur': 'Word embedding methods require a large number of occurrences of a word to accurately learn its embedding. However, out of vocabulary (OOV) words that do not appear in the training corpus often emerge in the smaller downstream data. اچھا کام OOOV کی تعلیم کے ذریعہ ایک کم شٹ ریگرس مسئلہ کے طور پر اپنا استعمال کیا گیا ہے اور دکھایا گیا ہے کہ مٹا سیکھنا نتیجے اچھے کر سکتا ہے۔ However, the algorithm used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradients steps are used for parameter updates. اس کام میں ہم نے لیپ کا استعمال کرنا پیشنهاد کرتا ہے، ایک مٹا-سیکھنے الگوریتم جو صرف آغاز اور پایان پوینٹوں کے بجائے تمام سیکھنے کے پیدائش کا مسئلہ کرتا ہے، اور اسی طرح یہ دو مسئلہ اضافہ کرتا ہے. ہمارے بنچم مارک OOV کی آزمائش میں سیکھنے کے ڈیٹ سٹ اور خارجی ارزیابی میں، لیپ MAML سے برابر یا بہتر عمل کرتا ہے. ہم جانتے ہیں کہ کس کنٹکسٹس سے زیادہ فائدہ دیتے ہیں کہ ایک OOV میں داخل ہونے کے لئے اور اس سے پیشنهاد کریں کہ متوسط کا انتخاب مٹا سکھانے والے سے زیادہ اہم ہو سکتا ہے.', 'uz': "Name Lekin, taʼminlovchi kompyuterda koʻrsatilmagan so'zlar soni ko'pincha kichkina soʻzda chiqindilar. Yaqinda ishni o'rganishni bir necha ko'plab o'rganishni o'rganish muammolari sifatida ko'rsatadi va meta o'rganish natijalarini yaxshi ko'rsatadi. Lekin, parametr yangilash uchun katta gradient soʻzlari ishlatilganda, model-agnostik meta-o'rganish (MAML) haqida mavjud bo'lishi mumkin. Bu ishda, Lep, meta-o'rganish algorithining foydalanishini davom qilamiz. Bu ta'lim jarayonini faqat boshlashni va oxiriga qo'shishni o'zgartiradi, va bu ikkita muammolarni o'zgartiradi. Maʼlumotlar tizimini o'rganish va cheksiz qiymatda, Leap MAML'dan yaxshi ko'proq bajaradi. Biz o'rganamiz, qanday muammolarni o'rganishga eng foydalanuvchi bo'lgan OOV o'rganishni o'rganishga qarang, va muammolarning tanlash o'ylashni meta-o'rganishdan ortiq muhim bo'lishi mumkin.", 'vi': 'Phương pháp tập luyện bằng từ cần rất nhiều sự hiện diện của một từ để học chính xác cách lắp ghép của nó. Tuy nhiên, từ ngoài ngôn ngữ (OOOV) mà không xuất hiện trong tập thể huấn thường xuất hiện trong những dữ liệu phụ nhỏ hơn. Công việc mới thảo luận chất lượng OOOV thêm lớp học là một vấn đề dẫn đến hồi phục vài phát triển và chứng minh rằng meta-learn có thể cải thiện kết quả. Tuy nhiên, thuật to án được dùng, Chế độ đào tạo dị biến (MAML) được biết là không ổn định và làm tệ hơn khi một số giai đoạn dốc lớn được dùng để cập nhật tham số. Trong công việc này, chúng tôi đề nghị sử dụng Leap, một thuật toán meta-Learning mà sử dụng cả quỹ đạo của tiến trình học thay vì chỉ khởi đầu và kết thúc điểm, và cải thiện hai vấn đề này. Trong các thí nghiệm trên một tập tin dựa trên Chuột OOOV và trong một đánh giá khéo léo, Leap thực hiện tốt hơn MAML. Chúng tôi tiếp tục kiểm tra các đối tượng nào hữu ích nhất để học một nền tác nghiệp OOOV từ đó, và đề xuất lựa chọn ngữ cảnh có thể quan trọng hơn các meta-Learning viên.', 'bg': 'Методите за вграждане на думи изискват голям брой случаи на дума, за да научите точно нейното вграждане. Въпреки това, думи извън речника (OOV), които не се появяват в корпуса на обучение, често се появяват в по-малките данни надолу по веригата. Последната работа формулира изучаването като регресионен проблем с няколко изстрела и демонстрира, че метаобучението може да подобри получените резултати. Въпреки това, използваният алгоритъм, модел-агностично мета обучение (MAML) е известно, че е нестабилен и се представя по-лошо, когато голям брой градиентни стъпки се използват за актуализиране на параметрите. В тази работа предлагаме използването на алгоритъм за мета обучение, който използва цялата траектория на учебния процес вместо само началото и края, и по този начин подобрява тези два въпроса. В нашите експерименти с референтна OOV вграждаща учебна база данни и при външна оценка, се представя сравнително или по-добре от МАМЛ. Продължаваме да изследваме кои контексти са най-полезни за изучаване на вграждане и предлагаме изборът на контексти да има повече значение от използваното мета обучение.', 'hr': 'Metode učenja riječi zahtijevaju veliki broj događaja riječi kako bi precizno naučili njegovo uključenje. Međutim, riječi koje se ne pojavljuju u tržišnom korpusu često pojavljuju u manjim podacima. Nedavni rad formuliran OOV ugrađivanjem učenja kao problem a regresije s nekoliko snimaka i pokazao je da metaučenje može poboljšati dobijene rezultate. Međutim, upotrebljen algoritam, zna se da je model-agnostički metaučenje (MAML) nestabilan i čini gore kada se koristi veliki broj koraka gradient a za aktualiziranje parametara. U ovom poslu predlažemo korištenje lijeka, algoritma metaučenja koji utječe na cijeli put procesa učenja umjesto samo početka i krajnja točka, i tako ameliorira te dvije probleme. U našim eksperimentima na referenciji OOV-a koji ugrađuje komplet podataka o učenju i u ekstrinskoj procjeni, Leap izvodi usporedno ili bolje od MAML-a. Nastavljamo pregledati koji su konteksti najkorisniji za učenje OOV-a koji se uključuje i predlažemo da izbor konteksta može biti važan više od zaposlenih metaučenja.', 'de': 'Lernmethoden zur Worteinbettung erfordern eine große Anzahl von Vorkommen eines Wortes, um seine Einbettung genau zu erlernen. In den kleineren nachgelagerten Daten tauchen jedoch häufig Wörter außerhalb des Vokabulars (OOV) auf, die nicht im Trainingskorpus vorkommen. Neuere Arbeiten formulierten OOV-Einbettungslernen als Weniger-Schuss-Regressionsproblem und zeigten, dass Meta-Lernen die erzielten Ergebnisse verbessern kann. Der verwendete Algorithmus, Modell-agnostic Meta-Learning (MAML), ist jedoch dafür bekannt, instabil zu sein und schlechter zu funktionieren, wenn eine große Anzahl von Verlaufsschritten für Parameteraktualisierungen verwendet wird. In dieser Arbeit schlagen wir den Einsatz von Leap vor, einem Meta-Learning-Algorithmus, der die gesamte Trajektorie des Lernprozesses anstelle von nur Anfang und Ende nutzt und somit diese beiden Probleme verbessert. In unseren Experimenten an einem Benchmark OOV eingebetteten Lerndatensatz und in einer extrinsischen Auswertung schneidet Leap vergleichbar oder besser ab als MAML. Anschließend untersuchen wir, aus welchen Kontexten eine OOV-Einbettung am besten gelernt werden kann und schlagen vor, dass die Wahl der Kontexte wichtiger ist als das verwendete Meta-Lernen.', 'id': 'Metode pembelajaran kata membutuhkan sejumlah besar kejadian kata untuk mempelajari pembelajarannya dengan akurat. Namun, kata-kata luar dari vocabulari (OOV) yang tidak muncul dalam korpus latihan sering muncul dalam data turun yang lebih kecil. Recent work formulated OOV embedding learning as a few-shot regression problem and demonstrated that meta-learning can improve results obtained.  Namun, algoritma yang digunakan, model-agnostic meta-learning (MAML) dikenal sebagai tidak stabil dan melakukan lebih buruk ketika jumlah besar langkah gradien digunakan untuk pemutakhiran parameter. In this work, we propose the use of Leap, a meta-learning algorithm which leverages the entire trajectory of the learning process instead of just the beginning and the end points, and thus ameliorates these two issues.  Dalam eksperimen kami pada benchmark OOV memasukkan dataset belajar dan dalam evaluasi ekstrinsik, Leap berhasil dibandingkan atau lebih baik dari MAML. Kita terus memeriksa konteks mana yang paling berguna untuk belajar sebuah OOV yang terlibat dari, dan menyarankan bahwa pilihan konteks mungkin lebih penting daripada meta-belajar yang digunakan.', 'ko': '단어 삽입 학습 방법은 단어가 대량으로 나와야만 단어의 삽입을 정확하게 학습할 수 있다.그러나 훈련 자료 라이브러리에 나타나지 않은 어휘표 밖(OOV) 어휘는 비교적 작은 하류 데이터에 자주 나타난다.최근의 작업은 OOV 삽입 학습을 소수의 렌즈 회귀 문제로 묘사하고 원 학습이 얻은 결과를 개선할 수 있음을 증명했다.그러나 이미 사용된 알고리즘 모델은 알 수 없는 메타데이터 학습(MAML)이 불안정하고 대량의 계단식 보폭을 사용하여 매개 변수 업데이트를 할 때 성능이 떨어진다.이 작업에서 우리는 Leap을 사용하는 것을 권장한다. 이것은 학습 과정의 전체 궤적을 이용하는 메타 학습 알고리즘이다. 이것은 학습 과정의 전체 궤적을 이용하는 것이지 시작점과 끝점이 아니라 이 두 문제를 개선하는 것이다.기본 OOV가 학습 데이터 집합과 외부 평가에 삽입된 실험에서 Leap의 표현은 MAML과 비슷하거나 더 좋았다.우리는 어떤 언어 환경이 OOV 삽입을 배우는 데 가장 유리한지 계속 연구하고 언어 환경의 선택이 원 학습보다 더 중요할 수 있음을 제시한다.', 'nl': 'Leermethoden voor het inbedden van woorden vereisen een groot aantal voorvallen van een woord om het inbedden ervan nauwkeurig te leren. Woorden die niet in het trainingscorpus voorkomen, komen echter vaak voor in de kleinere downstream data. Recent werk formuleerde OOV embedded learning als een paar-shot regressie probleem en toonde aan dat meta-learning verkregen resultaten kan verbeteren. Het gebruikte algoritme MAML (model-agnostic meta-learning) is echter onstabiel en presteert slechter wanneer een groot aantal gradiëntstappen wordt gebruikt voor parameterupdates. In dit werk stellen we het gebruik van Leap voor, een meta-learning algoritme dat gebruik maakt van het gehele traject van het leerproces in plaats van alleen het begin- en eindpunt, en zo deze twee kwesties verbetert. In onze experimenten met een benchmark OOV embedded learning dataset en in een extrinsieke evaluatie presteert Leap vergelijkbaar of beter dan MAML. Vervolgens onderzoeken we welke contexten het voordeligst zijn om een OOV embedding te leren, en stellen we voor dat de keuze van contexten meer kan uitmaken dan de gebruikte meta-learning.', 'da': 'Ordindlejring læringsmetoder kræver et stort antal forekomster af et ord for nøjagtigt at lære dets indlejring. Imidlertid optræder ord uden for ordforråd (OOV), som ikke optræder i træningskorpusen ofte i de mindre downstream data. Nyligt arbejde formulerede OOV integrering læring som et par-shot regressionsproblem og viste, at meta-læring kan forbedre de opnåede resultater. Men den anvendte algoritme, model-agnostic meta-learning (MAML) er kendt for at være ustabil og udføre værre, når et stort antal gradient trin bruges til parameteropdateringer. I dette arbejde foreslår vi brugen af Leap, en meta-læringsalgoritme, der udnytter hele banen i læringsprocessen i stedet for blot begyndelsen og slutpunkterne, og dermed forbedrer disse to problemer. I vores eksperimenter med et benchmark OOV-indlejring af læringsdatasæt og i en ekstern evaluering klarer Leap sig sammenligneligt eller bedre end MAML. Vi går videre med at undersøge, hvilke sammenhænge der er mest gavnlige at lære en OOV indlejring af, og foreslår, at valget af sammenhænge kan betyde mere end den anvendte meta-læring.', 'tr': "Kelemek üçin sözleriň öwrenmek yöntemlerini dogry ýagdaýy öwrenmek üçin birnäçe söz girişini gerek. Ýöne, bu sözlerden daşary ýok sözler (OOV) köplenç ýokary daşary düşürmeýän sözler kiçi-de aşak maglumatlarda görünýär. Ýakynda Ullanylan OOV öwrenişi birnäçe atly regressiýa meselesi hökmünde formularlanýar we meta öwrenişiniň netijeleri gelip biljekdigini görkezildi. Ýagnostik meta-öwrenmek üçin ullanýan algoritmus (MAML) unstable bolmagy biler we üýtgeşik etmek üçin bir topar gradient çe adım ullanynda işleýär. Bu işde, Leap'i, meta-öwrenmek algoritmi ulanmagy teklip edip, bu iki meseleni öwrenmek prosesiniň bütin trajektyny başynyň we soňky noktalaryň ýerine täsirleýär. Biziň benchmarklarymyzda OOV öwrenmek sistemasynda we ekstrinsic deňleşmelerinde, Leap MAMLden has ýagdaýda ýa-da gowy çykýar. Biz haýsy contextleriň OOV içinden daşarylan ýagdaýy öwrenmek üçin iň kynçylyklygyny soramaga dowam edýäris we şertleriň saýlamasynyň meta-öwrenmek işgärliginden has möhüm bolmagyny maslahat berýäris.", 'af': "Woord inbêring leer metodes benodig 'n groot aantal voorkomste van 'n woord om die inbêring te leer. Maar, uit-woordeboek (OOV) woorde wat nie verskyn in die onderwerp korpus nie dikwels in die kleiner onderstreem data uitkom nie. Onlangse werk formeer OOV ingesluit leer as 'n paar- skoot regresie probleem en het gedemonstreer dat meta- leer resultate verkry kan word. Maar die algoritme wat gebruik word, is model- agnostiese meta- leer (MAML) bekend om unstable te wees en verder te doen wanneer 'n groot aantal gradient stappe gebruik word vir parameter opdaterings. In hierdie werk voorstel ons die gebruik van Leap, 'n meta-leer algoritme wat die hele trajektorie van die leerproses verwyder in plaas van net die begin en die einde punte, en sodat hierdie twee probleem verwyder. In ons eksperimente op 'n benchmark OOV ingesluit leer datastel en in 'n ekstrinsiese evaluering, Leap doen vergelykbaar of beter as MAML. Ons gaan voort om te ondersoek watter kontekste is mees voordeel om 'n OOV ingesluit te leer van, en voorstel dat die keuse van kontekste meer kan saak as die meta-leer wat gebruik is.", 'sw': 'Neno la kujifunza inahitaji matukio mengi ya neno ili kujifunza kwa usahihi. Hata hivyo, maneno yasiyo na lugha (OOV) ambayo hayaonekana katika makampuni ya mafunzo yanatokea mara nyingi katika takwimu ndogo ya mto. Kazi ya hivi karibuni ilitengeneza OOV kwa ajili ya kujifunza kama tatizo la kudhibiti vidogo vichache na ilionyesha kuwa elimu ya meta inaweza kuboresha matokeo yanayopata. However, the algorithm used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradient steps are used for parameter updates.  Katika kazi hii, tunapendekeza matumizi ya Leap, algorithi ya kujifunza kwa njia zote za kujifunza badala ya mwanzo na pointi za mwisho, na kwa hiyo inabadilisha masuala haya mawili. Katika majaribio yetu ya OOV inayoingia kwenye seti ya kujifunza na katika uchunguzi wa nje, Leap anafanya kazi inayofananisha au bora kuliko MAML. Tunaendelea kuangalia mikutano ni yenye manufaa zaidi kujifunza OOV inayoingia kutoka, na tunapendekeza kwamba uchaguzi wa miradi unaweza kuwa na muhimu zaidi ya kazi za mafunzo ya chumvi.', 'sq': 'Metodat e përfshirjes së fjalëve kërkojnë një numër të madh ngjarjesh të një fjale për të mësuar saktësisht përfshirjen e saj. Megjithatë, fjalët jashtë fjalëkalimit (OOV) që nuk shfaqen në trupin e trajnimit shfaqen shpesh në të dhënat më të vogla poshtë rrjedhës. Puna e kohëve të fundit formuloi OOV duke përfshirë mësimin si një problem regresioni me disa fotografi dhe demonstroi se meta-mësimi mund të përmirësojë rezultatet e arritura. However, the algorithm used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradient steps are used for parameter updates.  Në këtë punë, propozojmë përdorimin e Leap-it, një algoritëm meta-mësimi i cili përdorë tërë trajektorinë e procesit mësimi në vend të vetëm fillimit dhe pikave përfundimtare, dhe kështu përmirëson këto dy çështje. Në eksperimentet tona në një referencë OOV duke përfshirë të dhënat e mësimit dhe në një vlerësim të jashtëm, Leap performon në mënyrë krahasuese apo më të mirë se MAML. Ne vazhdojmë të shqyrtojmë se cilat kontekste janë më të dobishme për të mësuar një OOV të përfshirë nga, dhe propozojmë që zgjedhja e konteksteve mund të jetë më e rëndësishme se meta-mësimi i përdorur.', 'hy': 'Բառերի ներգրավման ուսումնասիրության մեթոդները պահանջում են բառի բազմաթիվ պատահականություններ, որպեսզի ճշգրիտ սովորեն դրանց ներգրավումը: Այնուամենայնիվ, բառերը բառերից դուրս, որոնք չեն հայտնվում մարզի մարզի մեջ, հաճախ հայտնվում են ավելի փոքր հետևյալ տվյալներում: Վերջին աշխատանքը ձևավորել է OOO-ը, ներառելով ուսումը որպես մի քանի կրկնօրինակ ռեգրեսիայի խնդիր և ցույց տվեց, որ մետաուսումը կարող է բարելավել ստացված արդյունքները: Այնուամենայնիվ, օգտագործված ալգորիթմը, մոդել-ագնոստիկ մետասովորելը (MAML) հայտնի է, որ անկայուն է և ավելի վատ է աշխատում, երբ մեծ քայլեր են օգտագործվում պարամետրերի վերականգնման համար: Այս աշխատանքում մենք առաջարկում ենք օգտագործել Լիփը, մետասովորելու ալգորիթմը, որը օգտագործում է սովորելու գործընթացի ողջ ճանապարհը միայն սկզբի և վերջի կետերի փոխարեն, և այսպես բարելավում է այս երկու խնդիրները: Մեր փորձարկումներում, որտեղ OOO-ն է ներառում ուսուցման տվյալների համակարգը և արտաքին գնահատման մեջ, Լիփը համեմատաբար կամ ավելի լավ է աշխատում, քան MAML-ը: We go on to examine which contexts are most beneficial to learn an OOV embedding from, and propose that the choice of contexts may matter more than the meta-learning employed.', 'bn': 'শিক্ষার পদ্ধতি ব্যবহার করার জন্য একটি শব্দের অনেক বিশাল সংখ্যা প্রয়োজন যা সঠিকভাবে তার প্রবেশ শ শিখতে পারে। তবে শব্দভাণ্ডারের বাইরে (OOV) শব্দগুলো যা প্রশিক্ষণের কোর্পাসে দেখা যাচ্ছে না তা প্রায়শই কম নদীর তথ্যে উঠে। সাম্প্রতিক কাজে অওভি শিক্ষার সৃষ্টি করেছে কয়েকটি গুলি শিক্ষা নিয়ন্ত্রণের সমস্যা হিসেবে এবং প্রমাণিত করেছে যে মেটা শিক্ষা  তবে অ্যালগরিদম ব্যবহার করা হয়েছে, মডেল-অ্যাঙ্কোস্টিক মেটা শিক্ষা (MAML) নিশ্চিত এবং প্যারামিটার আপডেটের জন্য বিশাল গ্রেডিয়েন্ড পদক্ষ এই কাজে আমরা লিপের ব্যবহারের প্রস্তাব করছি, একটি মেটা শিক্ষা অ্যালগরিদম যা শিক্ষা প্রক্রিয়ার পুরো পদক্ষেপ শুরু এবং শেষ বিন্দুর পরিবর্তে শিক্ষা প্রক্র আমাদের বেঞ্চাম্পার্ক অওভির একটি পরীক্ষায়, যেখানে শিক্ষা প্রদান করা তথ্য সেট এবং এক বিদেশী পরীক্ষায় লিপ ম্যাএমএলের চেয়ে তুলনায় অথব আমরা পরীক্ষা করতে যাচ্ছি কোন প্রতিযোগিতা সবচেয়ে সুবিধা লাভ করতে যাচ্ছে ওভির কাছ থেকে প্রবেশ করার এবং প্রস্তাব করছি যে প্রতিযোগিতার বেছে নেওয়া', 'fa': 'روش\u200cهای یادگیری کلمه\u200cها نیاز به تعداد زیادی از اتفاق\u200cهایی از یک کلمه برای دقیقا یاد گرفتن داخلش است. ولی کلمات غیر از کلمات (OOV) که اغلب در اطلاعات پایین\u200cترین پایین\u200cترین پایین\u200cترین کوچک ظاهر نمی\u200cشوند. کارهای اخیراً formulated OOV integrating learning as a few-shot regression problem and demonstrated that meta-learning can improve results obtained. با این حال، الگوریتم استفاده می\u200cشود، مدل- agnostic meta- learning (MAML) معلوم می\u200cشود که ثابت نیست و هنگامی که تعداد زیادی از قدم\u200cهای گرادیش برای آغاز\u200cسازی پارامتر استفاده می\u200cشود بدتر انجام می\u200cدهد. در این کار، ما استفاده از لیپ را پیشنهاد می\u200cکنیم، الگوریتم متیادگیری که کل مسیر فرایند یادگیری را به جای فقط نقطه\u200cهای آغاز و پایان می\u200cدهد، و بنابراین این دو مسئله را تغییر می\u200cدهد. در آزمایش\u200cهای ما روی یک برچسب OOV که مجموعه داده\u200cهای یادگیری را وارد می\u200cکند و در یک ارزیابی خارجی، لیپ در مقایسه یا بهتر از MAML انجام می\u200cدهد. و پیشنهاد می\u200cکنیم که انتخاب شرایط بیشتر از مطالعه یادگیری مفید باشد.', 'az': "Öyrənmə metodlarını daxil etmək üçün bir söz olaraq böyük bir sayı olaraq tələb edir. Lakin, təhsil korpusda görünməyən sözlərdən uzaq sözlər daha az aşağıdaki məlumatlarda yayılır. Son zamanlarda OOV öyrənməsini bir neçə-vuruş regresiya problemi kimi inşa edir və meta öyrənməsinin sonuçlarını yaxşılaşdırabilir. Lakin, kullanılan algoritm, model-agnostik meta-öyrənmə (MAML) unstable olduğunu bilər və böyük səviyyənin adımları parametru güncelləməsi üçün istifadə ediləndə daha pis işlər edir. Bu işdə Leap'in istifadəsini, meta-öyrənmə algoritmi təklif edirik ki, öyrənmə prosesinin bütün trajektörünü sadəcə başlangıç və son noktalarının yerinə təklif edir və bu iki məsələni dəyişdirir. Öyrənmək məlumatlarını və extrinsic değerlendirmələrində OOV-ın təcrübələrimizdə, Leap MAML-dən daha yaxşı və müxtəlif təcrübələrdə performans edir. Biz hansı müxtəlif müxtəlif məlumatların istifadə edilən OOV öyrənməsindən ən faydalı olduğunu təsdiqləyib, müxtəlif seçimlərin istifadə edilən meta öyrənməsindən daha çox mövcuddur.", 'cs': 'Metody vkládání slov vyžadují velký počet výskytů slova, aby se přesně naučily jeho vkládání. Nicméně slova mimo slovní zásobu (OOV), která se neobjevují v tréninkovém korpusu, se často objevují v menších následných datech. Nedávná práce formulovala OOV embeding learning jako několik regresních problémů a ukázala, že meta-learning může zlepšit získané výsledky. Je však známo, že použitý algoritmus, model-agnostické meta-learning (MAML), je nestabilní a funguje horší, když je pro aktualizace parametrů použit velký počet kroků přechodu. V této práci navrhujeme využití meta-learningového algoritmu Leap, který využívá celou trajektorii učebního procesu namísto počátečního a koncového bodu a tím zlepšuje tyto dvě problémy. V našich experimentech na benchmarkovém OOV vkládání učebních dat a v extrinsickém vyhodnocení, Leap výkonuje srovnatelně nebo lepší než MAML. Dále zkoumáme, z kterých kontextů je nejvýhodnější naučit se OOV embedding, a navrhujeme, že na výběru kontextů může záležet více než na použitém meta-learningu.', 'ca': "Métodes d'aprenentatge incorporant paraules requereixen moltes ocurrències d'una paraula per aprendre exactament la seva incorporació. Però les paraules fora de vocabulari (OOV) que no apareixen al cos d'entrenament apareixen sovint en les dades més petites a avall. La feina recent va formular OOV incorporant l'aprenentatge com un problem a de regressió de poques fotos i va demostrar que l'aprenentatge meta pot millorar els resultats obtenits. Però és conegut que l'algoritme utilitzat, el meta-aprenentatge model-agnòstic (MAML) és instable i fa pitjor quan s'utilitzen moltes etapes de gradient per actualitzar els paràmetres. En aquesta feina, proposem l'ús de Leap, un algoritme de meta-aprenentatge que aprofita tota la trajectòria del procés d'aprenentatge en lloc de només els punts inicials i finals, i millora aquests dos problemes. En els nostres experiments en un punt de referència OOV incorporant un conjunt de dades d'aprenentatge i en una evaluació extrínsica, Leap actua comparablement o millor que MAML. Continuem a estudiar quins contextos són més beneficiosos per aprendre una OOV integrada, i proposem que l'elecció de contextos pot importar més que el meta-aprenentatge empregat.", 'am': 'የቃላት መማር ዘዴዎች መግለጫ የቃላትን መግለጫ ለመፍጠር በቁጥር የሚበዛ ቁጥጥር ያስፈልጋል። ነገር ግን በማስተማር ካርፓስ ውስጥ የማይታወቅ ቃላት ከ-vocabulary (OOV) በሚታወቀው ከታናሹ ዳታ በጣም እጥፍ ይወጣል። የአሁኑ ሥራ የኦኦኦቪ ትምህርት በጥቂት የተነሳ ትምህርት መግለጫ መሆኑን አቀረበ፡፡ ምንም እንኳን፣ የተጠቀመው አልgorithም፣ model-agnostic meta-ትምህርት (MAML) የተመሠረተ እና ትክክለኛ እንደሆነ በአካባሮች ላይ የተጠቃሚ ደረጃዎች እንዲጠቀሙ ይታወቃል፡፡ በዚህ ሥራ፣ መጀመሪያ እና መጨረሻ ነጥቦች የሚለውን የመምህርት ፕሮግራሙን ሁሉ በመጠቀም የልp መተማር መተማር አሌጎርቲምን እናስጠጋለን፡፡ የዳታ ሳጥን እና ውጭ አዳራሽ ማስተማርን በሚያሳየው የኦኦኦቪ ፈተናዎች ውስጥ ሊp ከመMAML ይልቅ የተሻለ ይሠራል፡፡ We go on to examine which contexts are most beneficial to learn an OOV embedding from, and propose that the choice of contexts may matter more than the meta-learning employed.', 'bs': 'Metode učenja riječi zahtijevaju veliki broj događaja riječi kako bi precizno naučili njegovo uključenje. Međutim, riječi koje se ne pojavljuju u tržišnom korpusu često pojavljuju u manjim podacima. Nedavni rad formuliran OOV ugrađenje učenja kao problem regresije s nekoliko snimaka i pokazao je da metaučenje može poboljšati dobijene rezultate. Međutim, korišteni algoritam, zna se da je model-agnostički metaučenje (MAML) nestabilan i čini gore kada se koristi veliki broj koraka gradient a za aktualizacije parametara. U ovom poslu predlažemo korištenje Lipa, algoritma metaučenja koji utiče na cijeli put procesa učenja umjesto samo početka i krajnja tačka, i tako ameliorira te dve probleme. U našim eksperimentima na referenciji OOV-a koji ugrađuje komplet podataka o učenju i u ekstrinskoj procjeni, Leap izvodi usporedno ili bolje od MAML-a. Nastavljamo da pregledamo koji su konteksti najkorisniji da naučimo OOV-u koji se uključuje, i predlažemo da izbor konteksta može biti važan više od zaposlenog metaučenja.', 'fi': 'Sanan upottamisen oppimismenetelmät vaativat suuren määrän esiintymiä, jotta se oppii upottamisen tarkasti. Kuitenkin sanaston ulkopuolisia sanoja, joita ei esiinny koulutuskorpusessa, esiintyy usein pienemmissä jatkojalostustiedoissa. Äskettäinen työ formuloi OOV-upotusoppimisen muutaman kuvan regressioongelmana ja osoitti, että meta-oppiminen voi parantaa saavutettuja tuloksia. Käytetyn algoritmin, malliagnostisen meta-oppimisen (MAML) tiedetään kuitenkin olevan epävakaa ja suoriutuvan huonommin, kun parametrien päivittämiseen käytetään useita gradienttivaiheita. Tässä työssä ehdotamme Leapin käyttöä, metaoppimisalgoritmia, joka hyödyntää oppimisprosessin koko kehityskulkua alku- ja loppupisteiden sijaan ja näin parantaa näitä kahta asiaa. Leap suoriutuu verrattain tai paremmin kuin MAML:llä tehdyissä kokeissa ja ulkoisessa arvioinnissa. Jatkossa tutkimme, mitkä kontekstit ovat hyödyllisimpiä oppia OOV upotus, ja ehdotamme, että kontekstien valinta voi olla tärkeämpää kuin käytetty meta-oppiminen.', 'et': 'Sõna manustamise õppemeetodid nõuavad sõna manustamise täpseks õppimiseks suurt hulka kordumisi. Vähemates järgnevates andmetes ilmnevad sageli sõnavara välised sõnad, mida koolituskorpuses ei esine. Hiljutine töö sõnastas OOV manustamise õppe kui mõne võimaluse regressiooniprobleemina ja näitas, et metaõpe võib saavutatud tulemusi parandada. Kuid kasutatud algoritm, mudeliga agnostiline metaõpe (MAML) on teadaolevalt ebastabiilne ja toimib halvemini, kui parameetrite värskendamiseks kasutatakse suurt hulka gradientide etappe. Selles töös pakume välja metaõppe algoritmi Leap kasutamise, mis võimendab kogu õppeprotsessi trajektoori, mitte ainult algus- ja lõpppunktide asemel, ning seega parandab neid kahte küsimust. Meie eksperimentides võrdlusaluse OOV manustamisega õppeandmekogumi ja välishindamise käigus toimib Leap võrreldavalt või paremini kui MAML. Me uurime edasi, millised kontekstid on kõige kasulikumad õppida OOV manustamist, ja teeme ettepaneku, et kontekstide valik võib olulisem kui kasutatav metaõpe.', 'sk': 'Metode učenja vdelave besed zahtevajo veliko število pojavov besede, da se natančno naučijo njene vdelave. Vendar pa se besede zunaj besedišča (OOV), ki se ne pojavljajo v korpusu usposabljanja, pogosto pojavljajo v manjših nadaljnjih podatkih. Nedavno delo je oblikovalo vključevanje učenja OOV kot problem regresije nekaj posnetkov in pokazalo, da lahko meta učenje izboljša dosežene rezultate. Vendar pa je znano, da je uporabljeni algoritem, model agnostic meta-learning (MAML), nestabilen in slabši, kadar se za posodobitve parametrov uporablja veliko število stopenj prelivanja. V tem delu predlagamo uporabo Leap, meta-učnega algoritma, ki uporablja celotno pot učnega procesa namesto samo začetka in konca ter s tem izboljšuje ta dve vprašanji. V naših eksperimentih na primerjavi OOV, ki vključuje nabor učnih podatkov, in v zunanji vrednotenju Leap deluje primerljivo ali bolje kot MAML. Nadalje preučujemo, katere kontekste so najbolj koristne za učenje vključevanja OOV, in predlagamo, da je izbira kontekstov lahko pomembnejša od uporabljenega meta učenja.', 'jv': 'embed" is a verb (command description). Ora, mengko-mengko kelalakno sing ora bisa pasar tentang karo perusahaan Open recent file politenessoffpolite"), and when there is a change ("assertivepoliteness Nang iné, kéné ngerti nggunakake Layp, Algorithm meta-Learn sing ngewehke nggawe barang nggawe barang nggawe sistem kanggo ngilangno sistem sampek mulai kanggo sampek sampek kanggo tukang sampek mulai, lan iki bakal dhéwé beraksi iki sakjane. Name Awak dhéwé mulai ngono ujian sing conteks sing paling nggawe bener kanggo nggambar OOOOw iki, lan supoyo nggawe ngubah dhéwé contextual iso dianggap luwih apik dhéwé meta-sapa uwis.', 'ha': "Tsarin da aka shigar da maganar da aka karanta, yana ƙayyade ƙidãya mai girma wa kalma da za'a sanar da zura cikin shirinsa. Ina kasa, da kuma-out-dictionary (OOV) maganar waɗanda ba su fito cikin shirin umarni, za'a fita mara kaɗan a ƙarami cikin data masu ƙarami. A yanzu aikin da aka formet OOV da aka shigar da learning kamar masu ƙara masu sauya na haramtar rajistarwa kuma ya nuna cewa ana ƙara matsalar da meta-leari. However, the algorithm used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradient steps are used for parameter updates.  Daga wannan aikin, Munã bukãtar da amfani da Leap, algoritin meta-leari, wanda ke ƙarfafa duk hanyon aikin da za'a yi amfani da shi ba na farko da pointin ƙarshen, kuma kamar wannan na sami masu biyu. Daga jarrabai da aka samar a kan bangon OOV na ƙara danne da kuma a cikin ƙidãya mai ƙaranci, Leap yana aiki daidai ko mafi alhẽri daga MAML. Munã tafiya domin ka jarraba wanne matsayin da yake mafi amfani ga ka sanar da wani OOV wanda ya fito daga, kuma tuna shawarar cẽwa zaɓallin matsayin za ta kasance mafi muhimmin ka da mafiya amfani ga mafarako da an yi amfani da shi.", 'he': 'שיטות הלימוד של מילים דורשות מספר גדול של התרחשות של מילה כדי ללמוד בדיוק את ההכנות שלה. בכל אופן, מילים מחוץ למילים (OOV) שלא מופיעות בקורפוס האימונים מופיעות לעתים קרובות במידע הקטן למטה. העבודה האחרונה הוצרה OOV תוכנית למידה כבעיה של חזרה של כמה יריות והוכיחה כי מטה-למידה יכולה לשפר את התוצאות שנקבלו. עם זאת, האלגוריתם המשתמש, מודל-אגנוסטי meta-learning (MAML) ידוע להיות לא יציב ומבצע גרוע יותר כאשר מספר גדול של צעדים gradient משתמשים למעדכונים פרמטרים. בעבודה הזו, אנו מציעים להשתמש באלגוריתם של ליפ, אלגוריתם מטה-למידה שממשיך את כל המסלול של תהליך הלימוד במקום רק את התחלה והנקודות הסופיות, ולכן משפר את שני הנושאים האלה. In our experiments on a benchmark OOV embedding learning dataset and in an extrinsic evaluation, Leap performs comparably or better than MAML.  אנו ממשיכים לבדוק אילו קשרים הם היעילים ביותר ללמוד OOV מתכנן ממנו, ולהציע שבבחירת הקשר עשויה להיות חשובה יותר מאשר המטה-למידה המשתמשת.', 'bo': 'གནས་འཇུག་པའི་འཇུག་སྣོད་ཀྱི་ཐབས་ལམ་ལ་གསལ་པོ་ཞིག་གི་འབྱུང་རིམ་ཆ་རྐྱེན་དགོས་པ ཡིན་ནའང་། བརྡ་སྤྲོད་མིག་པའི་གནད་དོན་དག་གི་ནང་དུ་ཡོད་མེད་པའི་བརྡ་སྤྲོད་ཀྱི་ནང་དུ་བཏུབ་པས། འཕྲལ However, the algorithm used, model-agnostic meta-learning (MAML) is known to be unstable and perform worse when a large number of gradient steps are used for parameter updates. འོན་ཀྱང་། ང་ཚོས་ལོ་གྲངས་སྒྲིག་གི་སྔོན་གྱི་ལྟ་བུའི་སྤྱོད་སྟངས་གསལ་རྒྱབ་སྐྱོར་གྱི་གྲངས་སྒྲིག་རིམ་དེ་གིས་འགོ་བཙུགས་དང་མཐའ་མཇུག་གི་སྐབས་ཡོད་ In our experiments on a benchmark OOV embedding learning dataset and in an extrinsic evaluation, Leap performs comparably or better than MAML. འུ་ཅག་གིས་ཁྱེར་ཚོའི་ནང་དུ་ཡོད་པའི་མི་ཚེག་རྣམས་ལས་ཕན་ཐོགས་ཅན་ཡོད་པའི་OOV ཞིག་བསླབ་ཀྱི་ཡོད་ཚད་ལྟ་ཞིབ་བྱས་མེད།'}
{'en': 'An Empirical Study of Compound PCFGs PCFG s', 'fr': 'Une étude empirique des PCFG composés', 'pt': 'Um estudo empírico de PCFGs compostos', 'es': 'Un estudio empírico de PCFG compuestos', 'ar': 'دراسة تجريبية لمركبات PCFG المركبة', 'hi': 'यौगिक PCFGs का एक अनुभवजन्य अध्ययन', 'zh': '复合PCFG之实', 'ja': '化合物ＰＣＦＧの実証的研究', 'ru': 'Эмпирическое исследование соединений PCFG', 'ga': 'Staidéar Eimpíreach ar PCFGanna Comhdhéanta', 'el': 'Εμπειριακή μελέτη σύνθετων PCFG', 'hu': 'Összetett PCFG-k empirikus vizsgálata', 'ka': 'Compound PCFG', 'it': 'Uno studio empirico dei PCFG composti', 'kk': 'Тіркелген PCFG империялық зерттеу', 'lt': 'Komplektinių PCFG empirinis tyrimas', 'mk': 'Империска студија на комплексни PCFG', 'ms': 'Name', 'ml': 'പിസിഎഫിജികളുടെ ഒരു എമിരിക്കല്\u200d പഠനംName', 'mt': 'Studju Empiriku ta’ PCFGs Komposti', 'mn': 'Комплекс PCFG-ийн эзэмшигийн судалгаа', 'pl': 'Badanie empiryczne związków PCFG', 'ro': 'Un studiu empiric al PCFG-urilor compuse', 'sr': 'Impiričko istraživanje kombinovanih PCFG-a', 'no': 'Name', 'si': 'Name', 'so': 'Waxbarashada shahaadada ee Compound PCFGs', 'sv': 'En empirisk studie av sammansatta PCFG', 'ta': 'Name', 'ur': 'Name', 'uz': 'Name', 'vi': 'A Empirical study of Componend PCFG', 'bg': 'Емпирично проучване на съединените PCFG', 'hr': 'Empiričko ispitivanje kombiniranih PCFG-a', 'nl': "Een empirische studie van samengestelde PCOV's", 'da': 'Et empirisk studie af sammensatte PCFGs', 'de': 'Eine empirische Studie von zusammengesetzten PCFGs', 'sw': 'Utafiti wa Uwezeshaji wa PCFGs', 'tr': "Birleşik PCFG'yň Impirik öwrenmesi", 'ko': '복합 PCFGs의 실증 연구', 'id': 'Penelitian Empirik PCFG Komponen', 'fa': 'یک مطالعه امپراتیک از PCFG ترکیب', 'af': 'Name', 'sq': 'Një studim perandor i PCFG kompozuar', 'az': "M칲xt톛lif PCFG'l톛r haqq캼nda 캻mpariki 칐yr톛nm톛si", 'bn': 'Name', 'bs': 'Impiričko ispitivanje kompleksnih PCFG-a', 'ca': 'Un estudi empíric de PCFG complexos', 'cs': 'Empirická studie složených PCFG', 'et': 'Ühendatud PCFGde empiiriline uuring', 'hy': 'Միմպիրական ուսումնասիրություն', 'am': '瘠ｨPCFGs 瘉昵渥瘡ｫ瘠若何', 'fi': 'Yhdistettyjen PCFG-yhdisteiden empiirinen tutkimus', 'sk': 'Empirična študija sestavljenih PCFG', 'he': 'מחקר אימפריקלי של PCFG מורכבים', 'ha': 'KCharselect unicode block name', 'bo': 'ཆ་ཚར་ཡོད་པའི་PCFGs གི་སློབ་སྟངས་གཙོ་རིམ་ཞིག་', 'jv': 'Name'}
{'en': 'Compound probabilistic context-free grammars (C-PCFGs) have recently established a new state of the art for phrase-structure grammar induction. However, due to the high  time-complexity  of chart-based representation and inference, it is difficult to investigate them comprehensively. In this work, we rely on a fast implementation of C-PCFGs to conduct evaluation complementary to that of (CITATION). We highlight three key findings : (1) C-PCFGs are data-efficient, (2) C-PCFGs make the best use of global sentence-level information in preterminal rule probabilities, and (3) the best configurations of C-PCFGs on  English  do not always generalize to morphology-rich languages.', 'fr': "Les grammaires sans contexte probabilistes composées (c-PCFG) ont récemment établi un nouvel état de la technique pour l'induction grammaticale de structure de phrase. Cependant, en raison de la grande complexité temporelle de la représentation et de l'inférence basées sur des graphiques, il est difficile de les étudier de manière exhaustive. Dans ce travail, nous nous appuyons sur une mise en œuvre rapide des c-PCFG pour mener une évaluation complémentaire à celle de (CITATION). Nous soulignons trois résultats clés\xa0: (1) les c-PCFG sont efficaces en termes de données, (2) les c-PCFG utilisent au mieux les informations globales au niveau de la phrase dans les probabilités des règles préterminales, et (3) les meilleures configurations de c-PCFG en anglais ne se généralisent pas toujours aux langues riches en morphologie.", 'ar': 'لقد أنشأت القواعد النحوية الاحتمالية المركبة الخالية من السياق (C-PCFGs) مؤخرًا حالة جديدة من الفن لاستقراء قواعد بناء العبارات. ومع ذلك ، نظرًا للتعقيد الزمني الكبير للتمثيل والاستدلال المستند إلى الرسوم البيانية ، فمن الصعب التحقيق فيها بشكل شامل. في هذا العمل ، نعتمد على التنفيذ السريع لـ C-PCFGs لإجراء تقييم مكمل لتقييم (CITATION). نسلط الضوء على ثلاث نتائج رئيسية: (1) C-PCFGs فعالة في استخدام البيانات ، (2) C-PCFGs تحقق أفضل استخدام للمعلومات العامة على مستوى الجملة في احتمالات القاعدة قبل الأوان ، و (3) أفضل تكوينات C-PCFGs على لا تعمم اللغة الإنجليزية دائمًا على اللغات الغنية بالمورفولوجيا.', 'es': 'Las gramáticas probabilísticas compuestas libres de contexto (C-PCFG) han establecido recientemente un nuevo estado de la técnica para la inducción gramatical de estructura de frases. Sin embargo, debido a la alta complejidad temporal de la representación e inferencia basada en gráficos, es difícil investigarlas exhaustivamente. En este trabajo, confiamos en una implementación rápida de los C-PCFG para llevar a cabo una evaluación complementaria a la de (CITATION). Destacamos tres hallazgos clave: (1) los C-PCFG son eficientes en cuanto a datos, (2) los C-PCFG hacen el mejor uso de la información global a nivel de oración en las probabilidades de reglas preterminales, y (3) las mejores configuraciones de C-PCFG en inglés no siempre se generalizan a idiomas ricos en morfología.', 'pt': 'Gramáticas livres de contexto probabilísticas compostas (C-PCFGs) estabeleceram recentemente um novo estado da arte para indução de gramática de estrutura de frase. No entanto, devido à alta complexidade de tempo da representação e inferência baseada em gráficos, é difícil investigá-los de forma abrangente. Neste trabalho, contamos com uma rápida implementação de C-PCFGs para realizar avaliação complementar à de (CITATION). Destacamos três descobertas principais: (1) C-PCFGs são eficientes em dados, (2) C-PCFGs fazem o melhor uso de informações globais em nível de sentença em probabilidades de regras pré-terminais e (3) as melhores configurações de C-PCFGs em O inglês nem sempre generaliza para idiomas ricos em morfologia.', 'ru': 'Сложные вероятностные контекстно-свободные грамматики (C-PCFG) недавно создали новый уровень техники для индукции грамматики структуры фразы. Однако в связи с высокой сложностью представления и вывода на основе диаграмм с точки зрения времени их сложно всесторонне исследовать. В этой работе мы полагаемся на быстрое осуществление К-ПКФГ для проведения оценки в дополнение к оценке (ЦИТИРОВАНИЕ). Мы выделяем три ключевых вывода: (1) C-PCFG являются эффективными с точки зрения данных, (2) C-PCFG наилучшим образом используют глобальную информацию на уровне предложений в доконцевых вероятностях правил и (3) лучшие конфигурации C-PCFG на английском языке не всегда обобщают богатые морфологией языки.', 'zh': '复合概率上下文无关语法(C-PCFGs)近为短语构语法归立新技术水平。 然图表推理有高时复杂性,难以周究。 以此之务,吾因C-PCFGs之速行以质(引文)互补也。 三要:(1)C-PCFG有数效率,(2)C-PCFG在预定之法概率充用全局句息,及(3)C-PCFG在英语上之最佳者,不辄推及形容丰盛之语。', 'ja': '化合物の確率論的文脈自由文法（ Ｃ － ＰＣＦＧ ）は、最近、語句構造文法誘導のための新たな最先端を確立した。しかしながら、チャートベースの表現と推論の時間複雑性が高いため、それらを総合的に調査することは困難である。本研究では、C - PCFGの迅速な実装に依存して、（引用）の評価を補完する評価を行っている。私たちは、3つの主要な知見を強調しています。（ 1 ） C - PCFGはデータ効率が良い、（ 2 ） C - PCFGは終末前ルール確率におけるグローバルな文章レベルの情報を最大限に活用し、（ 3 ）英語におけるC - PCFGの最良の構成は、必ずしも形態豊富な言語に一般化しない。', 'hi': 'यौगिक संभाव्य संदर्भ-मुक्त व्याकरण (सी-पीसीएफजी) ने हाल ही में वाक्यांश-संरचना व्याकरण प्रेरण के लिए कला की एक नई स्थिति स्थापित की है। हालांकि, चार्ट-आधारित प्रतिनिधित्व और अनुमान की उच्च समय-जटिलता के कारण, उन्हें व्यापक रूप से जांचना मुश्किल है। इस काम में, हम सी-पीसीएफजी के तेजी से कार्यान्वयन पर भरोसा करते हैं ताकि मूल्यांकन (उद्धरण) के पूरक मूल्यांकन का संचालन किया जा सके। हम तीन प्रमुख निष्कर्षों को उजागर करते हैं: (1) सी-पीसीएफजी डेटा-कुशल हैं, (2) सी-पीसीएफजी प्रीटर्मिनल नियम संभावनाओं में वैश्विक वाक्य-स्तर की जानकारी का सबसे अच्छा उपयोग करते हैं, और (3) अंग्रेजी पर सी-पीसीएफजी के सर्वोत्तम कॉन्फ़िगरेशन हमेशा आकृति विज्ञान-समृद्ध भाषाओं के लिए सामान्यीकृत नहीं होते हैं।', 'ga': 'Le déanaí bhunaigh gramadach neamh-chomhthéacs dóchúlachta (C-PCFGanna) scéim úrscothach le haghaidh ionduchtú gramadaí struchtúr frásaí. Mar sin féin, mar gheall ar chastacht ard ama na hionadaíochta cairt-bhunaithe agus tátal, tá sé deacair iad a imscrúdú go cuimsitheach. San obair seo, táimid ag brath ar C-PCFGanna a chur i bhfeidhm go tapa chun meastóireacht a dhéanamh a chomhlánaíonn (CITATION). Cuirimid béim ar thrí phríomhthoradh: (1) Tá C-PCFGanna tíosach ar shonraí, (2) baineann C-PCFGanna an úsáid is fearr as faisnéis dhomhanda ar leibhéal na habairte i ndóchúlachtaí réamhrialacha, agus (3) na cumraíochtaí is fearr de C-PCFGanna ar. Ní i gcónaí a ghineann an Béarla go teangacha atá saibhir i moirfeolaíocht.', 'el': 'Οι σύνθετες πιθανολογικές γραμματικές χωρίς πλαίσιο (C-PCFG) έχουν πρόσφατα καθιερώσει μια νέα κατάσταση της τεχνολογίας για την επαγωγή γραμματικής δομής φράσεων. Ωστόσο, λόγω της μεγάλης χρονικής πολυπλοκότητας της απεικόνισης και συμπερασμάτων με βάση το γράφημα, είναι δύσκολο να διερευνηθούν διεξοδικά. Σε αυτό το έργο, βασιζόμαστε στην ταχεία εφαρμογή των ΚΠΠΑ για τη διενέργεια αξιολόγησης συμπληρωματικής με αυτή της (CITATION). Επισημαίνουμε τρία βασικά ευρήματα: (1) Τα C-PCFG είναι αποδοτικά σε δεδομένα, (2) Τα C-PCFG χρησιμοποιούν την καλύτερη χρήση των παγκόσμιων πληροφοριών σε επίπεδο πρότασης στις πιθανότητες πρόωρου κανόνα, και (3) οι καλύτερες διαμορφώσεις των C-PCFG στα αγγλικά δεν γενικεύονται πάντα σε γλώσσες πλούσιες σε μορφολογία.', 'hu': 'Az összetett valószínűleg kontextusmentes nyelvtanfolyamok (C-PCFG) a közelmúltban új korszerűséget hoztak létre a kifejezésszerkezet nyelvtani indukció terén. A diagramalapú ábrázolás és következtetés nagy időbonyolultsága miatt azonban nehéz átfogóan vizsgálni őket. Ebben a munkában a C-PCFG-k gyors végrehajtására támaszkodunk, hogy a (CITATION) értékelést kiegészítsük. Három kulcsfontosságú megállapítást emelünk ki: (1) a C-PCFG-k adathatékonyak, (2) a C-PCFG-k a lehető legjobban használják ki a globális mondatszintű információkat a koraszülött szabályok valószínűségeiben, és (3) a C-PCFG-k legjobb angol konfigurációi nem mindig általánosítják a morfológiában gazdag nyelveket.', 'ka': 'შექმნილი შესაბამისებელი კონტექსტურის გარეშე გრამიმარები (C- PCFGs) უკვე ახალი სურათის შესაბამისება ფრაზების სტრუქტურის გრამიმარის ინდიქციისთვის. მაგრამ, ფარატიური განსაზღვრება და ინფერენციის მაღალი დროის კომპლექსიტებით, ისინი ძალიან რთულია გააკეთება ყველაფერი განსაზღვრებით. ამ სამუშაოში, ჩვენ დავიყენებთ C-PCFG-ის ბრძნელი გამოყენებაზე, რომ გავაკეთოთ უფლების კომპლემენტური გამოყენება (CITATION). C-PCFGs არის მონაცემები ეფექტიური, 2) C-PCFGs გლობალური მონაცემების ინფორმაციის საუკეთესო გამოყენება პრეტერიმინალური წესების შესაძლებლობაში, და 3) საუკეთესო კონფიგურაცია C-PCFGs ინგლისოში არ ყოველთვის ჯერალურად მოპრო', 'it': "Grammatiche composte senza contesto probabilistico (C-PCFGs) hanno recentemente stabilito un nuovo stato dell'arte per l'induzione grammaticale della struttura delle frasi. Tuttavia, a causa dell'elevata complessità temporale della rappresentazione e dell'inferenza basate su grafici, è difficile investigarli in modo completo. In questo lavoro, ci affidiamo ad una rapida attuazione dei C-PCFG per condurre una valutazione complementare a quella di (CITATION). Evidenziamo tre risultati chiave: (1) C-PCFG sono dati-efficienti, (2) C-PCFG fanno il miglior uso delle informazioni globali a livello di frase nelle probabilità di regole premature, e (3) le migliori configurazioni di C-PCFG in inglese non sempre generalizzano alle lingue ricche di morfologia.", 'kk': 'Жуырдағы мүмкіндік контексті бос граммалар (C- PCFG) фраз- құрылғысының грамматикалық индукциясының жаңа күйін құрылған. Бірақ, диаграмма негіздеген кеңістіктердің жұмыс уақыт және көпшіліктердің көпшілігі болса, оларды толық түрде зерттеу қиын. Бұл жұмыс ішінде, C-PCFG жұмыс істеу үшін (CITATION) бағалау үшін тез жұмыс істеуде тәуелдік. Біз үш кілт тапсырмаларын таңдаймыз: (1) C- PCFG деректерді эффективті, 2) C- PCFG- деректерді жүйелік сөз деңгейінің ең жақсы мәліметін бағдарламалық ережелерде қолдану үшін, және (3) ағылшын тілдегі C- PCFG- деректерінің ең жақсы баптаулары', 'ml': 'സാധ്യതയുള്ള കോണ്\u200dട്ടെക്സ്റ്റോക്സ് ഫ്രീഡ് ഗ്രാമാര്\u200d (C- PCFGs) അടുത്തുതന്നെ വാക്ക്- structure grammar induction നിര്\u200dമ്മിക്കുന്നതിനുള്ള ഒരു  എന്നാലും, കാര്\u200dട്ട് അടിസ്ഥാനമായി പ്രതിനിധിക്കുന്നതിന്\u200dറെയും അപകടത്തിന്\u200dറെയും ഉയര്\u200dന്ന സമയത്തിന്\u200dറെയും സങ്കീര്\u200dണ്ണതയാലും,  ഈ ജോലിയില്\u200d, C-PCFGകളുടെ വേഗത്തില്\u200d നമ്മള്\u200d ആശ്രയിക്കുന്നു. അതിന്റെ വിലാസപ്രകാരം ചെയ്യാന്\u200d. ഞങ്ങള്\u200d മൂന്നു താക്കോലുകള്\u200d കണ്ടുപിടിച്ചുകൊടുക്കുന്നു: (1) സി- പിസിഎഫ്ജികള്\u200d ഡാറ്റ-ഫാക്ടിവെച്ചിരിക്കുന്നു, (2) സി- പിസിഎഫ്ജികള്\u200d പ്രത്യേക വാക്ക് നിയമങ്ങളിലെ ഏറ്റവും നല്ല വിവരങ്ങള', 'ms': 'Gramatika bebas konteks kemungkinan komponen (C-PCFGs) baru-baru ini telah menetapkan keadaan seni baru untuk induksi gramatika frasa-struktur. Namun, kerana masa-kompleksiti tinggi perwakilan berdasarkan kad dan kesimpulan, sukar untuk menyelidikinya secara menyeluruh. In this work, we rely on a fast implementation of C-PCFGs to conduct evaluation complementary to that of (CITATION).  Kami menyatakan tiga penemuan kunci: (1) C-PCFGs adalah data-efisien, (2) C-PCFGs membuat penggunaan terbaik maklumat tahap kalimat global dalam kebarangkalian peraturan sebelum tempoh, dan (3) konfigurasi terbaik C-PCFGs dalam bahasa Inggeris tidak sentiasa umumkan kepada bahasa kaya morfologi.', 'mt': 'Grammatiċi kumplessi probabilistiċi ħielsa mill-kuntest (C-PCFGs) reċentement stabbilixxew avvanz ġdid għall-induzzjoni grammatika tal-istruttura tal-frażi. However, due to the high time-complexity of chart-based representation and inference, it is difficult to investigate them comprehensively.  F’dan ix-xogħol, a ħna nistrieħu fuq implimentazzjoni mgħaġġla ta’ C-PCFGs biex twettaq evalwazzjoni kumplimentari għal dik ta’ (CITATION). Aħna tenfasizzaw tliet sejbiet ewlenin: (1) C-PCFGs huma effiċjenti fid-dejta, (2) C-PCFGs jagħmlu l-aħjar użu mill-informazzjoni globali fil-livell tas-sentenzi fil-probabbiltà tar-regola preterminali, u (3) l-aħjar konfigurazzjonijiet tal-C-PCFGs fuq l-Ingliż mhux dejjem jiġġeneralizzaw għal lingwi rikki fil-morfoloġija.', 'mk': 'Компондираните граматики без контекст (C-PCFGs) неодамна воспоставија нова технологија за индукција на граматиката со фраза-структура. Сепак, поради високата временска комплексност на претставувањето на картата и конференцијата, тешко е да се истражуваат целосно. In this work, we rely on a fast implementation of C-PCFGs to conduct evaluation complementary to that of (CITATION).  Ние ги истакнуваме трите клучни откритија: (1) Ц-ПЦФГ се ефикасни на податоци, (2) Ц-ПЦФГ се најдобро користат информации на глобално ниво на реченици во претерминалните веројатности на правилата, и (3) најдобрите конфигурации на Ц-ПЦФГ на англиски не секогаш се генерализираат на јазици богати', 'pl': 'Złożone prawdopodobieństwo bezkontekstowe gramatyki (C-PCFG) ustanowiły ostatnio nowy stan techniki w zakresie indukcji gramatyki struktury fraz. Jednak ze względu na dużą złożoność czasową reprezentacji i wniosków opartych na wykresach trudno jest je zbadać kompleksowo. W niniejszej pracy opieramy się na szybkim wdrożeniu C-PCFG w celu przeprowadzenia oceny uzupełniającej (CITATION). Podkreślamy trzy kluczowe ustalenia: (1) C-PCFG są wydajne danymi, (2) C-PCFG najlepiej wykorzystują globalne informacje na poziomie zdań w prawdopodobieństwach reguł przedterminalnych, a (3) najlepsze konfiguracje C-PCFG na angielskim nie zawsze uogólniają się do języków bogatych w morfologię.', 'lt': 'Sudėtinės gramatikos be konteksto (C-PCFG) neseniai sukūrė naują pažangą frazių struktūros gramatikos indukcijai. Vis dėlto dėl didelio grafinėmis sąlygomis pagrįsto atstovavimo ir išvadų sudėtingumo sunku juos išsamiai ištirti. Šiame darbe tikimės greito C-PCFG įgyvendinimo siekiant atlikti vertinimą, papildantį (CITATION) vertinimą. Mes atkreipiame dėmesį į tris pagrindinius faktus: 1) C-PCFG yra veiksmingi duomenys, 2) C-PCFG geriausiai naudoja pasaulinę informaciją apie sakinius iki galutinės taisyklės tikimybės atžvilgiu, ir 3) geriausios C-PCFG konfigūracijos anglų kalba ne visada paplito į daug morfologijos turinčias kalbas.', 'no': 'Sammensatte sannsynlige kontekstfri grammar (C-PCFG) har nyleg oppretta eit nytt tilstand til kunsten for grammatiske induksjon frå fråstrukturen. Det er imidlertid vanskeleg å undersøke dei kompleksiteten av diagrambasert representasjon og infeksjon. I denne arbeiden vert vi brukt på ein rask implementering av C-PCFG for å gjera evalueringskomplementar til det av (CITATION). Vi markerer tre nøkkelopplysningar: (1) C-PCFG er data-effektiv, (2) C-PCFG gjer det beste bruk av globale setningsnivåinformasjon i sannsynlege reglar, og (3) dei beste oppsetta av C-PCFG på engelsk er ikkje alltid generellisert til morfologiske rike språk.', 'mn': 'Хамтдаа магадгүй орчин үеийн үнэгүй грамм (C-PCFGs) саяхан phrase-structure грамматын үйлдвэрлэлийн урлагийн шинэ нөхцөл байгуулсан. Гэвч цаг хугацааны төвөгтэй төвөгтэй харилцааны төвөгтэй төвөгтэй байдалд тэднийг бүрэн судлах нь хэцүү. Энэ ажлын тулд бид C-PCFG-ийн хурдан хэрэгжүүлэхэд (CITATION) үүнийг нэмэгдүүлэх шалгалтыг хийх болно. Бид гурван чухал ололтуудыг тайлбарлаж байна: (1) С-PCFG нь өгөгдлийн үр дүнтэй, (2) С-PCFG нь дэлхийн өгүүлбэр-түвшин мэдээллийг хамгийн сайн хэрэглэдэг бөгөөд (3) Англи хэлний C-PCFG-ын хамгийн сайн тодорхойлолтууд үргэлж морфологи баян хэлний хувьд ерөнхийлөгдө', 'ro': 'Grammaticile compuse fără context probabilistic (C-PCFGs) au stabilit recent o nouă stare de tehnologie pentru inducția gramaticală a structurii frazelor. Cu toate acestea, datorită complexității ridicate de timp a reprezentării și deducției bazate pe diagrame, este dificil să le investigăm în mod cuprinzător. În această activitate, ne bazăm pe o implementare rapidă a C-PCFG pentru a efectua evaluări complementare celei de (CITATION). Evidențiem trei constatări cheie: (1) C-PCFG sunt eficiente din punct de vedere al datelor, (2) C-PCFG utilizează cel mai bine informațiile globale la nivel de frază în probabilitățile de regulă prematură, și (3) cele mai bune configurații ale C-PCFG în limba engleză nu generalizează întotdeauna la limbile bogate în morfologie.', 'so': 'Qoraalka ugu dhow waxaa laga yaabaa in ay sameeyaan xaalad cusub oo art for phrase-structure grammar induction. Si kastaba ha ahaatee waxaa ku adag in la baarito si buuxda ah. Markaas waxan waxaynu ku kalsoonnahay in dhaqso lagu soo dejiyo C-PCFGs si aan u sameeyo qiimeynta kaamilka ah oo lagu sameynayo (CITATION). Saddex baaritaan oo furan ah: (1) C-PCFGs waa data-efficient, (2) C-PCFGs ayaa ugu fiican isticmaala macluumaadka heerka caalamiga ah ee sharciga hore ku qoran, iyo (3) sawirada ugu fiican ee C-PCFGs ee Ingiriiska ku qoran marwalba ma generaliso luuqadaha morphology-rich ah.', 'sr': 'Nedavno su utvrdili novo stanje umetnosti za gramatičku indukciju fraze-strukture. Međutim, zbog visokog vremenskog kompleksnosti zastupanja i infekcije na grafiku, teško je da ih sveobuhvatno istražujemo. U ovom poslu se oslanjamo na brzu provedbu C-PCFG-a kako bi proveli dodatnu procjenu (CITATION). Podižemo tri ključna nalaza: (1) C-PCFG su efikasni podaci, (2) C-PCFG su najbolja upotreba globalnih informacija na nivou rečenica u verovatnoj pravilima, a (3) najbolja konfiguracija C-PCFG na engleskom jeziku ne uvek generalizuje na morfologiju bogate jezike.', 'ta': 'சாத்தியமான சூழல்- இலவச வரைப்படங்கள் (C- PCFGs) சமீபத்தில் சொற்றொடர்- structure grammar செயல்பாட்டிற்கான புதிய நிலையை உருவாக்கியது. ஆனால், அடிப்படையில் உள்ள மிக நேரம் சிக்கல் மற்றும் நோய் மற்றும் அதிகமாக இருக்கும் காரணத்தால், அவர்களை முழுமையாக ஆராய்ச்ச இந்த வேலையில், நாம் C-PCFGகள் வேகமாக செயல்படுத்த நம்பிக்கை கொண்டிருக்கிறோம் ( சிடிடேஷன்) என்று மதிப்பினை சிகிச் நாம் மூன்று விசை கண்டுபிடிப்புகளை தனிப்படுத்துகிறோம்: (1) C- PCFGs data-efficiency, (2) C- PCFGs முன்னிருப்பு விதியாசத்தில் உலகல வாக்கு- நிலை தகவலை சிறந்த பயன்படுத்துகிறது, மற்றும் (3) ஆங்கிலத்', 'si': 'සම්පූර්ණ සංභාවිත සංවේදනය නිදහස් ග්\u200dරාමාර්ස් (C-PCFGs) අලුත් වෙලාවට ප්\u200dරතිචාර්ය- සංවේදනය ග්\u200dරාමාර්ක නමුත්, චාර්ට් අධාරිත ප්\u200dරතිචාරයක් සහ ප්\u200dරතිචාරයක් නිසා අමාරුයි, ඒවා සම්පූර්ණයෙන් පරීක්ෂණය කරන්න අ මේ වැඩේ අපි විශ්වාස කරනවා C-PCFGs ගේ ඉක්මනින් ප්\u200dරමාණයක් විතරයි (CITATION) ගැන විශ්වාස කරන්න. අපි යතුරු තුනක් හොයාගන්න තියෙනවා: (1) C-PCFGs are data-Effect, (2) C-PCFGs make the optimum use of globe word-level info in antideterminal Rule Chancellities, and (3) the good setup of C-PCFGs on English do not always generoulize to Morology-rich language.', 'sv': 'Sammansatta sannolikhetsfria kontextfria grammater (C-PCFGs) har nyligen etablerat ett nytt toppmodernt sätt för frasstruktur grammatik induktion. Men på grund av den höga tidskomplexiteten hos diagrambaserad representation och slutsats är det svårt att undersöka dem ingående. I detta arbete förlitar vi oss på ett snabbt genomförande av C-PCFG för att genomföra utvärderingar som kompletterar (CITATION). Vi lyfter fram tre viktiga fynd: (1) C-PCFG är dataeffektiva, (2) C-PCFG använder sig bäst av global meningsnivå information i förterminal regelsannolikhet, och (3) de bästa konfigurationerna av C-PCFG på engelska generaliserar inte alltid till morfologirika språk.', 'ur': 'مجموعہ احتمالات منصفات کے منصفات گرامر (C-PCFGs) نے اچھے وقت فریز-ساختاری گراماری اپنا نئی حالت بنائی ہے۔ However, due to the high time-complexity of chart based representation and inference, it is difficult to investigate them fully. اس کام میں ہم نے C-PCFGs کے سریع عملومات پر امانت رکھا ہے کہ اس کے مطابق (CITATION) کا مطابق ارزیابی کریں۔ ہم تین کلیدوں کی پیدا کرتی ہیں: (1) C-PCFGs ڈاٹ-فعالیت ہیں, (2) C-PCFGs کلوب لفظ-سطح معلومات کی بہترین استعمال کرتی ہیں، اور (3) انگلیسی پر سی-PCFGs کی بہترین پیکربندی ہمیشہ موفورپولوژی-ثروت زبانوں میں نہیں آتی۔', 'vi': 'C ác tạp chí phức tạp tự do ngữ cảnh (C-PCFG) gần đây đã thiết lập một trạng thái mới cho ngữ pháp từ điển thành ngữ. Tuy nhiên, vì sự phức tạp thời gian cao của mô hình biểu đồ và ngụ ý, nó rất khó điều tra chúng một cách to àn diện. Trong công việc này, chúng ta dựa vào một tiến triển nhanh chóng của C-PCFG để thực hiện đánh giá bổ sung với Thị trưởng (độc lập). Chúng tôi nhấn mạnh ba phát hiện chìa khóa: 1) C-PCFG là hiệu quả dữ liệu, 2) C-PCFG sử dụng tốt nhất thông tin cấp bản án to àn cầu trong khả năng quy định chuẩn, và (3) cấu hình tốt nhất của C-PCFG trên tiếng Anh không phải lúc nào cũng tổng hợp thành ngôn ngữ có nhiều chuẩn.', 'uz': "Name Lekin, chart tashkilotlari va kichkina murakkablik sababi, ularni butunlay o'rganish juda qiyin. Bu vazifani biz C-PCFG'larni tez ishlashga ishonchinamiz va bu narsalarni qiymatga murakkab qilish uchun. Biz uchta kalit natijalarini kichiklashtiramiz: (1) C-PCFGs data-effekti, (2) C-PCFGs pre-terminal qoidadagi dunyo darajadagi imkoniyatlarni eng yaxshi foydalanish mumkin, va (3) ingliz tilidagi C-PCFGning eng yaxshi moslamalari har doim morfologiy- rich tillarda umumiy emas.", 'bg': 'Съединените вероятностни граматики без контекст наскоро създадоха ново състояние на изкуството за индукция на граматиката на фраза-структура. Въпреки това, поради високата времева сложност на графичното представяне и заключение, е трудно да се изследват изчерпателно. В тази работа разчитаме на бързото прилагане на С-ПКФР за извършване на оценка, допълваща тази на (ЦИТАЦИЯ). Подчертаваме три ключови констатации: (1) С-ПСФГ са ефективни с данни, (2) С-ПСФГ използват най-добре глобалната информация за ниво изречение при вероятността за претерминални правила и (3) най-добрите конфигурации на С-ПСФГ на английски език не винаги се обобщават към богати на морфология езици.', 'nl': "Samengestelde probabilistische contextvrije grammatica's (C-PCFG's) hebben onlangs een nieuwe stand van de techniek voor frase-structure grammatica inductie vastgesteld. Vanwege de hoge tijdcomplexiteit van grafiekgebaseerde representatie en inferentie is het echter moeilijk om deze uitvoerig te onderzoeken. In dit werk vertrouwen we op een snelle implementatie van C-PCOV's om evaluaties uit te voeren die complementair zijn aan die van (CITATION). We benadrukken drie belangrijke bevindingen: (1) C-PCFG's zijn data-efficiënt, (2) C-PCFG's maken het beste gebruik van globale informatie op zinsniveau in preterminale regel waarschijnlijkheden, en (3) de beste configuraties van C-PCFG's op Engels generaliseren niet altijd naar morfologische rijke talen.", 'da': "Sammensatte sandsynlighedsmæssige kontekstfrie grammatikker (C-PCFGs) har for nylig etableret en ny state of te art for sætningsstruktur grammatik induktion. På grund af den høje tidskompleksitet af diagrambaseret repræsentation og konklusion er det imidlertid vanskeligt at undersøge dem omfattende. I dette arbejde er vi afhængige af en hurtig gennemførelse af C-PCFG'er til at gennemføre evaluering, der supplerer (CITATION). Vi fremhæver tre vigtige resultater: (1) C-PCFG'er er dataeffektive, (2) C-PCFG'er gør den bedste brug af globale sætningsniveau oplysninger i præerminale regelsandsynligheder, og (3) de bedste konfigurationer af C-PCFG'er på engelsk genererer ikke altid til morfologirige sprog.", 'hr': 'Vjerojatnost složenih gramara bez konteksta (C-PCFG) nedavno je utvrdio novo stanje umjetnosti za gramatičku indukciju fraze strukture. Međutim, zbog visokog vremenskog kompleksnosti zastupanja i infekcije na grafiku, teško je ih posvećeno istražiti. U ovom poslu se oslanjamo na brzu provedbu C-PCFG-a kako bi proveli procjenu dodatno na to (CITATION). Podsjećamo tri ključna nalaza: (1) C-PCFG-ovi su učinkoviti podataka, (2) C-PCFG-ovi čine najbolje upotrebu informacija o razini rečenica na globalnoj rečenici u vjerojatnosti pretvaranja pravila, a (3) najbolje konfiguracije C-PCFG-ova na engleskom jeziku ne uvijek generaliziraju na morfologiju bogate jezike.', 'de': 'Compound probabilistische kontextfreie Grammatiken (C-PCFGs) haben kürzlich einen neuen Stand der Technik für Phrasenstrukturgrammatikinduktion etabliert. Aufgrund der hohen zeitlichen Komplexität der chartbasierten Darstellung und Inferenz ist es jedoch schwierig, diese umfassend zu untersuchen. In dieser Arbeit setzen wir auf eine schnelle Implementierung von C-PCFGs, um ergänzende Evaluierungen durchzuführen (CITATION). Wir heben drei wichtige Ergebnisse hervor: (1) C-PCFGs sind dateneffizient, (2) C-PCFGs nutzen globale Informationen auf Satzebene in präterminalen Regelwahrscheinlichkeiten am besten und (3) die besten Konfigurationen von C-PCFGs auf Englisch verallgemeinern sich nicht immer auf morphologische Sprachen.', 'id': 'Gramatika bebas konteks probabilis komponen (C-PCFGs) baru-baru ini telah menetapkan state of the art baru untuk induksi gramatika frasa-struktur. Namun, karena waktu-kompleksitas tinggi dari representation berdasarkan grafik dan kesimpulan, sulit untuk menyelidikinya secara menyeluruh. Dalam pekerjaan ini, kami bergantung pada implementasi cepat C-PCFG untuk melakukan evaluasi komplementari dengan (CITATION). Kami mempertimbangkan tiga penemuan kunci: (1) C-PCFG adalah data-efisien, (2) C-PCFG membuat penggunaan terbaik dari informasi global tingkat kalimat dalam kemungkinan peraturan preterminal, dan (3) konfigurasi terbaik dari C-PCFG dalam bahasa Inggris tidak selalu menyebar ke bahasa kaya morfologi.', 'fa': 'جمع احتمالات گرامرات آزاد محیط (C-PCFGs) اخیراً یک وضعیت جدید از هنر برای تولید گرامرات ساختار عبارت ساخته شده است. با این حال، به دلیل پیچیدگی زمان بالا از نمایش و آلودگی بر اساس نقشه\u200cها، تحقیق کردن آنها کاملاً سخت است. در این کار، ما بر سریع عملکرد C-PCFG برای انجام ارزیابی با آن (CITATION) اعتماد می\u200cکنیم. ما سه نتیجه کلید را مشخص می\u200cکنیم: (۱) C-PCFGs اطلاعات تاثیر\u200cپذیر هستند, (۲) C-PCFGs بهترین استفاده از اطلاعات سطح جمله جهانی در احتمال قانون تغییر قانون می\u200cکنند، و (۳) بهترین تنظیمات C-PCFGs در انگلیسی همیشه به زبان\u200cهای پولدار مورفولوژی نمی', 'ko': '복합확률상하문무관문법(C-PCFGs)은 최근 단어구조문법귀납을 위한 새로운 기술을 세웠다.그러나 도표를 바탕으로 하는 표시와 추리는 매우 높은 시간 복잡성을 가지기 때문에 이를 전면적으로 연구하기 어렵다.이 작업에서 우리는 C-PCFGs의 신속한 실현에 의존하여 (인용문) 평가와 상호 보완적인 평가를 진행한다.우리는 세 가지 관건적인 발견을 강조했다. (1) C-PCFGs는 데이터가 효율적이고 (2) C-PCFGs는 전 종결 규칙 확률에서 전역 문장급 정보를 충분히 이용했다. (3) C-PCFGs가 영어에서 가장 좋은 배치는 형태가 풍부한 언어로 항상 보급되지 않는다.', 'tr': "Birleşik mykdarlyk kontekstsuz grammalar (C-PCFG) fraz strukturasy gramatik indukçy üçin bir sanat taýýarlapdyr. Çizelge dayanan temsillerin ve aşağılıkların yüksek zamanlı karmaşıklığına göre, onları bütünleşmelikle incelemek zor. Bu işde C-PCFG'laryň (CITATION) ýagdaýyny üýtgetmek üçin tiz implementasyna ynanýarys. Biz üç açyk netijesi ýagtylaýarys: (1) C-PCFGlar data etkinlik edendir, (2) C-PCFGlar dünýä sözlem derejesi mümkinçiliklerde dünýä sözlem derejesi bilen ullanýar we (3) Iňlisçe C-PCFGlaryň iň gowy düzümlerni hemişe morphologiýa-baý dillere döredilmez.", 'sw': 'Hivi karibuni wameanzisha hali mpya ya sanaa kwa ajili ya kutengeneza karatasi za mifumo-muundo. Hata hivyo, kwa sababu ya utata mkubwa wa kuwawakilisha chart na udhalilishaji, ni vigumu kuwachunguza kwa ujumla. Katika kazi hii, tunategemea utekelezaji wa haraka wa C-PCFG kutekeleza uchunguzi wa tathmini unaohusika na hilo (CITATION). Tunaonyesha matokeo matatu ya ufunguo: (1) C-PCFGs ni yenye ufanisi wa data, (2) C-PCFGs hufanya matumizi bora ya taarifa za kiwango cha hukumu duniani katika uwezekano wa utawala wa kizamani, na (3) miundombinu bora ya C-PCFGs katika Kiingereza haijajumuisha lugha zenye utajiri wa kifolojia.', 'af': "Kombineerde waarskynlik kontekstvry gramme (C-PCFG) het onlangs 'n nuwe staat van die kuns vir frase-struktuur grammatiese induksie geïnstalleer. Maar vanweë die hoë tyd-kompleksiteit van kaart-gebaseerde voorstelling en inferensie, is dit moeilik om hulle kompleksief te ondersoek. In hierdie werk vertrou ons op 'n vinnige implementasie van C-PCFG om die evaluasie complementeer tot dit van (CITATION) te doen. Ons verlig drie sleutel vindings: (1) C-PCFG is data-effektief, (2) C-PCFG maak die beste gebruik van globaal seting-vlak inligting in preterminaal reël waarskynlikheid, en (3) die beste opstelling van C-PCFG op Engels doen nie altyd genereer na morfologie-ryk tales nie.", 'sq': "Gramatikat e kompozuara pa kontekst probabilist (C-PCFGs) kanë krijuar kohët e fundit një gjendje të re të artit për induksionin gramatik të frazës-strukturës. Megjithatë, për shkak të ndërlikueshmërisë së lartë kohore të përfaqësimit dhe përfundimit të bazuar në harta, është e vështirë t'i hetojmë ato në mënyrë të plotë. In this work, we rely on a fast implementation of C-PCFGs to conduct evaluation complementary to that of (CITATION).  Ne theksojmë tre gjetje kryesore: (1) C-PCFGs janë të efektshme me të dhënat, (2) C-PCFGs bëjnë përdorimin më të mirë të informacionit të nivelit të dënimit botëror në probabilitetet e rregullave paraprake dhe (3) konfigurimet më të mira të C-PCFGs në anglisht nuk gjeneralizohen gjithmonë në gjuhë të pasura me morfologji.", 'am': 'phrase-structure grammar induction ምንም እንኳን፣ ካርታ-አካላት እና በቁጥር ግንኙነት ምክንያት በሙሉ መፍጠር አግኝቷል፡፡ በዚህ ስራ ላይ የCPCFG ፈጥኖ ማድረግ እናምናለን፡፡ ሦስት የቁልፎች ፍላጎቶችን እናሳውቃለን:(1) C-PCFGs ዳታ-efficient ናቸው:(2) C-PCFGs የዓለምአቀፍ ቃላት-ደረጃ መረጃ በተሻለ የፊርማኔ ሥርዓት ግንኙነት እንዲጠቀም እና (3) የC-PCFGs በንግግሊዝኛ ላይ በመልካም ምርጫዎች ዘወትር ለሞፎሎጂ-ሀብታም ቋንቋዎች አይጠቅሙም።', 'az': "Müxtəlif mümkünlük kontekst boş grammalar (C-PCFG) çox yaxın zamanda fraz-yapısı grammatik induksyonu üçün sanatın yeni bir durumu qurdu. Lakin, grafik-tabanlı göstəricilərin və aşağılıqların yüksək vaxt kompleksitəsi üzündən onları bütün sınamaq çətin. Bu işdə, C-PCFG'lərin tələsik istifadə edilməsi üçün (CITATION) ilə müəyyən edilməsi üçün təvəkkül edirik. Biz üç anahtar çəkilməsini işıqlandırırıq: (1) C-PCFG verilən faydalandırıcıdır, (2) C-PCFG qlobal cümə-seviyyəti məlumatının ən yaxşı istifadəsini müəyyən edir, və (3) İngilizce dilində C-PCFG'nin ən yaxşı yapılandırmaları hər zaman morfolojik zənginli dillərə generalizə edilməz.", 'bs': 'Vjerojatnost složenih gramara bez konteksta (C-PCFG) nedavno su uspostavili novo stanje umjetnosti za gramatičku indukciju fraze-strukture. Međutim, zbog visokog vremenskog kompleksnosti zastupanja i infekcije na grafiku, teško je ih sveobuhvatno istražiti. U ovom poslu se oslanjamo na brzu provedbu C-PCFG-a kako bi proveli procjenu dodatno na to (CITATION). Naciljamo tri ključna otkrića: (1) C-PCFGs su efikasni podaci, (2) C-PCFGs najbolje upotrebljavaju globalne informacije o nivou kazne u verovatnoj pravilima, a (3) najbolje konfiguracije C-PCFGs na engleskom jeziku ne uvijek generalizuju na morfologiju bogate jezike.', 'ca': "Gramàtiques combinades sense contest probabilístic (C-PCFG) han establit recentment un nou estat d'art per a l'inducció gramàtica de frases-estructura. Però, degut a la gran complexitat del temps de la representació basada en els gràfics i la inferència, és difícil investigar-los de manera completa. En aquesta feina, confiem en una implementació ràpida de C-PCFG per fer una evaluació complementar a la de (CITATION). Hem destacat tres descobriments clau: (1) els C-PCFG són eficients en dades, (2) els C-PCFG fan el millor ús de la informació global sobre el nivell de frases en probabilitats de regla pre-terminal, i (3) les millors configuracions dels C-PCFG en anglès no sempre s'generalitzen a llengües rics en morfologia.", 'hy': 'Հիմնական հավանական գրամագրությունները առանց կոնտեքստի (C-PDFGs) վերջերս ստեղծել են նոր տեխնոլոգիա արտահայտության համար արտահայտության-կառուցվածքի գրամագրական ինդուկցիայի համար: Այնուամենայնիվ, քարտեզի հիմնված ներկայացման և եզրակացության բարդ ժամանակի պատճառով, դժվար է դրանք ամբողջովին ուսումնասիրել: Այս աշխատանքի մեջ մենք հույս ունենք C-PFG-ների արագ իրականացման վրա, որպեսզի կատարենք գնահատումը, որը համալրացվում է (CITItion) մոտ: Մենք ներկայացնում ենք երեք կարևոր հայտնաբերություններ: (1) C-PFG-ները տվյալներ արդյունավետ են, (2) C-PFG-ները լավագույն օգտագործում են գլոբալ նախադասությունների մակարդակի տեղեկատվությունը նախադասության կանոնների հավանականության մեջ, և (3) C-PFG-ների լավագույն կազմակերպությունները անգլերենում միշտ չեն ընդհանուր', 'et': 'Kombineeritud tõenäosuslikud kontekstivabad grammatikad (C-PCFG) on hiljuti loonud uue tehnika fraasistruktuuri grammatika induktsiooniks. Diagrammipõhise esituse ja järelduste suure aja keerukuse tõttu on aga raske neid põhjalikult uurida. Selles töös toetume C-PCFG kiirele rakendamisele, et viia läbi hindamine, mis täiendab (CITATION) hindamist. Rõhutame kolme peamist tulemust: (1) C-PCFG on andmetõhusad, (2) C-PCFG kasutab parimal viisil üldist lausetaseme teavet ennetähtaegse reegli tõenäosuse korral ja (3) C-PCFG parimad konfiguratsioonid inglise keeles ei üldista alati morfoloogiarikkatele keeltele.', 'bn': 'সম্ভাব্য কন্টেক্স-ফ্রি গ্রামার (C-PCFGs) সম্প্রতি ব্যাক্তি-কাঠামো গ্রামার শিল্পের জন্য নতুন অবস্থা স্থাপন করেছে। তবে চার্ট-ভিত্তিক প্রতিনিধিত্ব এবং অসংক্রান্ত বেশী সময়-জটিলতার কারণে তাদের সম্পূর্ণ তদন্ত করা কঠিন। এই কাজে আমরা সি-পিসিএফজির দ্রুত বাস্তবায়িত্ব নির্ভর করি যাতে এই বিষয়টির (সিটাশন) মূল্যায়নের জটিল করা যায়। আমরা তিনটি কী ফিনিসের উল্লেখ করেছি: (1) সি-পিসিএফজি ডাটা কার্যকর, (2) সি-পিসিএফজি প্রথম নিয়মিনাল শাসনের সবচেয়ে ভালো তথ্য ব্যবহার করেছে, আর (3) ইংরেজীতে সি-পিসিএফজির সবচেয়ে ভাল কনফিগার', 'cs': 'Složené pravděpodobnostní bezkontextové gramatiky (C-PCFG) nedávno zavedly nový stav techniky pro indukci frázové struktury gramatiky. Nicméně vzhledem k vysoké časové složitosti grafové reprezentace a inference je obtížné je komplexně zkoumat. V této práci se spoléháme na rychlou implementaci C-PCFG pro provádění doplňkového hodnocení (CITATION). Zdůrazňujeme tři klíčové zjištění: (1) C-PCFG jsou datově efektivní, (2) C-PCFG nejlépe využívají globální informace na úrovni vět v pravděpodobnostech předčasného pravidla a (3) nejlepší konfigurace C-PCFG v angličtině se vždy zobecňují na jazyky bohaté na morfologii.', 'fi': 'Yhdistetyt todennäköiset kontekstivapaat kieliopit (C-PCFG) ovat hiljattain luoneet uuden tekniikan fraasirakenteen kieliopin induktioon. Kaaviopohjaisen esittämisen ja päättelyn suuren aikamonimutkaisuuden vuoksi niitä on kuitenkin vaikea tutkia kattavasti. Tässä työssä luotamme C-PCFG:n nopeaan täytäntöönpanoon (CITATION) täydentävien arviointien suorittamiseksi. Korostamme kolmea keskeistä havaintoa: (1) C-PCFG:t ovat datatehokkaita, (2) C-PCFG:t hyödyntävät parhaiten globaalia lausetason tietoa ennenaikaisissa sääntötodennäköisyyksissä ja (3) C-PCFG:n parhaat konfiguraatiot englanniksi eivät aina yleisty morfologiarikkaisiin kieliin.', 'jv': 'politenessoffpolite"), and when there is a change ("assertivepoliteness Nanging, ngendong akeh langgar-akeh pisan neng akeh lanjut komplikasi diagram-pakan lan kelas, kuwi susahe kanggo ujian akeh di akeh akeh. Nang barêng-barêng iki, kéné iso ngewehi luwih operasi C-PFGs kanggo ngilangno langgar tarjamahan sing wis ambang (CITITITI). politenessoffpolite"), and when there is a change ("assertivepoliteness', 'he': 'גרמטיקות מסובכות ללא הקשר (C-PCFGs) קיבלו לאחרונה מצב חדש של האמנות עבור induction גרמטיקה מבנה ביטויים. בכל אופן, בגלל המורכבות הזמן הגבוהה של מייצג ומסכם מבוסס על רשימות, קשה לחקור אותם באופן כללי. בעבודה הזו, אנו סומכים על ההפעלה מהירה של C-PCFG כדי לבצע עריכה תוספת לזה של (CITATION). We highlight three key findings: (1) C-PCFGs are data-efficient, (2) C-PCFGs make the best use of global sentence-level information in preterminal rule probabilities, and (3) the best configurations of C-PCFGs on English do not always generalize to morphology-rich languages.', 'sk': 'Sestavljene verjetnostne slovnice brez konteksta (C-PCFG) so pred kratkim vzpostavile novo stanje tehnike za indukcijo slovnice frazne strukture. Vendar pa jih je zaradi velike časovne kompleksnosti grafične reprezentacije in sklepanja težko celovito raziskati. Pri tem delu se zanašamo na hitro izvajanje C-PCFG za izvajanje vrednotenja, ki dopolnjuje vrednotenje (CITATION). Poudarjamo tri ključne ugotovitve: (1) C-PCFG so podatkovno učinkoviti, (2) C-PCFG kar najbolje izkoriščajo globalne informacije o nivoju stavkov pri verjetnostih preddokončnih pravil in (3) najboljše konfiguracije C-PCFG v angleščini ne posplošujejo vedno na morfološko bogate jezike.', 'bo': 'མཉམ་སྒྲིག་གི་ཆེད་འཛིན ཡིན་ནའང་། རིགས་རིས་གཞི་བཙུགས་ཀྱི་དུས་ཚོད་ལས་ཆེན་རྐྱེན་བྱས་པ་དེ་ལྟར་དཀའ་ངལ་ཡོད། འོན་ཀྱང་། ང་ཚོས་C-PCFGs་ལ་མགྱོག་ངལ་རྩོལ་བ་འདིའི་ནང་གི་ཉེན་ཁ་ཇི་ལྟར་མཇུག་བསྡུ་ཡོད། We highlight three key findings: (1) C-PCFGs are data-efficient, (2) C-PCFGs make the best use of global sentence-level information in preterminal rule probabilities, and (3) the best configurations of C-PCFGs on English do not always generalize to morphology-rich languages.', 'ha': "KCharselect unicode block name A lokacin da ya yi adadi ga masu amintar da karatun na karatun, sai ya yi nauyi a gane su da fasan. Daga wannan aikin, munã dõgara a kan aikin ƙwarai da ake amfani da C-PCFGs dõmin ya aikata evaluation da kamfata zuwa wannan (CITATI). Tuna ƙayyade fassaran nau'i uku: (1) C-PCFGs ne masu da amfani da data-mai amfani, (2) C-PCFGs za'a aikata mafi kyaun amfani da amfani da shaidar-daraja a cikin kunnufi masu yiwuwa a gabatar da, kuma (3) mafi kyaun canza zaɓallin C-PCFGs a kan Ingiriya ba ta zama daidai zuwa lugha-matalauci."}
{'en': 'Effective Distant Supervision for Temporal Relation Extraction', 'fr': "Supervision à distance efficace pour l'extraction de la relation", 'pt': 'Supervisão Distante Eficaz para Extração de Relação Temporal', 'es': 'Supervisión a distancia efectiva para la extracción de relaciones temporales', 'ar': 'الإشراف الفعال عن بعد لاستخراج العلاقة الزمنية', 'ja': '時間的関係抽出のための効果的な遠隔監督', 'zh': '时之所取者远程督之', 'hi': 'अस्थायी संबंध निष्कर्षण के लिए प्रभावी दूर पर्यवेक्षण', 'ru': 'Эффективное дистанционное наблюдение за темпоральной экстракцией', 'ga': 'Maoirseacht Chian Éifeachtach le haghaidh Eastóscadh Caidreamh Sealadach', 'ka': 'Name', 'el': 'Αποτελεσματική εξ αποστάσεως εποπτεία για την εξαγωγή προσωρινών σχέσεων', 'hu': 'Hatékony távoli felügyelet az ideiglenes kapcsolatok kivonására', 'it': "Supervisione a distanza efficace per l'estrazione di relazioni temporali", 'lt': 'Veiksminga nuotolinė laikinųjų santykių ekstrakcijos priežiūra', 'mk': 'Ефективна оддалечена надзора за екстракција на временска врска', 'kk': 'Температуралық қатынастарды тарқату үшін эффективті қашықтық қашықтығын бақылау', 'mt': 'Superviżjoni Distanza Effettiva għall-Estrazzjoni tar-Relazzjoni Temporali', 'ms': 'Pengawalan Jauh Efektif untuk Ekstraksi Hubungan Temporal', 'ml': 'താല്\u200dപ്പരാല്\u200d ബന്ധപ്പെടുത്തുന്നതിനുള്ള സൂപ്പര്\u200dവ്വതനം', 'mn': 'Температурын харилцааны хайлтын эффектив хол нөлөөлөлт', 'pl': 'Skuteczny nadzór na odległość dla ekstrakcji relacji czasowych', 'ro': 'Supraveghere efectivă la distanță pentru extragerea relațiilor temporale', 'sr': 'Efektivni udaljeni nadzor za ekstrakciju privremenih odnosa', 'si': 'Name', 'so': 'Heeganka degmada', 'sv': 'Effektiv fjärrövervakning för utvinning av temporal relation', 'ta': 'தற்காலிக உறவு பிரித்தலுக்கான விளைவான தூரத்தை கண்காணிப்பு', 'no': 'Effektivt avstand- oversikt for ekstraksjon av mellombels relasjon', 'ur': 'Name', 'uz': 'Comment', 'vi': 'Hiệu quả kiểm soát xa đối với Thời Gian', 'bg': 'Ефективен дистанционен надзор за извличане на времеви взаимоотношения', 'hr': 'Efektivni udaljeni nadzor za ekstrakciju privremenih odnosa', 'da': 'Effektiv fjerntilsyn med udvinding af tidsmæssige relationer', 'nl': 'Effectief toezicht op afstand voor tijdelijke relatieontwikkeling', 'de': 'Effektive Fernüberwachung für die temporale Beziehungsextraktion', 'ko': '시간 관계 추출의 효과적인 원격 모니터링', 'id': 'Pengawalan Jauh Efektif untuk Ekstraksi Hubungan Temporal', 'fa': 'بررسی فاصله\u200cای برای اخراج رابطه\u200cهای معمولی', 'sw': 'Tovuti ya Kutokana na Ushirikiano', 'tr': '=', 'af': 'Name', 'sq': 'Kontroll efektiv i largët për nxjerrjen e marrëdhënieve të përkohshme', 'az': 'Temporal Relationship Extraction üçün Etkinlik Uzaqlı Gözləmə', 'am': 'ምርጫዎች', 'bs': 'Efektivni udaljeni nadzor za ekstrakciju privremenih odnosa', 'ca': "Supervisió efectiva a distància per l'extracció de relacions temporals", 'cs': 'Účinný vzdálený dohled pro extrakci časových vztahů', 'et': 'Tõhus kaugjärelevalve ajalise suhte väljavõtmiseks', 'fi': 'Aikasuhteiden poiston tehokas etävalvonta', 'bn': 'Temporal Relation Extractionের জন্য সক্রিয় দূরত্ব সুপারেশন', 'hy': 'Comment', 'jv': 'Effectve Distant super view for Tenoral Relative extract', 'ha': 'KCharselect unicode block name', 'sk': 'Učinkovit daljni nadzor za ekstrakcijo časovnih odnosov', 'bo': 'སྤྱིར་བཏང་བའི་བར་ཐག་མཐོང་དང་མཉམ་སྦྲེལ་མཐུད་ཕྱིར་བསྐྲུན་གཏོང', 'he': 'פיקוח מרוחק אפקטיבי לחלץ יחסים זמניים'}
{'en': 'A principal barrier to training temporal relation extraction models in new domains is the lack of varied, high quality examples and the challenge of collecting more. We present a  method  of automatically collecting distantly-supervised examples of temporal relations. We scrape and automatically label event pairs where the temporal relations are made explicit in text, then mask out those explicit cues, forcing a  model  trained on this  data  to learn other signals. We demonstrate that a pre-trained Transformer model is able to transfer from the weakly labeled examples to human-annotated benchmarks in both zero-shot and few-shot settings, and that the masking scheme is important in improving generalization.', 'es': 'Una barrera principal para el entrenamiento de modelos de extracción de relaciones temporales en nuevos dominios es la falta de ejemplos variados y de alta calidad y el desafío de recopilar más. Presentamos un método para recopilar automáticamente ejemplos de relaciones temporales supervisados a distancia. Scrapamos y etiquetamos automáticamente los pares de eventos donde las relaciones temporales se hacen explícitas en el texto, luego enmascaramos esas señales explícitas, obligando a un modelo entrenado en estos datos a aprender otras señales. Demostramos que un modelo de Transformer previamente entrenado es capaz de pasar de los ejemplos débilmente etiquetados a puntos de referencia anotados por humanos tanto en configuraciones de tiro cero como de pocos disparos, y que el esquema de enmascaramiento es importante para mejorar la generalización.', 'ar': 'يتمثل العائق الرئيسي لتدريب نماذج استخراج العلاقة الزمنية في المجالات الجديدة في الافتقار إلى أمثلة متنوعة وعالية الجودة والتحدي المتمثل في جمع المزيد. نقدم طريقة لجمع أمثلة العلاقات الزمنية الخاضعة للإشراف عن بُعد تلقائيًا. نقوم بكشط وتسمية أزواج الأحداث تلقائيًا حيث تكون العلاقات الزمنية واضحة في النص ، ثم نخفي تلك الإشارات الواضحة ، مما يجبر نموذجًا مدربًا على هذه البيانات على تعلم إشارات أخرى. نوضح أن نموذج المحولات المدرب مسبقًا قادر على الانتقال من الأمثلة ذات العلامات الضعيفة إلى معايير مشروحة بشريًا في كل من إعدادات اللقطة الصفرية والقليلة ، وأن مخطط التقنيع مهم في تحسين التعميم.', 'pt': 'Uma barreira principal para treinar modelos de extração de relações temporais em novos domínios é a falta de exemplos variados e de alta qualidade e o desafio de coletar mais. Apresentamos um método de coleta automática de exemplos de relações temporais supervisionados à distância. Raspamos e rotulamos automaticamente pares de eventos onde as relações temporais são explicitadas no texto, depois mascaramos essas pistas explícitas, forçando um modelo treinado nesses dados a aprender outros sinais. Demonstramos que um modelo Transformer pré-treinado é capaz de transferir de exemplos fracamente rotulados para benchmarks anotados por humanos em configurações de zero e poucos tiros, e que o esquema de mascaramento é importante para melhorar a generalização.', 'fr': "L'un des principaux obstacles à l'apprentissage des modèles d'extraction de relations temporelles dans de nouveaux domaines est le manque d'exemples variés et de haute qualité et le défi d'en collecter davantage. Nous présentons une méthode de collecte automatique d'exemples de relations temporelles supervisés à distance. Nous grattons et étiquetons automatiquement les paires d'événements où les relations temporelles sont explicites dans le texte, puis nous masquons ces indices explicites, forçant un modèle formé à partir de ces données à apprendre d'autres signaux. Nous démontrons qu'un modèle de transformateur pré-entraîné est capable de passer des exemples faiblement étiquetés à des repères annotés par l'homme dans les paramètres de tir zéro et de faible dose, et que le schéma de masquage est important pour améliorer la généralisation.", 'ja': '新しいドメインで時間的関係抽出モデルをトレーニングするための主な障壁は、多様で高品質な例の欠如と、より多く収集することの課題です。遠隔監督された時間関係の例を自動的に収集する方法を提示する。時間的関係がテキストで明示されているイベントペアをスクレイブして自動的にラベル付けし、それらの明示的なキューをマスクして、このデータでトレーニングされたモデルに他の信号を学習させます。事前に訓練されたトランスフォーマーモデルは、ゼロショットと数ショットの両方の設定で、弱いラベル付けされた例から人間が注釈したベンチマークに転送することができ、マスキングスキームが一般化を改善する上で重要であることを実証します。', 'hi': 'नए डोमेन में अस्थायी संबंध निष्कर्षण मॉडल के प्रशिक्षण के लिए एक प्रमुख बाधा विविध, उच्च गुणवत्ता वाले उदाहरणों की कमी और अधिक एकत्र करने की चुनौती है। हम अस्थायी संबंधों के दूरस्थ रूप से पर्यवेक्षित उदाहरणों को स्वचालित रूप से एकत्र करने की एक विधि प्रस्तुत करते हैं। हम स्क्रैप करते हैं और स्वचालित रूप से ईवेंट जोड़े को लेबल करते हैं जहां अस्थायी संबंधों को पाठ में स्पष्ट किया जाता है, फिर उन स्पष्ट संकेतों को मुखौटा करते हैं, जिससे इस डेटा पर प्रशिक्षित मॉडल को अन्य संकेतों को सीखने के लिए मजबूर किया जाता है। हम प्रदर्शित करते हैं कि एक पूर्व-प्रशिक्षित ट्रांसफॉर्मर मॉडल कमजोर रूप से लेबल किए गए उदाहरणों से शून्य-शॉट और कुछ-शॉट सेटिंग्स दोनों में मानव-एनोटेट बेंचमार्क में स्थानांतरित करने में सक्षम है, और यह कि मास्किंग योजना सामान्यीकरण में सुधार करने में महत्वपूर्ण है।', 'ru': 'Основным препятствием на пути обучения моделям извлечения временных связей в новых областях является отсутствие разнообразных, высококачественных примеров и проблема сбора большего количества данных. Представляем метод автоматического сбора дистанционно контролируемых примеров временных отношений. Мы соскабливаем и автоматически помечаем пары событий, где временные отношения явно выражены в тексте, а затем маскируем эти явные сигналы, заставляя модель, обученную этим данным, изучать другие сигналы. Мы демонстрируем, что предварительно обученная модель трансформатора способна переходить от слабо обозначенных примеров к аннотированным человеком эталонам как в настройках с нулевым, так и с малым выстрелом, и что схема маскировки важна для улучшения обобщения.', 'zh': '新领时态关系提模一障,乏多样化、高质量示例及收更多示例挑战。 举自收日月远监示例之法。 我们抓取自己标记时间关系在文本中显式事对,然后屏蔽这些显式线索,使基于这数据训练的模样学习他的信号。 臣等证明,预练Transformer模形,能于零次少射设中从弱标记者示例移于人工注注之准,而掩方于改善泛化甚重。', 'ga': 'Príomh-bhacainn ar oiliúint a chur ar mhúnlaí asbhainte caidrimh ama i réimsí nua is ea an easpa samplaí ilghnéitheacha ardchaighdeáin agus an dúshlán a bhaineann le níos mó a bhailiú. Cuirimid i láthair modh chun samplaí de chaidreamh ama atá faoi mhaoirseacht i bhfad i gcéin a bhailiú go huathoibríoch. Scríobaimid agus déanaimid lipéadú go huathoibríoch ar phéirí teagmhais ina ndéantar an caidreamh ama soiléir i dtéacs, ansin folaímid na leideanna follasacha sin, rud a chuireann brú ar mhúnla atá oilte ar na sonraí seo chun comharthaí eile a fhoghlaim. Léirímid go bhfuil samhail Trasfhoirmeora réamhoilte in ann aistriú ó na samplaí lag-lipéadaithe go tagarmharcanna a bhfuil anót daonna orthu i suíomhanna náid agus cúpla seat, agus go bhfuil an scéim chumhdaigh tábhachtach chun ginearálú a fheabhsú.', 'ka': 'პრინციპური ბარიერი ტემპალური შესახებ ექსტრექციის მოდელების შესახებ ახალი დიომენში არის განსხვავებული, მეტი კალგატიური მაგალითების და მეტი შესახებ. ჩვენ ავტომატურად დავყენებთ განსხვავებული დამართებული მაგალითების მაგალითი. ჩვენ მოვლენების თავსუფალური შესახებ ტექსტში გავაკეთებთ და ავტომატურად აღწერებთ, სადაც ტექსტში თავსუფალური შესახებ გავაკეთება, და შემდეგ მაქსიკურად გავაკეთებთ ამ მოვლენების მოდე ჩვენ გამოჩვენებთ, რომ პრეტრანსტრებერის მოდელი შეუძლია გადატანდეს ცოტა მაგალითან მაგალითან ადამიანის ანოტირებულ ბენქმარკებში, რომლებიც ნულ სტანქტირებულია და რამდენიმე სტანქტირებულია, და რომ მაქსირე', 'el': 'Ένα βασικό εμπόδιο στην κατάρτιση μοντέλων εξαγωγής χρονικών σχέσεων σε νέους τομείς είναι η έλλειψη ποικίλων, υψηλής ποιότητας παραδειγμάτων και η πρόκληση συλλογής περισσότερων. Παρουσιάζουμε μια μέθοδο αυτόματης συλλογής εξ αποστάσεως εποπτευόμενων παραδειγμάτων χρονικών σχέσεων. Καθαρίζουμε και αυτόματα επισημαίνουμε ζεύγη συμβάντων όπου οι χρονικές σχέσεις γίνονται σαφείς στο κείμενο, στη συνέχεια αποκρύπτουμε αυτές τις ρητές ενδείξεις, αναγκάζοντας ένα μοντέλο εκπαιδευμένο σε αυτά τα δεδομένα να μάθει άλλα σήματα. Αποδεικνύουμε ότι ένα προ-εκπαιδευμένο μοντέλο μετασχηματιστή είναι σε θέση να μεταφέρει από τα αδύναμα επισημασμένα παραδείγματα σε σημεία αναφοράς με σχόλια από ανθρώπους τόσο σε ρυθμίσεις μηδενικής βολής όσο και σε ρυθμίσεις λίγων βολών, και ότι το σχέδιο κάλυψης είναι σημαντικό για τη βελτίωση της γενικοποίησης.', 'hu': 'Az új területeken történő időkapcsolat-kitermelési modellek képzésének fő akadálya a változatos, magas színvonalú példák hiánya és a több gyűjtés kihívása. Bemutatunk egy olyan módszert, amely automatikusan gyűjti a távolról felügyelt időbeli kapcsolatok példáit. Azokat az eseménypárokat kaparjuk és automatikusan címkézzük, ahol az időbeli kapcsolatok kifejezetten szövegben vannak kifejezve, majd eltüntetjük ezeket a kifejezett utakat, kényszerítve egy ilyen adatokra képzett modellt, hogy más jeleket tanuljon. Bemutatjuk, hogy egy előre képzett Transformer modell képes átmenni a gyengén feliratozott példákról az emberi megjegyzésekkel ellátott referenciaértékekre mind a nulla lövéses, mind a néhány lövéses beállításokban, és hogy a maszkolási séma fontos az általánosítás javításában.', 'it': 'Un ostacolo principale alla formazione dei modelli di estrazione delle relazioni temporali in nuovi settori è la mancanza di esempi diversificati e di alta qualità e la sfida di raccogliere di più. Presentiamo un metodo per raccogliere automaticamente esempi di relazioni temporali sorvegliati a distanza. Raschiamo ed etichetiamo automaticamente coppie di eventi in cui le relazioni temporali sono rese esplicite nel testo, quindi mascheramo quei segnali espliciti, costringendo un modello addestrato su questi dati a imparare altri segnali. Dimostriamo che un modello di Transformer pre-addestrato è in grado di trasferire dagli esempi scarsamente etichettati a parametri di riferimento con annotazione umana sia in impostazioni zero-shot che pochi-shot, e che lo schema di mascheramento è importante per migliorare la generalizzazione.', 'kk': 'Жаңа домендерде уақытша қатынас үлгілеріне қатынастыру үлгілерінің негізгі барьері - өзгертілген, жоғары сапатты мысалдар және көбірек жинақтау үлгілерін жоқ. Біз уақытша қатынастың мысалдарын автоматты түрде қашықтық қатынасын біріктіру әдісін көрсетедік. Біз оқиғалардың екеуін автоматты түрде жазып жазып, мәтінде уақытты қатынастар көрсетілген, содан кейін бұл белгілерді жасырып, басқа сигналдарды оқыту үшін оқиғалардың үлгісін жасырып, жа Біз алдын- ала оқылған Трансформация үлгісін бақылау үлгісінен адамдардың белгіленген мәселелеріне нөл сүру мен бірнеше сүру параметрлерінде, қалқалау сұлбасы жалпы түрлендіру үшін маңызды деп көрсетедік.', 'lt': 'Pagrindinė kliūtis mokymui naujų sričių laikinųjų santykių gavybos modeliuose yra įvairių, aukštos kokybės pavyzdžių trūkumas ir sunkumas rinkti daugiau. Pateikiame metodą automatiškai rinkti toli prižiūrimus laikinųjų santykių pavyzdžius. Mes skruostame ir automatiškai ženkliname įvykių poras, kuriose laikiniai santykiai yra aiškiai išreikšti tekste, tada paslėpiame tuos aiškius ženklus, priversti model į, apmokytą šiais duomenimis, mokytis kitų signalų. Mes įrodome, kad iš anksto parengtas Transformer model is gali pereiti nuo silpnai pažymėtų pavyzdžių prie žmogaus pažymėtų lyginamųjų rodiklių tiek nuliniu, tiek nedideliu mastu, ir kad maskavimo sistema yra svarbi siekiant pagerinti generalizaciją.', 'mk': 'Главната бариера за обуката на моделите за екстракција на временски односи во новите домени е недостатокот на различни, висококвалитетни примери и предизвикот за собирање повеќе. Презентираме метод за автоматско собирање на примери на температурни односи. Ние скршуваме и автоматски ги означуваме паровите на настани каде временските односи се направени експлицитно во текст, а потоа ги маскираме тие експлицитни знаци, принудувајќи модел трениран на овие податоци да научи други сигнали. Демонстрираме дека предобучениот трансформен модел е во можност да се префрли од слабите примери на човековите анатирани споредби во поставувањата со нула и неколку снимки, и дека шемата за маскирање е важна за подобрување на генерализацијата.', 'ms': 'Sebuah halangan utama untuk melatih model ekstraksi hubungan sementara dalam domain baru adalah kekurangan contoh berbeza, kualiti tinggi dan cabaran untuk mengumpulkan lebih. Kami memperkenalkan kaedah untuk mengumpulkan secara automatik contoh hubungan sementara yang diawasi jauh. Kami menggaruk dan automatik label pasangan peristiwa di mana hubungan sementara dibuat eksplicit dalam teks, kemudian menutup keluar isyarat eksplicit itu, memaksa model dilatih pada data ini untuk belajar isyarat lain. Kami menunjukkan bahawa model Transformer yang dilatih-dilatih boleh dipindahkan dari contoh yang lemah yang ditandai kepada tanda referensi yang ditandai manusia dalam kedua-dua tetapan 0-shot dan beberapa-shot, dan bahawa skema topeng adalah penting untuk meningkatkan keseluruhan.', 'ml': 'പുതിയ ഡൊമെയിനിലെ നേരിട്ടുള്ള ബന്ധമെടുക്കുന്ന മോഡലുകളെ പരിശീലിപ്പിക്കാന്\u200d പ്രധാനപ്പെട്ട തടസ്സരമാണ് വ്യത്യസ് നേരിട്ടുള്ള ബന്ധങ്ങളുടെ ഉദാഹരണങ്ങള്\u200d സൂക്ഷിക്കുന്നതിനായി നമ്മള്\u200d ഒരു രീതിയില്\u200d കൊണ്ടുവരുന്നു. ഞങ്ങള്\u200d വായിലുകളില്\u200d വ്യക്തമായ ബന്ധങ്ങള്\u200d വ്യക്തമാക്കിയിരിക്കുന്നു, പിന്നീട് ആ വ്യക്തമായ ക്യൂസുകള്\u200d മുഖം തട്ടിയിടുന്നു, മറ്റു സിഗ്നലുകള്\u200d പഠിക മുമ്പ് പരിശീലിക്കപ്പെട്ട ട ട്രാന്\u200dസ്ഫോര്\u200dമാള്\u200d മോഡലില്\u200d നിന്നും ദുര്\u200dബലപ്പെട്ട ഉദാഹരണങ്ങളില്\u200d നിന്നും മാറ്റാന്\u200d സാധിക്കുന്നത് മനുഷ്യരുടെ പ്രശ്നമായ ബെന്\u200dമ', 'mn': 'Шинэ хэсэгт цаг хугацааны дасгал хөгжүүлэх загварын тухай үндсэн бэрхшээл нь өөр өөр, өндөр чанартай жишээ, илүү олон цуглуулах зорилго юм. Бид цаг хугацааны харилцааны жишээ автоматаар холын харилцааныг автоматаар цуглуулах аргыг харуулж байна. Бид хугацааны харилцаа текст дээр тодорхойлж байгаа хоёрыг автоматжуулж, хугацааны харилцаа холбоотой, дараа нь тодорхойлолтой тодорхойлолтой тодорхойлолтой холбоотой, энэ өгөгдлийн дээр сургалтын загварыг бусад си Бид өмнө сургалтын Трансформатор загварын загвар нь хүн төрөлхтний тэмдэглэгдсэн жишээнээс бага зэрэг цөл болон цөөн зураг загварын тулд хүн төрөлхтний тэмдэглэгдсэн бага зэрэг шилжүүлж чадна гэдгийг харуулж байна. Халцуулах загвар', 'mt': 'Ostaklu ewlieni għat-taħriġ ta’ mudelli ta’ estrazzjoni tar-relazzjonijiet temporali f’oqsma ġodda huwa n-nuqqas ta’ eżempji varjati u ta’ kwalità għolja u l-isfida li jinġabru aktar. Aħna nippreżentaw metodu ta’ ġbir awtomatiku ta’ eżempji ta’ relazzjonijiet temporali sorveljati mill-bogħod. We scrape and automatically label event pairs where the temporal relations are made explicit in text, then mask out those explicit cues, forcing a model trained on this data to learn other signals.  We demonstrate that a pre-trained Transformer model is able to transfer from the weakly labeled examples to human-annotated benchmarks in both zero-shot and few-shot settings, and that the masking scheme is important in improving generalization.', 'no': 'Ein hovudbarrier til trening av tidlegare relasjonsmodeller for ekstraksjon i nye domene er manglende eksemplar for varierte, høg kvalitet og utfordringa for å samla meir. Vi presenterer ein metode for å samla automatisk avstand-oversikt eksemplar på temporale relasjonar. Vi skriv inn og automatisk merkelapp for hendingar der tidsstyrkene er gjort eksplisitt i tekst, så maskerer desse eksplisitte merkelappene, og forandrar ein model trent på denne data for å lære andre signaler. Vi demonstrerer at eit føretrained transformeringsmodell kan overføra frå dei viktige eksemplane med merkelappen til menneskelige markerte benchmarker i både innstillingane for null- og få- bilete, og at maskeringsskjema er viktig i forbedring av generellisering.', 'pl': 'Główną barierą w szkoleniu modeli ekstrakcji relacji czasowych w nowych dziedzinach jest brak zróżnicowanych, wysokiej jakości przykładów oraz wyzwanie związane z gromadzeniem większej liczby. Przedstawiamy metodę automatycznego gromadzenia oddalonych przykładów relacji czasowych. Skrapujemy i automatycznie oznaczamy pary zdarzeń, gdzie relacje czasowe są wyraźne w tekście, a następnie maskujemy te wyraźne wskazówki, zmuszając model trenowany na tych danych do nauki innych sygnałów. Pokazujemy, że wstępnie przeszkolony model Transformera jest w stanie przenieść się z słabo oznaczonych przykładów do anotatowanych przez człowieka punktów odniesienia zarówno w ustawieniach zero-shot, jak i kilku-shot, oraz że schemat maskowania jest ważny w poprawie uogólnienia.', 'sr': 'Glavna barijera za trening vremenskih modela izvlačenja odnosa u novim domenama je nedostatak različitih, visokokvalitetnih primjera i izazov skupljanja više. Predstavljamo metodu automatskog sakupljanja daleko nadziranih primjera privremenih odnosa. Skrebamo i automatski označavamo pare događaja gde se privremeni odnosi izražavaju u tekstu, zatim maskiramo te eksplicite znakove, prisiljavajući model obučen na ovim podacima da naučimo druge signale. Pokazujemo da je predobučeni model transformera u mogućnosti da prenese iz slabih primjera označenih na ljudske notirane kriterije u nastavku nula i nekoliko snimaka i da je maskiranje važno u poboljšanju generalizacije.', 'ro': 'O barieră principală în calea formării modelelor de extracție a relațiilor temporale în domenii noi este lipsa unor exemple variate și de înaltă calitate și provocarea colectării mai multor. Vă prezentăm o metodă de colectare automată a exemplelor supravegheate la distanță de relații temporale. Răzuim și etichetăm automat perechile de evenimente în care relațiile temporale sunt explicite în text, apoi mascăm acele indicii explicite, forțând un model instruit pe aceste date să învețe alte semnale. Demonstrăm că un model de Transformer pre-instruit este capabil să se transfere de la exemplele slab etichetate la criterii de referință adnotate de om atât în setările zero-shot, cât și că schema de mascare este importantă în îmbunătățirea generalizării.', 'si': 'අළුත් ඩෝමේන්ස් වල ප්\u200dරධාන අවධානය සම්බන්ධ වෙන්න ප්\u200dරධාන අවධානයක් තමයි වෙනස් විශේෂ, උත්සත් උදාහරණ උදා අපි ස්වයංක්\u200dරීය විදියට ප්\u200dරවේශයක් පෙන්වන්නේ කාලය සම්බන්ධතාවක් සම්බන්ධ වෙනුවෙන් දුරටත්  අපි ස්ක්\u200dරේප් කරන්න සහ ස්වයංක්\u200dරියාවිතයෙන් ලේබල් සම්බන්ධයක් ලේබල් කරන්න, පස්සේ ප්\u200dරකේෂණ සම්බන්ධයක් ලේබල් සම්බන්ධ වෙන්න, ඊ අපි ප්\u200dරකාශ කරනවා මුලින් ප්\u200dරියෝජනය කරපු ප්\u200dරියෝජනයක් ප්\u200dරියෝජනය කරන්න පුළුවන් කියලා, දුර්වලින් ලේබල් කරපු උදාහරණ වලින් මිනිස්සු ප්\u200dරත', 'so': 'Halka ugu muhiimka ah ee ku tababarida tusaalaha lagu soo bixiyo meelaha cusub waa baahida tusaalooyin kala duduwan, tusaalayaal sare iyo dhibaatada soo ururista. Waxaannu keennaa qaab ay si automati ah u soo ururiyaan tusaalayaal meel fog oo la ilaaliyo, tusaale ahaan xiriirka waqtiga ah. Waxaannu qornaa noocyada dhacdooyinka, meesha lagu caddeeyo xiriirka waqtiga ah, kadibna waxaynu ku baahan nahay cudurka cad, waxaana lagu qasbaynaa tusaale lagu baray macluumaadkan si ay u barto sawirada kale. Waxaynu muujinnay in qaab horay loo tababariyey uu sameyn karo tusaalayaasha tabarta yaraa oo uu ka beddeli karo meelaha lagu soo bandhigi karo dadka iyo meelaha lagu qoray zero iyo wax yar, iyo in qorshaha maskaxu ay muhiim u tahay in uu horumariyo dhalashada.', 'sv': 'Ett huvudhinder för att utbilda modeller för extraktion av tidsrelationer inom nya områden är bristen på varierande exempel av hög kvalitet och utmaningen att samla in mer. Vi presenterar en metod för att automatiskt samla in fjärrövervakade exempel på temporala relationer. Vi skrapar och märker automatiskt händelsepar där de temporala relationerna görs uttryckliga i text, och maskerar sedan ut dessa uttryckliga ledtrådar, vilket tvingar en modell som tränas på dessa data att lära sig andra signaler. Vi visar att en färdigutbildad Transformermodell kan överföras från de svagt märkta exemplen till människokommenterade riktmärken i både noll- och få-skottsinställningar, och att maskeringsschemat är viktigt för att förbättra generaliseringen.', 'ta': 'புதிய களங்களில் காலாவதியான தொடர்பு பிரிப்பு மாதிரிகளை பயிற்சி செய்ய ஒரு முக்கியமான தடை என்பது வேறு வித்தியாசமான, உயர் We present a method of automatically collecting distantly-supervised examples of temporal relations.  நாம் தானாகவே விளக்கச் செய்யும் நிகழ்வு ஜோடிகளை உரையில் வெளிப்படுத்தப்பட்டுள்ளது, பின்னர் அந்த வெளிப்படையான கூம்களை மூடி வெளியேற்றுகிறோம முன்பயிற்சி மாற்றும் மாதிரி மாதிரி பலஹீனமாக்கப்பட்ட உதாரணங்களிலிருந்து மனித குறிப்பிடப்பட்ட பென்கார்க்குகளுக்கு மாற்ற முடியும் என்பதை காட்டுகிறோ', 'ur': 'نو ڈومین میں فعالیت کے معاملہ کی تعلیم کے لئے ایک اصلی بازی ہے کہ نو ڈومین میں فعالیت کے معاملہ میں متفاوت اور زیادہ جمع کرنے کی چال ہے. ہم ایک طریقہ پیش کرتے ہیں کہ مدت کے معاملات کی مثالیں دور کی نظارت سے جمع کریں۔ ہم اسکاپ کر رہے ہیں اور اپنے ساتھ لیبل ایڈینٹ جوڑے جہاں موقت رابطہ تفصیل میں واضح کر دی جاتی ہے، پھر ان واضح نشانیوں کو ماسک کر رہے ہیں، اور اس ڈیٹ پر ایک مدل کو دوسرے سیگنالوں کی تعلیم کرنے کے لئے مجبور کر رہے ہیں. ہم دکھاتے ہیں کہ ایک پیش آموزش کی ترنفسر موڈل کمزور لکڑے ہوئے مثالوں سے انسان کے ذریعہ ذریعہ ذریعہ ذریعہ ذریعہ ذریعہ ذریعہ ذریعہ ذریعہ اور تھوڑے شٹ سیٹیوں میں منتقل کر سکتا ہے اور یہ کہ ماسک سیٹ سیٹ کی عمومی تدبی', 'uz': "Yangi dometdagi vaqt aloqalarni o'rganish modellarini o'rganish asosiy barriesi, har xil, yuqori sifatida va ko'proq narsalarni birlashtirish qiymati yo'q. Biz avtomatik avtomatik taʼminlovchi vaqt aloqalarining masallarini olib tashlash usulini hozir qilamiz. Biz vaqt aloqalarini matn bilan aniqlash va avtomatik ravishda qo'llanmiz, keyin bu xususiyatlarni boshqa imkoniyatlarni o'rganish uchun modelni o'rganish mumkin. Biz oldin o'rganilgan Transfer modeli qo'llangan misollardan qo'shimcha misollardan o'zgartira qo'llaniladi, nuqta va kichkina cheksiz moslamalarda qo'shimcha qolip umumiy tuzishda muhim.", 'vi': 'Một rào cản chính trong việc huấn luyện các mô hình về mối quan hệ thời gian trong những lĩnh vực mới là sự thiếu các ví dụ khác nhau, chất lượng cao và thách thức thu thập nhiều hơn. Chúng tôi đưa ra một phương pháp tự động thu thập những ví dụ về thái dương được giám sát. Chúng tôi xáo trộn và tự động dán nhãn các sự kiện khi các mối quan hệ thời gian được ghi rõ trong văn bản, rồi che giấu các dấu hiệu rõ ràng, ép một mẫu được đào tạo trên dữ liệu này để học các tín hiệu khác. Chúng tôi chứng minh rằng một mô hình biến hình đã được huấn luyện trước có thể chuyển từ ví dụ mềm yếu sang những tiêu chuẩn ghi chú con người trong cả trường hợp bắn không và bắn ít, và rằng kế hoạch che đậy là quan trọng để cải tiến quy mô.', 'da': 'En vigtig hindring for uddannelse af modeller for udvinding af tidsmæssige relationer på nye områder er manglen på varierede eksempler af høj kvalitet og udfordringen ved at indsamle flere. Vi præsenterer en metode til automatisk at indsamle fjernovervågede eksempler på tidsmæssige relationer. Vi skraber og mærker automatisk begivenhedspar, hvor de tidsmæssige relationer gøres eksplicitte i tekst, og maskerer derefter disse eksplicitte signaler ud, hvilket tvinger en model trænet på disse data til at lære andre signaler. Vi demonstrerer, at en forududdannet Transformer model er i stand til at overføre fra de svagt mærkede eksempler til menneskelige-annoterede benchmarks i både nul-shot og få-shot indstillinger, og at maskeringsordningen er vigtig for at forbedre generaliseringen.', 'de': 'Ein Haupthindernis für die Ausbildung von Modellen zur Extraktion zeitlicher Beziehungen in neuen Bereichen ist das Fehlen vielfältiger, qualitativ hochwertiger Beispiele und die Herausforderung, mehr zu sammeln. Wir präsentieren eine Methode zur automatischen Sammlung von fernüberwachten Beispielen zeitlicher Beziehungen. Wir kratzen und markieren automatisch Ereignispaare, bei denen die zeitlichen Beziehungen explizit im Text gemacht werden, und blenden diese expliziten Hinweise aus, wodurch ein Modell, das auf diesen Daten trainiert wird, gezwungen wird, andere Signale zu lernen. Wir zeigen, dass ein vortrainiertes Transformer-Modell in der Lage ist, von den schwach markierten Beispielen auf human-annotierte Benchmarks sowohl in Zero-Shot- als auch in Few-Shot-Einstellungen zu übertragen, und dass das Maskierungsschema wichtig ist, um die Generalisierung zu verbessern.', 'id': 'Sebuah penghalang utama untuk melatih model ekstraksi hubungan temporal dalam domain baru adalah kekurangan contoh berbagai, kualitas tinggi dan tantangan untuk mengumpulkan lebih banyak. Kami mempersembahkan metode untuk mengumpulkan secara otomatis contoh dari hubungan sementara yang diawasi jauh. Kami menggaruk dan otomatis label pasangan peristiwa di mana hubungan temporal dibuat eksplicit dalam teks, kemudian menutup keluar isyarat eksplicit itu, memaksa model yang dilatih pada data ini untuk belajar sinyal lain. Kami menunjukkan bahwa model Transformer yang dilatih-dilatih mampu memindahkan dari contoh yang lemah yang ditandai ke tanda referensi yang ditandai oleh manusia dalam seting 0-shot dan beberapa-shot, dan bahwa skema topeng penting untuk meningkatkan generalisasi.', 'ko': '새로운 분야에서 훈련 시간 관계 추출 모델의 주요 장애는 다양하고 질 좋은 예시가 부족하고 더 많은 예시를 수집하는 도전이다.우리는 시간 관계를 자동으로 수집하는 원격 감독의 예시적인 방법을 제시했다.우리는 텍스트에서 시간 관계가 명확한 사건에 대해 스크래치와 자동 표시를 한 다음에 이러한 명확한 단서를 덮어 이러한 데이터 트레이닝 모델을 바탕으로 다른 신호를 학습하도록 한다.우리는 예비 훈련된 변압기 모형이 영포와 소포 설정에서 약한 표기의 예시에서 인류 주석의 기준 테스트로 전환될 수 있음을 증명했고 엄폐 방안은 범화 능력을 향상시키는 데 매우 중요하다.', 'fa': 'یک بازداشت اصلی برای تمرین رابطه موقتی در مدل استخراج در دامنهای جدید، کمبود نمونه های مختلف، کیفیت بالا و چالش جمع بیشتری است. ما روش خودکار جمع کردن مثالهایی از رابطه\u200cهای موقتی را به دور و دور نشان می\u200cدهیم. ما جفت\u200cهای رویداد\u200cهای موقتی را در متن مشخص می\u200cکنیم و به طور خودکار برچسب می\u200cکنیم، سپس آن نشانه\u200cهای مشخص را ماسک می\u200cکنیم، مجبور می\u200cکنیم یک مدل روی این داده آموزش داده شود تا علامت\u200cهای دیگر را یاد بگیرد. ما نشان می دهیم که یک مدل تغییر\u200cپذیر پیش آموزش داده شده قادر است از مثالهای ضعیف که علامت زده شده\u200cاند به نشان\u200cپذیر\u200cپذیر بشر در حالی که تصاویر\u200cپذیر صفر و چند تصاویر\u200cپذیر است، و این برنامه\u200cپذیر\u200cپذیر در بهترین تغی', 'sw': 'Kizuizi kikuu cha kufundisha mfumo wa kujitenga kwa muda katika maeneo mapya ni ukosefu wa mifano tofauti, ubora wa juu na changamoto ya kukusanya zaidi. Tunaweza kutengeneza njia ya kukusanya mifano yenye kudhibitiwa mbali ya mahusiano ya muda. Tunaweza kuchapisha na kutengeneza viwili vya tukio hilo kwa faragha ambapo mahusiano ya muda yanawekwa wazi kwa ujumbe wa maandishi, kisha kufungua vifaa vya wazi, na kuilazimisha mtindo wa mafunzo katika taarifa hii ili kujifunza alama nyingine. Tunaonyesha kuwa modeli ya Transfer iliyoendelea kabla inaweza kuhamisha kutoka mifano dhaifu iliyotambuliwa na tabia hizo za kibinadamu katika mazingira yasiyo ya risasi na michache yaliyopigwa risasi, na kwamba mpango wa kuvaa nguo ni muhimu katika kuboresha uzalishaji.', 'tr': 'T채ze sahalarda wagtlary 첵uwla힊ma nusgalary 체챌in esasy barrier t채ze sahalarda 체첵tge힊ik, 첵okary kalitede mysallary we k철p체r채k toplamagy흫 kyn챌ylygy d채ldir. Biz 철z-철z체ne wagt baglany힊lary흫 철rneklerini 철z체nden g철zle첵채n bir y철ntemi 챌ykar첵arys. Sa첵lanan hatlary metin i챌inde a 챌캇klandyryl첵an we otomatik etiket 챌iplerini 챌ap첵arys, so흫ra bu m철h체mlerden ba힊ga sygnal 철wrenmesine tertible첵채ris. Biz 철흫체nden 철흫체nden bilinmi힊 bir Transformer nusgasynda i흫 za첵 etilen mysallardan hem n채hili surat 챌ekil첵채n 챌yky힊lardan hem n채hili surat 챌ekil첵채n 챌yky힊lara ta첵첵arlap biljekdigini g철rkezip bil첵채ris we maskerin tasla첵yny umumy geli힊tirmek 체챌in has m철h체m.', 'af': "'n Hoofbarrier na onderwerp tydelike verwanting van uittrekkingsmodele in nuwe domeine is die ontbreek van verskeie, hoë kwaliteit voorbeelde en die uitdrukking van meer te samel. Ons stel 'n metode van outomaties versamel van afstand-ondersoekte voorbeelde van tydelike verhouding. Ons skrap en automaties etiket gebeurtenis paar waar die tydelike verwantings in teks uitgespreek word, dan masker daardie uitgespreidige teks, verdruk 'n model opgelei op hierdie data om ander signale te leer. Ons wys dat 'n vooraf-onderwerpende Transformer model kan oordra van die swakkelik gemerkte voorbeelde na mens-annotateerde benchmarke in beide zero-skoot en paar-skoot instellings, en dat die maskering skema belangrik is in die verbetering van generalisering.", 'sq': 'Një pengesë kryesore për trajnimin e modeleve të nxjerrjes së marrëdhënieve të përkohshme në fusha të reja është mungesa e shembujve të ndryshëm, të cilësisë së lartë dhe sfida e mbledhjes së më shumë. Ne paraqesim një metodë të mbledhjes automatike të shembujve të mbikqyrur nga larg të marrëdhënieve temporale. Ne shkrapsim dhe automatikisht etiketojmë çiftet e ngjarjeve ku marrëdhëniet temporale janë bërë të qarta në tekst, pastaj maskojmë a to shenja të qarta, duke detyruar një model të trajnuar në këto të dhëna për të mësuar sinjale të tjera. Ne demonstrojmë se një model i trajnuar para Transformer është në gjendje të transferohet nga shembujt e etiketuar dobësisht në shembujt e shënuar nga njerëzit në rregullime si zero-shot ashtu edhe pak-shot dhe se skema e maskimit është e rëndësishme në përmirësimin e gjeneralizimit.', 'am': 'በአዲስ አካባቢዎች ውስጥ የጥያቄ ግንኙነት አካባቢ መግለጫ፣ የጥያቄ ምሳሌ እና የበዛ ማሰናከል ውጤት ነው፡፡ የጊዜው ግንኙነት ምሳሌዎችን ከራቁ የተጠበቀውን የራሳቸውን ማሰብሰብ የሥርዓት ሥርዓት እናደርጋለን፡፡ የአሁኑን ግንኙነት በጽሑፍ ግንኙነት የሚገልጽበት እና የራሳዊ ግንኙነቶችን እናሳርፋለን፡፡ ከዚህ በፊት ተማርነው የነበረው ተርጓሚ ሞዴል ከደካማ ምሳሌዎች ወደ ሰው ተቃውሞ ወደሚገልጽ በzero-shot እና ጥቂት-shoot ማህበረሰብ ማቀናቀል ያስችላል፡፡', 'hy': 'Նոր ոլորտներում ժամանակական հարաբերությունների վերացման մոդելների կրթության հիմնական խոչընդոտը տարբեր, բարձր որակի օրինակների բացակայությունն է և ավելին հավաքելու մարտահրավերը: Մենք ներկայացնում ենք ժամանակական հարաբերությունների հեռավորության վերահսկվող օրինակների ավտոմատ հավաքածու մեթոդ: Մենք ինքնաբերաբար կտրում ենք իրադարձությունների զույգեր, որտեղ ժամանակական հարաբերությունները բացատրված են տեքստում, ապա ծածկում ենք այդ բացատրական ազդանշանները, ստիպում ենք այս տվյալների վրա վարժեցված մոդելը այլ ազդանշաններ սովորելու: Մենք ցույց ենք տալիս, որ նախապատրաստված տրանսֆերմերի մոդելը կարողանում է փոխանցել թույլ նշաններով նշված օրինակներից մարդկային նշված համեմատային նշանների՝ զրո-նկարների և քիչ նկարների միջոցով, և որ դիմակային ծրագիրը կարևոր է ընդհանուր զարգացման համար:', 'az': 'Yeni domenilərdə vaxtlı müxtəlif məsəllər və daha çox toplamaq üçün müəyyən edilmiş, yüksək kaliteli məsəllərin yoxdur. Biz müxtəlif ilişkilərin məsəllərini avtomatik olaraq gözləyirik. Biz müxtəlif əlaqələrin mətndə a çıq-aydın edildiyi yerdə təkrarlayır və avtomatik etiket çiftəri çəkirik, sonra bu açıq işaretləri çəkirik, bu məlumatlarda başqa sinyalləri öyrənmək üçün təkrarlanmış modeli çəkirik. Biz göstəririk ki, əvvəlcə təhsil edilmiş Transformer modeli zəif etiketli nümunələrdən insan-etiketli etiketli etiketlərə, sıfır-vuruş və az-vuruş ayarlarında hərəkət edə bilər və masking taslağı generalizasyonu yaxşılaşdırmaq üçün vacibdir.', 'bn': 'নতুন ডোমেইনে সাময়িক সম্পর্ক বের করার মডেল প্রশিক্ষণ প্রধান বাধা প্রদান করা হচ্ছে বিভিন্ন, উচ্চমানের উদাহরণ এবং আরও সংগ্রহের চ্য আমরা স্বয়ংক্রিয়ভাবে সাময়িক সম্পর্কের উদাহরণ সংগ্রহ করার একটি পদ্ধতি উপস্থাপন করি। আমরা স্বয়ংক্রিয়ভাবে প্রতিষ্ঠানের স্বয়ংক্রিয়ভাবে প্রতিষ্ঠানের জোড়া জোড়ায় যেখানে সাময়িক সম্পর্ক লেখায় ব্যাখ্যা করা হয়, তারপর সেই প্রকাশ্য আমরা দেখাচ্ছি যে পূর্ব প্রশিক্ষিত ট্রান্সফ্রান্সফ্রান্স মডেল দুর্বল চিত্রের উদাহরণ থেকে মানুষের বিরক্তিকর বেনমার্কের কাছ থেকে স্থানান্তর করতে পারে, যেখা', 'bs': 'Glavna barijera za trening vremenskih modela izvlačenja u novim domenama je nedostatak različitih, visokokvalitetnih primjera i izazov skupljanja više. Predstavljamo metodu automatskog sakupljanja daleko nadziranih primjera privremenih odnosa. Skrebamo i automatski označavamo pare događaja gdje se privremeni odnosi pojavljuju u tekstu, zatim maskiramo te pojasne znakove, prisiljavajući model obučen na ovim podacima da naučimo druge signale. Pokazujemo da je predobučeni model transformera u stanju prebaciti iz slabe označene primjere na ljudske notirane kriterije u nastavku nule snimke i nekoliko snimka, i da je shma maskiranja važna u poboljšanju generalizacije.', 'ca': "Una barrera principal a l'entrenament de models d'extracció temporal de relacions en nous dominis és la falta d'exemples variats i d'alta qualitat i el repte de recollir més. Presentam un mètode de recollir automàticament exemples de relacions temporals supervisades a distància. Destrobem i etiquetem automàticament parells d'eventos on les relacions temporals es fan explícites en text, després mascaram aquestes indicacions explícites, forçant un model entrenat en aquestes dades a aprendre altres senyals. Demostram que un model de Transformer pré-entrenat pot transferir des dels exemples fracament etiquetats a punts de referència anotats per humans tant en ambients de dispars zero com pocs, i que l'esquema de mascara és important per millorar la generalització.", 'cs': 'Hlavní překážkou pro trénink modelů extrakce časových vztahů v nových oblastech je nedostatek různorodých a kvalitních příkladů a výzva shromažďovat více. Představujeme metodu automatického sběru vzdáleně dohlížených příkladů časových vztahů. Seškrábáme a automaticky označujeme páry událostí, kde jsou časové vztahy explicitně vyjadřovány v textu, poté maskujeme tyto explicitní návody a nutíme model trénovaný na těchto datech učit se další signály. Ukázali jsme, že předškolený model Transformeru je schopen přenést ze slabě označených příkladů do lidských anotovaných referenčních měřítek v nastavení nulového i několika záběrů a že maskovací schéma je důležité pro zlepšení zobecnění.', 'et': 'Peamine takistus ajaliste suhete ekstraheerimise mudelite koolitamisel uutes valdkondades on mitmekesiste ja kvaliteetsete näidete puudumine ja väljakutse koguda rohkem. Esitleme meetodit, mille abil automaatselt koguda kaugjärelevalve all olevaid ajaliste suhete näiteid. Me kraapime ja märgistame automaatselt sündmustepaarid, kus ajalised suhted on tekstis selgesõnalised, seejärel varjame need selgesõnalised vihjed, sundides mudeli, kes on koolitatud nende andmetega, õppima muid signaale. Näitame, et eelkoolitud transformaatori mudel suudab üle kanda nõrgalt märgistatud näidetest inimese märgistatud võrdlusnäitajatele nii null- kui ka vähese pildi seadetes ning et maskeerimisskeem on oluline üldistamise parandamisel.', 'fi': 'Tärkein este ajallisten suhteiden uuttamismallien kouluttamiselle uusilla aloilla on monipuolisten ja laadukkaiden esimerkkien puute ja haaste kerätä lisää. Esittelemme menetelmän, jolla kerätään automaattisesti etävalvottuja esimerkkejä ajallisista suhteista. Kaappaamme ja merkitsemme automaattisesti tapahtumapareja, joissa aikasuhteet on selkeytetty tekstissä, sitten piilotamme nämä nimenomaiset vihjeet, pakottaen mallin oppimaan muita signaaleja. Osoitamme, että esikoulutettu Transformer-malli kykenee siirtymään heikosti merkityistä esimerkeistä ihmisen merkitsemiin vertailuarvoihin sekä nolla- että harva-shot-asetuksissa, ja että maskeerausjärjestelmä on tärkeä yleistymisen parantamisessa.', 'nl': 'Een belangrijke belemmering voor het trainen van modellen voor de extractie van temporele relaties in nieuwe domeinen is het ontbreken van gevarieerde voorbeelden van hoge kwaliteit en de uitdaging om meer te verzamelen. We presenteren een methode om automatisch voorbeelden van tijdelijke relaties van afstand te verzamelen. We schrapen en labelen automatisch gebeurtenisparen waar de temporele relaties expliciet worden gemaakt in tekst, dan maskeren we die expliciete aanwijzingen, waardoor een model getraind op deze gegevens andere signalen leert. We tonen aan dat een vooraf getraind Transformer model in staat is om over te zetten van de zwak gelabelde voorbeelden naar door mensen geannoteerde benchmarks in zowel zero-shot als few-shot instellingen, en dat het maskeringsschema belangrijk is om generalisatie te verbeteren.', 'bg': 'Основна бариера пред обучението на модели за извличане на времеви взаимоотношения в нови области е липсата на разнообразни, висококачествени примери и предизвикателството за събиране на повече. Представяме метод за автоматично събиране на дистанционно контролирани примери за времеви взаимоотношения. Ние изстъргваме и автоматично етикетираме двойки събития, където времевите отношения са изрично изразени в текста, след което маскираме тези изрични знаци, принуждавайки модел, обучен върху тези данни, да научи други сигнали. Ние демонстрираме, че предварително обучен модел трансформатор е способен да се прехвърли от слабо обозначените примери към човешки анотирани бенчмаркове както в настройки с нулев, така и с няколко изстрела и че маскиращата схема е важна за подобряване на обобщението.', 'hr': 'Glavna barijera za trening vremenskih modela izvlačenja u novim domenima je nedostatak različitih primjera kvalitete i izazov skupljanja više. Predstavljamo metodu automatskog sakupljanja daleko nadziranih primjera privremenih odnosa. Skrabimo i automatski označavamo pare događaja gdje se privremeni odnosi pojavljuju u tekstu, zatim maskiramo te pojasne znakove, prisiljavajući model obučen na ovim podacima da naučimo druge signale. Pokazujemo da je predobučeni model transformera u mogućnosti prenijeti iz slabih primjera označenih na ljudske notirane kriterije u postavkama nula i nekoliko snimaka i da je plan maskiranja važan u poboljšanju generalizacije.', 'jv': 'Barêr sing dipunangé kanggo nglanggar tarjamahan kanggo model urip nggawe winih dhéwé Awak dhéwé éntukno sistem kanggo nggawe barang akeh tarjamah luwih-luwih dumateng ning resmi dadi. Awak dhéwé éntuk kesalakno lan nambah perbudhakan langgar sampeyan anyar tentang kanggo kelas nèng text, podho mask diwurung barang kelas nang cukup, terus model sing nyimpen kanggo ngerasakno sampeyan liyane Awak dhéwé éntuk sistem transformer sing gawe ngubah perusahaan pri-cara anyar tentang kanggo mulasai iki dadi sing dirampakan sing apik trus tambah bantuan.', 'sk': 'Glavna ovira za usposabljanje modelov ekstrakcije časovnih relacij na novih področjih je pomanjkanje raznolikih, visokokakovostnih primerov in izziv zbiranja več. Predstavljamo metodo avtomatskega zbiranja daljno nadzorovanih primerov časovnih odnosov. Samodejno označimo pare dogodkov, kjer so časovne relacije eksplicitne v besedilu, nato pa zakrijemo te eksplicitne namige in prisilimo model, ki se uči na teh podatkih, da se nauči drugih signalov. Dokazujemo, da je vnaprej usposobljen model transformatorjev sposoben prenesti iz slabo označenih primerov na človeško označene referenčne vrednosti v nastavitvah ničelnega in malo posnetka, in da je shema maskiranja pomembna za izboljšanje generalizacije.', 'he': 'המחסום העיקרי לאימון דוגמני חיפוש מערכת יחסים זמנית בתחומים חדשים הוא חוסר דוגמאות מגוונות, איכות גבוהה ואתגר לאספת יותר. אנחנו מציגים שיטה לאסוף אוטומטית דוגמאות של מערכות יחסים זמניות. אנו מגרדים ואוטומטיים זוגות אירועים שבו מערכות היחסים הזמניות הופכות ברורות בטקסט, ואז מסיכים את הסימנים הראויים האלה, מכריחים מודל מאומן על הנתונים האלה ללמוד אותות אחרות. אנחנו מראים שמודל טרנספורר מאומן מראש מסוגל להעביר מהדוגמאות המתווידות החולשות למקומות רמז המתווידים על ידי האדם במסגרות של אפס-יריות ובמעט-יריות, ושמערכת המסיכה חשובה לשפר את הגנרליזציה.', 'bo': 'ས་ཁོངས་གསར་པའི་ནང་དུ་དམིགས་འཛུགས་ཀྱི་ཐབས་ལམ་ལུགས་ཐལ་མཚམས་འཇུག་ནུས་མཚམས་འཇུག ང་ཚོས་རང་འགུལ་གྱིས་ཐག་རིང་ལ་བསྡད་པའི་དཔེར་བརྗོད་ཀྱི་ཐབས་ལམ་གཅིག་སྟོན་ཡོད། ང་ཚོས་རང་འགུལ་གྱིས་བྱ་འགུལ་གྱི་རྣམ་གྲངས་དང་རང་འགུལ་གྱིས་ཤོག་བྱང་ཆོག We demonstrate that a pre-trained Transformer model is able to transfer from the weakly labeled examples to human-annotated benchmarks in both zero-shot and few-shot settings, and that the masking scheme is important in improving generalization.', 'ha': "A principal barrier to training temporal relation extraction models in new domains is the lack of varied, high quality examples and the challenge of collecting more.  Tuna zuwa wani hanyor da ke samun misãlai da aka tsare masu yin farat ɗaya da ke tsaro masu mutane. Tuna karatun nau'i biyu na shiryoyin ayuka farat ɗaya, inda aka bayyana danganta cikin littãfin, sa'an nan kuma Mu rufe wasu misunnai masu bayyanawa, kuma Mu lazimta wani motel wanda aka yi wa shirin wannan danne dõmin su sanar da wasu ayuka. Tuna nũna cewa wani misalin Transformer ya iya amfani da shi ya iya shige daga misãlai masu rauni da aka rubutu wa mutum zuwa gungo masu basu'a cikin tsarin sifo-shot da few shekara, kuma shi na muhimu a wajen fikar fuskar kwamfyutan ta wajen kyautatawa."}
{'en': 'Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation', 'ar': 'تحليل التبعية عبر اللغات بدون طلقة من خلال تحويل التضمين السياقي', 'pt': 'Análise de dependência entre idiomas de tiro zero por meio da transformação de incorporação contextual', 'es': 'Análisis de dependencias interlingüe de tiro cero a través de la transformación de incrustación contextual', 'fr': "Analyse de dépendance interlinguale Zero-Shot via la transformation d'intégration contextuelle", 'ja': 'コンテキスト埋め込み変換によるゼロショットのクロスリンガル依存関係の解析', 'hi': 'शून्य-शॉट क्रॉस-लिंगुअल निर्भरता प्रासंगिक एम्बेडिंग परिवर्तन के माध्यम से पार्सिंग', 'zh': '因上下文嵌转零次跨语赖解析', 'ru': 'Синтаксический анализ кросс-лингвальной зависимости с нулевым выстрелом через преобразование контекстного встраивания', 'ga': 'Spleáchas Trastheangach Zero-shot Parsáil trí Chlaochlú Leabú Comhthéacsúil', 'ka': 'Name', 'el': 'Ανάλυση διαγώνιας εξάρτησης μηδενικού πυροβολισμού μέσω μετασχηματισμού περιεκτικής ενσωμάτωσης', 'hu': 'Zéró felvételes nyelvközi függőség értelmezése kontextuális beágyazási átalakításon keresztül', 'it': "Analisi della dipendenza linguistica attraverso la trasformazione dell'incorporazione contestuale", 'lt': 'Nutrauktas tarpkalbinis priklausomybės analizavimas atliekant kontekstinę įterpimo transformaciją', 'mk': 'Анализирање на нула- пукање на крослингвалната зависност преку контекстна внатрешна трансформација', 'ms': 'Penghuraian Dependensi Selasa-Bahasa Semula-Tembak Melalui Penjelmaan Konteksual', 'kk': 'Нөл- шектелген тілікті көшектелген тәуелдік талдау контексті ендіру түрлендірімі арқылы', 'ml': 'ക്രോസ്- ലിന്\u200dഗുവല്\u200d ആശ്രയിക്കുന്ന പാര്\u200dസ് ചെയ്യുന്നു', 'mt': 'Analiżi tad-Dipendenza Cross-Lingwali Żero-Shot permezz ta’ Trasformazzjoni Kontekswali ta’ Inkorporazzjoni', 'pl': 'Parsowanie zależności między językami zerowymi poprzez kontekstową transformację osadzania', 'mn': 'Нөлөө-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation', 'no': 'Tolking av null-Shot krysslingsavhengighet gjennom kontekstalt innbyggingstransformasjon', 'ro': 'Analizarea dependenței interlingvistice Zero-Shot prin transformarea încorporării contextuale', 'sr': 'Nula-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation', 'so': 'Baaritaanka iskuulka-jarida-jarida ee ku xiran luqada', 'si': 'Zero- Shot Cross- Lingual Dependency Parsing', 'sv': 'Zero-Shot tvärspråklig beroendetolkning genom kontextuell inbäddande omvandling', 'ta': 'சூழ்நிலை குறுக்கு- குறுக்கு சார்ந்த சார்ந்த தொகுதி உள்ளமைப்பு மாற்றுதல் வழியாக பாசிங்கு', 'ur': 'Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation', 'uz': 'Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation', 'vi': 'Chế độ phân phối chữ số vớ vẩn:', 'bg': 'Анализиране на междулингвистична зависимост чрез трансформация в контекстното вграждане', 'da': 'Zero-Shot tværsproglig afhængighedsanalyse gennem kontekstuel indlejring transformation', 'hr': 'Nulo-pucanje preko-Lingualne zavisnosti analiziranja kroz kontekstualnu transformaciju uključenja', 'nl': 'Zero-Shot cross-linguale afhankelijkheidsparsing door contextuele embedding transformatie', 'de': 'Zero-Shot Cross-Lingual Dependency Parsing durch kontextuelle Einbettungstransformation', 'id': '0-Shot Cross-Lingual Dependency Analysing melalui Transformasi Embedding Konteksual', 'fa': 'تغییر تغییر تغییر تغییر تغییر تغییر تغییر محیط\u200c', 'sw': 'Kuchapisha Msalaba Zero-Shot-Lingua Kutegemea Uchunguzi wa Mitandao', 'tr': 'Zero-Shot Çapraz Hatlaryň Baýlyklylykyny Kontekst Girişi Görkez', 'af': 'Nuwe- Shot Kruis- Linguale Afhanklikheid Ontlegging deur Konteksual Inbêer Transformasie', 'am': 'undo-type', 'sq': 'Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation', 'az': 'Sıfır-Shot Çərz-Lingual Bağlamlığı Tərcümə Ünvan Aracılığı', 'bn': 'ক্রস- লিঙ্গুয়ালের নির্ভরিত পার্সিং বিভিন্ন অভ্যন্তরীণ এমবেডিং পরিবর্তনের মাধ্যমে', 'ko': '상하문 삽입 변환에 기반한 제로 트리거 크로스 언어 의존 분석', 'bs': 'Nulo-pucanje preko-Lingualne zavisnosti za analizu kroz kontekstualnu transformaciju uključenja', 'ca': "Analització de dependencies translingües sense dispars a través de la transformació d'integració contextual", 'et': 'Zero-Shot keeleülese sõltuvuse parsimine kontekstipõimimise kaudu', 'cs': 'Zero-Shot Cross-lingvální analýza závislosti prostřednictvím kontextové transformace vkládání', 'hy': 'Ազրո-կրակի միջլեզվային կախվածություն վերլուծում կոնտեքստալ ներգրավված վերաձևման միջոցով', 'fi': 'Zero-Shot Cross-Lingual Dependency Parsing kontekstuaalisen upotusmuunnoksen avulla', 'jv': 'nulo-shot', 'he': 'אפס-ירי תלויות לשונות', 'ha': 'KCharselect unicode block name', 'sk': 'Razčlenitev medjezikovne odvisnosti z ničelnim posnetkom s preoblikovanjem kontekstualne vdelave', 'bo': 'Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation'}
{'en': 'Linear embedding transformation has been shown to be effective for zero-shot cross-lingual transfer tasks and achieve surprisingly promising results. However, cross-lingual embedding space mapping is usually studied in static word-level embeddings, where a space transformation is derived by aligning representations of translation pairs that are referred from dictionaries. We move further from this line and investigate a contextual embedding alignment approach which is sense-level and dictionary-free. To enhance the quality of the  mapping , we also provide a deep view of properties of contextual embeddings, i.e., the  anisotropy problem  and its solution. Experiments on zero-shot dependency parsing through the concept-shared space built by our embedding transformation substantially outperform state-of-the-art methods using multilingual embeddings.', 'ar': 'لقد ثبت أن تحويل التضمين الخطي فعال لمهام النقل عبر اللغات التي لا تتطلب إطلاقًا كبيرًا ويحقق نتائج واعدة بشكل مدهش. ومع ذلك ، عادةً ما تتم دراسة تعيين مساحة التضمين عبر اللغات في حفلات الزفاف الثابتة على مستوى الكلمات ، حيث يتم اشتقاق تحويل الفضاء عن طريق محاذاة تمثيلات أزواج الترجمة التي تتم إحالتها من القواميس. ننتقل بعيدًا عن هذا الخط ونبحث في نهج محاذاة التضمين السياقي والذي يكون على مستوى المعنى وخالي من القاموس. لتعزيز جودة رسم الخرائط ، نقدم أيضًا نظرة عميقة لخصائص الزخارف السياقية ، أي مشكلة التباين وحلها. تتفوق التجارب التي أجريت على تحليل التبعية بدون طلقة من خلال المساحة المشتركة للمفهوم التي تم بناؤها من خلال تحول التضمين لدينا بشكل كبير على أحدث الأساليب باستخدام حفلات الزفاف متعددة اللغات.', 'es': 'Se ha demostrado que la transformación de incrustación lineal es eficaz para las tareas de transferencia interlingüística de tiro cero y logra resultados sorprendentemente prometedores. Sin embargo, el mapeo de espacios de incrustación multilingüe generalmente se estudia en incrustaciones estáticas a nivel de palabra, donde una transformación de espacio se deriva alineando representaciones de pares de traducción que se refieren desde los diccionarios. Nos alejamos de esta línea e investigamos un enfoque de alineación de incrustación contextual que esté a nivel sensorial y libre de diccionarios. Para mejorar la calidad del mapeo, también ofrecemos una visión profunda de las propiedades de las incrustaciones contextuales, es decir, el problema de la anisotropía y su solución. Los experimentos sobre el análisis cero de dependencias a través del espacio de conceptos compartidos creado por nuestra transformación de incrustación superan sustancialmente a los métodos de última generación que utilizan incrustaciones multilingües.', 'pt': 'A transformação de incorporação linear mostrou-se eficaz para tarefas de transferência linguística cruzada zero-shot e alcança resultados surpreendentemente promissores. No entanto, o mapeamento de espaço de incorporação de vários idiomas geralmente é estudado em incorporações estáticas em nível de palavra, onde uma transformação de espaço é derivada alinhando representações de pares de tradução que são referenciados em dicionários. Nós nos afastamos dessa linha e investigamos uma abordagem de alinhamento de incorporação contextual que é de nível de sentido e livre de dicionário. Para melhorar a qualidade do mapeamento, também fornecemos uma visão profunda das propriedades dos embeddings contextuais, ou seja, o problema da anisotropia e sua solução. Experimentos em análise de dependência zero-shot através do espaço compartilhado de conceito construído por nossa transformação de incorporação superam substancialmente os métodos de última geração usando incorporações multilíngues.', 'fr': "Il a été démontré que la transformation linéaire par intégration est efficace pour les tâches de transfert interlinguale à jet zéro et permet d'obtenir des résultats étonnamment prometteurs. Cependant, la cartographie de l'espace d'intégration multilingue est généralement étudiée dans le cadre d'intégrations statiques au niveau des mots, où une transformation d'espace est dérivée en alignant des représentations de paires de traductions qui sont référencées à partir de dictionnaires. Nous nous éloignons de cette ligne et étudions une approche d'alignement d'intégration contextuelle qui est au niveau des sens et sans dictionnaire. Pour améliorer la qualité de la cartographie, nous fournissons également une vue approfondie des propriétés des intégrations contextuelles, c'est-à-dire le problème d'anisotropie et sa solution. Les expériences sur l'analyse de dépendance à zéro coup dans l'espace partagé de concepts construit par notre transformation d'intégration surpassent considérablement les méthodes de pointe utilisant des intégrations multilingues.", 'ja': '線形埋め込み変換は、ゼロショットのクロスリンガル転送タスクに有効であり、驚くほど有望な結果を達成することが示されている。しかし、クロスリンガル埋め込み空間マッピングは、通常、静的な単語レベルの埋め込みで研究され、空間変換は、辞書から参照される翻訳ペアの表現を整列させることによって導出される。このラインからさらに進み、センスレベルで辞書なしのコンテキスト埋め込みアライメントアプローチを調査します。マッピングの品質を向上させるために、コンテキスト埋め込みのプロパティ、すなわち異方性問題とその解決策の深いビューも提供します。埋め込み変換によって構築されたコンセプト共有空間を通じたゼロショット依存性解析の実験は、多言語埋め込みを使用した最先端の方法を大幅に上回ります。', 'ru': 'Было показано, что линейное преобразование встраивания эффективно для задач кросс-лингвистической передачи с нулевым выстрелом и достижения удивительно многообещающих результатов. Тем не менее, межъязычное отображение пространства вложений обычно изучается в статических вложениях на уровне слов, где преобразование пространства получается путем выравнивания представлений пар переводов, которые ссылаются из словарей. Мы двигаемся дальше от этой линии и исследуем подход к выравниванию контекстного встраивания, который на уровне смысла и без словаря. Для повышения качества картирования мы также обеспечиваем глубокий обзор свойств контекстных вложений, то есть проблемы анизотропии и ее решения. Эксперименты по анализу зависимостей с нулевым выстрелом через совместное пространство концепций, построенное нашим преобразованием вложений, значительно превосходят современные методы с использованием многоязычных вложений.', 'zh': '线性嵌易已验零次跨语转效,而得可讶者。 然跨语嵌空,常于静态词级嵌中研究,其间变易,因齐字典所引译对文推导之。 从这条线更远,并究一上下文嵌齐法,其法感级无字典也。 供上下文嵌视图,各向异性问解决方案。 凡嵌转换之概共享空零镜头依赖性解析者实验大优于多言嵌最先进之法。', 'hi': 'रैखिक एम्बेडिंग परिवर्तन को शून्य-शॉट क्रॉस-लिंगुअल ट्रांसफर कार्यों के लिए प्रभावी दिखाया गया है और आश्चर्यजनक रूप से आशाजनक परिणाम प्राप्त किए गए हैं। हालांकि, क्रॉस-लिंगुअल एम्बेडिंग स्पेस मैपिंग का अध्ययन आमतौर पर स्थैतिक शब्द-स्तर एम्बेडिंग में किया जाता है, जहां एक अंतरिक्ष परिवर्तन अनुवाद जोड़े के प्रतिनिधित्व को संरेखित करके प्राप्त किया जाता है जिन्हें शब्दकोशों से संदर्भित किया जाता है। हम इस लाइन से आगे बढ़ते हैं और एक प्रासंगिक एम्बेडिंग संरेखण दृष्टिकोण की जांच करते हैं जो भावना-स्तर और शब्दकोश-मुक्त है। मानचित्रण की गुणवत्ता को बढ़ाने के लिए, हम प्रासंगिक एम्बेडिंग के गुणों का एक गहरा दृश्य भी प्रदान करते हैं, यानी, अनिसोट्रॉपी समस्या और इसके समाधान। हमारे एम्बेडिंग परिवर्तन द्वारा निर्मित अवधारणा-साझा स्थान के माध्यम से शून्य-शॉट निर्भरता पार्सिंग पर प्रयोगों ने बहुभाषी एम्बेडिंग का उपयोग करके अत्याधुनिक तरीकों को काफी हद तक मात दी।', 'ga': 'Léiríodh go bhfuil claochlú leabaithe líneach éifeachtach do thascanna aistrithe trasteangacha gan aon lámhaigh agus go mbaintear torthaí iontacha a bhfuil gealladh fúthu. Mar sin féin, is iondúil go ndéantar staidéar ar léarscáiliú spáis leabaithe trasteangacha i leabaithe statacha ar leibhéal na bhfocal, áit a ndéantar claochlú spáis trí léiriú ar phéirí aistriúcháin a thagraíonn ó fhoclóirí a ailíniú. Gluaisimid níos faide ón líne seo agus fiosraíonn muid cur chuige um ailíniú leabú comhthéacsúil atá saor ó thaobh ciall agus foclóireachta. Chun cáilíocht na mapála a fheabhsú, cuirimid radharc domhain ar fáil freisin ar airíonna leabaithe comhthéacsúla, i.e., an fhadhb aniseatrópachta agus a réiteach. Is fearr go mór na turgnaimh ar pharsáil spleáchais náid-shot tríd an spás comhroinnte coincheapa a thóg ár gclaochlú leabaithe modhanna úrscothacha ag baint úsáide as leabaithe ilteangach.', 'ka': 'ჩვენ ჩვენებულია, რომ ლინიური შებრუნებული ტრანფორმაცია იქნება ძალიან ეფექტიური კრისტრანგულური ტრანფორმაციის მომხმარებისთვის და გადავადგინოთ გასაკვეთ მაგრამ, მრავალური სიტყვების კონფიგურაცია სიტყვების კონფიგურაციაში სტატიკური სიტყვების კონფიგურაციაში, სადაც სიტყვების კონფიგურაცია შეიძლება სიტყვების განსაზღვრებით ჩვენ ამ ხაზიდან უფრო გადავიწყეთ და შევხედოთ კონტექსტური დაწყვეტილება, რომელიც სიტყვების დონე და სიტყვების გარეშე. ჩვენ კონტექსტური ინტერქსტური ინტერქსტური პრობლემა და მისი პასუხის მნიშვნელობას დავიწყებთ. 0-სურათის დასაწყვებულობის პანსპერიმენტების გამოყენება კონცექტის გაყოფილი სივრცე, რომელიც ჩვენი შემოწყვებული ტრანსპერიფიკაციის გამოყენებული მრავალური შემოწყვების გამო', 'hu': 'A lineáris beágyazási transzformáció hatékonynak bizonyult a nullás nyelvű transzfer feladatokhoz, és meglepően ígéretes eredményeket ért el. A többnyelvű beágyazási térleképezést azonban általában statikus szószintű beágyazásokban tanulmányozzák, ahol a tértranszformáció a szótárakból hivatkozott fordítási párok reprezentációinak igazításával származik. Távolabb lépünk ettől a vonaltól, és egy kontextuális beágyazási igazítási megközelítést vizsgálunk, amely érzékszintű és szótármentes. A térképezés minőségének javítása érdekében mélyreható képet nyújtunk a kontextuális beágyazások tulajdonságairól, azaz az anizotrópia problémáról és annak megoldásáról. A beágyazási átalakításunk által létrehozott koncepció-megosztott területen végzett zero-shot függőség elemzésére irányuló kísérletek lényegesen felülmúlják a legkorszerűbb módszereket a többnyelvű beágyazások alkalmazásával.', 'el': 'Ο γραμμικός μετασχηματισμός ενσωμάτωσης έχει αποδειχθεί αποτελεσματικός για εργασίες διασυνοριακής μεταφοράς μηδενικών πυροβολισμών και επιτυγχάνει εκπληκτικά υποσχόμενα αποτελέσματα. Ωστόσο, η διασταυρούμενη χαρτογράφηση χώρου ενσωμάτωσης μελετάται συνήθως σε στατικές ενσωμάτωση σε επίπεδο λέξεων, όπου ένας μετασχηματισμός χώρου προέρχεται από την ευθυγράμμιση αναπαραστάσεων των μεταφραστικών ζευγαριών που αναφέρονται από λεξικά. Προχωρούμε περαιτέρω από αυτή τη γραμμή και ερευνούμε μια προσέγγιση ευθυγράμμισης που είναι σε επίπεδο αισθήσεων και χωρίς λεξικό. Για να βελτιωθεί η ποιότητα της χαρτογράφησης, παρέχουμε επίσης μια βαθιά εικόνα των ιδιοτήτων των περιεχομένων ενσωμάτωσης, δηλαδή του προβλήματος ανισοτροπίας και της επίλυσής του. Τα πειράματα για την ανάλυση εξάρτησης μηδενικού πυροβολισμού μέσα από τον κοινόχρηστο χώρο που χτίστηκε από τον μετασχηματισμό ενσωμάτωσης μας ξεπερνούν σημαντικά τις σύγχρονες μεθόδους που χρησιμοποιούν πολυγλωσσικές ενσωματώσεις.', 'it': "La trasformazione lineare di incorporazione ha dimostrato di essere efficace per le attività di trasferimento cross-lingual zero-shot e di ottenere risultati sorprendentemente promettenti. Tuttavia, la mappatura dello spazio di incorporamento cross-lingual è solitamente studiata in incorporazioni statiche a livello di parola, dove una trasformazione dello spazio è derivata allineando rappresentazioni di coppie di traduzioni a cui si fa riferimento dai dizionari. Ci spostiamo più lontano da questa linea e indaghiamo un approccio contestuale di allineamento che è a livello di senso e senza dizionario. Per migliorare la qualità della mappatura, forniamo anche una visione approfondita delle proprietà delle incorporazioni contestuali, cioè il problema dell'anisotropia e la sua soluzione. Gli esperimenti sull'analisi delle dipendenze zero-shot attraverso lo spazio condiviso basato sul concetto costruito dalla nostra trasformazione di embedding superano sostanzialmente i metodi all'avanguardia utilizzando embedding multilingue.", 'lt': 'Buvo įrodyta, kad linijinis įterpimo transformavimas yra veiksmingas atliekant nulinius tarpkalbinius perdavimo uždavinius ir pasiekti stebuklingai žadančius rezultatus. Tačiau tarpkalbis įdėjimo erdvės žemėlapis paprastai tiriamas statiniais žodžių lygio įdėjimais, kai erdvės transformacija atliekama derinant vertimo poros, nurodytų žodynuose, reprezentacijas. Mes einame toliau nuo šios linijos ir tiriame kontekstinį integravimo suderinimo metodą, kuris yra prasmingas ir be žodynų. Siekiant pagerinti žemėlapių kokybę, mes taip pat i šsamiai apžvelgiame kontekstinių sudedamųjų dalių savybes, t. y. anisotropijos problem ą ir jos sprendimą. Eksperimentai, susiję su nulinės priklausomybės analize per koncepciją pasidalijusią erdvę, sukurtą mūsų įterpiant transformaciją, gerokai viršija pažangiausius metodus naudojant daugiakalbius įterpimus.', 'kk': 'Сызықты ендіру түрлендірімі нөл тілді жіберу тапсырмалары үшін эффективні болып, әлі әлі үлкен нәтижелерді жеткізу үшін көрсетілді. Бірақ тілдерді бірнеше ендіру кеңістік картасының кәдімгі сөздер деңгейіндегі ендіруде зерттейді. Ол жерде кеңістік түрлендірімі сөздіктерден жазылған аудармалардың кеңістіктерін түрлен Біз осы жолдан қашыққа жылжып, мәзірдегі ендіру тәртібін зерттеп, сезім деңгейі мен сөздік бос. Карталау сапатын көтеру үшін, сондай-ақ контекстік ендіру қасиеттерінің қасиеттерін, яғни анисотропия мәселесін және оның шешімін қараймыз. Біздің ендіру түрлендіріміздің көптілік ендіру арқылы жасалған концепциялық орынды талдау тәуелдігінің зерттеулері көп тілдік ендіру арқылы жасалған.', 'mk': 'Линијалната трансформација на вградување се покажа дека е ефикасна за нула-снимки преку јазички трансфер задачи и постигнува изненадувачки ветувачки резултати. Сепак, трансформацијата на просторот за вклучување на меѓујазиците обично се проучува во статички вклучувања на ниво на зборови, каде што вселенската трансформација се изведува со израмнување на претставувањата на парови на превод кои се реферираат од речниците. Одиме понатаму од оваа линија и истражуваме контекстен пристап на вградување на пристапот кој е безчувствителен и без речник. За да го подобриме квалитетот на мапирањето, ние исто така обезбедуваме длабок поглед на сопственостите на контекстните вградувања, т.е. проблемот со анитропијата и нејзиното решение. Експериментите на нула-стрелка зависност анализирање преку концепт-споделениот простор изграден од нашата вградена трансформација значително ги надминуваат најсовремените методи користејќи мултијазични вградувања.', 'ms': 'Penukaran penyembedding linear telah dipaparkan sebagai berkesan untuk tugas pemindahan saling bahasa 0-shot dan mencapai keputusan yang mengejutkan. Namun, pemetaan ruang penyembedding saling bahasa biasanya dipelajari dalam penyembedding tahap perkataan statik, di mana pengubahan ruang dibina dengan mengajarkan perwakilan pasangan terjemahan yang direferensikan dari kamus. Kita bergerak jauh dari garis ini dan menyelidiki pendekatan penyesuaian penyelesaian kontekstual yang adalah aras-akal dan bebas kamus. Untuk meningkatkan kualiti pemetaan, kita juga menyediakan pandangan dalam ciri-ciri penyambungan kontekstual, iaitu masalah anisotropi dan penyelesaiannya. Eksperimen mengenai penghuraian dependensi 0-shot melalui ruang konsep-berkongsi yang dibina oleh pengubahan penyembedding kami secara konsisten melebihi kaedah state-of-the-art menggunakan penyembedding berbilang bahasa.', 'mt': 'It-trasformazzjoni lineari ta’ inkorporazzjoni ntweriet li hija effettiva għal kompiti ta’ trasferiment translingwi b’sensiela żero u tikseb riżultati promettenti b’mod sorprendenti. Madankollu, l-immappjar tal-ispazju ta’ inkorporazzjoni translingwi ġeneralment jiġi studjat f’inkorporazzjonijiet statiċi fil-livell tal-kliem, fejn trasformazzjoni tal-ispazju tiġi dderivata billi jiġu allinjati r-rappreżentazzjonijiet ta’ pari ta’ traduzzjoni li huma riferiti mid-dikjararji. Aħna nimxu lil hinn minn din il-linja u ninvestigaw approċċ ta’ allinjament ta’ inkorporazzjoni kuntestwali li huwa sens-level u ħieles mid-dikjaratur. Biex titjieb il-kwalità tal-immappjar, a ħna nipprovdu wkoll stampa profonda tal-karatteristiċi tal-inkorporazzjonijiet kuntestwali, jiġifieri l-problem a tal-anisotropija u s-soluzzjoni tagħha. L-esperimenti dwar l-analizzazzjoni tad-dipendenza żero-shot permezz tal-ispazju kondiviż tal-kunċett mibni mit-trasformazzjoni inkorporattiva tagħna sostanzjalment jeċċedu l-metodi l-aktar avvanzati bl-użu ta’ inkorporazzjonijiet multilingwi.', 'ml': 'പൂജ്യത്തിലേക്ക് വെടിവെക്കുന്ന ക്രില്\u200dഭാഷകങ്ങളുടെ ജോലികള്\u200dക്ക് പ്രാവര്\u200dത്തികമായി ലൈനൈറ്റ് മാറ്റങ്ങള്\u200d ചെയ്യുന് എങ്കിലും ക്രിസ്ലിങ്ങ് ലൈന്\u200dഗ്വില്\u200d ഉള്\u200dപ്പെടുത്തുന്ന സ്പെയിന്\u200dറ് മാപ്പിങ് സാധാരണയായി സ്റ്റാറ്റിക് വാക്ക്- നില മാറ്റങ്ങളില്\u200d പഠിക ഈ വരിയില്\u200d നിന്നും നമ്മള്\u200d മുന്നോട്ട് നീങ്ങുന്നു എന്നിട്ട് നിര്\u200dണ്ണയിക്കുന്ന ഒരു നിര്\u200dണ്ണയത്തിന്റെ പ്രോഗത് മാപ്പിങ്ങിന്റെ ഗുണവും കൂടുതല്\u200d വര്\u200dദ്ധിപ്പിക്കാന്\u200d, നിലവിലുള്ള പ്രശ്നത്തിന്റെ പ്രശ്നത്തിന്റെ ഗുണഗണങ്ങളുടെ ആഴത്തില പൂര്\u200dണ്ണമായ വെടിവെക്കുന്നതിന്\u200dറെ ആശ്രയിതമായ പരീക്ഷണങ്ങള്\u200d പല ഭാഷയിലുള്ള മാറ്റങ്ങള്\u200d ഉപയോഗിച്ച് നിര്\u200dമ്മിക്കുന്ന സ്ഥാനത്തില്\u200d പങ്കെടുത്', 'mn': 'Шинэ хэлний шилжүүлэлтийн өөрчлөлт нь тэгш хэлний шилжүүлэлтийн ажил дээр үр дүнтэй болж, гайхалтай амлалтай үр дүнг гаргаж байна. Гэхдээ хэлний олон орон зайн газрын зураг ихэвчлэн хэлний түвшинд судалдаг. Тэнд орон зайн өөрчлөлт нь сөрөгчийн хэлбэрээс илэрхийлэгддэг орчуулагчийн хоёрын илэрхийллийг тодорхойлж байдаг. Бид энэ шулуунаас илүү хөдөлж, мэдрэмжгүй түвшин болон үгүйсгэл үнэгүй орчин зүйлсийг судалж байна. Картингийн сайн чанарыг нэмэгдүүлэхийн тулд бид мөн нөхцөл байдлын шинж чанарын гүн гүнзгий харагддаг, яг энизотропийн асуудал болон шийдэл юм. Зуун зураг хамааралтай байдлын туршилтын туршилт нь олон хэлний нэвтрүүлэлтийг ашиглан бүтээсэн ойлголтын хуваалцан орон зайд гаргасан.', 'no': 'Lineær innbyggingstransformasjon er vist som effektiv for krysspråksoppgåver med null-shot og oppnår overføring av lysstyrke resultat. Dette er imidlertid gjennomsiktig innbygging av mellomrommappa i stasjonske innbygging av ordnivå, der eit mellomromtransformasjon er avhengig av å justera representasjonar av omsetjingspar som vert referert frå ordboka. Vi flyttar framover frå denne linja og undersøker ein kontekstisk innbyggingstilnærming som er sensnivå og ordbokfritt. For å forbetra kvaliteten på kartet, gir vi også ein dyp vising av eigenskapar for kontekstiske innbygging, t.d. anisotropiske problemet og løysinga. Eksperimentar om tolking av avhengighet med null-bilde gjennom det delte området som er bygd av innebyggende transformasjonen vårt substituelt utfører tilstanden av kunsten med fleirspråk innbygging.', 'pl': 'Wykazano, że liniowa transformacja osadzenia jest skuteczna w przypadku zadań transferu wielojęzycznego zero-shot i osiąga zaskakująco obiecujące rezultaty. Jednak wielojęzyczne mapowanie przestrzeni osadzania jest zazwyczaj badane w statycznych osadzeniach na poziomie słów, gdzie transformacja przestrzeni jest pochodzona poprzez wyrównanie reprezentacji par tłumaczeń, które są odwoływane ze słowników. Idziemy dalej od tej linii i badamy kontekstowe podejście do osadzenia równowagi, które jest poziomowe i wolne od słownika. Aby poprawić jakość mapowania, zapewniamy również głęboki wgląd w właściwości kontekstowych osadzeń, tj. problem anizotropii i jego rozwiązanie. Eksperymenty na temat analizy zależności zero-shot poprzez przestrzeń współdzieloną koncepcją zbudowaną przez naszą transformację osadzenia znacznie przewyższają najnowocześniejsze metody wykorzystujące wielojęzyczne osadzenia.', 'ro': 'Transformarea liniară a încorporării s-a dovedit a fi eficientă pentru sarcinile de transfer translingvistic zero-shot și obține rezultate surprinzător de promițătoare. Cu toate acestea, cartografierea spațiului de încorporare interlingvă este de obicei studiată în încorporări statice la nivel de cuvânt, unde o transformare a spațiului este derivată prin alinierea reprezentărilor perechilor de traduceri care sunt trimise din dicționare. Ne deplasăm mai departe de această linie și investigăm o abordare contextuală de aliniere care este la nivel de simțuri și fără dicționar. Pentru a îmbunătăți calitatea cartografiei, oferim, de asemenea, o imagine profundă a proprietăților încorporărilor contextuale, adică problema anisotropie și soluția acesteia. Experimentele privind analizarea dependenței zero-shot prin spațiul conceptual-partajat construit de transformarea noastră de încorporare depășesc substanțial metodele de ultimă oră folosind încorporarea multilingvă.', 'sr': 'Pokazano je da je linijska transformacija uključena kao efikasna za zadatak preko jezika nula i da postignu iznenađujuće obećavajuće rezultate. Međutim, ukršteno ugrađenje svemirskog mapiranja obično se proučava u statičnim ugrađenjima na nivou reči, gde se svemirska transformacija proizvodi poravnanjem predstavljanja prevodnih parova koje se odnose iz reči. Prelazimo dalje od ove linije i istražujemo kontekstualni pristup usklađivanja koji je nivo osjećaja i bez rečnika. Da bi poboljšali kvalitet mapiranja, takođe pružimo duboko pogledanje vlasništva kontekstualnih integracija, tj. anisotropijskog problem a i njegovo rješenje. Eksperimenti o analiziranju zavisnosti od nule pucnjave kroz koncept-deli prostor izgrađen od strane našeg ugrađenog transformacije značajno iznosi nacin umjetnosti metode koristeći multijezičke integracije.', 'ta': 'சூழ்நிலை மொழி மாற்றும் பணிகளுக்கு வெளிப்படையாக உள்ள வரிசை மாற்றம் தெரியும் மற்றும் ஆச்சரியமான வாக்குறுதியான முடிவ ஆயினும், மொழிமொழியில் உள்ள இடைவெளி வரைப்படத்தை வழக்கமாக நிலையான சொல்- மட்டத்தில் படிக்கப்படுகிறது, அங்கு ஒரு வெளி மாற்றம் மொழிபெயர்ப்பு ஜோடிகள நாம் இந்த வரியிலிருந்து மேலும் நகர்த்தி ஒரு தற்காலிகமான உடையும் ஒழுங்குபடுத்தும் செயல்பாட்டை ஆராய்ச்சி ச வரைப்படத்தின் தரம் அதிகரிக்க, நாம் தற்போதைய குறிப்புகளின் தன்மைகளை ஆழமான பார்வையை கொடுக்கிறோம், அதாவது, அசைவூட்டுப் பிரச் பூஜ்ஜியத்தின் சார்பு சார்பு பாசார்ச்சை வழியாக எங்கள் உள்ளமைந்த மாற்றம் உருவாக்கப்பட்ட கருத்து பகிர்ந்த இடைவெளியில் பூஜ்ஜியமாக்கப்பட்டுள', 'so': 'Isbedelka maroonka waxaa looga muujiyey mid faa’iido u leh shaqaalaha wareejinta luqada nuurka ah iyo arimaha la yaab leh. Si kastaba ha ahaatee kartooyinka cirka ee ku qoran luuqadaha kala duduwan waxaa sida caadiga ah lagu baraa qorshaha heerka saxda ah, meesha lagu soo beddelayo isbedelka space looga soo bandhigi karo noocyada turjumidda ee laga soo jeedo dictionaries. Waxaannu ka guuraynaa halkan, waxaana baaritaannaa qaab isbedelka ah oo ah heerka sinnaanta iyo luqada lacag la’aanta ah. Si aan u kordhino qiimaha sawirka, waxaynu sidoo kale siinaynaa muuqasho aad u dheer oo ah xuquuqda meelaha la soo galo, tusaale ahaan dhibaatada dhimirka iyo xafiiska. Imtixaanka ku saabsan jardiinada ku saabsan zero-shoodka oo ku qoran saxda loo qaybsaday ee lagu dhisay qaababka beddelashada ee noocyada farshaxanka ah ee lagu isticmaalo qalabka luuqadaha kala duduwan.', 'sv': 'Linjär inbäddning transformation har visat sig vara effektiv för noll-skott cross-lingual transfer uppgifter och uppnå förvånansvärt lovande resultat. Men tvärspråklig inbäddningsutrymmeskartering studeras vanligtvis i statiska inbäddningar på ordnivå, där en rymdomvandling härleds genom att justera representationer av översättningspar som refereras från ordböcker. Vi rör oss längre från denna linje och undersöker en kontextuell inbäddning justeringsmetod som är sinnesnivå och ordbok-fri. För att förbättra kvaliteten på kartläggningen ger vi också en djup bild av egenskaper hos kontextuella inbäddningar, dvs anisotropi problemet och dess lösning. Experiment på noll-shot beroendetolkning genom konceptdelat utrymme byggt av vår inbäddningsformation överträffar avsevärt toppmoderna metoder med flerspråkiga inbäddningar.', 'ur': 'لینیر ابڈینگ تغییر دکھائی گئی ہے کہ صفر-شٹ کرس-زبان ٹرنس ٹرنس ٹرنس ٹرنس ٹرنس ٹرنس ٹرنس ٹرنس ٹرنس کے لئے اثر ہے اور عجیب وعدہ دینے والی نتائج حاصل کریں۔ However, cross-lingual embedding space mapping is usually studied in static word-level embeddings, where a space transformation is derived by aligning representations of translation pairs that are referred to by dictionaries. ہم اس لائن سے دور چلتے ہیں اور ایک متوسط انڈینگ الیٹینٹ کے مطابق تحقیق کرتے ہیں جو حس-سطح اور لفظ-آزاد ہے. نقشه کی کیفیت زیادہ کرنے کے لئے، ہم نے بھی متوسط انڈینگ کے خصوصے کی ایک عمیق نظر بھی دیتے ہیں، یعنی آنزیٹروپی مشکل اور اس کا حل. صفر-شٹ اعتباری پارس کے ذریعے ہمارے انڈمبینڈ تغییر کے ذریعے بنائے ہوئے مفصل-شریک جگہ کے ذریعے مطابق بہت زیادہ مطابق انٹیٹ کی حالت-آرت طریقے سے کام لیتے ہیں.', 'si': 'ලිනියර් සම්බන්ධ වෙනස් වෙනස් පෙන්වන්න පුළුවන් විදිහට ශූන්ය වෙනස් භාෂාව ප්\u200dරවර්තනය වැඩක් සඳහා ප්\u200d නමුත්, ක්\u200dරීස් භාෂාවක් සම්බන්ධ අවසානය සාමාන්\u200dයයෙන්ම ස්ථිර වචන- තත්වය සම්බන්ධ විදිහට අධ්\u200dයාපනය කරනවා, කොහෙද අවසානය වෙ අපි මේ ලායින් වලින් පරික්ෂා කරනවා සම්බන්ධතාවක් සම්බන්ධතාවක් සම්බන්ධතාවක් සම්බන්ධතාවක් ප්\u200dරවේශ සැකසුම් විශේෂතාවක් වැඩ කරන්න, අපි සම්බන්ධ විශේෂතාවක් ගොඩක් බලන්න ප්\u200dරශ්නයක්, ඉතින්, ඇනිස්ට්\u200dරෝපිය ප්\u200dරශ්න සුන්ධ-ශෝට් විශේෂතාවක් පරීක්ෂණය සඳහා පරීක්ෂණය සඳහා අපේ ඇම්බෙන්ඩ් විස්තරයෙන් නිර්මාණය කරලා තියෙන්නේ සුන්ධ-ශ', 'uz': "Name Lekin bir necha tillar ichki boʻsh joy kartoni odatda static so'z- darajadagi joylarda o'rganadi. Bu yerda space transformations derivati lugʻatlardan tashqi qoʻllanmalarni aniqlash imkoniyatlaridan foydalanadi. We move further from this line and investigate a contextual embedding alignment approach which is sense-level and dictionary-free.  Mavzuning sifatini oshirish uchun, biz davom etilgan xossalarning xossalarini ko'rib chiqaramiz, balki anisotroppy muammolari va ularning solutions. Bir necha tildan foydalanuvchi bo'lgan shaxsiy tasdiqlar bilan bir necha tildan foydalanishga qo'llangan boʻlgan joy orqali o'zgarishga ishlatiladigan.", 'vi': 'Quá trình cấy ghép tuyến đã được cho thấy hiệu quả với việc chuyển giao ngôn ngữ khác nhau bằng không bắn được và đạt được kết quả đáng ngạc nhiên hứa hẹn. Tuy nhiên, bản đồ khai thác không gian xuyên ngôn ngữ học thường được nghiên cứu trong sự nhúng vào các từ ngữ tĩnh, nơi sự chuyển đổi không gian được tính ra bằng cách chỉnh lại các biểu tượng của các cặp đã được gọi từ các từ điển. Chúng ta di chuyển xa khỏi dòng này và điều tra một phương pháp định hướng liên kết mà không có ý thức và từ điển. Để nâng cao chất lượng của bản đồ, chúng tôi cũng cung cấp một tầm nhìn sâu về các thuộc tính của sự tác nhân ngữ, tức là vấn đề dị tính và giải pháp của nó. Thí nghiệm độ phụ thuộc không phát triển qua khoảng không chia sẻ khái niệm được xây dựng bởi sự thay đổi của chúng tôi hoàn toàn vượt qua các phương pháp hiện đại, sử dụng sự nhúng vào đa dạng.', 'da': 'Lineær indlejring transformation har vist sig at være effektiv til nulskud tværsprogede overførselsopgaver og opnå overraskende lovende resultater. Men kortlægning af pladser på tværs af sprog undersøges normalt i statiske indlejringer på ordniveau, hvor en rumtransformation udledes ved at justere repræsentationer af oversættelsespar, der henvises til fra ordbøger. Vi bevæger os længere fra denne linje og undersøger en kontekstuel integrering justering tilgang, som er sanse-niveau og ordbogfri. For at forbedre kvaliteten af kortlægningen giver vi også et dybt billede af egenskaber af kontekstuelle indlejringer, dvs. anisotropi problemet og dets løsning. Eksperimenter med nulskud afhængighedsanalyse gennem konceptdelt rum bygget af vores integreringstransformation overgår væsentligt de nyeste metoder ved hjælp af flersprogede integreringer.', 'bg': 'Линейната трансформация на вграждане е доказана като ефективна при нулеви задачи за междуезичен трансфер и постига изненадващо обещаващи резултати. Въпреки това, междуезичното вграждане на пространствено картографиране обикновено се изучава в статични вграждания на ниво дума, където пространствена трансформация се получава чрез подравняване на представянето на двойки преводи, които се отнасят от речниците. Преминаваме по-далеч от този ред и изследваме контекстуален подход за вграждане на подравняване, който е на ниво сетива и без речници. За да подобрим качеството на картографирането, предоставяме и задълбочен поглед върху свойствата на контекстуалните вграждания, т.е. проблема с анизотропията и неговото решение. Експериментите за анализиране на зависимостта от нулев изстрел чрез споделеното пространство, изградено от нашата вградена трансформация, значително превъзхождат най-съвременните методи, използвайки многоезични вграждания.', 'nl': 'Lineaire embedding transformatie is effectief gebleken voor zero-shot cross-lingual transfer taken en levert verrassend veelbelovende resultaten op. Meertalige insluitruimtetoewijzing wordt echter meestal bestudeerd in statische insluitingen op woordniveau, waarbij een ruimtetransformatie wordt afgeleid door representaties van vertaalparen uit woordenboeken uit te lijnen. We gaan verder van deze lijn en onderzoeken een contextuele embedding alignment benadering die zintuigloos en woordenboek-vrij is. Om de kwaliteit van de mapping te verbeteren, bieden we ook een diepgaand inzicht in de eigenschappen van contextuele embeddings, dat wil zeggen het anisotropieprobleem en de oplossing ervan. Experimenten met zero-shot afhankelijkheid parsen via de concept-shared space die is gebouwd door onze embedding transformatie presteren aanzienlijk beter dan state-of-the-art methoden met meertalige embeddings.', 'hr': 'Pokazano je da je linijska transformacija ugrađena učinkovita za zadatke preko jezika nula upućene i ostvarila iznenađujuće obećavajuće rezultate. Međutim, ukršteno ugrađenje svemirskih mapiranja obično se proučava u statičnim ugrađenjima razine riječi, gdje se svemirska transformacija proizvodi uspoređivanjem predstavljanja prevodnih parova koje se navode iz riječi. Prelazimo dalje od ove linije i istražujemo kontekstualni pristup usklađivanja koji je razina osjećaja i bez rečnika. Da bismo poboljšali kvalitet mapiranja, također pružali duboko pogledanje vlasništva kontekstualnih integracija, tj. anisotropijskog problem a i njegovo rješenje. Eksperimenti o analiziranju zavisnosti od nule pucnjave kroz koncept-dijeljeno prostor izgrađen od strane našeg ugrađenog transformacije značajno iznosi metode stanja umjetnosti koristeći multijezičke integracije.', 'de': 'Die lineare Einbettungstransformation hat sich als effektiv für Zero-Shot crosslinguale Transferaufgaben erwiesen und erzielt überraschend vielversprechende Ergebnisse. Allerdings wird die sprachübergreifende Einbettungsraumzuordnung in der Regel in statischen Einbettungen auf Wortebene untersucht, wobei eine Raumtransformation durch Ausrichten von Repräsentationen von Übersetzungspaaren abgeleitet wird, die aus Wörterbüchern referenziert werden. Wir gehen weiter von dieser Linie und untersuchen einen kontextuellen Einbettungsansatz, der sinnlich und wortwörterbrei ist. Um die Qualität des Mappings zu verbessern, bieten wir auch einen tiefen Einblick in die Eigenschaften kontextueller Einbettungen, d.h. das Anisotropieproblem und seine Lösung. Experimente zum Parsen von Zero-Shot-Abhängigkeiten durch den Concept-Shared Space, der durch unsere Einbettungstransformation erstellt wurde, übertreffen im Wesentlichen modernste Methoden, die mehrsprachige Einbettungen verwenden.', 'fa': 'تغییر تغییر پیوند خط\u200cبندی نشان داده شده که برای کارهای انتقال\u200cدادن زبان\u200cهای متفاوت صفر موثر است و نتیجه\u200cهای قول\u200cدهنده\u200cای به طور عجیب رسیده است. با این حال، نقشه\u200cسازی فضای متوسط زبان معمولاً در انجمن\u200cسازی\u200cهای سطح کلمه\u200cهای ثابت تحقیق می\u200cشود، جایی یک تغییر فضایی با تغییر نمایش\u200cسازی جفت\u200cهای ترجمه\u200cسازی که از لغوی\u200cها منظور می\u200cشوند، تو ما از این خط دورتر حرکت می کنیم و یک طریق تطبیعی متوسط را تحقیق می کنیم که سطح حس و لغوی آزاد است. برای افزایش کیفیت نقشه\u200cبندی، ما همچنین یک نگاه عمیق از ویژه\u200cهای پیوند\u200cبندی\u200cهای موضوع، یعنی مشکل آنیزوروپی و راه حلش را پیشنهاد می\u200cکنیم. تجربه\u200cهای بستگی صفر از طریق فضای مشترک مفهوم ساخته شده توسط تغییرات پیدا کردن ما خیلی زیادی از روش\u200cهای هنری که با استفاده از پیدا کردن بستگی زبان\u200cهای زیادی بیشتر است.', 'id': 'Transformasi embedding linear telah menunjukkan efektif untuk tugas transfer saling bahasa tanpa tembakan nol dan mencapai hasil yang mengejutkan yang berjanji. Namun, pemetaan ruang penerbangan saling bahasa biasanya dipelajari dalam penerbangan kata-level statis, di mana transformasi ruang didirikan dengan menyesuaikan reprezentasi pasangan terjemahan yang direferensikan dari kamus. Kita bergerak lebih jauh dari garis ini dan menyelidiki pendekatan penyesuaian kontekstual yang merupakan tingkat masuk dan bebas kamus. To enhance the quality of the mapping, we also provide a deep view of properties of contextual embeddings, i.e., the anisotropy problem and its solution.  Eksperimen tentang dependensi zero-shot menghindari melalui ruang konsep-berbagi dibangun oleh transformasi embedding kami secara konsisten melebihi metode state-of-the-art menggunakan embedding multibahasa.', 'sw': 'Mabadiliko ya kiungo kimeonekana kuwa na ufanisi wa kazi za usafirishaji wa lugha zisizo na sifa na kupata matokeo ya kutarajiwa. Hata hivyo, ramani ya anga za lugha zinazoingia kwa lugha nyingine mara nyingi inasomwa katika maeneo ya kiwango cha maneno, ambapo mabadiliko ya anga yanatokana na kutambulisha wakilisha ndoa za tafsiri zinazotajwa kutoka kwa lugha. Tunaendelea zaidi kutoka kwenye mstari huu na tunachunguza mbinu za usambazaji wa kawaida ambazo ni kiwango cha maana na lugha huru. Ili kuongeza ubora wa ramani, pia tunatoa mtazamo wa kina wa utajiri wa maeneo yanayoendelea, yaani tatizo la kutisha na ufumbuzi wake. Majaribio yanayohusu kutegemea kwa kutumia nafasi inayosambazwa na dhana iliyoandaliwa na mabadiliko yetu kwa kiasi kikubwa yanafanya mbinu za sanaa kwa kutumia mabadiliko ya lugha mbalimbali.', 'tr': 'Çyzykly gabdaly gaýd etmek üçin bir syýal göçürmek üçin täsirli bolup görkezildi we geň galamly netijelere ýetip biler. Ýöne, çoklu dilli ekleýän seleňler görkezilişinde adatça statik söz seviňinde okalýar. Bu ýerde sözlerniň terjime çiftleriniň täzeliklerini çykaryp netijesi bar. Biz bu hatdan daşyrak geçirip, özümizi duýgun we sözlük boş bir metini barlaýarys. Haritasyonun kalitesini geliştirmek i çin, mesela aniotropi sorunlarının ve çözümünün derin bir görüntüsü sağlayacağız. Görkezilişimiz görkezilişinde nähili görkezilişinde baglanyşygymyz üçin guruldy concept-paylaşyk boýunça çykyşynda örän çykyş şeklinde guruldy.', 'ko': '선형 삽입 변환은 효과적인 제로 렌즈 크로스 언어 이동 임무로 증명되었고 놀랍게도 희망적인 결과를 얻었다.그러나 크로스 언어 삽입 공간 매핑은 보통 정적 단어 삽입에서 연구한 것이고 정적 단어 삽입에서 공간 변환은 사전에서 인용한 번역 대조의 표시를 맞추어 도출한 것이다.우리는 한층 더 이런 각도에서 출발하여 상하문 삽입 정렬 방법을 연구했는데 이 방법은 의미 수준과 사전은 무관하다.비추는 질을 높이기 위해 우리는 상하문에 박힌 특성, 즉 각방향 이성 문제와 해결 방안을 깊이 연구했다.우리의 삽입 전환을 통해 구축된 개념 공유 공간을 통해 영포 의존 해석 실험을 하는 것은 다국어 삽입을 사용하는 최신 방법보다 훨씬 낫다.', 'af': "Lineêre inbêring transformasie is vertoon om effektief te wees vir nul-skoot kruistale oordrag opdragte en verwonderlik beloftende resultate te bereik. Maar, kruistale inbêring spasiekaart word gewoonlik in statiese woord-vlak inbêring onderwyser, waar 'n spasietransformasie is afgeleide deur verligting van vertalings van vertalings paar wat van woordeboekvorms verwys word. Ons beweeg verder van hierdie lyn en ondersoek 'n kontekslike inbêde oplyn toegang wat sin-vlak en woordeboek-vry is. Om die kwaliteit van die kaart te verbeter, verskaf ons ook 'n diep aansig van eienskappe van contextual inbêding, t.d. die anisotropie problem e en sy oplossing. Eksperimente op nul-skoot afhanklikheid verwerking deur die konsept-gedeelde ruimte gebou deur ons inbêring transformasie substantieel uitvoer staat-van-kuns metodes deur multilingse inbêding te gebruik.", 'am': 'የኢትዮጵያ ቋንቋ-ቋንቋ ስራዎችን ለመቀላቀል እና የተስፋ ፍሬዎችን ለማግኘት የሚያስፈልገውን መስመር አግኝቷል፡፡ ምንም እንኳን የቋንቋ ቋንቋ-ቋንቋዎች የስፋት ማረጃዎች በተለየ ቃላት-ደረጃ ውስጥ በተለየ ይታያል፤ ቦታ ለውጥ ከቋንቋዎች የተለየ ተርጓሚዎች ዓይነቶች በተለየ መጠቀም ይታያል፡፡ ከዚህም መስመር ወደ ፊት እንነሳቅሳለን እና የቋንቋው ነፃ ያለበትን የግንኙነት አካባቢ ስርዓት እናሳውቃለን፡፡ ካርታ ጥሩ ለማድረግ፣ የአሁኑን ግንኙነት የግንኙነቱን ጥልቅ ማየት እናደርጋለን፡፡ በክፍለ ፎቶ የክፍል ታማኝነት ማዘጋጀት በብዙ ቋንቋ አካባቢዎችን በመጠቀም የክፍለ ስፍራችን በተካፈሉት የክፍተት የክፍተት ውጤት አካባቢ ነው፡፡', 'sq': 'Transformimi linear i përfshirjes është treguar të jetë efektiv për detyra zero-shot trans-gjuhësore transferimi dhe të arrijë rezultate surprizuese premtuese. Megjithatë, hartimi ndërgjuhësor i hapësirës përfshirë zakonisht studiohet në përfshirje statike të nivelit të fjalëve, ku një transformim hapësiror nxirret nga renditja e përfaqësimeve të çifteve të përkthimit që janë referuar nga fjalorët. Ne lëvizim më tej nga kjo linjë dhe hetojmë një metodë kontekstuale të përshtatjes së përshtatjes që është e kuptueshme dhe pa fjalor. Për të përmirësuar cilësinë e hartimit, ne ofrojmë gjithashtu një pamje të thellë të pronave të përfshirjeve kontekstuale, pra, problemet e anizotropisë dhe zgjidhjes së saj. Experiments on zero-shot dependency parsing through the concept-shared space built by our embedding transformation substantially outperform state-of-the-art methods using multilingual embeddings.', 'hy': "Պարզվել է, որ գծային ներդրման փոխակերպումը արդյունավետ է զրոյի միջլեզվային փոխանցման խնդիրների համար և զարմանալիորեն խոստացող արդյունքներ է ստանում: Այնուամենայնիվ, տիեզերքի խաչլեզվով ներգրավված քարտեզագրությունը սովորաբար ուսումնասիրում է բառերի ստատիկ մակարդակի ներգրավումների մեջ, որտեղ տիեզերքի վերափոխությունը կատարվում է բառարաններից վերաբերվող թարգմանման զույգերի ներկայացումների հավաս Մենք հեռու ենք գնում այս գծից և ուսումնասիրում ենք մի կոնտեքստալ ներառող հարմարեցման մոտեցում, որը զգայական մակարդակի և բառարանի ազատ է: Քարտեզագրության որակը բարելավելու համար մենք նաև խորը պատկերացում ենք կոնտեքստային ներդրումների հատկություններին, այսինքն' անիզոտրոպիայի խնդիրը և դրա լուծումը: Զերոյական կախվածության փորձարկումները, որոնք վերլուծում են բազմալեզու տարածության միջոցով կառուցված մեր ներդրող վերափոխության շնորհիվ, հիմնականում գերազանցում են ամենաբարձր մեթոդները՝ օգտագործելով բազլեզու ներդրումներ:", 'az': 'Sıfır dillərin qarşılaşdırılmış işləri üçün dəyişiklik inşa edilməsi və təəccüblü vədə verən sonuçları başa düşür. Lakin, çox dilli birləşdirilmiş uzay mapası genellikle statik söz-seviyyəti içərisində təhsil edilir. Lakin sözlərdən danışılan təhsil çiftlərinin təhsil edilməsi ilə uzay transformasyonu təhsil edir. Biz bu səhifədən uzaqlaşdırırıq və anlayış-seviyyəti və sözlük-boş bir müxtəlif yerləşdirmək tərəfindən istifadə edirik. Haritasının keyfiyyətini artırmaq üçün, həmçinin müxtəlif inşalların özelliklərini, yani anisotropiya problemini və çözümünü görürük. Sıfır-fəsad bağlılığı təcrübələrini çoxlu dil inşallarını istifadə edərək inşa edilən məlumatlarımızın paylaşılmış yer vasitəsilə çəkilən sıfır-fəsad təcrübələrinin təcrübələrini çoxlu dəlillərdən üstün etdik.', 'ca': "S'ha demostrat que la transformació linear d'incorporació és eficaç en tasques de transfer ència translingüística de zero fotografies i aconsegueix resultats sorprenentment prometedors. No obstant això, el mapatge espacial d'incorporació translingüística es estudia normalment en incorporacions estatiques de nivell de paraules, on una transformació espacial es deriva alliniant representacions de parells de traducció que es refereixen dels diccionaris. Vam més lluny d'aquesta línia i investigam un enfocament contextual d'integració que és sense sentit i sense diccionari. To enhance the quality of the mapping, we also provide a deep view of properties of contextual embeddings, i.e., the anisotropy problem and its solution.  Els experiments d'analitzar la dependencia de zero-shot a través de l'espai compartit de concepte construït per la nostra transformació incorporadora superen substancialment els mètodes més avançats utilitzant incorporacions multilingües.", 'bs': 'Pokazalo se da je linijska transformacija uključena u obzir da je učinkovita za zadatak preko jezika nula i da postignu iznenađujuće obećavajuće rezultate. Međutim, ukršteno ugrađenje svemirske mapiranje obično se proučava u statičkim ugrađenjima na razini riječi, gdje se svemirska transformacija proizvodi usklađivanjem predstavljanja prevodnih parova koje se odnose iz riječi. Prelazimo dalje od ove linije i istražujemo kontekstualni pristup usklađenja koji je nivo osjećaja i bez rečnika. Da bismo poboljšali kvalitet mapiranja, također pružali duboko pogledanje vlasništva kontekstualnih integracija, tj. anisotropijskog problem a i njegovo rješenje. Eksperimenti o analiziranju zavisnosti od nule pucnjave kroz podjeljeno prostor koncepta izgrađeno od strane našeg ugrađenog transformacije značajno iznosi metode stanja umjetnosti koristeći multijezičke integracije.', 'bn': 'শুধুমাত্র ক্রস-শুটের ক্রস-ভাষাভাষী পরিবর্তনের জন্য লাইনের পরিবর্তন কার্যকর এবং বিস্ময়কর প্রতিশ্রুতিশীল ফলাফল অর্জন করা হয়েছে। তবে স্থানীয় শব্দ-স্তরের বিভিন্ন বিভিন্ন বিভিন্ন ভাষায় বিভিন্ন স্পেস ম্যাপ পিং সাধারণত গবেষণা করা হয়, যেখানে অনুবাদ জোড়ার প্রতিনিধির সাথ আমরা এই লাইন থেকে আরো এগিয়ে যাচ্ছি এবং বিদ্যমান বিভিন্ন স্তর এবং অভিভাবকে মুক্ত করার ক্ষেত্রে তদন্ত করি। মানচিত্রের মান বৃদ্ধি করার জন্য আমরা সাধারণ বিষয়বস্তুর বৈশিষ্ট্যের বৈশিষ্ট্যের গভীর দৃষ্টিভঙ্গি প্রদান করি, যেমন অ্যানিস আমাদের বিভিন্ন পরিবর্তনের মাধ্যমে আমাদের বিভিন্ন ভাগাভাগি স্থান নির্মিত পরীক্ষার মাধ্যমে শুধুমাত্র শুটের নির্ভর পার্সিং নির্ভর করে আমা', 'cs': 'Bylo prokázáno, že lineární vkládání transformace je efektivní pro nulové přenosové úlohy mezi jazyky a dosahuje překvapivě slibných výsledků. Nicméně, mapování mezi jazyky pro vkládání prostoru je obvykle studováno v statických vkládáních na úrovni slova, kde je transformace prostoru odvozena zarovnáním reprezentací překladových párů, které jsou odkazovány ze slovníků. Přecházíme dál od této řádky a zkoumáme kontextový přístup zarovnávání, který je smyslový a bez slovníku. Pro zvýšení kvality mapování poskytujeme také hluboký pohled na vlastnosti kontextových vložení, tj. problém anizotropie a jeho řešení. Experimenty s analýzou závislosti nulového výstřelu prostřednictvím konceptu sdíleného prostoru vytvořeného naší transformací vkládání podstatně překonávají nejmodernější metody využívající vícejazyčné vložení.', 'et': 'Lineaarne manustamise transformatsioon on osutunud efektiivseks nullkeelsete ülekannete puhul ja saavutab üllatavalt paljulubavaid tulemusi. Kuid keeleülest manustamisruumi kaardistamist uuritakse tavaliselt staatilistes sõnataseme manustamistes, kus ruumi muundamine tuletatakse sõnaraamatutest viidatud tõlkepaaride esituste joondamise teel. Me liigume sellest reast kaugemale ja uurime kontekstilist põimitud joondamise lähenemisviisi, mis on mõistetasemel ja sõnastikuvaba. Kaardistamise kvaliteedi parandamiseks pakume sügavat ülevaadet kontekstiliste manustamiste omadustest, st anisotroopia probleemist ja selle lahendusest. Eksperimentid null-shot sõltuvuse parsimiseks kontseptsiooniga jagatud ruumi kaudu, mis on loodud meie manustamise muundamisega, on oluliselt paremad kui kaasaegsed meetodid, kasutades mitmekeelseid manustamisi.', 'fi': 'Lineaarisen upotusmuunnoksen on osoitettu olevan tehokas nollashotin monikielisissä siirtotehtävissä ja saavuttavan yllättävän lupaavia tuloksia. Kuitenkin monikielistä upotusavaruuden kartoitusta tutkitaan yleensä staattisissa sanatason upotuksissa, joissa avaruuden muunnos johdetaan tasaamalla sanakirjoista viitattujen käännösparien representaatioita. Siirrymme tästä rivistä kauemmas ja tutkimme kontekstuaalista sulauttamista linjaukseen perustuvaa lähestymistapaa, joka on aistitason ja sanakirjan vapaa. Kartoituksen laadun parantamiseksi tarjoamme myös syvällisen kuvan kontekstuaalisten upotusten ominaisuuksista eli anisotropia-ongelmasta ja sen ratkaisusta. Kokeet nolla-shot-riippuvuuden jäsentämisestä upotustransformaatiomme rakentaman konsepti-jaetun tilan kautta ovat huomattavasti parempia kuin uusimmat menetelmät monikielisten upotusten avulla.', 'jv': 'text-tool-action politenessoffpolite"), and when there is a change ("assertivepoliteness text-tool-action bookmark User:', 'he': 'הוכחה שהשינוי הקולינרי של התכנית יעיל עבור משימות העברה דרך שפתיים של אפס יריות ולהשיג תוצאות מבטיחות באופן מפתיע. בכל אופן, מפת חלל שיפוי דרך שפתיים בדרך כלל נלמדת בתכניות סטטיות ברמה מילים, שבו שינוי חלל מושלם על ידי התייצבות של זוגות התרגום שמדורשות ממילונים. אנחנו הולכים רחוק מהשורה הזו וחקירים גישה קונטקסטלית של התאמה אשר היא רמת הגיון וחסרת מילון. כדי לשפר את איכות המפה, אנחנו גם מספקים נוף עמוק של תכונות של תוכניות קונטקסטיות, כלומר, בעיית האניזוטרופיה והפתרון שלה. ניסויים על תלויות אפס-יריות מתחקרים דרך חלל משותף-מושג בנוי על ידי השינוי הקבוע שלנו ביצע שיטות מצוינות במידה משמעותית באמצעות שיטות רבות שפות.', 'sk': 'Linearna transformacija vdelave se je izkazala za učinkovito pri ničelnih medjezičnih nalogah prenosa in dosega presenetljivo obetavne rezultate. Vendar pa se medjezično mapiranje prostora za vdelavo običajno proučuje v statičnih vdelavah na ravni besed, kjer se prostorska transformacija izpelje z usklajevanjem predstavitev prevajalskih parov, ki se sklicujejo iz slovarjev. Oddaljujemo se od te vrstice in raziskujemo kontekstualni pristop poravnave, ki je brez slovarja in na ravni smisla. Za izboljšanje kakovosti kartiranja zagotavljamo tudi poglobljen pogled na lastnosti kontekstualnih vdelav, tj. problem anizotropije in njeno rešitev. Poskusi razčlenitve odvisnosti brez posnetkov skozi konceptno deljeni prostor, zgrajen z našo transformacijo vdelave, so bistveno boljši od najsodobnejših metod z uporabo večjezičnih vdelav.', 'ha': "An nuna canza shifotan linje da aka shigar da shi yana mai amfani da wa aikin transfer na sifiri-tsohon-linguin kuma ya sami matsalan mai yiwuwa da ke yi wa'adi. A lokacin da aka shigar da filinaikin cikin tsunan-linguin na ɗabi'a, ana karatun karmaptin space na ɗabi'a a cikin shirin taƙaita cikin tsarin-daraja, a wurin da za'a motsar wata shigewar space da ke samanta da siffanti masu nau'in fassarar da aka sanar da su daga dictionaries. Munã tafiyar da sauri daga wannan line kuma munã tambayi wata hanyor cikin juyin-faɗi wanda ke cikin a guda, wanda ke da ma'anar-daraja da wata dictionary-ba'a. To, dõmin ya ƙãra sifar ramani, za'a ƙayyade misãlai masu tsari na masu cikin shirin da ke cikin takarda, misali, matabbatarwa da ke sami. Tajararin da ke kan tsari da sifire-shekara da aka samar da shi cikin filin da aka raba shi da shirin hanyoyinmu da za'a samar da shi cikin hanyon-na-kunyar da za'a yi amfani da sauri masu buƙata.", 'bo': 'གྲལ་ཐིག་གནས་ཡུལ་གྱི་བསྒྱུར་བཅོས་ནི་རྣམས་ཀྱིས་ཐལ་ཡིག་ཐབས་ལམ་སྐྱེལ་བ་དང ཡིན་ནའང་། སྐད་ཡིག ང་ཚོས་གྲལ་ཐོག་འདི་ནས་གཤམ་དུ་སྤོར་ནས་ཁུལ་ཡུལ་གྱི་གནས་སྟངས་བཙུགས་པའི་གྲལ་སྒྲིག་ཕྱོགས་ཞིག་ལ་ཞིབ་བཤེར། ས་ཁྲ་པར་ཀྱི་རིམ་ཐང་ཡར་རྒྱས་ལྗོངས་ཀྱི་ངོ་བོའི་ཁྱད་པར་ཕྱོགས་ཀྱི་ཁྱད་ཆོས་ཀྱི་རང་བོའི་ལྟ་སྟངས་ཀྱང་ཡང་མཐོང་ཡོད། དཔེར་ན། ཨ་ནི་ Experiments on zero-shot dependency parsing through the concept-shared space built by our embedding transformation substantially outperform state-of-the-art methods using multilingual embeddings.'}
{'en': 'Gradual Fine-Tuning for Low-Resource Domain Adaptation', 'fr': "Réglage fin progressif pour l'adaptation des domaines à faibles ressources", 'ar': 'الضبط الدقيق التدريجي لتكييف المجال منخفض الموارد', 'es': 'Ajuste gradual para la adaptación de dominios de bajos recursos', 'pt': 'Ajuste fino gradual para adaptação de domínio de poucos recursos', 'zh': '针下资源域渐微', 'hi': 'कम-संसाधन डोमेन अनुकूलन के लिए क्रमिक ठीक ट्यूनिंग', 'ja': '低リソースドメインアダプテーションのための段階的な微調整', 'ru': 'Постепенная точная настройка для адаптации домена с низким уровнем ресурсов', 'ga': 'Mionchoigeartú de réir a chéile le haghaidh Oiriúnú Fearainn Íseal Acmhainní', 'ka': 'დამატებული რესურსის დიომენის ადაპტიფიკაციისთვის დრადუალური კონფიგურაცია', 'hu': 'Fokozatos finomhangolás az alacsony erőforrású tartományok adaptálásához', 'el': 'Σταδιακός λεπτός συντονισμός για την προσαρμογή του τομέα χαμηλών πόρων', 'it': "Regolazione graduale per l'adattamento del dominio a basse risorse", 'kk': 'Төменгі ресурс домен адаптациясы үшін градикалық таңдау', 'ms': 'Gradual Fine-Tuning for Low-Resource Domain Adaptation', 'ml': 'കുറഞ്ഞ വിഭവവിഭവങ്ങളുടെ ഡോമെയിന്\u200d അഡാപ്റ്റേഷന്\u200d', 'mn': 'Бага боловсролын домайн адаптацийн төвшин хөрөнгө тооцоолох', 'lt': 'Laipsniškas koregavimas mažai išteklių turinčiam domenui pritaikyti', 'no': 'Fargeoppsett for låg ressursdomene', 'mk': 'Градонално фино прилагодување за адаптација на доменот со ниски ресурси', 'pl': 'Stopniowe dostosowywanie domen o niskich zasobach', 'ro': 'Reglare fină treptată pentru adaptarea domeniului cu resurse reduse', 'mt': 'Gradual Fine-Tuning for Low-Resource Domain Adaptation', 'sr': 'Gradualno fino prilagođenje za adaptaciju domena niskih resursa', 'si': 'අඩුම සම්පත්ත ඩොමේන් අනුමාණය වෙනුවෙන් ප්\u200dරමාණය සඳහා ප්\u200dරමාණය සඳහා හොඳ සම්පූර්ණය', 'ta': 'குறைந்த மூலத்தின் மேற்கோள்', 'so': 'Adaptation domain', 'sv': 'Gradvis finjustering för anpassning av domäner med låga resurser', 'ur': 'نیچے رسورس ڈومین اڈپٹیٹ کے لئے گرڈیوال فین ٹونگ', 'uz': 'Name', 'vi': 'KCharselect unicode block name', 'bg': 'Постепенно фино настройване за адаптиране на домейна с ниски ресурси', 'nl': 'Geleidelijke fine-tuning voor aanpassing van domeinen met weinig hulpbronnen', 'hr': 'Gradualno fino prilagođenje domena s niskim resursima', 'da': 'Gradvis finjustering for tilpasning af domæne med lav ressource', 'de': 'Stufenweise Feinabstimmung für ressourcenarme Domänenanpassung', 'ko': '점차 미세하게 조정하여 저자원역에 적응하다', 'id': 'Penyesuaian Baik Gradual untuk Adaptasi Domain Sumber Terrendah', 'fa': 'تنظیم پایه\u200cای برای تغییرات دامنۀ کم منبع', 'tr': 'Hejim Ressurat Ululyky', 'af': 'Name', 'am': 'ምርጫዎች', 'sw': 'Utamasishaji wa viwili kwa ajili ya Kampeni ya Rasilimali chini', 'hy': 'Անվատ ռեսուրսների բնագավառի հարմարեցման համար', 'bn': 'নিম্ন- সম্পদ ডোমেইন পরিচালনার জন্য গ্রাডুল ফাইন-টিউনিং', 'bs': 'Gradualno fino prilagođenje za adaptaciju domena niskih resursa', 'az': 'Aşağı Kaynaq Domena Adjustasyonu', 'cs': 'Postupné jemné ladění pro adaptaci domén s nízkými zdroji', 'ca': "Un ajustament gradual per a l'adaptació a dominis de baix recursos", 'fi': 'Vähitellinen hienosäätö vähäisten resurssien verkkotunnusten mukauttamista varten', 'et': 'järkjärguline täpsustamine vähese ressursiga domeeni kohandamiseks', 'sq': 'Ndryshimi gradual i mirë për përshtatjen e domenit me burime të ulta', 'jv': 'ProgressBar', 'ha': '@ action', 'bo': 'རྫུན་པ་ཆུང་ལ་ཉུང་བའི་ཁ་སྒྲིག་འགོད་ལ་འཛུགས་པ།', 'he': 'Gradual Fine-Tuning for Low-Resource Domain Adaptation', 'sk': 'Postopno natančno nastavitev za prilagajanje domen z nizkimi viri'}
{'en': 'Fine-tuning is known to improve NLP models by adapting an initial  model  trained on more plentiful but less domain-salient examples to data in a target domain. Such  domain adaptation  is typically done using one stage of  fine-tuning . We demonstrate that gradually fine-tuning in a multi-step process can yield substantial further gains and can be applied without modifying the  model  or learning objective.', 'es': 'Se sabe que el ajuste fino mejora los modelos de PNL al adaptar un modelo inicial entrenado en ejemplos más abundantes pero menos relevantes en el dominio a los datos de un dominio objetivo. Tal adaptación de dominio se realiza normalmente utilizando una etapa de ajuste fino. Demostramos que el ajuste gradual en un proceso de varios pasos puede generar ganancias adicionales sustanciales y se puede aplicar sin modificar el modelo o el objetivo de aprendizaje.', 'fr': "Le réglage fin est connu pour améliorer les modèles de PNL en adaptant un modèle initial formé sur des exemples plus nombreux mais moins saillants dans le domaine aux données d'un domaine cible. Une telle adaptation de domaine est généralement effectuée au moyen d'une étape de réglage fin. Nous démontrons qu'un ajustement progressif dans un processus en plusieurs étapes peut générer des gains supplémentaires substantiels et peut être appliqué sans modifier le modèle ou l'objectif d'apprentissage.", 'ar': 'يُعرف الضبط الدقيق بتحسين نماذج البرمجة اللغوية العصبية من خلال تكييف نموذج أولي تم تدريبه على أمثلة أكثر وفرة ولكن أقل بروزًا في المجال مع البيانات الموجودة في المجال المستهدف. يتم عادةً تكييف المجال باستخدام مرحلة واحدة من الضبط الدقيق. نثبت أن الضبط الدقيق تدريجيًا في عملية متعددة الخطوات يمكن أن يؤدي إلى مكاسب إضافية كبيرة ويمكن تطبيقه دون تعديل النموذج أو هدف التعلم.', 'pt': 'O ajuste fino é conhecido por melhorar os modelos de PNL adaptando um modelo inicial treinado em exemplos mais abundantes, mas menos salientes de domínio, a dados em um domínio de destino. Essa adaptação de domínio é normalmente feita usando um estágio de ajuste fino. Demonstramos que o ajuste fino gradual em um processo de várias etapas pode gerar ganhos adicionais substanciais e pode ser aplicado sem modificar o modelo ou o objetivo de aprendizado.', 'ja': '微調整は、より豊富であるがドメインサリエントの少ない例で訓練された初期モデルを標的ドメイン内のデータに適応させることによって、NLPモデルを改善することが知られている。そのようなドメイン適応は、典型的には、微調整の１つの段階を使用して行われる。私たちは、多段階プロセスで徐々に微調整することで、実質的なさらなる利得を生み出すことができ、モデルや学習目標を変更することなく適用できることを実証します。', 'hi': 'फाइन-ट्यूनिंग को लक्ष्य डोमेन में डेटा के लिए अधिक भरपूर मात्रा में लेकिन कम डोमेन-प्रमुख उदाहरणों पर प्रशिक्षित प्रारंभिक मॉडल को अनुकूलित करके एनएलपी मॉडल में सुधार करने के लिए जाना जाता है। इस तरह के डोमेन अनुकूलन आमतौर पर ठीक ट्यूनिंग के एक चरण का उपयोग करके किया जाता है। हम प्रदर्शित करते हैं कि एक बहु-चरणीय प्रक्रिया में धीरे-धीरे ठीक-ट्यूनिंग पर्याप्त आगे लाभ प्राप्त कर सकती है और मॉडल या सीखने के उद्देश्य को संशोधित किए बिना लागू की जा सकती है।', 'zh': '凡所周知,微调可以丰基而不甚异域者示例教之初,宜用于域中之数,以改NLP之。 此方宜用一微调。 吾证多步骤之渐微,可以生实质性益,而可以不修模样、学问者也。', 'ru': 'Известно, что тонкая настройка улучшает модели NLP, адаптируя начальную модель, обученную на более многочисленных, но менее чувствительных к домену примерах, к данным в целевой области. Такая адаптация домена обычно выполняется с использованием одного этапа тонкой настройки. Мы демонстрируем, что постепенная доработка в многоэтапном процессе может принести существенные дополнительные выгоды и может быть применена без изменения модели или цели обучения.', 'ga': 'Is eol go bhfeabhsaítear samhlacha NLP trí mhionchoigeartú a dhéanamh ar mhúnla tosaigh atá oilte ar shamplaí níos flúirseach ach nach bhfuil chomh suntasach céanna leis an bhfearann do shonraí i spriocfhearann. De ghnáth déantar oiriúnú fearainn den sórt sin ag baint úsáide as céim amháin den mhionchoigeartú. Léirímid gur féidir le mionchoigeartú de réir a chéile i bpróiseas ilchéime dul chun cinn suntasach a bhaint amach agus gur féidir é a chur i bhfeidhm gan an tsamhail nó an cuspóir foghlama a mhodhnú.', 'el': 'Η τελειοποίηση είναι γνωστή για τη βελτίωση των μοντέλων προσαρμόζοντας ένα αρχικό μοντέλο εκπαιδευμένο σε πιο άφθονα αλλά λιγότερο σημαντικά παραδείγματα σε δεδομένα σε έναν τομέα-στόχο. Μια τέτοια προσαρμογή του τομέα συνήθως γίνεται χρησιμοποιώντας ένα στάδιο λεπτού συντονισμού. Αποδεικνύουμε ότι η σταδιακή προσαρμογή σε μια διαδικασία πολλαπλών βημάτων μπορεί να αποφέρει σημαντικά περαιτέρω οφέλη και μπορεί να εφαρμοστεί χωρίς τροποποίηση του μοντέλου ή του μαθησιακού στόχου.', 'ka': 'უკეთესი კონფიგურაცია იცნობა NLP მოდელების უფრო მეტი მოდელზე აეპტიგურაციით, მაგრამ უფრო მეტი მოდელში მომხმარებული, მაგრამ უკეთესი მოდელების მაგალითების შე ასეთი დიომინის აკაპეტიფიკაცია ტიპოლურად გავაკეთებულია, რომელიც ერთი კონფიგურაციის ფაეზი გამოყენებულია. ჩვენ ევმონსტრაცით, რომ მრავალდომების პროცესში სწორად კონფიგურაცია შეიძლება გავიღოთ მნიშვნელოვანი მოქმედება და შეიძლება გააყენება მოდელს ან სწავლების მიზეზი', 'hu': 'A finomhangolás ismert, hogy javítja az NLP modelleket azáltal, hogy egy kezdeti modellt alkalmaznak a bőségesebb, de kevésbé domain-kiemelkedő példákra vonatkozóan képzett modellekhez egy céltartományban található adatokhoz. Az ilyen domain adaptáció általában a finomhangolás egy szakaszával történik. Bemutatjuk, hogy a fokozatos finomhangolás egy többlépcsős folyamatban jelentős további előnyöket eredményezhet, és alkalmazható a modell vagy a tanulási cél módosítása nélkül.', 'kk': 'Жақсы баптау NLP үлгілерін өзгерту үшін бастапқы үлгілерді көптеген, бірақ мақсатты домендегі деректер үшін доменнің салайлану үлгілерін баптау үшін біледі. Бұл домендің адаптациясы әдетте бір сәйкес баптау кезегін қолданады. Біз бірнеше қадам процесінде тәжірибелі баптауларды көп қадамдастыруға мүмкіндік бере аламыз. Үлгі не оқыту мақсатын өзгертпей қолдануға болады.', 'it': "Il fine-tuning è noto per migliorare i modelli NLP adattando un modello iniziale addestrato su esempi più abbondanti ma meno salienti di dominio ai dati in un dominio di destinazione. Tale adattamento del dominio è tipicamente fatto utilizzando una fase di messa a punto. Dimostriamo che la messa a punto graduale in un processo multi-step può produrre notevoli ulteriori guadagni e può essere applicata senza modificare il modello o l'obiettivo di apprendimento.", 'ms': 'Penyesuaian baik diketahui untuk meningkatkan model NLP dengan menyesuaikan model awal dilatih pada contoh yang lebih banyak tetapi kurang penting domain kepada data dalam domain sasaran. Penyesuaian domain seperti ini biasanya dilakukan menggunakan satu tahap penyesuaian. Kami menunjukkan bahawa perlahan-perlahan dalam proses berbilang-langkah boleh memberikan keuntungan yang lebih besar dan boleh dilaksanakan tanpa mengubahsuai model atau objektif belajar.', 'lt': 'Žinoma, kad patobulinimas gerina NLP modelius pritaikydamas pradinį model į, parengtą daugeliu, bet mažiau dominuojančių pavyzdžių prie tikslinės srities duomenų. Toks domeno pritaikymas paprastai atliekamas vienu tikslinimo etapu. We demonstrate that gradually fine-tuning in a multi-step process can yield substantial further gains and can be applied without modifying the model or learning objective.', 'ml': 'മോഡല്\u200d മെച്ചപ്പെടുത്തുന്നതിനാല്\u200d മെച്ചപ്പെടുത്തുന്നതിനായി നല്ല മാതൃകങ്ങള്\u200d പരിശീലിക്കുന്നതിനായി അറിയുന്നു ഇതുപോലുള്ള ഡൊമെന്\u200d അഡാപ്റ്റേഷന്\u200d സാധാരണ ഒരു സ്റ്റേജ് ഉപയോഗിച്ച് ചെയ്തിരിക്കുന്നു. ഞങ്ങള്\u200d കാണിച്ചു കൊണ്ടിരിക്കുന്നു ഒരു പലിപടി പ്രക്രിയയില്\u200d സാവധാനപ്പെടുത്തുന്നത് മാതൃകയോ പഠിക്കാതിരിക്കുന്നതിന് മാത', 'no': 'Fine-tuning er kjent for å forbetra NLP-modeller ved å tilpassa ein opphavsmedyl trengt på meir fleire, men mindre eksemplar for domenesalient for data i eit målgområde. Denne domeneadaptasjonen er vanlegvis ferdige ved å bruka ein stad med fine-tuning. Vi viser at gradvis finnstilling i ein fleirsteg-prosess kan gje store forskjellige forsøk og kan brukast utan å endra modellen eller læringsmålet.', 'mn': 'НЛП загваруудыг сайжруулахын тулд илүү их боловсруулагдсан, гэхдээ зорилготой зорилготой өгөгдлийн жишээ бага байдаг. Ийм холбоотой адилтгал хийгддэг нь ихэвчлэн нэг шатаар сайжруулах хэрэгтэй. Бид олон алхам үйл явцдаа бага зэрэг загвар, суралцах зорилго өөрчлөгдсөн боломжтой болно гэдгийг харуулж байна.', 'mt': 'L-a ġġustament finat huwa magħruf li jtejjeb il-mudelli tal-NLP billi jadatta mudell inizjali mħarreġ fuq eżempji aktar abbundanti iżda inqas salienti fid-dominju għad-dejta f’dominju fil-mira. Tali adattament tad-dominju tipikament isir bl-użu ta’ stadju wieħed ta’ rfinar. Aħna nippruvaw li l-a ġġustament gradwali fi proċess f’diversi stadji jista’ jwassal għal kisbiet sostanzjali ulterjuri u jista’ jiġi applikat mingħajr ma jiġi modifikat il-mudell jew l-objettiv tat-tagħlim.', 'pl': 'Dostrajanie jest znane z usprawnienia modeli NLP poprzez dostosowanie początkowego modelu przeszkolonego na bardziej licznych, ale mniej istotnych przykładach domeny do danych w domenie docelowej. Taka adaptacja domeny odbywa się zwykle przy użyciu jednego etapu dostrajania. Pokazujemy, że stopniowe dostrajanie w procesie wieloetapowym może przynieść znaczne dalsze zyski i może być stosowane bez modyfikacji modelu lub celu uczenia się.', 'mk': 'Познато е дека подобрувањето на моделите на НЛП се подобрува со адаптација на првичниот модел трениран на повеќе, но помалку важни примери од доменот на податоците во доменот на целта. Таквата адаптација на доменот обично се прави користејќи една фаза на финетизирање. Демонстрираме дека постепено финетизирањето во повеќекратниот процес може да даде значителни понатамошни профили и може да се примени без моделот или целта на учењето.', 'ro': 'Ajustarea fină este cunoscută pentru a îmbunătăți modelele PNL prin adaptarea unui model inițial instruit pe exemple mai abundente, dar mai puțin importante de domeniu la datele dintr-un domeniu țintă. O astfel de adaptare a domeniului se face de obicei folosind o etapă de reglare fină. Demonstrăm că reglarea treptată într-un proces în mai multe etape poate genera câștiguri suplimentare substanțiale și poate fi aplicată fără a modifica modelul sau obiectivul de învățare.', 'sr': 'Poznato je da poboljšavaju NLP modele prilagođavajući početni model obučen na puno, ali manje primjera salijanja domena podataka u ciljnoj domenu. Takva domena adaptacija se obično čini korištenjem jedne faze fino podešavanja. Pokazujemo da postupno ispravljanje u multikoracijskom procesu može donijeti značajne dodatne dobitke i da se može primijeniti bez modifikacije model a ili cilja učenja.', 'si': 'හොඳ ටුන්ජින් එක දන්නවා NLP මොඩල් වඩා වැඩි කරලා පටන් ගත්ත මොඩල් එකක් විශේෂ කරලා පටන් ගත්තා විශේෂ කරලා තියෙනවා ඒත්  මෙච්චර ප්\u200dරමාණයක් සාමාන්\u200dයයෙන්ම ප්\u200dරමාණයක් වෙනුවෙන් ප්\u200dරමාණය කරනවා. අපි පැහැදිලි කරනවා කියලා විශාල පැත්තක් වැඩි පැත්තක් විසින් විසින් විසින් විසින් විසින් විසින් ප්\u200dරශ්නයක් දෙන්', 'so': "Waxaa loo yaqaan in la kordhinayo modelalka NLP, in lagu beddelo model bilowga ah oo lagu barto tusaalooyin aad u badan, laakiin wax ka yar oo ka sameynaya domain-saldhig tusaale ahaan macluumaad ku jira gudaha. Such domain adaptation is typically done using one stage of fine-tuning.  Waxaynu muujinnaa in horay u dhaqdhaqaaqi karo baaritaanka kala duduwan waxay soo bixi karaan faa'iido badan, waxaana la codsan karaa in aan beddelin muusikada ama hagitaanka waxbarashada.", 'sv': 'Finjustering är känd för att förbättra NLP-modeller genom att anpassa en initial modell som utbildats på fler men mindre domänframträdande exempel till data i en måldomän. Sådan domänanpassning görs vanligtvis med hjälp av ett steg av finjustering. Vi visar att gradvis finjustering i en flerstegsprocess kan ge betydande ytterligare vinster och kan tillämpas utan att ändra modellen eller inlärningsmålet.', 'ta': 'மேலும் அதிகமாக பயிற்சிக்கப்பட்ட மாதிரியை மாற்றுவதற்காக முதலில் NLP மாதிரிகளை மேம்படுத்துவதற்கு நன்கறியப்பட்டுள்ளது ஆனால் குறைந்த க இத்தகைய டொமைன் ஒதுக்கீடு வழக்கமாக செய்யப்பட்டுள்ளது சரியான தூண்டுதலின் ஒரு நிலையை பயன்படுத்தி மாதிரி அல்லது கற்றல் பொருளை மாற்றாமல் செயல்படுத்த முடியும்.', 'ur': 'اچھی ٹونگ NLP موڈل کو اچھی طرح پہچان لیا جاتا ہے کہ اس سے پہلی موڈل کو اچھی طرح اچھی طرح دکھایا جاتا ہے مگر کم ڈومین-سلیئن مثالیں ایک موقع ڈومین میں ڈاٹ کے لئے۔ یہ ڈومین اضافہ معمولاً ایک صحنه کے مطابق کیا جاتا ہے۔ ہم دکھاتے ہیں کہ ایک بہت سی قدم پرسس میں آہستہ آہستہ تنظیم کرنا بہت اضافہ حاصل کرسکتا ہے اور مدل یا سیکھنے کا موضوع بدلنے کے بغیر لازم کر سکتا ہے.', 'uz': "Name Ushbu domen oʻzgarishlar odatda yaxshi bir daraja yordamida bajarildi. Biz bir necha qadam jarayonida darajada yaxshi tug'ilgan narsalarni ko'rsatamiz va modelni o'zgartirishni yoki o'rganishni o'zgartirish mumkin.", 'vi': 'Việc chỉnh sửa ổn định là có khả năng cải tiến mô hình Chọc NLP bằng cách sửa một mẫu đầu tiên được đào tạo trên nhiều ví dụ còn hiếm gặp hơn với dữ liệu trong miền đích. Sự thích nghi miền này thường được thực hiện bằng một giai đoạn độ cẩn thận. Chúng tôi chứng minh rằng việc chỉnh sửa dần dần theo quy trình đa bước có thể đem lại nhiều lợi nhuận lớn và có thể được áp dụng mà không thay đổi mục tiêu mô hình hay học tập.', 'da': 'Finjustering er kendt for at forbedre NLP-modeller ved at tilpasse en indledende model, der er trænet på flere rige, men mindre domænefremhævede eksempler, til data i et måldomæne. En sådan domænetilpasning foretages typisk ved hjælp af ét trin af finjustering. Vi demonstrerer, at gradvis finjustering i en multi-trins proces kan give betydelige yderligere gevinster og kan anvendes uden at ændre modellen eller læringsmålet.', 'hr': 'Poznato je poboljšavanje NLP model a prilagođenjem početnog modela obučenog na puno, ali manje primjera salijanja domena podataka u ciljnoj domenu. Takva domena prilagodba se obično čini koristeći jednu fazu dobre prilagodbe. Pokazujemo da postupno ispravljanje u multikoracijskom procesu može donijeti značajne dodatne dobitke i mogu se primjenjivati bez modificiranja model a ili cilja učenja.', 'nl': 'Fine-tuning is bekend om het verbeteren van NLP-modellen door een initieel model dat is getraind op meer overvloedige maar minder domeinrelevante voorbeelden aan te passen aan gegevens in een doeldomein. Een dergelijke domeinaanpassing wordt meestal gedaan met behulp van één fase van fine-tuning. We tonen aan dat geleidelijke finetuning in een meerstappenproces aanzienlijke verdere voordelen kan opleveren en kan worden toegepast zonder het model of leerdoel te wijzigen.', 'bg': 'Известно е, че фината настройка подобрява моделите на НЛП чрез адаптиране на първоначалния модел, обучен върху по-изобилни, но по-малко значими примери към данни в целевата област. Такава адаптация на домейна обикновено се извършва с помощта на един етап на фина настройка. Ние демонстрираме, че постепенното фино настройване в многоетапен процес може да доведе до значителни допълнителни печалби и може да се приложи без промяна на модела или учебната цел.', 'de': 'Die Feinabstimmung verbessert NLP-Modelle, indem ein anfängliches Modell, das auf zahlreicheren, aber weniger domänenrelevanten Beispielen trainiert wurde, an Daten in einer Zieldomäne angepasst wird. Eine solche Domänenanpassung erfolgt typischerweise in einer Phase der Feinabstimmung. Wir zeigen, dass eine schrittweise Feinabstimmung in einem mehrstufigen Prozess erhebliche weitere Vorteile bringen kann und ohne Modifikation des Modells oder Lernziels angewendet werden kann.', 'id': 'Penyesuaian baik dikenal untuk meningkatkan model NLP dengan menyesuaikan model awal dilatih pada contoh yang lebih banyak tetapi kurang penting domain ke data di domain target. Adaptasi domain seperti ini biasanya dilakukan menggunakan satu tahap penyesuaian. We demonstrate that gradually fine-tuning in a multi-step process can yield substantial further gains and can be applied without modifying the model or learning objective.', 'sw': 'Utafiti mzuri unafahamika kuboresha mifano ya NLP kwa kubadilisha mtindo wa mwanzo ulioelekezwa kwa matumizi mengi lakini ndogo ya mifano ya ndani kwa takwimu katika eneo la lengo. Mabadiliko ya ndani kama hayo kwa kawaida yanafanywa kwa kutumia hatua moja ya mafanikio mazuri. Tunaonyesha kwamba mafunzo ya taratibu katika mchakato wa hatua mbalimbali yanaweza kutoa mafanikio makubwa zaidi na yanaweza kutumika bila kubadilisha mifano au malengo ya kujifunza.', 'tr': 'Gowy tunlamak NLP modellerini köpräk bol şekilde bilinýär. Başlangıç modelleri köpräk taýýarlanan emma baýram domaýynda maglumatlar üçin azajyk domain-salient örnekler gelinýär. Bähili domena adaptasy adatça fine-tuning sahypasyny ulanýar. Biz birnäçe-adım prosesde azalýan şekilde täze gazanlygyny görkezip bilýäris we nusgany ýa-da öwrenmek maksadyny üýtgetmeden uygulanabilir.', 'ko': '마이크로스피커는 더 풍부하지만 두드러지지 않은 분야의 예시 훈련을 바탕으로 하는 초기 모델을 목표 분야의 데이터에 적응시키는 방법으로 NLP 모델을 개선할 수 있다는 것은 잘 알려져 있다.이런 영역은 보통 미세 조정 단계를 사용해서 완성한다.우리는 여러 단계 과정에서 점차적으로 미세하게 조정하면 실질적인 진일보한 수익을 얻을 수 있고 모델이나 학습 목표를 수정하지 않은 상황에서 응용할 수 있음을 증명했다.', 'fa': 'تنظیمات نیکو به بهترین مدل NLP شناخته می\u200cشود با توجه به یک مدل اولیه آموزش داده شده بر فراوان\u200cتر اما کمتر از مثال\u200cهای دامنه\u200cسازی کمتر برای داده\u200cها در یک دامنه هدف. این تغییرات دامنی معمولا با استفاده از یک مرحله از تغییرات خوب انجام می شود. ما نشان می دهیم که در یک فرایند چندین مرحله به تدریجی تدریجی به طور تدریجی می\u200cتواند پیروزی بیشتری را به دست آورد و می\u200cتواند بدون تغییر مدل یا هدف یادگیری کاربرد شود.', 'am': 'Fine-tuning is known to improve NLP models by adapting an initial model trained on more plentiful but less domain-salient examples to data in a target domain.  እንደዚህ የዶሜን አቀማመጥ በተለየ አንድ ደረጃ በመጠቀም ይደረጋል፡፡ በብዙ ደረጃዎች ውስጥ በጥቅልቅ ጥሩ መቀናቀል እናስታውቃለን፡፡', 'af': "Fine-tuning is bekend om NLP modele te verbeter deur 'n aanvanklike model te pas wat op meer volledige maar minder domein-salient voorbeelde a an data in 'n doel domein opgelei word. Soos domein-aanpassing is tipik gedoen deur een stadium van fin-tuning. Ons wys dat gradief fin-tuning in 'n multi-stap proses substantieel verkry kan gee en kan aanwend word sonder om die model of leer objekte te verander.", 'bn': 'প্রাথমিক মডেল প্রশিক্ষণের মাধ্যমে এনএলপি মডেল উন্নত করার জন্য পরিচিত ভালোভাবে পরিচিত, কিন্তু টার্গেট ডোমেইনে তথ্য প্রদান করা কম ডোমেইন এই ধরনের ডোমেইন অ্যাডপেটশন সাধারণত ভালো টুইনিং এর একটি স্টেজ ব্যবহার করা হয়। আমরা দেখাচ্ছি যে একটি বহুধাপ্রক্রিয়ায় ধীরে ধীরে ভালোভাবে ভালোভাবে সংশোধন করতে পারি এবং মডেল পরিবর্তন অথবা শিক্ষা না পরিবর্তনের লক্ষ্য ছ', 'az': 'NLP modell…ôrini daha √ßox geniŇü bir modell…ôrd…ô t…ôhsil edil…ôn, amma daha az domain-salient m…ôs…ôll…ôr…ô m…ôlumatlarńĪn m…ôlumatlarńĪna uyńüunlaŇüdńĪrmaq √ľ√ß√ľn nLP modell…ôrini yaxŇüńĪlaŇüdńĪrmaq m…ôqs…ôdil…ô tanńĪnńĪr. B√ľt√ľn bu domena uyńüunlaŇüdńĪrmasńĪ genellikle bir d…ôst…ôd…ôn istifad…ô edilir. Biz g√∂st…ôririk ki, √ßoxlu-adńĪm s√ľr…ôtind…ô t…ôdricl…ô t…ôdricl…ônm…ôk daha b√∂y√ľk q…ônim…ôtl…ôr yetir…ô bil…ôr v…ô modeli v…ô √∂yr…ônm…ôk m…ôqs…ôdili d…ôyiŇüm…ôd…ôn istifad…ô edil…ô bil…ôr.', 'hy': 'Գիտել է, որ բարելավում է ՆԼՊ մոդելները՝ հարմարեցնելով սկզբնական մոդելը, որը պատրաստված է ավելի բազմաթիվ, բայց ավելի քիչ տիեզերական օրինակների վրա նպատակային տիեզերքում տեղեկատվության հետ: Այսպիսի բնագավառի ադապտացիան սովորաբար կատարվում է օգտագործելով բարելավման մի փուլում: Մենք ցույց ենք տալիս, որ բազմաքային գործընթացում աստիճանաբար բարելավումը կարող է ավելի մեծ շահույթներ բերել և կարող է կիրառվել առանց փոխելու մոդելը կամ սովորելու նպատակը:', 'ca': "Es coneix que la perfeccionació millora els models NLP adaptant un model inicial entrenat en exemples més abundants però menys salients en dominis a les dades d'un domini d'objectiu. Aquesta adaptació de domini es fa normalment amb una etapa d'ajustament. Demonstrem que la perfeccionació gradual d'un procés a molts passos pot produir avanços substancials i pot ser aplicada sense modificar el model o l'objectiu d'aprenentatge.", 'sq': 'Përrregullimi i mirë është i njohur për të përmirësuar modelet NLP duke përshtatur një model fillestar të trajnuar në shembuj më të shumtë por më pak të rëndësishëm në domeni me të dhënat në një domeni objektiv. Përpërshtatja e këtij domeni tipikisht bëhet duke përdorur një fazë të rregullimit. We demonstrate that gradually fine-tuning in a multi-step process can yield substantial further gains and can be applied without modifying the model or learning objective.', 'fi': 'Hienosäädön tiedetään parantavan NLP-malleja mukauttamalla kohdealueen tietoihin alkuperäistä mallia, joka on koulutettu runsaammiin mutta vähemmän merkittäviin esimerkkeihin. Tällainen verkkotunnuksen mukauttaminen tapahtuu yleensä yhden hienosäätövaiheen avulla. Osoitamme, että vaiheittainen hienosäätö monivaiheisessa prosessissa voi tuottaa merkittäviä lisähyötyjä ja sitä voidaan soveltaa muuttamatta mallia tai oppimistavoitetta.', 'bs': 'Poznato je da poboljšavaju NLP modele prilagođavajući početni model obučen na puno, ali manje primjera salijanja domena podataka u ciljnoj domenu. Takva domena adaptacija se obično čini koristeći jednu fazu dobre prilagodbe. Pokazujemo da postupno ispravljanje u multikoracijskom procesu može donijeti značajne dodatne dobitke i da se može primjenjivati bez modifikacije model a ili cilja učenja.', 'cs': 'Jemné ladění je známo tím, že zlepšuje modely NLP přizpůsobením počátečního modelu trénovaného na mnohem více, ale méně významných příkladech domény k datům v cílové doméně. Taková adaptace domény se obvykle provádí pomocí jedné fáze jemného ladění. Ukazujeme, že postupné jemné ladění ve vícestupňovém procesu může přinést značné další zisky a lze je aplikovat bez úpravy modelu nebo cíle učení.', 'et': 'Peenhäälestamine parandab teadaolevalt uue õppekava mudeleid, kohandades sihtvaldkonna andmetega esialgset mudelit, mis on koolitatud rohkemate, kuid vähem valdkondlike näidete põhjal. Selline valdkonna kohandamine toimub tavaliselt ühe peenhäälestuse etapi abil. Näitame, et järk-järguline täpsustamine mitmeastmelises protsessis võib anda märkimisväärset edasist kasu ja seda saab rakendada ilma mudelit või õppeeesmärki muutmata.', 'jv': 'Fine So domain modification is Typically used one phase of Fine-tuning. Awak dhéwé éntuk sawetara kelas-kelas lan akeh-kelas telas nang sampek banter', 'ha': 'Ana sani mai gyare-gyare zuwa gyare-motel na NLP, da kwamfyuta wata motel na farko wanda aka yi wa shirin da shi kan zafi, kuma da ƙaranci misãlai-sali cikin guda masu tsari zuwa data cikin shirin fili. @ info: whatsthis We demonstrate that gradually fine-tuning in a multi-step process can yield substantial further gains and can be applied without modifying the model or learning objective.', 'sk': 'Znano je, da fino nastavitev izboljšuje modele NLP s prilagajanjem začetnega modela, usposobljenega za številnejše, vendar manj pomembne primere domene, podatkom v ciljni domeni. Takšna prilagoditev domen se običajno izvede z eno fazo finega uravnavanja. Dokazujemo, da lahko postopno fino uravnavanje v večstopenjskem procesu prinese znatne dodatne koristi in se lahko uporabi brez spreminjanja modela ali učnega cilja.', 'he': 'התאמה מוכרת לשפר את דוגמני NLP על ידי התאמה מודל ראשון מאומן על דוגמאות רבות יותר אבל פחות משמעותיות למידע בתחום המטרה. התאמה כזו בדרך כלל נעשה באמצעות שלב אחד של התאמה. אנו מראים כי התאימון בהדרגה בתהליך רב צעדים יכול להביא רווחים נוספים משמעותיים ויכול להשתמש בלי לשנות את המודל או אובייקטיבי הלימוד.', 'bo': 'Fine-tuning is known to improve NLP models by adapting an initial model trained on more plentiful but less domain-salient examples to data in a target domain. Such domain adaptation is typically done using one stage of fine-tuning. ང་ཚོས་རྒྱལ་ཁབ་ཀྱི་ལས་སྦྱོར་ནང་ལ་ཕར་མེད་བཟོ་བཅོས་བྱེད་པའི་རྒྱུ་དངོས་མང་ཙམ་རྐྱེན་ཐུབ་པ་དང་། མིག་སྔར་སྒྲིག་དང་སློབ་གྱི་དམིགས'}
{'en': 'Semantic Parsing of Brief and Multi-Intent Natural Language Utterances', 'ar': 'التحليل الدلالي للألفاظ اللغوية الطبيعية الموجزة ومتعددة النوايا', 'fr': "Analyse sémantique d'énoncés brefs et multi-intentions en langage naturel", 'es': 'Análisis semántico de enunciados en lenguaje natural breves y con múltiples intenciones', 'pt': 'Análise semântica de enunciados de linguagem natural breves e multi-intencionais', 'ja': '簡潔かつ多重の意図的な自然言語の単語の意味論的解析', 'hi': 'संक्षिप्त और बहु-इरादे प्राकृतिक भाषा उच्चारण के शब्दार्थ पार्सिंग', 'zh': '简多意自然语言语语义解析', 'ru': 'Семантический анализ кратких и многозначных высказываний на естественном языке', 'ga': 'Parsáil Shéimeantach ar abairtí Nádúrtha Teangacha Achomair agus Ilintinne', 'ka': 'ჟვმანრთფვჟკთ პაჱბთპაŒვ ნა ბპჲკლთგ თ მნჲდს თნრვპვნრთფნთ ნაპჲენთფნთ ენვჟკთ სრვპანუთ', 'el': 'Σημαντική ανάλυση σύντομων και πολλαπλών προθέσεων φυσικών γλωσσών', 'hu': 'Rövid és többszándékú természetes nyelvi kifejezések szemantikus értelmezése', 'it': 'Parsing semantico di brevi e multi-intenzionati linguaggi naturali', 'kk': 'Қысқа және көп- қатынасыз тіл уттерансының семантикалық талдауы', 'mk': 'Семантично анализирање на кратки и мултинамерни природни јазици', 'ms': 'Penghuraian Semantik Peralatan Bahasa Semantik Sederhana dan Berbermaksud', 'lt': 'Semantinis trumpų ir daugiakalbių gamtinių kalbų naudojimo priemonių analizavimas', 'mt': 'Analiżi Semantika ta’ Utteranzi Lingwistiċi Naturali Brief u Multi-Intenti', 'mn': 'Байгаль болон олон ухаантай байгаль хэлний хэрэглэгчдийн Semantic Parsing of Brief and Multi-Intent Natural Language Utterances', 'no': 'Name', 'ml': 'സെമാന്റിക് പാര്\u200dസിങ്ങ് ചെറുതും അധികം സ്വാഭാവിക ഭാഷയുടെയും പാര്\u200dജിങ്ങ്', 'pl': 'Parsowanie semantyczne krótkich i wieloznacznych wypowiedzi języka naturalnego', 'sr': 'Semantièno razmatranje kratkih i višenamjernih prirodnih jezika', 'si': 'සෙමැන්ටික් විශ්ලේෂණය සහ ගොඩක් තියෙන ස්වභාවික භාෂාව ප්\u200dරයෝජනය', 'ro': 'Interpretarea semantică a experiențelor scurte și multi-intenții ale limbajului natural', 'sv': 'Semantisk tolkning av kortfattade och flerintensiva naturliga språkuttryck', 'so': 'Jardiinooyinka afka asalka ah ee Semantic', 'ta': 'சிறிய மற்றும் பல- உள்ளமைப்பு இயல்பான மொழி முடிவுகளின் செமான்டிக் பாடல்', 'ur': 'کم اور بہت زیادہ مطابق طبیعی زبان کے مطابق سیمنٹی پارسینگ', 'vi': 'Bố trí truyền thuyết ngôn ngữ tự nhiên', 'uz': 'Name', 'nl': 'Semantische Parsing van korte en multiintente natuurlijke taal uitspraken', 'da': 'Semantisk fortolkning af korte og flere intentioner natursprog udtalelser', 'de': 'Semantische Parsing von kurzen und multiintenten natürlichen Sprachäußerungen', 'bg': 'Семантично анализиране на кратки и многоцелеви естествени езикови изказвания', 'id': 'Semantic Parsing of Brief and Multi-Intent Natural Language Utterances', 'ko': '간단명료하고 다의도적인 자연 언어 언어의 의미 분석', 'fa': 'تجزیه\u200cهای استفاده\u200cهای زبان طبیعی کوچک و بسیار زیادی', 'sw': 'Uimbaji wa Kisemantic wa Utumiaji wa Lugha za Kiasili', 'tr': 'Köp we köp-akyllykly tiýaly diller üçin gözlemek', 'af': 'Name', 'am': 'ምርጫዎች', 'az': 'Q캼sqa v톛 칞oxlu-칞oxlu t톛bi톛tli dil istifad톛si', 'bn': 'সেম্যান্টিক পার্সিং অফ স্বাভাবিক ভাষার বিষয়বস্তু', 'hy': 'Կարճ և բազմապատկած բնական լեզվի օգտագործման սեմանտիկ վերլուծությունը', 'sq': 'Analizimi Semantik i Mëshiruesve të Gjuhave Natyrore të Shkurta dhe Multi-Intentive', 'ca': 'Semantic Parsing of Brief and Multi-Intent Natural Language Utterances', 'cs': 'Sémantická analýza stručných a víceúmyslných přirozených jazykových vyjádření', 'bs': 'Semantičko razmatranje kratkog i višenamjernog prirodnog jezika', 'et': 'Lühikeste ja mitmeotstarbeliste looduslike keeleväljendite semantiline parsimine', 'fi': 'Lyhyiden ja monikäyttöisten luonnollisten kielisanomien semanttinen käsittely', 'hr': 'Semantičko razmatranje kratkog i višenamjernog prirodnog jezika', 'jv': 'semanti Tarjamahan kelas Bukak lan Multi-INT Language Utterance', 'sk': 'Semantna razčlenitev kratkih in večnamenskih naravnih jezikovnih izjav', 'ha': 'KCharselect unicode block name', 'he': 'מחקר סמנטי של שיפוטים טבעיים קצרים ויותרים בכוונות רבות', 'bo': 'ཆུ་ཚོད་དང་བསླབས་པའི་སྐད་རིགས་ཀྱི་ལག་ལེན་འཐབ་རྩིས་བྱུང་།'}
{'en': 'Many military communication domains involve rapidly conveying situation awareness with few words. Converting natural language utterances to logical forms in these domains is challenging, as these utterances are brief and contain multiple intents. In this paper, we present a first effort toward building a weakly-supervised semantic parser to transform brief, multi-intent natural utterances into logical forms. Our findings suggest a new projection and reduction method that iteratively performs projection from natural to canonical utterances followed by reduction of natural utterances is the most effective. We conduct extensive experiments on two military and a general-domain dataset and provide a new baseline for future research toward accurate parsing of multi-intent utterances.', 'ar': 'تتضمن العديد من مجالات الاتصال العسكري نقل الوعي بالموقف بسرعة بكلمات قليلة. يعد تحويل الكلام اللغوي الطبيعي إلى أشكال منطقية في هذه المجالات أمرًا صعبًا ، حيث أن هذه الأقوال مختصرة وتحتوي على نوايا متعددة. في هذه الورقة ، نقدم جهدًا أوليًا نحو بناء محلل دلالي خاضع للإشراف الضعيف لتحويل الأقوال الطبيعية المختصرة ومتعددة النوايا إلى أشكال منطقية. تشير النتائج التي توصلنا إليها إلى أن طريقة "الإسقاط والتقليل" الجديدة التي تؤدي بشكل متكرر الإسقاط من الأقوال الطبيعية إلى الأقوال المتعارف عليها متبوعة بتقليل الكلام الطبيعي هي الأكثر فعالية. نجري تجارب مكثفة على مجموعتين من البيانات العسكرية ومجال عام ونوفر أساسًا جديدًا للبحث المستقبلي نحو التحليل الدقيق للألفاظ متعددة النوايا.', 'fr': "De nombreux domaines de communication militaire impliquent de transmettre rapidement la conscience de la situation en quelques mots. La conversion d'énoncés en langage naturel en formes logiques dans ces domaines est un défi, car ces énoncés sont brefs et contiennent des intentions multiples. Dans cet article, nous présentons un premier effort pour créer un analyseur sémantique faiblement supervisé afin de transformer des énoncés naturels brefs et multi-intentions en formes logiques. Nos résultats suggèrent qu'une nouvelle méthode de «\xa0projection et réduction\xa0» qui effectue de manière itérative une projection à partir d'énoncés naturels vers des énoncés canoniques suivie d'une réduction des énoncés naturels est la plus efficace. Nous menons des expériences approfondies sur deux jeux de données militaires et un jeu de données du domaine général et fournissons une nouvelle base de référence pour les recherches futures en vue d'une analyse précise d'énoncés multi-intentions.", 'pt': 'Muitos domínios de comunicação militar envolvem transmitir rapidamente a consciência da situação com poucas palavras. A conversão de enunciados de linguagem natural em formas lógicas nesses domínios é um desafio, pois esses enunciados são breves e contêm várias intenções. Neste artigo, apresentamos um primeiro esforço para construir um analisador semântico fracamente supervisionado para transformar enunciados naturais breves e multi-intencionais em formas lógicas. Nossas descobertas sugerem que um novo método de “projeção e redução” que realiza iterativamente a projeção de enunciados naturais para canônicos seguido pela redução de enunciados naturais é o mais eficaz. Conduzimos experimentos extensivos em dois conjuntos de dados militares e de domínio geral e fornecemos uma nova linha de base para pesquisas futuras para análise precisa de enunciados com várias intenções.', 'es': 'Muchos dominios de comunicación militar implican transmitir rápidamente conciencia de la situación con pocas palabras. Convertir los enunciados del lenguaje natural en formas lógicas en estos dominios es un desafío, ya que estos enunciados son breves y contienen múltiples intenciones. En este artículo, presentamos un primer esfuerzo para construir un analizador semántico débilmente supervisado para transformar enunciados naturales breves y de múltiples intenciones en formas lógicas. Nuestros hallazgos sugieren que un nuevo método de «proyección y reducción» que realiza iterativamente la proyección de enunciados naturales a canónicos, seguido de la reducción de los enunciados naturales, es el más eficaz. Llevamos a cabo experimentos exhaustivos en dos conjuntos de datos militares y uno de dominio general y proporcionamos una nueva base para futuras investigaciones sobre el análisis preciso de expresiones con múltiples intenciones.', 'ja': '多くの軍事通信ドメインは、ほとんど言葉を使わずに状況認識を迅速に伝えることを伴う。これらの領域では、自然言語の発話を論理的な形式に変換することは困難です。これらの発話は簡潔であり、複数の意図を含んでいるからです。この論文では、弱い監督下にある意味構文解析器を構築して、簡潔で多意な自然な発話を論理的な形式に変換するための最初の努力を提示する。我々の知見は、自然発話から正準発話への投影を繰り返し実行し、続いて自然発話を減少させる新しい「投影と減少」方法が最も効果的であることを示唆している。私たちは、2つの軍事データセットと1つの一般ドメインデータセットについて広範な実験を行い、マルチインテント発話の正確な解析に向けた将来の研究のための新しいベースラインを提供します。', 'zh': '诸军事通信领地多单词速传情感。 凡此诸域中,转自然语言语为逻辑有挑战性,以其语甚简短,且多有意。 本文中,结构弱监语义解析器首力,以短多意自然语转为逻辑。 臣等考结果表明,投影损益,最为有效,当以迭代行自然之语,然后减自然之言。 两军一通用域数据集广实验,为未来研正解析多资新基线。', 'hi': 'कई सैन्य संचार डोमेन में कुछ शब्दों के साथ तेजी से स्थिति जागरूकता व्यक्त करना शामिल है। इन डोमेन में प्राकृतिक भाषा के उच्चारण को तार्किक रूपों में परिवर्तित करना चुनौतीपूर्ण है, क्योंकि ये कथन संक्षिप्त हैं और इसमें कई इरादे हैं। इस पेपर में, हम संक्षिप्त, बहु-इरादे वाले प्राकृतिक कथनों को तार्किक रूपों में बदलने के लिए एक कमजोर-पर्यवेक्षित शब्दार्थ पार्सर के निर्माण की दिशा में एक पहला प्रयास प्रस्तुत करते हैं। हमारे निष्कर्ष एक नई "प्रक्षेपण और कमी" विधि का सुझाव देते हैं जो पुनरावर्ती रूप से प्राकृतिक से विहित कथनों के प्रक्षेपण का प्रदर्शन करता है, जिसके बाद प्राकृतिक कथनों में कमी सबसे प्रभावी है। हम दो सैन्य और एक सामान्य डोमेन डेटासेट पर व्यापक प्रयोग करते हैं और बहु-इरादे वाले कथनों के सटीक पार्सिंग की दिशा में भविष्य के अनुसंधान के लिए एक नई आधार रेखा प्रदान करते हैं।', 'ru': 'Многие области военной связи предполагают быструю передачу информации о ситуации с помощью нескольких слов. Преобразование высказываний на естественном языке в логические формы в этих областях является сложной задачей, поскольку эти высказывания являются краткими и содержат множество намерений. В этой статье мы представляем первую попытку построить слабо контролируемый семантический парсер для преобразования кратких, многоцелевых естественных высказываний в логические формы. Наши результаты показывают, что новый метод «проекции и сокращения», который итерационно выполняет проекцию от естественных к каноническим высказываниям с последующим уменьшением естественных высказываний, является наиболее эффективным. Мы проводим обширные эксперименты над двумя военными и набором данных в общей области и предоставляем новую базовую линию для будущих исследований в направлении точного разбора многоцелевых высказываний.', 'ga': 'Baineann go leor réimsí cumarsáide míleata le feasacht ar chásanna a chur in iúl go tapa gan focail. Tá sé dúshlánach nathanna cainte nádúrtha a thiontú go foirmeacha loighciúla sna fearainn seo, mar go bhfuil na cainteanna seo gearr agus go bhfuil cuspóirí iolracha iontu. Sa pháipéar seo, cuirimid i láthair ár gcéad iarracht i dtreo parsálaí shéimeantach faoi mhaoirseacht lag a thógáil chun abairtí nádúrtha gairide ilintinne a athrú ina bhfoirmeacha loighciúla. Tugann ár dtorthaí le tuiscint gurb é modh nua “teilgean agus laghdú” a fheidhmíonn go atriallach teilgean ó chaint nádúrtha go canónacha agus ina dhiaidh sin laghdú ar chainteanna nádúrtha an ceann is éifeachtaí. Déanaimid turgnaimh fhairsing ar dhá thacar sonraí míleata agus fearainn ghinearálta agus cuirimid bonnlíne nua ar fáil le haghaidh taighde amach anseo i dtreo parsáil chruinn ar chainteanna ilintinne.', 'ka': "бғӣбғңбғІбғ“бғІ бғ’бғІВЈбғңбғ— бғҷбғІбғӣбғЎбғңбғ—бғҷбғҗбғЈбғ—ВЈбғңбғ— бғ”бғІбғӣбғ•бғңбғ— бғ—бғӣбғҗбғҗбғ\xa0 бғ‘бғһбғұбғІ бғІбғ‘бғ’бғ•бғһбғЎбғ’бғҗЕ’бғ• бғңбғҗ бғҹбғ—бғ\xa0бғЎбғҗбғЈбғ—Вјбғҗбғ\xa0бғҗ бғҹбғІ бғңбғ•бғҷбғІбғҡбғҷбғЎ бғ”бғЎбғӣбғҗ. бғ‘бғЈбғңбғ”бғ‘бғЈбғ\xa0бғҳ бғ”бғңбғҳбғЎ бғЎбғҳбғўбғ§бғ•бғҗбғ\xa0бғ”бғ‘бғҳ бғҡбғқбғ’бғҳбғҷбғЈбғ\xa0бғҳ бғӨбғқбғ\xa0бғӣбғ”бғ‘бғЁбғҳ бғ’бғҗбғ“бғҗбғ¬бғ§бғ•бғ”бғўбғҗ бғЁбғ”бғЎбғҗбғ«бғҡбғ”бғ‘бғ”бғҡбғҳбғҗ, бғ\xa0бғҗбғ“бғ’бғҗбғң бғ”бғЎ бғЎбғҳбғўбғ§бғ•бғҗбғ\xa0бғ”бғ‘бғҳ бғҷбғ\xa0бғ”бғўбғЈбғ\xa0бғҳбғҗ бғ“бғҗ бғӣбғ\xa0бғҗбғ•бғҗбғҡбғЈбғ\xa0бғҳ бғЎбғҗбғ–бғқбғ’бғҗбғ“бғқбғ”бғ‘бғ”бғ‘бғҳ бғҗбғ\xa0бғҳбғҗбғң. бғҗбғӣ бғ“бғқбғӣбғ”бғңбғўбғЁбғҳ бғ©бғ•бғ”бғң бғһбғҳбғ\xa0бғ•бғ”бғҡбғҳ бғ«бғҗбғҡбғҳбғҗбғң бғ’бғҗбғ•бғҗбғҷбғ”бғ—бғ”бғ‘бғ— бғЎбғҳбғӣбғ”бғңбғўбғҳбғҷбғЈбғ\xa0бғҳ бғһбғҗбғңбғЎбғўбғ\xa0бғҳбғҘбғўбғқбғ\xa0бғҳбғЎ бғЁбғ”бғЎбғҗбғ®бғ”бғ‘, бғ\xa0бғқбғӣбғ”бғҡбғЎбғҗбғӘ бғӣбғ\xa0бғҗбғ•бғҗбғҡбғЈбғ\xa0бғҳ, бғӣбғ\xa0бғҗбғ•бғҗбғҡбғЈбғ\xa0бғҳ бғЎбғҗбғ–бғқбғ’бғҗбғ“бғқбғ”бғ‘бғЈбғҡбғҳ бғңбғҗбғ®бғ•бғҗ бғҡбғқбғ’бғҳбғҷбғЈбғ\xa0бғҳ бғӨбғқбғ\xa0бғӣбғ”бғ‘бғЁбғҳ бғ’бғҗбғ“бғҗбғўбғҗбғңбғҘбғ\xa0бғ“бғ”бғ‘бғҗ бғ©бғ•бғ”бғңбғҳ бғЁбғ”бғЎбғҗбғ«бғҡбғ”бғ‘бғҡбғқбғ‘бғ”бғ‘бғҳ бғҗбғ®бғҗбғҡбғҳ 'бғһбғ\xa0бғқбғ”бғҘбғӘбғҳбғҗ бғ“бғҗ бғЁбғ”бғӣбғӘбғҳбғ\xa0бғ”бғ‘бғҗ' бғӣбғ”бғўбғҳ, бғ\xa0бғқбғӣбғ”бғҡбғҳбғӘ бғ—бғ”бғўбғ\xa0бғҗбғўбғҳбғЈбғ\xa0бғҗбғ“ бғһбғ\xa0бғқбғ”бғҘбғӘбғҳбғҗ бғңбғҗбғ®бғ•бғҳбғҡбғҳбғ“бғҗбғң бғҷбғҗбғңбғқбғңбғҳбғҷбғҗбғҡбғЈбғ\xa0бғҳ бғЎбғҳбғўбғ§бғ•бғ”бғ‘бғ”бғ‘бғҳбғ“бғҗбғң бғЁбғ”бғӣбғӘбғҳбғ\xa0бғ”бғ‘бғЈбғҡбғҳ бғңбғҗбғ®бғ•бғҳбғҡбғҳбғЎ бғЁбғ”бғӣбғӘбғҳбғ\xa0бғ”бғ‘бғҳбғ— бғЈбғӨбғ\xa0бғқ бғ”бғӨбғ” бғ©бғ•бғ”бғң бғ”бғҘбғЎбғһбғ”бғ\xa0бғҳбғӣбғ”бғңбғўбғ”бғ‘бғҳ бғ”бғҘбғЎбғһбғ”бғ\xa0бғҳбғӣбғ”бғңбғўбғ”бғ‘бғҳ бғқбғ\xa0бғҳ бғҗбғ\xa0бғӣбғҳбғҗбғңбғҳ бғ“бғҗ бғ’бғ”бғңбғ”бғ\xa0бғҗбғҡбғЈбғ\xa0бғҳ бғ“бғҳбғҗбғўбғҗбғўбғЈбғ\xa0бғҳ бғЎбғҗбғ–бғқбғ’бғҗбғ“бғқбғӣбғҳбғҗбғңбғҳ бғ“бғҗбғ•бғ§бғ”бғңбғ”бғ‘бғ— бғ“бғҗ бғӣбғқбғӣбғҗбғ•бғҗбғҡбғЈбғ\xa0бғҳ бғЎбғ¬бғҗбғ•бғҡбғ”бғ‘бғҳбғЎ бғҗбғ®бғҗбғҡбғҳ бғӨбғ”бғЎбғһбғ”бғ\xa0бғҳбғӣбғ”бғңбғўбғҳ бғ“бғҗбғ•бғ§бғ”бғңбғ”бғ‘бғ— бғӣбғ\xa0бғҗ", 'hu': 'Számos katonai kommunikációs terület magában foglalja a helyzet tudatosságának gyors közvetítését néhány szóval. A természetes nyelvi kifejezések logikai formákká alakítása ezeken a területeken kihívást jelent, mivel ezek a kifejezések rövidek és többféle szándékot tartalmaznak. Ebben a tanulmányban bemutatjuk az első erőfeszítést egy gyengén felügyelt szemantikai elemző felépítésére, amely rövid, többszándékú természetes kifejezéseket logikai formákká alakít. Eredményeink azt sugallják, hogy a leghatékonyabb egy új "vetítés és redukció" módszer, amely iteratív vetítést végez a természetes kifejezésekről a kanonikus kifejezésekre, majd a természetes kifejezések csökkentésére. Széleskörű kísérleteket végzünk két katonai és egy általános tartományú adatkészleten, és új alapot biztosítunk a jövőbeli kutatásokhoz a többszándékú kimondások pontos elemzéséhez.', 'el': 'Πολλοί τομείς στρατιωτικής επικοινωνίας περιλαμβάνουν ταχεία μετάδοση επίγνωσης της κατάστασης με λίγα λόγια. Η μετατροπή των εκφράσεων φυσικής γλώσσας σε λογικές μορφές σε αυτούς τους τομείς είναι πρόκληση, καθώς αυτές οι εκφράσεις είναι σύντομες και περιέχουν πολλαπλές προθέσεις. Σε αυτή την εργασία, παρουσιάζουμε μια πρώτη προσπάθεια για την οικοδόμηση ενός ασθενώς εποπτευόμενου σημασιολογικού αναλυτή για τη μετατροπή σύντομων, πολλαπλών προθέσεων φυσικών εκφράσεων σε λογικές μορφές. Τα ευρήματά μας υποδηλώνουν ότι η πιο αποτελεσματική είναι μια νέα μέθοδος "προβολής και μείωσης", η οποία επαναλαμβανόμενη εκτέλεση προβολής από φυσικές σε κανονικές εκφράσεις, ακολουθούμενη από μείωση φυσικών εκφράσεων. Διεξάγουμε εκτεταμένα πειράματα σε δύο στρατιωτικά και σε ένα σύνολο δεδομένων γενικού τομέα και παρέχουμε μια νέα βάση για μελλοντική έρευνα προς την ακριβή ανάλυση των εκφράσεων πολλαπλών προθέσεων.', 'it': 'Molti campi di comunicazione militare implicano la rapida trasmissione della consapevolezza della situazione con poche parole. Convertire le espressioni del linguaggio naturale in forme logiche in questi domini è difficile, poiché queste parole sono brevi e contengono molteplici intenti. In questo articolo, presentiamo un primo sforzo verso la costruzione di un parser semantico debolmente supervisionato per trasformare brevi, multi-intenti espressioni naturali in forme logiche. I nostri risultati suggeriscono un nuovo metodo di "proiezione e riduzione" che esegue iterativamente la proiezione dalle espressioni naturali a quelle canoniche seguita dalla riduzione delle espressioni naturali è il più efficace. Conduciamo estesi esperimenti su due set di dati militari e un set di dati di dominio generale e forniamo una nuova base di riferimento per la ricerca futura verso un\'analisi accurata delle dichiarazioni multi-intenti.', 'kk': 'Көпшілік армиялық коммуникациялық домендері бірнеше сөздермен тәжірибелеу үшін көпшілік жағдайды түсініктіреді. Бұл домендердің табиғи тілдерді логикалық пішімдерге аудару қиыншылық, себебі бұл сөздердің қысқаша болып бірнеше мақсаттары бар. Бұл қағазда бірінші бақылап, көп мақсатты табиғи сөздерді логикалық пішімдерге аудару үшін бақылап тұрған семантикалық талдаушысын құру үшін бірінші жұмыс істейміз. Біздің тапсырмамыз табиғи сөздерді қайталау және азайту әдісін табиғи сөздерден каноникалық сөздерге қайталап жасайтын жаңа "проекция және азайту" әдісін ұсынады. Біз екі армиялық және жалпы домендық деректер қорларына кеңейтілген тәжірибелер жасап, келесі зерттеулердің дұрыс талдау үшін жаңа негізгі сызық береміз.', 'mk': 'Многу воени комуникациски домени вклучуваат брзо пренесување на свеста за ситуацијата со неколку зборови. Претворањето на природните изрази на јазик во логични форми во овие домени е предизвикувачки, бидејќи овие изрази се кратки и содржат повеќе намери. Во овој весник, претставуваме први напори за изградба на слабо надгледуван семантичен анализатор за трансформирање на кратки, мултинамерни природни изрази во логични форми. Нашите откритија покажуваат дека новиот метод „проекција и намалување“ кој итеративно ја спроведува проекцијата од природни до канонички изрази, по што следи намалување на природните изрази е најефикасен. Ние спроведуваме експерименти на две воени и генерални податоци и обезбедуваме нова основа за идните истражувања кон прецизно анализирање на мултинамерни изрази.', 'lt': 'Daugelis karinių ryšių sričių reikalauja greitai informuoti apie padėtį keliais žodžiais. Gamtinių kalbų išraiškų konvertavimas į logiškas formas šiose srityse yra sudėtingas, nes šie išraiškiniai yra trumpi ir apima daugelį ketinimų. Šiame dokumente mes pirmą kartą stengiamės sukurti silpnai prižiūrimą semantinį analizatorių, kad trumpas, daugiakalbis gamtinis išraiškas paverstų logiška forma. Mūsų išvados rodo, kad naujas "prognozavimo ir mažinimo" metodas, kuris pakartotinai atlieka prognozavimą nuo natūralių iki kanoniškų išraiškų, po kurio sumažėja natūralių išraiškų, yra veiksmingiausias. Atliekame plataus masto eksperimentus su dviem kariniais ir bendrosios srities duomenų rinkiniais ir suteikiame naują pagrindą būsimiems moksliniams tyrimams, kuriais siekiama tiksliai išnagrinėti daugiakalbius žodžius.', 'ml': "കുറച്ചു വാക്കുകള്\u200d കൊണ്ട് സൈനിക കമ്മിക്കോണിങ്ങ് ഡോമിലുകള്\u200d പെട്ടന്ന് സ്ഥിതിയുടെ വിശ്വാസ സ്വാഭാവിക ഭാഷയുടെ വാക്കുകള്\u200d ഈ ഡോമെന്\u200dസിലെ ലോഗിക്കല്\u200d രൂപത്തിലേക്ക് മാറ്റുന്നത് വിലാസങ്ങളാണ്. ഈ വാക്കുകള്\u200d ചെറുതാ ഈ പത്രത്തില്\u200d, നമ്മള്\u200d ദുര്\u200dബലരായ സെമാന്\u200dറിക് പാര്\u200dസര്\u200d പണിയാനുള്ള ആദ്യ ശ്രമം കാണിക്കുന്നു. കുറച്ച് ചെറുതായി മാറ്റുന്നു. പലാതി നമ്മുടെ കണ്ടെത്തുന്നത് പുതിയ 'പ്രോജക്ഷനും കുറഞ്ഞത്' രീതിയാണ്. സ്വാഭാവികമായി സ്വാഭാവികമായി വാക്കുകളില്\u200d നിന്നും കാനോണിക്കല്\u200d വ നമ്മള്\u200d രണ്ടു സൈനിക പരീക്ഷണങ്ങളിലും ഒരു ജനറല്\u200d ഡോമെന്\u200d ഡാറ്റാസെറ്റിലും വിശാലമായ പരീക്ഷണങ്ങള്\u200d പ്രവര്\u200dത്തിക്കുന്നു. ഒരു പുതിയ പരിശ", 'mt': "Ħafna oqsma ta’ komunikazzjoni militari jinvolvu t-trażmissjoni rapida ta’ kuxjenza dwar is-sitwazzjoni bi ftit kliem. Il-konverżjoni tal-espressjonijiet lingwistiċi naturali f’forom loġiċi f’dawn l-oqsma hija sfida, peress li dawn l-espressjonijiet huma qosra u fihom diversi intenzjonijiet. F’dan id-dokument, qed nippreżentaw l-ewwel sforz lejn il-bini ta’ analizzatur semantiku b’superviżjoni dgħajfa biex jittrasformaw espressjonijiet naturali qosra, b’ħafna intenzjonijiet f’forom loġiċi. Our findings suggest a new 'projection and reduction' method that iteratively performs projection from natural to canonical utterances followed by reduction of natural utterances is the most effective.  We conduct extensive experiments on two military and a general-domain dataset and provide a new baseline for future research toward accurate parsing of multi-intent utterances.", 'ms': "Banyak domain komunikasi tentera melibatkan dengan cepat menyebarkan kesadaran situasi dengan beberapa kata. Mengubah ungkapan bahasa alami ke bentuk logik dalam domain ini adalah menantang, kerana ungkapan-ungkapan ini adalah singkat dan mengandungi tujuan berbilang. Dalam kertas ini, kami memperkenalkan usaha pertama untuk membina penghurai semantik yang mengawasi lemah untuk mengubah ungkapan semulajadi singkat, bermaksud berbilang menjadi bentuk logik. Penemuan kami menunjukkan kaedah 'projeksi dan pengurangan' baru yang secara berulang-ulang melakukan projeksi dari perkataan biasa ke perkataan canonical diikuti oleh pengurangan perkataan biasa adalah yang paling efektif. Kami melakukan eksperimen luas pada dua tentera dan set data domain umum dan menyediakan dasar baru untuk kajian masa depan menuju penghuraian tepat ucapan bermaksud berbilang.", 'mn': 'Ихэнх цэргийн харилцаа холбогдол нь нөхцөл байдлыг хэдэн үгээр хурдан мэдрэмжтэй болгодог. Байгалийн хэл илтгэлийг эдгээр хэсэгт логикийн хэлбэрүүдэд шилжүүлэх нь хэцүү, учир нь эдгээр хэлбэрүүд нь тодорхой, олон зорилго байдаг. Энэ цаасан дээр бид хамгийн бага зэрэг удирдлагатай семантик хуваагч бүтээх анхны хичээл үзүүлнэ. Бидний ололтууд нь байгалийн хэлбэрээс хамгийн үр дүнтэй гэдэг шинэ "төсөөлөл, багасгал" аргыг дахин давтагддаг. Бид хоёр цэргийн, ерөнхий хэлбэрийн өгөгдлийн сан дээр маш их туршилт хийж ирээдүйн судалгааны шинэ суурь шууд хэлбэрээр олон зорилготой хэлбэрүүдийг тодорхойлж өгдөг.', 'pl': 'Wiele dziedzin komunikacji wojskowej wymaga szybkiego przekazywania świadomości sytuacyjnej za pomocą kilku słów. Przekształcenie wypowiedzi języka naturalnego na formy logiczne w tych dziedzinach jest wyzwaniem, ponieważ wypowiedzi te są krótkie i zawierają wiele intencji. W niniejszym artykule przedstawiamy pierwszy wysiłek zmierzający do zbudowania słabo nadzorowanego parsera semantycznego w celu przekształcenia krótkich, wielozadaniowych naturalnych wypowiedzi w formy logiczne. Nasze odkrycia sugerują, że najbardziej skuteczna jest nowa metoda "projekcji i redukcji", która iteracyjnie wykonuje projekcję z wypowiedzi naturalnych do kanonicznych, a następnie redukcję wypowiedzi naturalnych. Przeprowadzamy obszerne eksperymenty na dwóch zbiorach danych wojskowych i ogólnej domeny i zapewniamy nową bazę podstawową dla przyszłych badań w kierunku dokładnego analizowania wypowiedzi wielozadaniowych.', 'no': 'Mange militære kommunikasjonskområder involverer raskt opplæring av situasjonen med få ord. Konvertering av naturspråk til logiske skjemar i desse domene er vanskeleg, sidan desse uttalene er korte og inneheld fleire uttrykk. I denne papiret viser vi første forsøk mot å bygge eit viktig semantisk analyser for å transformera kort, fleirgjennomsiktig naturlege uttrykk til logiske formar. Finningane våre foreslår eit ny «projeksjon og reduksjon» metode som gjenoppretter projeksjonen frå naturleg til kanonisk uttaler etter reduksjon av naturuttaler er det mest effektive. Vi gjer utvida eksperimenter på to militære og generelle domenedataset og gir ei ny baseline for framtidige forskning mot nøyaktig tolking av fleire uttrykk.', 'sr': 'Mnoge vojne komunikacijske domene uključuju brzo prenošenje svijesti situacije sa nekoliko reči. Pretvaranje prirodnih jezika u logičke forme u ovim domenama je izazovno, jer su te reči kratke i sadrže višestruke namjere. U ovom papiru predstavljamo prvi napor u izgradnji slabi semantičkog analizatora da transformiše kratke, višenamjerne prirodne reči u logičke forme. Naši nalazi sugeriraju novu metodu "projekcije i smanjenje", koja iterativno izvršava projekciju iz prirodne do kanoničke reči nakon smanjenja prirodnih reči, najefikasniji je. Provodimo široke eksperimente na dva vojna i generalna domena podataka i pružamo novu osnovnu liniju za buduće istraživanje prema točnom analizu višenamjernih reči.', 'ro': 'Multe domenii de comunicare militară implică transmiterea rapidă a conștientizării situației cu câteva cuvinte. Conversia expresiilor limbajului natural în forme logice în aceste domenii este o provocare, deoarece aceste expresii sunt scurte și conțin intenții multiple. În această lucrare, prezentăm un prim efort pentru construirea unui parser semantic slab supravegheat pentru a transforma expresiile naturale scurte și multi-intenții în forme logice. Descoperirile noastre sugerează o nouă metodă de "proiecție și reducere" care efectuează iterativ proiecția de la pronunțările naturale la cele canonice urmată de reducerea pronunțărilor naturale este cea mai eficientă. Realizăm experimente extinse pe două seturi de date militare și un set de date de domeniu general și oferim o nouă bază pentru cercetările viitoare spre analizarea precisă a pronunțărilor multi-intenție.', 'sv': 'Många militära kommunikationsområden innebär att snabbt förmedla situationsmedvetenhet med få ord. Att omvandla naturliga språkuttryck till logiska former inom dessa områden är utmanande, eftersom dessa uttalanden är korta och innehåller flera intentioner. I denna uppsats presenterar vi ett första försök att bygga en svagt övervakad semantisk parser för att omvandla korta, multi-intention naturliga yttranden till logiska former. Våra resultat tyder på en ny "projektion och reduktion"-metod som iterativt utför projektion från naturliga till kanoniska yttranden följt av reduktion av naturliga yttranden är den mest effektiva. Vi genomför omfattande experiment på två militära och ett allmänt domändataset och ger en ny baslinje för framtida forskning mot korrekt tolkning av multi-intent yttranden.', 'so': 'Goobo badan oo la xiriira bulshada ayaa si dhaqso ah u qabanqaabiya ogeysiinta xaaladda si yar. Erayada afka dabiiciga ah si ay noocyo caqli ah ugu bedeshaan deegaankan, sababtoo ah hadalladaas waa fudud yihiin oo waxay ku jiraan noocyo badan. Qoraalkan waxaynu ku soo bandhignaynaa isku dayactirka dhisidda qoraal cayiman oo taageer ah si aan ugu beddelno hadal fudud, hadal dabiicadda ah oo kala duduwan. Shaqooyinkayada waxaa loola jeedaa qaab cusub oo ah wax soo saara, si rasmi ah waxyaabaha laga sameeyo hadallada asalka ah ilaa hadalka kanonika, taas oo ka dib lagaga dhigo hadallada dabiicadda ah waa mid ugu caqli badan. Waxaannu sameynaa imtixaan dheeraad ah oo ku qoran labo askar ah iyo labada macluumaad ee gudaha oo guud ah, waxaana u sameynaa wax cusub oo lagu baran karo baarlamaanka si saxda ah oo hadal badan loo qoro.', 'si': "ගොඩක් හමුදානු සම්බන්ධ ක්\u200dරියාත්මක විදියට වේගයෙන් ස්ථානය අවශ්\u200dය කරනවා වචන කිහිපයකට. ස්වාභාවික භාෂාව භාෂාව ප්\u200dරතිචාරයක් මේ ඩෝමේන් වල ලෝජික ප්\u200dරතිචාරයකට වෙනස් කරන්න ප්\u200dරශ්නයක් වෙන මේ පැත්තේ, අපි පළමු උත්සහ හැදුවා දුර්වලින් පරික්ෂා කරපු සෙමැන්ටික් පරික්ෂා කරුණාකාරයෙක් වෙනස් කරගන්න බොහොම අපේ හොයාගන්න ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් තියෙනවා අලුත් 'ප්\u200dරශ්නයක් සහ අඩංගන්න' විධානයක් කියලා, ඒක ස්වභාවිකයෙන් ප්\u200dරශ අපි හමුදාව දෙකක් සහ සාමාන්\u200dය ඩොමේන් සෙට් එකේ විශාල පරීක්ෂණයක් කරනවා, අනාගතයේ පරීක්ෂණය සඳහා අනාගතයේ අවස්ථාවක් ප", 'ta': "பெரும்பாலான தொடர்பு கூட்டத்தினர் சில வார்த்தைகளுடன் விரைவாக செலுத்தும் நிலையை உணர்வு செய் இயல்பான மொழிகளின் வார்த்தைகளை நுழைய வடிவங்களில் மாற்றுவது சவாலாக இருக்கிறது, ஏனெனில் இந்த வார்த்தைகள் சிறிதாக இருக்க இந்த காகிதத்தில், நாம் ஒரு முதல் முயற்சி காண்பிக்கிறோம் ஒரு பலவீனமான கண்காணிக்கப்பட்ட பெம்மான்டிக் பரிசுருக்கும் சிறிய, பல-intent எங்கள் கண்டுபிடிப்புகள் புதிய 'திட்டமைப்பு மற்றும் குறைப்பு' முறைமையை நிர்ணயிக்கிறது இயற்கையிலிருந்து கானோனிக் சொல்லுகளை க நாம் இரண்டு படைப்புகள் மற்றும் ஒரு பொது டொமைன் தரவுத்தளத்தில் விரிவான சோதனைகளை செய்கிறோம் மற்றும் ஒரு புதிய அடித்தளத்திற்க", 'ur': "بہت سی نظامی ارتباط ڈومین کی جگہ تھوڑی کلمات کے ساتھ تیز طور پر آگاہ ہونے کی موقعیت ہے. ان دامنوں میں طبیعی زبان کی کلمات کو منطقی فرموں میں تبدیل کرنے کا مشکل ہے، کیونکہ یہ کلمات تھوڑے ہیں اور بہت سی قصد رکھتے ہیں. اس کاغذ میں ہم پہلی تلاش کررہے ہیں کہ ایک کمزور نظر رکھے ہوئے سیمنٹی پارٹر بنانے کے لئے تھوڑی، بہت سی مطلب طبیعی باتیں منطقی فرموں میں تبدیل کریں۔ ہمارے نتیجے ایک نوی 'پروژیکशन اور کم' طریقہ سے پیش کرتے ہیں جو طبیعت سے کانونیک کلمات تک پروژیکशन کرتی ہے اور اس کے بعد طبیعت کلمات کم کرتی ہے سب سے اثر ہے. ہم دو نظامی اور ایک عمومی ڈاٹ سٹ پر وسیع آزمائش کرتے ہیں اور آینده تحقیقات کے لئے ایک نئی بنسٹ لین دیتے ہیں بہت سی ارادہ کلمات کے مطابق تحقیقات کے لئے۔", 'uz': 'Ko\'pchilik aloqa davlatlari holatni bir necha so\'zlar bilan taqdim qilishga harakat qiladi. Bu davlatlarda tabiiy so\'zlarni o\'zgartirib turadi, chunki bu so\'zlar qisqa va bir necha qatlam bor. Bu qogʻozda, biz qo\'shilgan semantik parametrlarini yaratishga birinchi harakat qilamiz, bir necha qanchalik asl so\'zlarini logik shakllarini o\'zgartiradik. Bizning murakkablarimiz yangi "taqdimot va kamaytirish" usulini anglatadi. Bu tashkilotni tabiiy so\'zlardan kanonik so\'zlaridan tashkilotni bajaradi va keyin tabiiy so\'zlarni kamaytirish uchun eng ishlaydi. Biz ikkita ta\'lim va umumiy domen maʼlumotlari tarkibini bajaramiz va bir necha so\'zlarni tasdiqlash uchun yangi asboblarni bajaramiz.', 'vi': 'Nhiều lĩnh vực liên lạc quân sự bao gồm việc phát triển nhanh chóng nhận thức tình hình. Việc chuyển đổi ngôn ngữ tự nhiên thành dạng logic trong những miền này là một thử thách, vì những từ đó rất ngắn và có nhiều ý đồ. Trong tờ giấy này, chúng tôi giới thiệu nỗ lực đầu tiên nhằm xây dựng một nhà phân biệt ngữ pháp thiếu luật được giám sát để biến thành dạng logic ngắn gọn, đa ý thức. Những phát hiện của chúng tôi cho thấy một phương pháp "phân chiếu và giảm" mới, thực hiện theo một phương pháp nào đó, từ ngôn ngữ tự nhiên đến giáo khoa, sau đó là hiệu quả nhất. Chúng tôi tiến hành nhiều thí nghiệm về hai bộ dữ liệu quân sự và phổ thông và cung cấp một cơ sở mới cho nghiên cứu tương lai về việc phân tích chính xác những từ đa ý định.', 'nl': "Veel militaire communicatiedomeinen omvatten het snel overbrengen van situatiebewustzijn met weinig woorden. Het omzetten van natuurlijke taaluitingen naar logische vormen in deze domeinen is een uitdaging, omdat deze uitingen kort zijn en meerdere intenties bevatten. In dit artikel presenteren we een eerste poging om een zwak begeleide semantische parser te bouwen om korte, multi-intent natuurlijke uitingen om te zetten in logische vormen. Onze bevindingen suggereren dat een nieuwe 'projectie en reductie'-methode die iteratief projectie uitvoert van natuurlijke naar canonieke uitingen gevolgd door reductie van natuurlijke uitingen de meest effectieve is. We voeren uitgebreide experimenten uit op twee militaire en een algemeen domein dataset en bieden een nieuwe basis voor toekomstig onderzoek naar nauwkeurige parsing van multi-intent uitingen.", 'bg': 'Много военни комуникационни области включват бързо предаване на информираност за ситуацията с няколко думи. Превръщането на речи на естествения език в логически форми в тези области е предизвикателство, тъй като тези изказвания са кратки и съдържат множество намерения. В тази статия представяме първото усилие за изграждане на слабо контролиран семантичен анализатор, който да трансформира кратки, многоцелеви естествени изказвания в логически форми. Нашите констатации предполагат, че най-ефективен е нов метод за "проекция и редукция", който итеративно изпълнява проекция от естествени към канонични изказвания, последван от редукция на естествените изказвания. Ние провеждаме обширни експерименти върху два военни и общоприет набор от данни и осигуряваме нова база за бъдещи изследвания за точно анализиране на многонамерения изказвания.', 'id': "Banyak bidang komunikasi militer melibatkan dengan cepat menyampaikan kesadaran situasi dengan beberapa kata. Mengubah ungkapan bahasa alam ke bentuk logis dalam domain ini adalah tantangan, karena ungkapan ini singkat dan mengandung banyak tujuan. Dalam kertas ini, kami mempersembahkan usaha pertama untuk membangun parser semantis yang terlihat lemah untuk mengubah ucapan alam singkat, multi-tujuan menjadi bentuk logis. Our findings suggest a new 'projection and reduction' method that iteratively performs projection from natural to canonical utterances followed by reduction of natural utterances is the most effective.  Kami melakukan eksperimen ekstensif pada dua militer dan dataset domain umum dan menyediakan dasar baru untuk penelitian masa depan menuju pemeriksaan akurat dari ucapan multi-tujuan.", 'de': 'Viele militärische Kommunikationsbereiche beinhalten eine schnelle Vermittlung von Situationsbewusstsein mit wenigen Worten. Die Umwandlung natürlicher Sprachäußerungen in logische Formen in diesen Bereichen ist eine Herausforderung, da diese Äußerungen kurz sind und mehrere Absichten enthalten. In diesem Beitrag stellen wir einen ersten Versuch vor, einen schwach überwachten semantischen Parser zu entwickeln, um kurze, multiintentive natürliche Äußerungen in logische Formen zu verwandeln. Unsere Ergebnisse deuten darauf hin, dass eine neue "Projektion und Reduktion"-Methode, die iterativ Projektion von natürlichen zu kanonischen Äußerungen und anschließender Reduktion natürlicher Äußerungen durchführt, am effektivsten ist. Wir führen umfangreiche Experimente an zwei militärischen und einem allgemeinen Datensatz durch und bieten eine neue Grundlage für zukünftige Forschung zur genauen Analyse von Multi-Intent-Äußerungen.', 'da': "Mange militære kommunikationsområder involverer hurtigt at formidle situationsbevidsthed med få ord. Det er udfordrende at konvertere naturlige sprogudtalelser til logiske former på disse områder, da disse udtalelser er korte og indeholder flere hensigter. I denne artikel præsenterer vi en første indsats mod at opbygge en svagt overvåget semantisk fortolker til at omdanne korte, multi-intention naturlige udtalelser til logiske former. Vores resultater tyder på en ny 'projektion og reduktion' metode, der iterativt udfører projektion fra naturlige til kanoniske udtalelser efterfulgt af reduktion af naturlige udtalelser er den mest effektive. Vi gennemfører omfattende eksperimenter på to militære og et generelt domæne datasæt og giver en ny base for fremtidig forskning mod nøjagtig fortolkning af multi-intent udtalelser.", 'sw': "Makazi mengi ya mawasiliano ya kijeshi yanahusisha kwa haraka kukutana na uelewa wa hali ilivyo kwa maneno machache. Kubadilisha hotuba za lugha za asili katika aina za kisiasa katika maeneo haya inachangamoto, kwa sababu maneno haya ni fupi na yanajumuisha nia mbalimbali. Katika karatasi hii, tunaonyesha juhudi za kwanza za za kutengeneza mpango wa sekondari unaoendelewa dhaifu ili kubadilisha maneno ya asili yenye nia mbalimbali katika namna za kisaikolojia. Matokeo yetu yanapendekeza njia mpya ya 'uzalishaji na kupunguza' ambazo kwa ujumla unafanya mradi kutoka kwa mazungumzo ya asili hadi kikanoni kufuatia kupunguza maneno ya asili ni yenye ufanisi zaidi. Tunafanya majaribio makubwa juu ya takwimu mbili za kijeshi na jumla ya ndani na kutoa msingi mpya wa utafiti wa baadaye kwa ajili ya kuendesha mazungumzo sahihi ya maneno mengi.", 'ko': "많은 군사 통신 분야는 아주 적은 언어로 태세 감지를 신속하게 전달하는 것과 관련이 있다.이런 분야에서 자연 언어의 언어를 논리적 형식으로 바꾸는 것은 도전이다. 왜냐하면 이 말들은 짧고 다양한 의도를 포함하기 때문이다.본고에서 우리는 처음으로 약한 감독 의미 해석기를 구축하여 간단하고 다의도적인 자연 언어를 논리 형식으로 전환시키려고 시도했다.우리의 연구 결과에 따르면 새로운'투영과 환원'방법이 가장 효과적이다. 이 방법은 자연 언어에서 규범화된 언어의 투영을 반복적으로 집행한 다음에 자연 언어를 환원시킨다.우리는 두 개의 군용 데이터 집합과 한 통용 분야의 데이터 집합에서 광범위한 실험을 진행하여 미래의 다목적 언어의 정확한 해석 연구에 새로운 기선을 제공하였다.", 'af': "Baie militêre kommunikasiedemene involver vinnig toepassing bekend met paar woorde. Omskakeling van natuurlike taal uitspraak na logiese vorms in hierdie domeine is moeilik, want hierdie uitspraak is kort en bevat veelvuldige doels. In hierdie papier stel ons 'n eerste versoek voor die bou van 'n swak-ondersoekte semantiese ontwerker om kort, multi-intensie natuurlike uitdrukkings in logiese vorms te transformeer. Ons gevinde beveel 'n nuwe 'projeksie en reduksie' metode wat iteratief projeksie uitvoer van natuurlike na kanoniese uitdrukkings volg deur reduksie van natuurlike uitdrukkings is die mees effektief. Ons doen uitbreidige eksperimente op twee militêre en 'n algemene-domein-dataset en verskaf 'n nuwe basislien vir toekomstige ondersoek tot presies verwerking van multi-doelde uitdrukkings.", 'sq': "Shumë fusha të komunikimit ushtarak përfshijnë komunikimin e shpejtë të ndërgjegjësimit të situatës me pak fjalë. Konvertimi i shprehjeve të gjuhës natyrore në forme logjike në këto fusha është sfidues, pasi këto shprehje janë të shkurtra dhe përmbajnë qëllime të shumta. Në këtë letër, ne paraqesim një përpjekje të parë për të ndërtuar një analizues semantik të mbikqyrur dobësisht për të transformuar shprehje të shkurtra, me shumë qëllime natyrore në forme logjike. Zbulimet tona sugjerojnë një metodë të re 'projeksioni dhe reduktimi' që përsëritur kryen projeksionin nga shprehjet natyrore në kanonike pasuar nga reduktimi i shprehjeve natyrore është më efektive. Ne kryejmë eksperimente të gjerë në dy ushtarakë dhe një grup të dhënash të fushës së përgjithshme dhe ofrojmë një bazë të re për kërkimet e ardhshme drejt analizimit të saktë të shprehjeve me shumë qëllime.", 'am': "ብዙ የጦርነት ግንኙነት አካባቢዎች በጥቂት ቃላት የግንኙነትን ማስታወቂያ ፈጥኖ ያጋራሉ፡፡ የአፍሪካዊ ቋንቋ ቃላትን ለሌክሲካዊ ፎች በመለወጥ እና ይህች ቃላት ትንሽ እና በብዙ አሳብ ይኖራል፡፡ በዚህ ፕሮግራም፣ ደካማ የተጠበቀው የsemantic ፓርላማ ለመሥራት የመጀመሪያውን ድጋፍ እናደርጋለን፡፡ ፍጥረታችን አዲስ 'ፕሮጀክት እና አጉዳይ' ልማድ ነው፡፡ በሁለት ሰራዊ እና በጠቅላላ ድምፅ ዳታዎችን እናደርጋለን፡፡ እና ለብዙ አዋጅ ንግግር ማጋራት ለመፍጠር አዲስ የመረጃ ጥያቄ እናደርጋለን፡፡", 'hy': 'Շատ զինվորական հաղորդակցման ոլորտներ ներառում են արագ տեղեկացնել իրավիճակի գիտակցությունը մի քանի բառերով: Այս ոլորտներում բնական լեզվի արտահայտությունները տրամաբանական ձևերի փոխարինելը դժվար է, քանի որ այս արտահայտությունները կարճ են և պարունակում են բազմաթիվ նպատակներ: In this paper, we present a first effort toward building a weakly-supervised semantic parser to transform brief, multi-intent natural utterances into logical forms.  Մեր հայտնաբերությունները առաջարկում են, որ նոր "պրոեկցիա և նվազեցման" մեթոդը, որը կրկնօրինակ կատարում է պրոեկցիա բնական և կանոնական արտահայտություններից, հետևում է բնական արտահայտությունների նվազեցման ամենաարդյունավե Մենք երկու զինվորական և ընդհանուր բնագավառի տվյալների համակարգի վրա էքսպանցիոնալ փորձարկումներ ենք կատարում և ապագա հետազոտությունների համար նոր հիմք ենք տրամադրում բազմապատկած արտահայտությունների ճշգրիտ վերլուծու', 'az': 'Birçox əsgərli iletişim sahələri tələm-tələsik vəziyyəti az sözlərlə bildirir. Təbiətli dil sözlərini bu sahələrdə lojik formlara çevirmək çətindir, çünki bu sözlər qısa və çoxlu niyyətlərdən istifadə edir. Bu kağızda, az, çox niyyətli təbiətli sözləri logik formlara çevirmək üçün zəif gözləyirli semantik parçacısı inşa etmək üçün ilk çabaları göstəririk. Bizim tapındıqlarımız təbii sözlərin azaltmasından sonra təbii sözlərin ən etkilidir. Biz iki əsgərlik və general domena veri quruluğu ilə geniş təcrübələr etdik və gələcək təcrübələr üçün çoxlu niyyətli sözlərin doğru ayrılmasına tərəf yeni təcrübə çəkirik.', 'tr': "Köp askeri komunikaýat alanlarynyň ýagdaýynyň habaryny az söz bilen çalt syýahat edip biler. Doýal dil sözlerini bu sahypalarda logik biçimlere üýtgetmek kynçylykdyr, sebäbi bu sözleriň gysga bolup we birnäçe maksady bar. Bu kagyzda, biz iň azyp gözleýän semantik täzelikçisini saýlamak üçin birinji çabalary görkeýäris. Biziň tapylarymyz täze bir 'projeksion we azaltmak' yöntemi, dogrudan kanonik sözlerini azaltmakdan soňra täze bir şekilde üýtgedir. Biz iki asker we umumy domena veri setirinde döwletli deneyler çykýarys we gelejek araştyrmalar üçin birnäçe maksady sözleriniň dogry a ýlamagyna täze bir baseçin saýlaýrys.", 'bn': "অনেক সামরিক যোগাযোগের ডোমেন্ট দ্রুত পরিস্থিতি সচেতনতার সাথে জড়িত। এই ডোমেইনে প্রাকৃতিক ভাষার ভাষার কথাগুলো লজিক ফর্মে পরিবর্তন করা চ্যালেঞ্জ করছে, কারণ এই বক্তব্য সংক্ষিপ্ত এবং বেশ কিছু উদ্দ এই কাগজটিতে আমরা একটি প্রথম প্রচেষ্টা দেখাচ্ছি দুর্বল সেমান্টিক পার্সার বানানোর জন্য যাতে সংক্ষিপ্ত, বহু উদ্দেশ্যে প্রাকৃতি আমাদের আবিস্কার পরামর্শ প্রদান করা হচ্ছে নতুন 'প্রজেক্ট এবং কমানো' পদ্ধতি যে প্রাকৃতিক ভাষণ থেকে ক্যানোনিক ভাষার পর প্রকল্প প প্রকাশ করে যায় আমরা দুই সামরিক বাহিনী এবং একটি জেনারেল ডোমেইন ডাটাসেটের উপর বিস্তারিত পরীক্ষা করি এবং ভবিষ্যতের গবেষণার জন্য একটি নতুন বেসালাইন দিয়ে", 'fa': 'بسیاری از دامنهای ارتباطی نظامی به سرعت آگاهی موقعیت با چند کلمه درگیر می\u200cشود. تبدیل سخنرانی زبان طبیعی به شکل منطقی در این دامنه\u200cها سخت\u200cگیری است، زیرا این سخنرانی کوتاه و تعداد زیادی هدف دارند. در این کاغذ، ما اولین تلاش را برای ساختن یک بازیگر semantic ضعیف تحت نظر قرار می دهیم تا کلمات طبیعی کوتاه و متفاوت را به شکل منطقی تبدیل کند. نتیجه\u200cهای ما پیشنهاد می\u200cدهند یک روش جدید «پروژه\u200cسازی و کاهش» که از طبیعت به کلمات کانونیکی\u200cها دوباره پروژه\u200cسازی را انجام می\u200cدهد، پس از کاهش کلمات طبیعت، بهترین موثر است. ما آزمایش\u200cهای گسترده در دو نظامی و یک مجموعه داده\u200cهای ژنرال دامنی انجام می\u200cدهیم و یک خط بنیادی جدید برای تحقیقات آینده به طریق بررسی دقیق گفته\u200cهای چندین هدف می\u200cدهیم.', 'hr': 'Mnoge vojne komunikacijske domene uključuju brzo prenošenje svijesti situacije sa nekoliko riječi. Pretvaranje prirodnih jezičkih izraza u logičke oblike u ovim domenama je izazovno, jer su te izraze kratke i sadrže višestruke namjere. U ovom papiru predstavljamo prvi napor na izgradnju slabi semantičkog analizatora kako bi transformirali kratke, višenamjerne prirodne izraze u logičke oblike. Naši nalazi sugeriraju novu metodu "projekcije i smanjenje", koja iterativno izvršava projekciju iz prirodne do kanoničkih izraza, nakon što je smanjenje prirodnih izraza najučinkovitiji. Provodimo široke eksperimente na dva vojna i općeg domena podataka i pružamo novu osnovnu liniju za buduće istraživanje prema točnom analizu višenamjernih izraza.', 'et': 'Paljud sõjalised kommunikatsioonivaldkonnad hõlmavad olukorra teadlikkuse kiiret edastamist vähese sõnaga. Looduskeelsete väljendite muutmine loogilisteks vormideks nendes valdkondades on keeruline, sest need väljendid on lühikesed ja sisaldavad mitmeid eesmärke. Käesolevas töös tutvustame esimest jõupingutust nõrgalt juhitava semantilise parseri ehitamiseks, et muuta lühikesed, mitmekesised looduslikud väljendid loogilisteks vormideks. Meie tulemused viitavad sellele, et kõige efektiivsem on uus projektsiooni- ja reduktsioonimeetod, mis iteratiivselt teostab projektsiooni looduslikest väljenditest kanooniliste väljenditeni, millele järgneb looduslike väljendite vähendamine. Me teeme laiaulatuslikke eksperimente kahe sõjaväe- ja üldmealase andmekogumiga ning anname uue aluse tulevastele uuringutele, mille eesmärk on mitmekesiste sõnade täpne parsimine.', 'fi': "Monet sotilaallisen viestinnän alat edellyttävät tilannetietoisuuden nopeaa välittämistä muutamalla sanalla. Luonnonkielen ilmaisujen muuntaminen loogisiksi muodoiksi näillä aloilla on haastavaa, koska nämä ilmaisut ovat lyhyitä ja sisältävät useita tarkoituksia. Tässä artikkelissa esitellään ensimmäinen yritys rakentaa heikosti valvottu semanttinen jäsentäjä, joka muuntaa lyhyet, monitarkoitukselliset luonnolliset lauseet loogisiksi muodoiksi. Tuloksemme viittaavat siihen, että tehokkain on uusi 'projisointi- ja reduktiomenetelmä', joka iteratiivisesti suorittaa projisoinnin luonnollisista kanonisiin lauseisiin ja sen jälkeen pelkistää luonnollisia lauseita. Teemme laajoja kokeita kahdella sotilas- ja yleismaailmallisella aineistolla ja tarjoamme uuden lähtökohdan tulevaisuuden tutkimukselle monikaikeisten sanomien tarkalle jäsentämiselle.", 'ca': "Many military communication domains involve rapidly conveying situation awareness with few words.  Converting natural language utterances to logical forms in these domains is challenging, as these utterances are brief and contain multiple intents.  In this paper, we present a first effort toward building a weakly-supervised semantic parser to transform brief, multi-intent natural utterances into logical forms.  Els nostres descobriments suggereixen que un nou mètode de 'projecció i reducció' que fa projecció iterativa des de les expressions naturals a canòniques seguides de la reducció de les expressions naturals és el més efectiu. We conduct extensive experiments on two military and a general-domain dataset and provide a new baseline for future research toward accurate parsing of multi-intent utterances.", 'cs': 'Mnoho oblastí vojenské komunikace zahrnuje rychlé sdělování situačního povědomí pomocí několika slov. Převod výroků přirozeného jazyka na logické formy v těchto oblastech je náročný, protože tyto výroky jsou krátké a obsahují více záměrů. V tomto článku představujeme první snahu vytvořit slabě dohlížený sémantický parser pro transformaci krátkých, víceúmyslných přirozených výroků do logických forem. Naše zjištění naznačují, že nejúčinnější je nová metoda "projekce a redukce", která iterativně provádí projekci z přirozených až kanonických výroků následně redukce přirozených výroků. Provádíme rozsáhlé experimenty na dvou vojenských a obecných datových sadách a poskytujeme nový základ pro budoucí výzkum směrem k přesné analýze víceúmyslných výroků.', 'bs': 'Mnoge vojne komunikacijske domene uključuju brzo prenošenje svijesti situacije sa nekoliko riječi. Pretvaranje prirodnih jezičkih govora u logičke oblike u ovim domenama je izazovno, jer su te govore kratke i sadrže višestruke namjere. U ovom papiru predstavljamo prvi napor u izgradnji slabi semantičkog analizatora kako bi transformirali kratke, višenamjerne prirodne izraze u logičke oblike. Naši nalazi sugeriraju novu metodu "projekcije i smanjenje", koja iterativno izvodi projekciju iz prirodne do kanoničkih izraza, nakon što je smanjenje prirodnih izraza najefikasniji. Provodimo široke eksperimente na dva vojna i općeg domena podataka i pružamo novu osnovnu liniju za buduće istraživanje prema točnom analizu višenamjernih izraza.', 'jv': 'Awak dhèwèké hukum sing ngerasakno ning awak dhèwèké kesempatan kelas nang awak dhèwèké. politenessoffpolite"), and when there is a change ("assertivepoliteness Awak dhéwé éntuk kuwi, awak dhéwé éntuk sing perusahaan kanggo nggawe semanti sing nguasai luwih-luwih apik têpakan kanggo ngubah ujaran, multi-nambah layanan sing ngawe logik têpakan. Awakdhéwé éntuk dhéwé ngerasah akeh \'Proyektion lan tambah\' method sing bisa ngomong basa perusahaan anyar dadi, dadi kaonton ngéwé, dadi kapan kelangan kelangan langkung wih apik dhéwé. We kondo esteranse Esperansi nang militer iki lan generic-domain dataset dan nyengke alam sistem sing dibenaanye nggo langgar sampek kanggo nggawe geraksi yang cukup karo perasaan kelas multi-enth words.', 'ha': "Akwai masu yawa daga mazaɓa na kommuni na miliki sunã aiki da gaugãwa ga su sami fahimtar hali da take kaɗan. Yana musanya magana masu natura zuwa fomat na logiki cikin wannan garwaya yana mai zartar da, kwanan da waɗannan maganar gaske masu ƙaranci ne kuma yana ƙunsa da kashi masu yawa. Daga wannan takardan, Munã gabatar da aikin farko zuwa ka samar da wani sheantic wanda aka tsare shi mai rauni ko kuma za mu musanya magana masu nau'i, cikin fassarar masu yiwuwa da yawa. FantayinMu na gaya wata new 'shirin da ƙaranci' metode da za'a iya samar da shirin ayuka na kanoniki a kanoniki wanda ya yi ƙaranci ga magana na kawaici, shi ne mafiya amfani. Munã samun jarrabi masu ƙaranci a kan jama'a biyu na milima da data guda, kuma munã samar da wani matsayi na daban zuwa birnin karatun masu karatun karatun masu tsari zuwa ga parse-bayani masu kashfa.", 'sk': 'Številna vojaška komunikacijska področja vključujejo hitro prenašanje ozaveščenosti o situacijah z nekaj besedami. Pretvorba izjav naravnega jezika v logične oblike na teh področjih je izziv, saj so ti izjavi kratki in vsebujejo več namenov. V prispevku predstavljamo prvi prizadevanj za izgradnjo slabo nadzorovanega semantičnega razčlenjevalnika, ki bi kratke, večnamenske naravne izraze pretvoril v logične oblike. Naše ugotovitve kažejo, da je najbolj učinkovita nova metoda projekcije in redukcije, ki iterativno izvaja projekcijo od naravnih do kanoničnih izgovorov, ki ji sledi redukcija naravnih izgovorov. Izvajamo obsežne eksperimente na dveh vojaških in splošnih podatkovnih nizih ter zagotavljamo novo osnovo za prihodnje raziskave za natančno razčlenitev večnamenskih izgovorov.', 'he': 'Many military communication domains involve rapidly conveying situation awareness with few words.  להפוך מילים שפה טבעיים לצורות הגיוניים בתחומים אלה הוא מאתגר, כיוון שהמילים האלה קצרים ומכילים כוונות רבות. בעיתון הזה, אנו מציגים מאמץ ראשון כדי לבנות חוקר סמנטי מפקח חלש כדי להפוך מילים טבעיים קצרים ויותר כוונות לצורות הגיוניות. הממצאים שלנו מציעים שיטה חדשה של "פרוייקציה ומפחית" שמבצעת באופן חוזר פרוייקציה מבטאות טבעיות לקנוניות, לאחר מכן בהפחית מבטאות טבעיות, היא הכי יעילה. אנו מבצעים ניסויים רחבים על שני צבאים וקבוצת נתונים של שטח כללי ומספקים בסיס חדש עבור מחקר עתיד לכיוון בדיוק של מבטות ממטרות רבות.', 'bo': 'དམག་འཁྲུག་གི་སྦྲེལ་མཐུད་ཆེན་པོ་སྣང་ཚོར་དུས་གཏོང་བའི་གནས་སྟངས་ནི་བརྗོད་པ་ཉུང་ཡིན་པ། སྲོང་ཁྱེར་ནང་གི་མི་སྤྱིར་བཏང་བའི་སྐད་ཡིག་ལའང་ལ་ནུས་གཏོང་བའི་དབྱིབས་ཡིག་ཅིག་བསྒྱུར་ན་ལ། འོག་གི་ཤོག་བུ་འདིའི་ནང་དུ་ང་ཚོས་རྐྱེན་པའི་ལྟ་བུའི་རྩོམ་པ་སྔོན་བ་གཅིག་གི་སྐྱེན་ཡུལ་ལ་མཐོང་མེད་པའི་སྣང་བ་ཞིག་བཟོ་བྱེ ང་ཚོའི་མཐོང་སྣང་གི་ལྟ་བུའི་མཐོང་སྣང་གསརཔ་ཞིག་དཔག་རྩིས་བ་ཞིག་ཡོད། ང་ཚོས་དམག་འཁྲུག'}
