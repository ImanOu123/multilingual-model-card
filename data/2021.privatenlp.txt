{'en': 'Learning and Evaluating a Differentially Private Pre-trained Language Model', 'ar': 'تعلم وتقييم نموذج لغوي خاص تم تدريبه مسبقًا بشكل تفاضلي', 'fr': "Apprentissage et évaluation d'un modèle linguistique préformé différentiellement privé", 'es': 'Aprendizaje y evaluación de un modelo lingüístico preentrenado diferencialmente privado', 'pt': 'Aprendendo e Avaliando um Modelo de Linguagem Pré-treinado Diferencialmente Privado', 'ja': '差別化された民間の事前訓練された言語モデルの学習と評価', 'zh': '学与评估差分私有预训习语言模样', 'hi': 'सीखना और एक विभेदक रूप से निजी पूर्व-प्रशिक्षित भाषा मॉडल का मूल्यांकन करना', 'ru': 'Изучение и оценка дифференциально частной предварительно обученной языковой модели', 'ga': 'Samhail Teanga Réamh-oilte go Difreálach a Fhoghlaim agus a Mheastóireacht', 'ka': 'სწავლება და განსაზღვრება სხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვადასხვ', 'el': 'Μάθηση και αξιολόγηση ενός διαφορετικά ιδιωτικού προ-εκπαιδευμένου γλωσσικού μοντέλου', 'hu': 'Egy differenciálisan privát, előképzett nyelvi modell tanulása és értékelése', 'it': 'Imparare e valutare un modello linguistico pre-addestrato differentemente privato', 'kk': 'Өзгеше жеке алдындағы тіл үлгісін үйрену және оқу', 'lt': 'Mokymasis ir vertinimas', 'ms': 'Membelajar dan menilai Model Bahasa Terlatih Berbeza Pribadi', 'ml': 'വ്യത്യസ്ത പ്രൈവറ്റ് പരിശീലിച്ച ഭാഷ മോഡല്\u200d പഠിക്കുകയും പരിശീലിക്കുകയും ചെയ്യുന്നു', 'mk': 'Учење и евалуирање на различно приватен предобучен јазик модел', 'mt': 'It-tagħlim u l-Evalwazzjoni ta’ Mudell tal-Lingwa Mħarreġ minn Qabel Differenzjalment Privat', 'no': 'Læring og evaluering av ein forskjellig privat føretreng språk- modell', 'pl': 'Uczenie się i ocena różnicowo prywatnego modelu językowego', 'mn': 'Өөр төрлийн хувьд сургалтын өмнө сургалтын хэл загварыг сурах, үнэлэх', 'ro': 'Învățarea și evaluarea unui model lingvistic pre-instruit diferențial privat', 'so': 'Waxbarashada iyo qiimeynta nooca afka hore ee gaarka loo leeyahay', 'sr': 'Učenje i procjena različitih privatnih predobučenih jezičkih modela', 'sv': 'Att lära sig och utvärdera en differentierat privat förklädd språkmodell', 'si': 'වෙනස් විශේෂයෙන් පුරුද්ගලික භාෂාව ප්\u200dරශ්නයක් ඉගෙනගන්න සහ අවශ්\u200dයය කරන්න', 'ur': 'ایک مختلف مختلف خصوصی پیش تربین کی زبان موڈل کی تعلیم اور ارزیابی', 'ta': 'தனிப்பட்ட பயிற்சி மொழி மாதிரியை கற்றுக்கொள்வதும், மதிப்பிடுவதும்', 'uz': 'Name', 'vi': 'Học và Đánh giá ngôn ngữ riêng biệt', 'bg': 'Учене и оценка на диференциално частен предобучен езиков модел', 'da': 'Læring og evaluering af en differentielt privat forududdannet sprogmodel', 'nl': 'Leren en evalueren van een Differentieel Privé Vooropgeleid Taalmodel', 'hr': 'Učenje i procjena različitih privatnih predobučenih jezičkih modela', 'de': 'Lernen und Evaluieren eines differenziell privaten vortrainierten Sprachmodells', 'fa': 'یادگیری و ارزیابی یک مدل زبان پیش آموزش شخصی مختلف', 'sw': 'Kufundisha na Kupima Utawala tofauti tofauti', 'ko': '서로 다른 개인 예비 교육 언어 모델을 학습하고 평가하다', 'tr': 'Öňünden Aýratyn Çahsy Dili Öwrenmek we Taýýarlamak', 'sq': 'Mësimi dhe vlerësimi i një modeli gjuhësh të trajnuar në mënyrë të ndryshme private', 'id': 'Belajar dan Evaluasi Model Bahasa Terlatih Berbeda Pribadi', 'am': 'ምርጫዎች', 'bn': 'প্রাইভেট প্রশিক্ষিত ভাষা মডেল শিক্ষা এবং মূল্যায়ন করা হচ্ছে', 'az': '칐zg칲r 칐zg칲r Dil Modelini 칬yr톛nm톛k v톛 Q톛rcl톛m톛k', 'hy': 'Learning and Evaluating a Differentially Private Pre-trained Language Model', 'ca': 'Aprendre i evaluar un model de llenguatge pré-entrenat diferencialment privat', 'et': 'Erinevalt eraõppe keelemudeli õppimine ja hindamine', 'cs': 'Učení se a hodnocení diferenciálně soukromého předškoleného jazykového modelu', 'fi': 'Eri yksityisen kielimallin oppiminen ja arviointi Esikoulutetun kielimallin oppiminen', 'af': "Leer en evalueer 'n Verskillende Privaat Voorgevordergevorderde Taal Model", 'bs': 'Učenje i procjena različitih privatnih predobučenih jezičkih modela', 'jv': 'FindOK', 'he': 'ללמוד ולעריך מודל שפה מאומן מראש פרטי', 'ha': '@ action', 'sk': 'Učenje in ocenjevanje diferencialno zasebnega vnaprej usposobljenega jezikovnega modela', 'bo': 'སྒེར་གྱི་སྔོན་ལྟར་གྱི་སྐད་རིགས་མ་དབྱེ་བ་དང་དབྱེ་ཞིབ་བྱེད་པ'}
{'en': 'Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data. One method to guarantee the privacy of such individuals is to train a differentially-private model, but this usually comes at the expense of model performance. Moreover, it is hard to tell given a privacy parameter $ \\epsilon$ what was the effect on the trained representation. In this work we aim to guide future practitioners and researchers on how to improve privacy while maintaining good model performance. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of $ \\epsilon=1 $ and with only a small degradation in performance. We experiment on a dataset of clinical notes with a model trained on a target entity extraction task, and compare it to a similar model trained without differential privacy. Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.', 'pt': 'Os modelos de linguagem contextuais levaram a resultados significativamente melhores em uma infinidade de tarefas de compreensão da linguagem, especialmente quando pré-treinadas nos mesmos dados da tarefa downstream. Embora esse pré-treinamento adicional geralmente melhore o desempenho, ele pode levar ao vazamento de informações e, portanto, colocar em risco a privacidade dos indivíduos mencionados nos dados de treinamento. Um método para garantir a privacidade de tais indivíduos é treinar um modelo diferencialmente privado, mas isso geralmente ocorre às custas do desempenho do modelo. Além disso, é difícil dizer, dado um parâmetro de privacidade $\\epsilon$, qual foi o efeito na representação treinada. Neste trabalho, pretendemos orientar futuros profissionais e pesquisadores sobre como melhorar a privacidade, mantendo o bom desempenho do modelo. Demonstramos como treinar um modelo de linguagem pré-treinado diferencialmente privado (ou seja, BERT) com garantia de privacidade de $\\epsilon=1$ e com apenas uma pequena degradação no desempenho. Experimentamos um conjunto de dados de notas clínicas com um modelo treinado em uma tarefa de extração de entidade alvo e o comparamos com um modelo semelhante treinado sem privacidade diferencial. Por fim, apresentamos experimentos mostrando como interpretar a representação diferencialmente privada e compreender as informações perdidas e mantidas nesse processo.', 'es': 'Los modelos lingüísticos contextuales han dado lugar a resultados significativamente mejores en una gran cantidad de tareas de comprensión lingüística, especialmente cuando se entrenan previamente con los mismos datos que la tarea posterior. Si bien esta capacitación previa adicional generalmente mejora el rendimiento, puede provocar la filtración de información y, por lo tanto, poner en riesgo la privacidad de las personas mencionadas en los datos de capacitación. Un método para garantizar la privacidad de estas personas es entrenar un modelo diferencialmente privado, pero esto generalmente se produce a expensas del rendimiento del modelo. Además, es difícil decir, dado un parámetro de privacidad $\\ epsilon$, cuál fue el efecto en la representación entrenada. En este trabajo, nuestro objetivo es guiar a los futuros profesionales e investigadores sobre cómo mejorar la privacidad mientras se mantiene un buen rendimiento del modelo. Demostramos cómo entrenar un modelo lingüístico previamente entrenado y diferencialmente privado (por ejemplo, BERT) con una garantía de privacidad de $\\ epsilon=1$ y con solo una pequeña degradación en el rendimiento. Experimentamos con un conjunto de datos de notas clínicas con un modelo entrenado en una tarea de extracción de la entidad objetivo y lo comparamos con un modelo similar entrenado sin privacidad diferencial. Finalmente, presentamos experimentos que muestran cómo interpretar la representación diferencialmente privada y entender la información perdida y mantenida en este proceso.', 'ar': 'أدت نماذج اللغة السياقية إلى نتائج أفضل بشكل ملحوظ في عدد كبير من مهام فهم اللغة ، خاصةً عندما يتم تدريبها مسبقًا على نفس البيانات مثل المهمة النهائية. في حين أن هذا التدريب المسبق الإضافي عادةً ما يحسن الأداء ، إلا أنه يمكن أن يؤدي إلى تسرب المعلومات وبالتالي يخاطر بخصوصية الأفراد المذكورين في بيانات التدريب. تتمثل إحدى طرق ضمان خصوصية هؤلاء الأفراد في تدريب نموذج تفاضلي-خاص ، ولكن هذا يأتي عادةً على حساب أداء النموذج. علاوة على ذلك ، من الصعب معرفة تأثير معلمة الخصوصية $ \\ epsilon $ على التمثيل المدرب. نهدف في هذا العمل إلى توجيه الممارسين والباحثين المستقبليين حول كيفية تحسين الخصوصية مع الحفاظ على أداء النموذج الجيد. نوضح كيفية تدريب نموذج لغة مُدرَّب مسبقًا تفاضليًا خاصًا (على سبيل المثال ، BERT) مع ضمان خصوصية $ \\ epsilon = 1 $ وبتدهور بسيط في الأداء. نجرب على مجموعة بيانات من الملاحظات السريرية باستخدام نموذج تم تدريبه على مهمة استخراج كيان مستهدف ، ومقارنته بنموذج مشابه تم تدريبه دون خصوصية تفاضلية. أخيرًا ، نقدم تجارب توضح كيفية تفسير التمثيل التفاضلي-الخاص وفهم المعلومات المفقودة والمحافظة عليها في هذه العملية.', 'fr': "Les modèles linguistiques contextuels ont permis d'obtenir de meilleurs résultats sur une pléthore de tâches de compréhension de la langue, en particulier lorsqu'elles ont été préformées sur les mêmes données que la tâche en aval. Bien que cette pré-formation supplémentaire améliore généralement les performances, elle peut entraîner une fuite d'informations et donc mettre en danger la vie privée des personnes mentionnées dans les données de formation. Une méthode pour garantir la confidentialité de ces personnes consiste à former un modèle différencié privé, mais cela se fait généralement au détriment des performances du modèle. De plus, il est difficile de dire, compte tenu d'un paramètre de confidentialité $\xa0\\ epsilon$, quel a été l'effet sur la représentation entraînée. Dans ce travail, nous visons à guider les futurs praticiens et chercheurs sur la manière d'améliorer la confidentialité tout en maintenant de bonnes performances de modèle. Nous montrons comment entraîner un modèle de langage pré-formé différentiellement privé (c'est-à-dire BERT) avec une garantie de confidentialité de $\xa0\\ epsilon=1$ et avec une faible dégradation des performances. Nous expérimentons un ensemble de données de notes cliniques avec un modèle formé sur une tâche d'extraction d'entité cible, et nous le comparons à un modèle similaire formé sans confidentialité différentielle. Enfin, nous présentons des expériences montrant comment interpréter la représentation différentielle privée et comprendre les informations perdues et conservées au cours de ce processus.", 'ja': '文脈言語モデルは、特に下流のタスクと同じデータで事前にトレーニングを受けた場合、多くの言語理解タスクで有意に良い結果をもたらしました。 この追加の事前トレーニングは通常パフォーマンスを向上させますが、情報漏洩につながる可能性があり、したがってトレーニングデータに記載されている個人のプライバシーを危険にさらす可能性があります。 そのような個人のプライバシーを保証する方法の1つは、差別化されたプライベートモデルをトレーニングすることですが、これは通常、モデルのパフォーマンスを犠牲にします。 さらに、プライバシーパラメータ$\\ ε $が訓練された表現にどのような影響を与えたかを判断するのは困難です。 本作では、モデルのパフォーマンスを良好に維持しながらプライバシーを向上させる方法について、将来の実践者や研究者を指導することを目指しています。 私たちは、$\\ ε = 1 $のプライバシー保証とパフォーマンスのわずかな低下を伴う差別化されたプライベートな事前トレーニング言語モデル（すなわち、BERT ）をトレーニングする方法を実演します。 ターゲットエンティティ抽出タスクでトレーニングされたモデルを使用して、臨床ノートのデータセットを実験し、プライバシーの違いなしにトレーニングされた同様のモデルと比較します。 最後に、差異的プライベート表現を解釈し、このプロセスで失われ、維持される情報を理解する方法を示す実験を提示します。', 'zh': '上下文言语明白,特当预练于下流之数。 虽复预练常高性能,庶几信息漏泄,危及练数之私。 一法教异,常以牺牲为价。 此外给定私参数 $\\epsilon$ ,难以定训练。 此等事,吾道未来之从业者,与治人保持良好模样同时改善隐私。 我们演了如何训练一个差分私或预练言语模样(即BERT),其隐私保定为$ \\epsilon = 1 $,而且性能只有很小的降下。 吾等取质于临床笔记数实验之,与无差私者相校也。 最后,我们发出些实验,展示了解释微分私有表示,并知道这些事中失护的信息。', 'ru': 'Контекстуальные языковые модели привели к значительно лучшим результатам по множеству задач по пониманию языка, особенно при предварительном обучении на тех же данных, что и последующая задача. Хотя это дополнительное предварительное обучение обычно улучшает производительность, оно может привести к утечке информации и, следовательно, риску для конфиденциальности лиц, упомянутых в данных обучения. Одним из способов гарантировать конфиденциальность таких лиц является обучение дифференцированно-частной модели, но обычно это происходит за счет эффективности модели. Более того, трудно сказать, учитывая параметр конфиденциальности $\\epsilon$, каково было влияние на обученное представление. В этой работе мы стремимся помочь будущим практикам и исследователям улучшить конфиденциальность, сохраняя при этом хорошую эффективность модели. Мы демонстрируем, как обучить дифференциально-частную предварительно обученную языковую модель (т.е. BERT) с гарантией конфиденциальности $\\epsilon=1$ и с небольшим снижением производительности. Мы экспериментируем на наборе данных клинических примечаний с моделью, обученной задаче извлечения целевого объекта, и сравниваем ее с аналогичной моделью, обученной без дифференциальной конфиденциальности. Наконец, мы представляем эксперименты, показывающие, как интерпретировать дифференциально-частное представление и понимать информацию, потерянную и поддерживаемую в этом процессе.', 'hi': 'प्रासंगिक भाषा मॉडल ने भाषा समझने वाले कार्यों की अधिकता पर काफी बेहतर परिणाम दिए हैं, खासकर जब डाउनस्ट्रीम कार्य के समान डेटा पर पूर्व-प्रशिक्षित किया जाता है। हालांकि यह अतिरिक्त पूर्व-प्रशिक्षण आमतौर पर प्रदर्शन में सुधार करता है, यह जानकारी रिसाव का कारण बन सकता है और इसलिए प्रशिक्षण डेटा में उल्लिखित व्यक्तियों की गोपनीयता को जोखिम में डाल सकता है। ऐसे व्यक्तियों की गोपनीयता की गारंटी देने का एक तरीका एक विभेदक-निजी मॉडल को प्रशिक्षित करना है, लेकिन यह आमतौर पर मॉडल प्रदर्शन की कीमत पर आता है। इसके अलावा, यह एक गोपनीयता पैरामीटर को देखते हुए बताना मुश्किल है $ \\ epsilon $ प्रशिक्षित प्रतिनिधित्व पर क्या प्रभाव पड़ा। इस काम में हम भविष्य के चिकित्सकों और शोधकर्ताओं को मार्गदर्शन करने का लक्ष्य रखते हैं कि अच्छे मॉडल प्रदर्शन को बनाए रखते हुए गोपनीयता में सुधार कैसे किया जाए। हम प्रदर्शित करते हैं कि $\\epsilon = 1$ की गोपनीयता गारंटी के साथ और प्रदर्शन में केवल एक छोटी गिरावट के साथ एक विभेदक-निजी पूर्व-प्रशिक्षित भाषा मॉडल (यानी, BERT) को कैसे प्रशिक्षित किया जाए। हम एक लक्ष्य इकाई निष्कर्षण कार्य पर प्रशिक्षित एक मॉडल के साथ नैदानिक नोट्स के डेटासेट पर प्रयोग करते हैं, और इसकी तुलना विभेदक गोपनीयता के बिना प्रशिक्षित एक समान मॉडल से करते हैं। अंत में, हम प्रयोगों को दिखाते हैं कि विभेदक-निजी प्रतिनिधित्व की व्याख्या कैसे करें और इस प्रक्रिया में खोई और बनाए रखी गई जानकारी को समझें।', 'ga': 'Tá torthaí i bhfad níos fearr mar thoradh ar mhúnlaí comhthéacsúla teanga ar raidhse tascanna tuiscint teanga, go háirithe nuair a cuireadh réamhoiliúint ar na sonraí céanna leis an tasc iartheachtach. Cé go bhfeabhsaíonn an réamhoiliúint bhreise seo feidhmíocht de ghnáth, d’fhéadfadh sceitheadh faisnéise a bheith mar thoradh air agus dá bhrí sin cuireann sé príobháideacht na ndaoine aonair a luaitear sna sonraí oiliúna i mbaol. Modh amháin chun príobháideacht daoine aonair den sórt sin a ráthú ná múnla difreálach príobháideach a oiliúint, ach is gnách go dtagann sé seo ar chostas feidhmíochta an mhúnla. Ina theannta sin, tá sé deacair a rá, nuair a thugtar paraiméadar príobháideachta $\\epsilon$, cén éifeacht a bhí ar an ionadaíocht oilte. San obair seo tá sé mar aidhm againn cleachtóirí agus taighdeoirí amach anseo a threorú maidir le conas príobháideacht a fheabhsú agus dea-fheidhmíocht eiseamláireach a choinneáil. Léirímid conas múnla teanga réamh-oilte difreálach-phríobháideach a oiliúint (i.e., BERT) le ráthaíocht príobháideachta $\\epsilon=1$ agus gan ach díghrádú beag ar fheidhmíocht. Déanaimid tástáil ar thacar sonraí de nótaí cliniciúla le múnla atá oilte ar thasc asbhainte aonáin sprice, agus cuirimid i gcomparáid é le múnla comhchosúil atá oilte gan príobháideacht dhifreálach. Ar deireadh, cuirimid turgnaimh i láthair a léiríonn conas an léiriú difreálach-phríobháideach a léirmhíniú agus an fhaisnéis a chailltear agus a chothaítear sa phróiseas seo a thuiscint.', 'hu': 'A kontextusnyelvi modellek számos nyelvértési feladat tekintetében jelentősen jobb eredményt eredményeztek, különösen akkor, ha előzetesen ugyanazon adatokra készültek, mint a downstream feladat. Bár ez a kiegészítő előképzés általában javítja a teljesítményt, információszivárgáshoz vezethet, és ezáltal kockáztatja a képzési adatokban említett egyének magánéletét. Az ilyen személyek magánéletének biztosításának egyik módszere egy differenciálisan magánmodell képzése, de ez általában a modell teljesítményének rovására jár. Ezenkívül nehéz megmondani egy $\\epsilon$ adatvédelmi paraméter miatt, hogy mi volt a hatás a képzett reprezentációra. Ebben a munkában arra törekszünk, hogy iránymutatást nyújtsunk a jövőbeli szakembereknek és kutatóknak, hogyan lehet javítani a magánéletet, miközben fenntartjuk a jó modellteljesítményt. Bemutatjuk, hogyan képezhetünk egy differenciálisan privát, előre képzett nyelvi modellt (azaz BERT), amelynek adatvédelmi garanciája $\\epsilon=1$, és csak kis mértékben romlik a teljesítmény. Klinikai megjegyzésekből álló adatkészleten kísérletezünk egy célentitás extrakciós feladatra képzett modell segítségével, és összehasonlítjuk egy hasonló, különböző adatvédelem nélkül képzett modellel. Végezetül kísérleteket mutatunk be, amelyek bemutatják, hogyan lehet értelmezni a differenciálisan privát reprezentációt és megérteni az ebben a folyamatban elveszett és fenntartott információkat.', 'ka': 'კონტექსტური ენის მოდელები უფრო უკეთესი შედეგი წარმოდგენა ენის გაგრძელება, განსაკუთრებით როდესაც წარმოდგენა იგივე მონაცემებით, როგორც ჩვენი დაკავშირებული მაგრამ ეს დამატებული პროცემენტის განათლება უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო უფრო შე ერთი პროცემი, რომელიც ამ ადამიანების პირადი სიცოცხლეობის გადარჩენა, არის განსხვავებული-პირადი მოდელის გარჩენა, მაგრამ ეს საშუალოდ მოდელის გამოყენება. დამატებით, ძალიან რთულია აღწეროთ პრივიატური პარამეტრი $\\ epsilon$ რომელიც იყო შესწავლობული გამოსახულებაზე. ამ სამუშაოში ჩვენ მივიღეთ მომავალე პრაქტიკონტერები და მსწავლობელი გავაკეთოთ თუ როგორ უფრო უფრო უფრო უფრო უფრო უფრო უფრო უ ჩვენ გამოჩვენებთ, როგორ განსხვავებულად პრიგრამიურად პროგრამიურად განსწავლებული ენის მოდელს (მაგალითად BERT) პრიგრამიურად დარანტია $\\epsilon=1$ და მხოლოდ პატარა განსხვავება პროგრამიურად. ჩვენ ექსპერიმენტით კლინიკური ნოტების მონაცემების შესახებ მოდელს, რომელიც მიზეზი ინტერქქტურის ექსპექტურაციის დავაკეთებული დავაკეთებული მოდელზე, და გადამყვეტით საბოლოოდ, ჩვენ აჩვენებთ ექსპერიმენტები, როგორ განსხვავება პირადი რესპერიმენტის განსხვავება და გავიგეთ ინფორმაციას, რომლებიც გამოიყენება და გადა', 'it': "I modelli linguistici contestuali hanno portato a risultati significativamente migliori su una pletora di attività di comprensione della lingua, soprattutto se pre-addestrati sugli stessi dati del compito a valle. Anche se questo pre-training aggiuntivo migliora solitamente le prestazioni, può portare a perdite di informazioni e quindi rischiare la privacy delle persone menzionate nei dati di formazione. Un metodo per garantire la privacy di tali individui è quello di formare un modello differentemente privato, ma questo di solito viene a scapito delle prestazioni del modello. Inoltre, è difficile dire dato un parametro privacy $\\epsilon$ quale sia stato l'effetto sulla rappresentazione addestrata. In questo lavoro puntiamo a guidare i futuri professionisti e ricercatori su come migliorare la privacy mantenendo buone prestazioni del modello. Dimostriamo come addestrare un modello linguistico pre-addestrato differenziatamente privato (BERT) con una garanzia di privacy di $\\epsilon=1$ e con solo un piccolo degrado delle prestazioni. Sperimentiamo su un set di dati di note cliniche con un modello addestrato su un compito di estrazione di entità target e lo confrontiamo con un modello simile addestrato senza privacy differenziale. Infine, presentiamo esperimenti che mostrano come interpretare la rappresentazione differenzialmente privata e comprendere le informazioni perse e mantenute in questo processo.", 'el': 'Τα μοντέλα περιεκτικών γλωσσών έχουν οδηγήσει σε σημαντικά καλύτερα αποτελέσματα σε μια πληθώρα εργασιών κατανόησης γλωσσών, ειδικά όταν έχουν προετοιμαστεί για τα ίδια δεδομένα με την επόμενη εργασία. Ενώ αυτή η πρόσθετη προεκπαίδευση συνήθως βελτιώνει τις επιδόσεις, μπορεί να οδηγήσει σε διαρροή πληροφοριών και συνεπώς να διακινδυνεύσει την ιδιωτικότητα των ατόμων που αναφέρονται στα δεδομένα κατάρτισης. Μια μέθοδος για να διασφαλιστεί η ιδιωτικότητα αυτών των ατόμων είναι να εκπαιδεύσει ένα διαφορετικό-ιδιωτικό μοντέλο, αλλά αυτό συνήθως έρχεται σε βάρος της απόδοσης του μοντέλου. Επιπλέον, είναι δύσκολο να πούμε δεδομένης της παραμέτρου απορρήτου $\\epsilon$ ποια ήταν η επίδραση στην εκπαιδευμένη εκπροσώπηση. Σε αυτή την εργασία στοχεύουμε να καθοδηγήσουμε μελλοντικούς επαγγελματίες και ερευνητές σχετικά με το πώς να βελτιώσουμε την ιδιωτικότητα διατηρώντας παράλληλα την καλή απόδοση του μοντέλου. Επιδεικνύουμε πώς να εκπαιδεύσουμε ένα διαφοροποιημένο-ιδιωτικό προ-εκπαιδευμένο γλωσσικό μοντέλο (δηλαδή BERT) με εγγύηση απορρήτου $\\epsilon=1$ και μόνο με μικρή υποβάθμιση της απόδοσης. Πειραματιζόμαστε σε ένα σύνολο δεδομένων κλινικών σημειώσεων με ένα μοντέλο εκπαιδευμένο σε μια εργασία εξαγωγής οντότητας στόχου, και το συγκρίνουμε με ένα παρόμοιο μοντέλο εκπαιδευμένο χωρίς διαφορική ιδιωτικότητα. Τέλος, παρουσιάζουμε πειράματα που δείχνουν πώς να ερμηνεύσουμε τη διαφοροποιημένη-ιδιωτική αναπαράσταση και να κατανοήσουμε τις πληροφορίες που χάνονται και διατηρούνται σε αυτή τη διαδικασία.', 'kk': 'Контекстік тіл үлгілері тілді тапсырмаларды түсініп, өзгертілген кезде, төменгі тапсырманың бір мәліметінің алдында оқылған деректеріне көп жақсы нәтижелерін жасады. Бұл қосымша алдыңғы оқыту кәдімгі әрекеттерді жақсартқан болса, ол мәліметті ақпаратты түсіруге болады, сондықтан оқыту деректерінде айтылған адамдардың жеке тәуе Бұл адамдардың жеке тәуелсіздігін қамтамасыз ететін бір әдіс - әртүрлі жеке үлгісін оқыту, бірақ әдетте бұл үлгілі тәжірибесінің бағасы. Сонымен қатар, жұмыс істеу параметрін $\\ epsilon$ дегенге қандай нәтижесін көрсету қиын. Бұл жұмыста біз болашақ тәжірибелерді және зерттеушілерді жақсы үлгі істеуді қалай жақсарту үшін қалай жақсарту үшін көмектесу мақсатымыз. Біз әрқайсыз жеке тіл үлгісін қалай оқытуды көрсетедік (мысалы, BERT) $\\epsilon=1$ және тек кішкентай деградациялау үшін. Біз клиникалық жазбалардың деректер жиынына мәліметті мақсатты нысандарды тарқату тапсырмасына көмектесілген үлгілерімен тәжірибе және оларды әртүрлі жеке тәсілдік емес ұқсас Соңында, біз тәжірибелерді бұл процестің жоғалтылып қалған мәліметті түсініп, қалай түсініктерді көрсетеді.', 'ml': 'ഭാഷയിലുള്ള ഭാഷ മോഡലുകള്\u200d ഭാഷ വിവേകത്തിന്റെ ജോലികളില്\u200d ഏറ്റവും മെച്ചപ്പെട്ട ഫലങ്ങള്\u200d കൊണ്ടുവന്നിരിക്കുന്നു. പ്രത്യേകിച്ച് ഡാ ഈ കൂടുതല്\u200d പരിശീലനം സാധാരണ പ്രവര്\u200dത്തനങ്ങള്\u200d മെച്ചപ്പെടുത്തുമ്പോള്\u200d, അത് വിവരങ്ങളുടെ ലിക്കേജ് കാണിക്കും. അതുകൊണ്ട് പരിശീലനത് ഈ വ്യക്തികളുടെ സ്വകാര്യം ഉറപ്പ് വരുത്താന്\u200d ഒരു രീതി പിന്നെ, ഒരു സ്വകാര്യ പരാമീറ്റര്\u200d എപ്പിസിലോന്\u200d ഡോളര്\u200d കൊടുക്കുന്നത് എന്താണ് പരിശീലന പ്രധാനം. ഈ ജോലിയില്\u200d നമ്മള്\u200d ഭാവിയുടെ പരിശീലിക്കുന്നവരെയും ശ്രദ്ധിക്കുന്നവരെയും സ്വകാര്യം മെച്ചപ്പെടുത്തുന്നതിനെക്കു നമ്മള്\u200d വ്യത്യാസ്ത്രീയ-സ്വകാര്യത്തില്\u200d പരിശീലിക്കപ്പെട്ട ഭാഷ മോഡല്\u200d പരിശീലിപ്പിക്കുന്നത് എങ്ങനെയാണെന്ന് പ്രദര്\u200dശിപ്പിക്കുന്നു We experiment on a dataset of clinical notes with a model trained on a target entity extraction task, and compare it to a similar model trained without differential privacy.  Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.', 'lt': 'Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task.  Nors šis papildomas parengiamasis mokymas paprastai gerina veiklos rezultatus, jis gali sukelti informacijos nutekėjimą ir todėl gali kelti pavojų mokymo duomenise nurodytų asmenų privatumui. Vienas iš tokių asmenų privatumo užtikrinimo metodų yra mokyti skirtingai privatų model į, tačiau tai paprastai atsiranda modelio veiklos sąskaita. Be to, atsižvelgiant į privatumo parametrą $\\epsilon$, sunku nustatyti, koks poveikis turėjo apmokytas atstovavimas. Šiame darbe siekiame orientuoti būsimus praktikantus ir mokslininkus į tai, kaip pagerinti privatumą ir kartu išlaikyti gerus modelio rezultatus. Mes parodome, kaip mokyti skirtingai privatų i š anksto parengtą kalbos model į (t. y. BERT), užtikrinant privatumą $\\epsilon=1$ ir tik nedidelį veiklos pablogėjimą. We experiment on a dataset of clinical notes with a model trained on a target entity extraction task, and compare it to a similar model trained without differential privacy.  Galiausiai pristatome eksperimentus, rodančius, kaip aiškinti skirtingai privatų atstovavimą ir suprasti šiame procese prarastą ir išlaikytą informaciją.', 'ms': 'Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task.  While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data.  Satu kaedah untuk menjamin privasi individu seperti ini adalah untuk melatih model peribadi berbeza, tetapi ini biasanya datang pada biaya prestasi model. Moreover, it is hard to tell given a privacy parameter $\\epsilon$ what was the effect on the trained representation.  In this work we aim to guide future practitioners and researchers on how to improve privacy while maintaining good model performance.  Kami menunjukkan bagaimana untuk melatih model bahasa yang terlatih secara berbeza-peribadi (i.e., BERT) dengan jaminan privasi $\\epsilon=1$ dan hanya dengan degradation kecil dalam prestasi. Kami eksperimen pada set data nota klinik dengan model yang dilatih pada tugas ekstraksi entiti sasaran, dan membandingkannya dengan model yang sama dilatih tanpa privasi berbeza. Akhirnya, kami mempersembahkan eksperimen menunjukkan bagaimana untuk menerangkan perwakilan berbeza-peribadi dan memahami maklumat yang hilang dan disimpan dalam proses ini.', 'mt': 'Il-mudelli tal-lingwi kuntestwali wasslu għal riżultati sinifikanti a ħjar fuq għadd kbir ta’ kompiti ta’ fehim tal-lingwi, speċjalment meta mħarrġa minn qabel dwar l-istess dejta bħall-kompitu downstream. Filwaqt li dan it-taħriġ addizzjonali normalment itejjeb il-prestazzjoni, jista’ jwassal għal tnixxija ta’ informazzjoni u għalhekk jirriskja l-privatezza tal-individwi msemmija fid-dejta tat-taħriġ. Metodu wieħed li jiggarantixxi l-privatezza ta’ dawn l-individwi huwa t-taħriġ ta’ mudell differenzjalment privat, iżda dan ġeneralment jiġi bi spejjeż tal-prestazzjoni tal-mudell. Barra minn hekk, huwa diffiċli li wieħed jgħid meta wieħed iqis il-parametru tal-privatezza $\\epsilon$ x’kien l-effett fuq ir-rappreżentanza mħarrġa. F’dan ix-xogħol għandna l-għan li niggwidaw lill-prattikanti u r-riċerkaturi futuri dwar kif tittejjeb il-privatezza filwaqt li tinżamm prestazzjoni tajba tal-mudell. Aħna nippruvaw kif in ħarrġu mudell tal-lingwa mħarreġ minn qabel (jiġifieri BERT) b’garanzija tal-privatezza ta’ $\\epsilon=1$ u b’degradazzjoni żgħira biss fil-prestazzjoni. Aħna ninsperimentaw fuq sett ta’ dejta ta’ noti kliniċi b’mudell imħarreġ fuq kompitu ta’ estrazzjoni ta’ entità fil-mira, u nqabblu ma’ mudell simili mħarreġ mingħajr privatezza differenzjali. Fl-aħħar nett, qed nippreżentaw esperimenti li juru kif wieħed għandu jinterpreta r-rappreżentanza differenzjalment privata u jifhem l-informazzjoni mitlufa u miżmuma f’dan il-proċess.', 'pl': 'Kontekstowe modele językowe doprowadziły do znacznie lepszych wyników w wielu zadaniach rozumienia języka, zwłaszcza gdy są wstępnie przeszkolone na tych samych danych co zadanie kolejne. Chociaż takie dodatkowe szkolenie przedszkoleniowe zwykle poprawia wydajność, może prowadzić do wycieku informacji i w związku z tym zagraża prywatności osób wymienionych w danych treningowych. Jedną ze sposobów zagwarantowania prywatności takich osób jest trening modelu różnicowo-prywatnego, ale zwykle jest to kosztem wydajności modelu. Co więcej, trudno powiedzieć, biorąc pod uwagę parametr prywatności $\\epsilon$ jaki był wpływ na przeszkoloną reprezentację. W niniejszej pracy chcemy wskazać przyszłym praktykom i badaczom, jak poprawić prywatność przy zachowaniu dobrej wydajności modelu. Pokazujemy, jak trenować różnicowo-prywatny model językowy (tj. BERT) z gwarancją prywatności $\\epsilon=1$ i z niewielką degradacją wydajności. Eksperymentujemy na zbiorze danych notatek klinicznych z modelem przeszkolonym na zadaniu ekstrakcji jednostek docelowych i porównujemy go z podobnym modelem trenowanym bez różnicowej prywatności. Na koniec przedstawiamy eksperymenty pokazujące, jak interpretować różnicowo-prywatną reprezentację i zrozumieć informacje utracone i utrzymywane w tym procesie.', 'mk': 'Контекстуалните јазички модели доведоа до значително подобри резултати на многуте задачи за разбирање на јазикот, особено кога се предобучени на истите податоци како и понатамошната задача. И покрај тоа што оваа дополнителна предобука обично ја подобрува перформансата, таа може да доведе до протекување на информации и со тоа ризикува приватноста на поединците споменати во податоците за обука. Еден метод за гарантирање на приватноста на ваквите поединци е обуката на различно-приватен модел, но ова обично доаѓа на трошок на моделната изведба. Покрај тоа, тешко е да се каже со оглед на параметарот за приватност $\\epsilon$ кој беше ефектот на обученото претставување. Во оваа работа ние имаме за цел да ги водиме идните практики и истражувачи како да ја подобриме приватноста и да одржиме добра резултат на моделот. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of $\\epsilon=1$ and with only a small degradation in performance.  We experiment on a dataset of clinical notes with a model trained on a target entity extraction task, and compare it to a similar model trained without differential privacy.  Конечно, претставуваме експерименти кои покажуваат како да се интерпретира диференцијално-приватното претставување и да се разбере информацијата изгубена и одржана во овој процес.', 'ro': 'Modelele lingvistice contextuale au condus la rezultate semnificativ mai bune în ceea ce privește o multitudine de sarcini de înțelegere a limbilor străine, mai ales atunci când sunt pre-instruite pe aceleași date ca și sarcina din aval. Deși această pregătire prealabilă suplimentară îmbunătățește, de obicei, performanța, poate duce la scurgeri de informații și, prin urmare, riscă confidențialitatea persoanelor menționate în datele de pregătire. O metodă de a garanta confidențialitatea acestor persoane este de a instrui un model diferențial-privat, dar acest lucru vine de obicei în detrimentul performanței modelului. Mai mult decât atât, este greu de spus dat fiind un parametru de confidențialitate $\\epsilon$ care a fost efectul asupra reprezentării instruite. În această lucrare, ne propunem să ghidăm viitorii practicieni și cercetători cu privire la modul în care să îmbunătățim confidențialitatea, menținând în același timp performanța bună a modelului. Vă demonstrăm cum să instruiți un model de limbă pre-instruit diferențial privat (de exemplu, BERT) cu o garanție de confidențialitate de $\\epsilon=1$ și cu doar o mică degradare a performanței. Experimentăm pe un set de date de note clinice cu un model instruit pe o sarcină de extragere a entității țintă și îl comparăm cu un model similar instruit fără confidențialitate diferențială. În cele din urmă, prezentăm experimente care arată cum să interpreteze reprezentarea diferențial-privată și să înțeleagă informațiile pierdute și menținute în acest proces.', 'no': 'Kontekst språk-modeller har ført til signifikante bedre resultat på ein plethora av språk-forståking av oppgåver, spesielt når før-treng på samme data som nedtrekkoppgåva. Selv om denne ekstra føreøvinga oftast forbetrar utviklinga, kan det føre til å løysa informasjon og derfor riskerer privateten av personer som er oppgjeve i opplæringsdata. Ein måte å garanti privatet på slike individuane er å trenja eit ulike privat modell, men dette vanlegvis kjem på utvidinga av modellen. I tillegg er det vanskeleg å fortelle eit privatsparameter $\\epsilon$ kva var effekten på den trengte representasjonen. I dette arbeidet må vi hjelpa framtidige praktiseringar og forskere om korleis å forbetra privatet medan å beholda godt modell. Vi viser korleis å trenja eit forskjellig privat språk- modell (t.d. BERT) med eit privat garanti for $\\epsilon=1$ og berre med e in liten degradasjon i utviklinga. Vi eksperimenterer på eit dataset med kliniske notat med eit modell trent på eit utpakking av målsettingsoppgåve, og sammenligner det med eit liknande modell trent utan forskjellig privat. I slutt presenterer vi eksperimenter som viser korleis å tolka den ulike private representasjonen og forstå informasjonen tapte og vedlikehalde i denne prosessen.', 'mn': 'Сүүлийн үеийн хэл загварууд хэл ойлгох үйл ажиллагааны тухай илүү сайн үр дүн гаргасан, ялангуяа доорх үйл ажиллагаатай адилхан мэдээллийг суралцах үед. Энэ нэмэлт өмнөх сургалтын дасгал хөгжлийн үйл ажиллагааг сайжруулдаг ч, энэ нь мэдээллийг суулгах боломжтой. Иймээс сургалтын мэдээллээр хэлсэн хүмүүсийн хувийн амьдралыг эрсдэлтэй Нэг арга нь ийм хүмүүсийн хувийн амьдралыг баталгаалах нь өөр өөр хувийн загварын загварын төлөвлөгөө юм. Гэхдээ энэ нь ихэвчлэн загварын үйлдвэрлэлийн зардал дээр ирдэг. Үүнээс гадна хувийн амьдралын параметр $\\epsilon$ өгсөн нь сургалтын үзүүлэлтийн нөлөө юу байсан бэ гэдгийг хэлэх нь хэцүү. Энэ ажил дээр бид ирээдүйн мэргэжилтнүүд болон судлаачид хэрхэн хувийн загварын үйл ажиллагааг сайжруулах талаар хэрхэн сайжруулах талаар удирдах зорилго байна. Бид өөр өөр өөр хувийн хэл дээр сургалтын загварыг хэрхэн суралцах вэ гэдгийг харуулж байна (т.е. BERT) $\\epsilon=1$ болон үйлдвэрлэлд жижиг хэлбэртэй баталгаатай. Бид клиникийн нотуудын өгөгдлийн хэлбэр дээр зориулагдсан загварыг авах ажил дээр сургалтын загвартай туршиж, өөр өөр өөр загваргүй сургалтын төстэй загвартай харьцуулж байна. Эцэст нь, бид өөр өөр хувийн үзүүлэлтийг хэрхэн илэрхийлж, алдсан, хадгалагдсан мэдээллийг ойлгох туршилтуудыг харуулж байна.', 'so': 'Tusaalada luuqadda ku qoran waxay sababtay resulto aad u wanaagsan oo ku saabsan shaqada waxyaabaha garashada luuqada, khusuusan marka lagu baranayo iskuul ka horeysan isku mid ah macluumaadka shaqada hoose. While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data.  Qofka shakhsiyadiisa ah waa in loo tababariyo tusaale gaar ah oo kala duduwan, laakiin sida caadiga ah waxaa ku imaan kharashka sameynta tusaale ahaan. Sidoo kale waa adag tahay in loo sheego lambarka gaarka loo leeyahay $ epsilon, waxa saamayn ku leh wakiilka tababarida. Shaqadaas waxaynu ku talo galaynaa in aan ku hogaano shaqo-bixiyayaasha mustaqbalka ah iyo cilmi-baaritaanka ku saabsan sida loo kordhiyo gaar ahaanshaha, marka loo haysto sameynta sameynta muusikada wanaagsan. Waxaannu muujinnaa sida aad u tababarido qaab kala duwan oo gaar loo leeyahay, tusaale ahaan BERT, kaas oo ah garashada gaarka loo leeyahay $ epsilon=1$ iyo in lagu sameynayo ceeb yar. Waxaan ku tijaabinaynaa qoraal macluumaad dhakhaatiirta ah oo ku qoran qaab lagu baray shaqo ka soo bixinta jidhka goalka, waxaynu isbarbareynaa tusaale la mid ah oo lagu baray oo aan gaar u lahayn. Ugu dambaysta waxaan soo bandhignaa imtixaamo aan tusnaynaa sida loo turjumo wakiilka gaarka loo leeyahay iyo sidoo kale ayaynu garanaynaa macluumaadka ka lumaya iyo la sii haystay markan.', 'sv': 'Kontextuella språkmodeller har lett till betydligt bättre resultat på en uppsjö av språkförståelse uppgifter, särskilt när de är förberedda på samma data som nedströmsuppgiften. Även om denna extra fortbildning vanligtvis förbättrar prestationen kan den leda till informationsläckage och därmed riskerar privatlivet för de personer som nämns i utbildningsuppgifterna. En metod för att garantera privatlivet för sådana individer är att träna en differentialt privat modell, men detta sker vanligtvis på bekostnad av modellprestanda. Dessutom är det svårt att säga med en sekretessparameter $\\epsilon$ vad som var effekten på den utbildade representationen. I detta arbete syftar vi till att vägleda framtida praktiker och forskare om hur man kan förbättra integriteten samtidigt som man upprätthåller goda modellprestanda. Vi visar hur man tränar en differentialt privat förklädd språkmodell (dvs BERT) med en sekretessgaranti på $\\epsilon=1$ och med endast en liten försämring av prestanda. Vi experimenterar på en dataset med kliniska anteckningar med en modell utbildad på en målentitetsextraktionsuppgift, och jämför den med en liknande modell utbildad utan differentierad sekretess. Slutligen presenterar vi experiment som visar hur man tolkar den differentialt privata representationen och förstår den information som förlorats och bevarats i denna process.', 'ta': 'உள்ளடக்கமான மொழி மாதிரிகள் முழு மொழி புரிந்து கொள்ளும் பணிகளின் மேல் மிகவும் நல்ல முடிவு இந்த கூடுதல் முன் பயிற்சி வழக்கமாக செயல்பாட்டை மேன்மைப்படுத்தும் போது, அது தகவல் ஒட்டுதலை வழிகாட்டும் அதனால் பயிற்சி தகவலி இத்தகைய தனிப்பட்ட மாதிரியை பயிற்சி செய்ய ஒரு முறையாக, ஆனால் இது வழக்கமாக மாதிரி செயல்பாட்டின் செலவில் வருகிறது. மற்றும், ஒரு தனிப்பட்ட அளபுருவு $epsilon டாலர் கொடுக்கப்பட்டுள்ளது என்று சொல்ல மிகவும் கடினம். இந்த வேலையில் நாம் எதிர்கால பயிற்சியாளர்களையும் ஆராய்ச்சியாளர்களையும் செலுத்தும் போது நல்ல மாதிரி செயல்பாட நாம் எப்படி வேறுபட்ட தனிப்பட்ட பயிற்சி முன்பயிற்சி மொழி மாதிரியை பயிற்சி செய்ய வேண்டும் என்பதை காட்டுகிறோம் $epsilon=1$ மற்றும் செயல்பாட்டில் சி நாம் ஒரு சில குறிப்புகளின் தகவல் அமைப்பில் சோதனைப்படுத்தப்பட்டுள்ளோம் இலக்கு பொருள் பிரிப்பு பணியில் பயிற்சிக்கப்பட்ட மாதிரி ம இறுதியில், நாம் இந்த செயலில் இழந்து விட்டது மற்றும் பாதுகாக்கப்பட்ட தகவலை புரிந்து கொள்ளும் வித்தியாசமான தனிப்பட்', 'si': 'සම්බන්ධ භාෂාව මොඩේල් එක්ක වඩා හොඳ ප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප්\u200dරතිප මේ විශේෂ ප්\u200dරීක්ෂණය සාමාන්\u200dයයෙන්ම ප්\u200dරීක්ෂණය වැඩ කරන්න පුළුවන්, ඒක තොරතුරු ප්\u200dරීක්ෂණය කරලා තියෙන්න පුළු ඒ වගේම ප්\u200dරතිකාරයෝ ගැන පුද්ගලිකතාවක් ගැන ආරක්ෂා කරන්න එක විදියට වෙනස් විදිහට පුද්ගලික විදිහට පුද්ගලික ව ඉතින්, පුද්ගලික ප්\u200dරමාණයක් දෙන්න $\\epsilon$ කියලා කියන්න අමාරුයි. මේ වැඩේ අපි අදහස් කරනවා අනාගතයේ ප්\u200dරේක්ෂකයෝ සහ පරීක්ෂකයෝ ගැන කොහොමද පෞද්ගලිකවත් වැඩ කරන්නේ හොඳ මොඩ අපි ප්\u200dරදර්ශනය කරනවා කොහොමද වෙනස් විදිහට පුද්ගලික විදිහට පුද්ගලික විදිහට පුද්ගලික භාෂා මොඩේලයක් (ඉතින්, BERT) එක්ක $\\epsilon=1$ ගැ අපි පරීක්ෂණය කරන්නේ ප්\u200dරශ්නයක් තියෙන ප්\u200dරශ්නයක් තියෙන්නේ ඉලක්ෂාත්මක ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් තියෙන දත්ත සූද්ධ අන්තිමට, අපි පරීක්ෂණාවල් පෙන්වන්න පුළුවන් වෙනස් පුද්ගලික ප්\u200dරතිචාරයක් කොහොමද කියලා පෙන්වන්නේ, ඒ', 'ur': 'کنٹکسٹیول زبان موڈل نے زبان سمجھنے کے کاموں پر بہترین نتیجے بنائے ہیں، مخصوصا جب ڈانٹروم کے کام کے مطابق پہلے دکھائے جاتے ہیں۔ اگرچہ یہ اضافہ پیش آموزش کی تعلیم معمولاً عملکرد کو اضافہ کرتی ہے، اس کے ذریعہ یہ معلومات کی اضافہ کرتی ہے اور اس کے نتیجہ میں تدریس ڈیٹ میں ذکر ہوئے شخصوں کی خصوصی کا خطر ڈالت ایک طریقہ ایسے شخصوں کی خصوصی کا امن کرنا ہے کہ ایک مختلف طریقے سے خصوصی موڈل کی آموزش کرنا ہے، لیکن یہ معمولاً موڈل کی عملکرد کے خرچ پر آتا ہے. اور اس کے علاوہ، ایک خصوصی پارامیٹ $\\epsilon$ کو بتانے کا مشکل ہے کہ تعلیم کی تعلیم پر کیا اثر تھا. اس کام میں ہم مطابق مستقبل کارگروں اور تحقیقات کرنے والوں کو راہ دکھانے کا ارادہ رکھتے ہیں کہ کس طرح خصوصی کا تدبیر کریں جب وہ اچھی مدل کی عملکرد حفاظت کریں۔ ہم نشان دیتے ہیں کہ $\\epsilon=1$ کی خصوصی طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طرح طر ہم کلینیکی نوٹوں کے ایک ڈیٹ سٹ پر ایک موڈل کے ساتھ تدریس کیا گیا ہے ایک موڈل کے ذریعہ ایک ٹیٹ ایٹیٹ کے اخراج کام پر، اور اسے ایک ایسی موڈل کے مطابق تدریس کیا گیا ہے جو مختلف خصوصی کے ب آخر میں ہم تجربے پیش کرتے ہیں کہ کس طرح مختلف طریقے سے خصوصی نمونات کی تعبیر کریں اور اس طریقے میں خسارہ اور حفاظت کی معلومات کو سمجھیں۔', 'sr': 'Kontekstualni jezički modeli su doveli do značajno boljih rezultata na razumevanje jezičkih zadataka, posebno kada su predobučeni na istim podacima kao i zadatak koji se nalazi u potpunosti. Iako ova dodatna predobuka obično poboljšava učinkovitost, može dovesti do procurenja informacija i stoga rizikuje privatnost pojedinaca spominjenih u podacima o obuci. Jedan metod da garantuje privatnost takvih pojedinaca je trenirati diferencijalni privatni model, ali to obično dolazi na troškove model a izvršnosti. Osim toga, teško je reći s obzirom na parameter privatnosti $ epsilon$ koji je bio uticaj na obučeno predstavljanje. U ovom poslu ciljamo voditi buduće praktičnike i istraživače kako poboljšati privatnost dok održavamo dobre modele. Pokazujemo kako da treniramo diferencijalno-privatni predobučeni jezički model (tj. BERT) sa garancijom privatnosti od $\\epsilon=1$ i sa samo malim degradacijom u izvedbi. Eksperimentiramo na setu podataka kliničkih nota sa modelom obučenim na zadatku izvlačenju ciljnih entiteta i uspoređujemo je sa sličnim modelom obučenim bez diferencijalne privatnosti. Konačno predstavljamo eksperimente kako da interpretiramo diferencijalno-privatno predstavljanje i razumemo izgubljene i održane informacije u ovom procesu.', 'vi': 'Kiểu ngôn ngữ tương ứng đã dẫn đến kết quả tốt hơn nhiều so với một loạt các công việc hiểu biết ngôn ngữ, đặc biệt khi được huấn luyện trước với những dữ liệu tương tự với các công việc xuôi dòng. Trong khi sự bổ sung tiền đào tạo này thường cải thiện hiệu suất, nó có thể dẫn tới rò rỉ thông tin và do đó nguy hiểm tính riêng tư của cá nhân được nhắc đến trong dữ liệu huấn luyện. Một phương pháp bảo đảm sự riêng tư cho những cá nhân đó là đào tạo một mô hình riêng biệt, nhưng thường là làm tổn hại đến khả năng của mô hình. Hơn nữa, dựa vào tham số riêng tư thì rất khó xác định được hiệu quả của sự đóng góp được huấn luyện. Trong công việc này chúng tôi hướng dẫn các chuyên gia và nghiên cứu tương lai về cách cải thiện sự riêng tư trong khi duy trì các mô hình tốt. Chúng tôi chỉ cách huấn luyện một mô hình ngôn ngữ được đào tạo riêng tư (gọi là BERT) với một bảo đảm an ninh riêng tư cho tên là tham nhũng\\\\ epsilon=1., và chỉ với một sự giảm sút giảm trình độ nhỏ. Chúng tôi thí nghiệm trên một tập tin ghi chú lâm sàng với một mô hình được đào tạo về một nhiệm vụ thực thể đích, và so sánh nó với một mô hình tương tự được huấn luyện mà không có riêng tư. Cuối cùng, chúng tôi có thí nghiệm cho thấy cách phân tích sự phân biệt chủng tộc riêng tư và hiểu được thông tin bị mất và duy trì trong quá trình này.', 'uz': "@ info: whatsthis Ushbu qoʻshimcha taʼminlovchi vaqtda oddiy vazifani bajarishi mumkin, bu maʼlumot yozib olish mumkin va shunday qilib, taʼminlovchi maʼlumotdagi odamlarning shaxsiyatlarini xavfsiz qiladi. Bu odamlarning shaxsiyatlarini tasdiqlash uchun bir usul, boshqa shaxsiy modelni o'rganish mumkin, ammo bu odatda model bajarishning qiymatida keladi. Ko'rsatilgan, epsilon dollarning shaxsiy parametrlarini aytish juda qiyin edi. Bu vazifanda biz kelajakdagi ishlatuvchilarni va taʼminlovchilarni yaxshi model bajarishni davom etishda shaxsiyatlarni yaxshi ko'ramiz. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of $\\epsilon=1$ and with only a small degradation in performance.  Biz ma'lumotlar taʼminlovchi taʼminotlar soni taʼminlovchi modeli bilan o'rganamiz va uni o'xshash shaklga o'rganish modeliga kamaytirish mumkin. Oxirgi, biz o'zgarishni o'rganishni o'rganamiz va bu jarayonda qanday o'zgarishni o'rganishni ko'rsatamiz va qanday o'zgarishni o'rganamiz.", 'da': 'Kontekstuelle sprogmodeller har ført til betydeligt bedre resultater på en lang række sprogforståelsesopgaver, især når de er prætrænet på de samme data som downstream-opgaven. Selv om denne yderligere forudtræning normalt forbedrer ydeevnen, kan det føre til informationslækage og derfor risikerer privatlivets fred for personer, der er nævnt i træningsdataene. En metode til at garantere privatlivets fred for sådanne personer er at træne en differentialt privat model, men dette sker normalt på bekostning af modellens ydeevne. Desuden er det svært at sige på grund af en privatlivsparameter $\\epsilon$ hvad der var effekten på den uddannede repræsentation. I dette arbejde har vi til formål at vejlede fremtidige praktikere og forskere i, hvordan man kan forbedre privatlivets fred, samtidig med at man opretholder en god model performance. Vi demonstrerer, hvordan man træner en differentialt privat præuddannet sprogmodel (dvs. BERT) med en privatlivsgaranti på $\\epsilon=1$ og med kun en lille forringelse af ydeevnen. Vi eksperimenterer på et datasæt af kliniske noter med en model trænet på en målenhedsekstraktionsopgave, og sammenligner den med en lignende model trænet uden differentieret privatliv. Endelig præsenterer vi eksperimenter, der viser, hvordan man fortolker den differentialt-private repræsentation og forstår de oplysninger, der mistes og vedligeholdes i denne proces.', 'nl': 'Contextuele taalmodellen hebben geleid tot significant betere resultaten op een overvloed aan taalbegrijpingstaken, vooral wanneer vooraf getraind op dezelfde gegevens als de downstream taak. Hoewel deze aanvullende pre-training meestal de prestaties verbetert, kan het leiden tot informatielekken en daardoor de privacy van personen die in de trainingsgegevens worden genoemd, in gevaar brengen. Een methode om de privacy van dergelijke individuen te waarborgen is om een differentieel-privé model te trainen, maar dit gaat meestal ten koste van de prestaties van het model. Bovendien is het moeilijk te zeggen met een privacyparameter $\\epsilon$ wat het effect was op de getrainde vertegenwoordiging. In dit werk willen we toekomstige beoefenaars en onderzoekers begeleiden hoe ze privacy kunnen verbeteren met behoud van goede modelprestaties. We demonstreren hoe je een differentieel-privaat voorgetraind taalmodel (BERT) kunt trainen met een privacygarantie van $\\epsilon=1$ en met slechts een kleine achteruitgang in prestaties. We experimenteren met een dataset klinische aantekeningen met een model dat getraind is op een doelentiteitsextractie taak, en vergelijken het met een vergelijkbaar model dat getraind is zonder differentiële privacy. Tot slot presenteren we experimenten die laten zien hoe de differentieel-private representatie kan worden geïnterpreteerd en de informatie die verloren gaat en bewaard wordt in dit proces te begrijpen.', 'hr': 'Kontekstualni jezički modeli doveli su do značajno boljih rezultata na razumijevanje jezika zadataka, posebno kada su predobučeni na istim podacima kao i zadatak koji se nalazi u potpunosti. Iako ova dodatna predobuka obično poboljšava učinkovitost, može dovesti do curenja informacija i stoga rizikuje privatnost pojedinaca spomenute u podacima o obuci. Jedan način za garantiranje privatnosti takvih pojedinaca je obučavanje različitih privatnih model a, ali to obično dolazi na troškove modelnog izvođenja. Osim toga, teško je reći s obzirom na parameter privatnosti $\\epsilon$ koji je bio učinak na obučeno predstavljanje. U ovom poslu ciljamo voditi buduće praktičnike i istraživače kako poboljšati privatnost dok održavamo dobru modelsku funkciju. Mi pokazujemo kako trenirati diferencijalno-privatni predobučeni jezički model (tj. BERT) sa garancijom privatnosti od $\\epsilon=1$ i samo malim degradacijom u izvedbi. Eksperimentiramo na skupu podataka kliničkih bilješaka s modelom obučenim na zadatku izvlačenju ciljnih subjekta i uspoređujemo je s sličnim modelom obučenim bez diferencijalne privatnosti. Konačno predstavljamo eksperimente kako interpretirati diferencijalno-privatno predstavljanje i razumijeti izgubljene i održane informacije u ovom procesu.', 'bg': 'Контекстуалните езикови модели са довели до значително по-добри резултати при множество задачи за разбиране на езика, особено когато са предварително обучени по същите данни като задачата надолу по веригата. Въпреки че това допълнително предварително обучение обикновено подобрява ефективността, то може да доведе до изтичане на информация и следователно рискува неприкосновеността на личния живот на лицата, посочени в данните за обучение. Един от методите за гарантиране на неприкосновеността на личния живот на такива лица е да се обучи диференциално-частен модел, но това обикновено идва за сметка на производителността на модела. Освен това е трудно да се каже, като се има предвид параметър за поверителност $\\epsilon$ какъв е бил ефектът върху обучението представяне. В тази работа ние се стремим да насочим бъдещите практикуващи и изследователи как да подобрим неприкосновеността на личния живот, като същевременно поддържаме добро представяне на модела. Ние демонстрираме как да тренираме диференциално частен предварително обучен езиков модел (т.е. БЕРТ) с гаранция за поверителност $\\epsilon=1$ и само с малко влошаване на ефективността. Експериментираме върху набор от клинични бележки с модел, обучен за задача за екстракция на целеви субекти, и го сравняваме с подобен модел, обучен без диференциална поверителност. Накрая, представяме експерименти, показващи как да интерпретираме диференциално-частното представяне и да разберем загубената и поддържана информация в този процес.', 'fa': 'مدل\u200cهای زبان متوسط به نتیجه\u200cهای زیادی بهتر روی یک قسمت از کارهای درک زبان، مخصوصا وقتی پیش از این روی داده\u200cهای همانند کار پایین آموزش داده می\u200cشود. در حالی که این آموزش پیش از آموزش بیشتری معمولاً عملکرد را بهتر می\u200cکند، می\u200cتواند به تغییر دادن اطلاعات هدایت کند و به همین دلیل خطر خصوصی شخصی که در داده\u200cهای آموزش اشاره می\u200cشود، خ یک روش برای تضمین خصوصی این افراد این است که یک مدل خصوصی و متفاوتی را آموزش دهند، اما این معمولا به خرج عملکرد مدل می رسد. علاوه بر این، برای دادن یک پارامتر خصوصی $ epsilon$ تاثیر روی نمایش آموزش آموزش چیست سخت است. در این کار ما هدف داریم که آموزگاران و محققان آینده را راهنمایی کنیم که چگونه بهتر کردن خصوصی در زمان حفظ عملکرد مدل خوب باشند. ما نشان می دهیم که چگونه یک مدل پیش آموزش زبان مختلف و خصوصی را آموزش دهیم (یعنی BERT) با تضمین خصوصی از $ epsilon=1$ و تنها با یک کم نابودی در اجرایی. ما روی یک مجموعه داده\u200cهای نوشته\u200cهای کلینیک آزمایش می\u200cکنیم با یک مدل آموزش داده شده روی یک کار خارج کردن هدف، و آن را با یک مدل مانند آموزش داده شده بدون خصوصی متفاوتی مقایسه می\u200cکنیم. بالاخره، ما آزمایشات را نشان می دهیم که چگونه تعبیر نمایش خصوصی متفاوتی را انجام دهند و اطلاعات را درک کنیم که در این فرایند گم شده و نگه دارند.', 'id': 'Model bahasa konteks telah menyebabkan hasil yang jauh lebih baik pada banyak tugas pemahaman bahasa, terutama ketika dilatih-dilatih pada data yang sama dengan tugas downstream. Sementara prapelatihan tambahan ini biasanya meningkatkan prestasi, itu bisa menyebabkan kebocoran informasi dan karena itu risiko privasi individu yang disebut dalam data pelatihan. Satu metode untuk menjamin privasi orang-orang seperti itu adalah untuk melatih model yang berbeda-pribadi, tetapi ini biasanya datang pada biaya prestasi model. Selain itu, sulit untuk mengatakan mengingat parameter privasi $\\epsilon$ apa efek pada representation terlatih. In this work we aim to guide future practitioners and researchers on how to improve privacy while maintaining good model performance.  Kami menunjukkan bagaimana untuk melatih model bahasa pre-dilatih berbeda-pribadi (i.e., BERT) dengan jaminan privasi $\\epsilon=1$ dan hanya dengan degradasi kecil dalam prestasi. Kami eksperimen pada set data catatan klinis dengan model yang dilatih pada tugas ekstraksi entitas sasaran, dan membandingkannya dengan model yang sama dilatih tanpa privasi berbeda. Akhirnya, kami mempersembahkan eksperimen menunjukkan bagaimana untuk menerjemahkan representation berbeda-pribadi dan memahami informasi yang hilang dan terus dalam proses ini.', 'de': 'Kontextbasierte Sprachmodelle haben zu deutlich besseren Ergebnissen bei einer Vielzahl von Sprachverstﾃ､ndnisaufgaben gefﾃｼhrt, insbesondere wenn sie auf denselben Daten wie die nachgelagerte Aufgabe vortrainiert wurden. Wﾃ､hrend dieses zusﾃ､tzliche Vortraining in der Regel die Leistung verbessert, kann es zu Informationslecks fﾃｼhren und somit die Privatsphﾃ､re der in den Trainingsdaten genannten Personen gefﾃ､hrden. Eine Methode, um die Privatsphﾃ､re solcher Personen zu gewﾃ､hrleisten, besteht darin, ein differentiell-privates Modell zu trainieren, was jedoch in der Regel zu Lasten der Modellleistung geht. Darﾃｼber hinaus ist es schwer zu sagen, wenn man einen Datenschutzparameter $\\epsilon$ verwendet, was der Effekt auf die trainierte Reprﾃ､sentation war. In dieser Arbeit mﾃｶchten wir zukﾃｼnftige Praktiker und Forscher dazu fﾃｼhren, wie sie die Privatsphﾃ､re verbessern und gleichzeitig eine gute Modellleistung beibehalten kﾃｶnnen. Wir zeigen, wie man ein differenziell-privates vortrainiertes Sprachmodell (z.B. BERT) mit einer Datenschutzgarantie von $\\epsilon=1$ und mit nur geringem Leistungsverlust trainiert. Wir experimentieren an einem Datensatz klinischer Notizen mit einem Modell, das auf eine Zielentitﾃ､tsextraktionsaufgabe trainiert wurde, und vergleichen es mit einem ﾃ､hnlichen Modell, das ohne differentielle Privatsphﾃ､re trainiert wurde. Abschlieﾃ歹nd stellen wir Experimente vor, die zeigen, wie man die differentiell-private Reprﾃ､sentation interpretiert und die dabei verlorenen und erhaltenen Informationen versteht.', 'af': "Konteksual taal modelles het beter resultate gelei op 'n plethora van taal verstaan opdragte, veral wanneer voorafgelei word op dieselfde data as die onderstreem opdrag. Alhoewel hierdie addisionele voorvoerring gewoonlik die prestasie verbeter, kan dit lei na inligting uitlei en daarom riskeer die privateit van individuele wat in die onderwerp data ingementioneer is. Een metode om die privateit van sodanige individue te garanteer is om 'n verskillende-privaat model te trein, maar hierdie kom gewoonlik op die koste van model prestasie. Ook, dit is moeilik om 'n privateitsparameter gegee te vertel $\\epsilon$ wat was die effek op die opgelei voorstelling. In hierdie werk doen ons doel om toekomstige praktisers en ondersoekers te lei oor hoe om privateit te verbeter terwyl ons goeie model prestasie onderhou. Ons wys hoe om 'n verskillende-privaat voor-onderwerp taal model te tref (i.e. BERT) met 'n privateitsgarantie van $\\epsilon=1$ en met slegs 'n klein afbreiding in prestasie. Ons eksperimenteer op 'n dataset van kliniske notas met 'n model wat op 'n doel entiteit uittrek taak opgelei is, en vergelyk dit met 'n gelyklike model wat onderwerp is sonder verskillende privateit. Eindelik, ons voorstel eksperimente wat wys hoe om die verskillende-privaat verteenwoordigheid te interpreteer en verstaan die inligting verloor en onderhou in hierdie proses.", 'ko': '언어 환경 언어 모델은 대량의 언어 이해 임무에서 현저한 효과를 거두었고 특히 하류 임무와 같은 데이터에서 예비 훈련을 할 때 현저한 효과를 거두었다.이런 추가 예비 교육은 통상적으로 실적을 높일 수 있지만, 정보 유출을 초래하여 교육 데이터에 언급된 개인 프라이버시를 위태롭게 할 수 있다.이러한 개체의 프라이버시를 보장하는 방법의 하나는 서로 다른 사유 모델을 훈련시키는 것이지만, 이것은 통상적으로 모델의 성능을 희생하는 대가이다.또한 프라이버시 파라미터 $\\epsilon$을 지정하면 훈련을 거친 표시에 어떤 영향을 미치는지 판단하기 어렵다.이 업무에서, 우리는 미래의 종사자와 연구원들이 양호한 모델 성능을 유지하는 동시에 프라이버시를 개선하는 방법을 지도하는 데 목적을 두고 있다.프라이버시 보장이 $\\epsilon=1$인 상황에서 성능이 약간 떨어진 상황에서 서로 다른 개인 예훈련 언어 모델(즉 BERT)을 훈련하는 방법을 보여 드리겠습니다.우리는 임상 노트 데이터 집합에서 실험을 진행하여 목표 실체 추출 임무에서 훈련하는 모델을 사용하고 프라이버시 차이가 없는 상황에서 훈련하는 유사한 모델과 비교했다.마지막으로 우리는 이러한 차이점을 어떻게 설명하는지, 그리고 이 과정에서 잃어버리고 보존된 정보를 어떻게 이해하는지 실험을 통해 보여 주었다.', 'am': 'የቋንቋ ምሳሌዎች በቋንቋ ማስተዋል ስራቶች ላይ እጅግ የበለጠ ፍሬዎችን አግኝተዋል፤ በተለይም በሙሉ ዳታዎችን እንደ ታችኛው ስራ አስቀድሞ ተማርቷል፡፡ While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data.  One method to guarantee the privacy of such individuals is to train a differentially-private model, but this usually comes at the expense of model performance.  በተጨማሪም፣ የግል ብሔራዊ ተሟጋቾች የኢፌስሊዮን ዶላር ማድረግ በጣም ችግር ነው፡፡ በዚህ ስራ የፊተኛውን ባለማሪዎች እና አስተማሪዎችን መልካም ሞዴል ፈቃድ በመጠበቅ የግል ብሔርነታቸውን እንዴት እንዲያሳድግ እናሳውቃለን፡፡ የግል ልዩ-የፊተኛ የፊተኛውን የቋንቋ ምሳሌ (BERT) እንዳስተማርነው እናሳያቸዋለን፡፡ በተለየ ብልሽነት ሳይኖር በተማረ አካባቢ ስራ ላይ የተማረ ሞዴል እናስተያየዋለን፡፡ በመጨረሻም፣ የግል ብሔራዊ መልዕክት እንዴት እንዲተርጉም እና በዚህ ፕሮጀክት የጠፋውንና የተጠበቀውን መረጃ እናስተውል ዘንድ የምናሳየው ፈተናዎች እናቀርባለን፡፡', 'sw': 'Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task.  Wakati mafunzo haya ya zamani mara nyingi huwa yanaboresha utendaji wa mafunzo, inaweza kusababisha utoaji wa habari na kwa hiyo hatarisha faragha ya watu waliotajwa katika takwimu za mafunzo. Utawala mmoja wa kuhakikisha faragha ya watu hawa ni kufundisha muundo wa binafsi tofauti, lakini mara nyingi huu huja kwa gharama ya utendaji wa mifano. Zaidi, ni vigumu kuelezea kipimo cha kipimo cha faragha cha dola za epsilon kilichotokana na uwakilishi wa mafunzo. Katika kazi hii tunalenga kuwaongoza wataalamu na watafiti wa siku za uso wa namna ya kuboresha faragha wakati wa kuendelea utendaji wa mifano mazuri. Tunaonyesha namna ya kufundisha mtindo wa lugha binafsi wa kujifunza tofauti (yaani BERT) ambao una uhakika binafsi wa kiasi cha $epsilon=1 na kwa kiwango kidogo tu cha fedha cha utendaji. Tunajaribu kwenye seti ya taarifa za nota za kliniki na modeli inayofundishwa kwenye kazi ya utekelezaji wa vifaa vya lengo, na kulinganisha na mtindo uliofanana bila faragha tofauti. Mwisho, tunatoa majaribio yanayoonyesha namna ya kutafsiri uwakilishi binafsi tofauti na kuelewa taarifa zilizopoteza na kudhibitiwa katika mchakato huu.', 'tr': 'Metin dil nusgalary dillerini düşünmek üçin has gowy netijesi bolup geçirdi, ýöne-de durmuşy a şak täzelikleri bilen öňünden öňünden boşadylýanda. Bu ýene-de öňki öňki öňki okuw adatça ukyplary gowurap bilýän bolsa, bu şekilde maglumatlary çykyp bilýär we şonuň üçin okuw maglumatynda aýdylan adamlaryň hususiyetini riske edip biler. Böyle adamlaryň ýalňyşlygyny goramak üçin bir ýoly üýtgeşik nusgasyny üýtgetmekdir, ýöne bu köplenç nusgasy üçin bir nusgasy bar. Ayrıca, $\\epsilon$ kişisel bir parameter verilmek zor. Bu işde gelejekde praktikantlary we arkadaşlaryny gowy nusgasyny goramak üçin ýüzünligini gowy nusgasyny ýüzeltmäge maksadyk edýäris. Biz üýtgeşik, a ýratyn-aýratyn öň-bilim dili nusgasyny (meseläm, BERT) $\\epsilon=1$ diňe kiçi bir wezamlyk bilen üýtgetmelidigini görkeýäris. Biz kliniki notlarda bir nusga taýýarlanan bir nusga taýýarlanan bir nusga bilen test edip, muny farklı hususiyatsyz üçin üýtgedilen bir nusga karşılaştyrýarys. Soňunda, biz munyň düýbünden aýratyn-aýratyn suratyny nädip terjime etmelidigini we bu prosesde ýiten we saglanýan informasiýany düşünjegimizi görkeýäris.', 'hy': 'Կոնտեքստային լեզվի մոդելները հանգեցրել են լեզվի հասկանալու խնդիրների բազմաթիվ ավելի լավ արդյունքների, հատկապես երբ նախապատրաստված են նույն տվյալների վրա, ինչպիսիք են վերջնական խնդիրները: Մինչդեռ այս ավելացյալ նախապատրաստման արդյունքը սովորաբար բարելավում է արդյունքը, այն կարող է հանգեցնել տեղեկատվության արտահոսքի և, հետևաբար, վտանգ տալ կրթության տվյալներում նշված անհատների գաղտնիության Այսպիսի անհատների գաղտնիությունը երաշխավորելու մեթոդը տարբերականորեն-մասնավոր մոդելն է, բայց սա սովորաբար արժե մոդելների արտադրողության վրա: Ավելին, դժվար է ասել, եթե հաշվի առնենք $\\Epsilon$ անձնական գործընթացը, թե ինչ ազդեցություն ուներ վարժեցված ներկայացման վրա: Այս աշխատանքի ընթացքում մենք նպատակով ենք ուղղորդել ապագա փորձագետներին և հետազոտողներին, թե ինչպես բարելավել գաղտնիությունը, մինչդեռ պահպանել լավ մոդելներ: Մենք ցույց ենք տալիս, թե ինչպես վարժեցնել տարբերականորեն-մասնավոր նախապատրաստված լեզվի մոդելը (այսինքն, BER-ը)  $\\Epsilon=1 դոլարով գաղտնիքի ապահովությամբ և միայն փոքր դեգրադացիայի արդյունքում: Մենք փորձում ենք կլինիկական նոտաների տվյալների համակարգի վրա մի մոդելի հետ, որը պատրաստված է նպատակային էակի վերացման խնդրի վրա, և համեմատում ենք այն նման մոդելի հետ, որը պատրաստված է առանց տարբերակային մասնավորության: Վերջապես, մենք ներկայացնում ենք փորձեր, որոնք ցույց են տալիս, թե ինչպես մեկնաբանել տարբերականորեն-մասնավոր ներկայացումը և հասկանալ այս գործընթացի ընթացքում կորցված և պահպանված տեղեկատվությունը:', 'bn': 'বিভিন্ন ভাষার মডেল অনেক ভালো ফলাফলের কারণে প্রাপ্ত হয়েছে, বিশেষ করে যখন প্রশিক্ষণের পূর্বে ডাউট্রিম কাজের মতো প্রশিক্ষিত হয়। এই প্রশিক্ষণ সাধারণত পূর্ব প্রশিক্ষণের কার্যক্রম উন্নতি প্রদান করে, তথ্য লিকেজের ফলে তথ্য প্রদর্শন করা যাবে এবং তাই প্রশিক্ষণের তথ্য এই ধরনের ব্যক্তিদের গোপনীয়তা নিশ্চিত করার একটি পদ্ধতি হচ্ছে একটি আলাদা ব্যক্তিগত মডেল প্রশিক্ষণ করার জন্য, কিন্তু এটা সাধারণত মডেলের প্রদর এছাড়াও, প্রশিক্ষিত প্রতিনিধিত্বের প্রভাব কি হয়েছে তা বলা কঠিন। এই কাজে আমরা ভবিষ্যতের প্রশিক্ষকদের এবং গবেষকদের পরিচালিত করার উদ্দেশ্য হচ্ছি যে ভাল মডেলের প্রদর্শন নিয়ে কিভাবে গোপনীয়তা উন আমরা প্রদর্শন করি কিভাবে বিভিন্ন ভাষায় প্রশিক্ষিত ভাষার মডেল (যেমন বিরেটি) প্রদর্শন করা হয়েছে যার ব্যক্তিগত নিশ্চিত $ এপিসিলোন=১ মার্কিন ডলার এবং প্রদর্শনে  আমরা একটি ক্লিনিক্যাল নোটের ডাটাসেটে পরীক্ষা করছি যেখানে একটি টার্গেট বিনিময়ের কাজে প্রশিক্ষিত মডেল প্রশিক্ষণ করা হয়েছে, আর এটিকে তুলনা করি  Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.', 'sq': 'Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task.  While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data.  Një metodë për të garantuar privatësinë e individëve të tillë është të trajnohet një model ndryshe-privat, por kjo zakonisht vjen në shpenzim të shfaqjes së modelit. Përveç kësaj, është e vështirë të tregohet me një parametrë privatësie $\\epsilon$ çfarë ishte efekti në përfaqësimin e stërvitur. Në këtë punë ne synojmë të udhëzojmë praktikantët e ardhshëm dhe kërkuesit se si të përmirësojmë privatësinë duke mbajtur performancën e mirë të modelit. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of $\\epsilon=1$ and with only a small degradation in performance.  Ne eksperimentojmë në një grup të dhënash të shënimeve klinike me një model të stërvitur në një detyrë ekstrahimi të njësisë objektive, dhe e krahasojmë me një model të ngjashëm të stërvitur pa privatësi diferenciale. Më në fund, ne paraqesim eksperimente që tregojnë se si të interpretojmë përfaqësimin ndryshe-privat dhe të kuptojmë informacionin e humbur dhe të mbajtur në këtë proces.', 'cs': 'Kontextové jazykové modely vedly k výrazně lepším výsledkům na celé řadě úkolů porozumění jazyků, zejména když jsou předškoleny na stejných datech jako následný úkol. Zatímco tento dodatečný předškolení obvykle zlepšuje výkon, může vést k úniku informací, a tudíž ohrožuje soukromí jednotlivců uvedených v tréninkových údajích. Jednou ze způsobů, jak zaručit soukromí těchto jednotlivců, je trénovat diferenciálně-soukromý model, ale to obvykle jde na úkor výkonu modelu. Kromě toho je těžké říct při parametru ochrany soukromí $\\epsilon$ jaký byl vliv na trénovanou reprezentaci. Cílem této práce je pomoci budoucím odborníkům a výzkumným pracovníkům, jak zlepšit soukromí při zachování dobrého výkonu modelu. Ukážeme, jak trénovat diferenciálně-privátní předškolený jazykový model (tj. BERT) se zárukou soukromí $\\epsilon=1$ a s pouze malou degradací výkonu. Experimentujeme na datové sadě klinických poznámek s modelem trénovaným na extrakční úkol cílové entity a porovnáme ji s podobným modelem trénovaným bez diferenciálního soukromí. V závěru jsou prezentovány experimenty ukazující, jak interpretovat diferenciálně-soukromou reprezentaci a porozumět informacím ztraceným a udržovaným v tomto procesu.', 'bs': 'Kontekstualni jezički modeli su doveli do značajno boljih rezultata na razumijevanje jezika, posebno kada su predobučeni na istim podacima kao i zadatak koji se nalazi u potpunosti. Iako ova dodatna predobuka obično poboljšava učinkovitost, može dovesti do curenja informacija i stoga rizikuje privatnost pojedinaca spominjenih u podacima o obuci. Jedan metod da garantuje privatnost takvih pojedinaca je trenirati različitim privatnim modelom, ali to obično dolazi na troškove model a. Osim toga, teško je reći s obzirom na parameter privatnosti $\\epsilon$ koji je bio uticaj na obučeno predstavljanje. U ovom poslu ciljamo voditi buduće praktičnike i istraživače o tome kako poboljšati privatnost dok održavamo dobre modele. Pokazujemo kako trenirati diferencijalno-privatni predobučeni jezički model (tj. BERT) sa garancijom privatnosti od $\\epsilon=1$ i sa samo malim degradacijom u izvedbi. Eksperimentiramo na setu podataka kliničkih notova sa modelom obučenim na zadatku izvlačenju ciljnih entiteta i uspoređujemo je sa sličnim modelom obučenim bez diferencijalne privatnosti. Konačno predstavljamo eksperimente kako interpretirati diferencijalno-privatno predstavljanje i razumijeti izgubljene i održane informacije u ovom procesu.', 'az': 'Müxtəlif dil modelləri dil anlama işlərinin çoxluğuna daha yaxşı sonuçlarına yol göstərdilər, özlərinə də a şağı işləri ilə əvvəl təhsil ediləndə. Bu çox əvvəlki təhsil olaraq təhsil işlətməsini çox yaxşılaşdıran halda, bu məlumatların təhsil edilməsini təhsil edir və buna görə də təhsil məlumatlarında deyilən kişilərin xəbərsizliğini riskləndirir. Bütün kişilərin xəbərsizliğini garantiya etmək üçün bir yol, başqa-başqa şəxsi modeli təhsil etməkdir, amma bu genellikle modellərin təhsil vaxtında gəlir. Əksinə, təhsil göstərilməsi üzərində növbəsini $\\epsilon$ verilmək çətin idi. Bu işdə biz gələcək təhsil sahibləri və araştırmacıları yaxşı modellərin performansını qoruyarkən gizli təhsil etməyi necə yaxşılaşdırmağı haqqında yola yönəltmək istəyirik. Biz müxtəlif təhsil edilmiş dil model in i necə təhsil etməyi göstəririk (ya da BERT) $\\epsilon=1$ və ancaq küçük bir dəyişiklik göstəririk. Biz müəyyən edilmiş bir məqsədilə təhsil edilmiş modeli ilə klinik notların bir quruluğuna təcrübə edirik və onu müxtəlif təhsil olmadan təhsil edilmiş bənzər modeli ilə qarşılaşdırırıq. Sonunda, biz təcrübələr göstəririk ki, fərqli-xüsusi təcrübələrin necə yoxlamasını və bu proses içində çıxıb saxlanıldığını anlayır.', 'et': 'Kontekstilised keelemudelid on andnud märkimisväärselt paremaid tulemusi paljude keelte mõistmise ülesannete puhul, eriti juhul, kui neid on eelnevalt koolitatud samade andmetega nagu järgneva ülesande puhul. Kuigi see täiendav eelkoolitus parandab tavaliselt tulemuslikkust, võib see põhjustada teabe leket ja seega ohustada koolitusandmetes nimetatud isikute privaatsust. Üks meetod selliste isikute privaatsuse tagamiseks on koolitada diferentsiaalselt eraõiguslikku mudelit, kuid see toimub tavaliselt mudeli jõudluse arvelt. Lisaks on privaatsusparameetri $\\epsilon$ puhul raske öelda, milline oli mõju koolitatud esindusele. Selle töö eesmärk on juhendada tulevasi praktikuid ja teadlasi, kuidas parandada privaatsust, säilitades samas hea mudeli jõudluse. Näitame, kuidas treenida diferentsiaalselt privaatset eelõpetatud keelemudelit (st BERT), mille privaatsuse garantii on $\\epsilon=1$ ja jõudluse väike halvenemine. Me eksperimenteerime kliiniliste märkmete andmekogumi mudeliga, mis on koolitatud sihtorganite ekstraheerimise ülesandeks, ja võrdleme seda sarnase mudeliga, mis on koolitatud ilma diferentsiaalse privaatsuseta. Lõpuks tutvustame eksperimente, mis näitavad, kuidas tõlgendada diferentsiaalselt-privaatset esindust ning mõista selles protsessis kaotatud ja säilitatud informatsiooni.', 'fi': 'Konekstuaaliset kielimallit ovat johtaneet merkittävästi parempiin tuloksiin lukuisissa kielen ymmärtämisen tehtävissä, erityisesti kun niitä on koulutettu samoista tiedoista kuin jatkovaiheen tehtävässä. Vaikka tämä lisäkoulutus yleensä parantaa suorituskykyä, se voi johtaa tietovuotoon ja siten vaarantaa koulutustiedoissa mainittujen henkilöiden yksityisyyden. Yksi tapa taata tällaisten henkilöiden yksityisyys on kouluttaa erilaisesti yksityinen malli, mutta tämä tapahtuu yleensä mallin suorituskyvyn kustannuksella. Lisäksi on vaikea sanoa tietosuojaparametrin $\\epsilon$ perusteella, mikä oli vaikutus koulutettuun edustukseen. Tässä työssä pyrimme ohjaamaan tulevia ammattilaisia ja tutkijoita siihen, miten yksityisyyttä voidaan parantaa samalla, kun mallien suorituskyky säilyy hyvänä. Esittelemme, miten koulutamme erilaisesti yksityisen esikoulutetun kielimallin (BERT), jonka tietosuojatakuu on $\\epsilon=1$jasuorituskyky heikkenee vain vähän. Kokeilemme kliinisten muistiinpanojen aineistoa kohdekokonaisuuden uuttamiseen koulutetulla mallilla ja vertaamme sitä vastaavaan malliin, joka on koulutettu ilman erillistä yksityisyyttä. Lopuksi esittelemme kokeiluja, jotka osoittavat, miten erilaisesti yksityistä edustusta tulkitaan ja miten siinä menetetään ja säilytetään tietoa.', 'ca': "Els models de llenguatge contextual han portat a resultats significativament millors en una gran quantitat de tasques de comprensió del llenguatge, especialment quan s'han preparat previament amb les mateixes dades que la tasca downstream. Mentre aquesta pré-formació adicional normalment millora el rendiment, pot portar a fuga d'informació i, per tant, arrisca a la privacitat dels individus mencionats en les dades de formació. One method to guarantee the privacy of such individuals is to train a differentially-private model, but this usually comes at the expense of model performance.  A més, és difícil dir, dada un paràmetre de privacitat $\\epsilon$, quin va ser l'efecte en la representació entrenada. En aquesta feina busquem orientar futurs metges i investigadors sobre com millorar la privacitat mentre mantenim un bon rendiment model. Ens demostrem com entrenar un model de llenguatge pre-entrenat diferencialment privat (i.e., BERT) amb una garantia de privacitat de $\\epsilon=1$ i amb només una petita degradació en el rendiment. Experimentem en un conjunt de dades de notes clíniques amb un model entrenat en una tasca d'extracció d'una entitat alvo, i la comparam amb un model similar entrenat sense privacitat diferencial. Finalment, presentem experiments mostrant com interpretar la representació diferencialment privada i entendre la informació perduda i mantenida en aquest procés.", 'ha': "Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task.  A lokacin da wannan wa'adin na ƙaranci ko daidai ya improve performance, yana iya ƙara wa leakaji wa information, kuma don haka sai ya riske faranta ga mutane wanda aka ambaci cikin data na tattalin. Wata hanyoyi ta wajabta farat ɗaya ga waɗancan, yana yin wa'anar wani misali mai gaurawa a gauransa, kuma amma, ko da yaushe, wannan yana kasancẽwa a ƙara wa misalin mutane. Kayya, yã yi nau'i a gaya wani parameter farat ɗaya $applsilon Daga wannan aikin, Munã nufin mu shiryar da masu aikin aiki da mãsu ƙidãya a gaba, da watani kan yadda za su improve farat ɗaya a lokacin da za'a tsare taskõkin misali mai kyau. Tuna nuna yadda za Mu kõre misalin misalin harshen wanda aka yi wa zaman wani da aka sani (misali, BERT) da wani garanci farat ɗaya na $ applsilon=1 Dollar kuma da wani wulãkanci kaɗan a cikin aikin. Kana jarraba a kan kodi na takardar notori na kima da wata motel wanda aka yi wa wa aikin zartar abun abun shinen goani, kuma mu sami da shi ga misalin wanda aka yi wa shirin da shi na kami ko kuma ba da wani fara na daban-daban. Haƙĩƙa, Munã halatar da jarrabai idan za mu nuna yadda zã a bayyana fassarar masu tsari na gauraci kuma mu fahimta information lost da wanda aka tsare a cikin wannan aikin.", 'sk': 'Kontekstualni jezikovni modeli so pripeljali do bistveno boljših rezultatov pri številnih nalogah razumevanja jezika, zlasti če so bili vnaprej usposobljeni za iste podatke kot za nadaljnjo nalogo. Čeprav to dodatno predusposabljanje običajno izboljšuje uspešnost, lahko povzroči uhajanje informacij in s tem ogroža zasebnost posameznikov, navedenih v podatkih o usposabljanju. Eden od načinov zagotavljanja zasebnosti takih posameznikov je usposabljanje diferencialno-zasebnega modela, vendar to običajno pride na račun uspešnosti modela. Poleg tega je težko reči glede na parameter zasebnosti $\\epsilon$ kakšen je bil učinek na usposobljeno predstavitev. V tem delu želimo voditi bodoče strokovnjake in raziskovalce o tem, kako izboljšati zasebnost in hkrati ohraniti dobro delovanje modela. Prikazujemo, kako usposabljati diferencialno zasebni vnaprej usposobljeni jezikovni model (tj. BERT) z jamstvom za zasebnost $\\epsilon=1$ in le z majhnim poslabšanjem zmogljivosti. Na podatkovnem naboru kliničnih zapiskov eksperimentiramo z modelom, usposobljenim za nalogo ekstrakcije ciljne entitete, in ga primerjamo s podobnim modelom, usposobljenim brez diferencialne zasebnosti. Na koncu predstavljamo poskuse, ki kažejo, kako interpretirati diferencialno-zasebno reprezentacijo in razumeti informacije, ki so bile izgubljene in ohranjene v tem procesu.', 'jv': 'Menu item to Open \'Search for Open Files\' dialog Nejer sampeyan ingkang dianggap sing mengko nggawe geranggap perusahaan kelas, dadi iso nggawe informasi nggawe nguasai perusahaan pribadi sing nguasai perusahaan kanggo mbatalé perusahaan kuwi nggawe dadi nyong. Wurung sistem kanggo ngerasai pribadi kanggo wong liyane kuwi, dadi mau sekolah model sing gak bener, njuk saiki iki dadi ono wektu nggo ndelok model politenessoffpolite"), and when there is a change ("assertive Nang iki jalluk awak dhéwé ngerti nggawe praksi barêng-barêng lan yatak njaluké awak dhéwé kuwi nggawe ngubah pribadi kanggo ngubah mau ngerayakno paran sing apik dhéwé. We show up as to vlacene a separately-Personal advanced language model (i.e. BERT) with a Personal garance of $epsion=1$and with only a small degradition in success. Awak dhéwé éntuk data set dadine sing dadi nggawe model sing tukang nggawe Tarjamahan jenis, karo nggawe nyimpen karo model sing kudu nggawe bener user@example:button', 'he': 'דוגמני שפה קונטקסטיים הובילו לתוצאות טובות יותר משמעותיות בהרבה של משימות הבנה לשפה, במיוחד כאשר התאמנו מראש על אותם נתונים כמו המשימה התחתונה. בעוד האימונים הנוספים האלה בדרך כלל משתפרים ביצועים, הם יכולים להוביל לדליפת מידע ולכן מסתכנים בפרטיות של אנשים שמזכירים בנתונים האימונים. שיטה אחת להבטיח את הפרטיות של אנשים כאלה היא לאמן מודל פרטי-שונה, אבל זה בדרך כלל מגיע על חשבון ההופעה של מודל. חוץ מזה, קשה לומר בהתחשב בפרמטר פרטיות $\\epsilon$ מה היה ההשפעה על מייצג המאמן. In this work we aim to guide future practitioners and researchers on how to improve privacy while maintaining good model performance.  אנחנו מראים איך לאמן מודל שפה מאומן מראש (כלומר, BERT) בהבטחה פרטית של $\\epsilon=1$ ובשיפול קטן בלבד בהופעה. אנו מנסים על קבוצת נתונים של רשומות קליניות עם דוגמנית מאומנת על משימת יציאה של היחידה המטרה, ושווה אותה עם דוגמנית דומה מאומנת ללא פרטיות דיפרנציאלית. Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.', 'bo': 'སྐད་ཡིག་ཆ་མིག་དཔེ་དབྱེ་བ་དེ་ནི་སྐད་ཡིག་ཆ་ལྟ་བུའི་ལས་འགུལ་བཤད་ཀྱི་རྐྱེན་བ་མང་ཙམ་ཡིན། སྔོན་གྲོང་གླེང་སྔོན་གྲངས་ཀྱི་ལས་འགུལ་སྐྱོང་ཚད་ཕར་རྒྱས་གཏོང་བ་ཡིན་ནའང་། དེ་ནི་གནས་ཚུལ་གསལ་བཤད་ཀྱི་ཤུལ་མར་ཉེ དབྱེ་རིགས་འདིའི་མི་སྒེར་གྱི་རང་བཞིན་བདེ་སྦྱོར་བྱེད་ནི་ཕན་མེད་སྒེར་གྱི་མ་དཔེ་ཆས་ལ་རྒྱུན་ལྡན་མི་སྟོན་པའི་རྒྱུ་དང་། Moreover, it is hard to tell a privacy parameter $\\epsilon$ what was the effect on the trained representation. ང་ཚོས་མ་འོངས་པའི་ལས་འགན་འདིས་མི་འོངས་པར་སྤྲོད་རྒྱུ་མཁན་དང་། མི་རྩོལ་ལྟ་སྟངས་ཀྱིས་མི་ཚོགས་རྣམས་རང་ཉིད་ཀྱི་མི་ We demonstrate how to train a differentially-private pre-trained language model (i.e. BERT) with a private guarantee of $\\epsilon=1$ and with only a small degradation in performance. ང་ཚོས་གཞུང Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.'}
{'en': 'Using Confidential Data for Domain Adaptation of Neural Machine Translation', 'ar': 'استخدام البيانات السرية لتكييف المجال للترجمة الآلية العصبية', 'pt': 'Usando dados confidenciais para adaptação de domínio da tradução automática neural', 'fr': "Utilisation de données confidentielles pour l'adaptation du domaine de la traduction automatique neuronale", 'es': 'Uso de datos confidenciales para la adaptación de dominios de la traducción automática neuronal', 'ja': '神経機械翻訳のドメイン適応のための機密データの使用', 'zh': '用机密数神经机器翻译应之', 'hi': 'न्यूरल मशीन अनुवाद के डोमेन अनुकूलन के लिए गोपनीय डेटा का उपयोग करना', 'ru': 'Использование конфиденциальных данных для адаптации домена нейронного машинного перевода', 'ga': "Úsáid Sonraí Rúnda le haghaidh Oiriúnú Fearainn d'Aistriúchán Néar-Inneallta", 'ka': 'Name', 'el': 'Χρήση εμπιστευτικών δεδομένων για προσαρμογή τομέων της νευρωνικής μηχανικής μετάφρασης', 'hu': 'Bizalmas adatok használata a neurális gépi fordítás tartományi adaptálásához', 'it': "Utilizzo di dati riservati per l'adattamento del dominio della traduzione automatica neurale", 'kk': 'Невралдық машинаның аудармасының домен адаптациясы үшін құпиялық деректерді қолдану', 'lt': 'Naudojant konfidencialius duomenis neutraliųjų mašinų vertimo domenui pritaikyti', 'ms': 'Mengguna Data Rahsia untuk Penyesuaian Domain Terjemahan Mesin Neural', 'mk': 'Користење доверливи податоци за адаптација на доменот на превод на неврални машини', 'ml': 'നെയുറല്\u200d മെഷീന്\u200d പരിഭാഷപ്പെടുത്തുന്നതിനുള്ള ഡൊമെയിനിലെ രഹസ്യമായ ഡേറ്റാ ഉപയോഗിക്കുന്നു', 'mn': 'Сэтгэл машины хөгжүүлэлтийн домин адаптацийн мэдээллийг ашиглах', 'mt': 'L-użu ta’ Dejta Kunfidenzjali għall-Adattament tad-Dominju tat-Traduzzjoni tal-Magni Newrali', 'pl': 'Wykorzystanie poufnych danych do adaptacji domen neuronowego tłumaczenia maszynowego', 'no': 'Bruk konfidensielle data for domeneadaptering av neiralmaskineomsetjing', 'sr': 'Koristenje povjerljivih podataka za adaptaciju domena neuroloških prevoda', 'so': 'Isticmaalka macluumaadka qarsoodiga ee Domain Adaptation of Turjumista Mashinka Neural', 'sv': 'Använda konfidentiella data för domänanpassning av neural maskinöversättning', 'si': 'න්\u200dයුරල් මැෂින් වාර්ථාව සඳහා ඩෝමේන් අනුමාණය සඳහා විශ්වාසිත දත්ත භාවිත කරන්න', 'ro': 'Utilizarea datelor confidențiale pentru adaptarea domeniului de traducere automată neurală', 'ta': 'புதிய இயந்திரத்தின் மொழிபெயர்ப்பிற்கான டொமைன் மொழிபெயர்ப்பிற்கான இரகசிய தரவு', 'ur': 'نیورال ماشین ترجمہ کے ڈومین اڈپٹیٹ کے لئے محرمانہ دیٹا استعمال کیا جاتا ہے', 'uz': 'Name', 'vi': 'Dùng dữ liệu bí mật cho định vị sửa chữa vùng thần kinh của máy dịch', 'bg': 'Използване на поверителни данни за адаптиране на домейна на невралния машинен превод', 'nl': 'Vertrouwelijke gegevens gebruiken voor domeinaanpassing van neuronale machinevertaling', 'hr': 'Koristeći povjerljive podatke za adaptaciju domena neurološkog prevoda stroja', 'ko': '기밀 데이터 를 이용하여 신경 기계 번역 영역 의 자체 적응 을 하다', 'da': 'Brug af fortrolige data til domænetilpasning af neural maskinoversættelse', 'de': 'Verwendung vertraulicher Daten zur Domänenanpassung neuronaler maschineller Übersetzung', 'id': 'Menggunakan Data Konfidensial untuk Adaptasi Domain dari Translation Mesin Neural', 'fa': 'استفاده از داده\u200cهای محرمانه برای تغییر دادن دامن ماشین عصبی', 'af': 'Gebruik van Konfidenciale Data vir Domein Aanpassering van Neurale Masjien Vertaling', 'am': 'የውይይት መድረክ', 'sw': 'Kwa kutumia taarifa za siri kwa ajili ya Tafsiri ya Mashine ya Kifaransa', 'hy': 'Նյարդային մեքենայի թարգմանման բնական տվյալների օգտագործումը', 'tr': 'Neural Makina Terjimesi üçin Taýram Maglumaty ullanýa', 'sq': 'Përdorimi i të dhënave konfidenciale për përshtatjen e domenit të përkthimit të makinës nervore', 'az': 'N칬ral Makin 칂evirm톛sinin Domain Adjustasyonu 칲칞칲n Qeyd Veril톛ri istifad톛 edilir', 'bn': 'নিউরাল মেশিন অনুবাদের ডোমেইন পরিচালনার জন্য গোপন তথ্য ব্যবহার করা হচ্ছে', 'cs': 'Použití důvěrných dat pro doménovou adaptaci neuronového strojového překladu', 'bs': 'Koristeći povjerljive podatke za adaptaciju domena neuroloških prevoda', 'et': 'Konfidentsiaalsete andmete kasutamine neuroaalse masintõlke domeeni kohandamiseks', 'fi': 'Luottamuksellisten tietojen käyttäminen neurokonekäännöksen verkkotunnuksen mukauttamiseen', 'ca': 'Utilitzant dades confidencials per adaptar el domini a la traducció de màquines neurals', 'jv': 'Ngawe Perintari Dong Kondisentasi kanggo Tarjamahan Njuara Majin', 'he': 'השימוש בנתונים סודיים להסתגל למשטרה של התרגום של מכונת נוירולית', 'ha': '@ info: whatsthis', 'sk': 'Uporaba zaupnih podatkov za prilagoditev domene nevralnega strojnega prevoda', 'bo': 'Neural Machine Translation ་ལ་Domain Adaptation of Neural Machine'}
{'en': 'We study the problem of domain adaptation in Neural Machine Translation (NMT) when domain-specific data can not be shared due to confidentiality or copyright issues. As a first step, we propose to fragment data into phrase pairs and use a random sample to fine-tune a generic NMT model instead of the full sentences. Despite the loss of long segments for the sake of confidentiality protection, we find that NMT quality can considerably benefit from this adaptation, and that further gains can be obtained with a simple tagging technique.', 'ar': 'ندرس مشكلة تكييف المجال في الترجمة الآلية العصبية (NMT) عندما لا يمكن مشاركة البيانات الخاصة بالمجال بسبب قضايا السرية أو حقوق النشر. كخطوة أولى ، نقترح تجزئة البيانات إلى أزواج عبارات واستخدام عينة عشوائية لضبط نموذج NMT عام بدلاً من الجمل الكاملة. على الرغم من فقدان المقاطع الطويلة من أجل حماية السرية ، نجد أن جودة NMT يمكن أن تستفيد بشكل كبير من هذا التكيف ، وأنه يمكن الحصول على مكاسب إضافية باستخدام تقنية بسيطة لوضع العلامات.', 'fr': "Nous étudions le problème de l'adaptation de domaine dans la traduction automatique neuronale (NMT) lorsque des données spécifiques à un domaine ne peuvent pas être partagées pour des raisons de confidentialité ou de droits d'auteur. Dans un premier temps, nous proposons de fragmenter les données en paires de phrases et d'utiliser un échantillon aléatoire pour affiner un modèle NMT générique au lieu des phrases complètes. Malgré la perte de longs segments au nom de la protection de la confidentialité, nous constatons que la qualité NMT peut bénéficier considérablement de cette adaptation, et que d'autres gains peuvent être obtenus avec une simple technique de marquage.", 'es': 'Estudiamos el problema de la adaptación de dominios en la traducción automática neuronal (NMT) cuando los datos específicos del dominio no se pueden compartir debido a cuestiones de confidencialidad o derechos de autor. Como primer paso, proponemos fragmentar los datos en pares de frases y utilizar una muestra aleatoria para ajustar un modelo NMT genérico en lugar de las oraciones completas. A pesar de la pérdida de segmentos largos en aras de la protección de la confidencialidad, encontramos que la calidad de la NMT puede beneficiarse considerablemente de esta adaptación, y que se pueden obtener más beneficios con una técnica de etiquetado simple.', 'pt': 'Estudamos o problema de adaptação de domínio em Tradução Automática Neural (NMT) quando dados específicos de domínio não podem ser compartilhados devido a questões de confidencialidade ou direitos autorais. Como primeiro passo, propomos fragmentar os dados em pares de frases e usar uma amostra aleatória para ajustar um modelo NMT genérico em vez das frases completas. Apesar da perda de segmentos longos por questão de proteção de confidencialidade, descobrimos que a qualidade do NMT pode se beneficiar consideravelmente dessa adaptação e que ganhos adicionais podem ser obtidos com uma simples técnica de marcação.', 'ja': '私たちは、機密保持または著作権の問題によりドメイン固有のデータを共有できない場合の、ニューラル・マシン・トランスレーション（ NMT ）におけるドメイン適応の問題を研究しています。最初のステップとして、フレーズペアにデータを断片化し、ランダムサンプルを使用して、全文の代わりに一般的なNMTモデルを微調整することを提案します。機密保護のために長いセグメントを失ったにもかかわらず、NMT品質はこの適応からかなりの利益を得ることができ、単純なタグ付け技術でさらなる利得を得ることができることがわかります。', 'ru': 'Мы изучаем проблему адаптации домена в нейронном машинном переводе (НМП), когда специфические для домена данные не могут быть разделены из-за проблем конфиденциальности или авторских прав. В качестве первого шага мы предлагаем фрагментировать данные на пары фраз и использовать случайную выборку для тонкой настройки общей модели НМТ вместо полных предложений. Несмотря на потерю длинных сегментов ради защиты конфиденциальности, мы обнаружили, что качество НБК может значительно выиграть от этой адаптации и что дальнейшие выгоды могут быть получены с помощью простой техники маркировки.', 'zh': '论神经机器翻译(NMT)中之领,机密性版权不共特定域之数也。 第一步,议将数片段化为短语是,并用随机样本微调通用NMT模形而非全句。 虽密护而失长段,见NMT量可以受益匪浅,而可以易表而益之。', 'hi': 'हम न्यूरल मशीन ट्रांसलेशन (NMT) में डोमेन अनुकूलन की समस्या का अध्ययन करते हैं जब डोमेन-विशिष्ट डेटा गोपनीयता या कॉपीराइट मुद्दों के कारण साझा नहीं किया जा सकता है। पहले चरण के रूप में, हम वाक्यांश जोड़े में डेटा को विभाजित करने का प्रस्ताव करते हैं और पूर्ण वाक्यों के बजाय एक सामान्य एनएमटी मॉडल को ठीक करने के लिए एक यादृच्छिक नमूने का उपयोग करते हैं। गोपनीयता संरक्षण के लिए लंबे खंडों के नुकसान के बावजूद, हम पाते हैं कि एनएमटी गुणवत्ता इस अनुकूलन से काफी लाभ उठा सकती है, और यह कि आगे का लाभ एक साधारण टैगिंग तकनीक के साथ प्राप्त किया जा सकता है।', 'ga': 'Déanaimid staidéar ar an bhfadhb a bhaineann le hoiriúnú fearainn in Neural Machine Translation (NMT) nuair nach féidir sonraí a bhaineann go sonrach le fearann a roinnt mar gheall ar cheisteanna rúndachta nó cóipchirt. Mar chéad chéim, tá sé beartaithe againn sonraí a roinnt ina bpéirí frásaí agus sampla randamach a úsáid chun mionsamhail cineálach NMT a mhionchoigeartú in ionad na n-abairtí iomlána. In ainneoin go gcailltear codanna fada ar mhaithe le cosaint rúndachta, feicimid gur féidir le cáilíocht NMT leas mór a bhaint as an oiriúnú seo, agus gur féidir gnóthachain bhreise a fháil le teicníc clibeála simplí.', 'hu': 'A Neural Machine Translation (NMT) esetében a tartományspecifikus adatok titoktartási vagy szerzői jogi kérdések miatt nem oszthatók meg. Első lépésként azt javasoljuk, hogy az adatokat kifejezéspárokra bontsuk fel, és véletlenszerű mintával finomhangoljuk az általános NMT modellt a teljes mondatok helyett. Annak ellenére, hogy a hosszú szegmensek elvesztése a titokvédelem érdekében, úgy találjuk, hogy az NMT minősége jelentősen hasznos lehet ebből az adaptációból, és hogy egy egyszerű címkézési technikával további előnyöket érhetünk el.', 'ka': 'ჩვენ ვისწავლოთ დემომინის ადაპტიფიკაციის პრობლემა ნეიროლური მაქსინის გასაგრძელებაში (NMT) როცა დემომინის განსაკუთრებული მონაცემები არ შეიძლება გაყოფილი კ როგორც პირველი ნაგულისხმებით, ჩვენ გვეძლოთ ფრაზის ზოგიში ფრაზის მონაცემები გავაკეთოთ და გამოყენოთ გამოყენებული ნაგულისხმები NMT მოდელის შესაძლებელად, ყველა სიტყვე ჩვენ ვიღებთ, რომ NMT კაalitეტი შეუძლია გავიღოთ ამ აკაპრატიაციაზე და რომ დამატება შეიძლია გავიღოთ უფრო მხოლოდ მარტივი ტექნოგიით.', 'el': 'Μελετάμε το πρόβλημα της προσαρμογής του τομέα στην Νευρική Μηχανική Μετάφραση (όταν δεν μπορούν να μοιραστούν δεδομένα συγκεκριμένου τομέα λόγω εμπιστευτικότητας ή πνευματικών δικαιωμάτων. Ως πρώτο βήμα, προτείνουμε να χωρίσουμε τα δεδομένα σε ζεύγη φράσεων και να χρησιμοποιήσουμε ένα τυχαίο δείγμα για να τελειοποιήσουμε ένα γενικό μοντέλο αντί για τις πλήρεις προτάσεις. Παρά την απώλεια μακρών τμημάτων για λόγους προστασίας της εμπιστευτικότητας, διαπιστώνουμε ότι η ποιότητα των NMT μπορεί να ωφεληθεί σημαντικά από αυτή την προσαρμογή, και ότι περαιτέρω οφέλη μπορούν να επιτευχθούν με μια απλή τεχνική επισήμανσης.', 'it': "Studiamo il problema dell'adattamento del dominio in Neural Machine Translation (NMT) quando i dati specifici del dominio non possono essere condivisi a causa di problemi di riservatezza o copyright. Come primo passo, proponiamo di frammentare i dati in coppie di frasi e utilizzare un campione casuale per mettere a punto un modello NMT generico invece delle frasi complete. Nonostante la perdita di segmenti lunghi per motivi di riservatezza, scopriamo che la qualità NMT può beneficiare notevolmente di questo adattamento e che ulteriori guadagni possono essere ottenuti con una semplice tecnica di tagging.", 'lt': 'Mes tiriame problem ą, susijusią su srities pritaikymu neuroninių mašin ų vertimui (NMT), kai dėl konfidencialumo ar autorių teisių klausimų negali būti dalijamasi konkrečiai srities duomenimis. Pirmiausia siūlome suskirstyti duomenis į frazių poras ir naudoti atsitiktinį mėginį, kad tiksliau pritaikytume bendrą NMT model į, o ne visus sakinius. Nepaisant ilgų segmentų praradimo siekiant apsaugoti konfidencialumą, manome, kad NMT kokybė gali būti labai naudinga taikant šį pritaikymą ir kad papildomą naudą galima pasiekti taikant paprastą ženklinimo metodą.', 'mk': 'Го проучуваме проблемот со адаптацијата на домените во Неуралната машина транслекција (НМТ) кога податоците специфични за домените не можат да се споделат поради доверливоста или прашањата со авторските права. Како прв чекор, предлагаме да ги фрагментираме податоците во парови на фрази и да користиме случајно примерок за да го пофинираме генералниот модел на НМТ наместо целосните реченици. Despite the loss of long segments for the sake of confidentiality protection, we find that NMT quality can considerably benefit from this adaptation, and that further gains can be obtained with a simple tagging technique.', 'kk': 'Біз доменге аудару мәселесін зерттейміз. Доменге ерекше деректері сенімділік не автор құқықтарының мәселелерінен ортақтастырылмайды. Бірінші қадам ретінде, біз мәліметтерді фраз екеуіне бөлшектеу және толық сөйлемелердің орнына кездейсоқ NMT үлгісін баптау үшін кездейсоқ үлгісін қолданамыз. Сұхбаттылық қорғау үшін ұзын сегменттерді жоғалуға қарамастан, NMT сапасы осы адаптацияның маңызды пайдалануға болады деп ойлаймыз, және қарапайым тегтерді қолдану техникасы арқылы қолданылады.', 'ms': 'Kami mempelajari masalah penyesuaian domain dalam Terjemahan Mesin Neural (NMT) bila data khusus domain tidak boleh dikongsi kerana kerahasiaan atau isu hak cipta. Sebagai langkah pertama, kami melamar untuk memecahkan data ke pasangan frasa dan menggunakan sampel rawak untuk menyesuaikan model NMT generik selain dari kalimat penuh. Walaupun hilang segmen panjang demi perlindungan kerahasiaan, kami dapati kualiti NMT boleh bermanfaat dari penyesuaian ini, dan keuntungan lanjut boleh dicapai dengan teknik pengetik sederhana.', 'mt': 'Aħna nistudjaw il-problema tal-adattament tad-dominju fit-Traduzzjoni tal-Makkinarju Newrali (NMT) meta dejta speċifika għad-dominju ma tistax tinqasam minħabba kwistjonijiet ta’ kunfidenzjalità jew drittijiet tal-awtur. Bħala l-ewwel pass, nipproponu li d-dejta tiġi frammentata f’pari ta’ frażijiet u nużaw kampjun każwali biex jiġi rfinut mudell ġeneriku NMT minflok is-sentenzi sħa ħ. Minkejja t-telf ta’ segmenti twal għall-protezzjoni tal-kunfidenzjalità, isibu li l-kwalità tal-NMT tista’ tibbenefika konsiderevolment minn dan l-adattament, u li jistgħu jinkisbu aktar qligħ b’teknika sempliċi tat-tikkettar.', 'mn': 'Бид мэдээллийн тодорхой мэдээллийг өөрийн итгэл үнэмшил эсвэл copyright асуудлын шалтгаан хуваалцаж чадахгүй байх үед сэтгэл хөдлөлийн адилгацийн асуудлыг судалж байна. Эхний алхмын хувьд бид өгөгдлийг хэлбэрүүдийн хооронд хувааж, санамсаргүй жишээг ашиглаж, бүрэн өгүүлбэрүүдийн оронд ерөнхий NMT загварыг тодорхойлох гэж санал болно. Мэдээж итгэлтэй хамгаалахын тулд урт хэсэг алдагдсан ч, бид NMT-ын сайн чанар энэ адилтгалын тулд маш их ашиг авч чадна гэдгийг олж мэднэ. Үүнээс илүү олон ашиг энгийн маркингийн техник ашиглаж чадна.', 'no': 'Vi studerer problemet med domeneadaptasjonen i Neuralmaskineomsetjing (NMT) når domenespesifikke data kan ikkje delast på grunn av konfidencialitet eller opphavsrett. Som første steg, foreslår vi fragment data i fråparer og bruk eit tilfeldige prøve for å finne opp eit generelt NMT-modell i staden for fullstendige setningar. I tillegg til tapt av lange segmentar for sikkerhet på konfidencialitet, finn vi at NMT-kvaliteten kan betydelig nytte frå denne adaptasjonen, og at meir forståking kan få med ein enkel merking-teknikk.', 'pl': 'Badamy problem adaptacji domeny w neuronowym tłumaczeniu maszynowym (NMT), gdy dane specyficzne domeny nie mogą być udostępniane ze względu na poufność lub kwestie praw autorskich. W pierwszym kroku proponujemy fragmentację danych na pary fraz i wykorzystanie losowej próbki do dostrojenia ogólnego modelu NMT zamiast pełnych zdań. Pomimo utraty długich segmentów ze względu na ochronę poufności, stwierdzamy, że jakość NMT może znacznie skorzystać z tego dostosowania i że dalsze zyski można uzyskać za pomocą prostej techniki tagowania.', 'ml': 'നെയുറല്\u200d മെഷീന്\u200d പരിഭാഷത്തില്\u200d ഡോമെന്\u200d അഡാപ്റ്റേഷന്\u200d പ്രശ്നം നമ്മള്\u200d പഠിക്കുന്നു. ഡൊമെയിന്\u200d പ്രത്യേക വിവരങ്ങള്\u200d രഹസ്യം അല്ലെങ ആദ്യ പടിയായി നമ്മള്\u200d വാക്കിന്റെ ജോടികളിലേക്ക് ഡേറ്റാ പിരിച്ചുവെക്കാന്\u200d പ്രൊദ്ദേശിക്കുന്നു. പൂര്\u200dണ്ണമായ വാക്കുകള്\u200dക്ക്  രഹസ്യ സംരക്ഷിക്കുന്നതിന് വേണ്ടി നീണ്ട വിഭാഗങ്ങള്\u200d നഷ്ടപ്പെട്ടാലും നമ്മള്\u200d കണ്ടെത്തുന്നു NMT വ്യവസ്ഥയില്\u200d നിന്ന് വളരെ ഉപകരിക്കാന്\u200d സാധിക', 'ro': 'Studiem problema adaptării domeniului în Neural Machine Translation (NMT) atunci când datele specifice domeniului nu pot fi partajate din cauza confidențialității sau a problemelor legate de drepturile de autor. Ca prim pas, propunem fragmentarea datelor în perechi de fraze și utilizarea unui eșantion aleatoriu pentru a regla fin un model NMT generic în loc de propozițiile complete. În ciuda pierderii segmentelor lungi din motive de protecție a confidențialității, constatăm că calitatea NMT poate beneficia considerabil de această adaptare și că alte câștiguri pot fi obținute printr-o tehnică simplă de etichetare.', 'sr': 'Proučavamo problem adaptacije domena u Neuralnoj prevodi mašine (NMT), kada se podaci specifičnih domena ne mogu podijeliti zbog pitanja povjerljivosti ili autorskih prava. Kao prvi korak, predlažemo da delimo podatke u parove fraze i iskoristimo nasumični uzorak da sredimo generični model NMT umjesto punih rečenica. Uprkos gubitku dugih segmenta zbog zaštite povjerljivosti, smatramo da kvaliteta NMT može znatno koristiti od ove adaptacije, i da se daljnji dobitak može dobiti jednostavnom tehnikom označavanja.', 'si': 'අපි ප්\u200dරශ්නයක් ප්\u200dරශ්නයක් සහ ප්\u200dරශ්නයක් නිසා ප්\u200dරශ්නයක් සඳහා ප්\u200dරශ්නයක් නිර්මාණය මෂ්\u200dයාන්ත්\u200dරය භාවිත ( මුලින් පැත්තක් විදියට, අපි පැත්තක් දැනුම් පැත්තක් දෙන්න ප්\u200dරතිචාර දෙන්න ප්\u200dරතිචාර දෙන්න ප්\u200dරතිචාර දෙන්න ප්\u200dරතිච විශ්වාසිත ආරක්ෂාව සඳහා ලොකු කොටස් නැති වුනොත්, අපිට හොයාගන්න පුළුවන් NMT කුළුවට මේ සැකසුම් වලින් ගොඩක් ප්\u200dරයෝජනය කරන්න,', 'sv': 'Vi studerar problemet med domänanpassning i Neural Machine Translation (NMT) när domänspecifika data inte kan delas på grund av sekretess eller upphovsrättsfrågor. Som ett första steg föreslår vi att fragmentera data i fraspar och använda ett slumpmässigt exempel för att finjustera en generisk NMT-modell istället för hela meningarna. Trots förlusten av långa segment för sekretesskyddets skull finner vi att NMT-kvalitet kan dra stor nytta av denna anpassning, och att ytterligare vinster kan erhållas med en enkel märkningsteknik.', 'ta': 'நாம் நெயுரல் இயந்திரத்தின் மொழிபெயர்ப்பு (NMT) டோமைன் குறிப்பிட்ட தகவல்களை இரகசியம் அல்லது நகல் உரிமையின் காரணத்தால் பகிர்ந்து க முதல் படி என்றால், நாம் சொற்றொடர் ஜோடியாக தரவை பிரிக்க பரிந்துரைக்கிறோம் முழு வாக்கியங்களுக்கு பதிலாக பொதுவான NMT மாதிரி இரகசியமான பாதுகாப்பிற்கான நீண்ட துண்டுகள் இழந்த போதிலும், நாங்கள் கண்டுபிடிக்கிறோம் NMT தரம் இந்த ஒதுக்கீட்டிலிருந்து மிகவும் நன்ம', 'so': "Waxaannu baranaynaa dhibaatada beddelinta deegaanka ee turjumidda maskinenka Neural (NMT) marka looma qeybin karo macluumaad cayiman oo domain ah qarsoodiga ama arrimaha xuquuqda sameeya darteed. Qalad ugu horeysa, waxaynu u soo jeedaynaa in macluumaadka ka qeybinno labo noocyo ah, waxaana isticmaalaynaa samool fudud oo aan sameynno noocyada caadiga ah ee NMT, taasoo aan ka beddelin hadalka buuxda. Inta kastoo uu lumo qaybaha dheer badbaadada qarsoodiga ah, waxaynu aragnaa in qiimaha NMT ay si aad u faa'iido badan uga heli karto hababkan, iyo in faa'iido dheeraad ah lagu heli karo teknolojiyo fudud.", 'ur': 'ہم نے نئورل ماشین ترجمہ (NMT) میں ڈومین کے اضافہ کے مسئلہ کی تحقیق کی جب ڈومین کے مطابق مطابق ڈاٹی کے بارے میں شریک نہیں ہوسکتی۔ ایک پہلی قدم کے طور پر، ہم فریز جوڑوں میں ڈیٹا ٹکڑ کرنے کے لئے پیشنهاد کرتے ہیں اور تمام جماعتوں کے بدلے ایک ناقص نمونہ کا استعمال کریں۔ اس کے علاوہ بہت سی قسمتوں کے خسارہ کے لئے ہم دیکھتے ہیں کہ NMT کی کیفیت اس تدبیر سے بہت زیادہ فائدہ پہنچا سکتی ہے اور اس سے بھی بہت سی فائدہ پہنچا سکتی ہے ایک ساده ٹیگ ٹیکنیگ ٹیکنیک سے۔', 'uz': "@ info: whatsthis Birinchi darajada, biz birinchi darajaga maʼlumotni bir so'zlar qo'llashga harakat qilamiz va taʼminlovchi misol yordamida umumiy NMT modelini bir so'zlarni o'zgartirish mumkin. Sichqoncha xavfsizlik uchun uzun qismlari yo'q bo'lsa, biz NMT сифатида ушбу adaptsiya juda foydalanishi mumkin, va yana bir necha muvaffaqiyatli oddiy yozuvchi teknikani olish mumkin.", 'vi': 'Chúng tôi nghiên cứu vấn đề sửa chữa miền trong dịch lắp thần kinh (NMB) khi dữ liệu đặc trưng về miền không thể được chia sẻ do bí mật hay vấn đề tác quyền. Như bước đầu tiên, chúng tôi đề nghị ghép dữ liệu thành thành các cụm từ và dùng một mẫu ngẫu nhiên để hiệu chỉnh mô hình NMB chung thay vì câu đầy đủ. Mặc dù mất nhiều phân khúc để bảo vệ tính mạng, chúng tôi thấy chất lượng NMT có thể được hưởng nhiều từ sự thích hợp này, và nó có thể đạt thêm lợi nhuận nhờ một kỹ thuật đánh dấu đơn giản.', 'hr': 'Proučavamo problem adaptacije domena u Neuralnom prevodu strojeva (NMT) kada se podaci specifičnih domena ne mogu podijeliti zbog pitanja povjerljivosti ili autorskih prava. Kao prvi korak, predlažemo fragmentirati podatke u parove fraze i iskoristiti nasumični uzorak kako bi sredili generični model NMT umjesto punih rečenica. Unatoč gubitku dugih segmenta zbog zaštite povjerljivosti, smatramo da kvaliteta NMT može znatno koristiti od te adaptacije, i da se dalje dobiti s jednostavnom tehnikom označavanja.', 'bg': 'Проучваме проблема с адаптацията на домейна в невралния машинен превод (НМТ), когато специфичните за домейна данни не могат да бъдат споделяни поради проблеми с конфиденциалността или авторското право. Като първа стъпка предлагаме данните да бъдат фрагментирани в двойки фрази и да се използва случайна извадка за фино настройване на генеричен модел вместо пълните изречения. Въпреки загубата на дълги сегменти в името на защитата на поверителността, ние установихме, че качеството на НМТ може значително да се възползва от тази адаптация и че по-нататъшни ползи могат да бъдат постигнати с проста техника за маркиране.', 'id': 'We study the problem of domain adaptation in Neural Machine Translation (NMT) when domain-specific data cannot be shared due to confidentiality or copyright issues.  Sebagai langkah pertama, kami mengusulkan untuk memisahkan data menjadi pasangan frasa dan menggunakan sampel acak untuk memperbaiki model NMT generik selain kalimat penuh. Meskipun kehilangan segment panjang demi perlindungan rahasia, kami menemukan bahwa kualitas NMT bisa sangat berguna dari adaptasi ini, dan bahwa keuntungan lebih lanjut dapat diperoleh dengan teknik penanda sederhana.', 'nl': 'We bestuderen het probleem van domeinaanpassing in Neural Machine Translation (NMT) wanneer domeinspecifieke gegevens niet kunnen worden gedeeld vanwege vertrouwelijkheid of auteursrechtelijke kwesties. Als eerste stap stellen we voor om gegevens te fragmenteren in woordenparen en een willekeurige steekproef te gebruiken om een generiek NMT model te finetunen in plaats van de volledige zinnen. Ondanks het verlies van lange segmenten ter wille van vertrouwelijkheidsbescherming, vinden we dat NMT-kwaliteit aanzienlijk kan profiteren van deze aanpassing, en dat verdere voordelen kunnen worden behaald met een eenvoudige tagging techniek.', 'da': 'Vi undersøger problemet med domænetilpasning i Neural Machine Translation (NMT), når domænespecifikke data ikke kan deles på grund af fortrolighed eller ophavsretlige spørgsmål. Som et første skridt foreslår vi at opdele data i sætningspar og bruge en tilfældig prøve til at finjustere en generisk NMT-model i stedet for de fulde sætninger. På trods af tabet af lange segmenter af hensyn til fortrolighedsbeskyttelse, finder vi, at NMT kvalitet kan drage stor fordel af denne tilpasning, og at yderligere gevinster kan opnås med en simpel tagging teknik.', 'fa': 'ما مشکل تغییرات دامنین را در ترجمه ماشین عصبی (NMT) مطالعه می\u200cکنیم وقتی داده\u200cهای خاص دامنه نمی\u200cتوانند به دلیل مسائل مطمئنی یا حقوق copyright تقسیم شوند. به عنوان یک قدم اول، ما پیشنهاد می\u200cدهیم که داده\u200cها را به جفت\u200cهای عبارت تقسیم کنیم و از نمونه تصادفی استفاده کنیم تا یک مدل NMT عمومی را به جای جمله\u200cهای کامل تنظیم کنیم. با وجود خسارت بخش طولانی برای حفاظت محرمانه، می\u200cبینیم که کیفیت NMT می\u200cتواند از این تغییرات خیلی سود دهد، و این پیروزی بیشتری می\u200cتواند با یک تکنیک ساده\u200cی نقاشی پیدا شود.', 'de': 'Wir untersuchen das Problem der Domänenanpassung in der neuronalen maschinellen Übersetzung (NMT), wenn domänenspezifische Daten aufgrund von Vertraulichkeits- oder Urheberrechtsfragen nicht weitergegeben werden können. Als ersten Schritt schlagen wir vor, Daten in Phrasenpaare zu fragmentieren und ein zufälliges Sample zu verwenden, um ein generisches NMT-Modell anstelle der vollständigen Sätze zu verfeinern. Trotz des Verlustes von langen Segmenten aus Gründen des Vertraulichkeitsschutzes stellen wir fest, dass NMT-Qualität von dieser Anpassung erheblich profitieren kann und dass mit einer einfachen Tagging-Technik weitere Vorteile erzielt werden können.', 'sw': 'Tunafuatilia tatizo la kubadilisha ndani katika Tafsiri ya Mashine ya NMT (NMT) ambapo taarifa maalum za ndani haziwezi kusambazwa kwa sababu ya siri au masuala ya haki miliki. Kama hatua ya kwanza, tunapendekeza kugawanya taarifa katika mazingira ya msemo na kutumia sampuli isiyo na kawaida ili kutumia mtindo wa NMT wa kawaida badala ya hukumu kamili. Pamoja na kupoteza vipengele vya muda mrefu kwa ajili ya ulinzi wa siri, tunagundua kuwa viwango vya NMT vinaweza kuwa na faida kubwa kutoka kwa makubaliano haya, na kwamba mafanikio mengine yanaweza kupatikana kwa njia rahisi ya kuchagua alama.', 'tr': 'Biz Neural Makine terjimelerinde (NMT) domençylyk hasaplanyň konfidensiýat ýa-da awtomatik hakyk meselesinden bölünip bilmeýän zaman domençylyk adaptasynyň meselesini öwrenip barýarys. Ilkinji adım olarak, fraz çiftleri içine parçalamak we dolu sözleriň yerine, hassas bir örnek ullamak üçin bir NMT modelini düzeltmek üçin teklif ediyoruz. Gizlilik goragy üçin uzak segmentler ýitilmegine rağmen, NMT kaliwatynyň bu adaptasiýadan örän köp faydasy bolup biler we şu ýeterlik gazanlyklary basit bir taglama tekniýasy bilen alyp biler.', 'af': "Ons studeer die probleem van domein aanpassing in Neural Masjien Vertaling (NMT) wanneer domein-spesifieke data nie gedeel word vanweë vertroudheid of kopieregte probleme nie. As 'n eerste stap, voorstel ons om fragmentasie data in frase paar te gebruik en 'n willekeurige voorbeeld om 'n generieke NMT model te fin-tuneer in plaas van die volle setnings. Ons vind dat NMT-kwaliteit aansienlik van hierdie aanpassing kan voordeel, en dat verdere verkrygings kan verkry word met 'n eenvoudige etikettegniek.", 'ko': '우리는 신경기계번역(NMT)에서 특정 분야의 데이터가 비밀이나 판권 문제로 공유되지 않을 때의 영역 적응 문제를 연구했다.첫 번째 단계로, 우리는 데이터를 단어 쌍으로 나누고, 완전한 문장이 아닌 일반적인 NMT 모델을 무작위 샘플로 조정하는 것을 권장합니다.비록 비밀을 지키기 위해 긴 부분을 잃어버렸지만, 우리는 NMT의 품질이 이러한 적응에서 상당히 큰 이익을 얻을 수 있고, 간단한 표기 기술을 통해 더욱 큰 수익을 얻을 수 있다는 것을 발견했다.', 'az': 'Biz domenalı məlumatların gizli və təkrar haqqı məsələlərinə görə paylaşılmadığı zaman domenalı uyğunlaşdırma problemini təhsil edirik. İlk adım olaraq, məlumatları fraz çiftlərə parçalamaq və bütün cümlələr yerinə təsirli NMT modelini düzəltmək üçün təsirli nümunə istifadə edirik. Müvəffəqiyyət qoruması üçün uzun segmentlərin zərəri olmasına rağmen, NMT keyfiyyəti bu adaptasiyadan çox fayda verə bilər və daha çox qənimətlər basit etiketləmə tekniki ilə əldə edilə bilər.', 'sq': 'Ne studiojmë problem in e përshtatjes së domainit në Translation Neural Machine (NMT) kur të dhënat specifike për domainin nuk mund të ndahen për shkak të konfidencialitetit apo çështjeve të të drejtave të autorit. Si hap i parë, ne propozojmë të fragmentojmë të dhënat në çifte frazësh dhe të përdorim një mostra të rastësishme për të rregulluar një model NMT gjeneral në vend të frazëve të plota. Megjithë humbjen e segmenteve të gjata për hir të mbrojtjes së konfidencialitetit, ne gjejmë se cilësia e NMT mund të përfitojë konsiderueshëm nga kjo përshtatje dhe se fitime të mëtejshme mund të arrihen me një teknikë të thjeshtë etiketash.', 'am': 'የኖሜን አካባቢ (NMT) ጉዳይ በስውይት ወይም የቅጂ መብት ጉዳይ ምክንያት የዶሜን መክፈት (NMT) በተለየ ጊዜ እናስተምራለን፡፡ በመጀመሪያ ደረጃዎች፣ ዳራዎችን ወደ phrase ዓይነቶች ለመቆራረጥ እና በሙሉ ቃላት ፋንታ ለመጠቀም የgeneric NMT model እናስቀምጣለን፡፡ የረጅም ክፍሎች በተደብቀው መጠበቅ ሲጠፉ እንኳን NMT ጥጋት ከዚህ አካባቢው እጅግ ጥቅም እንዲችል እናገኛለን፡፡', 'hy': 'Մենք ուսումնասիրում ենք նյարդային մեքենայի թարգմանման (NMT) բնագավառի ադապտացիայի խնդիրը, երբ բնագավառի մասնավոր տվյալները չեն կարող կիսվել վստահության կամ հեղինակային իրավունքի խնդիրների պատճառով: Առաջին քայլն այն է, որ մենք առաջարկում ենք բաժանել տվյալները արտահայտությունների զույգերով և օգտագործել պատահական նմուշ, որպեսզի կարելի է լավագույնել NMT-ի ընդհանուր մոդելը ամբողջ նախադասությունների փոխարեն: Չնայած երկար սեգմենտների կորստին պաշտպանության համար, մենք հայտնաբերում ենք, որ NMT-ի որակը կարող է մեծ շահույթ ունենալ այս ադապտացիայից, և որ ավելի շատ շահույթ կարող է ստանալ պարզ նշանակման մեթոդով:', 'cs': 'Studujeme problém adaptace domén v neuronovém strojovém překladu (NMT), kdy doménově specifická data nemohou být sdílena z důvodu důvěrnosti nebo autorských práv. Jako první krok navrhujeme rozdělit data do frázových párů a použít náhodný vzorek k jemnému vyladění obecného NMT modelu namísto celých vět. Navzdory ztrátě dlouhých segmentů v zájmu ochrany důvěrnosti zjišťujeme, že kvalita NMT může značně těžit z této adaptace a že další zisky lze dosáhnout jednoduchou metodou tagování.', 'bn': 'আমরা নিউরাল মেশিন অনুবাদ (এনএমটি) নিজেদের ডোমেইন অ্যাডপেটশনের সমস্যা গবেষণা করি যখন ডোমেইন-নির্দিষ্ট তথ্য গোপনীয়তা বা কপিরাইটের বি প্রথম পদক্ষেপ হিসেবে আমরা ব্যাক্তির জোড়ায় তথ্য বিভক্ত করতে প্রস্তাব করি এবং পুরো বাক্যের বদলে সাধারণ NMT মডেল ব্যবহার করি। গোপনীয়তা রক্ষার জন্য দীর্ঘ অংশ হারিয়েছে সত্ত্বেও আমরা দেখতে পাচ্ছি যে এনএমটি মানের মান বেশী লাভ করতে পারে, আরো অর্জন পাওয়া যাবে একটি সহজ ট্যাগিং প্র', 'bs': 'Proučavamo problem adaptacije domena u Neuralnom prevodu strojeva (NMT) kada se podaci specifičnih domena ne mogu podijeliti zbog pitanja povjerljivosti ili autorskih prava. Kao prvi korak, predlažemo fragmentirati podatke u parove fraze i iskoristiti nasumični uzorak kako bi ispravili generični model NMT umjesto punih rečenica. Uprkos gubitku dugih segmenta zbog zaštite povjerljivosti, smatramo da kvalitet NMT može znatno koristiti od te adaptacije, i da se dalje dobiti s jednostavnom tehnikom označavanja.', 'fi': 'Tutkimme verkkotunnuksen sopeutumisen ongelmaa neurokonekäännöksessä, kun verkkotunnuskohtaista tietoa ei voida jakaa luottamuksellisuuden tai tekijänoikeuden vuoksi. Ensimmäisenä askeleena ehdotamme datan pirstoamista lausepareiksi ja satunnaisen näytteen avulla tarkennetaan yleinen NMT-malli täydellisten lauseiden sijaan. Huolimatta pitkien segmenttien menetyksestä luottamuksellisuuden turvaamiseksi toteamme, että NMT-laatu voi hyötyä huomattavasti tästä mukautuksesta ja että lisähyötyjä voidaan saavuttaa yksinkertaisella merkintätekniikalla.', 'ca': 'We study the problem of domain adaptation in Neural Machine Translation (NMT) when domain-specific data cannot be shared due to confidentiality or copyright issues.  Com a primer pas, proposem fragmentar les dades en parelles de frases i utilitzar una mostra aleatòria per ajustar un model NMT genèric en comptes de les frases completas. Despite the loss of long segments for the sake of confidentiality protection, we find that NMT quality can considerably benefit from this adaptation, and that further gains can be obtained with a simple tagging technique.', 'et': 'Uurime domeeni kohandamise probleemi neuromasintõlkes, kui domeenispetsiifilisi andmeid ei saa jagada konfidentsiaalsuse või autoriõiguse tõttu. Esimese sammuna teeme ettepaneku fragmenteerida andmed fraasipaarideks ja kasutada juhuslikku valimit üldise NMT mudeli täpsustamiseks täislausete asemel. Hoolimata pikkade segmentide kadumisest konfidentsiaalsuse kaitse huvides leiame, et NMT kvaliteet võib sellest kohandamisest märkimisväärselt kasu saada ja et lihtsa märgistamise meetodiga saab täiendavat kasu.', 'jv': 'We read the question of domain modification in Neral Masine translation (NMT) when domain-special data can be shared Dulah to confidity or Copyright points. Sampeyan pangan sing sampeyan, kita supoyo nggunakake data karo perusahaan seneng sampeyan kawit sampeyan anyar sampeyan kanggo nambah akeh model NMT nganggo perusahaan mulai Nanging kabèh perusahaan segment luwih dumadhi kanggo keamanan perusahaan sampeyan, awak dhéwé nggawe kalitas NMT iso nggawe barang apik iki, lan iki sampeyan luwih apik dhéwé iso nggawe sampeyan ngono teknik sing apik dhéwé.', 'sk': 'Proučujemo problem prilagajanja domen v nevralnem strojnem prevajanju (NMT), kadar podatkov, specifičnih za domeno, ni mogoče deliti zaradi zaupnosti ali avtorskih pravic. Kot prvi korak predlagamo razdrobitev podatkov v frazne pare in uporabo naključnega vzorca za natančno nastavitev generičnega NMT modela namesto celotnih stavkov. Kljub izgubi dolgih segmentov zaradi varovanja zaupnosti ugotavljamo, da lahko kakovost NMT bistveno koristi od te prilagoditve in da je mogoče dodatne koristi doseči s preprosto tehniko označevanja.', 'ha': "Munã karanta matsalar da adaptanci cikin Tarjima na Kikakci na Neural (NMT) idan ba za'a iya shirin data-na-ɗabi'a ba dõmin ƙarani ko masu sakar da hakki na ƙarani. As a first step, we propose to fragment data into phrase pairs and use a random sample to fine-tune a generic NMT model instead of the full sentences.  Inã rantsuwa da hasara tsawo wa tsarin tsari ga siri, sai mu gane cewa, ma'anar NMT yana iya amfani da mai girma daga wannan adadi, kuma kan an iya sãmu mafiya ƙari da zance mai sauƙi.", 'bo': 'We study the problem of domain adaptation in Neural Machine Translation (NMT) when domain-specific data cannot be shared due to confidentiality or copyright issues. ང་ཚོས་ཐོག་མའི་གྲལ་ཐེངས་དང་པོ་ཞིག་གིས་བརྗོད་ཀྱི་ཆ་ཤས་ཆུང་ཆུང་ནང་གི་སྔོན་སྒྲིག་པའི་དཔེ་དབྱིབས་ཞིག་བེད་སྤྱོད་རྒྱུ་བཤད་མ་ རྒྱ་ཆེ་མཐོང་སྐྱོང་བརྗོད་ཀྱི་ཆེད་དུ་རྐྱེན་པ་དེ་ཡིན་ནའང་ངེད་ཚོར་ཉེན་བརྗོད་པར། NMT་སྒྲིག་འགོད་འདིས་མཐུན་རྐྱེན་ཚད་ཉེན་ཁ་ཡོད་ཤས་ཆ', 'he': 'אנו לומדים את הבעיה של התאמה לתחום בתרגום מכונות נוירואליות (NMT) כאשר נתונים ספציפיים לתחום אינם יכולים לחלוק בגלל סודיות או בעיות זכויות עופר. כצעד ראשון, אנו מציעים לחלק נתונים לזוגות ביטויים ולהשתמש בדגימה אקראית כדי להתאים מודל NMT גנרלי במקום המשפטים המלאים. למרות האובדן של חלקים ארוכים למען הגנה על הסודיות, אנו מוצאים שאיכות NMT יכולה להרוויח באופן משמעותי מההתאמה הזו, ושיתרונות נוספות יכולות להשיג עם טכניקת תג פשוטה.'}
{'en': 'Private Text Classification with Convolutional Neural Networks', 'ar': 'تصنيف النص الخاص مع الشبكات العصبية التلافيفية', 'es': 'Clasificación de textos privados con redes neuronales convolucionales', 'fr': 'Classification de texte privé avec réseaux de neurones convolutifs', 'pt': 'Classificação de texto privado com redes neurais convolucionais', 'zh': '用卷积神经网络分私文本', 'ja': '畳み込みニューラルネットワークによるプライベートテキスト分類', 'hi': 'Convolutional तंत्रिका नेटवर्क के साथ निजी पाठ वर्गीकरण', 'ru': 'Частная текстовая классификация со сверточными нейронными сетями', 'ga': 'Aicmiú Téacs Príobháideach le Líonraí Néaracha Comhráiteacha', 'el': 'Ταξινόμηση ιδιωτικού κειμένου με τα εξελικτικά νευρωνικά δίκτυα', 'ka': 'Comment', 'hu': 'Privát szövegosztályozás konvolúciós ideghálózatokkal', 'lt': 'Privatus teksto klasifikavimas su konvoluciniais nerviniais tinklais', 'kk': 'Жеке мәтінді қатынасыз невралдық желілерден классификациялау', 'ms': 'Private Text Classification with Convolutional Neural Networks', 'it': 'Classificazione privata del testo con reti neurali convoluzionali', 'mt': 'Klassifikazzjoni tat-Test Privat b’Netwerks Newrali Konveoluzzjonali', 'mn': 'Хувийн Текст Классификация', 'pl': 'Klasyfikacja tekstu prywatnego z konwolucyjnymi sieciami neuronowymi', 'mk': 'Private Text Classification with Convolutional Neural Networks', 'ml': 'സ്വകാര്യ പദാവലി ക്ലാസിഷന്\u200d നെയൂറല്\u200d നെറ്റ്വര്\u200dക്കുകളോടൊപ്പം', 'sr': 'Klasifikacija privatnog teksta sa konvolucionalnim nervnim mrežama', 'so': 'Private Text Classification with Convolutional Neural Networks', 'si': 'පෞද්ගලික පාළුවන් ක්\u200dලාසික්ෂණය සමග සාමාන්\u200dය නිර්මාණ ජාලය', 'no': 'Privat tekstklassifikasjon med konvolusjonelle neiralnettverk', 'ro': 'Clasificarea privată a textului cu rețele neuronale convoluționale', 'ta': 'Comment', 'sv': 'Privat textklassificering med konvulutionella neurala nätverk', 'ur': 'خصوصی ٹیکسٹ کلاسیفٹ کنویرول نیورال نیٹورک کے ساتھ', 'uz': 'Name', 'vi': 'KCharselect unicode block name', 'bg': 'Класификация на частния текст с конвелуционни неврални мрежи', 'nl': 'Particuliere tekstclassificatie met convolutionele neurale netwerken', 'hr': 'Klasificija privatnog teksta sa konvolucionalnim neuronskim mrežama', 'da': 'Privat tekstklassifikation med konvulutionsneurale netværk', 'de': 'Private Textklassifizierung mit konvevolutionalen neuronalen Netzen', 'id': 'Private Text Classification with Convolutional Neural Networks', 'ko': '권적 신경 네트워크 기반의 비밀 텍스트 분류', 'fa': 'کلاس متن خصوصی با شبکه\u200cهای عصبی معمولی', 'sw': 'Makala binafsi yenye Mtandao wa Neurali', 'tr': 'Çaltylyk Jawanlar bilen Hususiy Metin Sınımlandyrma', 'sq': 'Klasifikimi i tekstit privat me rrjete nervore konvolutive', 'af': 'Privaat Teks Klassifikasie met Konvolusionele Neurale Netwerke', 'am': 'iCalImp', 'hy': 'Comment', 'az': 'Konvolucional Nöral Ağları ilə Özel Metin Sınıflaması', 'bn': 'Name', 'bs': 'Klasifikacija privatnog teksta sa konvolucionalnim neuronskim mrežama', 'et': 'Privaatteksti klassifitseerimine konvolutsiooniliste neuraalsete võrkudega', 'cs': 'Klasifikace soukromého textu s konvelučními neuronovými sítěmi', 'ca': 'Classificació de text privat amb xarxes neuronals convolucionals', 'fi': 'Yksityisen tekstin luokittelu konvolutionaalisilla hermoverkoilla', 'ha': 'Private Text Classification with Convolutional Neural Networks', 'jv': 'iCalImp', 'sk': 'Klasifikacija zasebnega besedila s konvolucijskimi živčnimi omrežji', 'bo': 'སྤྱིར་བཏང་བའི་དཔེ་དབྱིབས་དྲ་བ་དང་སྒེར་གྱི་ཡི་གེའི་དབྱེ་རིགས', 'he': 'מסווג טקסט פרטי עם רשתות עצביות משתנות'}
{'en': 'Text classifiers are regularly applied to personal texts, leaving users of these classifiers vulnerable to privacy breaches. We propose a solution for privacy-preserving text classification that is based on Convolutional Neural Networks (CNNs) and Secure Multiparty Computation (MPC). Our method enables the inference of a class label for a personal text in such a way that (1) the owner of the personal text does not have to disclose their text to anyone in an unencrypted manner, and (2) the owner of the text classifier does not have to reveal the trained model parameters to the text owner or to anyone else. To demonstrate the feasibility of our protocol for practical private text classification, we implemented it in the PyTorch-based MPC framework CrypTen, using a well-known additive secret sharing scheme in the honest-but-curious setting. We test the runtime of our privacy-preserving text classifier, which is fast enough to be used in practice.', 'ar': 'يتم تطبيق مصنفات النص بانتظام على النصوص الشخصية ، مما يجعل مستخدمي هذه المصنفات عرضة لانتهاكات الخصوصية. نقترح حلاً لتصنيف النص للحفاظ على الخصوصية والذي يعتمد على الشبكات العصبية التلافيفية (CNN) والحساب الآمن متعدد الأطراف (MPC). تمكّن طريقتنا من استنتاج ملصق فئة لنص شخصي بطريقة (1) لا يضطر مالك النص الشخصي إلى الكشف عن نصه لأي شخص بطريقة غير مشفرة ، و (2) مالك النص لا يتعين على المصنف الكشف عن معلمات النموذج المدربة لمالك النص أو لأي شخص آخر. لإثبات جدوى بروتوكولنا الخاص بالتصنيف العملي للنصوص الخاصة ، قمنا بتطبيقه في إطار عمل MPC المستند إلى PyTorch CrypTen ، باستخدام نظام مشاركة سرية مضافة مشهور في بيئة صادقة ولكن مثيرة للفضول. نحن نختبر وقت تشغيل مصنف النص الذي يحافظ على الخصوصية ، وهو سريع بما يكفي لاستخدامه في الممارسة العملية.', 'fr': "Les classificateurs de texte sont régulièrement appliqués à des textes personnels, ce qui rend les utilisateurs de ces classificateurs vulnérables aux atteintes à la vie privée. Nous proposons une solution de classification de texte préservant la confidentialité qui est basée sur les réseaux de neurones convolutifs (CNN) et le calcul multipartite sécurisé (MPC). Notre procédé permet l'inférence d'une étiquette de classe pour un texte personnel de telle sorte que (1) le propriétaire du texte personnel n'a pas à divulguer son texte à quiconque de manière non cryptée, et (2) le propriétaire du classificateur de texte n'a pas à révéler les paramètres du modèle appris au propriétaire du texte ou à quelqu'un d'autre. Pour démontrer la faisabilité de notre protocole pour une classification pratique de texte privé, nous l'avons implémenté dans le framework MPC CryptEN basé sur PyTorch, en utilisant un schéma de partage de secret additif bien connu dans le cadre le plus honnête mais curieux. Nous testons le temps d'exécution de notre classificateur de texte préservant la confidentialité, qui est suffisamment rapide pour être utilisé en pratique.", 'pt': 'Classificadores de texto são aplicados regularmente a textos pessoais, deixando os usuários desses classificadores vulneráveis a violações de privacidade. Propomos uma solução para classificação de texto com preservação de privacidade baseada em Redes Neurais Convolucionais (CNNs) e Computação Multipartidária Segura (MPC). Nosso método permite a inferência de um rótulo de classe para um texto pessoal de forma que (1) o proprietário do texto pessoal não precise divulgar seu texto a ninguém de maneira não criptografada e (2) o proprietário do texto classificador não precisa revelar os parâmetros do modelo treinado para o proprietário do texto ou para qualquer outra pessoa. Para demonstrar a viabilidade de nosso protocolo para classificação prática de texto privado, nós o implementamos na estrutura MPC baseada em PyTorch CrypTen, usando um esquema de compartilhamento de segredo aditivo bem conhecido no cenário honesto, mas curioso. Testamos o tempo de execução do nosso classificador de texto com preservação de privacidade, que é rápido o suficiente para ser usado na prática.', 'es': 'Los clasificadores de texto se aplican regularmente a los textos personales, lo que deja a los usuarios de estos clasificadores vulnerables a las violaciones de la privacidad. Proponemos una solución para la clasificación de textos que preserva la privacidad que se basa en redes neuronales convolucionales (CNN) y computación multipartita segura (MPC). Nuestro método permite la inferencia de una etiqueta de clase para un texto personal de tal manera que (1) el propietario del texto personal no tiene que divulgar su texto a nadie sin cifrar, y (2) el propietario del clasificador de texto no tiene que revelar los parámetros del modelo entrenado al propietario del texto ni a cualquier otra persona. Para demostrar la viabilidad de nuestro protocolo para la clasificación práctica de textos privados, lo implementamos en el marco de MPC Crypten basado en PyTorch, utilizando un conocido esquema de intercambio de secretos aditivos en el entorno honesto pero curioso. Probamos el tiempo de ejecución de nuestro clasificador de texto que preserva la privacidad, que es lo suficientemente rápido como para usarlo en la práctica.', 'ja': 'テキスト分類子は個人のテキストに定期的に適用されるため、これらの分類子のユーザーはプライバシー侵害に脆弱です。畳み込みニューラルネットワーク（ CNN ）とセキュアマルチパーティコンピューテーション（ MPC ）に基づいたプライバシー保護テキスト分類のソリューションを提案します。当社の方法は、(1)個人テキストの所有者が暗号化されていない方法でテキストを誰にも開示する必要がなく、(2)テキスト分類子の所有者が訓練されたモデルパラメータをテキスト所有者または他の誰にも明らかにする必要がないような方法で、個人テキストのクラスラベルの推論を可能にします。実用的なプライベートテキスト分類のためのプロトコルの実現可能性を実証するために、私たちはPyTorchベースのMPCフレームワークCrypTenで、正直でありながら好奇心旺盛な環境で有名な添加物秘密共有スキームを使用して実装しました。プライバシー保護テキスト分類子の実行時間をテストします。これは、実際に使用するのに十分な速さです。', 'ru': 'Текстовые классификаторы регулярно применяются к личным текстам, что делает пользователей этих классификаторов уязвимыми к нарушениям конфиденциальности. Мы предлагаем решение для сохранения конфиденциальности классификации текста, которое основано на сверточных нейронных сетях (CNN) и безопасных многопартийных вычислениях (MPC). Наш метод позволяет сделать вывод о метке класса для личного текста таким образом, что (1) владелец личного текста не должен раскрывать свой текст никому в незашифрованном виде, и (2) владелец классификатора текста не должен раскрывать обученные параметры модели владельцу текста или кому-либо еще. Чтобы продемонстрировать осуществимость нашего протокола для практической частной классификации текста, мы реализовали его в MPC-фреймворке CrypTen на базе PyTorch, используя хорошо известную схему аддитивного секретного обмена в честной, но любопытной обстановке. Мы тестируем время работы нашего сохраняющего конфиденциальность текстового классификатора, который достаточно быстрый, чтобы его можно было использовать на практике.', 'hi': 'पाठ क्लासिफायर नियमित रूप से व्यक्तिगत ग्रंथों पर लागू होते हैं, जिससे इन क्लासिफायरों के उपयोगकर्ताओं को गोपनीयता उल्लंघनों के लिए कमजोर छोड़ दिया जाता है। हम गोपनीयता-संरक्षण पाठ वर्गीकरण के लिए एक समाधान का प्रस्ताव करते हैं जो Convolutional Neural Networks (CNN) और Secure Multiparty Computation (MPC) पर आधारित है। हमारी विधि एक व्यक्तिगत पाठ के लिए एक वर्ग लेबल के अनुमान को इस तरह से सक्षम बनाती है कि (1) व्यक्तिगत पाठ के मालिक को अपने पाठ को अनएन्क्रिप्टेड तरीके से किसी को भी प्रकट करने की आवश्यकता नहीं है, और (2) पाठ क्लासिफायर के मालिक को पाठ के मालिक को या किसी और को प्रशिक्षित मॉडल पैरामीटर प्रकट करने की आवश्यकता नहीं है। व्यावहारिक निजी पाठ वर्गीकरण के लिए हमारे प्रोटोकॉल की व्यवहार्यता का प्रदर्शन करने के लिए, हमने इसे PyTorch-आधारित MPC फ्रेमवर्क CrypTen में लागू किया, ईमानदार-लेकिन-जिज्ञासु सेटिंग में एक प्रसिद्ध additive गुप्त साझाकरण योजना का उपयोग करके। हम अपने गोपनीयता-संरक्षण पाठ क्लासिफायर के रनटाइम का परीक्षण करते हैं, जो व्यवहार में उपयोग किए जाने के लिए पर्याप्त तेज़ है।', 'zh': '文本分类器期用于人文本,使其用户易于私泄。 建言卷积神经网络 (CNN) 安多方计 (MPC) 私护文本解决方案。 吾法能以此推其类:(1)人文本者所有者不必以未加密向何人披露其本,并(2)文本分类器所有者不必向文本所有者或他人透露参数。 证吾协议于实私文本分类之可行性,吾于PyTorch之MPC框架CrypTen中得之,于诚而用之于众所周知者而密共之。 试文本分类器行,分类器足疾,可在实践中用。', 'ga': "Cuirtear aicmitheoirí téacs i bhfeidhm go rialta ar théacsanna pearsanta, rud a fhágann úsáideoirí na n-aicmitheoirí seo i mbaol sáruithe príobháideachta. Molaimid réiteach maidir le haicmiú téacs a chaomhnaíonn príobháideacht atá bunaithe ar Líonraí Néaracha Comhdhlúite (CNNanna) agus ar Ríomh Ilpháirtí Slán (MPC). Cumasaíonn ár modh tátal a dhéanamh ar lipéad ranga do théacs pearsanta ar bhealach (1) nach gcaithfidh úinéir an téacs pearsanta a dtéacs a nochtadh do dhuine ar bith ar bhealach neamhchriptithe, agus (2) úinéir an téacs ní chaithfidh an t-aicmitheoir paraiméadair na samhla oilte a nochtadh d'úinéir an téacs nó d'aon duine eile. Chun féidearthacht ár bprótacail maidir le haicmiú téacs príobháideach praiticiúil a léiriú, chuireamar i bhfeidhm é i gcreat MPC PyTorch-bhunaithe CrypTen, ag baint úsáide as scéim comhroinnte rúnda breiseáin a bhfuil aithne mhaith air sa suíomh macánta-ach-aisteach. Déanaimid tástáil ar am rite ár n-aicmitheoir téacs a chaomhnaíonn príobháideacht, atá tapa go leor le húsáid go praiticiúil.", 'hu': 'A szövegosztályozókat rendszeresen alkalmazzák a személyes szövegekre, így az ilyen osztályozók felhasználóit sebezhetővé teszik a magánélet megsértésének. Javasoljuk az adatvédelem megőrzését célzó szövegosztályozás megoldását, amely konvolúciós ideghálózatokon (CNN) és biztonságos többpárti számítástechnikán (MPC) alapul. Módszerünk lehetővé teszi a személyes szöveg osztálycímkéjének következtetését oly módon, hogy (1) a személyes szöveg tulajdonosának nem kell titkosítatlan módon közzétennie szövegét senkinek, és (2) a szövegosztályozó tulajdonosának nem kell a képzett modellparamétereket a szövegtulajdonosnak vagy bárki másnak közölnie. Annak bizonyítására, hogy protokollunk megvalósíthatóságát a gyakorlati privát szövegosztályozásra vonatkozóan, a PyTorch-alapú MPC keretrendszerben valósítottuk meg, CrypTen, egy jól ismert additív titkos megosztási rendszer segítségével őszinte, de kíváncsi környezetben. Teszteljük adatvédelmi szövegosztályozónk futási idejét, amely elég gyors ahhoz, hogy a gyakorlatban használható legyen.', 'el': 'Οι ταξινομητές κειμένου εφαρμόζονται τακτικά σε προσωπικά κείμενα, αφήνοντας τους χρήστες αυτών ευάλωτους σε παραβιάσεις της ιδιωτικής ζωής. Προτείνουμε μια λύση για την ταξινόμηση κειμένου που διαφυλάσσει την ιδιωτικότητα, η οποία βασίζεται σε Convolutional Neural Networks (CNN) και Secure Multiparty Computing (MPC). Η μέθοδος μας επιτρέπει την εξαγωγή μιας ετικέτας κλάσης για ένα προσωπικό κείμενο με τέτοιο τρόπο ώστε (1) ο ιδιοκτήτης του προσωπικού κειμένου να μην χρειάζεται να αποκαλύψει το κείμενό του σε κανέναν χωρίς κρυπτογράφηση και (2) ο ιδιοκτήτης του ταξινομητή κειμένου να μην χρειάζεται να αποκαλύψει τις εκπαιδευμένες παραμέτρους του μοντέλου στον ιδιοκτήτη του κειμένου ή σε οποιονδήποτε άλλο. Για να καταδείξουμε τη σκοπιμότητα του πρωτοκόλλου μας για πρακτική ταξινόμηση ιδιωτικού κειμένου, το εφαρμόσαμε στο πλαίσιο με βάση το PyTorch χρησιμοποιώντας ένα γνωστό πρόσθετο μυστικό σύστημα κοινής χρήσης σε ένα ειλικρινές αλλά περίεργο περιβάλλον. Δοκιμάζουμε τον χρόνο εκτέλεσης του ταξινομητή κειμένου που διαφυλάσσει την ιδιωτικότητα, ο οποίος είναι αρκετά γρήγορος για να χρησιμοποιηθεί στην πράξη.', 'it': "I classificatori di testo vengono regolarmente applicati ai testi personali, lasciando gli utenti di questi classificatori vulnerabili alle violazioni della privacy. Proponiamo una soluzione per la classificazione dei testi che preserva la privacy basata su reti neurali convoluzionali (CNN) e Secure Multiparty Computation (MPC). Il nostro metodo consente di dedurre un'etichetta di classe per un testo personale in modo tale che (1) il proprietario del testo personale non debba rivelare il proprio testo a nessuno in modo non crittografato, e (2) il proprietario del classificatore di testo non debba rivelare i parametri del modello addestrato al proprietario del testo o a chiunque altro. Per dimostrare la fattibilità del nostro protocollo per la classificazione pratica del testo privato, l'abbiamo implementato nel framework MPC basato su PyTorch CrypTen, utilizzando un noto schema di condivisione segreta additiva in un ambiente onesto ma curioso. Testiamo il runtime del nostro classificatore di testo che preserva la privacy, che è abbastanza veloce da essere utilizzato nella pratica.", 'ka': 'ტექსტის კლასიფიკაციები პირადი ტექსტისთვის რედაქტიურად გამოყენება, რომელიც ამ კლასიფიკაციების გამოყენებელი პირადი სიცოცხლეობისთვის გადარჩენებ ჩვენ გვეძლევა პირველური სისტემის კლასიფიკაციისთვის გარეშე, რომელიც კონტუალური ნეიროლური ქსელების (CNNs) და მულტიპოტების კომპუტაციისთვის დაბაზეულია. ჩვენი მეტი შესაძლებელია პირადი ტექსტისთვის კლასის ნიშნალის ინფრენცია, როგორც (1) პირადი ტექსტის მიმართელი არ უნდა ტექსტის კონფიგურაციას ვინმეს უნდა გახსნა, და (2) ტექსტის კლასიფიგურაციის მიმართელი არ უნდა ტექსტის მიმარ რომ ჩვენი პროტოკოლას პრაქტიკური პირადი ტექსტის კლასიფიკაციისთვის შესაძლებლობას გაჩვენეთ, ჩვენ მისი პირადი MPC ფრამეტრის კრიპტენში გამოყენეთ, რომელიც გამოიყენეთ უცნობილი დამატებული შინის გაყოფილი სქე ჩვენ ტესტის კლასიფიკაციაში ჩვენი პირადი სიცოცხლეობის შესახებ ტექსტის კლასიფიკაციას შევცვალობთ, რომელიც ძალიან ბრძელია, რომ გამოყენება პ', 'kk': 'Мәтін классификациясы жеке мәтіндерге әдетте қолданылады, бұл классификациялардың пайдаланушыларын жеке тәуелсіздік қалдырып қалдырып, жеке тәуелсіздік Біз жеке сақтау мәтін классификациясының шешімін ұсынамыз. Бұл Конvolutional Neural Networks (CNNs) және Қауіпсіздік Мульпартиялық компьютерлер (MPC) негізінде негізделген. Біздің әдіміміз жеке мәтін үшін класс жарлығының көшірмесін рұқсат ету үшін (1) жеке мәтіннің иесі мәтінді шифрланбаған түрде келген жоқ, және (2) мәтін классификациясының иесі мәтін иесіне не басқаларға келген үлгі параметрлерін көрсету керек Практикалық жеке мәтін классификациясының протоколының мүмкіндігін көрсету үшін оны PyTorch-негіздеген MPC фреймінде CrypTen қолдандық. Біз өте қызықты, білмейтін қосымша қауіпсіздік ортақтастыру сұлбасын қолданып,  Мәтін классификациясының жеке сақтау уақытын тексереміз. Бұл әрекетте қолданылатын жерде тез.', 'ms': 'Pengklasifikasi teks dilaksanakan secara biasa pada teks peribadi, meninggalkan pengguna pengklasifikasi ini rentan terhadap pelanggaran privasi. We propose a solution for privacy-preserving text classification that is based on Convolutional Neural Networks (CNNs) and Secure Multiparty Computation (MPC).  Our method enables the inference of a class label for a personal text in such a way that (1) the owner of the personal text does not have to disclose their text to anyone in an unencrypted manner, and (2) the owner of the text classifier does not have to reveal the trained model parameters to the text owner or to anyone else.  Untuk menunjukkan kemudahan protokol kita untuk klasifikasi teks peribadi praktik, kami melaksanakannya dalam kerangka MPC berdasarkan PyTorch CrypTen, menggunakan skema berkongsi rahsia aditif yang diketahui dalam seting jujur-tetapi-penasaran. Kami menguji masa berjalan bagi pengklasifikasi teks yang memelihara privasi kami, yang cukup cepat untuk digunakan dalam latihan.', 'lt': 'Tekstų klasifikatoriai reguliariai taikomi asmeniniams tekstams, todėl šių klasifikatorių naudotojai yra pažeidžiami privatumo pažeidimais. Siūlome išsaugoti privatumo išsaugojimo tekstų klasifikaciją, grindžiamą konvoliuciniais nerviniais tinklais (CNN) ir saugiu daugiašaliu skaičiavimu (MPC). Mūsų metodas leidžia daryti asmeninio teksto klasės etiketę tokiu būdu, kad (1) asmeninio teksto savininkas neturi atskleisti savo teksto niekam be šifravimo, o (2) teksto klasifikatoriaus savininkas neturi atskleisti mokomų modelio parametrų teksto savininkui arba niekam kitam. Siekdami įrodyti, kad mūsų protokolas dėl praktinio privačiojo teksto klasifikavimo įmanomas, mes jį įgyvendinome PyTorch pagrindu pagrįstoje MPC sistemoje CrypTen, naudojant gerai žinomą priedų slapto pasidalijimo sistemą sąžiningomis, bet įdomomis aplinkybėmis. Bandome savo privatumo išsaugojimo teksto klasifikatoriaus veikimo laiką, kuris yra pakankamai greitas, kad būtų naudojamas praktikoje.', 'mt': 'Il-klassifikaturi tat-test jiġu applikati regolarment għal testi personali, u l-utenti ta’ dawn il-klassifikaturi jitħallew vulnerabbli għal ksur tal-privatezza. Aħna nipproponu soluzzjoni għall-klassifikazzjoni tat-test li tippreserva l-privatezza li hija bbażata fuq Netwerks Newrali Konveoluzzjonali (CNNs) u Komputazzjoni Multipartita Sigura (MPC). Il-metodu tagħna jippermetti l-inferenza ta’ tikketta tal-klassi għal test personali b’tali mod li (1) is-sid tat-test personali ma jkollux għalfejn jiżvela t-test tiegħu lil ħadd b’mod mhux ikkriptat, u (2) is-sid tal-klassifikatur tat-test ma jkollux għalfejn jiżvela l-parametri tal-mudell imħarreġ lis-sid tat-test jew lil ħadd ieħor. Biex nippruvaw il-fattibbiltà tal-protokoll tagħna għall-klassifikazzjoni prattika tat-test privat, implimentajna fil-qafas tal-MPC CrypTen ibbażat fuq PyTorch, bl-użu ta’ skema ta’ kondiviżjoni sigrieta ta’ addittivi magħrufa sew fl-ambjent onest iżda kurjuż. Aħna ntestjaw il-ħin li bih in ħaddmu l-klassifikatur tat-test tagħna li jippreserva l-privatezza, li huwa mgħaġġel biżżejjed biex jintuża fil-prattika.', 'ml': 'പദാവലി വിശദീകരിക്കുന്നവര്\u200d വ്യക്തിപരമായ പദാവലികള്\u200dക്ക് പ്രയോഗിക്കുന്നു. ഈ വ്യക്തിപരമാക്കുന്നവരുടെ ഉപയോക്താക്കളെ സ് We propose a solution for privacy-preserving text classification that is based on Convolutional Neural Networks (CNNs) and Secure Multiparty Computation (MPC).  വ്യക്തിപരമായ പദാവലിയുടെ (1) ഉടമസ്ഥന്\u200d അവയുടെ പദാവലിയെ രഹസ്യമാക്കാത്ത രീതിയില്\u200d വെളിപ്പെടുത്തിക്കൊടുക്കേണ്ട രീതിയില്\u200d നമ്മുടെ രീതിയിലുള്ള വ്യക്തിപ്പെടുത്തേണ്ടതില്ല. ടെക നമ്മുടെ പ്രാകൃതിക വിവരങ്ങളുടെ സ്വകാര്യ വിവരങ്ങള്\u200dക്കുള്ള പ്രൊട്ടോളിന്റെ സ്വകാര്യ വിവരങ്ങള്\u200dക്കുള്ള സ്വകാര്യം പ്രകടമാക്കാന്\u200d വേണ്ടി ഞങ്ങള്\u200d അത് പിയ് നമ്മുടെ സ്വകാര്യ-സൂക്ഷിക്കുന്ന പദാവലി ക്ലാസ്ഫിക്കറിന്റെ റൌണ്\u200dടൈറ്റ് സമയം ഞങ്ങള്\u200d പരീക്ഷിക്കുന്നു. പ്', 'mn': 'Текст хуваарилагчид хувийн бичиг дээр ихэвчлэн хэрэглэгддэг, хувийн амьдралын хувьд эмзэг байдаг. Бид хувийн амьдралын хамгаалах текст хэлбэрийн шийдлийг санал дэвшүүлнэ. Энэ нь Convolutional Neural Networks (CNNs) болон Secure Multiparty Computation (MPC) дээр суурилсан. Бидний арга нь хувийн текстийн хувьд хичээл загварын загварын инфекцийг ашиглаж чадна. Мөн (1) хувийн текстийн хувьд текстийг шинжилгээгүй хэн нэгэнд илэрхийлж чадахгүй. Мөн (2) текстийн хувьд хувийн загварын хувьд сургалтын загварын параметрыг текстийн хувьд эсвэл бусад хү Үнэндээ бидний хувийн текст хуваалтын протоколын боломжтойг харуулахын тулд бид үүнийг PyTorch-д суурилсан MPC хэлбэрээр CrypTen-д ашиглаж, нэмэлт нэмэлт нууц хуваалцах төлбөр ашиглаж байлаа. Бид хувийн амьдралын хамгаалах үеийн хугацааны туршилтыг шалгана. Энэ нь дасгал дээр хэрэглэгдэх хангалттай хурдан.', 'no': 'Tekstklassifikatorar vert regulært bruka til personlege tekstar, og brukarar av desse klassifikatorane vert sårbare til privatsforskjeller. Vi foreslår eit løysing for tekstklassifikasjon som er basert på konvolusjonelle neiralnettverk (CNNs) og sikre multipartitsdatamaskin (MPC). Metoden vårt slår på å sletta ein klassemerkelapp for ei personleg tekst slik at (1) eigaren av den personlege teksten ikkje må opna teksten til nokon på eit ukryptert måte, og (2) eigaren av tekstklassifiseren må ikkje opna dei trengte modeller til teksteigaren eller til nokon anna. For å demonstrere feiligheten til vårt protokoll for praktisk privat tekstklassifikasjon, implementerte vi det i PyTorch-basert MPC-rammeverket CrypTen, ved å bruka ein godt kjent ekstra hemmeleg delingsskjema i den ærlige, men curious innstillinga. Vi tester køyringsdata til tekstklassifiseringen vår for privat, som er rask nok for å brukast i praksis.', 'pl': 'Klasyfikatory tekstowe są regularnie stosowane do tekstów osobistych, co pozostawia użytkowników tych klasyfikatorów narażonych na naruszenia prywatności. Proponujemy rozwiązanie do klasyfikacji tekstu zachowującego prywatność oparte na Convolutional Neuroral Networks (CNN) oraz Secure Multiparty Computing (MPC). Nasza metoda umożliwia wnioskowanie etykiety klasowej dla tekstu osobistego w taki sposób, że (1) właściciel tekstu osobistego nie musi nikomu ujawniać swojego tekstu w sposób nieszyfrowany, a (2) właściciel klasyfikatora tekstu nie musi ujawniać przeszkolonych parametrów modelu właścicielowi tekstu ani nikomu innemu. Aby zademonstrować wykonalność naszego protokołu do praktycznej klasyfikacji tekstów prywatnych, wdrożyliśmy go w ramach MPC opartym na PyTorch CrypTen, wykorzystując dobrze znany dodatkowy schemat sekretnego udostępniania w szczerym, ale ciekawym otoczeniu. Testujemy czas działania naszego klasyfikatora tekstu chroniącego prywatność, który jest wystarczająco szybki, aby zostać wykorzystany w praktyce.', 'si': 'පුද්ගලික පණිවිඩයට පුද්ගලික පණිවිඩයට සාමාන්\u200dයයෙන් ප්\u200dරයෝජනය කරනවා, මේ පණිවිඩයට පුද්ගලික විර අපි පෞද්ගලික විශේෂතාවක් ආරක්ෂා කරන්න පාළුවක් විශේෂණය සඳහා ප්\u200dරතිකෘති විශේෂණය සඳහා ප්\u200dරතිකෘති විශේෂණයක් ස අපේ විද්\u200dයාව සක්\u200dරිය කරන්නේ පුද්ගලික පාළුවක් ලේබල් සඳහා පුද්ගලික පාළුවක් සඳහා ප්\u200dරතික්\u200dරියාත්මක පාළුවෙන් ඔවුන්ගේ පාළුවක් කිසිම කෙනෙක්ට ප්\u200dරතික්\u200dරියාත්මක වි අපේ ප්\u200dරොටොකොල් එකේ ප්\u200dරායෝක්ෂික පුද්ගලික පණිවිධානය සඳහා ප්\u200dරතික්\u200dරියාත්මක විශේෂතාවක් පෙන්වන්න, අපි ඒක PyTorch-අධාරිත MPC පණිවිධාන අපි පරීක්ෂා කරනවා අපේ පෞද්ගලිකතාවයේ පරික්ෂා කරනවා පාළුවන් පරික්ෂා කරනවා, ඒක ප්\u200dරයෝජනයේ පාවිච', 'ro': 'Clasificatorii de text sunt aplicați în mod regulat textelor personale, lăsând utilizatorii acestor clasificatori vulnerabili la încălcările confidențialității. Propunem o soluție pentru clasificarea textelor care păstrează confidențialitatea, bazată pe Rețele Neurale Convoluționale (CNN) și Secure Multiparty Computation (MPC). Metoda noastră permite deducerea unei etichete de clasă pentru un text personal astfel încât (1) proprietarul textului personal să nu fie obligat să dezvăluie textul său nimănui într-o manieră necriptată și (2) proprietarul clasificatorului de text să nu fie obligat să dezvăluie parametrii modelului instruit proprietarului textului sau altcuiva. Pentru a demonstra fezabilitatea protocolului nostru pentru clasificarea practică a textelor private, l-am implementat în cadrul MPC bazat pe PyTorch CrypTen, folosind o binecunoscută schemă de partajare secretă aditivă în cadrul onest, dar curios. Testăm timpul de funcționare al clasificatorului nostru de text care protejează confidențialitatea, care este suficient de rapid pentru a fi utilizat în practică.', 'sv': 'Textklassificerare tillämpas regelbundet på personliga texter, vilket gör användare av dessa klassificerare sårbara för integritetsbrott. Vi föreslår en lösning för sekretessbevarande textklassificering som baseras på konvulutionella neurala nätverk (CNN) och Secure Multiparty Computing (MPC). Vår metod gör det möjligt att dra slutsatsen av en klassetikett för en personlig text på ett sådant sätt att (1) ägaren av den personliga texten inte behöver avslöja sin text för någon på ett okrypterat sätt, och (2) ägaren av textklassificeraren inte behöver avslöja de utbildade modellparametrarna för textägaren eller någon annan. För att demonstrera genomförbarheten av vårt protokoll för praktisk privat textklassificering implementerade vi det i PyTorch-baserade MPC-ramverket CrypTen, med hjälp av ett välkänt additivt hemligt delningssystem i ärlig men nyfiken miljö. Vi testar körtiden för vår sekretessbevarande textklassificerare, som är tillräckligt snabb för att användas i praktiken.', 'so': 'Dadka fasaxa-qoraalka ah waxey si joogto ah u codsadaan qoraalka shakhsiyeed, waxayna isticmaalayaan isticmaalayaashaas si ay u burburaan burburka gaarka ah. We propose a solution for privacy-preserving text classification that is based on Convolutional Neural Networks (CNNs) and Secure Multiparty Computation (MPC).  Qoraalkayaga waxaa loola jeedin karaa qofka leh qoraal macluumaad shakhsi ah, sida (1) qofka leh qoraalka shakhsiyeed uma baahna in uu warqadooda u sheego qofna si a an loo qorin, wuxuuna leeyahay (2) qofka qoraalka fasaxda u qoran uma baahna in uu u muujiyo parameters tababar ah oo uu u qoray qoraalka ama qof kale. Si aan u muujinno awoodda sameynta qoraalkayaga si gaarka ah ee qoraalka gaarka loo leeyahay, waxaynu ku soo dejinnay qorshaha shirkadda ee MPC ee PyTorch ku saleysan CrypTen, si aad u yaqaan qorshaha shirkadda qarsoodiga ah oo qarsoodiga ah oo aad u yaqaan, iyadoo lagu qorayo qorshaha saabsan ee ku saabsan xarunta runta ah ee la yaabo. Waxaynu imtixaannaa waqtiga uu shaqsiyaadka u sameynayo qoraalka si gaar ah, taas oo ugu dhaqsahay in lagu isticmaalo isticmaalka.', 'sr': 'Klasifikatori teksta se redovno primjenjuju na osobne tekstove, ostavljajući korisnike tih klasifikatora ranjive na poremećaje privatnosti. Predlažemo rješenje za klasifikaciju teksta za saèuvanje privatnosti koja je bazirana na konvolucionalnim neuronskim mrežama (CNNs) i sigurnom multipartijskom raèunanju (MPC). Naša metoda omogućava infekciju etikete klase za lični tekst na način da (1) vlasnik ličnog teksta ne mora nikome da otkrije svoj tekst na neošifrovan način, a (2) vlasnik klasifikatora teksta ne mora da otkrije obučene modele parametre vlasniku teksta ili bilo kome drugom. Da bismo pokazali mogućnost našeg protokola za praktičnu privatnu klasifikaciju teksta, proveli smo ga u okviru MPC CrypTen na PyTorch-u, koristeći poznatu tajnu podjelu dodatnih tajnih šema u iskrenom, ali znatiželjnom nastavu. Testiramo trenutak našeg klasifikatora privatnosti, koji je dovoljno brz da se koristi na praksi.', 'mk': 'Класификаторите на текст се редовно применуваат на лични тексти, оставајќи ги корисниците на овие класификатори ранливи на прекршувања на приватноста. We propose a solution for privacy-preserving text classification that is based on Convolutional Neural Networks (CNNs) and Secure Multiparty Computation (MPC).  Нашиот метод овозможува конференција на класична етикета за личниот текст на начин (1) сопственикот на личниот текст не мора да го открие својот текст на никого на некриптиран начин, и (2) сопственикот на текстовиот класификатор не мора да ги открие обучените параметри на моделот на сопственикот на текст или на никој друг. За да ја демонстрираме физибилитетот на нашиот протокол за практична приватна класификација на текст, го спроведовме во рамката на МПЦ заснована на PyTorch CrypTen, користејќи добро позната тајна шема за поделба на додатоци во искрено-но-љубопитно место. Го тестираме времето на враќање на нашиот класификатор за зачувување на приватноста, кој е доволно брз за да се користи во практиката.', 'ta': 'உரை வகுப்பாளர் நாம் ஒரு தனியார்மறை பாதுகாப்பான பல்கட்டி கணிப்பொறி (MPC) அடிப்படையில் உள்ள உரை வகைப்பாட்டை பாதுகாப்பு அடிப்படையாக இருக்கும் பொருட் Our method enables the inference of a class label for a personal text in such a way that (1) the owner of the personal text does not have to disclose their text to anyone in an unencrypted manner, and (2) the owner of the text classifier does not have to reveal the trained model parameters to the text owner or to anyone else.  எங்கள் தனிப்பட்ட உரை வகைப்படுத்தலுக்கான நெறிமுறையின் தனிப்பட்ட வகைப்பை காட்டுவதற்கு, நாங்கள் அதை PyTorch-அடிப்படையிலான MPC சட்டத்தை கிரிப்ட்டென்ல் செயல்படுத்த நாம் எங்கள் தனிப்பட்ட உரை வகுப்பாளரின் இயக்க நேரத்தை சோதி', 'ur': 'پاکستان کلاسیٹر کو وصیت پیغام پر قابل کاروبار کیا جاتا ہے، یہ کلاسیٹر کے استعمال کرنے والوں کو خصوصی غلط کرنے کے لئے ناکام چھوڑ دیتا ہے. ہم ایک حل پیشنهاد کریں گے خصوصی حفاظت کرنے والی ٹکس کلاسپیٹ کی جگہ جو Convolutional Neural Networks (CNNs) اور Secure Multiparty Computation (MPC) پر بنیاد ہے. ہمارا طریقہ ایک کلاس لئبل کے ذریعہ شخصی پیغام کے لئے اس طرح قابل کرتا ہے کہ (1) شخصی پیغام کے مالک کو ان کے پیغام کو بغیر سیدھی طریقہ سے کھول دینے کی ضرورت نہیں ہے اور (2) پیغام کلاسیر کے مالک کو پیغام مالک یا کسی اور کے لئے تعلیم کی مدل پارامیٹوں کو ظاہر کرنے کی ضرورت نہیں ہے۔ ہم نے اسے PyTorch-based MPC Frame CrypTen (CrypTen) میں دکھائے، ایک معلوم اضافہ مخفی شریک طریقہ کے مطابق معلوم کر رہے ہیں۔ ہم اپنے خصوصی حفاظت کرنے والی ٹیکسٹ کلیسائر کی رونٹ زمانہ کی آزمائش کرتے ہیں، جو تمرین میں استعمال کرنے کے لئے کافی سریع ہے.', 'vi': 'Những người phân loại văn bản được áp dụng thường xuyên với các văn bản cá nhân, để người dùng của những loại này dễ bị xâm phạm. Chúng tôi đề xuất một giải pháp cho việc phân loại văn bản bảo vệ sự riêng tư dựa trên mạng thần kinh có liên hệ (CNN) và bảo mật máy tính đa đảng (MPC). Phương pháp của chúng tôi cho phép ngụ ý của một nhãn hạng cho một văn bản cá nhân theo một cách mà (1) người sở hữu văn bản cá nhân không phải tiết lộ văn bản của họ cho bất kỳ ai một cách không được mã hóa, và (2) người sở hữu của người phân phối văn bản không phải tiết lộ các tham số mô hình huấn luyện cho người sở hữu văn bản hay bất kỳ ai khác. Để chứng minh khả năng của giao thức cho việc phân loại văn bản cá nhân thực tế, chúng tôi đã thực hiện nó trong bộ chế tạo MPC nhà Pych, sử dụng một chế độ bí mật nổi tiếng trong một chế độ trung thực nhưng kì lạ. Chúng tôi kiểm tra thời gian chạy của người phân loại văn bản bảo vệ sự riêng tư, đủ nhanh để được sử dụng trong thực tế.', 'uz': "Name We propose a solution for privacy-preserving text classification that is based on Convolutional Neural Networks (CNNs) and Secure Multiparty Computation (MPC).  Бизнинг усулимиз шахс матни соҳиб қилиш учун фойдаланиши мумкин, чунки (1) содиқ матни соҳиб кимга етказмаган ҳолда текст кўрсатиш лозим эмас, ва (2) тарқатувчининг хокуси матн хокуси ёки бошқа кимга ҳам ўргатилган мослар кўрсатиш лозим эмас. Mavjud shaxsiy matnni darajalashtirish uchun protokollamizning imkoniyatini koʻrsatish uchun biz buni asosida PyTorch asosiy MPC freymi CrypTenda ishga tushirib ko'rsatdik, haqiqatgina va juda yaxshi qiziqarli qarshi qolipini ishlatish mumkin. Biz Maxfiy soʻzni saqlash uchun maxfiy soʻzni sinab ko'raymiz. Bu vazifani ishlatish juda tez yetarli.", 'da': "Tekstklassificerere anvendes regelmæssigt på personlige tekster, hvilket gør brugerne af disse klassificerere sårbare over for brud på privatlivets fred. Vi foreslår en løsning til beskyttelse af personlige oplysninger tekstklassifikation, der er baseret på konvoluzionelle neurale netværk (CNN'er) og Secure Multiparty Computation (MPC). Vores metode gør det muligt at udlede en klassetiket til en personlig tekst på en sådan måde, at (1) ejeren af den personlige tekst ikke behøver at videregive deres tekst til nogen på en ukrypteret måde, og (2) ejeren af tekstklassificeringen ikke behøver at afsløre de uddannede modelparametre til tekstejeren eller nogen anden. For at demonstrere gennemførligheden af vores protokol til praktisk privat tekstklassifikation implementerede vi den i PyTorch-baserede MPC ramme CryptTen, ved hjælp af en velkendt additiv hemmelig delingsordning i den ærlige, men nysgerrige omgivelser. Vi tester drifttiden af vores privatlivsbeskyttende tekstklassificering, som er hurtig nok til at blive brugt i praksis.", 'bg': 'Текстовите класификатори се прилагат редовно към лични текстове, оставяйки потребителите на тези класификатори уязвими от нарушаване на поверителността. Предлагаме решение за запазване на поверителността на текста, което се основава на конвелуционни неврални мрежи (CNN) и защитени многопартийни изчисления (MPC). Нашият метод позволява да се направи извод на класов етикет за личен текст по такъв начин, че (1) собственикът на личния текст не трябва да разкрива своя текст на никого по некриптиран начин, а (2) собственикът на текстовия класификатор не трябва да разкрива обучените параметри на модела на собственика на текста или на никого друг. За да демонстрираме осъществимостта на нашия протокол за практическа класификация на частни текстове, ние го внедрихме в базираната на ПиТорч рамка с помощта на добре известна схема за споделяне на тайни добавки в честна, но любопитна обстановка. Тестваме времето за изпълнение на нашия текстов класификатор, който е достатъчно бърз, за да се използва на практика.', 'hr': 'Klasifikatori teksta se redovno primjenjuju na osobne tekstove, ostavljajući korisnike tih klasifikatora ranjive na poremećaje privatnosti. Mi predlažemo rješenje za klasifikaciju teksta za očuvanje privatnosti koja je temeljena na konvolucionalnim neuronskim mrežama (CNNs) i sigurnom multistranskim računalom (MPC). Naša metoda omogućava infekciju etikete klase za osobni tekst na način da (1) vlasnik osobnog teksta ne mora nikome otkrivati svoj tekst na neošifrovan način, a (2) vlasnik klasifikatora teksta ne mora otkrivati obučene modele parametre vlasniku teksta ili bilo kojem drugom. Da bismo pokazali mogućnost našeg protokola za klasifikaciju praktičnog privatnog teksta, proveli smo ga u okviru MPC CrypTen na PyTorch-u, koristeći poznatu tajnu podjelu dodatnih tajnih programa u iskrenom, ali znatiželjnom nastavu. Testiramo provod našeg klasifikatora privatnosti, koji je dovoljno brz da se koristi na praksi.', 'nl': "Tekstclassificatoren worden regelmatig toegepast op persoonlijke teksten, waardoor gebruikers van deze classificatoren kwetsbaar zijn voor privacyschendingen. Wij stellen een oplossing voor privacybehoudende tekstclassificatie voor die gebaseerd is op Convolutional Neural Networks (CNN's) en Secure Multiparty Computing (MPC). Onze methode maakt het mogelijk om een klassenlabel voor een persoonlijke tekst op zodanige wijze te concluderen dat (1) de eigenaar van de persoonlijke tekst zijn tekst aan niemand op een onversleutelde manier hoeft te onthullen en (2) de eigenaar van de tekstclassificator de getrainde modelparameters niet hoeft te onthullen aan de teksteigenaar of aan iemand anders. Om de haalbaarheid van ons protocol voor praktische private text classificatie aan te tonen, hebben we het geïmplementeerd in het PyTorch-gebaseerde MPC framework CrypTen, met behulp van een bekend additief secret sharing schema in de eerlijke, maar nieuwsgierige setting. We testen de runtime van onze privacybehorende tekstclassificator, die snel genoeg is om in de praktijk te worden gebruikt.", 'de': 'Textklassifikatoren werden regelmäßig auf persönliche Texte angewendet, wodurch Benutzer dieser Klassifikatoren anfällig für Datenschutzverletzungen sind. Wir schlagen eine Lösung zur datenschutzerhaltenden Textklassifizierung vor, die auf Convolutional Neural Networks (CNNs) und Secure Multiparty Computing (MPC) basiert. Unsere Methode ermöglicht die Ableitung eines Klassenlabels für einen persönlichen Text so, dass (1) der Inhaber des persönlichen Textes seinen Text niemandem unverschlüsselt offenlegen muss und (2) der Inhaber des Textklassifikators die trainierten Modellparameter weder dem Texteigentümer noch anderen offenlegen muss. Um die Machbarkeit unseres Protokolls für die praktische Klassifizierung privater Texte zu demonstrieren, haben wir es im PyTorch-basierten MPC-Framework CrypTen implementiert, wobei ein bekanntes additives Secret Sharing-Schema im ehrlichen, aber neugierigen Rahmen verwendet wurde. Wir testen die Laufzeit unseres datenschutzerhaltenden Textklassifikators, der schnell genug ist, um in der Praxis eingesetzt zu werden.', 'id': 'Klasifikasi teks secara regular diterapkan pada teks pribadi, meninggalkan pengguna dari klasifikasi ini rentan terhadap pelanggaran privasi. Kami mengusulkan solusi untuk klasifikasi teks yang memelihara privasi yang berdasarkan Jaringan Neural Konvelusional (CNN) dan Komputasi Multiparti Aman (MPC). Metode kami memungkinkan kesimpulan dari label kelas untuk teks pribadi dengan cara (1) pemilik teks pribadi tidak perlu mengungkapkan teks mereka kepada siapapun dengan cara yang tidak dikripsi, dan (2) pemilik klasifikasi teks tidak perlu mengungkapkan parameter model terlatih kepada pemilik teks atau orang lain. To demonstrate the feasibility of our protocol for practical private text classification, we implemented it in the PyTorch-based MPC framework CrypTen, using a well-known additive secret sharing scheme in the honest-but-curious setting.  Kami menguji waktu berjalan dari klasifikasi teks yang memelihara privasi kami, yang cukup cepat untuk digunakan dalam praktek.', 'ko': '텍스트 분류기는 개인 텍스트에 자주 적용되어 이러한 분류기의 사용자가 프라이버시 침해를 받기 쉽다.우리는 권적신경망(CNN)과 보안 다자간 계산(MPC)을 기반으로 한 프라이버시 보호 텍스트 분류 솔루션을 제시했다.우리의 방법은 이러한 방식으로 개인 텍스트의 클래스 라벨을 추정할 수 있다. (1) 개인 텍스트의 소유자는 암호화되지 않은 방식으로 누구에게도 텍스트를 공개할 필요가 없고, (2) 텍스트 분류기의 소유자는 텍스트 소유자나 다른 사람에게 훈련된 모델 파라미터를 공개할 필요가 없다.우리의 협의가 실제 개인 텍스트 분류에 사용될 가능성을 증명하기 위해 우리는 PyTorch 기반의 MPC 프레임워크인 CrypTen에서 이를 실현했고 성실하지만 궁금한 환경에서 유명한 가성 비밀 공유 방안을 사용했다.Google은 Google 프라이버시 보호 텍스트 분류기가 실행될 때 속도가 충분하여 실천에서 사용할 수 있도록 테스트했습니다.', 'sw': 'Wasambaji wa maandishi mara kwa mara huwa wanatumika kutumia ujumbe binafsi, na kuwaacha watumiaji wa wataalamu hawa wenye hatari ili kuvunja mipaka ya faragha. Tunazipendekeza suluhisho la usalama wa maandishi binafsi unaohusika na Mtandao wa Neural (CNNs) na Makutano ya Kiusalama (MPC). Utawala wetu unawezesha kupunguzwa kwa alama ya darasa kwa kutumia ujumbe binafsi kwa namna (1) mmiliki wa maandishi binafsi haina haja ya kutangaza ujumbe wake kwa mtu yeyote kwa namna isiyo na siri, na (2) mmiliki wa mwandishi wa maandishi hawana haja ya kuonyesha vipimo vilivyofundishwa kwa mmiliki wa simu za maandishi au kwa yeyote mwingine. Ili kuonyesha uwezekano wa protoko yetu kwa ajili ya kutangaza maarifa ya maandishi binafsi, tulitekeleza katika mfumo wa Mfumo wa MPC anayeishi PyTorch, kwa kutumia mpango wa kushirikiana kwa siri katika mazingira ya ukweli lakini ya kusisimua. Tunajaribu muda wa kujilinda na mwandishi wa simu za faragha, ambazo ni haraka kiasi cha kutumiwa katika mazoea.', 'af': "Teks klassifiseerders word gewoonlik op persoonlike teks toegewend, en gebruikers van hierdie klassifiseerders laat verlaat wat vulnerabele is na privateitsbrekkings. Ons voorstel 'n oplossing vir privateit-beveiliging teks klasifikasie wat gebaseer is op Konvolusionele Neurale Netwerke (CNNs) en Secure Multiparty Computation (MPC). Ons metode aktiveer die inferensie van 'n klas etiket vir 'n persoonlike teks in sodanige 'n manier dat (1) die eienaar van die persoonlike teks nie het om hul teks na enige in 'n ongeenkripteerde manier te vertoon nie, en (2) die eienaar van die teks klassifiseerder nie het na vertoon die onderrigte model parameters na die teks eienaar of na enige ander. Om die feilikheid van ons protokol te wys vir praktiese privaat teks klasifikasie, het ons dit in die PyTorch-gebaseerde MPC raamwerk CrypTen geïmplementeer, gebruik 'n goed bekende additive geheime deel skema in die eerlik-maar-nuuskierige instelling. Ons probeer die uitvoer tyd van ons privateit-bewaarde teks klassifiseerder, wat vinnig genoeg is om in praksie gebruik te word.", 'fa': 'گروه\u200cهای متن معمولاً به متن شخصی کاربرد می\u200cشوند، و کاربردهای این گروه\u200cها را به شکست\u200cهای خصوصی آسیب می\u200cگذارند. ما پیشنهاد می\u200cکنیم راه حل برای محافظت متن محافظت خصوصی که بر روی شبکه\u200cهای عصبی (CNNs) و محاسبه\u200cهای چندیپارتی امن (MPC) بنیاد دارد. روش ما به عنوان تزریق یک برچسب کلاس برای یک متن شخصی توان می\u200cدهد که (۱) صاحب متن شخصی مجبور نیست متن خود را به هیچ کس به طریق بی\u200cرمز آشکار کند، و (۲) صاحب برچسب متن مجبور نیست پارامتر مدل آموزش را به صاحب متن یا به هیچ کس دیگر آشکار کند. برای نشان دادن قابلیت پروتکل ما برای کلیسازی متن خصوصی عملی، آن را در چهارچهارچهارچهارچهارچهارچهارچهارچهار MPC CrypTen، با استفاده از یک برنامه شریک مخفی اضافه شناخته شده در تنظیمات صادق و کنجکاوی انجام دادیم. ما زمان اجرای محافظت متن محافظت خصوصی خود را آزمایش می کنیم، که به اندازه کافی سریع برای استفاده در تمرین است.', 'sq': 'Klasifikuesit e tekstit aplikohen rregullisht në tekste personale, duke lënë përdoruesit e këtyre klasifikuesve të prekshëm ndaj shkeljeve të privatësisë. Ne propozojmë një zgjidhje për klasifikimin e tekstit për ruajtjen e privatësisë që bazohet në Rrjetet Neurale Konveolutive (CNNs) dhe Kompjutimin e Sigurt Multiparti (MPC). Metoda jonë mundëson përfundimin e një etikete klase për një tekst personal në një mënyrë që (1) pronari i tekstit personal nuk duhet të zbulojë tekstin e tyre askujt në një mënyrë të paskriptuar dhe (2) pronari i klasifikuesit të tekstit nuk duhet të zbulojë parametrat e modelit të trajnuar për pronarin e tekstit apo për askënd tjetër. Për të demonstruar realizueshmërinë e protokollit tonë për klasifikimin praktik të tekstit privat, ne e zbatuam atë në kuadrin e MPC me bazë në PyTorch CrypTen, duke përdorur një skemë të njohur të ndarjes së fshehtë shtesë në ambientin e ndershëm por kurioz. Ne testojmë kohën e zbatimit të klasifikuesit tonë të tekstit për ruajtjen e privatësisë, i cili është mjaft i shpejtë për të përdorur në praktikë.', 'am': 'Text classifiers are regularly applied to personal texts, leaving users of these classifiers vulnerable to privacy breaches.  በኮንፎሎጂ ኔural Network (CNNs) እና የደኅንነት ብዙኃን ፓርቲ ቁጥጥር (MPC) በሚገኘው የብሔራዊ ጽሑፍ መክፈቻን ለመጠበቅ እናስፈልጋለን፡፡ የፊደል ጽሑፉ ባለቤት (1) ጽሑፉን ለማንኛውም በክፍለ መልዕክት ማሳየት አያስፈልገውም (2) የጽሑፉ ክፍል ባለቤት የተጠማውን የዓይነት ማተሚያ ማተሚያዎች ለጽሑፍ ባለቤት ወይም ለሌላ ሰው ማናቸውንም ማሳየት አይገባውም፡፡ የብሔራዊ ጽሑፍ መግለጫ የፕሮጀክታችንን ፕሮጀክት ለማስታወቅ፣ በPyTorch-based MPC ፍሬማር ክራፕሬክትር ክራፕሬስቴን እናሳውቀው በተማረ ነገር ግን በተደጋጋጋሚ ስህተት ማጋራጨት ፕሮግራሙን በመጠቀም ነው፡፡ የግል አዳራሽ ጽሑፎችን ለመጠበቅ የግል ጊዜውን እናፈትናለን፤ በሥርዓት ለመጠቀም ይበቃል፡፡', 'hy': 'Տեքստի դասակարգումները պարբերաբար օգտագործվում են անձնական տեքստների վրա, թողնելով այս դասակարգումների օգտագործողներին խոցելի գաղտնիության խախտումների դեպքում: Մենք առաջարկում ենք լուծում սեփականության պահպանող տեքստի դասակարգման համար, որը հիմնված է Կանվոլյուցիոն Նեյրալ ցանցերի (CNN) և Անվտանգ բազմամասնության համակարգման (MPC) վրա: Մեր մեթոդը հնարավորություն է տալիս անձնական տեքստի դասակարգչային պիտակի եզրակացությունը այնպես, որ (1) անձնական տեքստի սեփականությունը պարտադիր չի տալիս բացահայտել իրենց տեքստը ոչ մեկին անծածկագրված կերպով, և (2) տեքստի դասակարգչակի սեփականությունը պարտադիր չի պարտադիր բացահայտել տեքստի սեփական Որպեսզի ապացուցենք մեր գործնական մասնավոր տեքստի դասակարգման պրոտոկոլի իրականացվությունը, մենք իրականացրեցինք այն Պիտորչի հիմնված MPC-ի շրջանակում "Քրիպտեն", օգտագործելով հայտնի գաղտնի ավելացիվ կիսման ծրագիր ազնիվ, բայց հետաքրքի Մենք փորձում ենք մեր սեփականության պահպանող տեքստի դասակարգիչը, որը բավականին արագ է, որպեսզի օգտագործվի իրականության մեջ:', 'bn': 'টেক্সট ব্যবহারকারীদের ব্যক্তিগত টেক্সটে নিয়মিত প্রয়োগ করা হয়, যারা এই ব্যবহারকারীদের ব্যক্তিগত ভাঙ্গার জন্য ব্যক্তিগত ভা আমরা প্রস্তাব করছি গোপনীয়তা সংরক্ষণের জন্য টেক্সট বিভাগের সমাধান যা কনভোলেশনের নেউরাল নেটওয়ার্ক (সিএনএন) এবং নিরাপদ বহুপার্টির কম্পিউট আমাদের পদ্ধতি ব্যক্তিগত টেক্সটের জন্য ব্যক্তিগত লেবেলের আক্রান্ত ব্যক্তিগত টেক্সটের আক্রান্ত ব্যবহার করে দেয় যেমন (১) ব্যক্তিগত টেক্সটের মালিকের কাছে তাদের টেক্সট প্রকাশ করা উচিত নয় অক্ প্রকৃত ব্যক্তিগত টেক্সটেক্সফিকেশনের জন্য আমাদের প্রোটোকলের বৈশিষ্ট্য প্রদর্শন করার জন্য আমরা এটি প্রকাশ করেছি পিয়টর্চ ভিত্তিক এমপিসি ফ্রেম্যাকার্ক ক্রিপ্ট আমরা আমাদের গোপনীয়তা সংরক্ষণের টেক্সট বিভাগের সময় পরীক্ষা করি, যা প্রাক্সিকে ব্যবহার করার জন্য যথেষ্ট দ্রুত।', 'az': 'Mətn klasifikatçıları müəyyən vaxtında kişisel mətnlərə uyğunlanır, bu klasifikatçıların istifadəçilərini gizli təhlükəsizlərə zəif qoyur. Biz Convolutional Neural Networks (CNNs) və Secure Multiparty Computation (MPC) təhlükəsizlik təhlükəsizlikləri üçün çözüm təklif edirik. Bizim metodumuz şəxsi metinin sahibinin mətnlərini heç kəsə şifrləməmiş bir şekilde göstərməsi lazımdır və (2) mətn klasifikatının sahibi mətn sahibinə və ya başqa kimsəyə öyrənməsi lazımdır. Praktik xüsusi mətn klasifikasyonu üçün protokolumuzun mümkün olduğunu göstərmək üçün, bunu PyTorch tabanlı MPC framework ü CrypTen vasitəsində istifadə etdik, dürüst-but-curious ayarlarda tanıdıqları qeyri gizli paylaşım taslağı istifadə etdik. Biz təhlükəsizlik təhlükəsizliyimiz məktub klasifikatının çalışma vaxtını sınağa çəkirik ki, praksiyada istifadə ediləcək qədər hızlı.', 'bs': 'Klasifikatori teksta se redovno primjenjuju na osobne tekstove, ostavljajući korisnike tih klasifikatora ranjive na poremećaje privatnosti. Mi predlažemo rješenje za klasifikaciju teksta za očuvanje privatnosti koja se temelji na konvolucionalnim neuronskim mrežama (CNNs) i sigurnom multipartijskom računalu (MPC). Naša metoda omogućava infekciju etikete klase za osobni tekst na način da (1) vlasnik osobnog teksta ne mora nikome otkrivati svoj tekst na neošifrovan način, a (2) vlasnik klasifikatora teksta ne mora otkrivati obučene modele parametre vlasniku teksta ili nikome drugom. Da bismo pokazali mogućnost našeg protokola za praktičnu privatnu klasifikaciju teksta, proveli smo ga u okviru MPC CrypTen na PyTorch-u, koristeći poznatu tajnu podjelu dodatne tajne u iskrenom, ali znatiželjnom postavku. Testiramo provod našeg klasifikatora privatnosti, koji je dovoljno brz da se koristi na praksi.', 'cs': 'Textové klasifikátory jsou pravidelně aplikovány na osobní texty, takže uživatelé těchto klasifikátorů jsou zranitelní porušení soukromí. Navrhujeme řešení pro klasifikaci textů zachovávající soukromí založené na konvelučních neuronových sítích (CNN) a Secure Multiparty Computing (MPC). Naše metoda umožňuje vyvodit třídní štítek pro osobní text takovým způsobem, že (1) vlastník osobního textu nemusí svůj text nikomu zveřejňovat nešifrovaně a (2) vlastník klasifikátoru textu nemusí prozrazovat trénované parametry modelu vlastníkovi textu nebo komukoli jinému. Abychom prokázali proveditelnost našeho protokolu pro praktickou klasifikaci soukromého textu, implementovali jsme ho do MPC frameworku CrypTen založeného na PyTorch, pomocí dobře známého aditivního tajného sdílení schématu v upřímném, ale zvědavém prostředí. Testujeme běh našeho textového klasifikátoru zachovávajícího ochranu soukromí, který je dostatečně rychlý pro použití v praxi.', 'tr': "hat Biz Convolutional Neural Networks (CNNs) ve Secure Multiparty Computation (MPC) üzerinde bulunan gizli saklama metin klasifikasyonu çözüm teklif ediyoruz. Biziň täbimiz şahsy metin üçin bir klas etişiniň azalyşyny mümkin edýär (1) şahsy metin eişiniň metini şifrelemeýän ýolça ýok bir şekilde a ýtmagyny mümkin edýär. we (2) metin klassifçisiniň jaýyny metin häzirine ýa-da başga biri üçin görkezilmeli modelleri belli etmeli däldir. Şahsy metin klasifikasyonuň praktik üçin protokollarymyzyň ýeterlikini görkezmek üçin, muny PyTorch-dan tabanly MPC çerçevesinde CrypTen'de implementetdik we dürüst-ýöne bilen tanyş bir gizli paylaşma taslamasynda ullanýardyk. Özlemämizi hasaplamak üçin metin klasifikatymyzyň başlangyşyny barýarys. Bu praktika üçin ýeterlik tiz.", 'fi': 'Tekstiluokituslaitteita käytetään säännöllisesti henkilökohtaisiin teksteihin, jolloin luokittelijoiden käyttäjät ovat alttiita yksityisyyden loukkauksille. Ehdotamme ratkaisua yksityisyyttä säilyttävään tekstiluokitukseen, joka perustuu konvolutional neural networks (CNN) ja Secure Multiparty Computation (MPC). Menetelmämme mahdollistaa henkilökohtaisen tekstin luokkamerkinnän päättelyn siten, että (1) henkilökohtaisen tekstin omistajan ei tarvitse paljastaa tekstiä kenellekään salaamattomalla tavalla ja (2) tekstin luokittelijan omistajan ei tarvitse paljastaa koulutettuja malliparametreja tekstin omistajalle tai kenellekään muulle. Osoittaaksemme käytännön yksityistekstien luokitteluprotokollamme toteutettavuuden toteutimme sen PyTorch-pohjaisessa MPC-kehyksessä CrypTen, käyttäen tunnettua additiivista salaista jakamista rehellisessä mutta uteliaassa ympäristössä. Testaamme yksityisyyttä säilyttävän tekstiluokittelijamme käyttöaikaa, joka on riittävän nopea käytettäväksi käytännössä.', 'ca': "Els classificadors de text s'aplican regularment als textos personals, deixant els usuaris d'aquests classificadors vulnerables a violacions de la privacitat. We propose a solution for privacy-preserving text classification that is based on Convolutional Neural Networks (CNNs) and Secure Multiparty Computation (MPC).  El nostre mètode permet la inferència d'una etiqueta de classe per un text personal de manera que (1) el propietari del text personal no ha de divulgar el seu text a ningú d'una manera no criptata, i (2) el propietari del classificador de text no ha de revelar els paràmetres del model entrenat al propietari del text o a ningú altre. Per demostrar la viabilitat del nostre protocol de classificació pràctica de textos privats, el vam implementar en el marc MPC CrypTen basat en PyTorch, utilitzant un conegut esquema de compartir secrets aditivs en un entorn honest però curios. Testem el temps d'execució del nostre classificador de text que conserva la privacitat, que és prou ràpid per ser utilitzat en la pràctica.", 'et': 'Teksti klassifitseerijaid rakendatakse regulaarselt isiklikele tekstidele, mistõttu nende klassifitseerijate kasutajad on privaatsuse rikkumiste suhtes haavatavad. Pakume välja lahenduse privaatsust säilitava teksti klassifitseerimiseks, mis põhineb konvolutsioonivõrkudel (CNN) ja turvalisel mitmepoolsel arvutusel (MPC). Meie meetod võimaldab isiklikule tekstile klassimärgist järeldada nii, et (1) isikliku teksti omanik ei pea oma teksti krüptimata avaldama kellelegi ja (2) tekstiklassifitseerija omanik ei pea väljaõpetatud mudeli parameetreid teksti omanikule ega kellelegi teisele avaldama. Et demonstreerida praktilise privaatse teksti klassifitseerimise protokolli teostatavust, rakendasime selle PyTorch-põhises MPC raamistikus CrypTen, kasutades ausas, kuid uudishimulikus keskkonnas tuntud lisanduva salajagamisskeemi. Testime privaatsust säilitava tekstiklassifikaatori tööaega, mis on praktikas kasutamiseks piisavalt kiire.', 'jv': 'email-custom-header-Security Awak dhéwé nggunaké perusahaan kanggo nggawe Kemerdekasasi Kemerdekasasi Teks sing basa ning convolutional Neral networks (SENs) lan Seure Multiparti komputasi (MP C). Rasané awak dhéwé iso nglanggar etiket sing kelas kanggo teks pribadi kanggo maneh (1) sak pernik teks pribadi kudu nggawe teks dhéwé nang sampeyan seneng gak dhéwé, lan (2) sak kelas terakhir teks kudu nambah dhéwé Ngawe ngomongke asnaning kapasithik protokol kanggo kelas nang teks pribadi sing pratike, kita mulai nggawe lan akeh dumadhi mpC sing basa nang Pirtorch batar cripTen, nambah sekretos sing wis ngerasakno nang sekretos ora bisa dumadhi, nganggo iso dianggap nggo ngerasakno Awak dhéwé éntuk sistem sing paling-sistem kanggo nggawe perusahaan batasan teks, iki dadi nyimpen kanggo nggawe praksi.', 'he': 'מסגרי טקסט מופעילים באופן קבוע לטקסטים אישיים, משאירים משתמשים של מסגרים אלה פגיעים לפרצות פרטיות. We propose a solution for privacy-preserving text classification that is based on Convolutional Neural Networks (CNNs) and Secure Multiparty Computation (MPC).  Our method enables the inference of a class label for a personal text in such a way that (1) the owner of the personal text does not have to disclose their text to anyone in an unencrypted manner, and (2) the owner of the text classifier does not have to reveal the trained model parameters to the text owner or to anyone else.  כדי להוכיח את האפשרות של הפרוטוקול שלנו לטקסט מסווג פרטי מעשי, הפענו אותו במסגרת MPC המבוססת על PyTorch CrypTen, באמצעות תכנית שיתוף סודי תוספים ידועה היטב במסגרת כנה-אבל-סקרנית. אנחנו בודקים את הזמן של מסגרת הטקסט שלנו לשמור על פרטיות, שהיא מספיק מהירה כדי להשתמש באימון.', 'bo': 'ཡི་གེའི་དབྱིབས་ཡིག་ཆ་རྣམས་སྒེར་གྱི་ཡི་གེའི་ནང་དུ་སྤྱོད་མཁན་གྱི་དབྱིབས་ཡིག་ཆ་རྣམས་མེད་པའི་སྒེར་གྱི་འགལ ང་ཚོས་རང་དབང་སྒེར་གྱི་ཞབས་ཞུ་བའི་ཡིག་གེ་དབྱེ་རིམ་གྱི་ཐབས་ཤེས་སྟོན་པ་ཞིག་བསམ Our method enables the inference of a class label for a personal text in such a way that (1) the owner of the personal text does not have to disclose their text to anyone in an unencrypted manner, and (2) the owner of the text classifier does not have to reveal the trained model parameters to the text owner or to anyone else. To demonstrate the feasibility of our protocol for practical private text classification, we implemented it in the PyTorch-based MPC framework CrypTen, using a well-known additive secret sharing scheme in the honest-but-curious setting. ང་ཚོས་རང་ཉིད་ཀྱི་མི་སྒེར་གྱི་ཉར་འཇོག་པའི་ཡིག', 'sk': 'Besedilni klasifikatorji se redno uporabljajo za osebna besedila, zaradi česar so uporabniki teh klasifikatorjev izpostavljeni kršitvam zasebnosti. Predlagamo rešitev za klasifikacijo besedila, ki ohranja zasebnost, ki temelji na konvolucijskih nevralnih omrežjih (CNN) in varnem večstranskem računalništvu (MPC). Naša metoda omogoča sklepanje oznake razreda za osebno besedilo tako, da (1) lastniku osebnega besedila ni treba razkriti svojega besedila nikomur na nešifriran način in (2) lastniku besedilnega klasifikatorja ni treba razkriti usposobljenih parametrov modela lastniku besedila ali komurkoli drugemu. Da bi dokazali izvedljivost našega protokola za praktično klasifikacijo zasebnega besedila, smo ga uvedli v okviru MPC CrypTen, ki temelji na PyTorch, z uporabo dobro znane sheme dodatne skrivnosti delitve v iskrenem, a radovednem okolju. Preizkusimo čas delovanja našega klasifikatorja besedila, ki ohranja zasebnost, ki je dovolj hiter za uporabo v praksi.', 'ha': "An karɓi misalin matsayin a karɓi takardan mutane da ake saka wa masu amfani da waɗannan darafõfi masu zartar da su zuwa abubuwa farat ɗaya. Tuna goyya da kafin wa tsari matsayin farat ɗaya wanda ke asansa da KCNNNs da Computation na Tsakiya Tsarin mu na zartar da kafin wani alama na fasani wa matsayin shakka kamar misali wanda (1) ma'abũcin rubutun na shawara ba ya kamata ya bayyana matsayin zuwa wani wanda ba'a yi banyaye ba, kuma (2) ma'abũcin mai daraja na matsayin, ba ya kamata in bayyana parameters da aka sanar da shi zuwa ma'an an matsayin ko ga wanda ya kasa wani. To, dõmin ya nuna masu tsarin lokacolinmu wa mai fassarar matsayin farat ɗaya, sai mu yi amfani da shi a cikin firam-rubutun MC na PyThch-based, rubutun rubutun na PyToch, don a yi amfani da wani shirin siri na daban da aka sani a cikin tsarin da-gaskiyar-da-gaskiyar. We test the runtime of our privacy-preserving text classifier, which is fast enough to be used in practice."}
